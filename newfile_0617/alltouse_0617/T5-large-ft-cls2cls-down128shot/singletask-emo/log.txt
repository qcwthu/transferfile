05/21/2022 21:31:07 - INFO - __main__ - Namespace(task_dir='data_128/emo/', task_name='emo', identifier='T5-large-ft-cls2cls-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:31:07 - INFO - __main__ - models/T5-large-ft-cls2cls-down128shot/singletask-emo
05/21/2022 21:31:07 - INFO - __main__ - Namespace(task_dir='data_128/emo/', task_name='emo', identifier='T5-large-ft-cls2cls-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:31:07 - INFO - __main__ - models/T5-large-ft-cls2cls-down128shot/singletask-emo
05/21/2022 21:31:09 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:31:09 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:31:09 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:31:09 - INFO - __main__ - Using 2 gpus
05/21/2022 21:31:09 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:31:09 - INFO - __main__ - Using 2 gpus
05/21/2022 21:31:09 - INFO - __main__ - Fine-tuning the following samples: ['emo_128_100', 'emo_128_13', 'emo_128_21', 'emo_128_42', 'emo_128_87']
05/21/2022 21:31:09 - INFO - __main__ - Fine-tuning the following samples: ['emo_128_100', 'emo_128_13', 'emo_128_21', 'emo_128_42', 'emo_128_87']
05/21/2022 21:31:14 - INFO - __main__ - Running ... prefix=emo_128_100, lr=0.0005, bsz=8 ...
05/21/2022 21:31:15 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 21:31:15 - INFO - __main__ - Printing 3 examples
05/21/2022 21:31:15 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 21:31:15 - INFO - __main__ - ['others']
05/21/2022 21:31:15 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 21:31:15 - INFO - __main__ - ['others']
05/21/2022 21:31:15 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 21:31:15 - INFO - __main__ - ['others']
05/21/2022 21:31:15 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:31:15 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 21:31:15 - INFO - __main__ - Printing 3 examples
05/21/2022 21:31:15 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 21:31:15 - INFO - __main__ - ['others']
05/21/2022 21:31:15 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 21:31:15 - INFO - __main__ - ['others']
05/21/2022 21:31:15 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 21:31:15 - INFO - __main__ - ['others']
05/21/2022 21:31:15 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:31:15 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:31:15 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:31:16 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 21:31:16 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 21:31:16 - INFO - __main__ - Printing 3 examples
05/21/2022 21:31:16 - INFO - __main__ -  [emo] when when it comes to you never why
05/21/2022 21:31:16 - INFO - __main__ - ['others']
05/21/2022 21:31:16 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/21/2022 21:31:16 - INFO - __main__ - ['others']
05/21/2022 21:31:16 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/21/2022 21:31:16 - INFO - __main__ - ['others']
05/21/2022 21:31:16 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:31:16 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 21:31:16 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 21:31:16 - INFO - __main__ - Printing 3 examples
05/21/2022 21:31:16 - INFO - __main__ -  [emo] when when it comes to you never why
05/21/2022 21:31:16 - INFO - __main__ - ['others']
05/21/2022 21:31:16 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/21/2022 21:31:16 - INFO - __main__ - ['others']
05/21/2022 21:31:16 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/21/2022 21:31:16 - INFO - __main__ - ['others']
05/21/2022 21:31:16 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:31:16 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:31:16 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:31:17 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 21:31:17 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 21:31:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 21:31:30 - INFO - __main__ - Starting training!
05/21/2022 21:31:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 21:31:30 - INFO - __main__ - Starting training!
05/21/2022 21:31:35 - INFO - __main__ - Step 10 Global step 10 Train loss 24.482279 on epoch=0
05/21/2022 21:31:40 - INFO - __main__ - Step 20 Global step 20 Train loss 19.265154 on epoch=0
05/21/2022 21:31:45 - INFO - __main__ - Step 30 Global step 30 Train loss 16.559540 on epoch=0
05/21/2022 21:31:50 - INFO - __main__ - Step 40 Global step 40 Train loss 15.286235 on epoch=1
05/21/2022 21:31:55 - INFO - __main__ - Step 50 Global step 50 Train loss 13.542521 on epoch=1
05/21/2022 21:31:59 - INFO - __main__ - Global step 50 Train loss 17.827147 Classification-F1 0.0 on epoch=1
05/21/2022 21:32:06 - INFO - __main__ - Step 60 Global step 60 Train loss 12.846827 on epoch=1
05/21/2022 21:32:11 - INFO - __main__ - Step 70 Global step 70 Train loss 10.317194 on epoch=2
05/21/2022 21:32:16 - INFO - __main__ - Step 80 Global step 80 Train loss 6.408072 on epoch=2
05/21/2022 21:32:21 - INFO - __main__ - Step 90 Global step 90 Train loss 4.215980 on epoch=2
05/21/2022 21:32:26 - INFO - __main__ - Step 100 Global step 100 Train loss 3.095397 on epoch=3
05/21/2022 21:32:30 - INFO - __main__ - Global step 100 Train loss 7.376694 Classification-F1 0.1 on epoch=3
05/21/2022 21:32:38 - INFO - __main__ - Step 110 Global step 110 Train loss 3.329873 on epoch=3
05/21/2022 21:32:43 - INFO - __main__ - Step 120 Global step 120 Train loss 3.507706 on epoch=3
05/21/2022 21:32:48 - INFO - __main__ - Step 130 Global step 130 Train loss 3.058239 on epoch=4
05/21/2022 21:32:53 - INFO - __main__ - Step 140 Global step 140 Train loss 2.671659 on epoch=4
05/21/2022 21:32:59 - INFO - __main__ - Step 150 Global step 150 Train loss 2.697944 on epoch=4
05/21/2022 21:33:03 - INFO - __main__ - Global step 150 Train loss 3.053084 Classification-F1 0.1 on epoch=4
05/21/2022 21:33:08 - INFO - __main__ - Step 160 Global step 160 Train loss 2.503968 on epoch=4
05/21/2022 21:33:13 - INFO - __main__ - Step 170 Global step 170 Train loss 2.644927 on epoch=5
05/21/2022 21:33:18 - INFO - __main__ - Step 180 Global step 180 Train loss 1.744399 on epoch=5
05/21/2022 21:33:23 - INFO - __main__ - Step 190 Global step 190 Train loss 1.783091 on epoch=5
05/21/2022 21:33:29 - INFO - __main__ - Step 200 Global step 200 Train loss 1.505584 on epoch=6
05/21/2022 21:33:33 - INFO - __main__ - Global step 200 Train loss 2.036394 Classification-F1 0.2053917515607103 on epoch=6
05/21/2022 21:33:39 - INFO - __main__ - Step 210 Global step 210 Train loss 1.387017 on epoch=6
05/21/2022 21:33:44 - INFO - __main__ - Step 220 Global step 220 Train loss 1.652461 on epoch=6
05/21/2022 21:33:49 - INFO - __main__ - Step 230 Global step 230 Train loss 1.190413 on epoch=7
05/21/2022 21:33:54 - INFO - __main__ - Step 240 Global step 240 Train loss 1.217294 on epoch=7
05/21/2022 21:34:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.989261 on epoch=7
05/21/2022 21:34:04 - INFO - __main__ - Global step 250 Train loss 1.287289 Classification-F1 0.11912393162393162 on epoch=7
05/21/2022 21:34:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.851937 on epoch=8
05/21/2022 21:34:14 - INFO - __main__ - Step 270 Global step 270 Train loss 1.030007 on epoch=8
05/21/2022 21:34:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.958926 on epoch=8
05/21/2022 21:34:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.848833 on epoch=9
05/21/2022 21:34:30 - INFO - __main__ - Step 300 Global step 300 Train loss 1.027588 on epoch=9
05/21/2022 21:34:34 - INFO - __main__ - Global step 300 Train loss 0.943458 Classification-F1 0.14552439088564964 on epoch=9
05/21/2022 21:34:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.797144 on epoch=9
05/21/2022 21:34:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.802079 on epoch=9
05/21/2022 21:34:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.805781 on epoch=10
05/21/2022 21:34:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.797642 on epoch=10
05/21/2022 21:34:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.810650 on epoch=10
05/21/2022 21:35:03 - INFO - __main__ - Global step 350 Train loss 0.802659 Classification-F1 0.4004190551439148 on epoch=10
05/21/2022 21:35:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.787280 on epoch=11
05/21/2022 21:35:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.792531 on epoch=11
05/21/2022 21:35:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.777052 on epoch=11
05/21/2022 21:35:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.786431 on epoch=12
05/21/2022 21:35:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.831546 on epoch=12
05/21/2022 21:35:33 - INFO - __main__ - Global step 400 Train loss 0.794968 Classification-F1 0.21485991689636727 on epoch=12
05/21/2022 21:35:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.727739 on epoch=12
05/21/2022 21:35:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.681857 on epoch=13
05/21/2022 21:35:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.758860 on epoch=13
05/21/2022 21:35:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.774719 on epoch=13
05/21/2022 21:35:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.654720 on epoch=14
05/21/2022 21:36:02 - INFO - __main__ - Global step 450 Train loss 0.719579 Classification-F1 0.40526177866877267 on epoch=14
05/21/2022 21:36:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.740137 on epoch=14
05/21/2022 21:36:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.636833 on epoch=14
05/21/2022 21:36:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.657783 on epoch=14
05/21/2022 21:36:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.649621 on epoch=15
05/21/2022 21:36:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.566481 on epoch=15
05/21/2022 21:36:32 - INFO - __main__ - Global step 500 Train loss 0.650171 Classification-F1 0.4711457479619146 on epoch=15
05/21/2022 21:36:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.711224 on epoch=15
05/21/2022 21:36:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.568700 on epoch=16
05/21/2022 21:36:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.628444 on epoch=16
05/21/2022 21:36:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.682656 on epoch=16
05/21/2022 21:36:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.472963 on epoch=17
05/21/2022 21:37:02 - INFO - __main__ - Global step 550 Train loss 0.612797 Classification-F1 0.5806016875816389 on epoch=17
05/21/2022 21:37:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.624753 on epoch=17
05/21/2022 21:37:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.484736 on epoch=17
05/21/2022 21:37:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.522156 on epoch=18
05/21/2022 21:37:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.530428 on epoch=18
05/21/2022 21:37:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.456423 on epoch=18
05/21/2022 21:37:33 - INFO - __main__ - Global step 600 Train loss 0.523699 Classification-F1 0.6726436843444515 on epoch=18
05/21/2022 21:37:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.403288 on epoch=19
05/21/2022 21:37:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.443857 on epoch=19
05/21/2022 21:37:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.522841 on epoch=19
05/21/2022 21:37:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.418540 on epoch=19
05/21/2022 21:38:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.350285 on epoch=20
05/21/2022 21:38:04 - INFO - __main__ - Global step 650 Train loss 0.427762 Classification-F1 0.71704982452646 on epoch=20
05/21/2022 21:38:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.339603 on epoch=20
05/21/2022 21:38:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.463585 on epoch=20
05/21/2022 21:38:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.287219 on epoch=21
05/21/2022 21:38:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.315662 on epoch=21
05/21/2022 21:38:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.373171 on epoch=21
05/21/2022 21:38:35 - INFO - __main__ - Global step 700 Train loss 0.355848 Classification-F1 0.7455543301370687 on epoch=21
05/21/2022 21:38:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.338427 on epoch=22
05/21/2022 21:38:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.221740 on epoch=22
05/21/2022 21:38:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.297275 on epoch=22
05/21/2022 21:38:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.253870 on epoch=23
05/21/2022 21:39:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.316319 on epoch=23
05/21/2022 21:39:06 - INFO - __main__ - Global step 750 Train loss 0.285526 Classification-F1 0.6905661116103776 on epoch=23
05/21/2022 21:39:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.341767 on epoch=23
05/21/2022 21:39:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.189359 on epoch=24
05/21/2022 21:39:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.253390 on epoch=24
05/21/2022 21:39:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.360933 on epoch=24
05/21/2022 21:39:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.229697 on epoch=24
05/21/2022 21:39:36 - INFO - __main__ - Global step 800 Train loss 0.275029 Classification-F1 0.7304560253260941 on epoch=24
05/21/2022 21:39:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.172148 on epoch=25
05/21/2022 21:39:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.137368 on epoch=25
05/21/2022 21:39:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.272101 on epoch=25
05/21/2022 21:39:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.164141 on epoch=26
05/21/2022 21:40:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.153275 on epoch=26
05/21/2022 21:40:06 - INFO - __main__ - Global step 850 Train loss 0.179806 Classification-F1 0.7413399290485704 on epoch=26
05/21/2022 21:40:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.137536 on epoch=26
05/21/2022 21:40:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.113632 on epoch=27
05/21/2022 21:40:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.118205 on epoch=27
05/21/2022 21:40:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.113832 on epoch=27
05/21/2022 21:40:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.161373 on epoch=28
05/21/2022 21:40:36 - INFO - __main__ - Global step 900 Train loss 0.128915 Classification-F1 0.7252919515993119 on epoch=28
05/21/2022 21:40:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.093709 on epoch=28
05/21/2022 21:40:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.156288 on epoch=28
05/21/2022 21:40:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.126716 on epoch=29
05/21/2022 21:40:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.072508 on epoch=29
05/21/2022 21:41:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.024764 on epoch=29
05/21/2022 21:41:06 - INFO - __main__ - Global step 950 Train loss 0.094797 Classification-F1 0.756315757629337 on epoch=29
05/21/2022 21:41:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.139733 on epoch=29
05/21/2022 21:41:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.051770 on epoch=30
05/21/2022 21:41:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.117063 on epoch=30
05/21/2022 21:41:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.067465 on epoch=30
05/21/2022 21:41:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.032983 on epoch=31
05/21/2022 21:41:33 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 21:41:33 - INFO - __main__ - Printing 3 examples
05/21/2022 21:41:33 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 21:41:33 - INFO - __main__ - ['others']
05/21/2022 21:41:33 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 21:41:33 - INFO - __main__ - ['others']
05/21/2022 21:41:33 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 21:41:33 - INFO - __main__ - ['others']
05/21/2022 21:41:33 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:41:34 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:41:34 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 21:41:34 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 21:41:34 - INFO - __main__ - Printing 3 examples
05/21/2022 21:41:34 - INFO - __main__ -  [emo] when when it comes to you never why
05/21/2022 21:41:34 - INFO - __main__ - ['others']
05/21/2022 21:41:34 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/21/2022 21:41:34 - INFO - __main__ - ['others']
05/21/2022 21:41:34 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/21/2022 21:41:34 - INFO - __main__ - ['others']
05/21/2022 21:41:34 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:41:34 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:41:35 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 21:41:36 - INFO - __main__ - Global step 1000 Train loss 0.081803 Classification-F1 0.7615879005632307 on epoch=31
05/21/2022 21:41:37 - INFO - __main__ - save last model!
05/21/2022 21:41:44 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 21:41:45 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 21:41:45 - INFO - __main__ - Printing 3 examples
05/21/2022 21:41:45 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 21:41:45 - INFO - __main__ - ['others']
05/21/2022 21:41:45 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 21:41:45 - INFO - __main__ - ['others']
05/21/2022 21:41:45 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 21:41:45 - INFO - __main__ - ['others']
05/21/2022 21:41:45 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:41:47 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:41:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 21:41:48 - INFO - __main__ - Starting training!
05/21/2022 21:41:52 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 21:42:33 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_100_0.0005_8_predictions.txt
05/21/2022 21:42:33 - INFO - __main__ - Classification-F1 on test data: 0.4882
05/21/2022 21:42:34 - INFO - __main__ - prefix=emo_128_100, lr=0.0005, bsz=8, dev_performance=0.7615879005632307, test_performance=0.48822713112657656
05/21/2022 21:42:34 - INFO - __main__ - Running ... prefix=emo_128_100, lr=0.0003, bsz=8 ...
05/21/2022 21:42:35 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 21:42:35 - INFO - __main__ - Printing 3 examples
05/21/2022 21:42:35 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 21:42:35 - INFO - __main__ - ['others']
05/21/2022 21:42:35 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 21:42:35 - INFO - __main__ - ['others']
05/21/2022 21:42:35 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 21:42:35 - INFO - __main__ - ['others']
05/21/2022 21:42:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:42:35 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:42:35 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 21:42:35 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 21:42:35 - INFO - __main__ - Printing 3 examples
05/21/2022 21:42:35 - INFO - __main__ -  [emo] when when it comes to you never why
05/21/2022 21:42:35 - INFO - __main__ - ['others']
05/21/2022 21:42:35 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/21/2022 21:42:35 - INFO - __main__ - ['others']
05/21/2022 21:42:35 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/21/2022 21:42:35 - INFO - __main__ - ['others']
05/21/2022 21:42:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:42:36 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:42:36 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 21:42:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 21:42:47 - INFO - __main__ - Starting training!
05/21/2022 21:42:52 - INFO - __main__ - Step 10 Global step 10 Train loss 24.657768 on epoch=0
05/21/2022 21:42:57 - INFO - __main__ - Step 20 Global step 20 Train loss 21.705919 on epoch=0
05/21/2022 21:43:02 - INFO - __main__ - Step 30 Global step 30 Train loss 18.182516 on epoch=0
05/21/2022 21:43:07 - INFO - __main__ - Step 40 Global step 40 Train loss 16.082876 on epoch=1
05/21/2022 21:43:12 - INFO - __main__ - Step 50 Global step 50 Train loss 15.786769 on epoch=1
05/21/2022 21:43:52 - INFO - __main__ - Global step 50 Train loss 19.283171 Classification-F1 0.0 on epoch=1
05/21/2022 21:43:58 - INFO - __main__ - Step 60 Global step 60 Train loss 14.521826 on epoch=1
05/21/2022 21:44:03 - INFO - __main__ - Step 70 Global step 70 Train loss 14.459384 on epoch=2
05/21/2022 21:44:08 - INFO - __main__ - Step 80 Global step 80 Train loss 13.649054 on epoch=2
05/21/2022 21:44:13 - INFO - __main__ - Step 90 Global step 90 Train loss 13.116760 on epoch=2
05/21/2022 21:44:19 - INFO - __main__ - Step 100 Global step 100 Train loss 11.488461 on epoch=3
05/21/2022 21:44:28 - INFO - __main__ - Global step 100 Train loss 13.447097 Classification-F1 0.0 on epoch=3
05/21/2022 21:44:33 - INFO - __main__ - Step 110 Global step 110 Train loss 11.407112 on epoch=3
05/21/2022 21:44:38 - INFO - __main__ - Step 120 Global step 120 Train loss 9.855205 on epoch=3
05/21/2022 21:44:43 - INFO - __main__ - Step 130 Global step 130 Train loss 8.414777 on epoch=4
05/21/2022 21:44:48 - INFO - __main__ - Step 140 Global step 140 Train loss 7.373513 on epoch=4
05/21/2022 21:44:54 - INFO - __main__ - Step 150 Global step 150 Train loss 6.066880 on epoch=4
05/21/2022 21:44:58 - INFO - __main__ - Global step 150 Train loss 8.623497 Classification-F1 0.025341130604288498 on epoch=4
05/21/2022 21:45:03 - INFO - __main__ - Step 160 Global step 160 Train loss 3.584208 on epoch=4
05/21/2022 21:45:08 - INFO - __main__ - Step 170 Global step 170 Train loss 2.882860 on epoch=5
05/21/2022 21:45:13 - INFO - __main__ - Step 180 Global step 180 Train loss 3.425154 on epoch=5
05/21/2022 21:45:18 - INFO - __main__ - Step 190 Global step 190 Train loss 3.436212 on epoch=5
05/21/2022 21:45:24 - INFO - __main__ - Step 200 Global step 200 Train loss 2.845096 on epoch=6
05/21/2022 21:45:27 - INFO - __main__ - Global step 200 Train loss 3.234706 Classification-F1 0.1 on epoch=6
05/21/2022 21:45:33 - INFO - __main__ - Step 210 Global step 210 Train loss 3.621083 on epoch=6
05/21/2022 21:45:38 - INFO - __main__ - Step 220 Global step 220 Train loss 2.662739 on epoch=6
05/21/2022 21:45:43 - INFO - __main__ - Step 230 Global step 230 Train loss 2.201497 on epoch=7
05/21/2022 21:45:49 - INFO - __main__ - Step 240 Global step 240 Train loss 3.746482 on epoch=7
05/21/2022 21:45:54 - INFO - __main__ - Step 250 Global step 250 Train loss 2.310906 on epoch=7
05/21/2022 21:45:57 - INFO - __main__ - Global step 250 Train loss 2.908542 Classification-F1 0.24874795432495422 on epoch=7
05/21/2022 21:46:03 - INFO - __main__ - Step 260 Global step 260 Train loss 3.435498 on epoch=8
05/21/2022 21:46:08 - INFO - __main__ - Step 270 Global step 270 Train loss 2.640514 on epoch=8
05/21/2022 21:46:13 - INFO - __main__ - Step 280 Global step 280 Train loss 1.671375 on epoch=8
05/21/2022 21:46:18 - INFO - __main__ - Step 290 Global step 290 Train loss 1.098460 on epoch=9
05/21/2022 21:46:23 - INFO - __main__ - Step 300 Global step 300 Train loss 1.096220 on epoch=9
05/21/2022 21:46:27 - INFO - __main__ - Global step 300 Train loss 1.988414 Classification-F1 0.1030587522713507 on epoch=9
05/21/2022 21:46:32 - INFO - __main__ - Step 310 Global step 310 Train loss 1.105675 on epoch=9
05/21/2022 21:46:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.969462 on epoch=9
05/21/2022 21:46:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.892925 on epoch=10
05/21/2022 21:46:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.875834 on epoch=10
05/21/2022 21:46:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.834116 on epoch=10
05/21/2022 21:46:56 - INFO - __main__ - Global step 350 Train loss 0.935603 Classification-F1 0.24024683946686587 on epoch=10
05/21/2022 21:47:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.971074 on epoch=11
05/21/2022 21:47:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.879075 on epoch=11
05/21/2022 21:47:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.940419 on epoch=11
05/21/2022 21:47:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.710113 on epoch=12
05/21/2022 21:47:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.826769 on epoch=12
05/21/2022 21:47:25 - INFO - __main__ - Global step 400 Train loss 0.865490 Classification-F1 0.22260213127086192 on epoch=12
05/21/2022 21:47:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.762270 on epoch=12
05/21/2022 21:47:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.772706 on epoch=13
05/21/2022 21:47:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.847745 on epoch=13
05/21/2022 21:47:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.783177 on epoch=13
05/21/2022 21:47:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.778072 on epoch=14
05/21/2022 21:47:54 - INFO - __main__ - Global step 450 Train loss 0.788794 Classification-F1 0.26627951985661336 on epoch=14
05/21/2022 21:48:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.720046 on epoch=14
05/21/2022 21:48:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.796012 on epoch=14
05/21/2022 21:48:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.795642 on epoch=14
05/21/2022 21:48:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.734177 on epoch=15
05/21/2022 21:48:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.658175 on epoch=15
05/21/2022 21:48:24 - INFO - __main__ - Global step 500 Train loss 0.740810 Classification-F1 0.3661901406087452 on epoch=15
05/21/2022 21:48:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.788924 on epoch=15
05/21/2022 21:48:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.715484 on epoch=16
05/21/2022 21:48:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.740655 on epoch=16
05/21/2022 21:48:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.748194 on epoch=16
05/21/2022 21:48:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.758364 on epoch=17
05/21/2022 21:48:55 - INFO - __main__ - Global step 550 Train loss 0.750324 Classification-F1 0.41548989670166897 on epoch=17
05/21/2022 21:49:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.759043 on epoch=17
05/21/2022 21:49:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.807299 on epoch=17
05/21/2022 21:49:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.673123 on epoch=18
05/21/2022 21:49:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.775159 on epoch=18
05/21/2022 21:49:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.762844 on epoch=18
05/21/2022 21:49:25 - INFO - __main__ - Global step 600 Train loss 0.755493 Classification-F1 0.3325352651188549 on epoch=18
05/21/2022 21:49:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.715730 on epoch=19
05/21/2022 21:49:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.744571 on epoch=19
05/21/2022 21:49:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.733496 on epoch=19
05/21/2022 21:49:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.732509 on epoch=19
05/21/2022 21:49:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.765613 on epoch=20
05/21/2022 21:49:55 - INFO - __main__ - Global step 650 Train loss 0.738384 Classification-F1 0.3688659612811032 on epoch=20
05/21/2022 21:50:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.718762 on epoch=20
05/21/2022 21:50:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.691915 on epoch=20
05/21/2022 21:50:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.682302 on epoch=21
05/21/2022 21:50:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.701162 on epoch=21
05/21/2022 21:50:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.749157 on epoch=21
05/21/2022 21:50:25 - INFO - __main__ - Global step 700 Train loss 0.708659 Classification-F1 0.4199186837155807 on epoch=21
05/21/2022 21:50:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.699276 on epoch=22
05/21/2022 21:50:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.702238 on epoch=22
05/21/2022 21:50:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.674785 on epoch=22
05/21/2022 21:50:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.701043 on epoch=23
05/21/2022 21:50:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.733888 on epoch=23
05/21/2022 21:50:56 - INFO - __main__ - Global step 750 Train loss 0.702246 Classification-F1 0.5372391185359702 on epoch=23
05/21/2022 21:51:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.658126 on epoch=23
05/21/2022 21:51:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.671811 on epoch=24
05/21/2022 21:51:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.675531 on epoch=24
05/21/2022 21:51:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.737399 on epoch=24
05/21/2022 21:51:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.688069 on epoch=24
05/21/2022 21:51:27 - INFO - __main__ - Global step 800 Train loss 0.686187 Classification-F1 0.6176516258677385 on epoch=24
05/21/2022 21:51:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.689143 on epoch=25
05/21/2022 21:51:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.604698 on epoch=25
05/21/2022 21:51:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.657575 on epoch=25
05/21/2022 21:51:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.564339 on epoch=26
05/21/2022 21:51:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.604461 on epoch=26
05/21/2022 21:51:57 - INFO - __main__ - Global step 850 Train loss 0.624043 Classification-F1 0.46507427657883715 on epoch=26
05/21/2022 21:52:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.574140 on epoch=26
05/21/2022 21:52:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.596488 on epoch=27
05/21/2022 21:52:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.586377 on epoch=27
05/21/2022 21:52:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.604752 on epoch=27
05/21/2022 21:52:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.567253 on epoch=28
05/21/2022 21:52:27 - INFO - __main__ - Global step 900 Train loss 0.585802 Classification-F1 0.5362694361183964 on epoch=28
05/21/2022 21:52:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.567800 on epoch=28
05/21/2022 21:52:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.580971 on epoch=28
05/21/2022 21:52:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.636796 on epoch=29
05/21/2022 21:52:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.498340 on epoch=29
05/21/2022 21:52:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.471277 on epoch=29
05/21/2022 21:52:57 - INFO - __main__ - Global step 950 Train loss 0.551037 Classification-F1 0.6056665467517588 on epoch=29
05/21/2022 21:53:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.546569 on epoch=29
05/21/2022 21:53:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.481017 on epoch=30
05/21/2022 21:53:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.445299 on epoch=30
05/21/2022 21:53:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.544769 on epoch=30
05/21/2022 21:53:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.509460 on epoch=31
05/21/2022 21:53:24 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 21:53:24 - INFO - __main__ - Printing 3 examples
05/21/2022 21:53:24 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 21:53:24 - INFO - __main__ - ['others']
05/21/2022 21:53:24 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 21:53:24 - INFO - __main__ - ['others']
05/21/2022 21:53:24 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 21:53:24 - INFO - __main__ - ['others']
05/21/2022 21:53:24 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:53:24 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:53:25 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 21:53:25 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 21:53:25 - INFO - __main__ - Printing 3 examples
05/21/2022 21:53:25 - INFO - __main__ -  [emo] when when it comes to you never why
05/21/2022 21:53:25 - INFO - __main__ - ['others']
05/21/2022 21:53:25 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/21/2022 21:53:25 - INFO - __main__ - ['others']
05/21/2022 21:53:25 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/21/2022 21:53:25 - INFO - __main__ - ['others']
05/21/2022 21:53:25 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:53:25 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:53:26 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 21:53:27 - INFO - __main__ - Global step 1000 Train loss 0.505423 Classification-F1 0.5454108013824301 on epoch=31
05/21/2022 21:53:27 - INFO - __main__ - save last model!
05/21/2022 21:53:34 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 21:53:35 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 21:53:35 - INFO - __main__ - Printing 3 examples
05/21/2022 21:53:35 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 21:53:35 - INFO - __main__ - ['others']
05/21/2022 21:53:35 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 21:53:35 - INFO - __main__ - ['others']
05/21/2022 21:53:35 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 21:53:35 - INFO - __main__ - ['others']
05/21/2022 21:53:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:53:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 21:53:36 - INFO - __main__ - Starting training!
05/21/2022 21:53:37 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:53:42 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 21:54:25 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_100_0.0003_8_predictions.txt
05/21/2022 21:54:25 - INFO - __main__ - Classification-F1 on test data: 0.5130
05/21/2022 21:54:25 - INFO - __main__ - prefix=emo_128_100, lr=0.0003, bsz=8, dev_performance=0.6176516258677385, test_performance=0.5130457699714427
05/21/2022 21:54:25 - INFO - __main__ - Running ... prefix=emo_128_100, lr=0.0002, bsz=8 ...
05/21/2022 21:54:26 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 21:54:26 - INFO - __main__ - Printing 3 examples
05/21/2022 21:54:26 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 21:54:26 - INFO - __main__ - ['others']
05/21/2022 21:54:26 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 21:54:26 - INFO - __main__ - ['others']
05/21/2022 21:54:26 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 21:54:26 - INFO - __main__ - ['others']
05/21/2022 21:54:26 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:54:27 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:54:27 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 21:54:27 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 21:54:27 - INFO - __main__ - Printing 3 examples
05/21/2022 21:54:27 - INFO - __main__ -  [emo] when when it comes to you never why
05/21/2022 21:54:27 - INFO - __main__ - ['others']
05/21/2022 21:54:27 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/21/2022 21:54:27 - INFO - __main__ - ['others']
05/21/2022 21:54:27 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/21/2022 21:54:27 - INFO - __main__ - ['others']
05/21/2022 21:54:27 - INFO - __main__ - Tokenizing Input ...
05/21/2022 21:54:27 - INFO - __main__ - Tokenizing Output ...
05/21/2022 21:54:28 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 21:54:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 21:54:40 - INFO - __main__ - Starting training!
05/21/2022 21:54:45 - INFO - __main__ - Step 10 Global step 10 Train loss 24.584265 on epoch=0
05/21/2022 21:54:50 - INFO - __main__ - Step 20 Global step 20 Train loss 22.072651 on epoch=0
05/21/2022 21:54:55 - INFO - __main__ - Step 30 Global step 30 Train loss 18.507975 on epoch=0
05/21/2022 21:55:00 - INFO - __main__ - Step 40 Global step 40 Train loss 17.481359 on epoch=1
05/21/2022 21:55:05 - INFO - __main__ - Step 50 Global step 50 Train loss 16.201206 on epoch=1
05/21/2022 21:56:56 - INFO - __main__ - Global step 50 Train loss 19.769493 Classification-F1 5.309546564723372e-05 on epoch=1
05/21/2022 21:57:02 - INFO - __main__ - Step 60 Global step 60 Train loss 16.159557 on epoch=1
05/21/2022 21:57:08 - INFO - __main__ - Step 70 Global step 70 Train loss 15.267792 on epoch=2
05/21/2022 21:57:13 - INFO - __main__ - Step 80 Global step 80 Train loss 14.742472 on epoch=2
05/21/2022 21:57:18 - INFO - __main__ - Step 90 Global step 90 Train loss 14.506674 on epoch=2
05/21/2022 21:57:24 - INFO - __main__ - Step 100 Global step 100 Train loss 13.671908 on epoch=3
05/21/2022 21:57:52 - INFO - __main__ - Global step 100 Train loss 14.869680 Classification-F1 0.0001631986944104447 on epoch=3
05/21/2022 21:57:58 - INFO - __main__ - Step 110 Global step 110 Train loss 13.209848 on epoch=3
05/21/2022 21:58:04 - INFO - __main__ - Step 120 Global step 120 Train loss 12.403102 on epoch=3
05/21/2022 21:58:09 - INFO - __main__ - Step 130 Global step 130 Train loss 12.026098 on epoch=4
05/21/2022 21:58:14 - INFO - __main__ - Step 140 Global step 140 Train loss 11.696845 on epoch=4
05/21/2022 21:58:19 - INFO - __main__ - Step 150 Global step 150 Train loss 10.008984 on epoch=4
05/21/2022 21:58:29 - INFO - __main__ - Global step 150 Train loss 11.868976 Classification-F1 0.0007385973724610716 on epoch=4
05/21/2022 21:58:35 - INFO - __main__ - Step 160 Global step 160 Train loss 9.807112 on epoch=4
05/21/2022 21:58:41 - INFO - __main__ - Step 170 Global step 170 Train loss 7.467440 on epoch=5
05/21/2022 21:58:46 - INFO - __main__ - Step 180 Global step 180 Train loss 6.763165 on epoch=5
05/21/2022 21:58:51 - INFO - __main__ - Step 190 Global step 190 Train loss 4.834418 on epoch=5
05/21/2022 21:58:56 - INFO - __main__ - Step 200 Global step 200 Train loss 4.674623 on epoch=6
05/21/2022 21:59:01 - INFO - __main__ - Global step 200 Train loss 6.709351 Classification-F1 0.1671509579208726 on epoch=6
05/21/2022 21:59:06 - INFO - __main__ - Step 210 Global step 210 Train loss 3.083121 on epoch=6
05/21/2022 21:59:12 - INFO - __main__ - Step 220 Global step 220 Train loss 4.661943 on epoch=6
05/21/2022 21:59:17 - INFO - __main__ - Step 230 Global step 230 Train loss 3.011871 on epoch=7
05/21/2022 21:59:22 - INFO - __main__ - Step 240 Global step 240 Train loss 3.240061 on epoch=7
05/21/2022 21:59:27 - INFO - __main__ - Step 250 Global step 250 Train loss 3.696548 on epoch=7
05/21/2022 21:59:31 - INFO - __main__ - Global step 250 Train loss 3.538709 Classification-F1 0.1 on epoch=7
05/21/2022 21:59:37 - INFO - __main__ - Step 260 Global step 260 Train loss 3.221733 on epoch=8
05/21/2022 21:59:42 - INFO - __main__ - Step 270 Global step 270 Train loss 3.602298 on epoch=8
05/21/2022 21:59:47 - INFO - __main__ - Step 280 Global step 280 Train loss 2.916224 on epoch=8
05/21/2022 21:59:52 - INFO - __main__ - Step 290 Global step 290 Train loss 2.567615 on epoch=9
05/21/2022 21:59:58 - INFO - __main__ - Step 300 Global step 300 Train loss 2.423914 on epoch=9
05/21/2022 22:00:02 - INFO - __main__ - Global step 300 Train loss 2.946357 Classification-F1 0.32687798359264014 on epoch=9
05/21/2022 22:00:08 - INFO - __main__ - Step 310 Global step 310 Train loss 2.411532 on epoch=9
05/21/2022 22:00:13 - INFO - __main__ - Step 320 Global step 320 Train loss 2.050913 on epoch=9
05/21/2022 22:00:18 - INFO - __main__ - Step 330 Global step 330 Train loss 2.501756 on epoch=10
05/21/2022 22:00:23 - INFO - __main__ - Step 340 Global step 340 Train loss 1.941381 on epoch=10
05/21/2022 22:00:29 - INFO - __main__ - Step 350 Global step 350 Train loss 2.474188 on epoch=10
05/21/2022 22:00:36 - INFO - __main__ - Global step 350 Train loss 2.275954 Classification-F1 0.07633850235905681 on epoch=10
05/21/2022 22:00:41 - INFO - __main__ - Step 360 Global step 360 Train loss 3.263236 on epoch=11
05/21/2022 22:00:47 - INFO - __main__ - Step 370 Global step 370 Train loss 1.944383 on epoch=11
05/21/2022 22:00:52 - INFO - __main__ - Step 380 Global step 380 Train loss 1.875908 on epoch=11
05/21/2022 22:00:57 - INFO - __main__ - Step 390 Global step 390 Train loss 1.758482 on epoch=12
05/21/2022 22:01:02 - INFO - __main__ - Step 400 Global step 400 Train loss 1.733503 on epoch=12
05/21/2022 22:01:07 - INFO - __main__ - Global step 400 Train loss 2.115103 Classification-F1 0.3088035798368841 on epoch=12
05/21/2022 22:01:12 - INFO - __main__ - Step 410 Global step 410 Train loss 1.615236 on epoch=12
05/21/2022 22:01:17 - INFO - __main__ - Step 420 Global step 420 Train loss 1.556581 on epoch=13
05/21/2022 22:01:22 - INFO - __main__ - Step 430 Global step 430 Train loss 1.425221 on epoch=13
05/21/2022 22:01:28 - INFO - __main__ - Step 440 Global step 440 Train loss 1.350285 on epoch=13
05/21/2022 22:01:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.981757 on epoch=14
05/21/2022 22:01:37 - INFO - __main__ - Global step 450 Train loss 1.385816 Classification-F1 0.7024970366698149 on epoch=14
05/21/2022 22:01:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.918485 on epoch=14
05/21/2022 22:01:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.788886 on epoch=14
05/21/2022 22:01:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.920365 on epoch=14
05/21/2022 22:01:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.786805 on epoch=15
05/21/2022 22:02:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.863905 on epoch=15
05/21/2022 22:02:08 - INFO - __main__ - Global step 500 Train loss 0.855689 Classification-F1 0.6783073135119752 on epoch=15
05/21/2022 22:02:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.619651 on epoch=15
05/21/2022 22:02:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.532054 on epoch=16
05/21/2022 22:02:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.669319 on epoch=16
05/21/2022 22:02:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.561132 on epoch=16
05/21/2022 22:02:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.616118 on epoch=17
05/21/2022 22:02:39 - INFO - __main__ - Global step 550 Train loss 0.599655 Classification-F1 0.7389584142138781 on epoch=17
05/21/2022 22:02:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.696464 on epoch=17
05/21/2022 22:02:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.451435 on epoch=17
05/21/2022 22:02:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.478251 on epoch=18
05/21/2022 22:03:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.446847 on epoch=18
05/21/2022 22:03:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.469645 on epoch=18
05/21/2022 22:03:10 - INFO - __main__ - Global step 600 Train loss 0.508528 Classification-F1 0.6956593815196329 on epoch=18
05/21/2022 22:03:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.463837 on epoch=19
05/21/2022 22:03:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.446558 on epoch=19
05/21/2022 22:03:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.357218 on epoch=19
05/21/2022 22:03:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.332321 on epoch=19
05/21/2022 22:03:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.291285 on epoch=20
05/21/2022 22:03:40 - INFO - __main__ - Global step 650 Train loss 0.378244 Classification-F1 0.7454408338918015 on epoch=20
05/21/2022 22:03:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.330792 on epoch=20
05/21/2022 22:03:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.292512 on epoch=20
05/21/2022 22:03:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.278330 on epoch=21
05/21/2022 22:04:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.450669 on epoch=21
05/21/2022 22:04:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.186917 on epoch=21
05/21/2022 22:04:11 - INFO - __main__ - Global step 700 Train loss 0.307844 Classification-F1 0.757678490015769 on epoch=21
05/21/2022 22:04:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.241337 on epoch=22
05/21/2022 22:04:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.357008 on epoch=22
05/21/2022 22:04:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.183427 on epoch=22
05/21/2022 22:04:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.312510 on epoch=23
05/21/2022 22:04:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.251541 on epoch=23
05/21/2022 22:04:42 - INFO - __main__ - Global step 750 Train loss 0.269165 Classification-F1 0.7884175397844454 on epoch=23
05/21/2022 22:04:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.277457 on epoch=23
05/21/2022 22:04:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.162494 on epoch=24
05/21/2022 22:04:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.231974 on epoch=24
05/21/2022 22:05:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.221321 on epoch=24
05/21/2022 22:05:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.260181 on epoch=24
05/21/2022 22:05:13 - INFO - __main__ - Global step 800 Train loss 0.230685 Classification-F1 0.7731074478465125 on epoch=24
05/21/2022 22:05:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.163326 on epoch=25
05/21/2022 22:05:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.211286 on epoch=25
05/21/2022 22:05:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.281880 on epoch=25
05/21/2022 22:05:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.157071 on epoch=26
05/21/2022 22:05:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.214265 on epoch=26
05/21/2022 22:05:43 - INFO - __main__ - Global step 850 Train loss 0.205566 Classification-F1 0.8060820821667921 on epoch=26
05/21/2022 22:05:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.218693 on epoch=26
05/21/2022 22:05:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.174402 on epoch=27
05/21/2022 22:05:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.155223 on epoch=27
05/21/2022 22:06:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.097175 on epoch=27
05/21/2022 22:06:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.183768 on epoch=28
05/21/2022 22:06:13 - INFO - __main__ - Global step 900 Train loss 0.165852 Classification-F1 0.7720644476571403 on epoch=28
05/21/2022 22:06:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.153968 on epoch=28
05/21/2022 22:06:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.143748 on epoch=28
05/21/2022 22:06:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.147456 on epoch=29
05/21/2022 22:06:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.094931 on epoch=29
05/21/2022 22:06:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.126681 on epoch=29
05/21/2022 22:06:43 - INFO - __main__ - Global step 950 Train loss 0.133357 Classification-F1 0.809999206952612 on epoch=29
05/21/2022 22:06:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.059675 on epoch=29
05/21/2022 22:06:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.088389 on epoch=30
05/21/2022 22:06:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.125928 on epoch=30
05/21/2022 22:07:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.118127 on epoch=30
05/21/2022 22:07:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.103193 on epoch=31
05/21/2022 22:07:10 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 22:07:10 - INFO - __main__ - Printing 3 examples
05/21/2022 22:07:10 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 22:07:10 - INFO - __main__ - ['others']
05/21/2022 22:07:10 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 22:07:10 - INFO - __main__ - ['others']
05/21/2022 22:07:10 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 22:07:10 - INFO - __main__ - ['others']
05/21/2022 22:07:10 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:07:10 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:07:11 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 22:07:11 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 22:07:11 - INFO - __main__ - Printing 3 examples
05/21/2022 22:07:11 - INFO - __main__ -  [emo] when when it comes to you never why
05/21/2022 22:07:11 - INFO - __main__ - ['others']
05/21/2022 22:07:11 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/21/2022 22:07:11 - INFO - __main__ - ['others']
05/21/2022 22:07:11 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/21/2022 22:07:11 - INFO - __main__ - ['others']
05/21/2022 22:07:11 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:07:11 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:07:12 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 22:07:13 - INFO - __main__ - Global step 1000 Train loss 0.099062 Classification-F1 0.8149395212630506 on epoch=31
05/21/2022 22:07:14 - INFO - __main__ - save last model!
05/21/2022 22:07:21 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 22:07:21 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 22:07:21 - INFO - __main__ - Printing 3 examples
05/21/2022 22:07:21 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 22:07:21 - INFO - __main__ - ['others']
05/21/2022 22:07:21 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 22:07:21 - INFO - __main__ - ['others']
05/21/2022 22:07:21 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 22:07:21 - INFO - __main__ - ['others']
05/21/2022 22:07:21 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:07:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 22:07:22 - INFO - __main__ - Starting training!
05/21/2022 22:07:23 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:07:29 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 22:08:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_100_0.0002_8_predictions.txt
05/21/2022 22:08:21 - INFO - __main__ - Classification-F1 on test data: 0.2388
05/21/2022 22:08:21 - INFO - __main__ - prefix=emo_128_100, lr=0.0002, bsz=8, dev_performance=0.8149395212630506, test_performance=0.23884746547672706
05/21/2022 22:08:21 - INFO - __main__ - Running ... prefix=emo_128_100, lr=0.0001, bsz=8 ...
05/21/2022 22:08:22 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 22:08:22 - INFO - __main__ - Printing 3 examples
05/21/2022 22:08:22 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 22:08:22 - INFO - __main__ - ['others']
05/21/2022 22:08:22 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 22:08:22 - INFO - __main__ - ['others']
05/21/2022 22:08:22 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 22:08:22 - INFO - __main__ - ['others']
05/21/2022 22:08:22 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:08:22 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:08:23 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 22:08:23 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 22:08:23 - INFO - __main__ - Printing 3 examples
05/21/2022 22:08:23 - INFO - __main__ -  [emo] when when it comes to you never why
05/21/2022 22:08:23 - INFO - __main__ - ['others']
05/21/2022 22:08:23 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/21/2022 22:08:23 - INFO - __main__ - ['others']
05/21/2022 22:08:23 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/21/2022 22:08:23 - INFO - __main__ - ['others']
05/21/2022 22:08:23 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:08:23 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:08:23 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 22:08:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 22:08:35 - INFO - __main__ - Starting training!
05/21/2022 22:08:39 - INFO - __main__ - Step 10 Global step 10 Train loss 24.540888 on epoch=0
05/21/2022 22:08:44 - INFO - __main__ - Step 20 Global step 20 Train loss 22.136648 on epoch=0
05/21/2022 22:08:49 - INFO - __main__ - Step 30 Global step 30 Train loss 19.402811 on epoch=0
05/21/2022 22:08:54 - INFO - __main__ - Step 40 Global step 40 Train loss 18.678133 on epoch=1
05/21/2022 22:08:59 - INFO - __main__ - Step 50 Global step 50 Train loss 18.778646 on epoch=1
05/21/2022 22:11:37 - INFO - __main__ - Global step 50 Train loss 20.707424 Classification-F1 9.72124091139139e-05 on epoch=1
05/21/2022 22:11:43 - INFO - __main__ - Step 60 Global step 60 Train loss 18.314800 on epoch=1
05/21/2022 22:11:48 - INFO - __main__ - Step 70 Global step 70 Train loss 17.257784 on epoch=2
05/21/2022 22:11:53 - INFO - __main__ - Step 80 Global step 80 Train loss 17.775146 on epoch=2
05/21/2022 22:11:58 - INFO - __main__ - Step 90 Global step 90 Train loss 16.343311 on epoch=2
05/21/2022 22:12:03 - INFO - __main__ - Step 100 Global step 100 Train loss 16.502346 on epoch=3
05/21/2022 22:14:33 - INFO - __main__ - Global step 100 Train loss 17.238678 Classification-F1 3.3198877877927724e-05 on epoch=3
05/21/2022 22:14:38 - INFO - __main__ - Step 110 Global step 110 Train loss 15.824081 on epoch=3
05/21/2022 22:14:43 - INFO - __main__ - Step 120 Global step 120 Train loss 16.732899 on epoch=3
05/21/2022 22:14:48 - INFO - __main__ - Step 130 Global step 130 Train loss 15.187796 on epoch=4
05/21/2022 22:14:53 - INFO - __main__ - Step 140 Global step 140 Train loss 15.127948 on epoch=4
05/21/2022 22:14:58 - INFO - __main__ - Step 150 Global step 150 Train loss 15.307368 on epoch=4
05/21/2022 22:17:12 - INFO - __main__ - Global step 150 Train loss 15.636018 Classification-F1 7.435911735727697e-05 on epoch=4
05/21/2022 22:17:17 - INFO - __main__ - Step 160 Global step 160 Train loss 15.117697 on epoch=4
05/21/2022 22:17:22 - INFO - __main__ - Step 170 Global step 170 Train loss 14.204526 on epoch=5
05/21/2022 22:17:27 - INFO - __main__ - Step 180 Global step 180 Train loss 14.103495 on epoch=5
05/21/2022 22:17:32 - INFO - __main__ - Step 190 Global step 190 Train loss 14.824514 on epoch=5
05/21/2022 22:17:37 - INFO - __main__ - Step 200 Global step 200 Train loss 12.532028 on epoch=6
05/21/2022 22:19:15 - INFO - __main__ - Global step 200 Train loss 14.156450 Classification-F1 4.985169121862459e-05 on epoch=6
05/21/2022 22:19:20 - INFO - __main__ - Step 210 Global step 210 Train loss 13.385243 on epoch=6
05/21/2022 22:19:25 - INFO - __main__ - Step 220 Global step 220 Train loss 13.147743 on epoch=6
05/21/2022 22:19:30 - INFO - __main__ - Step 230 Global step 230 Train loss 12.372635 on epoch=7
05/21/2022 22:19:35 - INFO - __main__ - Step 240 Global step 240 Train loss 12.576610 on epoch=7
05/21/2022 22:19:40 - INFO - __main__ - Step 250 Global step 250 Train loss 11.978975 on epoch=7
05/21/2022 22:20:58 - INFO - __main__ - Global step 250 Train loss 12.692242 Classification-F1 0.00013175230566534916 on epoch=7
05/21/2022 22:21:04 - INFO - __main__ - Step 260 Global step 260 Train loss 12.003542 on epoch=8
05/21/2022 22:21:09 - INFO - __main__ - Step 270 Global step 270 Train loss 11.271295 on epoch=8
05/21/2022 22:21:14 - INFO - __main__ - Step 280 Global step 280 Train loss 11.286264 on epoch=8
05/21/2022 22:21:19 - INFO - __main__ - Step 290 Global step 290 Train loss 10.100781 on epoch=9
05/21/2022 22:21:24 - INFO - __main__ - Step 300 Global step 300 Train loss 10.280566 on epoch=9
05/21/2022 22:22:24 - INFO - __main__ - Global step 300 Train loss 10.988490 Classification-F1 0.0002595888959525323 on epoch=9
05/21/2022 22:22:29 - INFO - __main__ - Step 310 Global step 310 Train loss 10.055091 on epoch=9
05/21/2022 22:22:34 - INFO - __main__ - Step 320 Global step 320 Train loss 9.228041 on epoch=9
05/21/2022 22:22:39 - INFO - __main__ - Step 330 Global step 330 Train loss 8.131853 on epoch=10
05/21/2022 22:22:44 - INFO - __main__ - Step 340 Global step 340 Train loss 7.015688 on epoch=10
05/21/2022 22:22:49 - INFO - __main__ - Step 350 Global step 350 Train loss 6.341908 on epoch=10
05/21/2022 22:23:29 - INFO - __main__ - Global step 350 Train loss 8.154516 Classification-F1 0.001090257487874034 on epoch=10
05/21/2022 22:23:35 - INFO - __main__ - Step 360 Global step 360 Train loss 5.098077 on epoch=11
05/21/2022 22:23:40 - INFO - __main__ - Step 370 Global step 370 Train loss 4.623828 on epoch=11
05/21/2022 22:23:45 - INFO - __main__ - Step 380 Global step 380 Train loss 4.216573 on epoch=11
05/21/2022 22:23:50 - INFO - __main__ - Step 390 Global step 390 Train loss 4.497553 on epoch=12
05/21/2022 22:23:55 - INFO - __main__ - Step 400 Global step 400 Train loss 3.666761 on epoch=12
05/21/2022 22:23:59 - INFO - __main__ - Global step 400 Train loss 4.420558 Classification-F1 0.18639149612760939 on epoch=12
05/21/2022 22:24:04 - INFO - __main__ - Step 410 Global step 410 Train loss 4.323682 on epoch=12
05/21/2022 22:24:09 - INFO - __main__ - Step 420 Global step 420 Train loss 3.579014 on epoch=13
05/21/2022 22:24:14 - INFO - __main__ - Step 430 Global step 430 Train loss 3.990252 on epoch=13
05/21/2022 22:24:19 - INFO - __main__ - Step 440 Global step 440 Train loss 4.064308 on epoch=13
05/21/2022 22:24:24 - INFO - __main__ - Step 450 Global step 450 Train loss 3.731986 on epoch=14
05/21/2022 22:24:28 - INFO - __main__ - Global step 450 Train loss 3.937848 Classification-F1 0.1 on epoch=14
05/21/2022 22:24:33 - INFO - __main__ - Step 460 Global step 460 Train loss 3.374828 on epoch=14
05/21/2022 22:24:38 - INFO - __main__ - Step 470 Global step 470 Train loss 4.059740 on epoch=14
05/21/2022 22:24:43 - INFO - __main__ - Step 480 Global step 480 Train loss 4.115687 on epoch=14
05/21/2022 22:24:48 - INFO - __main__ - Step 490 Global step 490 Train loss 3.006022 on epoch=15
05/21/2022 22:24:53 - INFO - __main__ - Step 500 Global step 500 Train loss 3.071509 on epoch=15
05/21/2022 22:24:57 - INFO - __main__ - Global step 500 Train loss 3.525557 Classification-F1 0.11958439405600617 on epoch=15
05/21/2022 22:25:02 - INFO - __main__ - Step 510 Global step 510 Train loss 3.359700 on epoch=15
05/21/2022 22:25:07 - INFO - __main__ - Step 520 Global step 520 Train loss 3.153236 on epoch=16
05/21/2022 22:25:12 - INFO - __main__ - Step 530 Global step 530 Train loss 3.372136 on epoch=16
05/21/2022 22:25:17 - INFO - __main__ - Step 540 Global step 540 Train loss 3.923990 on epoch=16
05/21/2022 22:25:22 - INFO - __main__ - Step 550 Global step 550 Train loss 3.290242 on epoch=17
05/21/2022 22:25:26 - INFO - __main__ - Global step 550 Train loss 3.419861 Classification-F1 0.1 on epoch=17
05/21/2022 22:25:31 - INFO - __main__ - Step 560 Global step 560 Train loss 2.805062 on epoch=17
05/21/2022 22:25:36 - INFO - __main__ - Step 570 Global step 570 Train loss 3.102027 on epoch=17
05/21/2022 22:25:41 - INFO - __main__ - Step 580 Global step 580 Train loss 3.171844 on epoch=18
05/21/2022 22:25:46 - INFO - __main__ - Step 590 Global step 590 Train loss 3.158463 on epoch=18
05/21/2022 22:25:51 - INFO - __main__ - Step 600 Global step 600 Train loss 3.095271 on epoch=18
05/21/2022 22:25:55 - INFO - __main__ - Global step 600 Train loss 3.066533 Classification-F1 0.13879416012330245 on epoch=18
05/21/2022 22:26:00 - INFO - __main__ - Step 610 Global step 610 Train loss 2.996840 on epoch=19
05/21/2022 22:26:05 - INFO - __main__ - Step 620 Global step 620 Train loss 2.623838 on epoch=19
05/21/2022 22:26:10 - INFO - __main__ - Step 630 Global step 630 Train loss 3.079314 on epoch=19
05/21/2022 22:26:15 - INFO - __main__ - Step 640 Global step 640 Train loss 3.093897 on epoch=19
05/21/2022 22:26:20 - INFO - __main__ - Step 650 Global step 650 Train loss 2.946361 on epoch=20
05/21/2022 22:26:23 - INFO - __main__ - Global step 650 Train loss 2.948050 Classification-F1 0.1270317711076005 on epoch=20
05/21/2022 22:26:28 - INFO - __main__ - Step 660 Global step 660 Train loss 2.937753 on epoch=20
05/21/2022 22:26:33 - INFO - __main__ - Step 670 Global step 670 Train loss 2.458316 on epoch=20
05/21/2022 22:26:39 - INFO - __main__ - Step 680 Global step 680 Train loss 2.500952 on epoch=21
05/21/2022 22:26:44 - INFO - __main__ - Step 690 Global step 690 Train loss 2.471487 on epoch=21
05/21/2022 22:26:49 - INFO - __main__ - Step 700 Global step 700 Train loss 1.868673 on epoch=21
05/21/2022 22:26:53 - INFO - __main__ - Global step 700 Train loss 2.447436 Classification-F1 0.45417758780954237 on epoch=21
05/21/2022 22:26:58 - INFO - __main__ - Step 710 Global step 710 Train loss 2.009537 on epoch=22
05/21/2022 22:27:03 - INFO - __main__ - Step 720 Global step 720 Train loss 2.283842 on epoch=22
05/21/2022 22:27:08 - INFO - __main__ - Step 730 Global step 730 Train loss 2.674291 on epoch=22
05/21/2022 22:27:13 - INFO - __main__ - Step 740 Global step 740 Train loss 2.370609 on epoch=23
05/21/2022 22:27:18 - INFO - __main__ - Step 750 Global step 750 Train loss 2.379331 on epoch=23
05/21/2022 22:27:22 - INFO - __main__ - Global step 750 Train loss 2.343522 Classification-F1 0.4062781276894319 on epoch=23
05/21/2022 22:27:27 - INFO - __main__ - Step 760 Global step 760 Train loss 2.465277 on epoch=23
05/21/2022 22:27:32 - INFO - __main__ - Step 770 Global step 770 Train loss 2.355227 on epoch=24
05/21/2022 22:27:37 - INFO - __main__ - Step 780 Global step 780 Train loss 2.055891 on epoch=24
05/21/2022 22:27:42 - INFO - __main__ - Step 790 Global step 790 Train loss 2.065386 on epoch=24
05/21/2022 22:27:47 - INFO - __main__ - Step 800 Global step 800 Train loss 2.156721 on epoch=24
05/21/2022 22:27:51 - INFO - __main__ - Global step 800 Train loss 2.219701 Classification-F1 0.45663109798381585 on epoch=24
05/21/2022 22:27:57 - INFO - __main__ - Step 810 Global step 810 Train loss 1.905666 on epoch=25
05/21/2022 22:28:02 - INFO - __main__ - Step 820 Global step 820 Train loss 1.705855 on epoch=25
05/21/2022 22:28:07 - INFO - __main__ - Step 830 Global step 830 Train loss 2.131432 on epoch=25
05/21/2022 22:28:12 - INFO - __main__ - Step 840 Global step 840 Train loss 1.855528 on epoch=26
05/21/2022 22:28:17 - INFO - __main__ - Step 850 Global step 850 Train loss 1.786508 on epoch=26
05/21/2022 22:28:21 - INFO - __main__ - Global step 850 Train loss 1.876998 Classification-F1 0.28576366540322773 on epoch=26
05/21/2022 22:28:26 - INFO - __main__ - Step 860 Global step 860 Train loss 1.791208 on epoch=26
05/21/2022 22:28:31 - INFO - __main__ - Step 870 Global step 870 Train loss 1.608289 on epoch=27
05/21/2022 22:28:36 - INFO - __main__ - Step 880 Global step 880 Train loss 1.939693 on epoch=27
05/21/2022 22:28:41 - INFO - __main__ - Step 890 Global step 890 Train loss 1.805079 on epoch=27
05/21/2022 22:28:46 - INFO - __main__ - Step 900 Global step 900 Train loss 1.739654 on epoch=28
05/21/2022 22:28:50 - INFO - __main__ - Global step 900 Train loss 1.776785 Classification-F1 0.5202299711753172 on epoch=28
05/21/2022 22:28:55 - INFO - __main__ - Step 910 Global step 910 Train loss 1.568533 on epoch=28
05/21/2022 22:29:01 - INFO - __main__ - Step 920 Global step 920 Train loss 1.753089 on epoch=28
05/21/2022 22:29:06 - INFO - __main__ - Step 930 Global step 930 Train loss 1.353956 on epoch=29
05/21/2022 22:29:11 - INFO - __main__ - Step 940 Global step 940 Train loss 1.500419 on epoch=29
05/21/2022 22:29:16 - INFO - __main__ - Step 950 Global step 950 Train loss 1.429293 on epoch=29
05/21/2022 22:29:20 - INFO - __main__ - Global step 950 Train loss 1.521058 Classification-F1 0.5156681469918443 on epoch=29
05/21/2022 22:29:25 - INFO - __main__ - Step 960 Global step 960 Train loss 1.781497 on epoch=29
05/21/2022 22:29:30 - INFO - __main__ - Step 970 Global step 970 Train loss 1.232921 on epoch=30
05/21/2022 22:29:36 - INFO - __main__ - Step 980 Global step 980 Train loss 1.332968 on epoch=30
05/21/2022 22:29:41 - INFO - __main__ - Step 990 Global step 990 Train loss 1.294372 on epoch=30
05/21/2022 22:29:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.998019 on epoch=31
05/21/2022 22:29:47 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 22:29:47 - INFO - __main__ - Printing 3 examples
05/21/2022 22:29:47 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 22:29:47 - INFO - __main__ - ['others']
05/21/2022 22:29:47 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 22:29:47 - INFO - __main__ - ['others']
05/21/2022 22:29:47 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 22:29:47 - INFO - __main__ - ['others']
05/21/2022 22:29:47 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:29:47 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:29:48 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 22:29:48 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 22:29:48 - INFO - __main__ - Printing 3 examples
05/21/2022 22:29:48 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/21/2022 22:29:48 - INFO - __main__ - ['others']
05/21/2022 22:29:48 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/21/2022 22:29:48 - INFO - __main__ - ['others']
05/21/2022 22:29:48 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/21/2022 22:29:48 - INFO - __main__ - ['others']
05/21/2022 22:29:48 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:29:48 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:29:48 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 22:29:50 - INFO - __main__ - Global step 1000 Train loss 1.327955 Classification-F1 0.6778853716989689 on epoch=31
05/21/2022 22:29:51 - INFO - __main__ - save last model!
05/21/2022 22:29:57 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 22:29:58 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 22:29:58 - INFO - __main__ - Printing 3 examples
05/21/2022 22:29:58 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 22:29:58 - INFO - __main__ - ['others']
05/21/2022 22:29:58 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 22:29:58 - INFO - __main__ - ['others']
05/21/2022 22:29:58 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 22:29:58 - INFO - __main__ - ['others']
05/21/2022 22:29:58 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:30:00 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:30:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 22:30:01 - INFO - __main__ - Starting training!
05/21/2022 22:30:06 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 22:30:47 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_100_0.0001_8_predictions.txt
05/21/2022 22:30:47 - INFO - __main__ - Classification-F1 on test data: 0.3547
05/21/2022 22:30:47 - INFO - __main__ - prefix=emo_128_100, lr=0.0001, bsz=8, dev_performance=0.6778853716989689, test_performance=0.3547360550501253
05/21/2022 22:30:47 - INFO - __main__ - Running ... prefix=emo_128_13, lr=0.0005, bsz=8 ...
05/21/2022 22:30:48 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 22:30:48 - INFO - __main__ - Printing 3 examples
05/21/2022 22:30:48 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 22:30:48 - INFO - __main__ - ['others']
05/21/2022 22:30:48 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 22:30:48 - INFO - __main__ - ['others']
05/21/2022 22:30:48 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 22:30:48 - INFO - __main__ - ['others']
05/21/2022 22:30:48 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:30:48 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:30:49 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 22:30:49 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 22:30:49 - INFO - __main__ - Printing 3 examples
05/21/2022 22:30:49 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/21/2022 22:30:49 - INFO - __main__ - ['others']
05/21/2022 22:30:49 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/21/2022 22:30:49 - INFO - __main__ - ['others']
05/21/2022 22:30:49 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/21/2022 22:30:49 - INFO - __main__ - ['others']
05/21/2022 22:30:49 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:30:49 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:30:50 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 22:31:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 22:31:01 - INFO - __main__ - Starting training!
05/21/2022 22:31:05 - INFO - __main__ - Step 10 Global step 10 Train loss 25.205429 on epoch=0
05/21/2022 22:31:10 - INFO - __main__ - Step 20 Global step 20 Train loss 18.580118 on epoch=0
05/21/2022 22:31:15 - INFO - __main__ - Step 30 Global step 30 Train loss 17.053162 on epoch=0
05/21/2022 22:31:20 - INFO - __main__ - Step 40 Global step 40 Train loss 14.295698 on epoch=1
05/21/2022 22:31:25 - INFO - __main__ - Step 50 Global step 50 Train loss 12.854393 on epoch=1
05/21/2022 22:31:29 - INFO - __main__ - Global step 50 Train loss 17.597761 Classification-F1 0.0 on epoch=1
05/21/2022 22:31:34 - INFO - __main__ - Step 60 Global step 60 Train loss 12.023958 on epoch=1
05/21/2022 22:31:39 - INFO - __main__ - Step 70 Global step 70 Train loss 10.307837 on epoch=2
05/21/2022 22:31:44 - INFO - __main__ - Step 80 Global step 80 Train loss 7.742100 on epoch=2
05/21/2022 22:31:49 - INFO - __main__ - Step 90 Global step 90 Train loss 4.917378 on epoch=2
05/21/2022 22:31:54 - INFO - __main__ - Step 100 Global step 100 Train loss 3.985391 on epoch=3
05/21/2022 22:31:58 - INFO - __main__ - Global step 100 Train loss 7.795333 Classification-F1 0.1 on epoch=3
05/21/2022 22:32:03 - INFO - __main__ - Step 110 Global step 110 Train loss 3.905875 on epoch=3
05/21/2022 22:32:08 - INFO - __main__ - Step 120 Global step 120 Train loss 3.301261 on epoch=3
05/21/2022 22:32:13 - INFO - __main__ - Step 130 Global step 130 Train loss 3.338774 on epoch=4
05/21/2022 22:32:18 - INFO - __main__ - Step 140 Global step 140 Train loss 3.297404 on epoch=4
05/21/2022 22:32:23 - INFO - __main__ - Step 150 Global step 150 Train loss 2.537775 on epoch=4
05/21/2022 22:32:27 - INFO - __main__ - Global step 150 Train loss 3.276218 Classification-F1 0.1 on epoch=4
05/21/2022 22:32:32 - INFO - __main__ - Step 160 Global step 160 Train loss 3.435175 on epoch=4
05/21/2022 22:32:37 - INFO - __main__ - Step 170 Global step 170 Train loss 1.825057 on epoch=5
05/21/2022 22:32:42 - INFO - __main__ - Step 180 Global step 180 Train loss 2.668842 on epoch=5
05/21/2022 22:32:47 - INFO - __main__ - Step 190 Global step 190 Train loss 2.033578 on epoch=5
05/21/2022 22:32:52 - INFO - __main__ - Step 200 Global step 200 Train loss 1.758660 on epoch=6
05/21/2022 22:32:56 - INFO - __main__ - Global step 200 Train loss 2.344262 Classification-F1 0.1 on epoch=6
05/21/2022 22:33:01 - INFO - __main__ - Step 210 Global step 210 Train loss 1.679915 on epoch=6
05/21/2022 22:33:06 - INFO - __main__ - Step 220 Global step 220 Train loss 1.911865 on epoch=6
05/21/2022 22:33:11 - INFO - __main__ - Step 230 Global step 230 Train loss 1.469189 on epoch=7
05/21/2022 22:33:16 - INFO - __main__ - Step 240 Global step 240 Train loss 1.379636 on epoch=7
05/21/2022 22:33:21 - INFO - __main__ - Step 250 Global step 250 Train loss 1.275107 on epoch=7
05/21/2022 22:33:25 - INFO - __main__ - Global step 250 Train loss 1.543142 Classification-F1 0.1 on epoch=7
05/21/2022 22:33:30 - INFO - __main__ - Step 260 Global step 260 Train loss 1.205222 on epoch=8
05/21/2022 22:33:35 - INFO - __main__ - Step 270 Global step 270 Train loss 4.089206 on epoch=8
05/21/2022 22:33:40 - INFO - __main__ - Step 280 Global step 280 Train loss 3.688919 on epoch=8
05/21/2022 22:33:44 - INFO - __main__ - Step 290 Global step 290 Train loss 1.101070 on epoch=9
05/21/2022 22:33:49 - INFO - __main__ - Step 300 Global step 300 Train loss 1.097579 on epoch=9
05/21/2022 22:33:53 - INFO - __main__ - Global step 300 Train loss 2.236399 Classification-F1 0.1 on epoch=9
05/21/2022 22:33:58 - INFO - __main__ - Step 310 Global step 310 Train loss 1.062954 on epoch=9
05/21/2022 22:34:03 - INFO - __main__ - Step 320 Global step 320 Train loss 1.012747 on epoch=9
05/21/2022 22:34:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.929283 on epoch=10
05/21/2022 22:34:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.961241 on epoch=10
05/21/2022 22:34:18 - INFO - __main__ - Step 350 Global step 350 Train loss 1.084644 on epoch=10
05/21/2022 22:34:22 - INFO - __main__ - Global step 350 Train loss 1.010174 Classification-F1 0.15253441802252815 on epoch=10
05/21/2022 22:34:28 - INFO - __main__ - Step 360 Global step 360 Train loss 1.028388 on epoch=11
05/21/2022 22:34:33 - INFO - __main__ - Step 370 Global step 370 Train loss 1.229229 on epoch=11
05/21/2022 22:34:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.965802 on epoch=11
05/21/2022 22:34:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.866785 on epoch=12
05/21/2022 22:34:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.961099 on epoch=12
05/21/2022 22:34:51 - INFO - __main__ - Global step 400 Train loss 1.010261 Classification-F1 0.1 on epoch=12
05/21/2022 22:34:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.855559 on epoch=12
05/21/2022 22:35:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.816920 on epoch=13
05/21/2022 22:35:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.892704 on epoch=13
05/21/2022 22:35:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.832566 on epoch=13
05/21/2022 22:35:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.889839 on epoch=14
05/21/2022 22:35:20 - INFO - __main__ - Global step 450 Train loss 0.857518 Classification-F1 0.10322977005441461 on epoch=14
05/21/2022 22:35:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.866246 on epoch=14
05/21/2022 22:35:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.844685 on epoch=14
05/21/2022 22:35:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.803969 on epoch=14
05/21/2022 22:35:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.781681 on epoch=15
05/21/2022 22:35:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.785796 on epoch=15
05/21/2022 22:35:49 - INFO - __main__ - Global step 500 Train loss 0.816476 Classification-F1 0.2442870922181267 on epoch=15
05/21/2022 22:35:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.784791 on epoch=15
05/21/2022 22:36:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.820000 on epoch=16
05/21/2022 22:36:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.892390 on epoch=16
05/21/2022 22:36:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.745525 on epoch=16
05/21/2022 22:36:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.729513 on epoch=17
05/21/2022 22:36:18 - INFO - __main__ - Global step 550 Train loss 0.794444 Classification-F1 0.351079432654327 on epoch=17
05/21/2022 22:36:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.793239 on epoch=17
05/21/2022 22:36:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.964606 on epoch=17
05/21/2022 22:36:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.779139 on epoch=18
05/21/2022 22:36:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.789815 on epoch=18
05/21/2022 22:36:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.813024 on epoch=18
05/21/2022 22:36:48 - INFO - __main__ - Global step 600 Train loss 0.827965 Classification-F1 0.30208582760150765 on epoch=18
05/21/2022 22:36:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.730732 on epoch=19
05/21/2022 22:36:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.760703 on epoch=19
05/21/2022 22:37:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.733642 on epoch=19
05/21/2022 22:37:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.612014 on epoch=19
05/21/2022 22:37:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.627362 on epoch=20
05/21/2022 22:37:17 - INFO - __main__ - Global step 650 Train loss 0.692891 Classification-F1 0.37824961060500706 on epoch=20
05/21/2022 22:37:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.558205 on epoch=20
05/21/2022 22:37:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.619709 on epoch=20
05/21/2022 22:37:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.498225 on epoch=21
05/21/2022 22:37:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.733657 on epoch=21
05/21/2022 22:37:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.693733 on epoch=21
05/21/2022 22:37:46 - INFO - __main__ - Global step 700 Train loss 0.620706 Classification-F1 0.4569214921741054 on epoch=21
05/21/2022 22:37:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.656673 on epoch=22
05/21/2022 22:37:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.695153 on epoch=22
05/21/2022 22:38:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.668825 on epoch=22
05/21/2022 22:38:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.530279 on epoch=23
05/21/2022 22:38:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.717480 on epoch=23
05/21/2022 22:38:16 - INFO - __main__ - Global step 750 Train loss 0.653682 Classification-F1 0.6692830019274948 on epoch=23
05/21/2022 22:38:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.755462 on epoch=23
05/21/2022 22:38:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.538524 on epoch=24
05/21/2022 22:38:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.660312 on epoch=24
05/21/2022 22:38:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.624930 on epoch=24
05/21/2022 22:38:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.493389 on epoch=24
05/21/2022 22:38:45 - INFO - __main__ - Global step 800 Train loss 0.614523 Classification-F1 0.6989458281762604 on epoch=24
05/21/2022 22:38:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.521424 on epoch=25
05/21/2022 22:38:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.590038 on epoch=25
05/21/2022 22:39:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.689730 on epoch=25
05/21/2022 22:39:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.435437 on epoch=26
05/21/2022 22:39:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.609556 on epoch=26
05/21/2022 22:39:15 - INFO - __main__ - Global step 850 Train loss 0.569237 Classification-F1 0.4304003730753769 on epoch=26
05/21/2022 22:39:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.626370 on epoch=26
05/21/2022 22:39:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.412551 on epoch=27
05/21/2022 22:39:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.549539 on epoch=27
05/21/2022 22:39:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.510967 on epoch=27
05/21/2022 22:39:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.317981 on epoch=28
05/21/2022 22:39:43 - INFO - __main__ - Global step 900 Train loss 0.483481 Classification-F1 0.7054938482570062 on epoch=28
05/21/2022 22:39:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.640908 on epoch=28
05/21/2022 22:39:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.498594 on epoch=28
05/21/2022 22:39:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.319314 on epoch=29
05/21/2022 22:40:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.513854 on epoch=29
05/21/2022 22:40:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.553069 on epoch=29
05/21/2022 22:40:13 - INFO - __main__ - Global step 950 Train loss 0.505148 Classification-F1 0.7061467543471854 on epoch=29
05/21/2022 22:40:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.387270 on epoch=29
05/21/2022 22:40:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.353738 on epoch=30
05/21/2022 22:40:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.411670 on epoch=30
05/21/2022 22:40:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.447367 on epoch=30
05/21/2022 22:40:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.354286 on epoch=31
05/21/2022 22:40:40 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 22:40:40 - INFO - __main__ - Printing 3 examples
05/21/2022 22:40:40 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 22:40:40 - INFO - __main__ - ['others']
05/21/2022 22:40:40 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 22:40:40 - INFO - __main__ - ['others']
05/21/2022 22:40:40 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 22:40:40 - INFO - __main__ - ['others']
05/21/2022 22:40:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:40:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:40:40 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 22:40:40 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 22:40:40 - INFO - __main__ - Printing 3 examples
05/21/2022 22:40:40 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/21/2022 22:40:40 - INFO - __main__ - ['others']
05/21/2022 22:40:40 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/21/2022 22:40:40 - INFO - __main__ - ['others']
05/21/2022 22:40:40 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/21/2022 22:40:40 - INFO - __main__ - ['others']
05/21/2022 22:40:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:40:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:40:41 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 22:40:43 - INFO - __main__ - Global step 1000 Train loss 0.390866 Classification-F1 0.5988090889053949 on epoch=31
05/21/2022 22:40:43 - INFO - __main__ - save last model!
05/21/2022 22:40:50 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 22:40:51 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 22:40:51 - INFO - __main__ - Printing 3 examples
05/21/2022 22:40:51 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 22:40:51 - INFO - __main__ - ['others']
05/21/2022 22:40:51 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 22:40:51 - INFO - __main__ - ['others']
05/21/2022 22:40:51 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 22:40:51 - INFO - __main__ - ['others']
05/21/2022 22:40:51 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:40:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 22:40:52 - INFO - __main__ - Starting training!
05/21/2022 22:40:53 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:40:58 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 22:41:42 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_13_0.0005_8_predictions.txt
05/21/2022 22:41:42 - INFO - __main__ - Classification-F1 on test data: 0.5757
05/21/2022 22:41:42 - INFO - __main__ - prefix=emo_128_13, lr=0.0005, bsz=8, dev_performance=0.7061467543471854, test_performance=0.5756853409407978
05/21/2022 22:41:42 - INFO - __main__ - Running ... prefix=emo_128_13, lr=0.0003, bsz=8 ...
05/21/2022 22:41:43 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 22:41:43 - INFO - __main__ - Printing 3 examples
05/21/2022 22:41:43 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 22:41:43 - INFO - __main__ - ['others']
05/21/2022 22:41:43 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 22:41:43 - INFO - __main__ - ['others']
05/21/2022 22:41:43 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 22:41:43 - INFO - __main__ - ['others']
05/21/2022 22:41:43 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:41:43 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:41:44 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 22:41:44 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 22:41:44 - INFO - __main__ - Printing 3 examples
05/21/2022 22:41:44 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/21/2022 22:41:44 - INFO - __main__ - ['others']
05/21/2022 22:41:44 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/21/2022 22:41:44 - INFO - __main__ - ['others']
05/21/2022 22:41:44 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/21/2022 22:41:44 - INFO - __main__ - ['others']
05/21/2022 22:41:44 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:41:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:41:44 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 22:41:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 22:41:56 - INFO - __main__ - Starting training!
05/21/2022 22:42:00 - INFO - __main__ - Step 10 Global step 10 Train loss 24.702774 on epoch=0
05/21/2022 22:42:05 - INFO - __main__ - Step 20 Global step 20 Train loss 20.973240 on epoch=0
05/21/2022 22:42:10 - INFO - __main__ - Step 30 Global step 30 Train loss 18.632082 on epoch=0
05/21/2022 22:42:15 - INFO - __main__ - Step 40 Global step 40 Train loss 16.518475 on epoch=1
05/21/2022 22:42:20 - INFO - __main__ - Step 50 Global step 50 Train loss 15.079950 on epoch=1
05/21/2022 22:44:12 - INFO - __main__ - Global step 50 Train loss 19.181303 Classification-F1 0.0 on epoch=1
05/21/2022 22:44:18 - INFO - __main__ - Step 60 Global step 60 Train loss 13.950668 on epoch=1
05/21/2022 22:44:23 - INFO - __main__ - Step 70 Global step 70 Train loss 13.467993 on epoch=2
05/21/2022 22:44:28 - INFO - __main__ - Step 80 Global step 80 Train loss 12.519667 on epoch=2
05/21/2022 22:44:33 - INFO - __main__ - Step 90 Global step 90 Train loss 11.966671 on epoch=2
05/21/2022 22:44:38 - INFO - __main__ - Step 100 Global step 100 Train loss 10.196467 on epoch=3
05/21/2022 22:44:42 - INFO - __main__ - Global step 100 Train loss 12.420293 Classification-F1 0.0 on epoch=3
05/21/2022 22:44:47 - INFO - __main__ - Step 110 Global step 110 Train loss 9.156705 on epoch=3
05/21/2022 22:44:52 - INFO - __main__ - Step 120 Global step 120 Train loss 6.822769 on epoch=3
05/21/2022 22:44:57 - INFO - __main__ - Step 130 Global step 130 Train loss 4.475152 on epoch=4
05/21/2022 22:45:02 - INFO - __main__ - Step 140 Global step 140 Train loss 4.141403 on epoch=4
05/21/2022 22:45:07 - INFO - __main__ - Step 150 Global step 150 Train loss 4.237886 on epoch=4
05/21/2022 22:45:11 - INFO - __main__ - Global step 150 Train loss 5.766783 Classification-F1 0.13755157218350042 on epoch=4
05/21/2022 22:45:17 - INFO - __main__ - Step 160 Global step 160 Train loss 3.138891 on epoch=4
05/21/2022 22:45:22 - INFO - __main__ - Step 170 Global step 170 Train loss 3.121864 on epoch=5
05/21/2022 22:45:27 - INFO - __main__ - Step 180 Global step 180 Train loss 3.304290 on epoch=5
05/21/2022 22:45:32 - INFO - __main__ - Step 190 Global step 190 Train loss 2.781181 on epoch=5
05/21/2022 22:45:37 - INFO - __main__ - Step 200 Global step 200 Train loss 2.525359 on epoch=6
05/21/2022 22:45:41 - INFO - __main__ - Global step 200 Train loss 2.974317 Classification-F1 0.11872072363107739 on epoch=6
05/21/2022 22:45:46 - INFO - __main__ - Step 210 Global step 210 Train loss 2.654233 on epoch=6
05/21/2022 22:45:51 - INFO - __main__ - Step 220 Global step 220 Train loss 3.042170 on epoch=6
05/21/2022 22:45:56 - INFO - __main__ - Step 230 Global step 230 Train loss 2.236575 on epoch=7
05/21/2022 22:46:01 - INFO - __main__ - Step 240 Global step 240 Train loss 2.519809 on epoch=7
05/21/2022 22:46:06 - INFO - __main__ - Step 250 Global step 250 Train loss 2.328583 on epoch=7
05/21/2022 22:46:10 - INFO - __main__ - Global step 250 Train loss 2.556274 Classification-F1 0.1785243741765481 on epoch=7
05/21/2022 22:46:16 - INFO - __main__ - Step 260 Global step 260 Train loss 2.372082 on epoch=8
05/21/2022 22:46:21 - INFO - __main__ - Step 270 Global step 270 Train loss 2.122032 on epoch=8
05/21/2022 22:46:26 - INFO - __main__ - Step 280 Global step 280 Train loss 1.535011 on epoch=8
05/21/2022 22:46:31 - INFO - __main__ - Step 290 Global step 290 Train loss 1.553235 on epoch=9
05/21/2022 22:46:36 - INFO - __main__ - Step 300 Global step 300 Train loss 3.423032 on epoch=9
05/21/2022 22:46:48 - INFO - __main__ - Global step 300 Train loss 2.201078 Classification-F1 0.09968984423192231 on epoch=9
05/21/2022 22:46:53 - INFO - __main__ - Step 310 Global step 310 Train loss 1.326136 on epoch=9
05/21/2022 22:46:58 - INFO - __main__ - Step 320 Global step 320 Train loss 1.038462 on epoch=9
05/21/2022 22:47:03 - INFO - __main__ - Step 330 Global step 330 Train loss 1.104339 on epoch=10
05/21/2022 22:47:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.840075 on epoch=10
05/21/2022 22:47:14 - INFO - __main__ - Step 350 Global step 350 Train loss 1.110146 on epoch=10
05/21/2022 22:47:17 - INFO - __main__ - Global step 350 Train loss 1.083832 Classification-F1 0.21030943604835228 on epoch=10
05/21/2022 22:47:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.817605 on epoch=11
05/21/2022 22:47:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.943567 on epoch=11
05/21/2022 22:47:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.916727 on epoch=11
05/21/2022 22:47:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.962688 on epoch=12
05/21/2022 22:47:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.936599 on epoch=12
05/21/2022 22:47:47 - INFO - __main__ - Global step 400 Train loss 0.915437 Classification-F1 0.1362847222222222 on epoch=12
05/21/2022 22:47:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.959011 on epoch=12
05/21/2022 22:47:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.740223 on epoch=13
05/21/2022 22:48:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.795356 on epoch=13
05/21/2022 22:48:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.845424 on epoch=13
05/21/2022 22:48:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.750551 on epoch=14
05/21/2022 22:48:16 - INFO - __main__ - Global step 450 Train loss 0.818113 Classification-F1 0.32031970814665156 on epoch=14
05/21/2022 22:48:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.881232 on epoch=14
05/21/2022 22:48:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.828731 on epoch=14
05/21/2022 22:48:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.796862 on epoch=14
05/21/2022 22:48:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.823758 on epoch=15
05/21/2022 22:48:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.751716 on epoch=15
05/21/2022 22:48:46 - INFO - __main__ - Global step 500 Train loss 0.816460 Classification-F1 0.3599660564859729 on epoch=15
05/21/2022 22:48:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.795035 on epoch=15
05/21/2022 22:48:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.649728 on epoch=16
05/21/2022 22:49:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.766560 on epoch=16
05/21/2022 22:49:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.802755 on epoch=16
05/21/2022 22:49:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.622238 on epoch=17
05/21/2022 22:49:15 - INFO - __main__ - Global step 550 Train loss 0.727263 Classification-F1 0.49001642980267535 on epoch=17
05/21/2022 22:49:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.805508 on epoch=17
05/21/2022 22:49:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.695145 on epoch=17
05/21/2022 22:49:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.639016 on epoch=18
05/21/2022 22:49:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.724749 on epoch=18
05/21/2022 22:49:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.682895 on epoch=18
05/21/2022 22:49:45 - INFO - __main__ - Global step 600 Train loss 0.709463 Classification-F1 0.4360816184497719 on epoch=18
05/21/2022 22:49:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.655757 on epoch=19
05/21/2022 22:49:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.718075 on epoch=19
05/21/2022 22:50:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.736604 on epoch=19
05/21/2022 22:50:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.622195 on epoch=19
05/21/2022 22:50:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.612707 on epoch=20
05/21/2022 22:50:14 - INFO - __main__ - Global step 650 Train loss 0.669067 Classification-F1 0.5044343684033662 on epoch=20
05/21/2022 22:50:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.602903 on epoch=20
05/21/2022 22:50:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.621259 on epoch=20
05/21/2022 22:50:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.616421 on epoch=21
05/21/2022 22:50:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.666171 on epoch=21
05/21/2022 22:50:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.671741 on epoch=21
05/21/2022 22:50:44 - INFO - __main__ - Global step 700 Train loss 0.635699 Classification-F1 0.6055042219928398 on epoch=21
05/21/2022 22:50:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.550376 on epoch=22
05/21/2022 22:50:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.616518 on epoch=22
05/21/2022 22:51:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.613017 on epoch=22
05/21/2022 22:51:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.568146 on epoch=23
05/21/2022 22:51:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.616072 on epoch=23
05/21/2022 22:51:14 - INFO - __main__ - Global step 750 Train loss 0.592826 Classification-F1 0.6640165175993828 on epoch=23
05/21/2022 22:51:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.559021 on epoch=23
05/21/2022 22:51:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.478642 on epoch=24
05/21/2022 22:51:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.661870 on epoch=24
05/21/2022 22:51:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.586251 on epoch=24
05/21/2022 22:51:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.499304 on epoch=24
05/21/2022 22:51:43 - INFO - __main__ - Global step 800 Train loss 0.557018 Classification-F1 0.6712007565821616 on epoch=24
05/21/2022 22:51:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.583121 on epoch=25
05/21/2022 22:51:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.542913 on epoch=25
05/21/2022 22:51:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.549849 on epoch=25
05/21/2022 22:52:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.391551 on epoch=26
05/21/2022 22:52:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.548568 on epoch=26
05/21/2022 22:52:13 - INFO - __main__ - Global step 850 Train loss 0.523201 Classification-F1 0.5713658011648548 on epoch=26
05/21/2022 22:52:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.537493 on epoch=26
05/21/2022 22:52:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.387467 on epoch=27
05/21/2022 22:52:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.643871 on epoch=27
05/21/2022 22:52:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.477692 on epoch=27
05/21/2022 22:52:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.387468 on epoch=28
05/21/2022 22:52:42 - INFO - __main__ - Global step 900 Train loss 0.486798 Classification-F1 0.5696981974896634 on epoch=28
05/21/2022 22:52:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.621463 on epoch=28
05/21/2022 22:52:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.511206 on epoch=28
05/21/2022 22:52:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.339045 on epoch=29
05/21/2022 22:53:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.470509 on epoch=29
05/21/2022 22:53:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.507217 on epoch=29
05/21/2022 22:53:11 - INFO - __main__ - Global step 950 Train loss 0.489888 Classification-F1 0.7012853914619894 on epoch=29
05/21/2022 22:53:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.367131 on epoch=29
05/21/2022 22:53:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.422768 on epoch=30
05/21/2022 22:53:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.444201 on epoch=30
05/21/2022 22:53:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.466199 on epoch=30
05/21/2022 22:53:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.302856 on epoch=31
05/21/2022 22:53:38 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 22:53:38 - INFO - __main__ - Printing 3 examples
05/21/2022 22:53:38 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 22:53:38 - INFO - __main__ - ['others']
05/21/2022 22:53:38 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 22:53:38 - INFO - __main__ - ['others']
05/21/2022 22:53:38 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 22:53:38 - INFO - __main__ - ['others']
05/21/2022 22:53:38 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:53:38 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:53:39 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 22:53:39 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 22:53:39 - INFO - __main__ - Printing 3 examples
05/21/2022 22:53:39 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/21/2022 22:53:39 - INFO - __main__ - ['others']
05/21/2022 22:53:39 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/21/2022 22:53:39 - INFO - __main__ - ['others']
05/21/2022 22:53:39 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/21/2022 22:53:39 - INFO - __main__ - ['others']
05/21/2022 22:53:39 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:53:39 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:53:40 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 22:53:41 - INFO - __main__ - Global step 1000 Train loss 0.400631 Classification-F1 0.6870801681058578 on epoch=31
05/21/2022 22:53:41 - INFO - __main__ - save last model!
05/21/2022 22:53:48 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 22:53:49 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 22:53:49 - INFO - __main__ - Printing 3 examples
05/21/2022 22:53:49 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 22:53:49 - INFO - __main__ - ['others']
05/21/2022 22:53:49 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 22:53:49 - INFO - __main__ - ['others']
05/21/2022 22:53:49 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 22:53:49 - INFO - __main__ - ['others']
05/21/2022 22:53:49 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:53:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 22:53:50 - INFO - __main__ - Starting training!
05/21/2022 22:53:51 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:53:56 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 22:54:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_13_0.0003_8_predictions.txt
05/21/2022 22:54:40 - INFO - __main__ - Classification-F1 on test data: 0.5576
05/21/2022 22:54:40 - INFO - __main__ - prefix=emo_128_13, lr=0.0003, bsz=8, dev_performance=0.7012853914619894, test_performance=0.5575821565822254
05/21/2022 22:54:40 - INFO - __main__ - Running ... prefix=emo_128_13, lr=0.0002, bsz=8 ...
05/21/2022 22:54:41 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 22:54:41 - INFO - __main__ - Printing 3 examples
05/21/2022 22:54:41 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 22:54:41 - INFO - __main__ - ['others']
05/21/2022 22:54:41 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 22:54:41 - INFO - __main__ - ['others']
05/21/2022 22:54:41 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 22:54:41 - INFO - __main__ - ['others']
05/21/2022 22:54:41 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:54:41 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:54:42 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 22:54:42 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 22:54:42 - INFO - __main__ - Printing 3 examples
05/21/2022 22:54:42 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/21/2022 22:54:42 - INFO - __main__ - ['others']
05/21/2022 22:54:42 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/21/2022 22:54:42 - INFO - __main__ - ['others']
05/21/2022 22:54:42 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/21/2022 22:54:42 - INFO - __main__ - ['others']
05/21/2022 22:54:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 22:54:42 - INFO - __main__ - Tokenizing Output ...
05/21/2022 22:54:43 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 22:54:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 22:54:55 - INFO - __main__ - Starting training!
05/21/2022 22:54:59 - INFO - __main__ - Step 10 Global step 10 Train loss 25.792675 on epoch=0
05/21/2022 22:55:04 - INFO - __main__ - Step 20 Global step 20 Train loss 19.719454 on epoch=0
05/21/2022 22:55:10 - INFO - __main__ - Step 30 Global step 30 Train loss 18.771938 on epoch=0
05/21/2022 22:55:15 - INFO - __main__ - Step 40 Global step 40 Train loss 16.817539 on epoch=1
05/21/2022 22:55:20 - INFO - __main__ - Step 50 Global step 50 Train loss 17.431488 on epoch=1
05/21/2022 22:55:26 - INFO - __main__ - Global step 50 Train loss 19.706621 Classification-F1 0.0 on epoch=1
05/21/2022 22:55:32 - INFO - __main__ - Step 60 Global step 60 Train loss 16.009760 on epoch=1
05/21/2022 22:55:37 - INFO - __main__ - Step 70 Global step 70 Train loss 15.014772 on epoch=2
05/21/2022 22:55:42 - INFO - __main__ - Step 80 Global step 80 Train loss 14.495886 on epoch=2
05/21/2022 22:55:47 - INFO - __main__ - Step 90 Global step 90 Train loss 13.456052 on epoch=2
05/21/2022 22:55:52 - INFO - __main__ - Step 100 Global step 100 Train loss 13.791346 on epoch=3
05/21/2022 22:55:58 - INFO - __main__ - Global step 100 Train loss 14.553563 Classification-F1 0.0 on epoch=3
05/21/2022 22:56:03 - INFO - __main__ - Step 110 Global step 110 Train loss 13.257387 on epoch=3
05/21/2022 22:56:08 - INFO - __main__ - Step 120 Global step 120 Train loss 12.006371 on epoch=3
05/21/2022 22:56:13 - INFO - __main__ - Step 130 Global step 130 Train loss 11.811247 on epoch=4
05/21/2022 22:56:18 - INFO - __main__ - Step 140 Global step 140 Train loss 11.235092 on epoch=4
05/21/2022 22:56:23 - INFO - __main__ - Step 150 Global step 150 Train loss 9.437984 on epoch=4
05/21/2022 22:56:28 - INFO - __main__ - Global step 150 Train loss 11.549617 Classification-F1 0.0 on epoch=4
05/21/2022 22:56:34 - INFO - __main__ - Step 160 Global step 160 Train loss 5.236177 on epoch=4
05/21/2022 22:56:39 - INFO - __main__ - Step 170 Global step 170 Train loss 1.729840 on epoch=5
05/21/2022 22:56:44 - INFO - __main__ - Step 180 Global step 180 Train loss 1.750133 on epoch=5
05/21/2022 22:56:49 - INFO - __main__ - Step 190 Global step 190 Train loss 2.372139 on epoch=5
05/21/2022 22:56:54 - INFO - __main__ - Step 200 Global step 200 Train loss 1.441542 on epoch=6
05/21/2022 22:56:58 - INFO - __main__ - Global step 200 Train loss 2.505966 Classification-F1 0.3041283196742251 on epoch=6
05/21/2022 22:57:04 - INFO - __main__ - Step 210 Global step 210 Train loss 1.577200 on epoch=6
05/21/2022 22:57:10 - INFO - __main__ - Step 220 Global step 220 Train loss 1.694153 on epoch=6
05/21/2022 22:57:15 - INFO - __main__ - Step 230 Global step 230 Train loss 1.424218 on epoch=7
05/21/2022 22:57:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.862504 on epoch=7
05/21/2022 22:57:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.642094 on epoch=7
05/21/2022 22:57:29 - INFO - __main__ - Global step 250 Train loss 1.240034 Classification-F1 0.5726672344458079 on epoch=7
05/21/2022 22:57:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.578513 on epoch=8
05/21/2022 22:57:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.591771 on epoch=8
05/21/2022 22:57:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.607249 on epoch=8
05/21/2022 22:57:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.438536 on epoch=9
05/21/2022 22:57:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.584571 on epoch=9
05/21/2022 22:58:01 - INFO - __main__ - Global step 300 Train loss 0.560128 Classification-F1 0.7318365926204075 on epoch=9
05/21/2022 22:58:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.579348 on epoch=9
05/21/2022 22:58:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.527983 on epoch=9
05/21/2022 22:58:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.365948 on epoch=10
05/21/2022 22:58:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.594740 on epoch=10
05/21/2022 22:58:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.479985 on epoch=10
05/21/2022 22:58:31 - INFO - __main__ - Global step 350 Train loss 0.509601 Classification-F1 0.7718803000602632 on epoch=10
05/21/2022 22:58:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.399988 on epoch=11
05/21/2022 22:58:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.469504 on epoch=11
05/21/2022 22:58:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.556851 on epoch=11
05/21/2022 22:58:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.360443 on epoch=12
05/21/2022 22:58:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.496390 on epoch=12
05/21/2022 22:59:02 - INFO - __main__ - Global step 400 Train loss 0.456635 Classification-F1 0.7738452809849463 on epoch=12
05/21/2022 22:59:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.342093 on epoch=12
05/21/2022 22:59:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.282377 on epoch=13
05/21/2022 22:59:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.468576 on epoch=13
05/21/2022 22:59:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.438935 on epoch=13
05/21/2022 22:59:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.206952 on epoch=14
05/21/2022 22:59:32 - INFO - __main__ - Global step 450 Train loss 0.347787 Classification-F1 0.7879957522418407 on epoch=14
05/21/2022 22:59:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.329079 on epoch=14
05/21/2022 22:59:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.381154 on epoch=14
05/21/2022 22:59:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.177499 on epoch=14
05/21/2022 22:59:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.170199 on epoch=15
05/21/2022 22:59:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.249825 on epoch=15
05/21/2022 23:00:03 - INFO - __main__ - Global step 500 Train loss 0.261551 Classification-F1 0.6405940696086766 on epoch=15
05/21/2022 23:00:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.370569 on epoch=15
05/21/2022 23:00:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.141966 on epoch=16
05/21/2022 23:00:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.208156 on epoch=16
05/21/2022 23:00:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.243976 on epoch=16
05/21/2022 23:00:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.166705 on epoch=17
05/21/2022 23:00:32 - INFO - __main__ - Global step 550 Train loss 0.226274 Classification-F1 0.7775877875171547 on epoch=17
05/21/2022 23:00:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.238510 on epoch=17
05/21/2022 23:00:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.231856 on epoch=17
05/21/2022 23:00:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.110033 on epoch=18
05/21/2022 23:00:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.161648 on epoch=18
05/21/2022 23:00:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.147048 on epoch=18
05/21/2022 23:01:02 - INFO - __main__ - Global step 600 Train loss 0.177819 Classification-F1 0.6462622242273097 on epoch=18
05/21/2022 23:01:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.139727 on epoch=19
05/21/2022 23:01:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.158601 on epoch=19
05/21/2022 23:01:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.262449 on epoch=19
05/21/2022 23:01:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.152927 on epoch=19
05/21/2022 23:01:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.158555 on epoch=20
05/21/2022 23:01:31 - INFO - __main__ - Global step 650 Train loss 0.174452 Classification-F1 0.8323912432509482 on epoch=20
05/21/2022 23:01:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.109454 on epoch=20
05/21/2022 23:01:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.133990 on epoch=20
05/21/2022 23:01:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.026155 on epoch=21
05/21/2022 23:01:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.142024 on epoch=21
05/21/2022 23:01:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.124747 on epoch=21
05/21/2022 23:02:02 - INFO - __main__ - Global step 700 Train loss 0.107274 Classification-F1 0.8292646435538198 on epoch=21
05/21/2022 23:02:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.081735 on epoch=22
05/21/2022 23:02:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.275405 on epoch=22
05/21/2022 23:02:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.054176 on epoch=22
05/21/2022 23:02:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.066919 on epoch=23
05/21/2022 23:02:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.104484 on epoch=23
05/21/2022 23:02:31 - INFO - __main__ - Global step 750 Train loss 0.116544 Classification-F1 0.6380780463073483 on epoch=23
05/21/2022 23:02:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.145841 on epoch=23
05/21/2022 23:02:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.073115 on epoch=24
05/21/2022 23:02:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.097470 on epoch=24
05/21/2022 23:02:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.051655 on epoch=24
05/21/2022 23:02:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.043832 on epoch=24
05/21/2022 23:03:00 - INFO - __main__ - Global step 800 Train loss 0.082383 Classification-F1 0.8277469495670637 on epoch=24
05/21/2022 23:03:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.046024 on epoch=25
05/21/2022 23:03:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.045469 on epoch=25
05/21/2022 23:03:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.046575 on epoch=25
05/21/2022 23:03:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.059711 on epoch=26
05/21/2022 23:03:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.067868 on epoch=26
05/21/2022 23:03:29 - INFO - __main__ - Global step 850 Train loss 0.053130 Classification-F1 0.6664199950841729 on epoch=26
05/21/2022 23:03:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.049789 on epoch=26
05/21/2022 23:03:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.066654 on epoch=27
05/21/2022 23:03:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.149312 on epoch=27
05/21/2022 23:03:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.063228 on epoch=27
05/21/2022 23:03:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.034569 on epoch=28
05/21/2022 23:03:58 - INFO - __main__ - Global step 900 Train loss 0.072711 Classification-F1 0.6667127321539915 on epoch=28
05/21/2022 23:04:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.025370 on epoch=28
05/21/2022 23:04:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.035121 on epoch=28
05/21/2022 23:04:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.055373 on epoch=29
05/21/2022 23:04:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.088058 on epoch=29
05/21/2022 23:04:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.035850 on epoch=29
05/21/2022 23:04:27 - INFO - __main__ - Global step 950 Train loss 0.047954 Classification-F1 0.5586807282552898 on epoch=29
05/21/2022 23:04:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.018274 on epoch=29
05/21/2022 23:04:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.021307 on epoch=30
05/21/2022 23:04:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.064355 on epoch=30
05/21/2022 23:04:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.022717 on epoch=30
05/21/2022 23:04:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.002354 on epoch=31
05/21/2022 23:04:53 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 23:04:53 - INFO - __main__ - Printing 3 examples
05/21/2022 23:04:53 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 23:04:53 - INFO - __main__ - ['others']
05/21/2022 23:04:53 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 23:04:53 - INFO - __main__ - ['others']
05/21/2022 23:04:53 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 23:04:53 - INFO - __main__ - ['others']
05/21/2022 23:04:53 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:04:53 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:04:54 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 23:04:54 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 23:04:54 - INFO - __main__ - Printing 3 examples
05/21/2022 23:04:54 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/21/2022 23:04:54 - INFO - __main__ - ['others']
05/21/2022 23:04:54 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/21/2022 23:04:54 - INFO - __main__ - ['others']
05/21/2022 23:04:54 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/21/2022 23:04:54 - INFO - __main__ - ['others']
05/21/2022 23:04:54 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:04:54 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:04:54 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 23:04:56 - INFO - __main__ - Global step 1000 Train loss 0.025802 Classification-F1 0.6618628540275695 on epoch=31
05/21/2022 23:04:56 - INFO - __main__ - save last model!
05/21/2022 23:05:03 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 23:05:04 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 23:05:04 - INFO - __main__ - Printing 3 examples
05/21/2022 23:05:04 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 23:05:04 - INFO - __main__ - ['others']
05/21/2022 23:05:04 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 23:05:04 - INFO - __main__ - ['others']
05/21/2022 23:05:04 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 23:05:04 - INFO - __main__ - ['others']
05/21/2022 23:05:04 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:05:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 23:05:05 - INFO - __main__ - Starting training!
05/21/2022 23:05:06 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:05:11 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 23:05:55 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_13_0.0002_8_predictions.txt
05/21/2022 23:05:55 - INFO - __main__ - Classification-F1 on test data: 0.0883
05/21/2022 23:05:55 - INFO - __main__ - prefix=emo_128_13, lr=0.0002, bsz=8, dev_performance=0.8323912432509482, test_performance=0.08830792700747776
05/21/2022 23:05:55 - INFO - __main__ - Running ... prefix=emo_128_13, lr=0.0001, bsz=8 ...
05/21/2022 23:05:56 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 23:05:56 - INFO - __main__ - Printing 3 examples
05/21/2022 23:05:56 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 23:05:56 - INFO - __main__ - ['others']
05/21/2022 23:05:56 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 23:05:56 - INFO - __main__ - ['others']
05/21/2022 23:05:56 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 23:05:56 - INFO - __main__ - ['others']
05/21/2022 23:05:56 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:05:56 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:05:57 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 23:05:57 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 23:05:57 - INFO - __main__ - Printing 3 examples
05/21/2022 23:05:57 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/21/2022 23:05:57 - INFO - __main__ - ['others']
05/21/2022 23:05:57 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/21/2022 23:05:57 - INFO - __main__ - ['others']
05/21/2022 23:05:57 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/21/2022 23:05:57 - INFO - __main__ - ['others']
05/21/2022 23:05:57 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:05:57 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:05:57 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 23:06:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 23:06:10 - INFO - __main__ - Starting training!
05/21/2022 23:06:14 - INFO - __main__ - Step 10 Global step 10 Train loss 24.739546 on epoch=0
05/21/2022 23:06:19 - INFO - __main__ - Step 20 Global step 20 Train loss 21.237610 on epoch=0
05/21/2022 23:06:24 - INFO - __main__ - Step 30 Global step 30 Train loss 19.617395 on epoch=0
05/21/2022 23:06:29 - INFO - __main__ - Step 40 Global step 40 Train loss 18.399054 on epoch=1
05/21/2022 23:06:34 - INFO - __main__ - Step 50 Global step 50 Train loss 17.429790 on epoch=1
05/21/2022 23:09:14 - INFO - __main__ - Global step 50 Train loss 20.284679 Classification-F1 0.0 on epoch=1
05/21/2022 23:09:19 - INFO - __main__ - Step 60 Global step 60 Train loss 18.074238 on epoch=1
05/21/2022 23:09:25 - INFO - __main__ - Step 70 Global step 70 Train loss 16.429588 on epoch=2
05/21/2022 23:09:30 - INFO - __main__ - Step 80 Global step 80 Train loss 16.779634 on epoch=2
05/21/2022 23:09:35 - INFO - __main__ - Step 90 Global step 90 Train loss 16.016872 on epoch=2
05/21/2022 23:09:40 - INFO - __main__ - Step 100 Global step 100 Train loss 15.850183 on epoch=3
05/21/2022 23:12:00 - INFO - __main__ - Global step 100 Train loss 16.630102 Classification-F1 0.0 on epoch=3
05/21/2022 23:12:05 - INFO - __main__ - Step 110 Global step 110 Train loss 15.669243 on epoch=3
05/21/2022 23:12:10 - INFO - __main__ - Step 120 Global step 120 Train loss 15.621181 on epoch=3
05/21/2022 23:12:15 - INFO - __main__ - Step 130 Global step 130 Train loss 14.357821 on epoch=4
05/21/2022 23:12:20 - INFO - __main__ - Step 140 Global step 140 Train loss 15.009171 on epoch=4
05/21/2022 23:12:25 - INFO - __main__ - Step 150 Global step 150 Train loss 14.177042 on epoch=4
05/21/2022 23:14:35 - INFO - __main__ - Global step 150 Train loss 14.966892 Classification-F1 0.0 on epoch=4
05/21/2022 23:14:40 - INFO - __main__ - Step 160 Global step 160 Train loss 14.606259 on epoch=4
05/21/2022 23:14:45 - INFO - __main__ - Step 170 Global step 170 Train loss 13.154699 on epoch=5
05/21/2022 23:14:51 - INFO - __main__ - Step 180 Global step 180 Train loss 13.199308 on epoch=5
05/21/2022 23:14:56 - INFO - __main__ - Step 190 Global step 190 Train loss 13.275146 on epoch=5
05/21/2022 23:15:01 - INFO - __main__ - Step 200 Global step 200 Train loss 12.447870 on epoch=6
05/21/2022 23:17:06 - INFO - __main__ - Global step 200 Train loss 13.336657 Classification-F1 7.809296968040452e-05 on epoch=6
05/21/2022 23:17:12 - INFO - __main__ - Step 210 Global step 210 Train loss 11.687159 on epoch=6
05/21/2022 23:17:17 - INFO - __main__ - Step 220 Global step 220 Train loss 11.571890 on epoch=6
05/21/2022 23:17:22 - INFO - __main__ - Step 230 Global step 230 Train loss 12.345226 on epoch=7
05/21/2022 23:17:27 - INFO - __main__ - Step 240 Global step 240 Train loss 11.500576 on epoch=7
05/21/2022 23:17:32 - INFO - __main__ - Step 250 Global step 250 Train loss 10.733146 on epoch=7
05/21/2022 23:18:54 - INFO - __main__ - Global step 250 Train loss 11.567599 Classification-F1 0.00037066215110821335 on epoch=7
05/21/2022 23:19:00 - INFO - __main__ - Step 260 Global step 260 Train loss 10.978274 on epoch=8
05/21/2022 23:19:05 - INFO - __main__ - Step 270 Global step 270 Train loss 10.628430 on epoch=8
05/21/2022 23:19:11 - INFO - __main__ - Step 280 Global step 280 Train loss 9.451770 on epoch=8
05/21/2022 23:19:16 - INFO - __main__ - Step 290 Global step 290 Train loss 8.810257 on epoch=9
05/21/2022 23:19:21 - INFO - __main__ - Step 300 Global step 300 Train loss 8.085741 on epoch=9
05/21/2022 23:20:28 - INFO - __main__ - Global step 300 Train loss 9.590895 Classification-F1 0.0005582971646232613 on epoch=9
05/21/2022 23:20:34 - INFO - __main__ - Step 310 Global step 310 Train loss 6.555543 on epoch=9
05/21/2022 23:20:39 - INFO - __main__ - Step 320 Global step 320 Train loss 5.984653 on epoch=9
05/21/2022 23:20:44 - INFO - __main__ - Step 330 Global step 330 Train loss 4.289577 on epoch=10
05/21/2022 23:20:49 - INFO - __main__ - Step 340 Global step 340 Train loss 3.768518 on epoch=10
05/21/2022 23:20:54 - INFO - __main__ - Step 350 Global step 350 Train loss 5.629389 on epoch=10
05/21/2022 23:20:58 - INFO - __main__ - Global step 350 Train loss 5.245536 Classification-F1 0.1 on epoch=10
05/21/2022 23:21:04 - INFO - __main__ - Step 360 Global step 360 Train loss 4.320443 on epoch=11
05/21/2022 23:21:09 - INFO - __main__ - Step 370 Global step 370 Train loss 3.993829 on epoch=11
05/21/2022 23:21:14 - INFO - __main__ - Step 380 Global step 380 Train loss 4.402610 on epoch=11
05/21/2022 23:21:19 - INFO - __main__ - Step 390 Global step 390 Train loss 3.828108 on epoch=12
05/21/2022 23:21:24 - INFO - __main__ - Step 400 Global step 400 Train loss 3.778299 on epoch=12
05/21/2022 23:21:28 - INFO - __main__ - Global step 400 Train loss 4.064657 Classification-F1 0.10403246351493978 on epoch=12
05/21/2022 23:21:34 - INFO - __main__ - Step 410 Global step 410 Train loss 3.588629 on epoch=12
05/21/2022 23:21:39 - INFO - __main__ - Step 420 Global step 420 Train loss 4.185050 on epoch=13
05/21/2022 23:21:44 - INFO - __main__ - Step 430 Global step 430 Train loss 3.692230 on epoch=13
05/21/2022 23:21:50 - INFO - __main__ - Step 440 Global step 440 Train loss 3.302630 on epoch=13
05/21/2022 23:21:55 - INFO - __main__ - Step 450 Global step 450 Train loss 3.887658 on epoch=14
05/21/2022 23:21:59 - INFO - __main__ - Global step 450 Train loss 3.731239 Classification-F1 0.23769422274214347 on epoch=14
05/21/2022 23:22:05 - INFO - __main__ - Step 460 Global step 460 Train loss 3.979731 on epoch=14
05/21/2022 23:22:10 - INFO - __main__ - Step 470 Global step 470 Train loss 3.220742 on epoch=14
05/21/2022 23:22:15 - INFO - __main__ - Step 480 Global step 480 Train loss 3.172413 on epoch=14
05/21/2022 23:22:20 - INFO - __main__ - Step 490 Global step 490 Train loss 3.337901 on epoch=15
05/21/2022 23:22:25 - INFO - __main__ - Step 500 Global step 500 Train loss 3.354710 on epoch=15
05/21/2022 23:22:29 - INFO - __main__ - Global step 500 Train loss 3.413100 Classification-F1 0.3393190521951896 on epoch=15
05/21/2022 23:22:35 - INFO - __main__ - Step 510 Global step 510 Train loss 3.558300 on epoch=15
05/21/2022 23:22:40 - INFO - __main__ - Step 520 Global step 520 Train loss 3.065040 on epoch=16
05/21/2022 23:22:45 - INFO - __main__ - Step 530 Global step 530 Train loss 3.742219 on epoch=16
05/21/2022 23:22:50 - INFO - __main__ - Step 540 Global step 540 Train loss 3.081630 on epoch=16
05/21/2022 23:22:55 - INFO - __main__ - Step 550 Global step 550 Train loss 2.834138 on epoch=17
05/21/2022 23:23:00 - INFO - __main__ - Global step 550 Train loss 3.256266 Classification-F1 0.26702066590126294 on epoch=17
05/21/2022 23:23:05 - INFO - __main__ - Step 560 Global step 560 Train loss 2.589039 on epoch=17
05/21/2022 23:23:10 - INFO - __main__ - Step 570 Global step 570 Train loss 2.754673 on epoch=17
05/21/2022 23:23:15 - INFO - __main__ - Step 580 Global step 580 Train loss 2.882431 on epoch=18
05/21/2022 23:23:20 - INFO - __main__ - Step 590 Global step 590 Train loss 2.427008 on epoch=18
05/21/2022 23:23:25 - INFO - __main__ - Step 600 Global step 600 Train loss 2.880646 on epoch=18
05/21/2022 23:23:29 - INFO - __main__ - Global step 600 Train loss 2.706759 Classification-F1 0.42565034910550326 on epoch=18
05/21/2022 23:23:35 - INFO - __main__ - Step 610 Global step 610 Train loss 3.522501 on epoch=19
05/21/2022 23:23:40 - INFO - __main__ - Step 620 Global step 620 Train loss 3.037793 on epoch=19
05/21/2022 23:23:45 - INFO - __main__ - Step 630 Global step 630 Train loss 2.247494 on epoch=19
05/21/2022 23:23:50 - INFO - __main__ - Step 640 Global step 640 Train loss 2.775223 on epoch=19
05/21/2022 23:23:55 - INFO - __main__ - Step 650 Global step 650 Train loss 2.461686 on epoch=20
05/21/2022 23:24:00 - INFO - __main__ - Global step 650 Train loss 2.808939 Classification-F1 0.24386621669944575 on epoch=20
05/21/2022 23:24:05 - INFO - __main__ - Step 660 Global step 660 Train loss 2.656075 on epoch=20
05/21/2022 23:24:10 - INFO - __main__ - Step 670 Global step 670 Train loss 2.589979 on epoch=20
05/21/2022 23:24:15 - INFO - __main__ - Step 680 Global step 680 Train loss 2.413672 on epoch=21
05/21/2022 23:24:20 - INFO - __main__ - Step 690 Global step 690 Train loss 2.770166 on epoch=21
05/21/2022 23:24:25 - INFO - __main__ - Step 700 Global step 700 Train loss 2.644109 on epoch=21
05/21/2022 23:24:29 - INFO - __main__ - Global step 700 Train loss 2.614800 Classification-F1 0.3398879373414406 on epoch=21
05/21/2022 23:24:34 - INFO - __main__ - Step 710 Global step 710 Train loss 1.982801 on epoch=22
05/21/2022 23:24:40 - INFO - __main__ - Step 720 Global step 720 Train loss 1.666693 on epoch=22
05/21/2022 23:24:45 - INFO - __main__ - Step 730 Global step 730 Train loss 1.815294 on epoch=22
05/21/2022 23:24:50 - INFO - __main__ - Step 740 Global step 740 Train loss 1.647997 on epoch=23
05/21/2022 23:24:55 - INFO - __main__ - Step 750 Global step 750 Train loss 1.623530 on epoch=23
05/21/2022 23:24:59 - INFO - __main__ - Global step 750 Train loss 1.747263 Classification-F1 0.556686765615337 on epoch=23
05/21/2022 23:25:05 - INFO - __main__ - Step 760 Global step 760 Train loss 1.329512 on epoch=23
05/21/2022 23:25:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.895743 on epoch=24
05/21/2022 23:25:15 - INFO - __main__ - Step 780 Global step 780 Train loss 1.065464 on epoch=24
05/21/2022 23:25:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.888971 on epoch=24
05/21/2022 23:25:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.550527 on epoch=24
05/21/2022 23:25:29 - INFO - __main__ - Global step 800 Train loss 0.946043 Classification-F1 0.7476537852644047 on epoch=24
05/21/2022 23:25:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.590802 on epoch=25
05/21/2022 23:25:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.731649 on epoch=25
05/21/2022 23:25:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.551971 on epoch=25
05/21/2022 23:25:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.443345 on epoch=26
05/21/2022 23:25:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.625729 on epoch=26
05/21/2022 23:25:59 - INFO - __main__ - Global step 850 Train loss 0.588699 Classification-F1 0.6826225741226706 on epoch=26
05/21/2022 23:26:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.494511 on epoch=26
05/21/2022 23:26:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.439837 on epoch=27
05/21/2022 23:26:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.617509 on epoch=27
05/21/2022 23:26:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.487236 on epoch=27
05/21/2022 23:26:25 - INFO - __main__ - Step 900 Global step 900 Train loss 0.448759 on epoch=28
05/21/2022 23:26:29 - INFO - __main__ - Global step 900 Train loss 0.497570 Classification-F1 0.7121902927685468 on epoch=28
05/21/2022 23:26:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.481410 on epoch=28
05/21/2022 23:26:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.544551 on epoch=28
05/21/2022 23:26:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.304985 on epoch=29
05/21/2022 23:26:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.495535 on epoch=29
05/21/2022 23:26:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.467705 on epoch=29
05/21/2022 23:26:59 - INFO - __main__ - Global step 950 Train loss 0.458837 Classification-F1 0.7549728822948637 on epoch=29
05/21/2022 23:27:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.335525 on epoch=29
05/21/2022 23:27:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.320726 on epoch=30
05/21/2022 23:27:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.460329 on epoch=30
05/21/2022 23:27:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.434463 on epoch=30
05/21/2022 23:27:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.294943 on epoch=31
05/21/2022 23:27:26 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 23:27:26 - INFO - __main__ - Printing 3 examples
05/21/2022 23:27:26 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 23:27:26 - INFO - __main__ - ['sad']
05/21/2022 23:27:26 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 23:27:26 - INFO - __main__ - ['sad']
05/21/2022 23:27:26 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 23:27:26 - INFO - __main__ - ['sad']
05/21/2022 23:27:26 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:27:26 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:27:27 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 23:27:27 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 23:27:27 - INFO - __main__ - Printing 3 examples
05/21/2022 23:27:27 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/21/2022 23:27:27 - INFO - __main__ - ['sad']
05/21/2022 23:27:27 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/21/2022 23:27:27 - INFO - __main__ - ['sad']
05/21/2022 23:27:27 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/21/2022 23:27:27 - INFO - __main__ - ['sad']
05/21/2022 23:27:27 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:27:27 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:27:28 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 23:27:29 - INFO - __main__ - Global step 1000 Train loss 0.369197 Classification-F1 0.7513154094440593 on epoch=31
05/21/2022 23:27:29 - INFO - __main__ - save last model!
05/21/2022 23:27:36 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 23:27:37 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 23:27:37 - INFO - __main__ - Printing 3 examples
05/21/2022 23:27:37 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 23:27:37 - INFO - __main__ - ['others']
05/21/2022 23:27:37 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 23:27:37 - INFO - __main__ - ['others']
05/21/2022 23:27:37 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 23:27:37 - INFO - __main__ - ['others']
05/21/2022 23:27:37 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:27:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 23:27:38 - INFO - __main__ - Starting training!
05/21/2022 23:27:39 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:27:44 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 23:28:28 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_13_0.0001_8_predictions.txt
05/21/2022 23:28:28 - INFO - __main__ - Classification-F1 on test data: 0.4445
05/21/2022 23:28:28 - INFO - __main__ - prefix=emo_128_13, lr=0.0001, bsz=8, dev_performance=0.7549728822948637, test_performance=0.44446398558795197
05/21/2022 23:28:28 - INFO - __main__ - Running ... prefix=emo_128_21, lr=0.0005, bsz=8 ...
05/21/2022 23:28:29 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 23:28:29 - INFO - __main__ - Printing 3 examples
05/21/2022 23:28:29 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 23:28:29 - INFO - __main__ - ['sad']
05/21/2022 23:28:29 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 23:28:29 - INFO - __main__ - ['sad']
05/21/2022 23:28:29 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 23:28:29 - INFO - __main__ - ['sad']
05/21/2022 23:28:29 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:28:29 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:28:30 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 23:28:30 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 23:28:30 - INFO - __main__ - Printing 3 examples
05/21/2022 23:28:30 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/21/2022 23:28:30 - INFO - __main__ - ['sad']
05/21/2022 23:28:30 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/21/2022 23:28:30 - INFO - __main__ - ['sad']
05/21/2022 23:28:30 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/21/2022 23:28:30 - INFO - __main__ - ['sad']
05/21/2022 23:28:30 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:28:30 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:28:30 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 23:28:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 23:28:42 - INFO - __main__ - Starting training!
05/21/2022 23:28:46 - INFO - __main__ - Step 10 Global step 10 Train loss 23.904194 on epoch=0
05/21/2022 23:28:51 - INFO - __main__ - Step 20 Global step 20 Train loss 18.480381 on epoch=0
05/21/2022 23:28:56 - INFO - __main__ - Step 30 Global step 30 Train loss 17.011478 on epoch=0
05/21/2022 23:29:01 - INFO - __main__ - Step 40 Global step 40 Train loss 14.520956 on epoch=1
05/21/2022 23:29:06 - INFO - __main__ - Step 50 Global step 50 Train loss 13.749234 on epoch=1
05/21/2022 23:29:11 - INFO - __main__ - Global step 50 Train loss 17.533247 Classification-F1 0.0 on epoch=1
05/21/2022 23:29:17 - INFO - __main__ - Step 60 Global step 60 Train loss 11.538737 on epoch=1
05/21/2022 23:29:22 - INFO - __main__ - Step 70 Global step 70 Train loss 9.967215 on epoch=2
05/21/2022 23:29:26 - INFO - __main__ - Step 80 Global step 80 Train loss 6.978796 on epoch=2
05/21/2022 23:29:31 - INFO - __main__ - Step 90 Global step 90 Train loss 2.875852 on epoch=2
05/21/2022 23:29:36 - INFO - __main__ - Step 100 Global step 100 Train loss 1.404929 on epoch=3
05/21/2022 23:29:41 - INFO - __main__ - Global step 100 Train loss 6.553106 Classification-F1 0.28998672167686257 on epoch=3
05/21/2022 23:29:47 - INFO - __main__ - Step 110 Global step 110 Train loss 1.944871 on epoch=3
05/21/2022 23:29:52 - INFO - __main__ - Step 120 Global step 120 Train loss 1.286920 on epoch=3
05/21/2022 23:29:57 - INFO - __main__ - Step 130 Global step 130 Train loss 1.345761 on epoch=4
05/21/2022 23:30:02 - INFO - __main__ - Step 140 Global step 140 Train loss 1.254285 on epoch=4
05/21/2022 23:30:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.932691 on epoch=4
05/21/2022 23:30:11 - INFO - __main__ - Global step 150 Train loss 1.352906 Classification-F1 0.4706596038691847 on epoch=4
05/21/2022 23:30:16 - INFO - __main__ - Step 160 Global step 160 Train loss 1.653755 on epoch=4
05/21/2022 23:30:21 - INFO - __main__ - Step 170 Global step 170 Train loss 2.741580 on epoch=5
05/21/2022 23:30:27 - INFO - __main__ - Step 180 Global step 180 Train loss 1.358856 on epoch=5
05/21/2022 23:30:32 - INFO - __main__ - Step 190 Global step 190 Train loss 1.221404 on epoch=5
05/21/2022 23:30:37 - INFO - __main__ - Step 200 Global step 200 Train loss 1.057794 on epoch=6
05/21/2022 23:30:41 - INFO - __main__ - Global step 200 Train loss 1.606678 Classification-F1 0.18417288679770727 on epoch=6
05/21/2022 23:30:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.846289 on epoch=6
05/21/2022 23:30:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.882506 on epoch=6
05/21/2022 23:30:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.678783 on epoch=7
05/21/2022 23:31:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.694029 on epoch=7
05/21/2022 23:31:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.801444 on epoch=7
05/21/2022 23:31:10 - INFO - __main__ - Global step 250 Train loss 0.780610 Classification-F1 0.4702950768122778 on epoch=7
05/21/2022 23:31:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.694539 on epoch=8
05/21/2022 23:31:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.615789 on epoch=8
05/21/2022 23:31:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.707008 on epoch=8
05/21/2022 23:31:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.665654 on epoch=9
05/21/2022 23:31:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.669317 on epoch=9
05/21/2022 23:31:39 - INFO - __main__ - Global step 300 Train loss 0.670461 Classification-F1 0.48772325813546963 on epoch=9
05/21/2022 23:31:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.702320 on epoch=9
05/21/2022 23:31:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.608427 on epoch=9
05/21/2022 23:31:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.715414 on epoch=10
05/21/2022 23:32:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.552470 on epoch=10
05/21/2022 23:32:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.673177 on epoch=10
05/21/2022 23:32:09 - INFO - __main__ - Global step 350 Train loss 0.650361 Classification-F1 0.5974236595210559 on epoch=10
05/21/2022 23:32:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.621923 on epoch=11
05/21/2022 23:32:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.522875 on epoch=11
05/21/2022 23:32:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.585972 on epoch=11
05/21/2022 23:32:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.611826 on epoch=12
05/21/2022 23:32:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.576410 on epoch=12
05/21/2022 23:32:40 - INFO - __main__ - Global step 400 Train loss 0.583801 Classification-F1 0.48526658013607704 on epoch=12
05/21/2022 23:32:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.576050 on epoch=12
05/21/2022 23:32:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.547409 on epoch=13
05/21/2022 23:32:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.468513 on epoch=13
05/21/2022 23:33:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.634108 on epoch=13
05/21/2022 23:33:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.469610 on epoch=14
05/21/2022 23:33:09 - INFO - __main__ - Global step 450 Train loss 0.539138 Classification-F1 0.6751828360018343 on epoch=14
05/21/2022 23:33:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.537023 on epoch=14
05/21/2022 23:33:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.447990 on epoch=14
05/21/2022 23:33:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.478628 on epoch=14
05/21/2022 23:33:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.446720 on epoch=15
05/21/2022 23:33:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.449310 on epoch=15
05/21/2022 23:33:39 - INFO - __main__ - Global step 500 Train loss 0.471934 Classification-F1 0.5901901655849024 on epoch=15
05/21/2022 23:33:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.508416 on epoch=15
05/21/2022 23:33:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.339482 on epoch=16
05/21/2022 23:33:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.260077 on epoch=16
05/21/2022 23:34:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.338238 on epoch=16
05/21/2022 23:34:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.383639 on epoch=17
05/21/2022 23:34:09 - INFO - __main__ - Global step 550 Train loss 0.365970 Classification-F1 0.6259881827745492 on epoch=17
05/21/2022 23:34:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.330426 on epoch=17
05/21/2022 23:34:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.302815 on epoch=17
05/21/2022 23:34:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.352585 on epoch=18
05/21/2022 23:34:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.348682 on epoch=18
05/21/2022 23:34:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.275550 on epoch=18
05/21/2022 23:34:38 - INFO - __main__ - Global step 600 Train loss 0.322011 Classification-F1 0.6498566350947895 on epoch=18
05/21/2022 23:34:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.246385 on epoch=19
05/21/2022 23:34:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.294174 on epoch=19
05/21/2022 23:34:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.264314 on epoch=19
05/21/2022 23:34:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.178976 on epoch=19
05/21/2022 23:35:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.203707 on epoch=20
05/21/2022 23:35:07 - INFO - __main__ - Global step 650 Train loss 0.237511 Classification-F1 0.5610208904207682 on epoch=20
05/21/2022 23:35:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.239016 on epoch=20
05/21/2022 23:35:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.291186 on epoch=20
05/21/2022 23:35:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.244132 on epoch=21
05/21/2022 23:35:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.160183 on epoch=21
05/21/2022 23:35:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.186205 on epoch=21
05/21/2022 23:35:37 - INFO - __main__ - Global step 700 Train loss 0.224144 Classification-F1 0.7191725802474322 on epoch=21
05/21/2022 23:35:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.183258 on epoch=22
05/21/2022 23:35:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.198364 on epoch=22
05/21/2022 23:35:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.223922 on epoch=22
05/21/2022 23:35:58 - INFO - __main__ - Step 740 Global step 740 Train loss 0.094190 on epoch=23
05/21/2022 23:36:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.144753 on epoch=23
05/21/2022 23:36:07 - INFO - __main__ - Global step 750 Train loss 0.168897 Classification-F1 0.7506320740283903 on epoch=23
05/21/2022 23:36:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.149379 on epoch=23
05/21/2022 23:36:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.061673 on epoch=24
05/21/2022 23:36:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.042310 on epoch=24
05/21/2022 23:36:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.132314 on epoch=24
05/21/2022 23:36:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.102182 on epoch=24
05/21/2022 23:36:37 - INFO - __main__ - Global step 800 Train loss 0.097571 Classification-F1 0.5641951345126401 on epoch=24
05/21/2022 23:36:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.099956 on epoch=25
05/21/2022 23:36:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.151898 on epoch=25
05/21/2022 23:36:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.067345 on epoch=25
05/21/2022 23:36:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.051485 on epoch=26
05/21/2022 23:37:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.040845 on epoch=26
05/21/2022 23:37:06 - INFO - __main__ - Global step 850 Train loss 0.082306 Classification-F1 0.5003171040427475 on epoch=26
05/21/2022 23:37:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.045813 on epoch=26
05/21/2022 23:37:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.047292 on epoch=27
05/21/2022 23:37:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.066764 on epoch=27
05/21/2022 23:37:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.063649 on epoch=27
05/21/2022 23:37:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.081138 on epoch=28
05/21/2022 23:37:36 - INFO - __main__ - Global step 900 Train loss 0.060931 Classification-F1 0.7247063037950987 on epoch=28
05/21/2022 23:37:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.063009 on epoch=28
05/21/2022 23:37:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.116953 on epoch=28
05/21/2022 23:37:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.027885 on epoch=29
05/21/2022 23:37:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.085100 on epoch=29
05/21/2022 23:38:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.033722 on epoch=29
05/21/2022 23:38:06 - INFO - __main__ - Global step 950 Train loss 0.065334 Classification-F1 0.6040092028734403 on epoch=29
05/21/2022 23:38:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.068503 on epoch=29
05/21/2022 23:38:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.010639 on epoch=30
05/21/2022 23:38:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.047892 on epoch=30
05/21/2022 23:38:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.018514 on epoch=30
05/21/2022 23:38:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.011096 on epoch=31
05/21/2022 23:38:32 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 23:38:32 - INFO - __main__ - Printing 3 examples
05/21/2022 23:38:32 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 23:38:32 - INFO - __main__ - ['sad']
05/21/2022 23:38:32 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 23:38:32 - INFO - __main__ - ['sad']
05/21/2022 23:38:32 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 23:38:32 - INFO - __main__ - ['sad']
05/21/2022 23:38:32 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:38:32 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:38:33 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 23:38:33 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 23:38:33 - INFO - __main__ - Printing 3 examples
05/21/2022 23:38:33 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/21/2022 23:38:33 - INFO - __main__ - ['sad']
05/21/2022 23:38:33 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/21/2022 23:38:33 - INFO - __main__ - ['sad']
05/21/2022 23:38:33 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/21/2022 23:38:33 - INFO - __main__ - ['sad']
05/21/2022 23:38:33 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:38:33 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:38:33 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 23:38:35 - INFO - __main__ - Global step 1000 Train loss 0.031329 Classification-F1 0.7483454964656067 on epoch=31
05/21/2022 23:38:35 - INFO - __main__ - save last model!
05/21/2022 23:38:42 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 23:38:43 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 23:38:43 - INFO - __main__ - Printing 3 examples
05/21/2022 23:38:43 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 23:38:43 - INFO - __main__ - ['others']
05/21/2022 23:38:43 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 23:38:43 - INFO - __main__ - ['others']
05/21/2022 23:38:43 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 23:38:43 - INFO - __main__ - ['others']
05/21/2022 23:38:43 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:38:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 23:38:44 - INFO - __main__ - Starting training!
05/21/2022 23:38:45 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:38:50 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 23:39:34 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_21_0.0005_8_predictions.txt
05/21/2022 23:39:34 - INFO - __main__ - Classification-F1 on test data: 0.1558
05/21/2022 23:39:34 - INFO - __main__ - prefix=emo_128_21, lr=0.0005, bsz=8, dev_performance=0.7506320740283903, test_performance=0.15576180057798986
05/21/2022 23:39:34 - INFO - __main__ - Running ... prefix=emo_128_21, lr=0.0003, bsz=8 ...
05/21/2022 23:39:35 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 23:39:35 - INFO - __main__ - Printing 3 examples
05/21/2022 23:39:35 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 23:39:35 - INFO - __main__ - ['sad']
05/21/2022 23:39:35 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 23:39:35 - INFO - __main__ - ['sad']
05/21/2022 23:39:35 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 23:39:35 - INFO - __main__ - ['sad']
05/21/2022 23:39:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:39:36 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:39:36 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 23:39:36 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 23:39:36 - INFO - __main__ - Printing 3 examples
05/21/2022 23:39:36 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/21/2022 23:39:36 - INFO - __main__ - ['sad']
05/21/2022 23:39:36 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/21/2022 23:39:36 - INFO - __main__ - ['sad']
05/21/2022 23:39:36 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/21/2022 23:39:36 - INFO - __main__ - ['sad']
05/21/2022 23:39:36 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:39:36 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:39:37 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 23:39:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 23:39:48 - INFO - __main__ - Starting training!
05/21/2022 23:39:52 - INFO - __main__ - Step 10 Global step 10 Train loss 24.727823 on epoch=0
05/21/2022 23:39:56 - INFO - __main__ - Step 20 Global step 20 Train loss 20.241518 on epoch=0
05/21/2022 23:40:01 - INFO - __main__ - Step 30 Global step 30 Train loss 18.412943 on epoch=0
05/21/2022 23:40:06 - INFO - __main__ - Step 40 Global step 40 Train loss 17.163912 on epoch=1
05/21/2022 23:40:11 - INFO - __main__ - Step 50 Global step 50 Train loss 16.622715 on epoch=1
05/21/2022 23:42:00 - INFO - __main__ - Global step 50 Train loss 19.433783 Classification-F1 0.0 on epoch=1
05/21/2022 23:42:06 - INFO - __main__ - Step 60 Global step 60 Train loss 15.104860 on epoch=1
05/21/2022 23:42:10 - INFO - __main__ - Step 70 Global step 70 Train loss 14.096251 on epoch=2
05/21/2022 23:42:15 - INFO - __main__ - Step 80 Global step 80 Train loss 13.086604 on epoch=2
05/21/2022 23:42:20 - INFO - __main__ - Step 90 Global step 90 Train loss 12.592961 on epoch=2
05/21/2022 23:42:25 - INFO - __main__ - Step 100 Global step 100 Train loss 11.995742 on epoch=3
05/21/2022 23:42:32 - INFO - __main__ - Global step 100 Train loss 13.375283 Classification-F1 0.0 on epoch=3
05/21/2022 23:42:36 - INFO - __main__ - Step 110 Global step 110 Train loss 10.808945 on epoch=3
05/21/2022 23:42:41 - INFO - __main__ - Step 120 Global step 120 Train loss 9.833651 on epoch=3
05/21/2022 23:42:46 - INFO - __main__ - Step 130 Global step 130 Train loss 6.832783 on epoch=4
05/21/2022 23:42:51 - INFO - __main__ - Step 140 Global step 140 Train loss 4.335555 on epoch=4
05/21/2022 23:42:56 - INFO - __main__ - Step 150 Global step 150 Train loss 5.258470 on epoch=4
05/21/2022 23:44:18 - INFO - __main__ - Global step 150 Train loss 7.413881 Classification-F1 0.005007278325618134 on epoch=4
05/21/2022 23:44:24 - INFO - __main__ - Step 160 Global step 160 Train loss 3.242772 on epoch=4
05/21/2022 23:44:29 - INFO - __main__ - Step 170 Global step 170 Train loss 2.334171 on epoch=5
05/21/2022 23:44:34 - INFO - __main__ - Step 180 Global step 180 Train loss 1.091312 on epoch=5
05/21/2022 23:44:39 - INFO - __main__ - Step 190 Global step 190 Train loss 1.028998 on epoch=5
05/21/2022 23:44:44 - INFO - __main__ - Step 200 Global step 200 Train loss 1.083449 on epoch=6
05/21/2022 23:44:47 - INFO - __main__ - Global step 200 Train loss 1.756140 Classification-F1 0.20710831821928777 on epoch=6
05/21/2022 23:44:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.841585 on epoch=6
05/21/2022 23:44:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.982740 on epoch=6
05/21/2022 23:45:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.775881 on epoch=7
05/21/2022 23:45:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.846647 on epoch=7
05/21/2022 23:45:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.890868 on epoch=7
05/21/2022 23:45:17 - INFO - __main__ - Global step 250 Train loss 0.867544 Classification-F1 0.1257936507936508 on epoch=7
05/21/2022 23:45:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.906787 on epoch=8
05/21/2022 23:45:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.874310 on epoch=8
05/21/2022 23:45:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.859798 on epoch=8
05/21/2022 23:45:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.861386 on epoch=9
05/21/2022 23:45:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.847456 on epoch=9
05/21/2022 23:45:46 - INFO - __main__ - Global step 300 Train loss 0.869947 Classification-F1 0.1306836540196946 on epoch=9
05/21/2022 23:45:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.946100 on epoch=9
05/21/2022 23:45:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.834931 on epoch=9
05/21/2022 23:46:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.873268 on epoch=10
05/21/2022 23:46:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.779290 on epoch=10
05/21/2022 23:46:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.886271 on epoch=10
05/21/2022 23:46:15 - INFO - __main__ - Global step 350 Train loss 0.863972 Classification-F1 0.11680174426287274 on epoch=10
05/21/2022 23:46:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.771792 on epoch=11
05/21/2022 23:46:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.832823 on epoch=11
05/21/2022 23:46:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.829713 on epoch=11
05/21/2022 23:46:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.788206 on epoch=12
05/21/2022 23:46:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.807809 on epoch=12
05/21/2022 23:46:44 - INFO - __main__ - Global step 400 Train loss 0.806069 Classification-F1 0.12381881835362642 on epoch=12
05/21/2022 23:46:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.835628 on epoch=12
05/21/2022 23:46:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.791372 on epoch=13
05/21/2022 23:46:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.753676 on epoch=13
05/21/2022 23:47:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.827537 on epoch=13
05/21/2022 23:47:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.812943 on epoch=14
05/21/2022 23:47:13 - INFO - __main__ - Global step 450 Train loss 0.804231 Classification-F1 0.10015649452269171 on epoch=14
05/21/2022 23:47:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.800858 on epoch=14
05/21/2022 23:47:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.791558 on epoch=14
05/21/2022 23:47:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.719970 on epoch=14
05/21/2022 23:47:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.816398 on epoch=15
05/21/2022 23:47:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.799714 on epoch=15
05/21/2022 23:47:41 - INFO - __main__ - Global step 500 Train loss 0.785700 Classification-F1 0.16399684873949583 on epoch=15
05/21/2022 23:47:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.740678 on epoch=15
05/21/2022 23:47:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.729085 on epoch=16
05/21/2022 23:47:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.895989 on epoch=16
05/21/2022 23:48:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.794773 on epoch=16
05/21/2022 23:48:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.753920 on epoch=17
05/21/2022 23:48:11 - INFO - __main__ - Global step 550 Train loss 0.782889 Classification-F1 0.20577873319339934 on epoch=17
05/21/2022 23:48:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.788919 on epoch=17
05/21/2022 23:48:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.775208 on epoch=17
05/21/2022 23:48:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.763361 on epoch=18
05/21/2022 23:48:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.721683 on epoch=18
05/21/2022 23:48:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.824013 on epoch=18
05/21/2022 23:48:41 - INFO - __main__ - Global step 600 Train loss 0.774637 Classification-F1 0.1266439909297052 on epoch=18
05/21/2022 23:48:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.781372 on epoch=19
05/21/2022 23:48:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.709228 on epoch=19
05/21/2022 23:48:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.801766 on epoch=19
05/21/2022 23:49:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.753963 on epoch=19
05/21/2022 23:49:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.775046 on epoch=20
05/21/2022 23:49:10 - INFO - __main__ - Global step 650 Train loss 0.764275 Classification-F1 0.13089569160997733 on epoch=20
05/21/2022 23:49:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.759927 on epoch=20
05/21/2022 23:49:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.832660 on epoch=20
05/21/2022 23:49:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.759632 on epoch=21
05/21/2022 23:49:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.833537 on epoch=21
05/21/2022 23:49:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.789846 on epoch=21
05/21/2022 23:49:40 - INFO - __main__ - Global step 700 Train loss 0.795121 Classification-F1 0.18272005772005773 on epoch=21
05/21/2022 23:49:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.777424 on epoch=22
05/21/2022 23:49:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.763775 on epoch=22
05/21/2022 23:49:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.784698 on epoch=22
05/21/2022 23:50:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.772712 on epoch=23
05/21/2022 23:50:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.762680 on epoch=23
05/21/2022 23:50:10 - INFO - __main__ - Global step 750 Train loss 0.772258 Classification-F1 0.13279638450286915 on epoch=23
05/21/2022 23:50:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.813927 on epoch=23
05/21/2022 23:50:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.704543 on epoch=24
05/21/2022 23:50:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.708568 on epoch=24
05/21/2022 23:50:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.788470 on epoch=24
05/21/2022 23:50:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.751586 on epoch=24
05/21/2022 23:50:40 - INFO - __main__ - Global step 800 Train loss 0.753419 Classification-F1 0.17097342389659848 on epoch=24
05/21/2022 23:50:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.781330 on epoch=25
05/21/2022 23:50:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.932563 on epoch=25
05/21/2022 23:50:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.742148 on epoch=25
05/21/2022 23:51:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.689770 on epoch=26
05/21/2022 23:51:06 - INFO - __main__ - Step 850 Global step 850 Train loss 1.141397 on epoch=26
05/21/2022 23:51:12 - INFO - __main__ - Global step 850 Train loss 0.857442 Classification-F1 0.36943911107479876 on epoch=26
05/21/2022 23:51:18 - INFO - __main__ - Step 860 Global step 860 Train loss 1.114044 on epoch=26
05/21/2022 23:51:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.733616 on epoch=27
05/21/2022 23:51:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.695183 on epoch=27
05/21/2022 23:51:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.754540 on epoch=27
05/21/2022 23:51:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.680494 on epoch=28
05/21/2022 23:51:43 - INFO - __main__ - Global step 900 Train loss 0.795575 Classification-F1 0.32307989585875846 on epoch=28
05/21/2022 23:51:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.702176 on epoch=28
05/21/2022 23:51:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.819265 on epoch=28
05/21/2022 23:51:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.686635 on epoch=29
05/21/2022 23:52:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.718043 on epoch=29
05/21/2022 23:52:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.665780 on epoch=29
05/21/2022 23:52:13 - INFO - __main__ - Global step 950 Train loss 0.718380 Classification-F1 0.36479006124348334 on epoch=29
05/21/2022 23:52:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.646710 on epoch=29
05/21/2022 23:52:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.694807 on epoch=30
05/21/2022 23:52:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.686633 on epoch=30
05/21/2022 23:52:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.746707 on epoch=30
05/21/2022 23:52:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.665496 on epoch=31
05/21/2022 23:52:40 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 23:52:40 - INFO - __main__ - Printing 3 examples
05/21/2022 23:52:40 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 23:52:40 - INFO - __main__ - ['sad']
05/21/2022 23:52:40 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 23:52:40 - INFO - __main__ - ['sad']
05/21/2022 23:52:40 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 23:52:40 - INFO - __main__ - ['sad']
05/21/2022 23:52:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:52:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:52:40 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 23:52:40 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 23:52:40 - INFO - __main__ - Printing 3 examples
05/21/2022 23:52:40 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/21/2022 23:52:40 - INFO - __main__ - ['sad']
05/21/2022 23:52:40 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/21/2022 23:52:40 - INFO - __main__ - ['sad']
05/21/2022 23:52:40 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/21/2022 23:52:40 - INFO - __main__ - ['sad']
05/21/2022 23:52:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:52:41 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:52:41 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 23:52:43 - INFO - __main__ - Global step 1000 Train loss 0.688071 Classification-F1 0.3722794872742048 on epoch=31
05/21/2022 23:52:43 - INFO - __main__ - save last model!
05/21/2022 23:52:50 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 23:52:51 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 23:52:51 - INFO - __main__ - Printing 3 examples
05/21/2022 23:52:51 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 23:52:51 - INFO - __main__ - ['others']
05/21/2022 23:52:51 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 23:52:51 - INFO - __main__ - ['others']
05/21/2022 23:52:51 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 23:52:51 - INFO - __main__ - ['others']
05/21/2022 23:52:51 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:52:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 23:52:52 - INFO - __main__ - Starting training!
05/21/2022 23:52:53 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:52:58 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 23:53:42 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_21_0.0003_8_predictions.txt
05/21/2022 23:53:42 - INFO - __main__ - Classification-F1 on test data: 0.1679
05/21/2022 23:53:42 - INFO - __main__ - prefix=emo_128_21, lr=0.0003, bsz=8, dev_performance=0.3722794872742048, test_performance=0.16789044546482806
05/21/2022 23:53:42 - INFO - __main__ - Running ... prefix=emo_128_21, lr=0.0002, bsz=8 ...
05/21/2022 23:53:43 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 23:53:43 - INFO - __main__ - Printing 3 examples
05/21/2022 23:53:43 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 23:53:43 - INFO - __main__ - ['sad']
05/21/2022 23:53:43 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 23:53:43 - INFO - __main__ - ['sad']
05/21/2022 23:53:43 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 23:53:43 - INFO - __main__ - ['sad']
05/21/2022 23:53:43 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:53:43 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:53:44 - INFO - __main__ - Loaded 512 examples from train data
05/21/2022 23:53:44 - INFO - __main__ - Start tokenizing ... 512 instances
05/21/2022 23:53:44 - INFO - __main__ - Printing 3 examples
05/21/2022 23:53:44 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/21/2022 23:53:44 - INFO - __main__ - ['sad']
05/21/2022 23:53:44 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/21/2022 23:53:44 - INFO - __main__ - ['sad']
05/21/2022 23:53:44 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/21/2022 23:53:44 - INFO - __main__ - ['sad']
05/21/2022 23:53:44 - INFO - __main__ - Tokenizing Input ...
05/21/2022 23:53:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 23:53:44 - INFO - __main__ - Loaded 512 examples from dev data
05/21/2022 23:53:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/21/2022 23:53:56 - INFO - __main__ - Starting training!
05/21/2022 23:54:00 - INFO - __main__ - Step 10 Global step 10 Train loss 25.361210 on epoch=0
05/21/2022 23:54:05 - INFO - __main__ - Step 20 Global step 20 Train loss 21.538515 on epoch=0
05/21/2022 23:54:10 - INFO - __main__ - Step 30 Global step 30 Train loss 17.587664 on epoch=0
05/21/2022 23:54:15 - INFO - __main__ - Step 40 Global step 40 Train loss 16.661236 on epoch=1
05/21/2022 23:54:20 - INFO - __main__ - Step 50 Global step 50 Train loss 16.268211 on epoch=1
05/21/2022 23:55:24 - INFO - __main__ - Global step 50 Train loss 19.483366 Classification-F1 8.380473496752567e-05 on epoch=1
05/21/2022 23:55:29 - INFO - __main__ - Step 60 Global step 60 Train loss 16.142595 on epoch=1
05/21/2022 23:55:35 - INFO - __main__ - Step 70 Global step 70 Train loss 15.798132 on epoch=2
05/21/2022 23:55:40 - INFO - __main__ - Step 80 Global step 80 Train loss 14.876818 on epoch=2
05/21/2022 23:55:45 - INFO - __main__ - Step 90 Global step 90 Train loss 14.154100 on epoch=2
05/21/2022 23:55:50 - INFO - __main__ - Step 100 Global step 100 Train loss 13.556038 on epoch=3
05/21/2022 23:55:55 - INFO - __main__ - Global step 100 Train loss 14.905537 Classification-F1 0.00026277755879647877 on epoch=3
05/21/2022 23:56:01 - INFO - __main__ - Step 110 Global step 110 Train loss 13.389671 on epoch=3
05/21/2022 23:56:06 - INFO - __main__ - Step 120 Global step 120 Train loss 11.569730 on epoch=3
05/21/2022 23:56:11 - INFO - __main__ - Step 130 Global step 130 Train loss 11.930063 on epoch=4
05/21/2022 23:56:16 - INFO - __main__ - Step 140 Global step 140 Train loss 10.983291 on epoch=4
05/21/2022 23:56:21 - INFO - __main__ - Step 150 Global step 150 Train loss 9.474226 on epoch=4
05/21/2022 23:56:26 - INFO - __main__ - Global step 150 Train loss 11.469397 Classification-F1 0.000971208604796391 on epoch=4
05/21/2022 23:56:32 - INFO - __main__ - Step 160 Global step 160 Train loss 9.391790 on epoch=4
05/21/2022 23:56:37 - INFO - __main__ - Step 170 Global step 170 Train loss 6.530920 on epoch=5
05/21/2022 23:56:42 - INFO - __main__ - Step 180 Global step 180 Train loss 4.973120 on epoch=5
05/21/2022 23:56:47 - INFO - __main__ - Step 190 Global step 190 Train loss 4.424132 on epoch=5
05/21/2022 23:56:52 - INFO - __main__ - Step 200 Global step 200 Train loss 3.238419 on epoch=6
05/21/2022 23:56:57 - INFO - __main__ - Global step 200 Train loss 5.711677 Classification-F1 0.29432339426553505 on epoch=6
05/21/2022 23:57:02 - INFO - __main__ - Step 210 Global step 210 Train loss 1.619554 on epoch=6
05/21/2022 23:57:07 - INFO - __main__ - Step 220 Global step 220 Train loss 1.732387 on epoch=6
05/21/2022 23:57:13 - INFO - __main__ - Step 230 Global step 230 Train loss 2.359808 on epoch=7
05/21/2022 23:57:18 - INFO - __main__ - Step 240 Global step 240 Train loss 1.773291 on epoch=7
05/21/2022 23:57:23 - INFO - __main__ - Step 250 Global step 250 Train loss 1.540317 on epoch=7
05/21/2022 23:57:27 - INFO - __main__ - Global step 250 Train loss 1.805071 Classification-F1 0.5108825055109305 on epoch=7
05/21/2022 23:57:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.787337 on epoch=8
05/21/2022 23:57:38 - INFO - __main__ - Step 270 Global step 270 Train loss 1.194254 on epoch=8
05/21/2022 23:57:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.753169 on epoch=8
05/21/2022 23:57:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.943040 on epoch=9
05/21/2022 23:57:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.648642 on epoch=9
05/21/2022 23:57:57 - INFO - __main__ - Global step 300 Train loss 0.865289 Classification-F1 0.6979114569582086 on epoch=9
05/21/2022 23:58:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.556380 on epoch=9
05/21/2022 23:58:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.578454 on epoch=9
05/21/2022 23:58:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.623715 on epoch=10
05/21/2022 23:58:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.436984 on epoch=10
05/21/2022 23:58:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.569642 on epoch=10
05/21/2022 23:58:27 - INFO - __main__ - Global step 350 Train loss 0.553035 Classification-F1 0.720036914407374 on epoch=10
05/21/2022 23:58:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.524252 on epoch=11
05/21/2022 23:58:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.448691 on epoch=11
05/21/2022 23:58:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.518072 on epoch=11
05/21/2022 23:58:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.545156 on epoch=12
05/21/2022 23:58:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.367630 on epoch=12
05/21/2022 23:58:57 - INFO - __main__ - Global step 400 Train loss 0.480760 Classification-F1 0.7385285557973122 on epoch=12
05/21/2022 23:59:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.488133 on epoch=12
05/21/2022 23:59:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.463046 on epoch=13
05/21/2022 23:59:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.333697 on epoch=13
05/21/2022 23:59:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.378857 on epoch=13
05/21/2022 23:59:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.426694 on epoch=14
05/21/2022 23:59:26 - INFO - __main__ - Global step 450 Train loss 0.418085 Classification-F1 0.7855730958713426 on epoch=14
05/21/2022 23:59:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.365780 on epoch=14
05/21/2022 23:59:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.293862 on epoch=14
05/21/2022 23:59:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.332311 on epoch=14
05/21/2022 23:59:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.352927 on epoch=15
05/21/2022 23:59:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.313917 on epoch=15
05/21/2022 23:59:56 - INFO - __main__ - Global step 500 Train loss 0.331760 Classification-F1 0.7704398777505613 on epoch=15
05/22/2022 00:00:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.350757 on epoch=15
05/22/2022 00:00:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.338349 on epoch=16
05/22/2022 00:00:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.187196 on epoch=16
05/22/2022 00:00:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.281752 on epoch=16
05/22/2022 00:00:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.379617 on epoch=17
05/22/2022 00:00:25 - INFO - __main__ - Global step 550 Train loss 0.307534 Classification-F1 0.7851967843144864 on epoch=17
05/22/2022 00:00:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.257231 on epoch=17
05/22/2022 00:00:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.274783 on epoch=17
05/22/2022 00:00:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.290430 on epoch=18
05/22/2022 00:00:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.297080 on epoch=18
05/22/2022 00:00:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.302772 on epoch=18
05/22/2022 00:00:54 - INFO - __main__ - Global step 600 Train loss 0.284459 Classification-F1 0.7910389673294975 on epoch=18
05/22/2022 00:01:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.275779 on epoch=19
05/22/2022 00:01:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.270820 on epoch=19
05/22/2022 00:01:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.214050 on epoch=19
05/22/2022 00:01:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.164197 on epoch=19
05/22/2022 00:01:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.224172 on epoch=20
05/22/2022 00:01:24 - INFO - __main__ - Global step 650 Train loss 0.229804 Classification-F1 0.5293338539455743 on epoch=20
05/22/2022 00:01:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.125014 on epoch=20
05/22/2022 00:01:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.171813 on epoch=20
05/22/2022 00:01:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.176803 on epoch=21
05/22/2022 00:01:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.186028 on epoch=21
05/22/2022 00:01:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.193872 on epoch=21
05/22/2022 00:01:54 - INFO - __main__ - Global step 700 Train loss 0.170706 Classification-F1 0.612628261013971 on epoch=21
05/22/2022 00:01:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.257993 on epoch=22
05/22/2022 00:02:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.150903 on epoch=22
05/22/2022 00:02:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.150248 on epoch=22
05/22/2022 00:02:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.240141 on epoch=23
05/22/2022 00:02:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.097523 on epoch=23
05/22/2022 00:02:23 - INFO - __main__ - Global step 750 Train loss 0.179362 Classification-F1 0.7607349256501698 on epoch=23
05/22/2022 00:02:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.322834 on epoch=23
05/22/2022 00:02:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.166985 on epoch=24
05/22/2022 00:02:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.154523 on epoch=24
05/22/2022 00:02:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.123884 on epoch=24
05/22/2022 00:02:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.096647 on epoch=24
05/22/2022 00:02:52 - INFO - __main__ - Global step 800 Train loss 0.172975 Classification-F1 0.6386967579148217 on epoch=24
05/22/2022 00:02:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.106451 on epoch=25
05/22/2022 00:03:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.094061 on epoch=25
05/22/2022 00:03:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.078646 on epoch=25
05/22/2022 00:03:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.120171 on epoch=26
05/22/2022 00:03:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.140873 on epoch=26
05/22/2022 00:03:21 - INFO - __main__ - Global step 850 Train loss 0.108041 Classification-F1 0.7753954476942448 on epoch=26
05/22/2022 00:03:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.108481 on epoch=26
05/22/2022 00:03:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.093536 on epoch=27
05/22/2022 00:03:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.050089 on epoch=27
05/22/2022 00:03:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.103594 on epoch=27
05/22/2022 00:03:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.055550 on epoch=28
05/22/2022 00:03:50 - INFO - __main__ - Global step 900 Train loss 0.082250 Classification-F1 0.8066408506505125 on epoch=28
05/22/2022 00:03:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.073066 on epoch=28
05/22/2022 00:04:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.096279 on epoch=28
05/22/2022 00:04:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.079753 on epoch=29
05/22/2022 00:04:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.042944 on epoch=29
05/22/2022 00:04:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.035242 on epoch=29
05/22/2022 00:04:21 - INFO - __main__ - Global step 950 Train loss 0.065457 Classification-F1 0.6404697910793832 on epoch=29
05/22/2022 00:04:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.065789 on epoch=29
05/22/2022 00:04:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.065375 on epoch=30
05/22/2022 00:04:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.064811 on epoch=30
05/22/2022 00:04:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.050485 on epoch=30
05/22/2022 00:04:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.084374 on epoch=31
05/22/2022 00:04:47 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:04:47 - INFO - __main__ - Printing 3 examples
05/22/2022 00:04:47 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/22/2022 00:04:47 - INFO - __main__ - ['sad']
05/22/2022 00:04:47 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/22/2022 00:04:47 - INFO - __main__ - ['sad']
05/22/2022 00:04:47 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/22/2022 00:04:47 - INFO - __main__ - ['sad']
05/22/2022 00:04:47 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:04:47 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:04:48 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 00:04:48 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:04:48 - INFO - __main__ - Printing 3 examples
05/22/2022 00:04:48 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/22/2022 00:04:48 - INFO - __main__ - ['sad']
05/22/2022 00:04:48 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/22/2022 00:04:48 - INFO - __main__ - ['sad']
05/22/2022 00:04:48 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/22/2022 00:04:48 - INFO - __main__ - ['sad']
05/22/2022 00:04:48 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:04:48 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:04:49 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 00:04:50 - INFO - __main__ - Global step 1000 Train loss 0.066167 Classification-F1 0.45220969844513265 on epoch=31
05/22/2022 00:04:50 - INFO - __main__ - save last model!
05/22/2022 00:04:57 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 00:04:58 - INFO - __main__ - Start tokenizing ... 5509 instances
05/22/2022 00:04:58 - INFO - __main__ - Printing 3 examples
05/22/2022 00:04:58 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/22/2022 00:04:58 - INFO - __main__ - ['others']
05/22/2022 00:04:58 - INFO - __main__ -  [emo] what you like very little things ok
05/22/2022 00:04:58 - INFO - __main__ - ['others']
05/22/2022 00:04:58 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/22/2022 00:04:58 - INFO - __main__ - ['others']
05/22/2022 00:04:58 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:04:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 00:04:59 - INFO - __main__ - Starting training!
05/22/2022 00:05:00 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:05:05 - INFO - __main__ - Loaded 5509 examples from test data
05/22/2022 00:05:53 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_21_0.0002_8_predictions.txt
05/22/2022 00:05:53 - INFO - __main__ - Classification-F1 on test data: 0.1495
05/22/2022 00:05:53 - INFO - __main__ - prefix=emo_128_21, lr=0.0002, bsz=8, dev_performance=0.8066408506505125, test_performance=0.14945858286078767
05/22/2022 00:05:53 - INFO - __main__ - Running ... prefix=emo_128_21, lr=0.0001, bsz=8 ...
05/22/2022 00:05:54 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:05:54 - INFO - __main__ - Printing 3 examples
05/22/2022 00:05:54 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/22/2022 00:05:54 - INFO - __main__ - ['sad']
05/22/2022 00:05:54 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/22/2022 00:05:54 - INFO - __main__ - ['sad']
05/22/2022 00:05:54 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/22/2022 00:05:54 - INFO - __main__ - ['sad']
05/22/2022 00:05:54 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:05:54 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:05:55 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 00:05:55 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:05:55 - INFO - __main__ - Printing 3 examples
05/22/2022 00:05:55 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/22/2022 00:05:55 - INFO - __main__ - ['sad']
05/22/2022 00:05:55 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/22/2022 00:05:55 - INFO - __main__ - ['sad']
05/22/2022 00:05:55 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/22/2022 00:05:55 - INFO - __main__ - ['sad']
05/22/2022 00:05:55 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:05:55 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:05:56 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 00:06:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 00:06:08 - INFO - __main__ - Starting training!
05/22/2022 00:06:12 - INFO - __main__ - Step 10 Global step 10 Train loss 25.690268 on epoch=0
05/22/2022 00:06:17 - INFO - __main__ - Step 20 Global step 20 Train loss 21.448040 on epoch=0
05/22/2022 00:06:22 - INFO - __main__ - Step 30 Global step 30 Train loss 20.057657 on epoch=0
05/22/2022 00:06:27 - INFO - __main__ - Step 40 Global step 40 Train loss 17.248011 on epoch=1
05/22/2022 00:06:32 - INFO - __main__ - Step 50 Global step 50 Train loss 17.815029 on epoch=1
05/22/2022 00:06:39 - INFO - __main__ - Global step 50 Train loss 20.451801 Classification-F1 0.00013481631277384565 on epoch=1
05/22/2022 00:06:44 - INFO - __main__ - Step 60 Global step 60 Train loss 17.622005 on epoch=1
05/22/2022 00:06:49 - INFO - __main__ - Step 70 Global step 70 Train loss 17.263006 on epoch=2
05/22/2022 00:06:54 - INFO - __main__ - Step 80 Global step 80 Train loss 17.359943 on epoch=2
05/22/2022 00:07:00 - INFO - __main__ - Step 90 Global step 90 Train loss 16.982412 on epoch=2
05/22/2022 00:07:05 - INFO - __main__ - Step 100 Global step 100 Train loss 16.042454 on epoch=3
05/22/2022 00:07:11 - INFO - __main__ - Global step 100 Train loss 17.053965 Classification-F1 0.00040949651980728334 on epoch=3
05/22/2022 00:07:17 - INFO - __main__ - Step 110 Global step 110 Train loss 15.430850 on epoch=3
05/22/2022 00:07:22 - INFO - __main__ - Step 120 Global step 120 Train loss 14.876930 on epoch=3
05/22/2022 00:07:27 - INFO - __main__ - Step 130 Global step 130 Train loss 14.720596 on epoch=4
05/22/2022 00:07:32 - INFO - __main__ - Step 140 Global step 140 Train loss 14.938202 on epoch=4
05/22/2022 00:07:37 - INFO - __main__ - Step 150 Global step 150 Train loss 13.908139 on epoch=4
05/22/2022 00:07:44 - INFO - __main__ - Global step 150 Train loss 14.774944 Classification-F1 0.0002871088142405972 on epoch=4
05/22/2022 00:07:49 - INFO - __main__ - Step 160 Global step 160 Train loss 13.520205 on epoch=4
05/22/2022 00:07:54 - INFO - __main__ - Step 170 Global step 170 Train loss 13.949768 on epoch=5
05/22/2022 00:07:59 - INFO - __main__ - Step 180 Global step 180 Train loss 14.091223 on epoch=5
05/22/2022 00:08:04 - INFO - __main__ - Step 190 Global step 190 Train loss 12.662638 on epoch=5
05/22/2022 00:08:09 - INFO - __main__ - Step 200 Global step 200 Train loss 12.511797 on epoch=6
05/22/2022 00:08:15 - INFO - __main__ - Global step 200 Train loss 13.347126 Classification-F1 0.0011329380959491763 on epoch=6
05/22/2022 00:08:21 - INFO - __main__ - Step 210 Global step 210 Train loss 12.248906 on epoch=6
05/22/2022 00:08:26 - INFO - __main__ - Step 220 Global step 220 Train loss 11.668173 on epoch=6
05/22/2022 00:08:31 - INFO - __main__ - Step 230 Global step 230 Train loss 11.608168 on epoch=7
05/22/2022 00:08:36 - INFO - __main__ - Step 240 Global step 240 Train loss 11.528178 on epoch=7
05/22/2022 00:08:41 - INFO - __main__ - Step 250 Global step 250 Train loss 10.744414 on epoch=7
05/22/2022 00:08:48 - INFO - __main__ - Global step 250 Train loss 11.559567 Classification-F1 0.006724332908964103 on epoch=7
05/22/2022 00:08:54 - INFO - __main__ - Step 260 Global step 260 Train loss 10.582169 on epoch=8
05/22/2022 00:08:59 - INFO - __main__ - Step 270 Global step 270 Train loss 9.750509 on epoch=8
05/22/2022 00:09:04 - INFO - __main__ - Step 280 Global step 280 Train loss 7.086999 on epoch=8
05/22/2022 00:09:09 - INFO - __main__ - Step 290 Global step 290 Train loss 5.089055 on epoch=9
05/22/2022 00:09:14 - INFO - __main__ - Step 300 Global step 300 Train loss 2.092332 on epoch=9
05/22/2022 00:09:18 - INFO - __main__ - Global step 300 Train loss 6.920213 Classification-F1 0.43783896467405203 on epoch=9
05/22/2022 00:09:24 - INFO - __main__ - Step 310 Global step 310 Train loss 1.419424 on epoch=9
05/22/2022 00:09:29 - INFO - __main__ - Step 320 Global step 320 Train loss 1.417966 on epoch=9
05/22/2022 00:09:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.834307 on epoch=10
05/22/2022 00:09:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.792713 on epoch=10
05/22/2022 00:09:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.851107 on epoch=10
05/22/2022 00:09:48 - INFO - __main__ - Global step 350 Train loss 1.063103 Classification-F1 0.5594418223848969 on epoch=10
05/22/2022 00:09:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.678186 on epoch=11
05/22/2022 00:09:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.642187 on epoch=11
05/22/2022 00:10:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.639949 on epoch=11
05/22/2022 00:10:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.642886 on epoch=12
05/22/2022 00:10:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.550076 on epoch=12
05/22/2022 00:10:18 - INFO - __main__ - Global step 400 Train loss 0.630657 Classification-F1 0.5778255809164715 on epoch=12
05/22/2022 00:10:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.592901 on epoch=12
05/22/2022 00:10:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.624325 on epoch=13
05/22/2022 00:10:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.517583 on epoch=13
05/22/2022 00:10:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.514830 on epoch=13
05/22/2022 00:10:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.501047 on epoch=14
05/22/2022 00:10:48 - INFO - __main__ - Global step 450 Train loss 0.550137 Classification-F1 0.7507607662828393 on epoch=14
05/22/2022 00:10:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.560521 on epoch=14
05/22/2022 00:10:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.477170 on epoch=14
05/22/2022 00:11:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.475151 on epoch=14
05/22/2022 00:11:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.585578 on epoch=15
05/22/2022 00:11:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.409625 on epoch=15
05/22/2022 00:11:18 - INFO - __main__ - Global step 500 Train loss 0.501609 Classification-F1 0.7562285370901979 on epoch=15
05/22/2022 00:11:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.454832 on epoch=15
05/22/2022 00:11:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.500219 on epoch=16
05/22/2022 00:11:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.348615 on epoch=16
05/22/2022 00:11:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.363820 on epoch=16
05/22/2022 00:11:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.468460 on epoch=17
05/22/2022 00:11:48 - INFO - __main__ - Global step 550 Train loss 0.427189 Classification-F1 0.7509019108838615 on epoch=17
05/22/2022 00:11:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.363942 on epoch=17
05/22/2022 00:11:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.389549 on epoch=17
05/22/2022 00:12:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.448511 on epoch=18
05/22/2022 00:12:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.326166 on epoch=18
05/22/2022 00:12:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.391519 on epoch=18
05/22/2022 00:12:18 - INFO - __main__ - Global step 600 Train loss 0.383937 Classification-F1 0.7585799303730439 on epoch=18
05/22/2022 00:12:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.381230 on epoch=19
05/22/2022 00:12:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.341977 on epoch=19
05/22/2022 00:12:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.314530 on epoch=19
05/22/2022 00:12:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.371595 on epoch=19
05/22/2022 00:12:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.336161 on epoch=20
05/22/2022 00:12:48 - INFO - __main__ - Global step 650 Train loss 0.349099 Classification-F1 0.7803118834487529 on epoch=20
05/22/2022 00:12:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.287790 on epoch=20
05/22/2022 00:12:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.375923 on epoch=20
05/22/2022 00:13:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.435543 on epoch=21
05/22/2022 00:13:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.245853 on epoch=21
05/22/2022 00:13:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.346357 on epoch=21
05/22/2022 00:13:19 - INFO - __main__ - Global step 700 Train loss 0.338293 Classification-F1 0.7500890351084127 on epoch=21
05/22/2022 00:13:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.467025 on epoch=22
05/22/2022 00:13:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.165058 on epoch=22
05/22/2022 00:13:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.260495 on epoch=22
05/22/2022 00:13:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.322901 on epoch=23
05/22/2022 00:13:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.231965 on epoch=23
05/22/2022 00:13:48 - INFO - __main__ - Global step 750 Train loss 0.289489 Classification-F1 0.7996054802426501 on epoch=23
05/22/2022 00:13:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.295122 on epoch=23
05/22/2022 00:13:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.337774 on epoch=24
05/22/2022 00:14:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.242896 on epoch=24
05/22/2022 00:14:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.346722 on epoch=24
05/22/2022 00:14:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.257902 on epoch=24
05/22/2022 00:14:18 - INFO - __main__ - Global step 800 Train loss 0.296083 Classification-F1 0.6419586941862113 on epoch=24
05/22/2022 00:14:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.509600 on epoch=25
05/22/2022 00:14:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.193168 on epoch=25
05/22/2022 00:14:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.193640 on epoch=25
05/22/2022 00:14:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.266198 on epoch=26
05/22/2022 00:14:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.201131 on epoch=26
05/22/2022 00:14:48 - INFO - __main__ - Global step 850 Train loss 0.272747 Classification-F1 0.8025543255432555 on epoch=26
05/22/2022 00:14:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.180251 on epoch=26
05/22/2022 00:14:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.209095 on epoch=27
05/22/2022 00:15:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.166289 on epoch=27
05/22/2022 00:15:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.230611 on epoch=27
05/22/2022 00:15:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.267231 on epoch=28
05/22/2022 00:15:18 - INFO - __main__ - Global step 900 Train loss 0.210695 Classification-F1 0.6143551943551943 on epoch=28
05/22/2022 00:15:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.147373 on epoch=28
05/22/2022 00:15:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.196747 on epoch=28
05/22/2022 00:15:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.190481 on epoch=29
05/22/2022 00:15:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.178847 on epoch=29
05/22/2022 00:15:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.483909 on epoch=29
05/22/2022 00:15:48 - INFO - __main__ - Global step 950 Train loss 0.239471 Classification-F1 0.6420289353826386 on epoch=29
05/22/2022 00:15:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.135528 on epoch=29
05/22/2022 00:15:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.223265 on epoch=30
05/22/2022 00:16:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.104971 on epoch=30
05/22/2022 00:16:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.150290 on epoch=30
05/22/2022 00:16:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.235174 on epoch=31
05/22/2022 00:16:14 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:16:14 - INFO - __main__ - Printing 3 examples
05/22/2022 00:16:14 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/22/2022 00:16:14 - INFO - __main__ - ['happy']
05/22/2022 00:16:14 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/22/2022 00:16:14 - INFO - __main__ - ['happy']
05/22/2022 00:16:14 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/22/2022 00:16:14 - INFO - __main__ - ['happy']
05/22/2022 00:16:14 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:16:14 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:16:15 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 00:16:15 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:16:15 - INFO - __main__ - Printing 3 examples
05/22/2022 00:16:15 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/22/2022 00:16:15 - INFO - __main__ - ['happy']
05/22/2022 00:16:15 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/22/2022 00:16:15 - INFO - __main__ - ['happy']
05/22/2022 00:16:15 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/22/2022 00:16:15 - INFO - __main__ - ['happy']
05/22/2022 00:16:15 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:16:15 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:16:16 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 00:16:17 - INFO - __main__ - Global step 1000 Train loss 0.169846 Classification-F1 0.618484706653547 on epoch=31
05/22/2022 00:16:17 - INFO - __main__ - save last model!
05/22/2022 00:16:24 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 00:16:25 - INFO - __main__ - Start tokenizing ... 5509 instances
05/22/2022 00:16:25 - INFO - __main__ - Printing 3 examples
05/22/2022 00:16:25 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/22/2022 00:16:25 - INFO - __main__ - ['others']
05/22/2022 00:16:25 - INFO - __main__ -  [emo] what you like very little things ok
05/22/2022 00:16:25 - INFO - __main__ - ['others']
05/22/2022 00:16:25 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/22/2022 00:16:25 - INFO - __main__ - ['others']
05/22/2022 00:16:25 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:16:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 00:16:26 - INFO - __main__ - Starting training!
05/22/2022 00:16:27 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:16:32 - INFO - __main__ - Loaded 5509 examples from test data
05/22/2022 00:17:15 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_21_0.0001_8_predictions.txt
05/22/2022 00:17:15 - INFO - __main__ - Classification-F1 on test data: 0.1729
05/22/2022 00:17:15 - INFO - __main__ - prefix=emo_128_21, lr=0.0001, bsz=8, dev_performance=0.8025543255432555, test_performance=0.17294083086318437
05/22/2022 00:17:16 - INFO - __main__ - Running ... prefix=emo_128_42, lr=0.0005, bsz=8 ...
05/22/2022 00:17:16 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:17:16 - INFO - __main__ - Printing 3 examples
05/22/2022 00:17:16 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/22/2022 00:17:16 - INFO - __main__ - ['happy']
05/22/2022 00:17:16 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/22/2022 00:17:16 - INFO - __main__ - ['happy']
05/22/2022 00:17:16 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/22/2022 00:17:16 - INFO - __main__ - ['happy']
05/22/2022 00:17:16 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:17:17 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:17:17 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 00:17:17 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:17:17 - INFO - __main__ - Printing 3 examples
05/22/2022 00:17:17 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/22/2022 00:17:17 - INFO - __main__ - ['happy']
05/22/2022 00:17:17 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/22/2022 00:17:17 - INFO - __main__ - ['happy']
05/22/2022 00:17:17 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/22/2022 00:17:17 - INFO - __main__ - ['happy']
05/22/2022 00:17:17 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:17:17 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:17:18 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 00:17:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 00:17:29 - INFO - __main__ - Starting training!
05/22/2022 00:17:33 - INFO - __main__ - Step 10 Global step 10 Train loss 23.911030 on epoch=0
05/22/2022 00:17:38 - INFO - __main__ - Step 20 Global step 20 Train loss 19.977512 on epoch=0
05/22/2022 00:17:43 - INFO - __main__ - Step 30 Global step 30 Train loss 17.575329 on epoch=0
05/22/2022 00:17:48 - INFO - __main__ - Step 40 Global step 40 Train loss 15.999680 on epoch=1
05/22/2022 00:17:53 - INFO - __main__ - Step 50 Global step 50 Train loss 14.310347 on epoch=1
05/22/2022 00:17:58 - INFO - __main__ - Global step 50 Train loss 18.354782 Classification-F1 0.0 on epoch=1
05/22/2022 00:18:04 - INFO - __main__ - Step 60 Global step 60 Train loss 13.139590 on epoch=1
05/22/2022 00:18:08 - INFO - __main__ - Step 70 Global step 70 Train loss 11.055749 on epoch=2
05/22/2022 00:18:13 - INFO - __main__ - Step 80 Global step 80 Train loss 7.976469 on epoch=2
05/22/2022 00:18:18 - INFO - __main__ - Step 90 Global step 90 Train loss 5.841402 on epoch=2
05/22/2022 00:18:23 - INFO - __main__ - Step 100 Global step 100 Train loss 3.893291 on epoch=3
05/22/2022 00:18:27 - INFO - __main__ - Global step 100 Train loss 8.381301 Classification-F1 0.13378499106611078 on epoch=3
05/22/2022 00:18:33 - INFO - __main__ - Step 110 Global step 110 Train loss 3.008054 on epoch=3
05/22/2022 00:18:38 - INFO - __main__ - Step 120 Global step 120 Train loss 4.042281 on epoch=3
05/22/2022 00:18:43 - INFO - __main__ - Step 130 Global step 130 Train loss 3.414991 on epoch=4
05/22/2022 00:18:48 - INFO - __main__ - Step 140 Global step 140 Train loss 2.387817 on epoch=4
05/22/2022 00:18:53 - INFO - __main__ - Step 150 Global step 150 Train loss 3.026809 on epoch=4
05/22/2022 00:18:57 - INFO - __main__ - Global step 150 Train loss 3.175991 Classification-F1 0.1 on epoch=4
05/22/2022 00:19:02 - INFO - __main__ - Step 160 Global step 160 Train loss 2.735275 on epoch=4
05/22/2022 00:19:07 - INFO - __main__ - Step 170 Global step 170 Train loss 2.724348 on epoch=5
05/22/2022 00:19:12 - INFO - __main__ - Step 180 Global step 180 Train loss 2.510913 on epoch=5
05/22/2022 00:19:17 - INFO - __main__ - Step 190 Global step 190 Train loss 1.939176 on epoch=5
05/22/2022 00:19:22 - INFO - __main__ - Step 200 Global step 200 Train loss 2.155860 on epoch=6
05/22/2022 00:19:26 - INFO - __main__ - Global step 200 Train loss 2.413114 Classification-F1 0.1 on epoch=6
05/22/2022 00:19:31 - INFO - __main__ - Step 210 Global step 210 Train loss 1.882551 on epoch=6
05/22/2022 00:19:36 - INFO - __main__ - Step 220 Global step 220 Train loss 1.690052 on epoch=6
05/22/2022 00:19:40 - INFO - __main__ - Step 230 Global step 230 Train loss 1.284204 on epoch=7
05/22/2022 00:19:45 - INFO - __main__ - Step 240 Global step 240 Train loss 1.068226 on epoch=7
05/22/2022 00:19:50 - INFO - __main__ - Step 250 Global step 250 Train loss 1.106262 on epoch=7
05/22/2022 00:19:54 - INFO - __main__ - Global step 250 Train loss 1.406259 Classification-F1 0.10350282215058661 on epoch=7
05/22/2022 00:19:59 - INFO - __main__ - Step 260 Global step 260 Train loss 1.104913 on epoch=8
05/22/2022 00:20:04 - INFO - __main__ - Step 270 Global step 270 Train loss 1.087874 on epoch=8
05/22/2022 00:20:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.980578 on epoch=8
05/22/2022 00:20:14 - INFO - __main__ - Step 290 Global step 290 Train loss 1.004349 on epoch=9
05/22/2022 00:20:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.968357 on epoch=9
05/22/2022 00:20:23 - INFO - __main__ - Global step 300 Train loss 1.029214 Classification-F1 0.1 on epoch=9
05/22/2022 00:20:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.945321 on epoch=9
05/22/2022 00:20:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.908720 on epoch=9
05/22/2022 00:20:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.974902 on epoch=10
05/22/2022 00:20:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.807133 on epoch=10
05/22/2022 00:20:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.892491 on epoch=10
05/22/2022 00:20:52 - INFO - __main__ - Global step 350 Train loss 0.905713 Classification-F1 0.19279222689762426 on epoch=10
05/22/2022 00:20:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.863762 on epoch=11
05/22/2022 00:21:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.942315 on epoch=11
05/22/2022 00:21:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.954794 on epoch=11
05/22/2022 00:21:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.836017 on epoch=12
05/22/2022 00:21:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.955304 on epoch=12
05/22/2022 00:21:22 - INFO - __main__ - Global step 400 Train loss 0.910438 Classification-F1 0.20902552358063609 on epoch=12
05/22/2022 00:21:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.842623 on epoch=12
05/22/2022 00:21:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.840353 on epoch=13
05/22/2022 00:21:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.767988 on epoch=13
05/22/2022 00:21:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.857910 on epoch=13
05/22/2022 00:21:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.808397 on epoch=14
05/22/2022 00:21:53 - INFO - __main__ - Global step 450 Train loss 0.823454 Classification-F1 0.1 on epoch=14
05/22/2022 00:21:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.830338 on epoch=14
05/22/2022 00:22:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.776436 on epoch=14
05/22/2022 00:22:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.795314 on epoch=14
05/22/2022 00:22:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.788729 on epoch=15
05/22/2022 00:22:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.827229 on epoch=15
05/22/2022 00:22:22 - INFO - __main__ - Global step 500 Train loss 0.803609 Classification-F1 0.13091216216216214 on epoch=15
05/22/2022 00:22:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.775652 on epoch=15
05/22/2022 00:22:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.774186 on epoch=16
05/22/2022 00:22:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.806832 on epoch=16
05/22/2022 00:22:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.879877 on epoch=16
05/22/2022 00:22:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.780282 on epoch=17
05/22/2022 00:22:52 - INFO - __main__ - Global step 550 Train loss 0.803366 Classification-F1 0.2672385490954613 on epoch=17
05/22/2022 00:22:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.798593 on epoch=17
05/22/2022 00:23:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.772570 on epoch=17
05/22/2022 00:23:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.798935 on epoch=18
05/22/2022 00:23:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.722665 on epoch=18
05/22/2022 00:23:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.798136 on epoch=18
05/22/2022 00:23:23 - INFO - __main__ - Global step 600 Train loss 0.778180 Classification-F1 0.20704664067495926 on epoch=18
05/22/2022 00:23:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.791073 on epoch=19
05/22/2022 00:23:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.756057 on epoch=19
05/22/2022 00:23:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.780518 on epoch=19
05/22/2022 00:23:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.797945 on epoch=19
05/22/2022 00:23:48 - INFO - __main__ - Step 650 Global step 650 Train loss 0.750821 on epoch=20
05/22/2022 00:23:52 - INFO - __main__ - Global step 650 Train loss 0.775283 Classification-F1 0.12845394736842106 on epoch=20
05/22/2022 00:23:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.754796 on epoch=20
05/22/2022 00:24:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.837204 on epoch=20
05/22/2022 00:24:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.765553 on epoch=21
05/22/2022 00:24:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.718441 on epoch=21
05/22/2022 00:24:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.768113 on epoch=21
05/22/2022 00:24:22 - INFO - __main__ - Global step 700 Train loss 0.768821 Classification-F1 0.11618584173328698 on epoch=21
05/22/2022 00:24:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.784758 on epoch=22
05/22/2022 00:24:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.735775 on epoch=22
05/22/2022 00:24:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.729522 on epoch=22
05/22/2022 00:24:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.752207 on epoch=23
05/22/2022 00:24:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.695426 on epoch=23
05/22/2022 00:24:51 - INFO - __main__ - Global step 750 Train loss 0.739538 Classification-F1 0.19212719298245615 on epoch=23
05/22/2022 00:24:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.734175 on epoch=23
05/22/2022 00:25:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.770409 on epoch=24
05/22/2022 00:25:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.751429 on epoch=24
05/22/2022 00:25:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.767259 on epoch=24
05/22/2022 00:25:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.724796 on epoch=24
05/22/2022 00:25:21 - INFO - __main__ - Global step 800 Train loss 0.749614 Classification-F1 0.3589088032168618 on epoch=24
05/22/2022 00:25:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.731482 on epoch=25
05/22/2022 00:25:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.683042 on epoch=25
05/22/2022 00:25:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.735701 on epoch=25
05/22/2022 00:25:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.743256 on epoch=26
05/22/2022 00:25:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.672690 on epoch=26
05/22/2022 00:25:51 - INFO - __main__ - Global step 850 Train loss 0.713234 Classification-F1 0.23537955724509046 on epoch=26
05/22/2022 00:25:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.700595 on epoch=26
05/22/2022 00:26:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.710683 on epoch=27
05/22/2022 00:26:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.604640 on epoch=27
05/22/2022 00:26:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.622351 on epoch=27
05/22/2022 00:26:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.573071 on epoch=28
05/22/2022 00:26:21 - INFO - __main__ - Global step 900 Train loss 0.642268 Classification-F1 0.6019892319420621 on epoch=28
05/22/2022 00:26:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.579257 on epoch=28
05/22/2022 00:26:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.568256 on epoch=28
05/22/2022 00:26:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.564896 on epoch=29
05/22/2022 00:26:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.669116 on epoch=29
05/22/2022 00:26:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.702229 on epoch=29
05/22/2022 00:26:51 - INFO - __main__ - Global step 950 Train loss 0.616751 Classification-F1 0.6809014723885455 on epoch=29
05/22/2022 00:26:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.504679 on epoch=29
05/22/2022 00:27:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.525564 on epoch=30
05/22/2022 00:27:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.446777 on epoch=30
05/22/2022 00:27:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.549632 on epoch=30
05/22/2022 00:27:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.495299 on epoch=31
05/22/2022 00:27:19 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:27:19 - INFO - __main__ - Printing 3 examples
05/22/2022 00:27:19 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/22/2022 00:27:19 - INFO - __main__ - ['happy']
05/22/2022 00:27:19 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/22/2022 00:27:19 - INFO - __main__ - ['happy']
05/22/2022 00:27:19 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/22/2022 00:27:19 - INFO - __main__ - ['happy']
05/22/2022 00:27:19 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:27:19 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:27:19 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 00:27:19 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:27:19 - INFO - __main__ - Printing 3 examples
05/22/2022 00:27:19 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/22/2022 00:27:19 - INFO - __main__ - ['happy']
05/22/2022 00:27:19 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/22/2022 00:27:19 - INFO - __main__ - ['happy']
05/22/2022 00:27:19 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/22/2022 00:27:19 - INFO - __main__ - ['happy']
05/22/2022 00:27:19 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:27:20 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:27:20 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 00:27:22 - INFO - __main__ - Global step 1000 Train loss 0.504390 Classification-F1 0.6852453876433595 on epoch=31
05/22/2022 00:27:23 - INFO - __main__ - save last model!
05/22/2022 00:27:30 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 00:27:30 - INFO - __main__ - Start tokenizing ... 5509 instances
05/22/2022 00:27:30 - INFO - __main__ - Printing 3 examples
05/22/2022 00:27:30 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/22/2022 00:27:30 - INFO - __main__ - ['others']
05/22/2022 00:27:30 - INFO - __main__ -  [emo] what you like very little things ok
05/22/2022 00:27:30 - INFO - __main__ - ['others']
05/22/2022 00:27:30 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/22/2022 00:27:30 - INFO - __main__ - ['others']
05/22/2022 00:27:30 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:27:32 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:27:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 00:27:33 - INFO - __main__ - Starting training!
05/22/2022 00:27:37 - INFO - __main__ - Loaded 5509 examples from test data
05/22/2022 00:28:21 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_42_0.0005_8_predictions.txt
05/22/2022 00:28:21 - INFO - __main__ - Classification-F1 on test data: 0.4832
05/22/2022 00:28:21 - INFO - __main__ - prefix=emo_128_42, lr=0.0005, bsz=8, dev_performance=0.6852453876433595, test_performance=0.4832303110199427
05/22/2022 00:28:21 - INFO - __main__ - Running ... prefix=emo_128_42, lr=0.0003, bsz=8 ...
05/22/2022 00:28:22 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:28:22 - INFO - __main__ - Printing 3 examples
05/22/2022 00:28:22 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/22/2022 00:28:22 - INFO - __main__ - ['happy']
05/22/2022 00:28:22 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/22/2022 00:28:22 - INFO - __main__ - ['happy']
05/22/2022 00:28:22 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/22/2022 00:28:22 - INFO - __main__ - ['happy']
05/22/2022 00:28:22 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:28:22 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:28:23 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 00:28:23 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:28:23 - INFO - __main__ - Printing 3 examples
05/22/2022 00:28:23 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/22/2022 00:28:23 - INFO - __main__ - ['happy']
05/22/2022 00:28:23 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/22/2022 00:28:23 - INFO - __main__ - ['happy']
05/22/2022 00:28:23 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/22/2022 00:28:23 - INFO - __main__ - ['happy']
05/22/2022 00:28:23 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:28:23 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:28:23 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 00:28:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 00:28:37 - INFO - __main__ - Starting training!
05/22/2022 00:28:41 - INFO - __main__ - Step 10 Global step 10 Train loss 26.751776 on epoch=0
05/22/2022 00:28:46 - INFO - __main__ - Step 20 Global step 20 Train loss 19.618006 on epoch=0
05/22/2022 00:28:51 - INFO - __main__ - Step 30 Global step 30 Train loss 17.466389 on epoch=0
05/22/2022 00:28:56 - INFO - __main__ - Step 40 Global step 40 Train loss 16.867966 on epoch=1
05/22/2022 00:29:01 - INFO - __main__ - Step 50 Global step 50 Train loss 14.793475 on epoch=1
05/22/2022 00:30:59 - INFO - __main__ - Global step 50 Train loss 19.099522 Classification-F1 5.0174355886706306e-05 on epoch=1
05/22/2022 00:31:05 - INFO - __main__ - Step 60 Global step 60 Train loss 14.009483 on epoch=1
05/22/2022 00:31:10 - INFO - __main__ - Step 70 Global step 70 Train loss 14.015120 on epoch=2
05/22/2022 00:31:15 - INFO - __main__ - Step 80 Global step 80 Train loss 12.555205 on epoch=2
05/22/2022 00:31:20 - INFO - __main__ - Step 90 Global step 90 Train loss 11.966162 on epoch=2
05/22/2022 00:31:25 - INFO - __main__ - Step 100 Global step 100 Train loss 11.079794 on epoch=3
05/22/2022 00:31:59 - INFO - __main__ - Global step 100 Train loss 12.725153 Classification-F1 0.001571621014818141 on epoch=3
05/22/2022 00:32:05 - INFO - __main__ - Step 110 Global step 110 Train loss 8.673651 on epoch=3
05/22/2022 00:32:09 - INFO - __main__ - Step 120 Global step 120 Train loss 4.637308 on epoch=3
05/22/2022 00:32:14 - INFO - __main__ - Step 130 Global step 130 Train loss 2.013781 on epoch=4
05/22/2022 00:32:19 - INFO - __main__ - Step 140 Global step 140 Train loss 1.517981 on epoch=4
05/22/2022 00:32:25 - INFO - __main__ - Step 150 Global step 150 Train loss 1.049555 on epoch=4
05/22/2022 00:32:29 - INFO - __main__ - Global step 150 Train loss 3.578454 Classification-F1 0.2411094147582697 on epoch=4
05/22/2022 00:32:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.961984 on epoch=4
05/22/2022 00:32:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.808078 on epoch=5
05/22/2022 00:32:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.850671 on epoch=5
05/22/2022 00:32:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.866331 on epoch=5
05/22/2022 00:32:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.910027 on epoch=6
05/22/2022 00:32:59 - INFO - __main__ - Global step 200 Train loss 0.879418 Classification-F1 0.37159461968269625 on epoch=6
05/22/2022 00:33:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.885454 on epoch=6
05/22/2022 00:33:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.927609 on epoch=6
05/22/2022 00:33:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.699544 on epoch=7
05/22/2022 00:33:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.872898 on epoch=7
05/22/2022 00:33:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.742516 on epoch=7
05/22/2022 00:33:29 - INFO - __main__ - Global step 250 Train loss 0.825604 Classification-F1 0.22735277385098093 on epoch=7
05/22/2022 00:33:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.769646 on epoch=8
05/22/2022 00:33:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.751282 on epoch=8
05/22/2022 00:33:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.711345 on epoch=8
05/22/2022 00:33:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.684615 on epoch=9
05/22/2022 00:33:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.607147 on epoch=9
05/22/2022 00:33:59 - INFO - __main__ - Global step 300 Train loss 0.704807 Classification-F1 0.5436562713939812 on epoch=9
05/22/2022 00:34:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.656722 on epoch=9
05/22/2022 00:34:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.638884 on epoch=9
05/22/2022 00:34:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.551101 on epoch=10
05/22/2022 00:34:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.557738 on epoch=10
05/22/2022 00:34:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.557275 on epoch=10
05/22/2022 00:34:29 - INFO - __main__ - Global step 350 Train loss 0.592344 Classification-F1 0.7699754901960785 on epoch=10
05/22/2022 00:34:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.507556 on epoch=11
05/22/2022 00:34:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.567060 on epoch=11
05/22/2022 00:34:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.513097 on epoch=11
05/22/2022 00:34:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.542951 on epoch=12
05/22/2022 00:34:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.488834 on epoch=12
05/22/2022 00:35:00 - INFO - __main__ - Global step 400 Train loss 0.523899 Classification-F1 0.7255663256694239 on epoch=12
05/22/2022 00:35:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.580671 on epoch=12
05/22/2022 00:35:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.441075 on epoch=13
05/22/2022 00:35:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.454025 on epoch=13
05/22/2022 00:35:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.438757 on epoch=13
05/22/2022 00:35:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.391578 on epoch=14
05/22/2022 00:35:29 - INFO - __main__ - Global step 450 Train loss 0.461221 Classification-F1 0.7502173397813137 on epoch=14
05/22/2022 00:35:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.374232 on epoch=14
05/22/2022 00:35:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.426380 on epoch=14
05/22/2022 00:35:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.449439 on epoch=14
05/22/2022 00:35:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.322154 on epoch=15
05/22/2022 00:35:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.411060 on epoch=15
05/22/2022 00:35:59 - INFO - __main__ - Global step 500 Train loss 0.396653 Classification-F1 0.7880795397922526 on epoch=15
05/22/2022 00:36:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.349578 on epoch=15
05/22/2022 00:36:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.428544 on epoch=16
05/22/2022 00:36:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.346939 on epoch=16
05/22/2022 00:36:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.371606 on epoch=16
05/22/2022 00:36:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.342883 on epoch=17
05/22/2022 00:36:29 - INFO - __main__ - Global step 550 Train loss 0.367910 Classification-F1 0.66880142330244 on epoch=17
05/22/2022 00:36:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.406355 on epoch=17
05/22/2022 00:36:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.351125 on epoch=17
05/22/2022 00:36:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.343582 on epoch=18
05/22/2022 00:36:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.246288 on epoch=18
05/22/2022 00:36:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.291807 on epoch=18
05/22/2022 00:36:59 - INFO - __main__ - Global step 600 Train loss 0.327831 Classification-F1 0.6494907016839762 on epoch=18
05/22/2022 00:37:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.350203 on epoch=19
05/22/2022 00:37:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.299475 on epoch=19
05/22/2022 00:37:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.411657 on epoch=19
05/22/2022 00:37:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.250558 on epoch=19
05/22/2022 00:37:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.279149 on epoch=20
05/22/2022 00:37:29 - INFO - __main__ - Global step 650 Train loss 0.318208 Classification-F1 0.5734005783505853 on epoch=20
05/22/2022 00:37:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.248015 on epoch=20
05/22/2022 00:37:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.262803 on epoch=20
05/22/2022 00:37:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.251052 on epoch=21
05/22/2022 00:37:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.246154 on epoch=21
05/22/2022 00:37:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.306204 on epoch=21
05/22/2022 00:37:59 - INFO - __main__ - Global step 700 Train loss 0.262845 Classification-F1 0.6460137393294845 on epoch=21
05/22/2022 00:38:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.201506 on epoch=22
05/22/2022 00:38:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.279196 on epoch=22
05/22/2022 00:38:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.322597 on epoch=22
05/22/2022 00:38:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.228313 on epoch=23
05/22/2022 00:38:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.185729 on epoch=23
05/22/2022 00:38:29 - INFO - __main__ - Global step 750 Train loss 0.243468 Classification-F1 0.45306950358494497 on epoch=23
05/22/2022 00:38:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.158139 on epoch=23
05/22/2022 00:38:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.313260 on epoch=24
05/22/2022 00:38:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.283587 on epoch=24
05/22/2022 00:38:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.209745 on epoch=24
05/22/2022 00:38:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.222357 on epoch=24
05/22/2022 00:39:00 - INFO - __main__ - Global step 800 Train loss 0.237418 Classification-F1 0.6288833646868739 on epoch=24
05/22/2022 00:39:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.240647 on epoch=25
05/22/2022 00:39:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.165193 on epoch=25
05/22/2022 00:39:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.204935 on epoch=25
05/22/2022 00:39:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.152288 on epoch=26
05/22/2022 00:39:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.175609 on epoch=26
05/22/2022 00:39:30 - INFO - __main__ - Global step 850 Train loss 0.187735 Classification-F1 0.47640683355562613 on epoch=26
05/22/2022 00:39:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.152847 on epoch=26
05/22/2022 00:39:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.104802 on epoch=27
05/22/2022 00:39:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.109005 on epoch=27
05/22/2022 00:39:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.131744 on epoch=27
05/22/2022 00:39:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.117367 on epoch=28
05/22/2022 00:40:00 - INFO - __main__ - Global step 900 Train loss 0.123153 Classification-F1 0.45671308244606906 on epoch=28
05/22/2022 00:40:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.147391 on epoch=28
05/22/2022 00:40:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.060702 on epoch=28
05/22/2022 00:40:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.132920 on epoch=29
05/22/2022 00:40:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.061225 on epoch=29
05/22/2022 00:40:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.050767 on epoch=29
05/22/2022 00:40:31 - INFO - __main__ - Global step 950 Train loss 0.090601 Classification-F1 0.8059862563974147 on epoch=29
05/22/2022 00:40:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.163766 on epoch=29
05/22/2022 00:40:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.098265 on epoch=30
05/22/2022 00:40:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.093951 on epoch=30
05/22/2022 00:40:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.095388 on epoch=30
05/22/2022 00:40:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.058437 on epoch=31
05/22/2022 00:40:58 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:40:58 - INFO - __main__ - Printing 3 examples
05/22/2022 00:40:58 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/22/2022 00:40:58 - INFO - __main__ - ['happy']
05/22/2022 00:40:58 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/22/2022 00:40:58 - INFO - __main__ - ['happy']
05/22/2022 00:40:58 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/22/2022 00:40:58 - INFO - __main__ - ['happy']
05/22/2022 00:40:58 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:40:58 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:40:59 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 00:40:59 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:40:59 - INFO - __main__ - Printing 3 examples
05/22/2022 00:40:59 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/22/2022 00:40:59 - INFO - __main__ - ['happy']
05/22/2022 00:40:59 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/22/2022 00:40:59 - INFO - __main__ - ['happy']
05/22/2022 00:40:59 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/22/2022 00:40:59 - INFO - __main__ - ['happy']
05/22/2022 00:40:59 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:40:59 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:40:59 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 00:41:02 - INFO - __main__ - Global step 1000 Train loss 0.101961 Classification-F1 0.8169953305563403 on epoch=31
05/22/2022 00:41:03 - INFO - __main__ - save last model!
05/22/2022 00:41:10 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 00:41:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 00:41:10 - INFO - __main__ - Starting training!
05/22/2022 00:41:11 - INFO - __main__ - Start tokenizing ... 5509 instances
05/22/2022 00:41:11 - INFO - __main__ - Printing 3 examples
05/22/2022 00:41:11 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/22/2022 00:41:11 - INFO - __main__ - ['others']
05/22/2022 00:41:11 - INFO - __main__ -  [emo] what you like very little things ok
05/22/2022 00:41:11 - INFO - __main__ - ['others']
05/22/2022 00:41:11 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/22/2022 00:41:11 - INFO - __main__ - ['others']
05/22/2022 00:41:11 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:41:13 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:41:18 - INFO - __main__ - Loaded 5509 examples from test data
05/22/2022 00:42:14 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_42_0.0003_8_predictions.txt
05/22/2022 00:42:14 - INFO - __main__ - Classification-F1 on test data: 0.1157
05/22/2022 00:42:15 - INFO - __main__ - prefix=emo_128_42, lr=0.0003, bsz=8, dev_performance=0.8169953305563403, test_performance=0.11573254716386765
05/22/2022 00:42:15 - INFO - __main__ - Running ... prefix=emo_128_42, lr=0.0002, bsz=8 ...
05/22/2022 00:42:16 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:42:16 - INFO - __main__ - Printing 3 examples
05/22/2022 00:42:16 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/22/2022 00:42:16 - INFO - __main__ - ['happy']
05/22/2022 00:42:16 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/22/2022 00:42:16 - INFO - __main__ - ['happy']
05/22/2022 00:42:16 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/22/2022 00:42:16 - INFO - __main__ - ['happy']
05/22/2022 00:42:16 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:42:16 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:42:17 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 00:42:17 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:42:17 - INFO - __main__ - Printing 3 examples
05/22/2022 00:42:17 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/22/2022 00:42:17 - INFO - __main__ - ['happy']
05/22/2022 00:42:17 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/22/2022 00:42:17 - INFO - __main__ - ['happy']
05/22/2022 00:42:17 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/22/2022 00:42:17 - INFO - __main__ - ['happy']
05/22/2022 00:42:17 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:42:17 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:42:17 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 00:42:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 00:42:29 - INFO - __main__ - Starting training!
05/22/2022 00:42:33 - INFO - __main__ - Step 10 Global step 10 Train loss 25.329714 on epoch=0
05/22/2022 00:42:38 - INFO - __main__ - Step 20 Global step 20 Train loss 21.963888 on epoch=0
05/22/2022 00:42:43 - INFO - __main__ - Step 30 Global step 30 Train loss 17.992950 on epoch=0
05/22/2022 00:42:48 - INFO - __main__ - Step 40 Global step 40 Train loss 17.740028 on epoch=1
05/22/2022 00:42:53 - INFO - __main__ - Step 50 Global step 50 Train loss 16.907650 on epoch=1
05/22/2022 00:44:20 - INFO - __main__ - Global step 50 Train loss 19.986845 Classification-F1 0.0 on epoch=1
05/22/2022 00:44:25 - INFO - __main__ - Step 60 Global step 60 Train loss 16.104565 on epoch=1
05/22/2022 00:44:30 - INFO - __main__ - Step 70 Global step 70 Train loss 16.304613 on epoch=2
05/22/2022 00:44:35 - INFO - __main__ - Step 80 Global step 80 Train loss 14.849180 on epoch=2
05/22/2022 00:44:40 - INFO - __main__ - Step 90 Global step 90 Train loss 14.002516 on epoch=2
05/22/2022 00:44:45 - INFO - __main__ - Step 100 Global step 100 Train loss 13.779485 on epoch=3
05/22/2022 00:45:09 - INFO - __main__ - Global step 100 Train loss 15.008071 Classification-F1 0.0 on epoch=3
05/22/2022 00:45:14 - INFO - __main__ - Step 110 Global step 110 Train loss 13.136225 on epoch=3
05/22/2022 00:45:19 - INFO - __main__ - Step 120 Global step 120 Train loss 12.222062 on epoch=3
05/22/2022 00:45:24 - INFO - __main__ - Step 130 Global step 130 Train loss 11.981752 on epoch=4
05/22/2022 00:45:29 - INFO - __main__ - Step 140 Global step 140 Train loss 11.417645 on epoch=4
05/22/2022 00:45:34 - INFO - __main__ - Step 150 Global step 150 Train loss 9.866220 on epoch=4
05/22/2022 00:45:41 - INFO - __main__ - Global step 150 Train loss 11.724780 Classification-F1 0.0005255551175929575 on epoch=4
05/22/2022 00:45:47 - INFO - __main__ - Step 160 Global step 160 Train loss 8.933329 on epoch=4
05/22/2022 00:45:52 - INFO - __main__ - Step 170 Global step 170 Train loss 7.968431 on epoch=5
05/22/2022 00:45:57 - INFO - __main__ - Step 180 Global step 180 Train loss 5.075159 on epoch=5
05/22/2022 00:46:02 - INFO - __main__ - Step 190 Global step 190 Train loss 3.822401 on epoch=5
05/22/2022 00:46:07 - INFO - __main__ - Step 200 Global step 200 Train loss 4.676019 on epoch=6
05/22/2022 00:46:11 - INFO - __main__ - Global step 200 Train loss 6.095067 Classification-F1 0.1 on epoch=6
05/22/2022 00:46:17 - INFO - __main__ - Step 210 Global step 210 Train loss 4.098762 on epoch=6
05/22/2022 00:46:22 - INFO - __main__ - Step 220 Global step 220 Train loss 4.629323 on epoch=6
05/22/2022 00:46:27 - INFO - __main__ - Step 230 Global step 230 Train loss 3.781536 on epoch=7
05/22/2022 00:46:32 - INFO - __main__ - Step 240 Global step 240 Train loss 4.069232 on epoch=7
05/22/2022 00:46:37 - INFO - __main__ - Step 250 Global step 250 Train loss 3.326462 on epoch=7
05/22/2022 00:46:41 - INFO - __main__ - Global step 250 Train loss 3.981063 Classification-F1 0.20143119098208306 on epoch=7
05/22/2022 00:46:46 - INFO - __main__ - Step 260 Global step 260 Train loss 3.864435 on epoch=8
05/22/2022 00:46:51 - INFO - __main__ - Step 270 Global step 270 Train loss 3.696167 on epoch=8
05/22/2022 00:46:56 - INFO - __main__ - Step 280 Global step 280 Train loss 3.826520 on epoch=8
05/22/2022 00:47:01 - INFO - __main__ - Step 290 Global step 290 Train loss 2.856194 on epoch=9
05/22/2022 00:47:06 - INFO - __main__ - Step 300 Global step 300 Train loss 2.927986 on epoch=9
05/22/2022 00:47:10 - INFO - __main__ - Global step 300 Train loss 3.434261 Classification-F1 0.1 on epoch=9
05/22/2022 00:47:15 - INFO - __main__ - Step 310 Global step 310 Train loss 2.761081 on epoch=9
05/22/2022 00:47:20 - INFO - __main__ - Step 320 Global step 320 Train loss 2.705050 on epoch=9
05/22/2022 00:47:25 - INFO - __main__ - Step 330 Global step 330 Train loss 2.978117 on epoch=10
05/22/2022 00:47:30 - INFO - __main__ - Step 340 Global step 340 Train loss 3.107856 on epoch=10
05/22/2022 00:47:35 - INFO - __main__ - Step 350 Global step 350 Train loss 2.599021 on epoch=10
05/22/2022 00:47:39 - INFO - __main__ - Global step 350 Train loss 2.830225 Classification-F1 0.15135104895104898 on epoch=10
05/22/2022 00:47:44 - INFO - __main__ - Step 360 Global step 360 Train loss 2.488744 on epoch=11
05/22/2022 00:47:49 - INFO - __main__ - Step 370 Global step 370 Train loss 2.455966 on epoch=11
05/22/2022 00:47:54 - INFO - __main__ - Step 380 Global step 380 Train loss 2.088917 on epoch=11
05/22/2022 00:47:59 - INFO - __main__ - Step 390 Global step 390 Train loss 2.278562 on epoch=12
05/22/2022 00:48:04 - INFO - __main__ - Step 400 Global step 400 Train loss 2.168406 on epoch=12
05/22/2022 00:48:08 - INFO - __main__ - Global step 400 Train loss 2.296119 Classification-F1 0.49599186991869915 on epoch=12
05/22/2022 00:48:14 - INFO - __main__ - Step 410 Global step 410 Train loss 1.727154 on epoch=12
05/22/2022 00:48:19 - INFO - __main__ - Step 420 Global step 420 Train loss 2.010677 on epoch=13
05/22/2022 00:48:24 - INFO - __main__ - Step 430 Global step 430 Train loss 1.645139 on epoch=13
05/22/2022 00:48:29 - INFO - __main__ - Step 440 Global step 440 Train loss 1.224756 on epoch=13
05/22/2022 00:48:34 - INFO - __main__ - Step 450 Global step 450 Train loss 1.289515 on epoch=14
05/22/2022 00:48:38 - INFO - __main__ - Global step 450 Train loss 1.579448 Classification-F1 0.1616378209069778 on epoch=14
05/22/2022 00:48:43 - INFO - __main__ - Step 460 Global step 460 Train loss 1.166977 on epoch=14
05/22/2022 00:48:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.839751 on epoch=14
05/22/2022 00:48:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.591666 on epoch=14
05/22/2022 00:48:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.708974 on epoch=15
05/22/2022 00:49:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.545497 on epoch=15
05/22/2022 00:49:07 - INFO - __main__ - Global step 500 Train loss 0.770573 Classification-F1 0.7253072787914429 on epoch=15
05/22/2022 00:49:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.596471 on epoch=15
05/22/2022 00:49:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.496199 on epoch=16
05/22/2022 00:49:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.454424 on epoch=16
05/22/2022 00:49:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.526499 on epoch=16
05/22/2022 00:49:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.425720 on epoch=17
05/22/2022 00:49:37 - INFO - __main__ - Global step 550 Train loss 0.499862 Classification-F1 0.6776150968561365 on epoch=17
05/22/2022 00:49:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.434871 on epoch=17
05/22/2022 00:49:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.447198 on epoch=17
05/22/2022 00:49:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.377041 on epoch=18
05/22/2022 00:49:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.376617 on epoch=18
05/22/2022 00:50:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.385389 on epoch=18
05/22/2022 00:50:06 - INFO - __main__ - Global step 600 Train loss 0.404223 Classification-F1 0.8234362026965849 on epoch=18
05/22/2022 00:50:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.369531 on epoch=19
05/22/2022 00:50:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.326032 on epoch=19
05/22/2022 00:50:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.302336 on epoch=19
05/22/2022 00:50:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.296944 on epoch=19
05/22/2022 00:50:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.395376 on epoch=20
05/22/2022 00:50:36 - INFO - __main__ - Global step 650 Train loss 0.338044 Classification-F1 0.771786999216362 on epoch=20
05/22/2022 00:50:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.261353 on epoch=20
05/22/2022 00:50:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.336128 on epoch=20
05/22/2022 00:50:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.308225 on epoch=21
05/22/2022 00:50:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.240536 on epoch=21
05/22/2022 00:51:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.275320 on epoch=21
05/22/2022 00:51:06 - INFO - __main__ - Global step 700 Train loss 0.284312 Classification-F1 0.8444375050548328 on epoch=21
05/22/2022 00:51:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.245797 on epoch=22
05/22/2022 00:51:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.176397 on epoch=22
05/22/2022 00:51:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.358337 on epoch=22
05/22/2022 00:51:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.216341 on epoch=23
05/22/2022 00:51:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.167689 on epoch=23
05/22/2022 00:51:36 - INFO - __main__ - Global step 750 Train loss 0.232912 Classification-F1 0.8186105021645792 on epoch=23
05/22/2022 00:51:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.244635 on epoch=23
05/22/2022 00:51:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.178229 on epoch=24
05/22/2022 00:51:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.209665 on epoch=24
05/22/2022 00:51:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.229960 on epoch=24
05/22/2022 00:52:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.179187 on epoch=24
05/22/2022 00:52:06 - INFO - __main__ - Global step 800 Train loss 0.208335 Classification-F1 0.8519018523726395 on epoch=24
05/22/2022 00:52:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.106593 on epoch=25
05/22/2022 00:52:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.150014 on epoch=25
05/22/2022 00:52:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.223910 on epoch=25
05/22/2022 00:52:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.151741 on epoch=26
05/22/2022 00:52:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.188073 on epoch=26
05/22/2022 00:52:36 - INFO - __main__ - Global step 850 Train loss 0.164066 Classification-F1 0.825621967047632 on epoch=26
05/22/2022 00:52:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.111623 on epoch=26
05/22/2022 00:52:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.131280 on epoch=27
05/22/2022 00:52:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.096606 on epoch=27
05/22/2022 00:52:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.165651 on epoch=27
05/22/2022 00:53:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.083403 on epoch=28
05/22/2022 00:53:06 - INFO - __main__ - Global step 900 Train loss 0.117713 Classification-F1 0.8498260436668176 on epoch=28
05/22/2022 00:53:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.176096 on epoch=28
05/22/2022 00:53:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.131682 on epoch=28
05/22/2022 00:53:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.102418 on epoch=29
05/22/2022 00:53:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.103075 on epoch=29
05/22/2022 00:53:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.097523 on epoch=29
05/22/2022 00:53:36 - INFO - __main__ - Global step 950 Train loss 0.122159 Classification-F1 0.859550134045973 on epoch=29
05/22/2022 00:53:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.061837 on epoch=29
05/22/2022 00:53:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.083168 on epoch=30
05/22/2022 00:53:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.113056 on epoch=30
05/22/2022 00:53:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.097409 on epoch=30
05/22/2022 00:54:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.066192 on epoch=31
05/22/2022 00:54:02 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:54:02 - INFO - __main__ - Printing 3 examples
05/22/2022 00:54:02 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/22/2022 00:54:02 - INFO - __main__ - ['happy']
05/22/2022 00:54:02 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/22/2022 00:54:02 - INFO - __main__ - ['happy']
05/22/2022 00:54:02 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/22/2022 00:54:02 - INFO - __main__ - ['happy']
05/22/2022 00:54:02 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:54:02 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:54:03 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 00:54:03 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:54:03 - INFO - __main__ - Printing 3 examples
05/22/2022 00:54:03 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/22/2022 00:54:03 - INFO - __main__ - ['happy']
05/22/2022 00:54:03 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/22/2022 00:54:03 - INFO - __main__ - ['happy']
05/22/2022 00:54:03 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/22/2022 00:54:03 - INFO - __main__ - ['happy']
05/22/2022 00:54:03 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:54:03 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:54:04 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 00:54:06 - INFO - __main__ - Global step 1000 Train loss 0.084332 Classification-F1 0.8691415941350656 on epoch=31
05/22/2022 00:54:07 - INFO - __main__ - save last model!
05/22/2022 00:54:14 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 00:54:14 - INFO - __main__ - Start tokenizing ... 5509 instances
05/22/2022 00:54:14 - INFO - __main__ - Printing 3 examples
05/22/2022 00:54:14 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/22/2022 00:54:14 - INFO - __main__ - ['others']
05/22/2022 00:54:14 - INFO - __main__ -  [emo] what you like very little things ok
05/22/2022 00:54:14 - INFO - __main__ - ['others']
05/22/2022 00:54:14 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/22/2022 00:54:14 - INFO - __main__ - ['others']
05/22/2022 00:54:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 00:54:14 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:54:14 - INFO - __main__ - Starting training!
05/22/2022 00:54:16 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:54:22 - INFO - __main__ - Loaded 5509 examples from test data
05/22/2022 00:55:13 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_42_0.0002_8_predictions.txt
05/22/2022 00:55:13 - INFO - __main__ - Classification-F1 on test data: 0.5446
05/22/2022 00:55:13 - INFO - __main__ - prefix=emo_128_42, lr=0.0002, bsz=8, dev_performance=0.8691415941350656, test_performance=0.5445557334889307
05/22/2022 00:55:13 - INFO - __main__ - Running ... prefix=emo_128_42, lr=0.0001, bsz=8 ...
05/22/2022 00:55:14 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:55:14 - INFO - __main__ - Printing 3 examples
05/22/2022 00:55:14 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/22/2022 00:55:14 - INFO - __main__ - ['happy']
05/22/2022 00:55:14 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/22/2022 00:55:14 - INFO - __main__ - ['happy']
05/22/2022 00:55:14 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/22/2022 00:55:14 - INFO - __main__ - ['happy']
05/22/2022 00:55:14 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:55:14 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:55:15 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 00:55:15 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 00:55:15 - INFO - __main__ - Printing 3 examples
05/22/2022 00:55:15 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/22/2022 00:55:15 - INFO - __main__ - ['happy']
05/22/2022 00:55:15 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/22/2022 00:55:15 - INFO - __main__ - ['happy']
05/22/2022 00:55:15 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/22/2022 00:55:15 - INFO - __main__ - ['happy']
05/22/2022 00:55:15 - INFO - __main__ - Tokenizing Input ...
05/22/2022 00:55:15 - INFO - __main__ - Tokenizing Output ...
05/22/2022 00:55:15 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 00:55:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 00:55:28 - INFO - __main__ - Starting training!
05/22/2022 00:55:32 - INFO - __main__ - Step 10 Global step 10 Train loss 25.679300 on epoch=0
05/22/2022 00:55:37 - INFO - __main__ - Step 20 Global step 20 Train loss 23.209898 on epoch=0
05/22/2022 00:55:42 - INFO - __main__ - Step 30 Global step 30 Train loss 19.622341 on epoch=0
05/22/2022 00:55:47 - INFO - __main__ - Step 40 Global step 40 Train loss 18.967884 on epoch=1
05/22/2022 00:55:52 - INFO - __main__ - Step 50 Global step 50 Train loss 17.978149 on epoch=1
05/22/2022 00:57:19 - INFO - __main__ - Global step 50 Train loss 21.091515 Classification-F1 0.0 on epoch=1
05/22/2022 00:57:24 - INFO - __main__ - Step 60 Global step 60 Train loss 18.304220 on epoch=1
05/22/2022 00:57:29 - INFO - __main__ - Step 70 Global step 70 Train loss 17.221226 on epoch=2
05/22/2022 00:57:34 - INFO - __main__ - Step 80 Global step 80 Train loss 16.374851 on epoch=2
05/22/2022 00:57:39 - INFO - __main__ - Step 90 Global step 90 Train loss 17.120693 on epoch=2
05/22/2022 00:57:45 - INFO - __main__ - Step 100 Global step 100 Train loss 16.193575 on epoch=3
05/22/2022 00:57:59 - INFO - __main__ - Global step 100 Train loss 17.042913 Classification-F1 0.0 on epoch=3
05/22/2022 00:58:04 - INFO - __main__ - Step 110 Global step 110 Train loss 16.031794 on epoch=3
05/22/2022 00:58:09 - INFO - __main__ - Step 120 Global step 120 Train loss 15.871361 on epoch=3
05/22/2022 00:58:14 - INFO - __main__ - Step 130 Global step 130 Train loss 15.166046 on epoch=4
05/22/2022 00:58:19 - INFO - __main__ - Step 140 Global step 140 Train loss 15.524765 on epoch=4
05/22/2022 00:58:25 - INFO - __main__ - Step 150 Global step 150 Train loss 14.783183 on epoch=4
05/22/2022 00:58:31 - INFO - __main__ - Global step 150 Train loss 15.475429 Classification-F1 0.0002733494810053773 on epoch=4
05/22/2022 00:58:37 - INFO - __main__ - Step 160 Global step 160 Train loss 14.411189 on epoch=4
05/22/2022 00:58:43 - INFO - __main__ - Step 170 Global step 170 Train loss 14.404299 on epoch=5
05/22/2022 00:58:48 - INFO - __main__ - Step 180 Global step 180 Train loss 13.918007 on epoch=5
05/22/2022 00:58:53 - INFO - __main__ - Step 190 Global step 190 Train loss 13.521619 on epoch=5
05/22/2022 00:58:58 - INFO - __main__ - Step 200 Global step 200 Train loss 14.025215 on epoch=6
05/22/2022 00:59:05 - INFO - __main__ - Global step 200 Train loss 14.056065 Classification-F1 0.00014652014652014652 on epoch=6
05/22/2022 00:59:10 - INFO - __main__ - Step 210 Global step 210 Train loss 12.921522 on epoch=6
05/22/2022 00:59:15 - INFO - __main__ - Step 220 Global step 220 Train loss 13.141136 on epoch=6
05/22/2022 00:59:20 - INFO - __main__ - Step 230 Global step 230 Train loss 12.260107 on epoch=7
05/22/2022 00:59:25 - INFO - __main__ - Step 240 Global step 240 Train loss 12.390083 on epoch=7
05/22/2022 00:59:30 - INFO - __main__ - Step 250 Global step 250 Train loss 10.986632 on epoch=7
05/22/2022 00:59:37 - INFO - __main__ - Global step 250 Train loss 12.339896 Classification-F1 0.00029700472455391954 on epoch=7
05/22/2022 00:59:42 - INFO - __main__ - Step 260 Global step 260 Train loss 11.562284 on epoch=8
05/22/2022 00:59:48 - INFO - __main__ - Step 270 Global step 270 Train loss 11.743901 on epoch=8
05/22/2022 00:59:53 - INFO - __main__ - Step 280 Global step 280 Train loss 10.577255 on epoch=8
05/22/2022 00:59:58 - INFO - __main__ - Step 290 Global step 290 Train loss 10.386079 on epoch=9
05/22/2022 01:00:03 - INFO - __main__ - Step 300 Global step 300 Train loss 10.292960 on epoch=9
05/22/2022 01:00:10 - INFO - __main__ - Global step 300 Train loss 10.912495 Classification-F1 0.0013925129596771388 on epoch=9
05/22/2022 01:00:16 - INFO - __main__ - Step 310 Global step 310 Train loss 9.090736 on epoch=9
05/22/2022 01:00:21 - INFO - __main__ - Step 320 Global step 320 Train loss 8.293783 on epoch=9
05/22/2022 01:00:26 - INFO - __main__ - Step 330 Global step 330 Train loss 8.297888 on epoch=10
05/22/2022 01:00:32 - INFO - __main__ - Step 340 Global step 340 Train loss 5.859111 on epoch=10
05/22/2022 01:00:37 - INFO - __main__ - Step 350 Global step 350 Train loss 6.120246 on epoch=10
05/22/2022 01:00:42 - INFO - __main__ - Global step 350 Train loss 7.532352 Classification-F1 0.08106312292358805 on epoch=10
05/22/2022 01:00:48 - INFO - __main__ - Step 360 Global step 360 Train loss 4.986333 on epoch=11
05/22/2022 01:00:53 - INFO - __main__ - Step 370 Global step 370 Train loss 4.289994 on epoch=11
05/22/2022 01:00:58 - INFO - __main__ - Step 380 Global step 380 Train loss 4.854518 on epoch=11
05/22/2022 01:01:04 - INFO - __main__ - Step 390 Global step 390 Train loss 4.360182 on epoch=12
05/22/2022 01:01:09 - INFO - __main__ - Step 400 Global step 400 Train loss 5.050484 on epoch=12
05/22/2022 01:01:13 - INFO - __main__ - Global step 400 Train loss 4.708302 Classification-F1 0.24173136419356722 on epoch=12
05/22/2022 01:01:19 - INFO - __main__ - Step 410 Global step 410 Train loss 4.377789 on epoch=12
05/22/2022 01:01:24 - INFO - __main__ - Step 420 Global step 420 Train loss 4.411360 on epoch=13
05/22/2022 01:01:29 - INFO - __main__ - Step 430 Global step 430 Train loss 3.863742 on epoch=13
05/22/2022 01:01:35 - INFO - __main__ - Step 440 Global step 440 Train loss 3.664467 on epoch=13
05/22/2022 01:01:40 - INFO - __main__ - Step 450 Global step 450 Train loss 3.969011 on epoch=14
05/22/2022 01:01:44 - INFO - __main__ - Global step 450 Train loss 4.057274 Classification-F1 0.10800578731613214 on epoch=14
05/22/2022 01:01:49 - INFO - __main__ - Step 460 Global step 460 Train loss 4.625091 on epoch=14
05/22/2022 01:01:54 - INFO - __main__ - Step 470 Global step 470 Train loss 3.254892 on epoch=14
05/22/2022 01:02:00 - INFO - __main__ - Step 480 Global step 480 Train loss 3.246086 on epoch=14
05/22/2022 01:02:05 - INFO - __main__ - Step 490 Global step 490 Train loss 2.696862 on epoch=15
05/22/2022 01:02:10 - INFO - __main__ - Step 500 Global step 500 Train loss 3.165073 on epoch=15
05/22/2022 01:02:15 - INFO - __main__ - Global step 500 Train loss 3.397601 Classification-F1 0.10999719760862603 on epoch=15
05/22/2022 01:02:20 - INFO - __main__ - Step 510 Global step 510 Train loss 2.539057 on epoch=15
05/22/2022 01:02:25 - INFO - __main__ - Step 520 Global step 520 Train loss 1.480136 on epoch=16
05/22/2022 01:02:30 - INFO - __main__ - Step 530 Global step 530 Train loss 1.070096 on epoch=16
05/22/2022 01:02:35 - INFO - __main__ - Step 540 Global step 540 Train loss 1.365419 on epoch=16
05/22/2022 01:02:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.968912 on epoch=17
05/22/2022 01:02:45 - INFO - __main__ - Global step 550 Train loss 1.484724 Classification-F1 0.5468143011479979 on epoch=17
05/22/2022 01:02:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.765137 on epoch=17
05/22/2022 01:02:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.806449 on epoch=17
05/22/2022 01:03:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.785420 on epoch=18
05/22/2022 01:03:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.751009 on epoch=18
05/22/2022 01:03:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.705050 on epoch=18
05/22/2022 01:03:16 - INFO - __main__ - Global step 600 Train loss 0.762613 Classification-F1 0.7001894507587234 on epoch=18
05/22/2022 01:03:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.691033 on epoch=19
05/22/2022 01:03:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.712223 on epoch=19
05/22/2022 01:03:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.645700 on epoch=19
05/22/2022 01:03:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.520934 on epoch=19
05/22/2022 01:03:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.617008 on epoch=20
05/22/2022 01:03:47 - INFO - __main__ - Global step 650 Train loss 0.637380 Classification-F1 0.6111144604555436 on epoch=20
05/22/2022 01:03:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.574883 on epoch=20
05/22/2022 01:03:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.678829 on epoch=20
05/22/2022 01:04:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.598076 on epoch=21
05/22/2022 01:04:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.653382 on epoch=21
05/22/2022 01:04:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.534328 on epoch=21
05/22/2022 01:04:17 - INFO - __main__ - Global step 700 Train loss 0.607900 Classification-F1 0.765006245874504 on epoch=21
05/22/2022 01:04:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.532417 on epoch=22
05/22/2022 01:04:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.605312 on epoch=22
05/22/2022 01:04:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.538907 on epoch=22
05/22/2022 01:04:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.527006 on epoch=23
05/22/2022 01:04:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.445773 on epoch=23
05/22/2022 01:04:48 - INFO - __main__ - Global step 750 Train loss 0.529883 Classification-F1 0.7719774025158179 on epoch=23
05/22/2022 01:04:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.538571 on epoch=23
05/22/2022 01:04:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.580173 on epoch=24
05/22/2022 01:05:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.423585 on epoch=24
05/22/2022 01:05:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.471198 on epoch=24
05/22/2022 01:05:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.487806 on epoch=24
05/22/2022 01:05:19 - INFO - __main__ - Global step 800 Train loss 0.500267 Classification-F1 0.811101404261446 on epoch=24
05/22/2022 01:05:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.403892 on epoch=25
05/22/2022 01:05:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.438693 on epoch=25
05/22/2022 01:05:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.477585 on epoch=25
05/22/2022 01:05:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.430817 on epoch=26
05/22/2022 01:05:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.433946 on epoch=26
05/22/2022 01:05:50 - INFO - __main__ - Global step 850 Train loss 0.436986 Classification-F1 0.7795658133518668 on epoch=26
05/22/2022 01:05:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.471095 on epoch=26
05/22/2022 01:06:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.387666 on epoch=27
05/22/2022 01:06:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.362449 on epoch=27
05/22/2022 01:06:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.482354 on epoch=27
05/22/2022 01:06:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.364669 on epoch=28
05/22/2022 01:06:20 - INFO - __main__ - Global step 900 Train loss 0.413647 Classification-F1 0.8287692971803957 on epoch=28
05/22/2022 01:06:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.384397 on epoch=28
05/22/2022 01:06:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.551640 on epoch=28
05/22/2022 01:06:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.405508 on epoch=29
05/22/2022 01:06:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.398177 on epoch=29
05/22/2022 01:06:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.418937 on epoch=29
05/22/2022 01:06:51 - INFO - __main__ - Global step 950 Train loss 0.431732 Classification-F1 0.8055494356265781 on epoch=29
05/22/2022 01:06:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.326651 on epoch=29
05/22/2022 01:07:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.367201 on epoch=30
05/22/2022 01:07:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.369452 on epoch=30
05/22/2022 01:07:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.375305 on epoch=30
05/22/2022 01:07:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.327878 on epoch=31
05/22/2022 01:07:18 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 01:07:18 - INFO - __main__ - Printing 3 examples
05/22/2022 01:07:18 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/22/2022 01:07:18 - INFO - __main__ - ['others']
05/22/2022 01:07:18 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/22/2022 01:07:18 - INFO - __main__ - ['others']
05/22/2022 01:07:18 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/22/2022 01:07:18 - INFO - __main__ - ['others']
05/22/2022 01:07:18 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:07:18 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:07:19 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 01:07:19 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 01:07:19 - INFO - __main__ - Printing 3 examples
05/22/2022 01:07:19 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/22/2022 01:07:19 - INFO - __main__ - ['others']
05/22/2022 01:07:19 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/22/2022 01:07:19 - INFO - __main__ - ['others']
05/22/2022 01:07:19 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/22/2022 01:07:19 - INFO - __main__ - ['others']
05/22/2022 01:07:19 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:07:19 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:07:20 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 01:07:21 - INFO - __main__ - Global step 1000 Train loss 0.353297 Classification-F1 0.7926379339122619 on epoch=31
05/22/2022 01:07:21 - INFO - __main__ - save last model!
05/22/2022 01:07:28 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 01:07:29 - INFO - __main__ - Start tokenizing ... 5509 instances
05/22/2022 01:07:29 - INFO - __main__ - Printing 3 examples
05/22/2022 01:07:29 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/22/2022 01:07:29 - INFO - __main__ - ['others']
05/22/2022 01:07:29 - INFO - __main__ -  [emo] what you like very little things ok
05/22/2022 01:07:29 - INFO - __main__ - ['others']
05/22/2022 01:07:29 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/22/2022 01:07:29 - INFO - __main__ - ['others']
05/22/2022 01:07:29 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:07:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 01:07:30 - INFO - __main__ - Starting training!
05/22/2022 01:07:31 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:07:36 - INFO - __main__ - Loaded 5509 examples from test data
05/22/2022 01:08:19 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_42_0.0001_8_predictions.txt
05/22/2022 01:08:19 - INFO - __main__ - Classification-F1 on test data: 0.5361
05/22/2022 01:08:20 - INFO - __main__ - prefix=emo_128_42, lr=0.0001, bsz=8, dev_performance=0.8287692971803957, test_performance=0.5360590365369775
05/22/2022 01:08:20 - INFO - __main__ - Running ... prefix=emo_128_87, lr=0.0005, bsz=8 ...
05/22/2022 01:08:21 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 01:08:21 - INFO - __main__ - Printing 3 examples
05/22/2022 01:08:21 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/22/2022 01:08:21 - INFO - __main__ - ['others']
05/22/2022 01:08:21 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/22/2022 01:08:21 - INFO - __main__ - ['others']
05/22/2022 01:08:21 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/22/2022 01:08:21 - INFO - __main__ - ['others']
05/22/2022 01:08:21 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:08:21 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:08:21 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 01:08:21 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 01:08:21 - INFO - __main__ - Printing 3 examples
05/22/2022 01:08:21 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/22/2022 01:08:21 - INFO - __main__ - ['others']
05/22/2022 01:08:21 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/22/2022 01:08:21 - INFO - __main__ - ['others']
05/22/2022 01:08:21 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/22/2022 01:08:21 - INFO - __main__ - ['others']
05/22/2022 01:08:21 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:08:22 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:08:22 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 01:08:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 01:08:33 - INFO - __main__ - Starting training!
05/22/2022 01:08:38 - INFO - __main__ - Step 10 Global step 10 Train loss 25.144566 on epoch=0
05/22/2022 01:08:43 - INFO - __main__ - Step 20 Global step 20 Train loss 19.031321 on epoch=0
05/22/2022 01:08:48 - INFO - __main__ - Step 30 Global step 30 Train loss 16.725512 on epoch=0
05/22/2022 01:08:53 - INFO - __main__ - Step 40 Global step 40 Train loss 14.954096 on epoch=1
05/22/2022 01:08:58 - INFO - __main__ - Step 50 Global step 50 Train loss 13.882685 on epoch=1
05/22/2022 01:09:03 - INFO - __main__ - Global step 50 Train loss 17.947636 Classification-F1 0.0005167958656330749 on epoch=1
05/22/2022 01:09:09 - INFO - __main__ - Step 60 Global step 60 Train loss 12.157152 on epoch=1
05/22/2022 01:09:14 - INFO - __main__ - Step 70 Global step 70 Train loss 10.914074 on epoch=2
05/22/2022 01:09:19 - INFO - __main__ - Step 80 Global step 80 Train loss 6.932609 on epoch=2
05/22/2022 01:09:23 - INFO - __main__ - Step 90 Global step 90 Train loss 4.775598 on epoch=2
05/22/2022 01:09:28 - INFO - __main__ - Step 100 Global step 100 Train loss 6.457995 on epoch=3
05/22/2022 01:09:33 - INFO - __main__ - Global step 100 Train loss 8.247485 Classification-F1 0.11562065121285507 on epoch=3
05/22/2022 01:09:39 - INFO - __main__ - Step 110 Global step 110 Train loss 3.571079 on epoch=3
05/22/2022 01:09:44 - INFO - __main__ - Step 120 Global step 120 Train loss 1.492419 on epoch=3
05/22/2022 01:09:49 - INFO - __main__ - Step 130 Global step 130 Train loss 1.252383 on epoch=4
05/22/2022 01:09:54 - INFO - __main__ - Step 140 Global step 140 Train loss 1.409576 on epoch=4
05/22/2022 01:09:59 - INFO - __main__ - Step 150 Global step 150 Train loss 1.069440 on epoch=4
05/22/2022 01:10:03 - INFO - __main__ - Global step 150 Train loss 1.758979 Classification-F1 0.2684560151143503 on epoch=4
05/22/2022 01:10:09 - INFO - __main__ - Step 160 Global step 160 Train loss 1.060165 on epoch=4
05/22/2022 01:10:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.944737 on epoch=5
05/22/2022 01:10:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.883088 on epoch=5
05/22/2022 01:10:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.829091 on epoch=5
05/22/2022 01:10:29 - INFO - __main__ - Step 200 Global step 200 Train loss 0.743380 on epoch=6
05/22/2022 01:10:33 - INFO - __main__ - Global step 200 Train loss 0.892092 Classification-F1 0.48689208355791314 on epoch=6
05/22/2022 01:10:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.747183 on epoch=6
05/22/2022 01:10:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.769744 on epoch=6
05/22/2022 01:10:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.633877 on epoch=7
05/22/2022 01:10:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.735221 on epoch=7
05/22/2022 01:10:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.645231 on epoch=7
05/22/2022 01:11:03 - INFO - __main__ - Global step 250 Train loss 0.706251 Classification-F1 0.651883724297731 on epoch=7
05/22/2022 01:11:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.562532 on epoch=8
05/22/2022 01:11:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.706785 on epoch=8
05/22/2022 01:11:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.492057 on epoch=8
05/22/2022 01:11:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.398337 on epoch=9
05/22/2022 01:11:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.560959 on epoch=9
05/22/2022 01:11:33 - INFO - __main__ - Global step 300 Train loss 0.544134 Classification-F1 0.759785961396517 on epoch=9
05/22/2022 01:11:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.433057 on epoch=9
05/22/2022 01:11:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.408840 on epoch=9
05/22/2022 01:11:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.483157 on epoch=10
05/22/2022 01:11:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.380064 on epoch=10
05/22/2022 01:11:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.339706 on epoch=10
05/22/2022 01:12:04 - INFO - __main__ - Global step 350 Train loss 0.408965 Classification-F1 0.7987380929439376 on epoch=10
05/22/2022 01:12:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.332536 on epoch=11
05/22/2022 01:12:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.380829 on epoch=11
05/22/2022 01:12:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.280548 on epoch=11
05/22/2022 01:12:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.409558 on epoch=12
05/22/2022 01:12:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.411569 on epoch=12
05/22/2022 01:12:34 - INFO - __main__ - Global step 400 Train loss 0.363008 Classification-F1 0.7954994239337508 on epoch=12
05/22/2022 01:12:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.234932 on epoch=12
05/22/2022 01:12:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.228646 on epoch=13
05/22/2022 01:12:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.388348 on epoch=13
05/22/2022 01:12:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.221586 on epoch=13
05/22/2022 01:13:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.397157 on epoch=14
05/22/2022 01:13:05 - INFO - __main__ - Global step 450 Train loss 0.294134 Classification-F1 0.7951769913322291 on epoch=14
05/22/2022 01:13:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.477825 on epoch=14
05/22/2022 01:13:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.315044 on epoch=14
05/22/2022 01:13:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.399585 on epoch=14
05/22/2022 01:13:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.324982 on epoch=15
05/22/2022 01:13:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.216693 on epoch=15
05/22/2022 01:13:35 - INFO - __main__ - Global step 500 Train loss 0.346826 Classification-F1 0.8071699051542212 on epoch=15
05/22/2022 01:13:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.160576 on epoch=15
05/22/2022 01:13:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.204405 on epoch=16
05/22/2022 01:13:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.152803 on epoch=16
05/22/2022 01:13:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.141880 on epoch=16
05/22/2022 01:14:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.160805 on epoch=17
05/22/2022 01:14:05 - INFO - __main__ - Global step 550 Train loss 0.164094 Classification-F1 0.8359236249966644 on epoch=17
05/22/2022 01:14:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.242693 on epoch=17
05/22/2022 01:14:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.110707 on epoch=17
05/22/2022 01:14:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.113966 on epoch=18
05/22/2022 01:14:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.161506 on epoch=18
05/22/2022 01:14:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.101198 on epoch=18
05/22/2022 01:14:35 - INFO - __main__ - Global step 600 Train loss 0.146014 Classification-F1 0.4035349960199294 on epoch=18
05/22/2022 01:14:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.070102 on epoch=19
05/22/2022 01:14:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.090077 on epoch=19
05/22/2022 01:14:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.062568 on epoch=19
05/22/2022 01:14:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.086789 on epoch=19
05/22/2022 01:15:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.120512 on epoch=20
05/22/2022 01:15:05 - INFO - __main__ - Global step 650 Train loss 0.086010 Classification-F1 0.7639390888711187 on epoch=20
05/22/2022 01:15:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.129379 on epoch=20
05/22/2022 01:15:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.344611 on epoch=20
05/22/2022 01:15:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.166867 on epoch=21
05/22/2022 01:15:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.246614 on epoch=21
05/22/2022 01:15:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.121910 on epoch=21
05/22/2022 01:15:35 - INFO - __main__ - Global step 700 Train loss 0.201876 Classification-F1 0.7927964023597767 on epoch=21
05/22/2022 01:15:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.141230 on epoch=22
05/22/2022 01:15:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.126512 on epoch=22
05/22/2022 01:15:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.088085 on epoch=22
05/22/2022 01:15:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.091937 on epoch=23
05/22/2022 01:16:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.130545 on epoch=23
05/22/2022 01:16:05 - INFO - __main__ - Global step 750 Train loss 0.115662 Classification-F1 0.8444907664862341 on epoch=23
05/22/2022 01:16:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.085480 on epoch=23
05/22/2022 01:16:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.125121 on epoch=24
05/22/2022 01:16:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.068507 on epoch=24
05/22/2022 01:16:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.119401 on epoch=24
05/22/2022 01:16:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.060246 on epoch=24
05/22/2022 01:16:36 - INFO - __main__ - Global step 800 Train loss 0.091751 Classification-F1 0.6623221249182559 on epoch=24
05/22/2022 01:16:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.025450 on epoch=25
05/22/2022 01:16:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.131235 on epoch=25
05/22/2022 01:16:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.179055 on epoch=25
05/22/2022 01:16:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.054230 on epoch=26
05/22/2022 01:17:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.040765 on epoch=26
05/22/2022 01:17:07 - INFO - __main__ - Global step 850 Train loss 0.086147 Classification-F1 0.8114103556339294 on epoch=26
05/22/2022 01:17:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.019913 on epoch=26
05/22/2022 01:17:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.031653 on epoch=27
05/22/2022 01:17:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.084898 on epoch=27
05/22/2022 01:17:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.030122 on epoch=27
05/22/2022 01:17:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.023103 on epoch=28
05/22/2022 01:17:37 - INFO - __main__ - Global step 900 Train loss 0.037938 Classification-F1 0.6686416392451042 on epoch=28
05/22/2022 01:17:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.070674 on epoch=28
05/22/2022 01:17:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.052500 on epoch=28
05/22/2022 01:17:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.063862 on epoch=29
05/22/2022 01:17:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.073915 on epoch=29
05/22/2022 01:18:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.075921 on epoch=29
05/22/2022 01:18:07 - INFO - __main__ - Global step 950 Train loss 0.067374 Classification-F1 0.8127146425940761 on epoch=29
05/22/2022 01:18:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.007275 on epoch=29
05/22/2022 01:18:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.032963 on epoch=30
05/22/2022 01:18:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.026243 on epoch=30
05/22/2022 01:18:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.012825 on epoch=30
05/22/2022 01:18:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.013669 on epoch=31
05/22/2022 01:18:34 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 01:18:34 - INFO - __main__ - Printing 3 examples
05/22/2022 01:18:34 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/22/2022 01:18:34 - INFO - __main__ - ['others']
05/22/2022 01:18:34 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/22/2022 01:18:34 - INFO - __main__ - ['others']
05/22/2022 01:18:34 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/22/2022 01:18:34 - INFO - __main__ - ['others']
05/22/2022 01:18:34 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:18:34 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:18:34 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 01:18:34 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 01:18:34 - INFO - __main__ - Printing 3 examples
05/22/2022 01:18:34 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/22/2022 01:18:34 - INFO - __main__ - ['others']
05/22/2022 01:18:34 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/22/2022 01:18:34 - INFO - __main__ - ['others']
05/22/2022 01:18:34 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/22/2022 01:18:34 - INFO - __main__ - ['others']
05/22/2022 01:18:34 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:18:35 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:18:35 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 01:18:38 - INFO - __main__ - Global step 1000 Train loss 0.018595 Classification-F1 0.6643809786943491 on epoch=31
05/22/2022 01:18:38 - INFO - __main__ - save last model!
05/22/2022 01:18:45 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 01:18:45 - INFO - __main__ - Start tokenizing ... 5509 instances
05/22/2022 01:18:45 - INFO - __main__ - Printing 3 examples
05/22/2022 01:18:45 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/22/2022 01:18:45 - INFO - __main__ - ['others']
05/22/2022 01:18:45 - INFO - __main__ -  [emo] what you like very little things ok
05/22/2022 01:18:45 - INFO - __main__ - ['others']
05/22/2022 01:18:45 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/22/2022 01:18:45 - INFO - __main__ - ['others']
05/22/2022 01:18:45 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:18:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 01:18:46 - INFO - __main__ - Starting training!
05/22/2022 01:18:47 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:18:52 - INFO - __main__ - Loaded 5509 examples from test data
05/22/2022 01:19:45 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_87_0.0005_8_predictions.txt
05/22/2022 01:19:45 - INFO - __main__ - Classification-F1 on test data: 0.1076
05/22/2022 01:19:46 - INFO - __main__ - prefix=emo_128_87, lr=0.0005, bsz=8, dev_performance=0.8444907664862341, test_performance=0.10764148050038477
05/22/2022 01:19:46 - INFO - __main__ - Running ... prefix=emo_128_87, lr=0.0003, bsz=8 ...
05/22/2022 01:19:46 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 01:19:46 - INFO - __main__ - Printing 3 examples
05/22/2022 01:19:46 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/22/2022 01:19:46 - INFO - __main__ - ['others']
05/22/2022 01:19:46 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/22/2022 01:19:46 - INFO - __main__ - ['others']
05/22/2022 01:19:46 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/22/2022 01:19:46 - INFO - __main__ - ['others']
05/22/2022 01:19:46 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:19:47 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:19:47 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 01:19:47 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 01:19:47 - INFO - __main__ - Printing 3 examples
05/22/2022 01:19:47 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/22/2022 01:19:47 - INFO - __main__ - ['others']
05/22/2022 01:19:47 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/22/2022 01:19:47 - INFO - __main__ - ['others']
05/22/2022 01:19:47 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/22/2022 01:19:47 - INFO - __main__ - ['others']
05/22/2022 01:19:47 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:19:47 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:19:48 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 01:19:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 01:19:59 - INFO - __main__ - Starting training!
05/22/2022 01:20:03 - INFO - __main__ - Step 10 Global step 10 Train loss 23.759495 on epoch=0
05/22/2022 01:20:08 - INFO - __main__ - Step 20 Global step 20 Train loss 18.777302 on epoch=0
05/22/2022 01:20:14 - INFO - __main__ - Step 30 Global step 30 Train loss 17.583998 on epoch=0
05/22/2022 01:20:19 - INFO - __main__ - Step 40 Global step 40 Train loss 16.086153 on epoch=1
05/22/2022 01:20:24 - INFO - __main__ - Step 50 Global step 50 Train loss 14.701851 on epoch=1
05/22/2022 01:20:49 - INFO - __main__ - Global step 50 Train loss 18.181759 Classification-F1 0.0 on epoch=1
05/22/2022 01:20:55 - INFO - __main__ - Step 60 Global step 60 Train loss 15.288971 on epoch=1
05/22/2022 01:21:00 - INFO - __main__ - Step 70 Global step 70 Train loss 12.620194 on epoch=2
05/22/2022 01:21:05 - INFO - __main__ - Step 80 Global step 80 Train loss 12.645597 on epoch=2
05/22/2022 01:21:10 - INFO - __main__ - Step 90 Global step 90 Train loss 11.283509 on epoch=2
05/22/2022 01:21:15 - INFO - __main__ - Step 100 Global step 100 Train loss 11.435132 on epoch=3
05/22/2022 01:22:02 - INFO - __main__ - Global step 100 Train loss 12.654680 Classification-F1 0.00024813895781637717 on epoch=3
05/22/2022 01:22:08 - INFO - __main__ - Step 110 Global step 110 Train loss 10.269639 on epoch=3
05/22/2022 01:22:13 - INFO - __main__ - Step 120 Global step 120 Train loss 7.661139 on epoch=3
05/22/2022 01:22:18 - INFO - __main__ - Step 130 Global step 130 Train loss 4.413821 on epoch=4
05/22/2022 01:22:23 - INFO - __main__ - Step 140 Global step 140 Train loss 4.258883 on epoch=4
05/22/2022 01:22:28 - INFO - __main__ - Step 150 Global step 150 Train loss 3.652859 on epoch=4
05/22/2022 01:22:32 - INFO - __main__ - Global step 150 Train loss 6.051268 Classification-F1 0.1109503494183985 on epoch=4
05/22/2022 01:22:37 - INFO - __main__ - Step 160 Global step 160 Train loss 3.712788 on epoch=4
05/22/2022 01:22:42 - INFO - __main__ - Step 170 Global step 170 Train loss 3.551630 on epoch=5
05/22/2022 01:22:47 - INFO - __main__ - Step 180 Global step 180 Train loss 3.807051 on epoch=5
05/22/2022 01:22:52 - INFO - __main__ - Step 190 Global step 190 Train loss 3.595099 on epoch=5
05/22/2022 01:22:57 - INFO - __main__ - Step 200 Global step 200 Train loss 2.920035 on epoch=6
05/22/2022 01:23:01 - INFO - __main__ - Global step 200 Train loss 3.517321 Classification-F1 0.11958439405600617 on epoch=6
05/22/2022 01:23:07 - INFO - __main__ - Step 210 Global step 210 Train loss 3.071553 on epoch=6
05/22/2022 01:23:13 - INFO - __main__ - Step 220 Global step 220 Train loss 2.765978 on epoch=6
05/22/2022 01:23:18 - INFO - __main__ - Step 230 Global step 230 Train loss 2.689287 on epoch=7
05/22/2022 01:23:23 - INFO - __main__ - Step 240 Global step 240 Train loss 2.604248 on epoch=7
05/22/2022 01:23:28 - INFO - __main__ - Step 250 Global step 250 Train loss 2.233697 on epoch=7
05/22/2022 01:23:31 - INFO - __main__ - Global step 250 Train loss 2.672952 Classification-F1 0.24502382954864033 on epoch=7
05/22/2022 01:23:37 - INFO - __main__ - Step 260 Global step 260 Train loss 2.589364 on epoch=8
05/22/2022 01:23:42 - INFO - __main__ - Step 270 Global step 270 Train loss 1.958961 on epoch=8
05/22/2022 01:23:47 - INFO - __main__ - Step 280 Global step 280 Train loss 1.552147 on epoch=8
05/22/2022 01:23:52 - INFO - __main__ - Step 290 Global step 290 Train loss 1.802904 on epoch=9
05/22/2022 01:23:57 - INFO - __main__ - Step 300 Global step 300 Train loss 1.885879 on epoch=9
05/22/2022 01:24:01 - INFO - __main__ - Global step 300 Train loss 1.957851 Classification-F1 0.1649872119765646 on epoch=9
05/22/2022 01:24:06 - INFO - __main__ - Step 310 Global step 310 Train loss 1.717518 on epoch=9
05/22/2022 01:24:11 - INFO - __main__ - Step 320 Global step 320 Train loss 1.400067 on epoch=9
05/22/2022 01:24:16 - INFO - __main__ - Step 330 Global step 330 Train loss 1.262620 on epoch=10
05/22/2022 01:24:21 - INFO - __main__ - Step 340 Global step 340 Train loss 1.770398 on epoch=10
05/22/2022 01:24:26 - INFO - __main__ - Step 350 Global step 350 Train loss 1.118082 on epoch=10
05/22/2022 01:24:30 - INFO - __main__ - Global step 350 Train loss 1.453737 Classification-F1 0.13270052803620822 on epoch=10
05/22/2022 01:24:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.970608 on epoch=11
05/22/2022 01:24:40 - INFO - __main__ - Step 370 Global step 370 Train loss 1.110283 on epoch=11
05/22/2022 01:24:45 - INFO - __main__ - Step 380 Global step 380 Train loss 1.127673 on epoch=11
05/22/2022 01:24:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.918300 on epoch=12
05/22/2022 01:24:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.872043 on epoch=12
05/22/2022 01:24:59 - INFO - __main__ - Global step 400 Train loss 0.999781 Classification-F1 0.14887474089428487 on epoch=12
05/22/2022 01:25:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.981471 on epoch=12
05/22/2022 01:25:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.847418 on epoch=13
05/22/2022 01:25:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.916625 on epoch=13
05/22/2022 01:25:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.886687 on epoch=13
05/22/2022 01:25:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.770350 on epoch=14
05/22/2022 01:25:28 - INFO - __main__ - Global step 450 Train loss 0.880510 Classification-F1 0.6555168481045851 on epoch=14
05/22/2022 01:25:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.810713 on epoch=14
05/22/2022 01:25:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.708522 on epoch=14
05/22/2022 01:25:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.737574 on epoch=14
05/22/2022 01:25:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.878073 on epoch=15
05/22/2022 01:25:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.632162 on epoch=15
05/22/2022 01:25:57 - INFO - __main__ - Global step 500 Train loss 0.753409 Classification-F1 0.685621630880962 on epoch=15
05/22/2022 01:26:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.639167 on epoch=15
05/22/2022 01:26:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.592230 on epoch=16
05/22/2022 01:26:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.669182 on epoch=16
05/22/2022 01:26:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.667611 on epoch=16
05/22/2022 01:26:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.521091 on epoch=17
05/22/2022 01:26:27 - INFO - __main__ - Global step 550 Train loss 0.617856 Classification-F1 0.7775510394105041 on epoch=17
05/22/2022 01:26:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.488319 on epoch=17
05/22/2022 01:26:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.440964 on epoch=17
05/22/2022 01:26:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.436956 on epoch=18
05/22/2022 01:26:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.531183 on epoch=18
05/22/2022 01:26:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.408660 on epoch=18
05/22/2022 01:26:57 - INFO - __main__ - Global step 600 Train loss 0.461216 Classification-F1 0.8178099188858847 on epoch=18
05/22/2022 01:27:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.325417 on epoch=19
05/22/2022 01:27:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.447576 on epoch=19
05/22/2022 01:27:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.333278 on epoch=19
05/22/2022 01:27:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.349483 on epoch=19
05/22/2022 01:27:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.311653 on epoch=20
05/22/2022 01:27:26 - INFO - __main__ - Global step 650 Train loss 0.353481 Classification-F1 0.8372298928047681 on epoch=20
05/22/2022 01:27:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.243326 on epoch=20
05/22/2022 01:27:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.306395 on epoch=20
05/22/2022 01:27:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.398125 on epoch=21
05/22/2022 01:27:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.220997 on epoch=21
05/22/2022 01:27:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.230235 on epoch=21
05/22/2022 01:27:56 - INFO - __main__ - Global step 700 Train loss 0.279816 Classification-F1 0.802513216984225 on epoch=21
05/22/2022 01:28:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.285950 on epoch=22
05/22/2022 01:28:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.271772 on epoch=22
05/22/2022 01:28:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.185308 on epoch=22
05/22/2022 01:28:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.315532 on epoch=23
05/22/2022 01:28:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.235431 on epoch=23
05/22/2022 01:28:25 - INFO - __main__ - Global step 750 Train loss 0.258799 Classification-F1 0.8328069812694753 on epoch=23
05/22/2022 01:28:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.196587 on epoch=23
05/22/2022 01:28:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.253582 on epoch=24
05/22/2022 01:28:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.173080 on epoch=24
05/22/2022 01:28:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.144944 on epoch=24
05/22/2022 01:28:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.173703 on epoch=24
05/22/2022 01:28:54 - INFO - __main__ - Global step 800 Train loss 0.188379 Classification-F1 0.829337914650922 on epoch=24
05/22/2022 01:28:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.180686 on epoch=25
05/22/2022 01:29:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.140552 on epoch=25
05/22/2022 01:29:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.131647 on epoch=25
05/22/2022 01:29:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.183791 on epoch=26
05/22/2022 01:29:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.110814 on epoch=26
05/22/2022 01:29:24 - INFO - __main__ - Global step 850 Train loss 0.149498 Classification-F1 0.8131824242946607 on epoch=26
05/22/2022 01:29:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.106755 on epoch=26
05/22/2022 01:29:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.147376 on epoch=27
05/22/2022 01:29:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.141530 on epoch=27
05/22/2022 01:29:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.055592 on epoch=27
05/22/2022 01:29:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.148964 on epoch=28
05/22/2022 01:29:53 - INFO - __main__ - Global step 900 Train loss 0.120043 Classification-F1 0.8116710498072937 on epoch=28
05/22/2022 01:29:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.058877 on epoch=28
05/22/2022 01:30:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.091454 on epoch=28
05/22/2022 01:30:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.137399 on epoch=29
05/22/2022 01:30:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.219686 on epoch=29
05/22/2022 01:30:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.780669 on epoch=29
05/22/2022 01:30:23 - INFO - __main__ - Global step 950 Train loss 0.257617 Classification-F1 0.3975570126413072 on epoch=29
05/22/2022 01:30:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.680930 on epoch=29
05/22/2022 01:30:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.459080 on epoch=30
05/22/2022 01:30:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.278267 on epoch=30
05/22/2022 01:30:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.660618 on epoch=30
05/22/2022 01:30:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.456006 on epoch=31
05/22/2022 01:30:49 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 01:30:49 - INFO - __main__ - Printing 3 examples
05/22/2022 01:30:49 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/22/2022 01:30:49 - INFO - __main__ - ['others']
05/22/2022 01:30:49 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/22/2022 01:30:49 - INFO - __main__ - ['others']
05/22/2022 01:30:49 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/22/2022 01:30:49 - INFO - __main__ - ['others']
05/22/2022 01:30:49 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:30:49 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:30:50 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 01:30:50 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 01:30:50 - INFO - __main__ - Printing 3 examples
05/22/2022 01:30:50 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/22/2022 01:30:50 - INFO - __main__ - ['others']
05/22/2022 01:30:50 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/22/2022 01:30:50 - INFO - __main__ - ['others']
05/22/2022 01:30:50 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/22/2022 01:30:50 - INFO - __main__ - ['others']
05/22/2022 01:30:50 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:30:50 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:30:50 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 01:30:52 - INFO - __main__ - Global step 1000 Train loss 0.506980 Classification-F1 0.6648581930061122 on epoch=31
05/22/2022 01:30:52 - INFO - __main__ - save last model!
05/22/2022 01:30:59 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 01:30:59 - INFO - __main__ - Start tokenizing ... 5509 instances
05/22/2022 01:30:59 - INFO - __main__ - Printing 3 examples
05/22/2022 01:30:59 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/22/2022 01:30:59 - INFO - __main__ - ['others']
05/22/2022 01:30:59 - INFO - __main__ -  [emo] what you like very little things ok
05/22/2022 01:30:59 - INFO - __main__ - ['others']
05/22/2022 01:30:59 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/22/2022 01:30:59 - INFO - __main__ - ['others']
05/22/2022 01:30:59 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:31:01 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:31:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 01:31:03 - INFO - __main__ - Starting training!
05/22/2022 01:31:07 - INFO - __main__ - Loaded 5509 examples from test data
05/22/2022 01:31:48 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_87_0.0003_8_predictions.txt
05/22/2022 01:31:48 - INFO - __main__ - Classification-F1 on test data: 0.5788
05/22/2022 01:31:48 - INFO - __main__ - prefix=emo_128_87, lr=0.0003, bsz=8, dev_performance=0.8372298928047681, test_performance=0.5787660021863856
05/22/2022 01:31:48 - INFO - __main__ - Running ... prefix=emo_128_87, lr=0.0002, bsz=8 ...
05/22/2022 01:31:49 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 01:31:49 - INFO - __main__ - Printing 3 examples
05/22/2022 01:31:49 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/22/2022 01:31:49 - INFO - __main__ - ['others']
05/22/2022 01:31:49 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/22/2022 01:31:49 - INFO - __main__ - ['others']
05/22/2022 01:31:49 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/22/2022 01:31:49 - INFO - __main__ - ['others']
05/22/2022 01:31:49 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:31:49 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:31:50 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 01:31:50 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 01:31:50 - INFO - __main__ - Printing 3 examples
05/22/2022 01:31:50 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/22/2022 01:31:50 - INFO - __main__ - ['others']
05/22/2022 01:31:50 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/22/2022 01:31:50 - INFO - __main__ - ['others']
05/22/2022 01:31:50 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/22/2022 01:31:50 - INFO - __main__ - ['others']
05/22/2022 01:31:50 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:31:50 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:31:51 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 01:32:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 01:32:01 - INFO - __main__ - Starting training!
05/22/2022 01:32:06 - INFO - __main__ - Step 10 Global step 10 Train loss 24.073988 on epoch=0
05/22/2022 01:32:11 - INFO - __main__ - Step 20 Global step 20 Train loss 20.973454 on epoch=0
05/22/2022 01:32:15 - INFO - __main__ - Step 30 Global step 30 Train loss 19.881208 on epoch=0
05/22/2022 01:32:20 - INFO - __main__ - Step 40 Global step 40 Train loss 17.578041 on epoch=1
05/22/2022 01:32:25 - INFO - __main__ - Step 50 Global step 50 Train loss 17.244621 on epoch=1
05/22/2022 01:32:40 - INFO - __main__ - Global step 50 Train loss 19.950262 Classification-F1 0.0 on epoch=1
05/22/2022 01:32:45 - INFO - __main__ - Step 60 Global step 60 Train loss 17.205389 on epoch=1
05/22/2022 01:32:50 - INFO - __main__ - Step 70 Global step 70 Train loss 15.996617 on epoch=2
05/22/2022 01:32:55 - INFO - __main__ - Step 80 Global step 80 Train loss 15.842955 on epoch=2
05/22/2022 01:33:00 - INFO - __main__ - Step 90 Global step 90 Train loss 14.509867 on epoch=2
05/22/2022 01:33:05 - INFO - __main__ - Step 100 Global step 100 Train loss 14.592669 on epoch=3
05/22/2022 01:33:11 - INFO - __main__ - Global step 100 Train loss 15.629499 Classification-F1 0.0 on epoch=3
05/22/2022 01:33:16 - INFO - __main__ - Step 110 Global step 110 Train loss 13.554232 on epoch=3
05/22/2022 01:33:21 - INFO - __main__ - Step 120 Global step 120 Train loss 13.213567 on epoch=3
05/22/2022 01:33:26 - INFO - __main__ - Step 130 Global step 130 Train loss 11.987440 on epoch=4
05/22/2022 01:33:31 - INFO - __main__ - Step 140 Global step 140 Train loss 12.153924 on epoch=4
05/22/2022 01:33:36 - INFO - __main__ - Step 150 Global step 150 Train loss 11.448647 on epoch=4
05/22/2022 01:33:42 - INFO - __main__ - Global step 150 Train loss 12.471562 Classification-F1 0.0 on epoch=4
05/22/2022 01:33:46 - INFO - __main__ - Step 160 Global step 160 Train loss 10.660957 on epoch=4
05/22/2022 01:33:51 - INFO - __main__ - Step 170 Global step 170 Train loss 9.682968 on epoch=5
05/22/2022 01:33:56 - INFO - __main__ - Step 180 Global step 180 Train loss 8.306746 on epoch=5
05/22/2022 01:34:01 - INFO - __main__ - Step 190 Global step 190 Train loss 7.049153 on epoch=5
05/22/2022 01:34:06 - INFO - __main__ - Step 200 Global step 200 Train loss 3.468472 on epoch=6
05/22/2022 01:34:10 - INFO - __main__ - Global step 200 Train loss 7.833660 Classification-F1 0.18338815789473684 on epoch=6
05/22/2022 01:34:16 - INFO - __main__ - Step 210 Global step 210 Train loss 4.787536 on epoch=6
05/22/2022 01:34:21 - INFO - __main__ - Step 220 Global step 220 Train loss 4.485737 on epoch=6
05/22/2022 01:34:26 - INFO - __main__ - Step 230 Global step 230 Train loss 3.668113 on epoch=7
05/22/2022 01:34:31 - INFO - __main__ - Step 240 Global step 240 Train loss 3.223760 on epoch=7
05/22/2022 01:34:36 - INFO - __main__ - Step 250 Global step 250 Train loss 1.733745 on epoch=7
05/22/2022 01:34:40 - INFO - __main__ - Global step 250 Train loss 3.579778 Classification-F1 0.29078539170976647 on epoch=7
05/22/2022 01:34:46 - INFO - __main__ - Step 260 Global step 260 Train loss 1.223734 on epoch=8
05/22/2022 01:34:51 - INFO - __main__ - Step 270 Global step 270 Train loss 1.978620 on epoch=8
05/22/2022 01:34:56 - INFO - __main__ - Step 280 Global step 280 Train loss 2.558764 on epoch=8
05/22/2022 01:35:01 - INFO - __main__ - Step 290 Global step 290 Train loss 3.185588 on epoch=9
05/22/2022 01:35:06 - INFO - __main__ - Step 300 Global step 300 Train loss 3.633305 on epoch=9
05/22/2022 01:35:10 - INFO - __main__ - Global step 300 Train loss 2.516002 Classification-F1 0.20845967125036893 on epoch=9
05/22/2022 01:35:15 - INFO - __main__ - Step 310 Global step 310 Train loss 3.422201 on epoch=9
05/22/2022 01:35:20 - INFO - __main__ - Step 320 Global step 320 Train loss 3.731689 on epoch=9
05/22/2022 01:35:25 - INFO - __main__ - Step 330 Global step 330 Train loss 2.455914 on epoch=10
05/22/2022 01:35:30 - INFO - __main__ - Step 340 Global step 340 Train loss 2.997851 on epoch=10
05/22/2022 01:35:35 - INFO - __main__ - Step 350 Global step 350 Train loss 3.382871 on epoch=10
05/22/2022 01:35:38 - INFO - __main__ - Global step 350 Train loss 3.198106 Classification-F1 0.39346942521501993 on epoch=10
05/22/2022 01:35:45 - INFO - __main__ - Step 360 Global step 360 Train loss 2.869899 on epoch=11
05/22/2022 01:35:50 - INFO - __main__ - Step 370 Global step 370 Train loss 3.162630 on epoch=11
05/22/2022 01:35:55 - INFO - __main__ - Step 380 Global step 380 Train loss 2.936018 on epoch=11
05/22/2022 01:36:00 - INFO - __main__ - Step 390 Global step 390 Train loss 2.599758 on epoch=12
05/22/2022 01:36:05 - INFO - __main__ - Step 400 Global step 400 Train loss 2.441630 on epoch=12
05/22/2022 01:36:09 - INFO - __main__ - Global step 400 Train loss 2.801987 Classification-F1 0.16969952112824288 on epoch=12
05/22/2022 01:36:13 - INFO - __main__ - Step 410 Global step 410 Train loss 2.996446 on epoch=12
05/22/2022 01:36:18 - INFO - __main__ - Step 420 Global step 420 Train loss 1.904812 on epoch=13
05/22/2022 01:36:23 - INFO - __main__ - Step 430 Global step 430 Train loss 2.527172 on epoch=13
05/22/2022 01:36:28 - INFO - __main__ - Step 440 Global step 440 Train loss 2.460038 on epoch=13
05/22/2022 01:36:33 - INFO - __main__ - Step 450 Global step 450 Train loss 2.722633 on epoch=14
05/22/2022 01:36:37 - INFO - __main__ - Global step 450 Train loss 2.522220 Classification-F1 0.1 on epoch=14
05/22/2022 01:36:42 - INFO - __main__ - Step 460 Global step 460 Train loss 2.302598 on epoch=14
05/22/2022 01:36:47 - INFO - __main__ - Step 470 Global step 470 Train loss 2.315370 on epoch=14
05/22/2022 01:36:52 - INFO - __main__ - Step 480 Global step 480 Train loss 2.061021 on epoch=14
05/22/2022 01:36:57 - INFO - __main__ - Step 490 Global step 490 Train loss 2.311676 on epoch=15
05/22/2022 01:37:02 - INFO - __main__ - Step 500 Global step 500 Train loss 1.886946 on epoch=15
05/22/2022 01:37:06 - INFO - __main__ - Global step 500 Train loss 2.175523 Classification-F1 0.14382910855475245 on epoch=15
05/22/2022 01:37:11 - INFO - __main__ - Step 510 Global step 510 Train loss 1.868714 on epoch=15
05/22/2022 01:37:16 - INFO - __main__ - Step 520 Global step 520 Train loss 1.844083 on epoch=16
05/22/2022 01:37:21 - INFO - __main__ - Step 530 Global step 530 Train loss 1.855312 on epoch=16
05/22/2022 01:37:26 - INFO - __main__ - Step 540 Global step 540 Train loss 1.738542 on epoch=16
05/22/2022 01:37:31 - INFO - __main__ - Step 550 Global step 550 Train loss 1.674311 on epoch=17
05/22/2022 01:37:34 - INFO - __main__ - Global step 550 Train loss 1.796193 Classification-F1 0.3507798043880518 on epoch=17
05/22/2022 01:37:39 - INFO - __main__ - Step 560 Global step 560 Train loss 1.515677 on epoch=17
05/22/2022 01:37:44 - INFO - __main__ - Step 570 Global step 570 Train loss 1.297564 on epoch=17
05/22/2022 01:37:49 - INFO - __main__ - Step 580 Global step 580 Train loss 1.093745 on epoch=18
05/22/2022 01:37:54 - INFO - __main__ - Step 590 Global step 590 Train loss 1.479881 on epoch=18
05/22/2022 01:37:59 - INFO - __main__ - Step 600 Global step 600 Train loss 1.361295 on epoch=18
05/22/2022 01:38:03 - INFO - __main__ - Global step 600 Train loss 1.349632 Classification-F1 0.2935153900825543 on epoch=18
05/22/2022 01:38:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.703981 on epoch=19
05/22/2022 01:38:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.923073 on epoch=19
05/22/2022 01:38:18 - INFO - __main__ - Step 630 Global step 630 Train loss 1.112141 on epoch=19
05/22/2022 01:38:23 - INFO - __main__ - Step 640 Global step 640 Train loss 1.296409 on epoch=19
05/22/2022 01:38:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.981495 on epoch=20
05/22/2022 01:38:32 - INFO - __main__ - Global step 650 Train loss 1.003420 Classification-F1 0.5719349359203635 on epoch=20
05/22/2022 01:38:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.994437 on epoch=20
05/22/2022 01:38:43 - INFO - __main__ - Step 670 Global step 670 Train loss 1.247393 on epoch=20
05/22/2022 01:38:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.880960 on epoch=21
05/22/2022 01:38:53 - INFO - __main__ - Step 690 Global step 690 Train loss 1.020505 on epoch=21
05/22/2022 01:38:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.906740 on epoch=21
05/22/2022 01:39:01 - INFO - __main__ - Global step 700 Train loss 1.010007 Classification-F1 0.4130285522942458 on epoch=21
05/22/2022 01:39:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.876362 on epoch=22
05/22/2022 01:39:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.913826 on epoch=22
05/22/2022 01:39:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.805754 on epoch=22
05/22/2022 01:39:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.849886 on epoch=23
05/22/2022 01:39:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.821093 on epoch=23
05/22/2022 01:39:30 - INFO - __main__ - Global step 750 Train loss 0.853384 Classification-F1 0.3383133568399699 on epoch=23
05/22/2022 01:39:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.769530 on epoch=23
05/22/2022 01:39:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.765762 on epoch=24
05/22/2022 01:39:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.800534 on epoch=24
05/22/2022 01:39:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.915450 on epoch=24
05/22/2022 01:39:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.798250 on epoch=24
05/22/2022 01:39:59 - INFO - __main__ - Global step 800 Train loss 0.809905 Classification-F1 0.15479829259484887 on epoch=24
05/22/2022 01:40:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.805826 on epoch=25
05/22/2022 01:40:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.732627 on epoch=25
05/22/2022 01:40:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.735029 on epoch=25
05/22/2022 01:40:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.753723 on epoch=26
05/22/2022 01:40:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.767424 on epoch=26
05/22/2022 01:40:28 - INFO - __main__ - Global step 850 Train loss 0.758926 Classification-F1 0.2701758666084214 on epoch=26
05/22/2022 01:40:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.802228 on epoch=26
05/22/2022 01:40:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.735310 on epoch=27
05/22/2022 01:40:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.764781 on epoch=27
05/22/2022 01:40:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.661632 on epoch=27
05/22/2022 01:40:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.710031 on epoch=28
05/22/2022 01:40:56 - INFO - __main__ - Global step 900 Train loss 0.734796 Classification-F1 0.38665323228795995 on epoch=28
05/22/2022 01:41:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.800932 on epoch=28
05/22/2022 01:41:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.718877 on epoch=28
05/22/2022 01:41:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.668049 on epoch=29
05/22/2022 01:41:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.717542 on epoch=29
05/22/2022 01:41:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.684455 on epoch=29
05/22/2022 01:41:25 - INFO - __main__ - Global step 950 Train loss 0.717971 Classification-F1 0.5049424475562028 on epoch=29
05/22/2022 01:41:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.684761 on epoch=29
05/22/2022 01:41:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.717895 on epoch=30
05/22/2022 01:41:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.708493 on epoch=30
05/22/2022 01:41:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.778942 on epoch=30
05/22/2022 01:41:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.678298 on epoch=31
05/22/2022 01:41:51 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 01:41:51 - INFO - __main__ - Printing 3 examples
05/22/2022 01:41:51 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/22/2022 01:41:51 - INFO - __main__ - ['others']
05/22/2022 01:41:51 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/22/2022 01:41:51 - INFO - __main__ - ['others']
05/22/2022 01:41:51 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/22/2022 01:41:51 - INFO - __main__ - ['others']
05/22/2022 01:41:51 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:41:51 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:41:52 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 01:41:52 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 01:41:52 - INFO - __main__ - Printing 3 examples
05/22/2022 01:41:52 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/22/2022 01:41:52 - INFO - __main__ - ['others']
05/22/2022 01:41:52 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/22/2022 01:41:52 - INFO - __main__ - ['others']
05/22/2022 01:41:52 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/22/2022 01:41:52 - INFO - __main__ - ['others']
05/22/2022 01:41:52 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:41:52 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:41:53 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 01:41:54 - INFO - __main__ - Global step 1000 Train loss 0.713678 Classification-F1 0.3642138803782271 on epoch=31
05/22/2022 01:41:54 - INFO - __main__ - save last model!
05/22/2022 01:42:01 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 01:42:01 - INFO - __main__ - Start tokenizing ... 5509 instances
05/22/2022 01:42:01 - INFO - __main__ - Printing 3 examples
05/22/2022 01:42:01 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/22/2022 01:42:01 - INFO - __main__ - ['others']
05/22/2022 01:42:01 - INFO - __main__ -  [emo] what you like very little things ok
05/22/2022 01:42:01 - INFO - __main__ - ['others']
05/22/2022 01:42:01 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/22/2022 01:42:01 - INFO - __main__ - ['others']
05/22/2022 01:42:01 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:42:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 01:42:03 - INFO - __main__ - Starting training!
05/22/2022 01:42:03 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:42:09 - INFO - __main__ - Loaded 5509 examples from test data
05/22/2022 01:42:52 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_87_0.0002_8_predictions.txt
05/22/2022 01:42:52 - INFO - __main__ - Classification-F1 on test data: 0.3901
05/22/2022 01:42:52 - INFO - __main__ - prefix=emo_128_87, lr=0.0002, bsz=8, dev_performance=0.5719349359203635, test_performance=0.39011087485957896
05/22/2022 01:42:52 - INFO - __main__ - Running ... prefix=emo_128_87, lr=0.0001, bsz=8 ...
05/22/2022 01:42:53 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 01:42:53 - INFO - __main__ - Printing 3 examples
05/22/2022 01:42:53 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/22/2022 01:42:53 - INFO - __main__ - ['others']
05/22/2022 01:42:53 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/22/2022 01:42:53 - INFO - __main__ - ['others']
05/22/2022 01:42:53 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/22/2022 01:42:53 - INFO - __main__ - ['others']
05/22/2022 01:42:53 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:42:53 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:42:54 - INFO - __main__ - Loaded 512 examples from train data
05/22/2022 01:42:54 - INFO - __main__ - Start tokenizing ... 512 instances
05/22/2022 01:42:54 - INFO - __main__ - Printing 3 examples
05/22/2022 01:42:54 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/22/2022 01:42:54 - INFO - __main__ - ['others']
05/22/2022 01:42:54 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/22/2022 01:42:54 - INFO - __main__ - ['others']
05/22/2022 01:42:54 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/22/2022 01:42:54 - INFO - __main__ - ['others']
05/22/2022 01:42:54 - INFO - __main__ - Tokenizing Input ...
05/22/2022 01:42:54 - INFO - __main__ - Tokenizing Output ...
05/22/2022 01:42:55 - INFO - __main__ - Loaded 512 examples from dev data
05/22/2022 01:43:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 01:43:07 - INFO - __main__ - Starting training!
05/22/2022 01:43:11 - INFO - __main__ - Step 10 Global step 10 Train loss 26.012894 on epoch=0
05/22/2022 01:43:16 - INFO - __main__ - Step 20 Global step 20 Train loss 21.940746 on epoch=0
05/22/2022 01:43:21 - INFO - __main__ - Step 30 Global step 30 Train loss 19.788624 on epoch=0
05/22/2022 01:43:26 - INFO - __main__ - Step 40 Global step 40 Train loss 18.655815 on epoch=1
05/22/2022 01:43:31 - INFO - __main__ - Step 50 Global step 50 Train loss 19.166626 on epoch=1
05/22/2022 01:46:19 - INFO - __main__ - Global step 50 Train loss 21.112942 Classification-F1 0.0 on epoch=1
05/22/2022 01:46:24 - INFO - __main__ - Step 60 Global step 60 Train loss 18.114603 on epoch=1
05/22/2022 01:46:30 - INFO - __main__ - Step 70 Global step 70 Train loss 17.311096 on epoch=2
05/22/2022 01:46:35 - INFO - __main__ - Step 80 Global step 80 Train loss 16.684566 on epoch=2
05/22/2022 01:46:40 - INFO - __main__ - Step 90 Global step 90 Train loss 16.003584 on epoch=2
05/22/2022 01:46:45 - INFO - __main__ - Step 100 Global step 100 Train loss 16.855801 on epoch=3
05/22/2022 01:49:08 - INFO - __main__ - Global step 100 Train loss 16.993929 Classification-F1 0.0 on epoch=3
05/22/2022 01:49:13 - INFO - __main__ - Step 110 Global step 110 Train loss 16.163523 on epoch=3
05/22/2022 01:49:18 - INFO - __main__ - Step 120 Global step 120 Train loss 14.870427 on epoch=3
05/22/2022 01:49:23 - INFO - __main__ - Step 130 Global step 130 Train loss 14.336382 on epoch=4
05/22/2022 01:49:28 - INFO - __main__ - Step 140 Global step 140 Train loss 14.804927 on epoch=4
05/22/2022 01:49:33 - INFO - __main__ - Step 150 Global step 150 Train loss 14.323359 on epoch=4
05/22/2022 01:51:38 - INFO - __main__ - Global step 150 Train loss 14.899725 Classification-F1 0.0 on epoch=4
05/22/2022 01:51:43 - INFO - __main__ - Step 160 Global step 160 Train loss 14.149241 on epoch=4
05/22/2022 01:51:48 - INFO - __main__ - Step 170 Global step 170 Train loss 13.701571 on epoch=5
05/22/2022 01:51:53 - INFO - __main__ - Step 180 Global step 180 Train loss 13.553862 on epoch=5
05/22/2022 01:51:58 - INFO - __main__ - Step 190 Global step 190 Train loss 13.539106 on epoch=5
05/22/2022 01:52:02 - INFO - __main__ - Step 200 Global step 200 Train loss 13.641449 on epoch=6
05/22/2022 01:53:42 - INFO - __main__ - Global step 200 Train loss 13.717045 Classification-F1 5.895009874141539e-05 on epoch=6
05/22/2022 01:53:48 - INFO - __main__ - Step 210 Global step 210 Train loss 12.754764 on epoch=6
05/22/2022 01:53:53 - INFO - __main__ - Step 220 Global step 220 Train loss 12.652638 on epoch=6
05/22/2022 01:53:58 - INFO - __main__ - Step 230 Global step 230 Train loss 12.208747 on epoch=7
05/22/2022 01:54:03 - INFO - __main__ - Step 240 Global step 240 Train loss 12.134951 on epoch=7
05/22/2022 01:54:08 - INFO - __main__ - Step 250 Global step 250 Train loss 11.250214 on epoch=7
05/22/2022 01:54:55 - INFO - __main__ - Global step 250 Train loss 12.200264 Classification-F1 0.0003213410190154376 on epoch=7
05/22/2022 01:55:00 - INFO - __main__ - Step 260 Global step 260 Train loss 10.905564 on epoch=8
05/22/2022 01:55:06 - INFO - __main__ - Step 270 Global step 270 Train loss 10.728740 on epoch=8
05/22/2022 01:55:11 - INFO - __main__ - Step 280 Global step 280 Train loss 10.785299 on epoch=8
05/22/2022 01:55:16 - INFO - __main__ - Step 290 Global step 290 Train loss 9.955301 on epoch=9
05/22/2022 01:55:21 - INFO - __main__ - Step 300 Global step 300 Train loss 10.605471 on epoch=9
05/22/2022 01:55:50 - INFO - __main__ - Global step 300 Train loss 10.596074 Classification-F1 0.00029988826556900613 on epoch=9
05/22/2022 01:55:55 - INFO - __main__ - Step 310 Global step 310 Train loss 8.838925 on epoch=9
05/22/2022 01:56:00 - INFO - __main__ - Step 320 Global step 320 Train loss 9.595263 on epoch=9
05/22/2022 01:56:06 - INFO - __main__ - Step 330 Global step 330 Train loss 8.219397 on epoch=10
05/22/2022 01:56:11 - INFO - __main__ - Step 340 Global step 340 Train loss 7.664060 on epoch=10
05/22/2022 01:56:16 - INFO - __main__ - Step 350 Global step 350 Train loss 7.441897 on epoch=10
05/22/2022 01:56:48 - INFO - __main__ - Global step 350 Train loss 8.351909 Classification-F1 0.000709005354250635 on epoch=10
05/22/2022 01:56:54 - INFO - __main__ - Step 360 Global step 360 Train loss 6.708821 on epoch=11
05/22/2022 01:56:59 - INFO - __main__ - Step 370 Global step 370 Train loss 6.501025 on epoch=11
05/22/2022 01:57:04 - INFO - __main__ - Step 380 Global step 380 Train loss 5.857657 on epoch=11
05/22/2022 01:57:09 - INFO - __main__ - Step 390 Global step 390 Train loss 5.275104 on epoch=12
05/22/2022 01:57:14 - INFO - __main__ - Step 400 Global step 400 Train loss 4.381539 on epoch=12
05/22/2022 01:57:17 - INFO - __main__ - Global step 400 Train loss 5.744829 Classification-F1 0.11958439405600617 on epoch=12
05/22/2022 01:57:23 - INFO - __main__ - Step 410 Global step 410 Train loss 3.935689 on epoch=12
05/22/2022 01:57:28 - INFO - __main__ - Step 420 Global step 420 Train loss 3.154421 on epoch=13
05/22/2022 01:57:33 - INFO - __main__ - Step 430 Global step 430 Train loss 4.022316 on epoch=13
05/22/2022 01:57:38 - INFO - __main__ - Step 440 Global step 440 Train loss 3.364531 on epoch=13
05/22/2022 01:57:43 - INFO - __main__ - Step 450 Global step 450 Train loss 3.510223 on epoch=14
05/22/2022 01:57:47 - INFO - __main__ - Global step 450 Train loss 3.597436 Classification-F1 0.220469693815454 on epoch=14
05/22/2022 01:57:53 - INFO - __main__ - Step 460 Global step 460 Train loss 3.680134 on epoch=14
05/22/2022 01:57:58 - INFO - __main__ - Step 470 Global step 470 Train loss 3.829386 on epoch=14
05/22/2022 01:58:03 - INFO - __main__ - Step 480 Global step 480 Train loss 3.604717 on epoch=14
05/22/2022 01:58:08 - INFO - __main__ - Step 490 Global step 490 Train loss 2.701486 on epoch=15
05/22/2022 01:58:13 - INFO - __main__ - Step 500 Global step 500 Train loss 3.210262 on epoch=15
05/22/2022 01:58:17 - INFO - __main__ - Global step 500 Train loss 3.405197 Classification-F1 0.11958439405600617 on epoch=15
05/22/2022 01:58:22 - INFO - __main__ - Step 510 Global step 510 Train loss 3.188735 on epoch=15
05/22/2022 01:58:27 - INFO - __main__ - Step 520 Global step 520 Train loss 2.878468 on epoch=16
05/22/2022 01:58:32 - INFO - __main__ - Step 530 Global step 530 Train loss 3.348219 on epoch=16
05/22/2022 01:58:37 - INFO - __main__ - Step 540 Global step 540 Train loss 2.754500 on epoch=16
05/22/2022 01:58:42 - INFO - __main__ - Step 550 Global step 550 Train loss 2.827332 on epoch=17
05/22/2022 01:58:46 - INFO - __main__ - Global step 550 Train loss 2.999450 Classification-F1 0.16510850712805109 on epoch=17
05/22/2022 01:58:51 - INFO - __main__ - Step 560 Global step 560 Train loss 3.542842 on epoch=17
05/22/2022 01:58:56 - INFO - __main__ - Step 570 Global step 570 Train loss 3.698152 on epoch=17
05/22/2022 01:59:01 - INFO - __main__ - Step 580 Global step 580 Train loss 2.876083 on epoch=18
05/22/2022 01:59:06 - INFO - __main__ - Step 590 Global step 590 Train loss 2.894169 on epoch=18
05/22/2022 01:59:11 - INFO - __main__ - Step 600 Global step 600 Train loss 2.345996 on epoch=18
05/22/2022 01:59:16 - INFO - __main__ - Global step 600 Train loss 3.071448 Classification-F1 0.17360192776749242 on epoch=18
05/22/2022 01:59:21 - INFO - __main__ - Step 610 Global step 610 Train loss 2.937930 on epoch=19
05/22/2022 01:59:26 - INFO - __main__ - Step 620 Global step 620 Train loss 2.404525 on epoch=19
05/22/2022 01:59:31 - INFO - __main__ - Step 630 Global step 630 Train loss 2.545669 on epoch=19
05/22/2022 01:59:36 - INFO - __main__ - Step 640 Global step 640 Train loss 2.492907 on epoch=19
05/22/2022 01:59:41 - INFO - __main__ - Step 650 Global step 650 Train loss 2.053742 on epoch=20
05/22/2022 01:59:45 - INFO - __main__ - Global step 650 Train loss 2.486955 Classification-F1 0.18942376260196223 on epoch=20
05/22/2022 01:59:50 - INFO - __main__ - Step 660 Global step 660 Train loss 2.618330 on epoch=20
05/22/2022 01:59:55 - INFO - __main__ - Step 670 Global step 670 Train loss 2.716428 on epoch=20
05/22/2022 02:00:00 - INFO - __main__ - Step 680 Global step 680 Train loss 2.712319 on epoch=21
05/22/2022 02:00:05 - INFO - __main__ - Step 690 Global step 690 Train loss 1.942077 on epoch=21
05/22/2022 02:00:10 - INFO - __main__ - Step 700 Global step 700 Train loss 2.324302 on epoch=21
05/22/2022 02:00:14 - INFO - __main__ - Global step 700 Train loss 2.462691 Classification-F1 0.2630825125638455 on epoch=21
05/22/2022 02:00:20 - INFO - __main__ - Step 710 Global step 710 Train loss 2.130722 on epoch=22
05/22/2022 02:00:25 - INFO - __main__ - Step 720 Global step 720 Train loss 2.059635 on epoch=22
05/22/2022 02:00:30 - INFO - __main__ - Step 730 Global step 730 Train loss 2.236314 on epoch=22
05/22/2022 02:00:35 - INFO - __main__ - Step 740 Global step 740 Train loss 1.494412 on epoch=23
05/22/2022 02:00:40 - INFO - __main__ - Step 750 Global step 750 Train loss 2.096424 on epoch=23
05/22/2022 02:00:44 - INFO - __main__ - Global step 750 Train loss 2.003501 Classification-F1 0.29654864788504637 on epoch=23
05/22/2022 02:00:50 - INFO - __main__ - Step 760 Global step 760 Train loss 1.952865 on epoch=23
05/22/2022 02:00:55 - INFO - __main__ - Step 770 Global step 770 Train loss 2.025910 on epoch=24
05/22/2022 02:01:00 - INFO - __main__ - Step 780 Global step 780 Train loss 1.606586 on epoch=24
05/22/2022 02:01:05 - INFO - __main__ - Step 790 Global step 790 Train loss 1.574600 on epoch=24
05/22/2022 02:01:11 - INFO - __main__ - Step 800 Global step 800 Train loss 2.161865 on epoch=24
05/22/2022 02:01:17 - INFO - __main__ - Global step 800 Train loss 1.864365 Classification-F1 0.2462093699165102 on epoch=24
05/22/2022 02:01:22 - INFO - __main__ - Step 810 Global step 810 Train loss 1.714009 on epoch=25
05/22/2022 02:01:27 - INFO - __main__ - Step 820 Global step 820 Train loss 1.083854 on epoch=25
05/22/2022 02:01:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.806381 on epoch=25
05/22/2022 02:01:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.842470 on epoch=26
05/22/2022 02:01:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.719916 on epoch=26
05/22/2022 02:01:46 - INFO - __main__ - Global step 850 Train loss 1.033326 Classification-F1 0.5628644426600482 on epoch=26
05/22/2022 02:01:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.711787 on epoch=26
05/22/2022 02:01:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.544381 on epoch=27
05/22/2022 02:02:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.717311 on epoch=27
05/22/2022 02:02:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.625356 on epoch=27
05/22/2022 02:02:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.640396 on epoch=28
05/22/2022 02:02:17 - INFO - __main__ - Global step 900 Train loss 0.647846 Classification-F1 0.6660219171682753 on epoch=28
05/22/2022 02:02:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.864615 on epoch=28
05/22/2022 02:02:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.784641 on epoch=28
05/22/2022 02:02:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.700480 on epoch=29
05/22/2022 02:02:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.609953 on epoch=29
05/22/2022 02:02:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.676778 on epoch=29
05/22/2022 02:02:47 - INFO - __main__ - Global step 950 Train loss 0.727293 Classification-F1 0.7480665106951873 on epoch=29
05/22/2022 02:02:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.425330 on epoch=29
05/22/2022 02:02:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.645698 on epoch=30
05/22/2022 02:03:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.505866 on epoch=30
05/22/2022 02:03:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.506664 on epoch=30
05/22/2022 02:03:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.536374 on epoch=31
05/22/2022 02:03:17 - INFO - __main__ - Global step 1000 Train loss 0.523986 Classification-F1 0.7699564833644899 on epoch=31
05/22/2022 02:03:18 - INFO - __main__ - save last model!
05/22/2022 02:03:25 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 02:03:26 - INFO - __main__ - Start tokenizing ... 5509 instances
05/22/2022 02:03:26 - INFO - __main__ - Printing 3 examples
05/22/2022 02:03:26 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/22/2022 02:03:26 - INFO - __main__ - ['others']
05/22/2022 02:03:26 - INFO - __main__ -  [emo] what you like very little things ok
05/22/2022 02:03:26 - INFO - __main__ - ['others']
05/22/2022 02:03:26 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/22/2022 02:03:26 - INFO - __main__ - ['others']
05/22/2022 02:03:26 - INFO - __main__ - Tokenizing Input ...
05/22/2022 02:03:28 - INFO - __main__ - Tokenizing Output ...
05/22/2022 02:03:33 - INFO - __main__ - Loaded 5509 examples from test data
05/22/2022 02:04:14 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-emo/emo_128_87_0.0001_8_predictions.txt
05/22/2022 02:04:14 - INFO - __main__ - Classification-F1 on test data: 0.4479
05/22/2022 02:04:15 - INFO - __main__ - prefix=emo_128_87, lr=0.0001, bsz=8, dev_performance=0.7699564833644899, test_performance=0.4478995313412252
