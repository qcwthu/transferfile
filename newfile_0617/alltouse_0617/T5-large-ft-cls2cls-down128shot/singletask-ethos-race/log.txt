05/23/2022 01:48:00 - INFO - __main__ - Namespace(task_dir='data_128/ethos-race/', task_name='ethos-race', identifier='T5-large-ft-cls2cls-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/23/2022 01:48:00 - INFO - __main__ - models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race
05/23/2022 01:48:00 - INFO - __main__ - Namespace(task_dir='data_128/ethos-race/', task_name='ethos-race', identifier='T5-large-ft-cls2cls-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/23/2022 01:48:00 - INFO - __main__ - models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race
05/23/2022 01:48:01 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/23/2022 01:48:01 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/23/2022 01:48:01 - INFO - __main__ - args.device: cuda:0
05/23/2022 01:48:01 - INFO - __main__ - args.device: cuda:1
05/23/2022 01:48:01 - INFO - __main__ - Using 2 gpus
05/23/2022 01:48:01 - INFO - __main__ - Using 2 gpus
05/23/2022 01:48:01 - INFO - __main__ - Fine-tuning the following samples: ['ethos-race_128_100', 'ethos-race_128_13', 'ethos-race_128_21', 'ethos-race_128_42', 'ethos-race_128_87']
05/23/2022 01:48:01 - INFO - __main__ - Fine-tuning the following samples: ['ethos-race_128_100', 'ethos-race_128_13', 'ethos-race_128_21', 'ethos-race_128_42', 'ethos-race_128_87']
05/23/2022 01:48:06 - INFO - __main__ - Running ... prefix=ethos-race_128_100, lr=0.0005, bsz=8 ...
05/23/2022 01:48:06 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 01:48:06 - INFO - __main__ - Printing 3 examples
05/23/2022 01:48:06 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
05/23/2022 01:48:06 - INFO - __main__ - ['false']
05/23/2022 01:48:06 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
05/23/2022 01:48:06 - INFO - __main__ - ['false']
05/23/2022 01:48:06 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
05/23/2022 01:48:06 - INFO - __main__ - ['false']
05/23/2022 01:48:06 - INFO - __main__ - Tokenizing Input ...
05/23/2022 01:48:06 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 01:48:06 - INFO - __main__ - Printing 3 examples
05/23/2022 01:48:06 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
05/23/2022 01:48:06 - INFO - __main__ - ['false']
05/23/2022 01:48:06 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
05/23/2022 01:48:06 - INFO - __main__ - ['false']
05/23/2022 01:48:06 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
05/23/2022 01:48:06 - INFO - __main__ - ['false']
05/23/2022 01:48:06 - INFO - __main__ - Tokenizing Input ...
05/23/2022 01:48:07 - INFO - __main__ - Tokenizing Output ...
05/23/2022 01:48:07 - INFO - __main__ - Tokenizing Output ...
05/23/2022 01:48:07 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 01:48:07 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 01:48:07 - INFO - __main__ - Printing 3 examples
05/23/2022 01:48:07 - INFO - __main__ -  [ethos-race] WOW THIS GUY TALKS LIKE A RETARD HAHAHAHAHAH
05/23/2022 01:48:07 - INFO - __main__ - ['false']
05/23/2022 01:48:07 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 01:48:07 - INFO - __main__ - ['false']
05/23/2022 01:48:07 - INFO - __main__ -  [ethos-race] Hindus and Muslims are same shit. They only know how to kill Peoples...See comments of Christians people no rudeness no abusing or bad words... Christians religion is the best religion in the world
05/23/2022 01:48:07 - INFO - __main__ - ['false']
05/23/2022 01:48:07 - INFO - __main__ - Tokenizing Input ...
05/23/2022 01:48:07 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 01:48:07 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 01:48:07 - INFO - __main__ - Printing 3 examples
05/23/2022 01:48:07 - INFO - __main__ -  [ethos-race] WOW THIS GUY TALKS LIKE A RETARD HAHAHAHAHAH
05/23/2022 01:48:07 - INFO - __main__ - ['false']
05/23/2022 01:48:07 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 01:48:07 - INFO - __main__ - ['false']
05/23/2022 01:48:07 - INFO - __main__ -  [ethos-race] Hindus and Muslims are same shit. They only know how to kill Peoples...See comments of Christians people no rudeness no abusing or bad words... Christians religion is the best religion in the world
05/23/2022 01:48:07 - INFO - __main__ - ['false']
05/23/2022 01:48:07 - INFO - __main__ - Tokenizing Input ...
05/23/2022 01:48:07 - INFO - __main__ - Tokenizing Output ...
05/23/2022 01:48:07 - INFO - __main__ - Tokenizing Output ...
05/23/2022 01:48:07 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 01:48:07 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 01:48:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 01:48:20 - INFO - __main__ - Starting training!
05/23/2022 01:48:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 01:48:20 - INFO - __main__ - Starting training!
05/23/2022 01:48:25 - INFO - __main__ - Step 10 Global step 10 Train loss 23.077784 on epoch=0
05/23/2022 01:48:30 - INFO - __main__ - Step 20 Global step 20 Train loss 18.823645 on epoch=1
05/23/2022 01:48:35 - INFO - __main__ - Step 30 Global step 30 Train loss 16.845915 on epoch=2
05/23/2022 01:48:40 - INFO - __main__ - Step 40 Global step 40 Train loss 14.391123 on epoch=3
05/23/2022 01:48:45 - INFO - __main__ - Step 50 Global step 50 Train loss 11.832415 on epoch=4
05/23/2022 01:48:55 - INFO - __main__ - Global step 50 Train loss 16.994177 Classification-F1 0.0 on epoch=4
05/23/2022 01:49:01 - INFO - __main__ - Step 60 Global step 60 Train loss 8.159586 on epoch=4
05/23/2022 01:49:06 - INFO - __main__ - Step 70 Global step 70 Train loss 3.109670 on epoch=5
05/23/2022 01:49:12 - INFO - __main__ - Step 80 Global step 80 Train loss 2.225667 on epoch=6
05/23/2022 01:49:17 - INFO - __main__ - Step 90 Global step 90 Train loss 2.532327 on epoch=7
05/23/2022 01:49:22 - INFO - __main__ - Step 100 Global step 100 Train loss 2.982418 on epoch=8
05/23/2022 01:49:23 - INFO - __main__ - Global step 100 Train loss 3.801934 Classification-F1 1.0 on epoch=8
05/23/2022 01:49:30 - INFO - __main__ - Step 110 Global step 110 Train loss 1.984395 on epoch=9
05/23/2022 01:49:35 - INFO - __main__ - Step 120 Global step 120 Train loss 1.861181 on epoch=9
05/23/2022 01:49:40 - INFO - __main__ - Step 130 Global step 130 Train loss 1.027244 on epoch=10
05/23/2022 01:49:45 - INFO - __main__ - Step 140 Global step 140 Train loss 1.807461 on epoch=11
05/23/2022 01:49:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.599998 on epoch=12
05/23/2022 01:49:51 - INFO - __main__ - Global step 150 Train loss 1.456056 Classification-F1 0.2808988764044944 on epoch=12
05/23/2022 01:49:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.516575 on epoch=13
05/23/2022 01:50:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.480276 on epoch=14
05/23/2022 01:50:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.438471 on epoch=14
05/23/2022 01:50:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.408110 on epoch=15
05/23/2022 01:50:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.461930 on epoch=16
05/23/2022 01:50:18 - INFO - __main__ - Global step 200 Train loss 0.461072 Classification-F1 1.0 on epoch=16
05/23/2022 01:50:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.437021 on epoch=17
05/23/2022 01:50:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.453440 on epoch=18
05/23/2022 01:50:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.399718 on epoch=19
05/23/2022 01:50:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.379883 on epoch=19
05/23/2022 01:50:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.378095 on epoch=20
05/23/2022 01:50:45 - INFO - __main__ - Global step 250 Train loss 0.409631 Classification-F1 0.015384615384615385 on epoch=20
05/23/2022 01:50:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.374541 on epoch=21
05/23/2022 01:50:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.429969 on epoch=22
05/23/2022 01:51:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.390776 on epoch=23
05/23/2022 01:51:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.449525 on epoch=24
05/23/2022 01:51:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.392206 on epoch=24
05/23/2022 01:51:11 - INFO - __main__ - Global step 300 Train loss 0.407404 Classification-F1 0.07246376811594203 on epoch=24
05/23/2022 01:51:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.352960 on epoch=25
05/23/2022 01:51:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.351916 on epoch=26
05/23/2022 01:51:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.383649 on epoch=27
05/23/2022 01:51:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.385088 on epoch=28
05/23/2022 01:51:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.423769 on epoch=29
05/23/2022 01:51:38 - INFO - __main__ - Global step 350 Train loss 0.379477 Classification-F1 0.49206349206349204 on epoch=29
05/23/2022 01:51:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.394340 on epoch=29
05/23/2022 01:51:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.336541 on epoch=30
05/23/2022 01:51:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.388644 on epoch=31
05/23/2022 01:51:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.350748 on epoch=32
05/23/2022 01:52:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.341877 on epoch=33
05/23/2022 01:52:05 - INFO - __main__ - Global step 400 Train loss 0.362430 Classification-F1 1.0 on epoch=33
05/23/2022 01:52:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.382296 on epoch=34
05/23/2022 01:52:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.380738 on epoch=34
05/23/2022 01:52:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.340881 on epoch=35
05/23/2022 01:52:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.386510 on epoch=36
05/23/2022 01:52:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.313034 on epoch=37
05/23/2022 01:52:32 - INFO - __main__ - Global step 450 Train loss 0.360692 Classification-F1 1.0 on epoch=37
05/23/2022 01:52:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.335358 on epoch=38
05/23/2022 01:52:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.320896 on epoch=39
05/23/2022 01:52:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.369040 on epoch=39
05/23/2022 01:52:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.302702 on epoch=40
05/23/2022 01:52:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.341058 on epoch=41
05/23/2022 01:52:59 - INFO - __main__ - Global step 500 Train loss 0.333811 Classification-F1 1.0 on epoch=41
05/23/2022 01:53:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.329788 on epoch=42
05/23/2022 01:53:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.317778 on epoch=43
05/23/2022 01:53:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.314383 on epoch=44
05/23/2022 01:53:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.303692 on epoch=44
05/23/2022 01:53:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.286667 on epoch=45
05/23/2022 01:53:26 - INFO - __main__ - Global step 550 Train loss 0.310462 Classification-F1 0.15789473684210525 on epoch=45
05/23/2022 01:53:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.287480 on epoch=46
05/23/2022 01:53:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.244376 on epoch=47
05/23/2022 01:53:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.269309 on epoch=48
05/23/2022 01:53:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.245940 on epoch=49
05/23/2022 01:53:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.167472 on epoch=49
05/23/2022 01:53:53 - INFO - __main__ - Global step 600 Train loss 0.242916 Classification-F1 0.4311111111111111 on epoch=49
05/23/2022 01:53:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.192923 on epoch=50
05/23/2022 01:54:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.284068 on epoch=51
05/23/2022 01:54:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.231438 on epoch=52
05/23/2022 01:54:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.186277 on epoch=53
05/23/2022 01:54:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.200912 on epoch=54
05/23/2022 01:54:20 - INFO - __main__ - Global step 650 Train loss 0.219123 Classification-F1 0.49606299212598426 on epoch=54
05/23/2022 01:54:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.226491 on epoch=54
05/23/2022 01:54:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.140702 on epoch=55
05/23/2022 01:54:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.180143 on epoch=56
05/23/2022 01:54:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.195534 on epoch=57
05/23/2022 01:54:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.118048 on epoch=58
05/23/2022 01:54:46 - INFO - __main__ - Global step 700 Train loss 0.172184 Classification-F1 0.4666666666666667 on epoch=58
05/23/2022 01:54:51 - INFO - __main__ - Step 710 Global step 710 Train loss 0.170107 on epoch=59
05/23/2022 01:54:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.175666 on epoch=59
05/23/2022 01:55:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.185659 on epoch=60
05/23/2022 01:55:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.190919 on epoch=61
05/23/2022 01:55:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.138555 on epoch=62
05/23/2022 01:55:13 - INFO - __main__ - Global step 750 Train loss 0.172181 Classification-F1 0.4980392156862745 on epoch=62
05/23/2022 01:55:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.128520 on epoch=63
05/23/2022 01:55:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.117347 on epoch=64
05/23/2022 01:55:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.140912 on epoch=64
05/23/2022 01:55:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.120814 on epoch=65
05/23/2022 01:55:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.209086 on epoch=66
05/23/2022 01:55:40 - INFO - __main__ - Global step 800 Train loss 0.143336 Classification-F1 0.4838709677419355 on epoch=66
05/23/2022 01:55:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.139149 on epoch=67
05/23/2022 01:55:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.101212 on epoch=68
05/23/2022 01:55:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.124035 on epoch=69
05/23/2022 01:56:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.078019 on epoch=69
05/23/2022 01:56:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.076343 on epoch=70
05/23/2022 01:56:08 - INFO - __main__ - Global step 850 Train loss 0.103752 Classification-F1 0.4838709677419355 on epoch=70
05/23/2022 01:56:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.096306 on epoch=71
05/23/2022 01:56:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.118313 on epoch=72
05/23/2022 01:56:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.087158 on epoch=73
05/23/2022 01:56:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.050074 on epoch=74
05/23/2022 01:56:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.090968 on epoch=74
05/23/2022 01:56:35 - INFO - __main__ - Global step 900 Train loss 0.088564 Classification-F1 0.459915611814346 on epoch=74
05/23/2022 01:56:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.100392 on epoch=75
05/23/2022 01:56:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.120638 on epoch=76
05/23/2022 01:56:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.087720 on epoch=77
05/23/2022 01:56:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.068221 on epoch=78
05/23/2022 01:57:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.080396 on epoch=79
05/23/2022 01:57:02 - INFO - __main__ - Global step 950 Train loss 0.091474 Classification-F1 0.488 on epoch=79
05/23/2022 01:57:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.068876 on epoch=79
05/23/2022 01:57:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.050986 on epoch=80
05/23/2022 01:57:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.053541 on epoch=81
05/23/2022 01:57:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.024208 on epoch=82
05/23/2022 01:57:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.033653 on epoch=83
05/23/2022 01:57:29 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 01:57:29 - INFO - __main__ - Printing 3 examples
05/23/2022 01:57:29 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
05/23/2022 01:57:29 - INFO - __main__ - ['false']
05/23/2022 01:57:29 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
05/23/2022 01:57:29 - INFO - __main__ - ['false']
05/23/2022 01:57:29 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
05/23/2022 01:57:29 - INFO - __main__ - ['false']
05/23/2022 01:57:29 - INFO - __main__ - Tokenizing Input ...
05/23/2022 01:57:29 - INFO - __main__ - Global step 1000 Train loss 0.046253 Classification-F1 0.46443514644351463 on epoch=83
05/23/2022 01:57:29 - INFO - __main__ - save last model!
05/23/2022 01:57:29 - INFO - __main__ - Tokenizing Output ...
05/23/2022 01:57:29 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 01:57:29 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 01:57:29 - INFO - __main__ - Printing 3 examples
05/23/2022 01:57:29 - INFO - __main__ -  [ethos-race] WOW THIS GUY TALKS LIKE A RETARD HAHAHAHAHAH
05/23/2022 01:57:29 - INFO - __main__ - ['false']
05/23/2022 01:57:29 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 01:57:29 - INFO - __main__ - ['false']
05/23/2022 01:57:29 - INFO - __main__ -  [ethos-race] Hindus and Muslims are same shit. They only know how to kill Peoples...See comments of Christians people no rudeness no abusing or bad words... Christians religion is the best religion in the world
05/23/2022 01:57:29 - INFO - __main__ - ['false']
05/23/2022 01:57:29 - INFO - __main__ - Tokenizing Input ...
05/23/2022 01:57:29 - INFO - __main__ - Tokenizing Output ...
05/23/2022 01:57:29 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 01:57:36 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 01:57:36 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 01:57:36 - INFO - __main__ - Printing 3 examples
05/23/2022 01:57:36 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 01:57:36 - INFO - __main__ - ['true']
05/23/2022 01:57:36 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 01:57:36 - INFO - __main__ - ['false']
05/23/2022 01:57:36 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 01:57:36 - INFO - __main__ - ['false']
05/23/2022 01:57:36 - INFO - __main__ - Tokenizing Input ...
05/23/2022 01:57:36 - INFO - __main__ - Tokenizing Output ...
05/23/2022 01:57:36 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 01:57:38 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_100_0.0005_8_predictions.txt
05/23/2022 01:57:38 - INFO - __main__ - Classification-F1 on test data: 0.4494
05/23/2022 01:57:38 - INFO - __main__ - prefix=ethos-race_128_100, lr=0.0005, bsz=8, dev_performance=1.0, test_performance=0.44936708860759494
05/23/2022 01:57:38 - INFO - __main__ - Running ... prefix=ethos-race_128_100, lr=0.0003, bsz=8 ...
05/23/2022 01:57:39 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 01:57:39 - INFO - __main__ - Printing 3 examples
05/23/2022 01:57:39 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
05/23/2022 01:57:39 - INFO - __main__ - ['false']
05/23/2022 01:57:39 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
05/23/2022 01:57:39 - INFO - __main__ - ['false']
05/23/2022 01:57:39 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
05/23/2022 01:57:39 - INFO - __main__ - ['false']
05/23/2022 01:57:39 - INFO - __main__ - Tokenizing Input ...
05/23/2022 01:57:39 - INFO - __main__ - Tokenizing Output ...
05/23/2022 01:57:39 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 01:57:39 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 01:57:39 - INFO - __main__ - Printing 3 examples
05/23/2022 01:57:39 - INFO - __main__ -  [ethos-race] WOW THIS GUY TALKS LIKE A RETARD HAHAHAHAHAH
05/23/2022 01:57:39 - INFO - __main__ - ['false']
05/23/2022 01:57:39 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 01:57:39 - INFO - __main__ - ['false']
05/23/2022 01:57:39 - INFO - __main__ -  [ethos-race] Hindus and Muslims are same shit. They only know how to kill Peoples...See comments of Christians people no rudeness no abusing or bad words... Christians religion is the best religion in the world
05/23/2022 01:57:39 - INFO - __main__ - ['false']
05/23/2022 01:57:39 - INFO - __main__ - Tokenizing Input ...
05/23/2022 01:57:40 - INFO - __main__ - Tokenizing Output ...
05/23/2022 01:57:40 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 01:57:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 01:57:40 - INFO - __main__ - Starting training!
05/23/2022 01:57:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 01:57:50 - INFO - __main__ - Starting training!
05/23/2022 01:57:55 - INFO - __main__ - Step 10 Global step 10 Train loss 23.646450 on epoch=0
05/23/2022 01:58:00 - INFO - __main__ - Step 20 Global step 20 Train loss 20.149181 on epoch=1
05/23/2022 01:58:05 - INFO - __main__ - Step 30 Global step 30 Train loss 17.123770 on epoch=2
05/23/2022 01:58:10 - INFO - __main__ - Step 40 Global step 40 Train loss 15.630981 on epoch=3
05/23/2022 01:58:15 - INFO - __main__ - Step 50 Global step 50 Train loss 14.551852 on epoch=4
05/23/2022 01:58:50 - INFO - __main__ - Global step 50 Train loss 18.220448 Classification-F1 0.0 on epoch=4
05/23/2022 01:58:56 - INFO - __main__ - Step 60 Global step 60 Train loss 13.331354 on epoch=4
05/23/2022 01:59:01 - INFO - __main__ - Step 70 Global step 70 Train loss 11.735654 on epoch=5
05/23/2022 01:59:06 - INFO - __main__ - Step 80 Global step 80 Train loss 10.616453 on epoch=6
05/23/2022 01:59:11 - INFO - __main__ - Step 90 Global step 90 Train loss 7.485519 on epoch=7
05/23/2022 01:59:16 - INFO - __main__ - Step 100 Global step 100 Train loss 4.666218 on epoch=8
05/23/2022 01:59:21 - INFO - __main__ - Global step 100 Train loss 9.567039 Classification-F1 0.459915611814346 on epoch=8
05/23/2022 01:59:28 - INFO - __main__ - Step 110 Global step 110 Train loss 2.397849 on epoch=9
05/23/2022 01:59:33 - INFO - __main__ - Step 120 Global step 120 Train loss 1.552430 on epoch=9
05/23/2022 01:59:38 - INFO - __main__ - Step 130 Global step 130 Train loss 1.558046 on epoch=10
05/23/2022 01:59:43 - INFO - __main__ - Step 140 Global step 140 Train loss 1.307584 on epoch=11
05/23/2022 01:59:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.916736 on epoch=12
05/23/2022 01:59:50 - INFO - __main__ - Global step 150 Train loss 1.546529 Classification-F1 0.36 on epoch=12
05/23/2022 01:59:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.535560 on epoch=13
05/23/2022 02:00:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.497173 on epoch=14
05/23/2022 02:00:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.435976 on epoch=14
05/23/2022 02:00:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.338022 on epoch=15
05/23/2022 02:00:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.382999 on epoch=16
05/23/2022 02:00:17 - INFO - __main__ - Global step 200 Train loss 0.437946 Classification-F1 1.0 on epoch=16
05/23/2022 02:00:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.321146 on epoch=17
05/23/2022 02:00:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.362489 on epoch=18
05/23/2022 02:00:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.387325 on epoch=19
05/23/2022 02:00:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.287852 on epoch=19
05/23/2022 02:00:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.310623 on epoch=20
05/23/2022 02:00:45 - INFO - __main__ - Global step 250 Train loss 0.333887 Classification-F1 0.38461538461538464 on epoch=20
05/23/2022 02:00:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.315220 on epoch=21
05/23/2022 02:00:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.271811 on epoch=22
05/23/2022 02:01:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.286673 on epoch=23
05/23/2022 02:01:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.233370 on epoch=24
05/23/2022 02:01:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.299564 on epoch=24
05/23/2022 02:01:12 - INFO - __main__ - Global step 300 Train loss 0.281327 Classification-F1 0.4775510204081633 on epoch=24
05/23/2022 02:01:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.248050 on epoch=25
05/23/2022 02:01:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.259064 on epoch=26
05/23/2022 02:01:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.233763 on epoch=27
05/23/2022 02:01:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.274994 on epoch=28
05/23/2022 02:01:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.237784 on epoch=29
05/23/2022 02:01:39 - INFO - __main__ - Global step 350 Train loss 0.250731 Classification-F1 0.49407114624505927 on epoch=29
05/23/2022 02:01:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.258977 on epoch=29
05/23/2022 02:01:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.208789 on epoch=30
05/23/2022 02:01:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.232766 on epoch=31
05/23/2022 02:02:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.193715 on epoch=32
05/23/2022 02:02:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.181201 on epoch=33
05/23/2022 02:02:06 - INFO - __main__ - Global step 400 Train loss 0.215090 Classification-F1 0.459915611814346 on epoch=33
05/23/2022 02:02:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.209599 on epoch=34
05/23/2022 02:02:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.183004 on epoch=34
05/23/2022 02:02:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.129174 on epoch=35
05/23/2022 02:02:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.170603 on epoch=36
05/23/2022 02:02:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.178280 on epoch=37
05/23/2022 02:02:33 - INFO - __main__ - Global step 450 Train loss 0.174132 Classification-F1 0.49407114624505927 on epoch=37
05/23/2022 02:02:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.163014 on epoch=38
05/23/2022 02:02:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.283859 on epoch=39
05/23/2022 02:02:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.188115 on epoch=39
05/23/2022 02:02:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.150900 on epoch=40
05/23/2022 02:02:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.151900 on epoch=41
05/23/2022 02:03:00 - INFO - __main__ - Global step 500 Train loss 0.187558 Classification-F1 0.49407114624505927 on epoch=41
05/23/2022 02:03:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.136020 on epoch=42
05/23/2022 02:03:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.115894 on epoch=43
05/23/2022 02:03:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.096065 on epoch=44
05/23/2022 02:03:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.154877 on epoch=44
05/23/2022 02:03:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.073052 on epoch=45
05/23/2022 02:03:28 - INFO - __main__ - Global step 550 Train loss 0.115182 Classification-F1 0.4796747967479675 on epoch=45
05/23/2022 02:03:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.137877 on epoch=46
05/23/2022 02:03:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.105284 on epoch=47
05/23/2022 02:03:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.049292 on epoch=48
05/23/2022 02:03:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.066101 on epoch=49
05/23/2022 02:03:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.047503 on epoch=49
05/23/2022 02:03:55 - INFO - __main__ - Global step 600 Train loss 0.081211 Classification-F1 0.36633663366336633 on epoch=49
05/23/2022 02:04:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.074299 on epoch=50
05/23/2022 02:04:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.064908 on epoch=51
05/23/2022 02:04:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.068123 on epoch=52
05/23/2022 02:04:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.111572 on epoch=53
05/23/2022 02:04:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.055648 on epoch=54
05/23/2022 02:04:22 - INFO - __main__ - Global step 650 Train loss 0.074910 Classification-F1 0.4838709677419355 on epoch=54
05/23/2022 02:04:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.095496 on epoch=54
05/23/2022 02:04:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.057229 on epoch=55
05/23/2022 02:04:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.100276 on epoch=56
05/23/2022 02:04:42 - INFO - __main__ - Step 690 Global step 690 Train loss 0.055064 on epoch=57
05/23/2022 02:04:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.068929 on epoch=58
05/23/2022 02:04:49 - INFO - __main__ - Global step 700 Train loss 0.075399 Classification-F1 0.4732510288065844 on epoch=58
05/23/2022 02:04:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.055100 on epoch=59
05/23/2022 02:04:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.088950 on epoch=59
05/23/2022 02:05:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.037282 on epoch=60
05/23/2022 02:05:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.032165 on epoch=61
05/23/2022 02:05:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.050711 on epoch=62
05/23/2022 02:05:16 - INFO - __main__ - Global step 750 Train loss 0.052841 Classification-F1 0.4859437751004016 on epoch=62
05/23/2022 02:05:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.073552 on epoch=63
05/23/2022 02:05:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.058010 on epoch=64
05/23/2022 02:05:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.014903 on epoch=64
05/23/2022 02:05:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.015250 on epoch=65
05/23/2022 02:05:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.062214 on epoch=66
05/23/2022 02:05:43 - INFO - __main__ - Global step 800 Train loss 0.044786 Classification-F1 0.4732510288065844 on epoch=66
05/23/2022 02:05:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.052422 on epoch=67
05/23/2022 02:05:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.010203 on epoch=68
05/23/2022 02:05:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.013198 on epoch=69
05/23/2022 02:06:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.062444 on epoch=69
05/23/2022 02:06:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.023499 on epoch=70
05/23/2022 02:06:10 - INFO - __main__ - Global step 850 Train loss 0.032353 Classification-F1 0.4900398406374502 on epoch=70
05/23/2022 02:06:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.011732 on epoch=71
05/23/2022 02:06:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.061196 on epoch=72
05/23/2022 02:06:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.007944 on epoch=73
05/23/2022 02:06:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.002699 on epoch=74
05/23/2022 02:06:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.051096 on epoch=74
05/23/2022 02:06:37 - INFO - __main__ - Global step 900 Train loss 0.026933 Classification-F1 0.4859437751004016 on epoch=74
05/23/2022 02:06:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.011273 on epoch=75
05/23/2022 02:06:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.019497 on epoch=76
05/23/2022 02:06:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.021377 on epoch=77
05/23/2022 02:06:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.034528 on epoch=78
05/23/2022 02:07:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.025260 on epoch=79
05/23/2022 02:07:04 - INFO - __main__ - Global step 950 Train loss 0.022387 Classification-F1 0.4900398406374502 on epoch=79
05/23/2022 02:07:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.031597 on epoch=79
05/23/2022 02:07:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.007943 on epoch=80
05/23/2022 02:07:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.028774 on epoch=81
05/23/2022 02:07:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.037522 on epoch=82
05/23/2022 02:07:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.041714 on epoch=83
05/23/2022 02:07:32 - INFO - __main__ - Global step 1000 Train loss 0.029510 Classification-F1 0.4775510204081633 on epoch=83
05/23/2022 02:07:32 - INFO - __main__ - save last model!
05/23/2022 02:07:32 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 02:07:32 - INFO - __main__ - Printing 3 examples
05/23/2022 02:07:32 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
05/23/2022 02:07:32 - INFO - __main__ - ['false']
05/23/2022 02:07:32 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
05/23/2022 02:07:32 - INFO - __main__ - ['false']
05/23/2022 02:07:32 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
05/23/2022 02:07:32 - INFO - __main__ - ['false']
05/23/2022 02:07:32 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:07:32 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:07:32 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 02:07:32 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 02:07:32 - INFO - __main__ - Printing 3 examples
05/23/2022 02:07:32 - INFO - __main__ -  [ethos-race] WOW THIS GUY TALKS LIKE A RETARD HAHAHAHAHAH
05/23/2022 02:07:32 - INFO - __main__ - ['false']
05/23/2022 02:07:32 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 02:07:32 - INFO - __main__ - ['false']
05/23/2022 02:07:32 - INFO - __main__ -  [ethos-race] Hindus and Muslims are same shit. They only know how to kill Peoples...See comments of Christians people no rudeness no abusing or bad words... Christians religion is the best religion in the world
05/23/2022 02:07:32 - INFO - __main__ - ['false']
05/23/2022 02:07:32 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:07:32 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:07:32 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 02:07:39 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 02:07:39 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 02:07:39 - INFO - __main__ - Printing 3 examples
05/23/2022 02:07:39 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 02:07:39 - INFO - __main__ - ['true']
05/23/2022 02:07:39 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 02:07:39 - INFO - __main__ - ['false']
05/23/2022 02:07:39 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 02:07:39 - INFO - __main__ - ['false']
05/23/2022 02:07:39 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:07:39 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:07:39 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 02:07:41 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_100_0.0003_8_predictions.txt
05/23/2022 02:07:41 - INFO - __main__ - Classification-F1 on test data: 0.4494
05/23/2022 02:07:41 - INFO - __main__ - prefix=ethos-race_128_100, lr=0.0003, bsz=8, dev_performance=1.0, test_performance=0.44936708860759494
05/23/2022 02:07:41 - INFO - __main__ - Running ... prefix=ethos-race_128_100, lr=0.0002, bsz=8 ...
05/23/2022 02:07:42 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 02:07:42 - INFO - __main__ - Printing 3 examples
05/23/2022 02:07:42 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
05/23/2022 02:07:42 - INFO - __main__ - ['false']
05/23/2022 02:07:42 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
05/23/2022 02:07:42 - INFO - __main__ - ['false']
05/23/2022 02:07:42 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
05/23/2022 02:07:42 - INFO - __main__ - ['false']
05/23/2022 02:07:42 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:07:42 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:07:43 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 02:07:43 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 02:07:43 - INFO - __main__ - Printing 3 examples
05/23/2022 02:07:43 - INFO - __main__ -  [ethos-race] WOW THIS GUY TALKS LIKE A RETARD HAHAHAHAHAH
05/23/2022 02:07:43 - INFO - __main__ - ['false']
05/23/2022 02:07:43 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 02:07:43 - INFO - __main__ - ['false']
05/23/2022 02:07:43 - INFO - __main__ -  [ethos-race] Hindus and Muslims are same shit. They only know how to kill Peoples...See comments of Christians people no rudeness no abusing or bad words... Christians religion is the best religion in the world
05/23/2022 02:07:43 - INFO - __main__ - ['false']
05/23/2022 02:07:43 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:07:43 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:07:43 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 02:07:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 02:07:45 - INFO - __main__ - Starting training!
05/23/2022 02:07:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 02:07:55 - INFO - __main__ - Starting training!
05/23/2022 02:08:00 - INFO - __main__ - Step 10 Global step 10 Train loss 24.535519 on epoch=0
05/23/2022 02:08:05 - INFO - __main__ - Step 20 Global step 20 Train loss 18.780792 on epoch=1
05/23/2022 02:08:10 - INFO - __main__ - Step 30 Global step 30 Train loss 17.629301 on epoch=2
05/23/2022 02:08:15 - INFO - __main__ - Step 40 Global step 40 Train loss 16.340551 on epoch=3
05/23/2022 02:08:20 - INFO - __main__ - Step 50 Global step 50 Train loss 15.522962 on epoch=4
05/23/2022 02:08:59 - INFO - __main__ - Global step 50 Train loss 18.561827 Classification-F1 0.0 on epoch=4
05/23/2022 02:09:05 - INFO - __main__ - Step 60 Global step 60 Train loss 16.107983 on epoch=4
05/23/2022 02:09:10 - INFO - __main__ - Step 70 Global step 70 Train loss 13.661621 on epoch=5
05/23/2022 02:09:15 - INFO - __main__ - Step 80 Global step 80 Train loss 13.612343 on epoch=6
05/23/2022 02:09:20 - INFO - __main__ - Step 90 Global step 90 Train loss 12.281748 on epoch=7
05/23/2022 02:09:26 - INFO - __main__ - Step 100 Global step 100 Train loss 11.622107 on epoch=8
05/23/2022 02:09:58 - INFO - __main__ - Global step 100 Train loss 13.457159 Classification-F1 0.0 on epoch=8
05/23/2022 02:10:04 - INFO - __main__ - Step 110 Global step 110 Train loss 11.113176 on epoch=9
05/23/2022 02:10:09 - INFO - __main__ - Step 120 Global step 120 Train loss 9.506910 on epoch=9
05/23/2022 02:10:14 - INFO - __main__ - Step 130 Global step 130 Train loss 6.515726 on epoch=10
05/23/2022 02:10:19 - INFO - __main__ - Step 140 Global step 140 Train loss 4.109491 on epoch=11
05/23/2022 02:10:24 - INFO - __main__ - Step 150 Global step 150 Train loss 3.284430 on epoch=12
05/23/2022 02:10:26 - INFO - __main__ - Global step 150 Train loss 6.905947 Classification-F1 0.49606299212598426 on epoch=12
05/23/2022 02:10:32 - INFO - __main__ - Step 160 Global step 160 Train loss 3.046415 on epoch=13
05/23/2022 02:10:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.786974 on epoch=14
05/23/2022 02:10:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.671136 on epoch=14
05/23/2022 02:10:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.366658 on epoch=15
05/23/2022 02:10:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.456593 on epoch=16
05/23/2022 02:10:54 - INFO - __main__ - Global step 200 Train loss 1.065555 Classification-F1 1.0 on epoch=16
05/23/2022 02:11:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.402977 on epoch=17
05/23/2022 02:11:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.342109 on epoch=18
05/23/2022 02:11:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.302788 on epoch=19
05/23/2022 02:11:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.299223 on epoch=19
05/23/2022 02:11:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.272679 on epoch=20
05/23/2022 02:11:22 - INFO - __main__ - Global step 250 Train loss 0.323955 Classification-F1 0.3786407766990291 on epoch=20
05/23/2022 02:11:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.245056 on epoch=21
05/23/2022 02:11:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.218654 on epoch=22
05/23/2022 02:11:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.216688 on epoch=23
05/23/2022 02:11:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.196834 on epoch=24
05/23/2022 02:11:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.246851 on epoch=24
05/23/2022 02:11:49 - INFO - __main__ - Global step 300 Train loss 0.224816 Classification-F1 0.4336283185840708 on epoch=24
05/23/2022 02:11:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.188100 on epoch=25
05/23/2022 02:12:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.173084 on epoch=26
05/23/2022 02:12:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.220731 on epoch=27
05/23/2022 02:12:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.117642 on epoch=28
05/23/2022 02:12:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.170550 on epoch=29
05/23/2022 02:12:17 - INFO - __main__ - Global step 350 Train loss 0.174021 Classification-F1 0.47107438016528924 on epoch=29
05/23/2022 02:12:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.141627 on epoch=29
05/23/2022 02:12:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.115716 on epoch=30
05/23/2022 02:12:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.088009 on epoch=31
05/23/2022 02:12:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.090727 on epoch=32
05/23/2022 02:12:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.101519 on epoch=33
05/23/2022 02:12:44 - INFO - __main__ - Global step 400 Train loss 0.107520 Classification-F1 0.4859437751004016 on epoch=33
05/23/2022 02:12:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.086768 on epoch=34
05/23/2022 02:12:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.124321 on epoch=34
05/23/2022 02:13:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.041168 on epoch=35
05/23/2022 02:13:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.180258 on epoch=36
05/23/2022 02:13:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.099699 on epoch=37
05/23/2022 02:13:12 - INFO - __main__ - Global step 450 Train loss 0.106443 Classification-F1 0.4666666666666667 on epoch=37
05/23/2022 02:13:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.061117 on epoch=38
05/23/2022 02:13:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.049211 on epoch=39
05/23/2022 02:13:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.062127 on epoch=39
05/23/2022 02:13:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.051535 on epoch=40
05/23/2022 02:13:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.072168 on epoch=41
05/23/2022 02:13:39 - INFO - __main__ - Global step 500 Train loss 0.059232 Classification-F1 0.4838709677419355 on epoch=41
05/23/2022 02:13:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.057615 on epoch=42
05/23/2022 02:13:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.067099 on epoch=43
05/23/2022 02:13:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.028874 on epoch=44
05/23/2022 02:14:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.048459 on epoch=44
05/23/2022 02:14:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.014238 on epoch=45
05/23/2022 02:14:06 - INFO - __main__ - Global step 550 Train loss 0.043257 Classification-F1 0.4838709677419355 on epoch=45
05/23/2022 02:14:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.043664 on epoch=46
05/23/2022 02:14:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.015039 on epoch=47
05/23/2022 02:14:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.031321 on epoch=48
05/23/2022 02:14:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.015781 on epoch=49
05/23/2022 02:14:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.008580 on epoch=49
05/23/2022 02:14:34 - INFO - __main__ - Global step 600 Train loss 0.022877 Classification-F1 0.46887966804979253 on epoch=49
05/23/2022 02:14:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.012789 on epoch=50
05/23/2022 02:14:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.133489 on epoch=51
05/23/2022 02:14:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.026207 on epoch=52
05/23/2022 02:14:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.082455 on epoch=53
05/23/2022 02:15:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.032975 on epoch=54
05/23/2022 02:15:01 - INFO - __main__ - Global step 650 Train loss 0.057583 Classification-F1 0.4817813765182186 on epoch=54
05/23/2022 02:15:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.033294 on epoch=54
05/23/2022 02:15:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.009182 on epoch=55
05/23/2022 02:15:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.036084 on epoch=56
05/23/2022 02:15:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.059326 on epoch=57
05/23/2022 02:15:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.010208 on epoch=58
05/23/2022 02:15:28 - INFO - __main__ - Global step 700 Train loss 0.029619 Classification-F1 0.4732510288065844 on epoch=58
05/23/2022 02:15:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.009862 on epoch=59
05/23/2022 02:15:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.022393 on epoch=59
05/23/2022 02:15:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.005362 on epoch=60
05/23/2022 02:15:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.047815 on epoch=61
05/23/2022 02:15:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.032633 on epoch=62
05/23/2022 02:15:56 - INFO - __main__ - Global step 750 Train loss 0.023613 Classification-F1 0.46443514644351463 on epoch=62
05/23/2022 02:16:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.061160 on epoch=63
05/23/2022 02:16:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.031545 on epoch=64
05/23/2022 02:16:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.022345 on epoch=64
05/23/2022 02:16:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.007461 on epoch=65
05/23/2022 02:16:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.015080 on epoch=66
05/23/2022 02:16:23 - INFO - __main__ - Global step 800 Train loss 0.027518 Classification-F1 0.47107438016528924 on epoch=66
05/23/2022 02:16:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.004352 on epoch=67
05/23/2022 02:16:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.014664 on epoch=68
05/23/2022 02:16:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.008777 on epoch=69
05/23/2022 02:16:44 - INFO - __main__ - Step 840 Global step 840 Train loss 0.017447 on epoch=69
05/23/2022 02:16:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.012941 on epoch=70
05/23/2022 02:16:51 - INFO - __main__ - Global step 850 Train loss 0.011636 Classification-F1 0.47107438016528924 on epoch=70
05/23/2022 02:16:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.016237 on epoch=71
05/23/2022 02:17:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.017125 on epoch=72
05/23/2022 02:17:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.007172 on epoch=73
05/23/2022 02:17:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.005306 on epoch=74
05/23/2022 02:17:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.003067 on epoch=74
05/23/2022 02:17:19 - INFO - __main__ - Global step 900 Train loss 0.009782 Classification-F1 0.47540983606557374 on epoch=74
05/23/2022 02:17:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.011071 on epoch=75
05/23/2022 02:17:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.007028 on epoch=76
05/23/2022 02:17:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.002208 on epoch=77
05/23/2022 02:17:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.004915 on epoch=78
05/23/2022 02:17:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.001049 on epoch=79
05/23/2022 02:17:46 - INFO - __main__ - Global step 950 Train loss 0.005254 Classification-F1 0.4838709677419355 on epoch=79
05/23/2022 02:17:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.006195 on epoch=79
05/23/2022 02:17:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.002477 on epoch=80
05/23/2022 02:18:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.023737 on epoch=81
05/23/2022 02:18:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.001099 on epoch=82
05/23/2022 02:18:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.002723 on epoch=83
05/23/2022 02:18:14 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 02:18:14 - INFO - __main__ - Printing 3 examples
05/23/2022 02:18:14 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
05/23/2022 02:18:14 - INFO - __main__ - ['false']
05/23/2022 02:18:14 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
05/23/2022 02:18:14 - INFO - __main__ - ['false']
05/23/2022 02:18:14 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
05/23/2022 02:18:14 - INFO - __main__ - ['false']
05/23/2022 02:18:14 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:18:14 - INFO - __main__ - Global step 1000 Train loss 0.007246 Classification-F1 0.4775510204081633 on epoch=83
05/23/2022 02:18:14 - INFO - __main__ - save last model!
05/23/2022 02:18:14 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:18:14 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 02:18:14 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 02:18:14 - INFO - __main__ - Printing 3 examples
05/23/2022 02:18:14 - INFO - __main__ -  [ethos-race] WOW THIS GUY TALKS LIKE A RETARD HAHAHAHAHAH
05/23/2022 02:18:14 - INFO - __main__ - ['false']
05/23/2022 02:18:14 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 02:18:14 - INFO - __main__ - ['false']
05/23/2022 02:18:14 - INFO - __main__ -  [ethos-race] Hindus and Muslims are same shit. They only know how to kill Peoples...See comments of Christians people no rudeness no abusing or bad words... Christians religion is the best religion in the world
05/23/2022 02:18:14 - INFO - __main__ - ['false']
05/23/2022 02:18:14 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:18:14 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:18:14 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 02:18:21 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 02:18:21 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 02:18:21 - INFO - __main__ - Printing 3 examples
05/23/2022 02:18:21 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 02:18:21 - INFO - __main__ - ['true']
05/23/2022 02:18:21 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 02:18:21 - INFO - __main__ - ['false']
05/23/2022 02:18:21 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 02:18:21 - INFO - __main__ - ['false']
05/23/2022 02:18:21 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:18:21 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:18:21 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 02:18:23 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_100_0.0002_8_predictions.txt
05/23/2022 02:18:23 - INFO - __main__ - Classification-F1 on test data: 0.4494
05/23/2022 02:18:23 - INFO - __main__ - prefix=ethos-race_128_100, lr=0.0002, bsz=8, dev_performance=1.0, test_performance=0.44936708860759494
05/23/2022 02:18:23 - INFO - __main__ - Running ... prefix=ethos-race_128_100, lr=0.0001, bsz=8 ...
05/23/2022 02:18:24 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 02:18:24 - INFO - __main__ - Printing 3 examples
05/23/2022 02:18:24 - INFO - __main__ -  [ethos-race] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
05/23/2022 02:18:24 - INFO - __main__ - ['false']
05/23/2022 02:18:24 - INFO - __main__ -  [ethos-race] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
05/23/2022 02:18:24 - INFO - __main__ - ['false']
05/23/2022 02:18:24 - INFO - __main__ -  [ethos-race] an Asian Moe Howard piece of shit
05/23/2022 02:18:24 - INFO - __main__ - ['false']
05/23/2022 02:18:24 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:18:24 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:18:25 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 02:18:25 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 02:18:25 - INFO - __main__ - Printing 3 examples
05/23/2022 02:18:25 - INFO - __main__ -  [ethos-race] WOW THIS GUY TALKS LIKE A RETARD HAHAHAHAHAH
05/23/2022 02:18:25 - INFO - __main__ - ['false']
05/23/2022 02:18:25 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 02:18:25 - INFO - __main__ - ['false']
05/23/2022 02:18:25 - INFO - __main__ -  [ethos-race] Hindus and Muslims are same shit. They only know how to kill Peoples...See comments of Christians people no rudeness no abusing or bad words... Christians religion is the best religion in the world
05/23/2022 02:18:25 - INFO - __main__ - ['false']
05/23/2022 02:18:25 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:18:25 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:18:25 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 02:18:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 02:18:27 - INFO - __main__ - Starting training!
05/23/2022 02:18:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 02:18:36 - INFO - __main__ - Starting training!
05/23/2022 02:18:40 - INFO - __main__ - Step 10 Global step 10 Train loss 23.647558 on epoch=0
05/23/2022 02:18:45 - INFO - __main__ - Step 20 Global step 20 Train loss 21.119053 on epoch=1
05/23/2022 02:18:50 - INFO - __main__ - Step 30 Global step 30 Train loss 18.057724 on epoch=2
05/23/2022 02:18:55 - INFO - __main__ - Step 40 Global step 40 Train loss 18.132242 on epoch=3
05/23/2022 02:19:01 - INFO - __main__ - Step 50 Global step 50 Train loss 17.409573 on epoch=4
05/23/2022 02:19:38 - INFO - __main__ - Global step 50 Train loss 19.673229 Classification-F1 0.0 on epoch=4
05/23/2022 02:19:44 - INFO - __main__ - Step 60 Global step 60 Train loss 17.354136 on epoch=4
05/23/2022 02:19:49 - INFO - __main__ - Step 70 Global step 70 Train loss 16.983713 on epoch=5
05/23/2022 02:19:54 - INFO - __main__ - Step 80 Global step 80 Train loss 16.200533 on epoch=6
05/23/2022 02:19:59 - INFO - __main__ - Step 90 Global step 90 Train loss 15.013318 on epoch=7
05/23/2022 02:20:05 - INFO - __main__ - Step 100 Global step 100 Train loss 14.835220 on epoch=8
05/23/2022 02:20:38 - INFO - __main__ - Global step 100 Train loss 16.077385 Classification-F1 0.0 on epoch=8
05/23/2022 02:20:43 - INFO - __main__ - Step 110 Global step 110 Train loss 14.687304 on epoch=9
05/23/2022 02:20:48 - INFO - __main__ - Step 120 Global step 120 Train loss 14.770198 on epoch=9
05/23/2022 02:20:53 - INFO - __main__ - Step 130 Global step 130 Train loss 14.385025 on epoch=10
05/23/2022 02:20:58 - INFO - __main__ - Step 140 Global step 140 Train loss 13.653447 on epoch=11
05/23/2022 02:21:04 - INFO - __main__ - Step 150 Global step 150 Train loss 13.299662 on epoch=12
05/23/2022 02:21:38 - INFO - __main__ - Global step 150 Train loss 14.159126 Classification-F1 0.0 on epoch=12
05/23/2022 02:21:43 - INFO - __main__ - Step 160 Global step 160 Train loss 13.119481 on epoch=13
05/23/2022 02:21:48 - INFO - __main__ - Step 170 Global step 170 Train loss 13.003920 on epoch=14
05/23/2022 02:21:53 - INFO - __main__ - Step 180 Global step 180 Train loss 11.848738 on epoch=14
05/23/2022 02:21:59 - INFO - __main__ - Step 190 Global step 190 Train loss 11.539142 on epoch=15
05/23/2022 02:22:04 - INFO - __main__ - Step 200 Global step 200 Train loss 11.097034 on epoch=16
05/23/2022 02:22:08 - INFO - __main__ - Global step 200 Train loss 12.121664 Classification-F1 0.0 on epoch=16
05/23/2022 02:22:13 - INFO - __main__ - Step 210 Global step 210 Train loss 9.085531 on epoch=17
05/23/2022 02:22:19 - INFO - __main__ - Step 220 Global step 220 Train loss 8.706121 on epoch=18
05/23/2022 02:22:24 - INFO - __main__ - Step 230 Global step 230 Train loss 6.715637 on epoch=19
05/23/2022 02:22:29 - INFO - __main__ - Step 240 Global step 240 Train loss 5.470114 on epoch=19
05/23/2022 02:22:34 - INFO - __main__ - Step 250 Global step 250 Train loss 2.918247 on epoch=20
05/23/2022 02:22:36 - INFO - __main__ - Global step 250 Train loss 6.579130 Classification-F1 0.3402061855670103 on epoch=20
05/23/2022 02:22:42 - INFO - __main__ - Step 260 Global step 260 Train loss 1.584657 on epoch=21
05/23/2022 02:22:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.920789 on epoch=22
05/23/2022 02:22:52 - INFO - __main__ - Step 280 Global step 280 Train loss 1.044892 on epoch=23
05/23/2022 02:22:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.479139 on epoch=24
05/23/2022 02:23:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.455920 on epoch=24
05/23/2022 02:23:03 - INFO - __main__ - Global step 300 Train loss 0.897079 Classification-F1 0.41013824884792627 on epoch=24
05/23/2022 02:23:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.501018 on epoch=25
05/23/2022 02:23:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.475110 on epoch=26
05/23/2022 02:23:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.363268 on epoch=27
05/23/2022 02:23:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.469220 on epoch=28
05/23/2022 02:23:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.372331 on epoch=29
05/23/2022 02:23:32 - INFO - __main__ - Global step 350 Train loss 0.436189 Classification-F1 0.49606299212598426 on epoch=29
05/23/2022 02:23:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.519675 on epoch=29
05/23/2022 02:23:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.287179 on epoch=30
05/23/2022 02:23:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.330277 on epoch=31
05/23/2022 02:23:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.260105 on epoch=32
05/23/2022 02:23:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.347255 on epoch=33
05/23/2022 02:24:00 - INFO - __main__ - Global step 400 Train loss 0.348898 Classification-F1 0.43859649122807015 on epoch=33
05/23/2022 02:24:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.228361 on epoch=34
05/23/2022 02:24:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.492086 on epoch=34
05/23/2022 02:24:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.205121 on epoch=35
05/23/2022 02:24:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.202542 on epoch=36
05/23/2022 02:24:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.266499 on epoch=37
05/23/2022 02:24:27 - INFO - __main__ - Global step 450 Train loss 0.278922 Classification-F1 0.488 on epoch=37
05/23/2022 02:24:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.205967 on epoch=38
05/23/2022 02:24:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.294172 on epoch=39
05/23/2022 02:24:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.211420 on epoch=39
05/23/2022 02:24:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.194715 on epoch=40
05/23/2022 02:24:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.170673 on epoch=41
05/23/2022 02:24:54 - INFO - __main__ - Global step 500 Train loss 0.215389 Classification-F1 0.47540983606557374 on epoch=41
05/23/2022 02:24:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.182181 on epoch=42
05/23/2022 02:25:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.170895 on epoch=43
05/23/2022 02:25:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.170483 on epoch=44
05/23/2022 02:25:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.194956 on epoch=44
05/23/2022 02:25:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.122180 on epoch=45
05/23/2022 02:25:21 - INFO - __main__ - Global step 550 Train loss 0.168139 Classification-F1 0.46887966804979253 on epoch=45
05/23/2022 02:25:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.118688 on epoch=46
05/23/2022 02:25:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.272018 on epoch=47
05/23/2022 02:25:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.105422 on epoch=48
05/23/2022 02:25:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.154459 on epoch=49
05/23/2022 02:25:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.156687 on epoch=49
05/23/2022 02:25:49 - INFO - __main__ - Global step 600 Train loss 0.161455 Classification-F1 0.4553191489361702 on epoch=49
05/23/2022 02:25:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.105642 on epoch=50
05/23/2022 02:25:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.133217 on epoch=51
05/23/2022 02:26:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.090342 on epoch=52
05/23/2022 02:26:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.123512 on epoch=53
05/23/2022 02:26:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.106724 on epoch=54
05/23/2022 02:26:16 - INFO - __main__ - Global step 650 Train loss 0.111888 Classification-F1 0.46218487394957986 on epoch=54
05/23/2022 02:26:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.126614 on epoch=54
05/23/2022 02:26:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.078750 on epoch=55
05/23/2022 02:26:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.129837 on epoch=56
05/23/2022 02:26:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.103306 on epoch=57
05/23/2022 02:26:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.158220 on epoch=58
05/23/2022 02:26:43 - INFO - __main__ - Global step 700 Train loss 0.119345 Classification-F1 0.47540983606557374 on epoch=58
05/23/2022 02:26:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.123588 on epoch=59
05/23/2022 02:26:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.180985 on epoch=59
05/23/2022 02:26:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.041681 on epoch=60
05/23/2022 02:27:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.163902 on epoch=61
05/23/2022 02:27:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.087495 on epoch=62
05/23/2022 02:27:10 - INFO - __main__ - Global step 750 Train loss 0.119530 Classification-F1 0.4817813765182186 on epoch=62
05/23/2022 02:27:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.055458 on epoch=63
05/23/2022 02:27:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.097568 on epoch=64
05/23/2022 02:27:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.056577 on epoch=64
05/23/2022 02:27:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.061755 on epoch=65
05/23/2022 02:27:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.056755 on epoch=66
05/23/2022 02:27:37 - INFO - __main__ - Global step 800 Train loss 0.065623 Classification-F1 0.4796747967479675 on epoch=66
05/23/2022 02:27:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.071642 on epoch=67
05/23/2022 02:27:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.026634 on epoch=68
05/23/2022 02:27:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.056654 on epoch=69
05/23/2022 02:27:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.084955 on epoch=69
05/23/2022 02:28:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.237613 on epoch=70
05/23/2022 02:28:04 - INFO - __main__ - Global step 850 Train loss 0.095499 Classification-F1 0.4732510288065844 on epoch=70
05/23/2022 02:28:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.087391 on epoch=71
05/23/2022 02:28:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.054404 on epoch=72
05/23/2022 02:28:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.088638 on epoch=73
05/23/2022 02:28:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.122669 on epoch=74
05/23/2022 02:28:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.069207 on epoch=74
05/23/2022 02:28:31 - INFO - __main__ - Global step 900 Train loss 0.084462 Classification-F1 0.4775510204081633 on epoch=74
05/23/2022 02:28:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.030005 on epoch=75
05/23/2022 02:28:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.067555 on epoch=76
05/23/2022 02:28:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.260008 on epoch=77
05/23/2022 02:28:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.104379 on epoch=78
05/23/2022 02:28:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.053896 on epoch=79
05/23/2022 02:28:58 - INFO - __main__ - Global step 950 Train loss 0.103169 Classification-F1 0.4775510204081633 on epoch=79
05/23/2022 02:29:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.256215 on epoch=79
05/23/2022 02:29:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.121528 on epoch=80
05/23/2022 02:29:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.092435 on epoch=81
05/23/2022 02:29:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.153640 on epoch=82
05/23/2022 02:29:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.185174 on epoch=83
05/23/2022 02:29:25 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 02:29:25 - INFO - __main__ - Printing 3 examples
05/23/2022 02:29:25 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
05/23/2022 02:29:25 - INFO - __main__ - ['false']
05/23/2022 02:29:25 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 02:29:25 - INFO - __main__ - ['false']
05/23/2022 02:29:25 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
05/23/2022 02:29:25 - INFO - __main__ - ['false']
05/23/2022 02:29:25 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:29:25 - INFO - __main__ - Global step 1000 Train loss 0.161798 Classification-F1 0.46443514644351463 on epoch=83
05/23/2022 02:29:25 - INFO - __main__ - save last model!
05/23/2022 02:29:25 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:29:26 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 02:29:26 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 02:29:26 - INFO - __main__ - Printing 3 examples
05/23/2022 02:29:26 - INFO - __main__ -  [ethos-race] Well France is finally a shithole This is what happens when you do open borders now both sides are suffering
05/23/2022 02:29:26 - INFO - __main__ - ['false']
05/23/2022 02:29:26 - INFO - __main__ -  [ethos-race] Equality should not apply on people of different religions except christianity.
05/23/2022 02:29:26 - INFO - __main__ - ['false']
05/23/2022 02:29:26 - INFO - __main__ -  [ethos-race] kill the gays
05/23/2022 02:29:26 - INFO - __main__ - ['false']
05/23/2022 02:29:26 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:29:26 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:29:26 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 02:29:33 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 02:29:33 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 02:29:33 - INFO - __main__ - Printing 3 examples
05/23/2022 02:29:33 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 02:29:33 - INFO - __main__ - ['true']
05/23/2022 02:29:33 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 02:29:33 - INFO - __main__ - ['false']
05/23/2022 02:29:33 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 02:29:33 - INFO - __main__ - ['false']
05/23/2022 02:29:33 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:29:33 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:29:33 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 02:29:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_100_0.0001_8_predictions.txt
05/23/2022 02:29:35 - INFO - __main__ - Classification-F1 on test data: 0.6743
05/23/2022 02:29:35 - INFO - __main__ - prefix=ethos-race_128_100, lr=0.0001, bsz=8, dev_performance=0.49606299212598426, test_performance=0.674344946731932
05/23/2022 02:29:35 - INFO - __main__ - Running ... prefix=ethos-race_128_13, lr=0.0005, bsz=8 ...
05/23/2022 02:29:36 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 02:29:36 - INFO - __main__ - Printing 3 examples
05/23/2022 02:29:36 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
05/23/2022 02:29:36 - INFO - __main__ - ['false']
05/23/2022 02:29:36 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 02:29:36 - INFO - __main__ - ['false']
05/23/2022 02:29:36 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
05/23/2022 02:29:36 - INFO - __main__ - ['false']
05/23/2022 02:29:36 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:29:36 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:29:36 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 02:29:36 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 02:29:36 - INFO - __main__ - Printing 3 examples
05/23/2022 02:29:36 - INFO - __main__ -  [ethos-race] Well France is finally a shithole This is what happens when you do open borders now both sides are suffering
05/23/2022 02:29:36 - INFO - __main__ - ['false']
05/23/2022 02:29:36 - INFO - __main__ -  [ethos-race] Equality should not apply on people of different religions except christianity.
05/23/2022 02:29:36 - INFO - __main__ - ['false']
05/23/2022 02:29:36 - INFO - __main__ -  [ethos-race] kill the gays
05/23/2022 02:29:36 - INFO - __main__ - ['false']
05/23/2022 02:29:36 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:29:37 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:29:37 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 02:29:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 02:29:37 - INFO - __main__ - Starting training!
05/23/2022 02:29:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 02:29:49 - INFO - __main__ - Starting training!
05/23/2022 02:29:54 - INFO - __main__ - Step 10 Global step 10 Train loss 23.717049 on epoch=0
05/23/2022 02:29:59 - INFO - __main__ - Step 20 Global step 20 Train loss 19.094135 on epoch=1
05/23/2022 02:30:04 - INFO - __main__ - Step 30 Global step 30 Train loss 15.808802 on epoch=2
05/23/2022 02:30:09 - INFO - __main__ - Step 40 Global step 40 Train loss 15.197469 on epoch=3
05/23/2022 02:30:14 - INFO - __main__ - Step 50 Global step 50 Train loss 12.524744 on epoch=4
05/23/2022 02:30:16 - INFO - __main__ - Global step 50 Train loss 17.268438 Classification-F1 0.0 on epoch=4
05/23/2022 02:30:21 - INFO - __main__ - Step 60 Global step 60 Train loss 10.447935 on epoch=4
05/23/2022 02:30:27 - INFO - __main__ - Step 70 Global step 70 Train loss 3.460105 on epoch=5
05/23/2022 02:30:32 - INFO - __main__ - Step 80 Global step 80 Train loss 2.404229 on epoch=6
05/23/2022 02:30:37 - INFO - __main__ - Step 90 Global step 90 Train loss 1.978109 on epoch=7
05/23/2022 02:30:42 - INFO - __main__ - Step 100 Global step 100 Train loss 1.634456 on epoch=8
05/23/2022 02:30:44 - INFO - __main__ - Global step 100 Train loss 3.984967 Classification-F1 0.3118279569892473 on epoch=8
05/23/2022 02:30:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.597211 on epoch=9
05/23/2022 02:30:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.521945 on epoch=9
05/23/2022 02:31:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.426221 on epoch=10
05/23/2022 02:31:06 - INFO - __main__ - Step 140 Global step 140 Train loss 0.367608 on epoch=11
05/23/2022 02:31:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.320230 on epoch=12
05/23/2022 02:31:12 - INFO - __main__ - Global step 150 Train loss 0.446643 Classification-F1 0.4859437751004016 on epoch=12
05/23/2022 02:31:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.377065 on epoch=13
05/23/2022 02:31:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.335910 on epoch=14
05/23/2022 02:31:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.272248 on epoch=14
05/23/2022 02:31:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.195718 on epoch=15
05/23/2022 02:31:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.227689 on epoch=16
05/23/2022 02:31:40 - INFO - __main__ - Global step 200 Train loss 0.281726 Classification-F1 0.4859437751004016 on epoch=16
05/23/2022 02:31:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.213616 on epoch=17
05/23/2022 02:31:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.221185 on epoch=18
05/23/2022 02:31:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.158636 on epoch=19
05/23/2022 02:32:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.128778 on epoch=19
05/23/2022 02:32:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.162586 on epoch=20
05/23/2022 02:32:08 - INFO - __main__ - Global step 250 Train loss 0.176960 Classification-F1 0.375609756097561 on epoch=20
05/23/2022 02:32:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.116936 on epoch=21
05/23/2022 02:32:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.110070 on epoch=22
05/23/2022 02:32:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.138598 on epoch=23
05/23/2022 02:32:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.096086 on epoch=24
05/23/2022 02:32:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.088549 on epoch=24
05/23/2022 02:32:35 - INFO - __main__ - Global step 300 Train loss 0.110048 Classification-F1 0.47540983606557374 on epoch=24
05/23/2022 02:32:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.061097 on epoch=25
05/23/2022 02:32:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.142516 on epoch=26
05/23/2022 02:32:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.119307 on epoch=27
05/23/2022 02:32:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.072813 on epoch=28
05/23/2022 02:33:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.112669 on epoch=29
05/23/2022 02:33:02 - INFO - __main__ - Global step 350 Train loss 0.101680 Classification-F1 0.4796747967479675 on epoch=29
05/23/2022 02:33:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.046395 on epoch=29
05/23/2022 02:33:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.059876 on epoch=30
05/23/2022 02:33:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.039676 on epoch=31
05/23/2022 02:33:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.044082 on epoch=32
05/23/2022 02:33:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.069143 on epoch=33
05/23/2022 02:33:30 - INFO - __main__ - Global step 400 Train loss 0.051835 Classification-F1 0.4817813765182186 on epoch=33
05/23/2022 02:33:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.159042 on epoch=34
05/23/2022 02:33:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.043191 on epoch=34
05/23/2022 02:33:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.060369 on epoch=35
05/23/2022 02:33:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.232380 on epoch=36
05/23/2022 02:33:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.472396 on epoch=37
05/23/2022 02:33:57 - INFO - __main__ - Global step 450 Train loss 0.193476 Classification-F1 0.4859437751004016 on epoch=37
05/23/2022 02:34:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.344904 on epoch=38
05/23/2022 02:34:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.336254 on epoch=39
05/23/2022 02:34:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.208010 on epoch=39
05/23/2022 02:34:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.155122 on epoch=40
05/23/2022 02:34:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.155062 on epoch=41
05/23/2022 02:34:25 - INFO - __main__ - Global step 500 Train loss 0.239870 Classification-F1 0.488 on epoch=41
05/23/2022 02:34:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.117778 on epoch=42
05/23/2022 02:34:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.175460 on epoch=43
05/23/2022 02:34:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.167297 on epoch=44
05/23/2022 02:34:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.049651 on epoch=44
05/23/2022 02:34:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.065756 on epoch=45
05/23/2022 02:34:53 - INFO - __main__ - Global step 550 Train loss 0.115189 Classification-F1 0.4796747967479675 on epoch=45
05/23/2022 02:34:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.027236 on epoch=46
05/23/2022 02:35:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.030061 on epoch=47
05/23/2022 02:35:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.011433 on epoch=48
05/23/2022 02:35:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.006254 on epoch=49
05/23/2022 02:35:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.024955 on epoch=49
05/23/2022 02:35:20 - INFO - __main__ - Global step 600 Train loss 0.019988 Classification-F1 0.4796747967479675 on epoch=49
05/23/2022 02:35:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.026977 on epoch=50
05/23/2022 02:35:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.018529 on epoch=51
05/23/2022 02:35:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.029425 on epoch=52
05/23/2022 02:35:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.029948 on epoch=53
05/23/2022 02:35:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.010732 on epoch=54
05/23/2022 02:35:48 - INFO - __main__ - Global step 650 Train loss 0.023122 Classification-F1 0.4775510204081633 on epoch=54
05/23/2022 02:35:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.023404 on epoch=54
05/23/2022 02:35:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.007423 on epoch=55
05/23/2022 02:36:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.009463 on epoch=56
05/23/2022 02:36:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.008530 on epoch=57
05/23/2022 02:36:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.005462 on epoch=58
05/23/2022 02:36:15 - INFO - __main__ - Global step 700 Train loss 0.010856 Classification-F1 0.4796747967479675 on epoch=58
05/23/2022 02:36:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.030601 on epoch=59
05/23/2022 02:36:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.032210 on epoch=59
05/23/2022 02:36:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.005229 on epoch=60
05/23/2022 02:36:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.003803 on epoch=61
05/23/2022 02:36:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000711 on epoch=62
05/23/2022 02:36:43 - INFO - __main__ - Global step 750 Train loss 0.014511 Classification-F1 0.4775510204081633 on epoch=62
05/23/2022 02:36:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.001682 on epoch=63
05/23/2022 02:36:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.001820 on epoch=64
05/23/2022 02:36:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.003459 on epoch=64
05/23/2022 02:37:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.004970 on epoch=65
05/23/2022 02:37:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.037318 on epoch=66
05/23/2022 02:37:10 - INFO - __main__ - Global step 800 Train loss 0.009850 Classification-F1 0.4817813765182186 on epoch=66
05/23/2022 02:37:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.039394 on epoch=67
05/23/2022 02:37:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.006272 on epoch=68
05/23/2022 02:37:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000488 on epoch=69
05/23/2022 02:37:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.001893 on epoch=69
05/23/2022 02:37:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000644 on epoch=70
05/23/2022 02:37:37 - INFO - __main__ - Global step 850 Train loss 0.009738 Classification-F1 0.4817813765182186 on epoch=70
05/23/2022 02:37:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.002521 on epoch=71
05/23/2022 02:37:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000150 on epoch=72
05/23/2022 02:37:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.030946 on epoch=73
05/23/2022 02:37:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.002999 on epoch=74
05/23/2022 02:38:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.019119 on epoch=74
05/23/2022 02:38:05 - INFO - __main__ - Global step 900 Train loss 0.011147 Classification-F1 0.4796747967479675 on epoch=74
05/23/2022 02:38:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.017888 on epoch=75
05/23/2022 02:38:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.004432 on epoch=76
05/23/2022 02:38:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.001209 on epoch=77
05/23/2022 02:38:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000792 on epoch=78
05/23/2022 02:38:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000175 on epoch=79
05/23/2022 02:38:32 - INFO - __main__ - Global step 950 Train loss 0.004899 Classification-F1 0.4838709677419355 on epoch=79
05/23/2022 02:38:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000683 on epoch=79
05/23/2022 02:38:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.001122 on epoch=80
05/23/2022 02:38:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.002089 on epoch=81
05/23/2022 02:38:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000271 on epoch=82
05/23/2022 02:38:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000222 on epoch=83
05/23/2022 02:38:59 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 02:38:59 - INFO - __main__ - Printing 3 examples
05/23/2022 02:38:59 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
05/23/2022 02:38:59 - INFO - __main__ - ['false']
05/23/2022 02:38:59 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 02:38:59 - INFO - __main__ - ['false']
05/23/2022 02:38:59 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
05/23/2022 02:38:59 - INFO - __main__ - ['false']
05/23/2022 02:38:59 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:38:59 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:38:59 - INFO - __main__ - Global step 1000 Train loss 0.000877 Classification-F1 0.4796747967479675 on epoch=83
05/23/2022 02:38:59 - INFO - __main__ - save last model!
05/23/2022 02:39:00 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 02:39:00 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 02:39:00 - INFO - __main__ - Printing 3 examples
05/23/2022 02:39:00 - INFO - __main__ -  [ethos-race] Well France is finally a shithole This is what happens when you do open borders now both sides are suffering
05/23/2022 02:39:00 - INFO - __main__ - ['false']
05/23/2022 02:39:00 - INFO - __main__ -  [ethos-race] Equality should not apply on people of different religions except christianity.
05/23/2022 02:39:00 - INFO - __main__ - ['false']
05/23/2022 02:39:00 - INFO - __main__ -  [ethos-race] kill the gays
05/23/2022 02:39:00 - INFO - __main__ - ['false']
05/23/2022 02:39:00 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:39:00 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:39:00 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 02:39:06 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 02:39:07 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 02:39:07 - INFO - __main__ - Printing 3 examples
05/23/2022 02:39:07 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 02:39:07 - INFO - __main__ - ['true']
05/23/2022 02:39:07 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 02:39:07 - INFO - __main__ - ['false']
05/23/2022 02:39:07 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 02:39:07 - INFO - __main__ - ['false']
05/23/2022 02:39:07 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:39:07 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:39:07 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 02:39:09 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_13_0.0005_8_predictions.txt
05/23/2022 02:39:09 - INFO - __main__ - Classification-F1 on test data: 0.9194
05/23/2022 02:39:09 - INFO - __main__ - prefix=ethos-race_128_13, lr=0.0005, bsz=8, dev_performance=0.488, test_performance=0.9194444444444444
05/23/2022 02:39:09 - INFO - __main__ - Running ... prefix=ethos-race_128_13, lr=0.0003, bsz=8 ...
05/23/2022 02:39:10 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 02:39:10 - INFO - __main__ - Printing 3 examples
05/23/2022 02:39:10 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
05/23/2022 02:39:10 - INFO - __main__ - ['false']
05/23/2022 02:39:10 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 02:39:10 - INFO - __main__ - ['false']
05/23/2022 02:39:10 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
05/23/2022 02:39:10 - INFO - __main__ - ['false']
05/23/2022 02:39:10 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:39:10 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:39:10 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 02:39:10 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 02:39:10 - INFO - __main__ - Printing 3 examples
05/23/2022 02:39:10 - INFO - __main__ -  [ethos-race] Well France is finally a shithole This is what happens when you do open borders now both sides are suffering
05/23/2022 02:39:10 - INFO - __main__ - ['false']
05/23/2022 02:39:10 - INFO - __main__ -  [ethos-race] Equality should not apply on people of different religions except christianity.
05/23/2022 02:39:10 - INFO - __main__ - ['false']
05/23/2022 02:39:10 - INFO - __main__ -  [ethos-race] kill the gays
05/23/2022 02:39:10 - INFO - __main__ - ['false']
05/23/2022 02:39:10 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:39:10 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:39:10 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 02:39:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 02:39:12 - INFO - __main__ - Starting training!
05/23/2022 02:39:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 02:39:21 - INFO - __main__ - Starting training!
05/23/2022 02:39:26 - INFO - __main__ - Step 10 Global step 10 Train loss 25.059628 on epoch=0
05/23/2022 02:39:31 - INFO - __main__ - Step 20 Global step 20 Train loss 17.827169 on epoch=1
05/23/2022 02:39:36 - INFO - __main__ - Step 30 Global step 30 Train loss 16.277575 on epoch=2
05/23/2022 02:39:41 - INFO - __main__ - Step 40 Global step 40 Train loss 15.068220 on epoch=3
05/23/2022 02:39:46 - INFO - __main__ - Step 50 Global step 50 Train loss 15.076704 on epoch=4
05/23/2022 02:40:22 - INFO - __main__ - Global step 50 Train loss 17.861860 Classification-F1 0.0 on epoch=4
05/23/2022 02:40:28 - INFO - __main__ - Step 60 Global step 60 Train loss 12.948721 on epoch=4
05/23/2022 02:40:33 - INFO - __main__ - Step 70 Global step 70 Train loss 11.602108 on epoch=5
05/23/2022 02:40:39 - INFO - __main__ - Step 80 Global step 80 Train loss 9.405509 on epoch=6
05/23/2022 02:40:44 - INFO - __main__ - Step 90 Global step 90 Train loss 6.386729 on epoch=7
05/23/2022 02:40:49 - INFO - __main__ - Step 100 Global step 100 Train loss 3.715496 on epoch=8
05/23/2022 02:40:50 - INFO - __main__ - Global step 100 Train loss 8.811712 Classification-F1 1.0 on epoch=8
05/23/2022 02:40:56 - INFO - __main__ - Step 110 Global step 110 Train loss 2.282525 on epoch=9
05/23/2022 02:41:01 - INFO - __main__ - Step 120 Global step 120 Train loss 3.649636 on epoch=9
05/23/2022 02:41:06 - INFO - __main__ - Step 130 Global step 130 Train loss 1.902056 on epoch=10
05/23/2022 02:41:12 - INFO - __main__ - Step 140 Global step 140 Train loss 1.996852 on epoch=11
05/23/2022 02:41:17 - INFO - __main__ - Step 150 Global step 150 Train loss 2.348707 on epoch=12
05/23/2022 02:41:18 - INFO - __main__ - Global step 150 Train loss 2.435955 Classification-F1 0.21516754850088182 on epoch=12
05/23/2022 02:41:23 - INFO - __main__ - Step 160 Global step 160 Train loss 1.040148 on epoch=13
05/23/2022 02:41:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.794196 on epoch=14
05/23/2022 02:41:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.471894 on epoch=14
05/23/2022 02:41:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.425460 on epoch=15
05/23/2022 02:41:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.426202 on epoch=16
05/23/2022 02:41:45 - INFO - __main__ - Global step 200 Train loss 0.631580 Classification-F1 1.0 on epoch=16
05/23/2022 02:41:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.389055 on epoch=17
05/23/2022 02:41:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.462542 on epoch=18
05/23/2022 02:42:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.505956 on epoch=19
05/23/2022 02:42:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.417961 on epoch=19
05/23/2022 02:42:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.365246 on epoch=20
05/23/2022 02:42:12 - INFO - __main__ - Global step 250 Train loss 0.428152 Classification-F1 0.32275132275132273 on epoch=20
05/23/2022 02:42:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.364420 on epoch=21
05/23/2022 02:42:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.335272 on epoch=22
05/23/2022 02:42:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.322656 on epoch=23
05/23/2022 02:42:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.375472 on epoch=24
05/23/2022 02:42:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.390493 on epoch=24
05/23/2022 02:42:39 - INFO - __main__ - Global step 300 Train loss 0.357662 Classification-F1 0.0 on epoch=24
05/23/2022 02:42:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.345345 on epoch=25
05/23/2022 02:42:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.339554 on epoch=26
05/23/2022 02:42:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.342303 on epoch=27
05/23/2022 02:43:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.338834 on epoch=28
05/23/2022 02:43:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.391665 on epoch=29
05/23/2022 02:43:06 - INFO - __main__ - Global step 350 Train loss 0.351540 Classification-F1 0.49606299212598426 on epoch=29
05/23/2022 02:43:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.327788 on epoch=29
05/23/2022 02:43:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.308737 on epoch=30
05/23/2022 02:43:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.306958 on epoch=31
05/23/2022 02:43:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.300381 on epoch=32
05/23/2022 02:43:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.292428 on epoch=33
05/23/2022 02:43:33 - INFO - __main__ - Global step 400 Train loss 0.307258 Classification-F1 0.4859437751004016 on epoch=33
05/23/2022 02:43:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.254910 on epoch=34
05/23/2022 02:43:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.275299 on epoch=34
05/23/2022 02:43:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.231404 on epoch=35
05/23/2022 02:43:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.286855 on epoch=36
05/23/2022 02:43:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.281658 on epoch=37
05/23/2022 02:44:00 - INFO - __main__ - Global step 450 Train loss 0.266025 Classification-F1 0.4859437751004016 on epoch=37
05/23/2022 02:44:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.241624 on epoch=38
05/23/2022 02:44:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.194202 on epoch=39
05/23/2022 02:44:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.213741 on epoch=39
05/23/2022 02:44:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.169830 on epoch=40
05/23/2022 02:44:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.155065 on epoch=41
05/23/2022 02:44:27 - INFO - __main__ - Global step 500 Train loss 0.194892 Classification-F1 0.4859437751004016 on epoch=41
05/23/2022 02:44:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.188479 on epoch=42
05/23/2022 02:44:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.193968 on epoch=43
05/23/2022 02:44:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.147471 on epoch=44
05/23/2022 02:44:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.200571 on epoch=44
05/23/2022 02:44:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.147685 on epoch=45
05/23/2022 02:44:54 - INFO - __main__ - Global step 550 Train loss 0.175635 Classification-F1 0.4666666666666667 on epoch=45
05/23/2022 02:44:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.144059 on epoch=46
05/23/2022 02:45:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.201347 on epoch=47
05/23/2022 02:45:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.167667 on epoch=48
05/23/2022 02:45:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.121816 on epoch=49
05/23/2022 02:45:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.103856 on epoch=49
05/23/2022 02:45:21 - INFO - __main__ - Global step 600 Train loss 0.147749 Classification-F1 0.4434782608695652 on epoch=49
05/23/2022 02:45:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.104719 on epoch=50
05/23/2022 02:45:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.102082 on epoch=51
05/23/2022 02:45:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.139577 on epoch=52
05/23/2022 02:45:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.082581 on epoch=53
05/23/2022 02:45:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.081603 on epoch=54
05/23/2022 02:45:48 - INFO - __main__ - Global step 650 Train loss 0.102112 Classification-F1 0.4796747967479675 on epoch=54
05/23/2022 02:45:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.069107 on epoch=54
05/23/2022 02:45:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.129533 on epoch=55
05/23/2022 02:46:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.058749 on epoch=56
05/23/2022 02:46:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.110405 on epoch=57
05/23/2022 02:46:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.147021 on epoch=58
05/23/2022 02:46:15 - INFO - __main__ - Global step 700 Train loss 0.102963 Classification-F1 0.4796747967479675 on epoch=58
05/23/2022 02:46:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.082375 on epoch=59
05/23/2022 02:46:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.128000 on epoch=59
05/23/2022 02:46:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.065647 on epoch=60
05/23/2022 02:46:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.083122 on epoch=61
05/23/2022 02:46:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.133072 on epoch=62
05/23/2022 02:46:42 - INFO - __main__ - Global step 750 Train loss 0.098443 Classification-F1 0.4796747967479675 on epoch=62
05/23/2022 02:46:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.048958 on epoch=63
05/23/2022 02:46:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.131820 on epoch=64
05/23/2022 02:46:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.057996 on epoch=64
05/23/2022 02:47:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.051784 on epoch=65
05/23/2022 02:47:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.063424 on epoch=66
05/23/2022 02:47:09 - INFO - __main__ - Global step 800 Train loss 0.070796 Classification-F1 0.4817813765182186 on epoch=66
05/23/2022 02:47:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.042413 on epoch=67
05/23/2022 02:47:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.051527 on epoch=68
05/23/2022 02:47:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.042271 on epoch=69
05/23/2022 02:47:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.034507 on epoch=69
05/23/2022 02:47:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.038097 on epoch=70
05/23/2022 02:47:36 - INFO - __main__ - Global step 850 Train loss 0.041763 Classification-F1 0.4817813765182186 on epoch=70
05/23/2022 02:47:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.088925 on epoch=71
05/23/2022 02:47:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.052537 on epoch=72
05/23/2022 02:47:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.032409 on epoch=73
05/23/2022 02:47:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.070708 on epoch=74
05/23/2022 02:48:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.082430 on epoch=74
05/23/2022 02:48:03 - INFO - __main__ - Global step 900 Train loss 0.065402 Classification-F1 0.4732510288065844 on epoch=74
05/23/2022 02:48:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.066519 on epoch=75
05/23/2022 02:48:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.085126 on epoch=76
05/23/2022 02:48:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.048456 on epoch=77
05/23/2022 02:48:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.069134 on epoch=78
05/23/2022 02:48:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.010674 on epoch=79
05/23/2022 02:48:31 - INFO - __main__ - Global step 950 Train loss 0.055982 Classification-F1 0.4817813765182186 on epoch=79
05/23/2022 02:48:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.090480 on epoch=79
05/23/2022 02:48:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.029731 on epoch=80
05/23/2022 02:48:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.011997 on epoch=81
05/23/2022 02:48:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.029932 on epoch=82
05/23/2022 02:48:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.021954 on epoch=83
05/23/2022 02:48:58 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 02:48:58 - INFO - __main__ - Printing 3 examples
05/23/2022 02:48:58 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
05/23/2022 02:48:58 - INFO - __main__ - ['false']
05/23/2022 02:48:58 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 02:48:58 - INFO - __main__ - ['false']
05/23/2022 02:48:58 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
05/23/2022 02:48:58 - INFO - __main__ - ['false']
05/23/2022 02:48:58 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:48:58 - INFO - __main__ - Global step 1000 Train loss 0.036819 Classification-F1 0.4817813765182186 on epoch=83
05/23/2022 02:48:58 - INFO - __main__ - save last model!
05/23/2022 02:48:58 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:48:58 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 02:48:58 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 02:48:58 - INFO - __main__ - Printing 3 examples
05/23/2022 02:48:58 - INFO - __main__ -  [ethos-race] Well France is finally a shithole This is what happens when you do open borders now both sides are suffering
05/23/2022 02:48:58 - INFO - __main__ - ['false']
05/23/2022 02:48:58 - INFO - __main__ -  [ethos-race] Equality should not apply on people of different religions except christianity.
05/23/2022 02:48:58 - INFO - __main__ - ['false']
05/23/2022 02:48:58 - INFO - __main__ -  [ethos-race] kill the gays
05/23/2022 02:48:58 - INFO - __main__ - ['false']
05/23/2022 02:48:58 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:48:58 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:48:58 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 02:49:05 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 02:49:05 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 02:49:05 - INFO - __main__ - Printing 3 examples
05/23/2022 02:49:05 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 02:49:05 - INFO - __main__ - ['true']
05/23/2022 02:49:05 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 02:49:05 - INFO - __main__ - ['false']
05/23/2022 02:49:05 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 02:49:05 - INFO - __main__ - ['false']
05/23/2022 02:49:05 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:49:05 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:49:06 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 02:49:08 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_13_0.0003_8_predictions.txt
05/23/2022 02:49:08 - INFO - __main__ - Classification-F1 on test data: 0.4494
05/23/2022 02:49:08 - INFO - __main__ - prefix=ethos-race_128_13, lr=0.0003, bsz=8, dev_performance=1.0, test_performance=0.44936708860759494
05/23/2022 02:49:08 - INFO - __main__ - Running ... prefix=ethos-race_128_13, lr=0.0002, bsz=8 ...
05/23/2022 02:49:09 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 02:49:09 - INFO - __main__ - Printing 3 examples
05/23/2022 02:49:09 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
05/23/2022 02:49:09 - INFO - __main__ - ['false']
05/23/2022 02:49:09 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 02:49:09 - INFO - __main__ - ['false']
05/23/2022 02:49:09 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
05/23/2022 02:49:09 - INFO - __main__ - ['false']
05/23/2022 02:49:09 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:49:09 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:49:09 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 02:49:09 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 02:49:09 - INFO - __main__ - Printing 3 examples
05/23/2022 02:49:09 - INFO - __main__ -  [ethos-race] Well France is finally a shithole This is what happens when you do open borders now both sides are suffering
05/23/2022 02:49:09 - INFO - __main__ - ['false']
05/23/2022 02:49:09 - INFO - __main__ -  [ethos-race] Equality should not apply on people of different religions except christianity.
05/23/2022 02:49:09 - INFO - __main__ - ['false']
05/23/2022 02:49:09 - INFO - __main__ -  [ethos-race] kill the gays
05/23/2022 02:49:09 - INFO - __main__ - ['false']
05/23/2022 02:49:09 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:49:09 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:49:09 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 02:49:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 02:49:11 - INFO - __main__ - Starting training!
05/23/2022 02:49:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 02:49:20 - INFO - __main__ - Starting training!
05/23/2022 02:49:25 - INFO - __main__ - Step 10 Global step 10 Train loss 23.677456 on epoch=0
05/23/2022 02:49:30 - INFO - __main__ - Step 20 Global step 20 Train loss 18.304285 on epoch=1
05/23/2022 02:49:35 - INFO - __main__ - Step 30 Global step 30 Train loss 16.897343 on epoch=2
05/23/2022 02:49:40 - INFO - __main__ - Step 40 Global step 40 Train loss 16.296116 on epoch=3
05/23/2022 02:49:45 - INFO - __main__ - Step 50 Global step 50 Train loss 15.691966 on epoch=4
05/23/2022 02:50:19 - INFO - __main__ - Global step 50 Train loss 18.173433 Classification-F1 0.0 on epoch=4
05/23/2022 02:50:25 - INFO - __main__ - Step 60 Global step 60 Train loss 14.851916 on epoch=4
05/23/2022 02:50:30 - INFO - __main__ - Step 70 Global step 70 Train loss 13.556743 on epoch=5
05/23/2022 02:50:36 - INFO - __main__ - Step 80 Global step 80 Train loss 13.251778 on epoch=6
05/23/2022 02:50:41 - INFO - __main__ - Step 90 Global step 90 Train loss 13.155645 on epoch=7
05/23/2022 02:50:46 - INFO - __main__ - Step 100 Global step 100 Train loss 12.484426 on epoch=8
05/23/2022 02:51:14 - INFO - __main__ - Global step 100 Train loss 13.460102 Classification-F1 0.0 on epoch=8
05/23/2022 02:51:19 - INFO - __main__ - Step 110 Global step 110 Train loss 11.150981 on epoch=9
05/23/2022 02:51:24 - INFO - __main__ - Step 120 Global step 120 Train loss 9.817996 on epoch=9
05/23/2022 02:51:29 - INFO - __main__ - Step 130 Global step 130 Train loss 8.439565 on epoch=10
05/23/2022 02:51:34 - INFO - __main__ - Step 140 Global step 140 Train loss 3.544794 on epoch=11
05/23/2022 02:51:39 - INFO - __main__ - Step 150 Global step 150 Train loss 1.347124 on epoch=12
05/23/2022 02:51:41 - INFO - __main__ - Global step 150 Train loss 6.860092 Classification-F1 1.0 on epoch=12
05/23/2022 02:51:47 - INFO - __main__ - Step 160 Global step 160 Train loss 1.229045 on epoch=13
05/23/2022 02:51:52 - INFO - __main__ - Step 170 Global step 170 Train loss 2.314604 on epoch=14
05/23/2022 02:51:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.958528 on epoch=14
05/23/2022 02:52:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.360509 on epoch=15
05/23/2022 02:52:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.495425 on epoch=16
05/23/2022 02:52:08 - INFO - __main__ - Global step 200 Train loss 1.071622 Classification-F1 1.0 on epoch=16
05/23/2022 02:52:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.416193 on epoch=17
05/23/2022 02:52:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.374131 on epoch=18
05/23/2022 02:52:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.379120 on epoch=19
05/23/2022 02:52:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.449050 on epoch=19
05/23/2022 02:52:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.375454 on epoch=20
05/23/2022 02:52:35 - INFO - __main__ - Global step 250 Train loss 0.398790 Classification-F1 0.4336283185840708 on epoch=20
05/23/2022 02:52:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.319249 on epoch=21
05/23/2022 02:52:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.314632 on epoch=22
05/23/2022 02:52:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.340367 on epoch=23
05/23/2022 02:52:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.398694 on epoch=24
05/23/2022 02:53:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.325602 on epoch=24
05/23/2022 02:53:02 - INFO - __main__ - Global step 300 Train loss 0.339709 Classification-F1 0.3875598086124402 on epoch=24
05/23/2022 02:53:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.291275 on epoch=25
05/23/2022 02:53:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.233311 on epoch=26
05/23/2022 02:53:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.272146 on epoch=27
05/23/2022 02:53:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.278821 on epoch=28
05/23/2022 02:53:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.302582 on epoch=29
05/23/2022 02:53:30 - INFO - __main__ - Global step 350 Train loss 0.275627 Classification-F1 0.49407114624505927 on epoch=29
05/23/2022 02:53:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.266374 on epoch=29
05/23/2022 02:53:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.201299 on epoch=30
05/23/2022 02:53:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.208062 on epoch=31
05/23/2022 02:53:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.227122 on epoch=32
05/23/2022 02:53:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.240784 on epoch=33
05/23/2022 02:53:57 - INFO - __main__ - Global step 400 Train loss 0.228728 Classification-F1 0.4859437751004016 on epoch=33
05/23/2022 02:54:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.223380 on epoch=34
05/23/2022 02:54:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.183706 on epoch=34
05/23/2022 02:54:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.153053 on epoch=35
05/23/2022 02:54:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.191385 on epoch=36
05/23/2022 02:54:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.231123 on epoch=37
05/23/2022 02:54:24 - INFO - __main__ - Global step 450 Train loss 0.196529 Classification-F1 0.4817813765182186 on epoch=37
05/23/2022 02:54:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.182633 on epoch=38
05/23/2022 02:54:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.204580 on epoch=39
05/23/2022 02:54:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.122727 on epoch=39
05/23/2022 02:54:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.131182 on epoch=40
05/23/2022 02:54:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.163301 on epoch=41
05/23/2022 02:54:51 - INFO - __main__ - Global step 500 Train loss 0.160885 Classification-F1 0.4838709677419355 on epoch=41
05/23/2022 02:54:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.159019 on epoch=42
05/23/2022 02:55:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.171708 on epoch=43
05/23/2022 02:55:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.112028 on epoch=44
05/23/2022 02:55:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.095789 on epoch=44
05/23/2022 02:55:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.145786 on epoch=45
05/23/2022 02:55:18 - INFO - __main__ - Global step 550 Train loss 0.136866 Classification-F1 0.4817813765182186 on epoch=45
05/23/2022 02:55:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.096028 on epoch=46
05/23/2022 02:55:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.157802 on epoch=47
05/23/2022 02:55:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.103334 on epoch=48
05/23/2022 02:55:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.108262 on epoch=49
05/23/2022 02:55:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.051251 on epoch=49
05/23/2022 02:55:45 - INFO - __main__ - Global step 600 Train loss 0.103335 Classification-F1 0.4838709677419355 on epoch=49
05/23/2022 02:55:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.061840 on epoch=50
05/23/2022 02:55:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.089277 on epoch=51
05/23/2022 02:56:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.062095 on epoch=52
05/23/2022 02:56:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.040385 on epoch=53
05/23/2022 02:56:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.050155 on epoch=54
05/23/2022 02:56:12 - INFO - __main__ - Global step 650 Train loss 0.060751 Classification-F1 0.4796747967479675 on epoch=54
05/23/2022 02:56:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.049515 on epoch=54
05/23/2022 02:56:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.060971 on epoch=55
05/23/2022 02:56:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.091190 on epoch=56
05/23/2022 02:56:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.119519 on epoch=57
05/23/2022 02:56:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.043758 on epoch=58
05/23/2022 02:56:39 - INFO - __main__ - Global step 700 Train loss 0.072991 Classification-F1 0.4817813765182186 on epoch=58
05/23/2022 02:56:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.061480 on epoch=59
05/23/2022 02:56:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.087835 on epoch=59
05/23/2022 02:56:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.023045 on epoch=60
05/23/2022 02:57:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.026678 on epoch=61
05/23/2022 02:57:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.046242 on epoch=62
05/23/2022 02:57:07 - INFO - __main__ - Global step 750 Train loss 0.049056 Classification-F1 0.4796747967479675 on epoch=62
05/23/2022 02:57:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.049222 on epoch=63
05/23/2022 02:57:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.033988 on epoch=64
05/23/2022 02:57:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.022117 on epoch=64
05/23/2022 02:57:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.056457 on epoch=65
05/23/2022 02:57:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.012354 on epoch=66
05/23/2022 02:57:34 - INFO - __main__ - Global step 800 Train loss 0.034828 Classification-F1 0.4838709677419355 on epoch=66
05/23/2022 02:57:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.040870 on epoch=67
05/23/2022 02:57:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.014339 on epoch=68
05/23/2022 02:57:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.007406 on epoch=69
05/23/2022 02:57:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.018370 on epoch=69
05/23/2022 02:57:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.020454 on epoch=70
05/23/2022 02:58:01 - INFO - __main__ - Global step 850 Train loss 0.020288 Classification-F1 0.4817813765182186 on epoch=70
05/23/2022 02:58:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.021082 on epoch=71
05/23/2022 02:58:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.028128 on epoch=72
05/23/2022 02:58:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.032043 on epoch=73
05/23/2022 02:58:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.009372 on epoch=74
05/23/2022 02:58:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.007561 on epoch=74
05/23/2022 02:58:28 - INFO - __main__ - Global step 900 Train loss 0.019637 Classification-F1 0.4838709677419355 on epoch=74
05/23/2022 02:58:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.021384 on epoch=75
05/23/2022 02:58:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.017357 on epoch=76
05/23/2022 02:58:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.005373 on epoch=77
05/23/2022 02:58:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.001651 on epoch=78
05/23/2022 02:58:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.023200 on epoch=79
05/23/2022 02:58:55 - INFO - __main__ - Global step 950 Train loss 0.013793 Classification-F1 0.4817813765182186 on epoch=79
05/23/2022 02:59:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.012710 on epoch=79
05/23/2022 02:59:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.090670 on epoch=80
05/23/2022 02:59:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.018976 on epoch=81
05/23/2022 02:59:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.018087 on epoch=82
05/23/2022 02:59:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.038666 on epoch=83
05/23/2022 02:59:22 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 02:59:22 - INFO - __main__ - Printing 3 examples
05/23/2022 02:59:22 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
05/23/2022 02:59:22 - INFO - __main__ - ['false']
05/23/2022 02:59:22 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 02:59:22 - INFO - __main__ - ['false']
05/23/2022 02:59:22 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
05/23/2022 02:59:22 - INFO - __main__ - ['false']
05/23/2022 02:59:22 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:59:23 - INFO - __main__ - Global step 1000 Train loss 0.035822 Classification-F1 0.4838709677419355 on epoch=83
05/23/2022 02:59:23 - INFO - __main__ - save last model!
05/23/2022 02:59:23 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:59:23 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 02:59:23 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 02:59:23 - INFO - __main__ - Printing 3 examples
05/23/2022 02:59:23 - INFO - __main__ -  [ethos-race] Well France is finally a shithole This is what happens when you do open borders now both sides are suffering
05/23/2022 02:59:23 - INFO - __main__ - ['false']
05/23/2022 02:59:23 - INFO - __main__ -  [ethos-race] Equality should not apply on people of different religions except christianity.
05/23/2022 02:59:23 - INFO - __main__ - ['false']
05/23/2022 02:59:23 - INFO - __main__ -  [ethos-race] kill the gays
05/23/2022 02:59:23 - INFO - __main__ - ['false']
05/23/2022 02:59:23 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:59:23 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:59:23 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 02:59:29 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 02:59:30 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 02:59:30 - INFO - __main__ - Printing 3 examples
05/23/2022 02:59:30 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 02:59:30 - INFO - __main__ - ['true']
05/23/2022 02:59:30 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 02:59:30 - INFO - __main__ - ['false']
05/23/2022 02:59:30 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 02:59:30 - INFO - __main__ - ['false']
05/23/2022 02:59:30 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:59:30 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:59:30 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 02:59:32 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_13_0.0002_8_predictions.txt
05/23/2022 02:59:32 - INFO - __main__ - Classification-F1 on test data: 0.4494
05/23/2022 02:59:32 - INFO - __main__ - prefix=ethos-race_128_13, lr=0.0002, bsz=8, dev_performance=1.0, test_performance=0.44936708860759494
05/23/2022 02:59:32 - INFO - __main__ - Running ... prefix=ethos-race_128_13, lr=0.0001, bsz=8 ...
05/23/2022 02:59:33 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 02:59:33 - INFO - __main__ - Printing 3 examples
05/23/2022 02:59:33 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
05/23/2022 02:59:33 - INFO - __main__ - ['false']
05/23/2022 02:59:33 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 02:59:33 - INFO - __main__ - ['false']
05/23/2022 02:59:33 - INFO - __main__ -  [ethos-race] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
05/23/2022 02:59:33 - INFO - __main__ - ['false']
05/23/2022 02:59:33 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:59:33 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:59:34 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 02:59:34 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 02:59:34 - INFO - __main__ - Printing 3 examples
05/23/2022 02:59:34 - INFO - __main__ -  [ethos-race] Well France is finally a shithole This is what happens when you do open borders now both sides are suffering
05/23/2022 02:59:34 - INFO - __main__ - ['false']
05/23/2022 02:59:34 - INFO - __main__ -  [ethos-race] Equality should not apply on people of different religions except christianity.
05/23/2022 02:59:34 - INFO - __main__ - ['false']
05/23/2022 02:59:34 - INFO - __main__ -  [ethos-race] kill the gays
05/23/2022 02:59:34 - INFO - __main__ - ['false']
05/23/2022 02:59:34 - INFO - __main__ - Tokenizing Input ...
05/23/2022 02:59:34 - INFO - __main__ - Tokenizing Output ...
05/23/2022 02:59:34 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 02:59:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 02:59:34 - INFO - __main__ - Starting training!
05/23/2022 02:59:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 02:59:46 - INFO - __main__ - Starting training!
05/23/2022 02:59:51 - INFO - __main__ - Step 10 Global step 10 Train loss 23.960997 on epoch=0
05/23/2022 02:59:56 - INFO - __main__ - Step 20 Global step 20 Train loss 21.760283 on epoch=1
05/23/2022 03:00:01 - INFO - __main__ - Step 30 Global step 30 Train loss 19.834837 on epoch=2
05/23/2022 03:00:06 - INFO - __main__ - Step 40 Global step 40 Train loss 17.524563 on epoch=3
05/23/2022 03:00:11 - INFO - __main__ - Step 50 Global step 50 Train loss 16.897789 on epoch=4
05/23/2022 03:00:52 - INFO - __main__ - Global step 50 Train loss 19.995693 Classification-F1 0.0 on epoch=4
05/23/2022 03:00:57 - INFO - __main__ - Step 60 Global step 60 Train loss 17.230616 on epoch=4
05/23/2022 03:01:02 - INFO - __main__ - Step 70 Global step 70 Train loss 16.182087 on epoch=5
05/23/2022 03:01:08 - INFO - __main__ - Step 80 Global step 80 Train loss 15.663374 on epoch=6
05/23/2022 03:01:13 - INFO - __main__ - Step 90 Global step 90 Train loss 15.542658 on epoch=7
05/23/2022 03:01:18 - INFO - __main__ - Step 100 Global step 100 Train loss 15.617940 on epoch=8
05/23/2022 03:01:56 - INFO - __main__ - Global step 100 Train loss 16.047335 Classification-F1 0.0 on epoch=8
05/23/2022 03:02:01 - INFO - __main__ - Step 110 Global step 110 Train loss 15.273153 on epoch=9
05/23/2022 03:02:06 - INFO - __main__ - Step 120 Global step 120 Train loss 14.355368 on epoch=9
05/23/2022 03:02:12 - INFO - __main__ - Step 130 Global step 130 Train loss 14.613088 on epoch=10
05/23/2022 03:02:17 - INFO - __main__ - Step 140 Global step 140 Train loss 13.338537 on epoch=11
05/23/2022 03:02:22 - INFO - __main__ - Step 150 Global step 150 Train loss 12.936029 on epoch=12
05/23/2022 03:03:00 - INFO - __main__ - Global step 150 Train loss 14.103236 Classification-F1 0.0 on epoch=12
05/23/2022 03:03:05 - INFO - __main__ - Step 160 Global step 160 Train loss 12.769557 on epoch=13
05/23/2022 03:03:11 - INFO - __main__ - Step 170 Global step 170 Train loss 12.678982 on epoch=14
05/23/2022 03:03:16 - INFO - __main__ - Step 180 Global step 180 Train loss 12.356966 on epoch=14
05/23/2022 03:03:21 - INFO - __main__ - Step 190 Global step 190 Train loss 11.765100 on epoch=15
05/23/2022 03:03:26 - INFO - __main__ - Step 200 Global step 200 Train loss 9.934140 on epoch=16
05/23/2022 03:04:03 - INFO - __main__ - Global step 200 Train loss 11.900949 Classification-F1 0.0 on epoch=16
05/23/2022 03:04:08 - INFO - __main__ - Step 210 Global step 210 Train loss 9.822347 on epoch=17
05/23/2022 03:04:13 - INFO - __main__ - Step 220 Global step 220 Train loss 7.154506 on epoch=18
05/23/2022 03:04:18 - INFO - __main__ - Step 230 Global step 230 Train loss 2.003151 on epoch=19
05/23/2022 03:04:23 - INFO - __main__ - Step 240 Global step 240 Train loss 1.703292 on epoch=19
05/23/2022 03:04:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.618271 on epoch=20
05/23/2022 03:04:30 - INFO - __main__ - Global step 250 Train loss 4.260313 Classification-F1 0.4410480349344978 on epoch=20
05/23/2022 03:04:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.776627 on epoch=21
05/23/2022 03:04:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.414593 on epoch=22
05/23/2022 03:04:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.378076 on epoch=23
05/23/2022 03:04:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.836409 on epoch=24
05/23/2022 03:04:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.499965 on epoch=24
05/23/2022 03:04:58 - INFO - __main__ - Global step 300 Train loss 0.581134 Classification-F1 0.17419354838709677 on epoch=24
05/23/2022 03:05:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.455333 on epoch=25
05/23/2022 03:05:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.461150 on epoch=26
05/23/2022 03:05:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.382007 on epoch=27
05/23/2022 03:05:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.405610 on epoch=28
05/23/2022 03:05:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.625577 on epoch=29
05/23/2022 03:05:25 - INFO - __main__ - Global step 350 Train loss 0.465936 Classification-F1 0.4859437751004016 on epoch=29
05/23/2022 03:05:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.314495 on epoch=29
05/23/2022 03:05:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.418051 on epoch=30
05/23/2022 03:05:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.294156 on epoch=31
05/23/2022 03:05:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.409353 on epoch=32
05/23/2022 03:05:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.377903 on epoch=33
05/23/2022 03:05:53 - INFO - __main__ - Global step 400 Train loss 0.362792 Classification-F1 0.49206349206349204 on epoch=33
05/23/2022 03:05:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.605725 on epoch=34
05/23/2022 03:06:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.308583 on epoch=34
05/23/2022 03:06:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.235959 on epoch=35
05/23/2022 03:06:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.297301 on epoch=36
05/23/2022 03:06:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.216711 on epoch=37
05/23/2022 03:06:22 - INFO - __main__ - Global step 450 Train loss 0.332856 Classification-F1 0.4775510204081633 on epoch=37
05/23/2022 03:06:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.261477 on epoch=38
05/23/2022 03:06:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.227677 on epoch=39
05/23/2022 03:06:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.215172 on epoch=39
05/23/2022 03:06:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.198862 on epoch=40
05/23/2022 03:06:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.228094 on epoch=41
05/23/2022 03:06:49 - INFO - __main__ - Global step 500 Train loss 0.226256 Classification-F1 0.488 on epoch=41
05/23/2022 03:06:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.230767 on epoch=42
05/23/2022 03:06:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.210903 on epoch=43
05/23/2022 03:07:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.183121 on epoch=44
05/23/2022 03:07:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.179022 on epoch=44
05/23/2022 03:07:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.151232 on epoch=45
05/23/2022 03:07:16 - INFO - __main__ - Global step 550 Train loss 0.191009 Classification-F1 0.452991452991453 on epoch=45
05/23/2022 03:07:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.162895 on epoch=46
05/23/2022 03:07:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.220286 on epoch=47
05/23/2022 03:07:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.208525 on epoch=48
05/23/2022 03:07:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.157542 on epoch=49
05/23/2022 03:07:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.111186 on epoch=49
05/23/2022 03:07:44 - INFO - __main__ - Global step 600 Train loss 0.172087 Classification-F1 0.47540983606557374 on epoch=49
05/23/2022 03:07:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.131584 on epoch=50
05/23/2022 03:07:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.138036 on epoch=51
05/23/2022 03:07:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.126914 on epoch=52
05/23/2022 03:08:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.189637 on epoch=53
05/23/2022 03:08:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.099803 on epoch=54
05/23/2022 03:08:11 - INFO - __main__ - Global step 650 Train loss 0.137195 Classification-F1 0.488 on epoch=54
05/23/2022 03:08:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.283502 on epoch=54
05/23/2022 03:08:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.083064 on epoch=55
05/23/2022 03:08:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.134900 on epoch=56
05/23/2022 03:08:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.119774 on epoch=57
05/23/2022 03:08:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.312227 on epoch=58
05/23/2022 03:08:39 - INFO - __main__ - Global step 700 Train loss 0.186693 Classification-F1 0.4859437751004016 on epoch=58
05/23/2022 03:08:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.093611 on epoch=59
05/23/2022 03:08:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.082437 on epoch=59
05/23/2022 03:08:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.072468 on epoch=60
05/23/2022 03:08:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.098813 on epoch=61
05/23/2022 03:09:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.097603 on epoch=62
05/23/2022 03:09:06 - INFO - __main__ - Global step 750 Train loss 0.088986 Classification-F1 0.4775510204081633 on epoch=62
05/23/2022 03:09:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.056355 on epoch=63
05/23/2022 03:09:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.085452 on epoch=64
05/23/2022 03:09:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.073578 on epoch=64
05/23/2022 03:09:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.087207 on epoch=65
05/23/2022 03:09:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.019923 on epoch=66
05/23/2022 03:09:33 - INFO - __main__ - Global step 800 Train loss 0.064503 Classification-F1 0.4859437751004016 on epoch=66
05/23/2022 03:09:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.031186 on epoch=67
05/23/2022 03:09:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.082896 on epoch=68
05/23/2022 03:09:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.107233 on epoch=69
05/23/2022 03:09:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.061332 on epoch=69
05/23/2022 03:09:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.024102 on epoch=70
05/23/2022 03:10:01 - INFO - __main__ - Global step 850 Train loss 0.061350 Classification-F1 0.47540983606557374 on epoch=70
05/23/2022 03:10:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.033704 on epoch=71
05/23/2022 03:10:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.044518 on epoch=72
05/23/2022 03:10:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.048435 on epoch=73
05/23/2022 03:10:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.019104 on epoch=74
05/23/2022 03:10:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.107590 on epoch=74
05/23/2022 03:10:28 - INFO - __main__ - Global step 900 Train loss 0.050670 Classification-F1 0.47107438016528924 on epoch=74
05/23/2022 03:10:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.030305 on epoch=75
05/23/2022 03:10:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.055230 on epoch=76
05/23/2022 03:10:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.042896 on epoch=77
05/23/2022 03:10:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.044276 on epoch=78
05/23/2022 03:10:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.049980 on epoch=79
05/23/2022 03:10:56 - INFO - __main__ - Global step 950 Train loss 0.044538 Classification-F1 0.4796747967479675 on epoch=79
05/23/2022 03:11:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.037667 on epoch=79
05/23/2022 03:11:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.014961 on epoch=80
05/23/2022 03:11:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.011233 on epoch=81
05/23/2022 03:11:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.014631 on epoch=82
05/23/2022 03:11:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.054833 on epoch=83
05/23/2022 03:11:23 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 03:11:23 - INFO - __main__ - Printing 3 examples
05/23/2022 03:11:23 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 03:11:23 - INFO - __main__ - ['false']
05/23/2022 03:11:23 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
05/23/2022 03:11:23 - INFO - __main__ - ['false']
05/23/2022 03:11:23 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
05/23/2022 03:11:23 - INFO - __main__ - ['false']
05/23/2022 03:11:23 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:11:23 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:11:23 - INFO - __main__ - Global step 1000 Train loss 0.026665 Classification-F1 0.4817813765182186 on epoch=83
05/23/2022 03:11:23 - INFO - __main__ - save last model!
05/23/2022 03:11:23 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 03:11:23 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 03:11:23 - INFO - __main__ - Printing 3 examples
05/23/2022 03:11:23 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 03:11:23 - INFO - __main__ - ['false']
05/23/2022 03:11:23 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
05/23/2022 03:11:23 - INFO - __main__ - ['false']
05/23/2022 03:11:23 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
05/23/2022 03:11:23 - INFO - __main__ - ['false']
05/23/2022 03:11:23 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:11:23 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:11:23 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 03:11:30 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 03:11:30 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 03:11:30 - INFO - __main__ - Printing 3 examples
05/23/2022 03:11:30 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 03:11:30 - INFO - __main__ - ['true']
05/23/2022 03:11:30 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 03:11:30 - INFO - __main__ - ['false']
05/23/2022 03:11:30 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 03:11:30 - INFO - __main__ - ['false']
05/23/2022 03:11:30 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:11:30 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:11:31 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 03:11:32 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_13_0.0001_8_predictions.txt
05/23/2022 03:11:32 - INFO - __main__ - Classification-F1 on test data: 0.7431
05/23/2022 03:11:33 - INFO - __main__ - prefix=ethos-race_128_13, lr=0.0001, bsz=8, dev_performance=0.49206349206349204, test_performance=0.7430872483221477
05/23/2022 03:11:33 - INFO - __main__ - Running ... prefix=ethos-race_128_21, lr=0.0005, bsz=8 ...
05/23/2022 03:11:34 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 03:11:34 - INFO - __main__ - Printing 3 examples
05/23/2022 03:11:34 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 03:11:34 - INFO - __main__ - ['false']
05/23/2022 03:11:34 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
05/23/2022 03:11:34 - INFO - __main__ - ['false']
05/23/2022 03:11:34 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
05/23/2022 03:11:34 - INFO - __main__ - ['false']
05/23/2022 03:11:34 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:11:34 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:11:34 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 03:11:34 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 03:11:34 - INFO - __main__ - Printing 3 examples
05/23/2022 03:11:34 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 03:11:34 - INFO - __main__ - ['false']
05/23/2022 03:11:34 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
05/23/2022 03:11:34 - INFO - __main__ - ['false']
05/23/2022 03:11:34 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
05/23/2022 03:11:34 - INFO - __main__ - ['false']
05/23/2022 03:11:34 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:11:34 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:11:34 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 03:11:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 03:11:36 - INFO - __main__ - Starting training!
05/23/2022 03:11:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 03:11:45 - INFO - __main__ - Starting training!
05/23/2022 03:11:49 - INFO - __main__ - Step 10 Global step 10 Train loss 23.687103 on epoch=0
05/23/2022 03:11:54 - INFO - __main__ - Step 20 Global step 20 Train loss 17.919954 on epoch=1
05/23/2022 03:11:59 - INFO - __main__ - Step 30 Global step 30 Train loss 17.260900 on epoch=2
05/23/2022 03:12:04 - INFO - __main__ - Step 40 Global step 40 Train loss 13.535624 on epoch=3
05/23/2022 03:12:09 - INFO - __main__ - Step 50 Global step 50 Train loss 12.136358 on epoch=4
05/23/2022 03:12:38 - INFO - __main__ - Global step 50 Train loss 16.907988 Classification-F1 0.0 on epoch=4
05/23/2022 03:12:44 - INFO - __main__ - Step 60 Global step 60 Train loss 8.479189 on epoch=4
05/23/2022 03:12:49 - INFO - __main__ - Step 70 Global step 70 Train loss 1.741968 on epoch=5
05/23/2022 03:12:54 - INFO - __main__ - Step 80 Global step 80 Train loss 1.192653 on epoch=6
05/23/2022 03:12:59 - INFO - __main__ - Step 90 Global step 90 Train loss 1.521101 on epoch=7
05/23/2022 03:13:04 - INFO - __main__ - Step 100 Global step 100 Train loss 1.251509 on epoch=8
05/23/2022 03:13:06 - INFO - __main__ - Global step 100 Train loss 2.837284 Classification-F1 1.0 on epoch=8
05/23/2022 03:13:12 - INFO - __main__ - Step 110 Global step 110 Train loss 2.285671 on epoch=9
05/23/2022 03:13:17 - INFO - __main__ - Step 120 Global step 120 Train loss 1.892259 on epoch=9
05/23/2022 03:13:22 - INFO - __main__ - Step 130 Global step 130 Train loss 1.857991 on epoch=10
05/23/2022 03:13:27 - INFO - __main__ - Step 140 Global step 140 Train loss 1.275756 on epoch=11
05/23/2022 03:13:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.892228 on epoch=12
05/23/2022 03:13:34 - INFO - __main__ - Global step 150 Train loss 1.640781 Classification-F1 1.0 on epoch=12
05/23/2022 03:13:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.632116 on epoch=13
05/23/2022 03:13:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.472635 on epoch=14
05/23/2022 03:13:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.685395 on epoch=14
05/23/2022 03:13:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.379917 on epoch=15
05/23/2022 03:13:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.565505 on epoch=16
05/23/2022 03:14:01 - INFO - __main__ - Global step 200 Train loss 0.547114 Classification-F1 1.0 on epoch=16
05/23/2022 03:14:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.375376 on epoch=17
05/23/2022 03:14:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.415856 on epoch=18
05/23/2022 03:14:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.371862 on epoch=19
05/23/2022 03:14:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.372410 on epoch=19
05/23/2022 03:14:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.345587 on epoch=20
05/23/2022 03:14:28 - INFO - __main__ - Global step 250 Train loss 0.376218 Classification-F1 0.3263157894736842 on epoch=20
05/23/2022 03:14:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.382345 on epoch=21
05/23/2022 03:14:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.349285 on epoch=22
05/23/2022 03:14:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.400317 on epoch=23
05/23/2022 03:14:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.390959 on epoch=24
05/23/2022 03:14:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.387692 on epoch=24
05/23/2022 03:14:55 - INFO - __main__ - Global step 300 Train loss 0.382120 Classification-F1 0.03759398496240601 on epoch=24
05/23/2022 03:15:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.321034 on epoch=25
05/23/2022 03:15:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.375538 on epoch=26
05/23/2022 03:15:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.387466 on epoch=27
05/23/2022 03:15:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.381013 on epoch=28
05/23/2022 03:15:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.379516 on epoch=29
05/23/2022 03:15:22 - INFO - __main__ - Global step 350 Train loss 0.368913 Classification-F1 1.0 on epoch=29
05/23/2022 03:15:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.353205 on epoch=29
05/23/2022 03:15:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.333837 on epoch=30
05/23/2022 03:15:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.321796 on epoch=31
05/23/2022 03:15:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.340657 on epoch=32
05/23/2022 03:15:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.332656 on epoch=33
05/23/2022 03:15:49 - INFO - __main__ - Global step 400 Train loss 0.336430 Classification-F1 1.0 on epoch=33
05/23/2022 03:15:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.328778 on epoch=34
05/23/2022 03:15:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.340586 on epoch=34
05/23/2022 03:16:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.296883 on epoch=35
05/23/2022 03:16:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.310333 on epoch=36
05/23/2022 03:16:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.310122 on epoch=37
05/23/2022 03:16:15 - INFO - __main__ - Global step 450 Train loss 0.317340 Classification-F1 1.0 on epoch=37
05/23/2022 03:16:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.323046 on epoch=38
05/23/2022 03:16:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.362322 on epoch=39
05/23/2022 03:16:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.314213 on epoch=39
05/23/2022 03:16:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.294324 on epoch=40
05/23/2022 03:16:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.366154 on epoch=41
05/23/2022 03:16:42 - INFO - __main__ - Global step 500 Train loss 0.332012 Classification-F1 1.0 on epoch=41
05/23/2022 03:16:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.338391 on epoch=42
05/23/2022 03:16:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.330082 on epoch=43
05/23/2022 03:16:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.388560 on epoch=44
05/23/2022 03:17:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.335814 on epoch=44
05/23/2022 03:17:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.329034 on epoch=45
05/23/2022 03:17:09 - INFO - __main__ - Global step 550 Train loss 0.344376 Classification-F1 0.49206349206349204 on epoch=45
05/23/2022 03:17:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.354973 on epoch=46
05/23/2022 03:17:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.352852 on epoch=47
05/23/2022 03:17:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.346767 on epoch=48
05/23/2022 03:17:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.350701 on epoch=49
05/23/2022 03:17:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.350593 on epoch=49
05/23/2022 03:17:36 - INFO - __main__ - Global step 600 Train loss 0.351177 Classification-F1 0.4576271186440678 on epoch=49
05/23/2022 03:17:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.338411 on epoch=50
05/23/2022 03:17:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.322293 on epoch=51
05/23/2022 03:17:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.334176 on epoch=52
05/23/2022 03:17:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.334170 on epoch=53
05/23/2022 03:18:02 - INFO - __main__ - Step 650 Global step 650 Train loss 0.341830 on epoch=54
05/23/2022 03:18:03 - INFO - __main__ - Global step 650 Train loss 0.334176 Classification-F1 0.4980392156862745 on epoch=54
05/23/2022 03:18:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.355873 on epoch=54
05/23/2022 03:18:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.311707 on epoch=55
05/23/2022 03:18:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.327477 on epoch=56
05/23/2022 03:18:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.297865 on epoch=57
05/23/2022 03:18:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.338875 on epoch=58
05/23/2022 03:18:30 - INFO - __main__ - Global step 700 Train loss 0.326359 Classification-F1 0.4980392156862745 on epoch=58
05/23/2022 03:18:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.319172 on epoch=59
05/23/2022 03:18:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.347648 on epoch=59
05/23/2022 03:18:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.277495 on epoch=60
05/23/2022 03:18:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.287760 on epoch=61
05/23/2022 03:18:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.271436 on epoch=62
05/23/2022 03:18:57 - INFO - __main__ - Global step 750 Train loss 0.300702 Classification-F1 0.4900398406374502 on epoch=62
05/23/2022 03:19:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.252188 on epoch=63
05/23/2022 03:19:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.295643 on epoch=64
05/23/2022 03:19:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.264101 on epoch=64
05/23/2022 03:19:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.256860 on epoch=65
05/23/2022 03:19:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.168691 on epoch=66
05/23/2022 03:19:24 - INFO - __main__ - Global step 800 Train loss 0.247496 Classification-F1 0.4980392156862745 on epoch=66
05/23/2022 03:19:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.132964 on epoch=67
05/23/2022 03:19:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.171381 on epoch=68
05/23/2022 03:19:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.168553 on epoch=69
05/23/2022 03:19:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.182304 on epoch=69
05/23/2022 03:19:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.179033 on epoch=70
05/23/2022 03:19:51 - INFO - __main__ - Global step 850 Train loss 0.166847 Classification-F1 0.46218487394957986 on epoch=70
05/23/2022 03:19:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.215482 on epoch=71
05/23/2022 03:20:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.161844 on epoch=72
05/23/2022 03:20:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.127785 on epoch=73
05/23/2022 03:20:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.135776 on epoch=74
05/23/2022 03:20:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.250262 on epoch=74
05/23/2022 03:20:18 - INFO - __main__ - Global step 900 Train loss 0.178230 Classification-F1 0.49206349206349204 on epoch=74
05/23/2022 03:20:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.196884 on epoch=75
05/23/2022 03:20:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.184323 on epoch=76
05/23/2022 03:20:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.173844 on epoch=77
05/23/2022 03:20:39 - INFO - __main__ - Step 940 Global step 940 Train loss 1.060932 on epoch=78
05/23/2022 03:20:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.199469 on epoch=79
05/23/2022 03:20:45 - INFO - __main__ - Global step 950 Train loss 0.363090 Classification-F1 0.49407114624505927 on epoch=79
05/23/2022 03:20:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.178390 on epoch=79
05/23/2022 03:20:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.208136 on epoch=80
05/23/2022 03:21:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.204637 on epoch=81
05/23/2022 03:21:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.241515 on epoch=82
05/23/2022 03:21:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.248708 on epoch=83
05/23/2022 03:21:12 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 03:21:12 - INFO - __main__ - Printing 3 examples
05/23/2022 03:21:12 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 03:21:12 - INFO - __main__ - ['false']
05/23/2022 03:21:12 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
05/23/2022 03:21:12 - INFO - __main__ - ['false']
05/23/2022 03:21:12 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
05/23/2022 03:21:12 - INFO - __main__ - ['false']
05/23/2022 03:21:12 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:21:12 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:21:12 - INFO - __main__ - Global step 1000 Train loss 0.216277 Classification-F1 0.4817813765182186 on epoch=83
05/23/2022 03:21:12 - INFO - __main__ - save last model!
05/23/2022 03:21:12 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 03:21:12 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 03:21:12 - INFO - __main__ - Printing 3 examples
05/23/2022 03:21:12 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 03:21:12 - INFO - __main__ - ['false']
05/23/2022 03:21:12 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
05/23/2022 03:21:12 - INFO - __main__ - ['false']
05/23/2022 03:21:12 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
05/23/2022 03:21:12 - INFO - __main__ - ['false']
05/23/2022 03:21:12 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:21:12 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:21:12 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 03:21:19 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 03:21:20 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 03:21:20 - INFO - __main__ - Printing 3 examples
05/23/2022 03:21:20 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 03:21:20 - INFO - __main__ - ['true']
05/23/2022 03:21:20 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 03:21:20 - INFO - __main__ - ['false']
05/23/2022 03:21:20 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 03:21:20 - INFO - __main__ - ['false']
05/23/2022 03:21:20 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:21:20 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:21:20 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 03:21:22 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_21_0.0005_8_predictions.txt
05/23/2022 03:21:22 - INFO - __main__ - Classification-F1 on test data: 0.3426
05/23/2022 03:21:22 - INFO - __main__ - prefix=ethos-race_128_21, lr=0.0005, bsz=8, dev_performance=1.0, test_performance=0.34263448969331317
05/23/2022 03:21:22 - INFO - __main__ - Running ... prefix=ethos-race_128_21, lr=0.0003, bsz=8 ...
05/23/2022 03:21:23 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 03:21:23 - INFO - __main__ - Printing 3 examples
05/23/2022 03:21:23 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 03:21:23 - INFO - __main__ - ['false']
05/23/2022 03:21:23 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
05/23/2022 03:21:23 - INFO - __main__ - ['false']
05/23/2022 03:21:23 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
05/23/2022 03:21:23 - INFO - __main__ - ['false']
05/23/2022 03:21:23 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:21:23 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:21:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 03:21:23 - INFO - __main__ - Starting training!
05/23/2022 03:21:23 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 03:21:23 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 03:21:23 - INFO - __main__ - Printing 3 examples
05/23/2022 03:21:23 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 03:21:23 - INFO - __main__ - ['false']
05/23/2022 03:21:23 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
05/23/2022 03:21:23 - INFO - __main__ - ['false']
05/23/2022 03:21:23 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
05/23/2022 03:21:23 - INFO - __main__ - ['false']
05/23/2022 03:21:23 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:21:23 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:21:24 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 03:21:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 03:21:34 - INFO - __main__ - Starting training!
05/23/2022 03:21:39 - INFO - __main__ - Step 10 Global step 10 Train loss 23.702604 on epoch=0
05/23/2022 03:21:44 - INFO - __main__ - Step 20 Global step 20 Train loss 18.571177 on epoch=1
05/23/2022 03:21:49 - INFO - __main__ - Step 30 Global step 30 Train loss 17.108749 on epoch=2
05/23/2022 03:21:54 - INFO - __main__ - Step 40 Global step 40 Train loss 15.687799 on epoch=3
05/23/2022 03:21:59 - INFO - __main__ - Step 50 Global step 50 Train loss 14.356442 on epoch=4
05/23/2022 03:22:23 - INFO - __main__ - Global step 50 Train loss 17.885355 Classification-F1 0.0 on epoch=4
05/23/2022 03:22:29 - INFO - __main__ - Step 60 Global step 60 Train loss 13.509100 on epoch=4
05/23/2022 03:22:34 - INFO - __main__ - Step 70 Global step 70 Train loss 13.593536 on epoch=5
05/23/2022 03:22:39 - INFO - __main__ - Step 80 Global step 80 Train loss 12.469866 on epoch=6
05/23/2022 03:22:45 - INFO - __main__ - Step 90 Global step 90 Train loss 9.786642 on epoch=7
05/23/2022 03:22:50 - INFO - __main__ - Step 100 Global step 100 Train loss 7.922228 on epoch=8
05/23/2022 03:23:14 - INFO - __main__ - Global step 100 Train loss 11.456273 Classification-F1 0.0 on epoch=8
05/23/2022 03:23:19 - INFO - __main__ - Step 110 Global step 110 Train loss 4.313297 on epoch=9
05/23/2022 03:23:24 - INFO - __main__ - Step 120 Global step 120 Train loss 2.685916 on epoch=9
05/23/2022 03:23:29 - INFO - __main__ - Step 130 Global step 130 Train loss 1.294681 on epoch=10
05/23/2022 03:23:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.681968 on epoch=11
05/23/2022 03:23:39 - INFO - __main__ - Step 150 Global step 150 Train loss 1.199859 on epoch=12
05/23/2022 03:23:40 - INFO - __main__ - Global step 150 Train loss 2.035145 Classification-F1 1.0 on epoch=12
05/23/2022 03:23:47 - INFO - __main__ - Step 160 Global step 160 Train loss 1.745154 on epoch=13
05/23/2022 03:23:52 - INFO - __main__ - Step 170 Global step 170 Train loss 1.431977 on epoch=14
05/23/2022 03:23:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.613719 on epoch=14
05/23/2022 03:24:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.427561 on epoch=15
05/23/2022 03:24:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.494956 on epoch=16
05/23/2022 03:24:08 - INFO - __main__ - Global step 200 Train loss 0.942673 Classification-F1 1.0 on epoch=16
05/23/2022 03:24:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.416092 on epoch=17
05/23/2022 03:24:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.398800 on epoch=18
05/23/2022 03:24:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.433848 on epoch=19
05/23/2022 03:24:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.369571 on epoch=19
05/23/2022 03:24:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.358903 on epoch=20
05/23/2022 03:24:35 - INFO - __main__ - Global step 250 Train loss 0.395443 Classification-F1 0.3298429319371728 on epoch=20
05/23/2022 03:24:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.423196 on epoch=21
05/23/2022 03:24:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.330507 on epoch=22
05/23/2022 03:24:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.405690 on epoch=23
05/23/2022 03:24:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.397315 on epoch=24
05/23/2022 03:25:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.323274 on epoch=24
05/23/2022 03:25:02 - INFO - __main__ - Global step 300 Train loss 0.375996 Classification-F1 0.16339869281045752 on epoch=24
05/23/2022 03:25:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.309658 on epoch=25
05/23/2022 03:25:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.396643 on epoch=26
05/23/2022 03:25:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.330565 on epoch=27
05/23/2022 03:25:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.346016 on epoch=28
05/23/2022 03:25:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.323931 on epoch=29
05/23/2022 03:25:29 - INFO - __main__ - Global step 350 Train loss 0.341363 Classification-F1 0.49407114624505927 on epoch=29
05/23/2022 03:25:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.315204 on epoch=29
05/23/2022 03:25:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.429883 on epoch=30
05/23/2022 03:25:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.283291 on epoch=31
05/23/2022 03:25:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.240928 on epoch=32
05/23/2022 03:25:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.334826 on epoch=33
05/23/2022 03:25:55 - INFO - __main__ - Global step 400 Train loss 0.320826 Classification-F1 0.4900398406374502 on epoch=33
05/23/2022 03:26:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.345925 on epoch=34
05/23/2022 03:26:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.283676 on epoch=34
05/23/2022 03:26:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.259743 on epoch=35
05/23/2022 03:26:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.297187 on epoch=36
05/23/2022 03:26:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.208555 on epoch=37
05/23/2022 03:26:22 - INFO - __main__ - Global step 450 Train loss 0.279017 Classification-F1 0.49206349206349204 on epoch=37
05/23/2022 03:26:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.219968 on epoch=38
05/23/2022 03:26:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.222469 on epoch=39
05/23/2022 03:26:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.261599 on epoch=39
05/23/2022 03:26:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.232338 on epoch=40
05/23/2022 03:26:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.232050 on epoch=41
05/23/2022 03:26:49 - INFO - __main__ - Global step 500 Train loss 0.233685 Classification-F1 0.49407114624505927 on epoch=41
05/23/2022 03:26:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.177129 on epoch=42
05/23/2022 03:26:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.192006 on epoch=43
05/23/2022 03:27:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.197496 on epoch=44
05/23/2022 03:27:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.204318 on epoch=44
05/23/2022 03:27:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.327524 on epoch=45
05/23/2022 03:27:16 - INFO - __main__ - Global step 550 Train loss 0.219694 Classification-F1 0.4732510288065844 on epoch=45
05/23/2022 03:27:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.228823 on epoch=46
05/23/2022 03:27:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.141415 on epoch=47
05/23/2022 03:27:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.144496 on epoch=48
05/23/2022 03:27:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.122040 on epoch=49
05/23/2022 03:27:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.165093 on epoch=49
05/23/2022 03:27:42 - INFO - __main__ - Global step 600 Train loss 0.160373 Classification-F1 0.375609756097561 on epoch=49
05/23/2022 03:27:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.194753 on epoch=50
05/23/2022 03:27:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.200609 on epoch=51
05/23/2022 03:27:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.137211 on epoch=52
05/23/2022 03:28:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.171508 on epoch=53
05/23/2022 03:28:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.165589 on epoch=54
05/23/2022 03:28:09 - INFO - __main__ - Global step 650 Train loss 0.173934 Classification-F1 0.4859437751004016 on epoch=54
05/23/2022 03:28:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.208403 on epoch=54
05/23/2022 03:28:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.140353 on epoch=55
05/23/2022 03:28:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.150714 on epoch=56
05/23/2022 03:28:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.091659 on epoch=57
05/23/2022 03:28:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.113970 on epoch=58
05/23/2022 03:28:36 - INFO - __main__ - Global step 700 Train loss 0.141020 Classification-F1 0.49206349206349204 on epoch=58
05/23/2022 03:28:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.161674 on epoch=59
05/23/2022 03:28:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.194435 on epoch=59
05/23/2022 03:28:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.155315 on epoch=60
05/23/2022 03:28:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.137624 on epoch=61
05/23/2022 03:29:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.078560 on epoch=62
05/23/2022 03:29:03 - INFO - __main__ - Global step 750 Train loss 0.145522 Classification-F1 0.49206349206349204 on epoch=62
05/23/2022 03:29:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.136566 on epoch=63
05/23/2022 03:29:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.143420 on epoch=64
05/23/2022 03:29:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.138396 on epoch=64
05/23/2022 03:29:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.165026 on epoch=65
05/23/2022 03:29:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.117531 on epoch=66
05/23/2022 03:29:30 - INFO - __main__ - Global step 800 Train loss 0.140188 Classification-F1 0.49407114624505927 on epoch=66
05/23/2022 03:29:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.119950 on epoch=67
05/23/2022 03:29:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.059977 on epoch=68
05/23/2022 03:29:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.109481 on epoch=69
05/23/2022 03:29:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.119515 on epoch=69
05/23/2022 03:29:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.106042 on epoch=70
05/23/2022 03:29:57 - INFO - __main__ - Global step 850 Train loss 0.102993 Classification-F1 0.4817813765182186 on epoch=70
05/23/2022 03:30:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.060099 on epoch=71
05/23/2022 03:30:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.058171 on epoch=72
05/23/2022 03:30:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.064100 on epoch=73
05/23/2022 03:30:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.068583 on epoch=74
05/23/2022 03:30:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.025242 on epoch=74
05/23/2022 03:30:23 - INFO - __main__ - Global step 900 Train loss 0.055239 Classification-F1 0.4155251141552511 on epoch=74
05/23/2022 03:30:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.050609 on epoch=75
05/23/2022 03:30:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.071012 on epoch=76
05/23/2022 03:30:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.029714 on epoch=77
05/23/2022 03:30:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.062940 on epoch=78
05/23/2022 03:30:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.027509 on epoch=79
05/23/2022 03:30:50 - INFO - __main__ - Global step 950 Train loss 0.048357 Classification-F1 0.488 on epoch=79
05/23/2022 03:30:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.176212 on epoch=79
05/23/2022 03:31:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.057993 on epoch=80
05/23/2022 03:31:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.078806 on epoch=81
05/23/2022 03:31:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.059418 on epoch=82
05/23/2022 03:31:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.084861 on epoch=83
05/23/2022 03:31:17 - INFO - __main__ - Global step 1000 Train loss 0.091458 Classification-F1 0.4900398406374502 on epoch=83
05/23/2022 03:31:17 - INFO - __main__ - save last model!
05/23/2022 03:31:17 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 03:31:17 - INFO - __main__ - Printing 3 examples
05/23/2022 03:31:17 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 03:31:17 - INFO - __main__ - ['false']
05/23/2022 03:31:17 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
05/23/2022 03:31:17 - INFO - __main__ - ['false']
05/23/2022 03:31:17 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
05/23/2022 03:31:17 - INFO - __main__ - ['false']
05/23/2022 03:31:17 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:31:17 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:31:17 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 03:31:17 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 03:31:17 - INFO - __main__ - Printing 3 examples
05/23/2022 03:31:17 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 03:31:17 - INFO - __main__ - ['false']
05/23/2022 03:31:17 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
05/23/2022 03:31:17 - INFO - __main__ - ['false']
05/23/2022 03:31:17 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
05/23/2022 03:31:17 - INFO - __main__ - ['false']
05/23/2022 03:31:17 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:31:18 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:31:18 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 03:31:24 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 03:31:25 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 03:31:25 - INFO - __main__ - Printing 3 examples
05/23/2022 03:31:25 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 03:31:25 - INFO - __main__ - ['true']
05/23/2022 03:31:25 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 03:31:25 - INFO - __main__ - ['false']
05/23/2022 03:31:25 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 03:31:25 - INFO - __main__ - ['false']
05/23/2022 03:31:25 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:31:25 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:31:25 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 03:31:27 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_21_0.0003_8_predictions.txt
05/23/2022 03:31:27 - INFO - __main__ - Classification-F1 on test data: 0.4494
05/23/2022 03:31:27 - INFO - __main__ - prefix=ethos-race_128_21, lr=0.0003, bsz=8, dev_performance=1.0, test_performance=0.44936708860759494
05/23/2022 03:31:27 - INFO - __main__ - Running ... prefix=ethos-race_128_21, lr=0.0002, bsz=8 ...
05/23/2022 03:31:28 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 03:31:28 - INFO - __main__ - Printing 3 examples
05/23/2022 03:31:28 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 03:31:28 - INFO - __main__ - ['false']
05/23/2022 03:31:28 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
05/23/2022 03:31:28 - INFO - __main__ - ['false']
05/23/2022 03:31:28 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
05/23/2022 03:31:28 - INFO - __main__ - ['false']
05/23/2022 03:31:28 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:31:28 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:31:28 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 03:31:28 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 03:31:28 - INFO - __main__ - Printing 3 examples
05/23/2022 03:31:28 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 03:31:28 - INFO - __main__ - ['false']
05/23/2022 03:31:28 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
05/23/2022 03:31:28 - INFO - __main__ - ['false']
05/23/2022 03:31:28 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
05/23/2022 03:31:28 - INFO - __main__ - ['false']
05/23/2022 03:31:28 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:31:28 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:31:28 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 03:31:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 03:31:28 - INFO - __main__ - Starting training!
05/23/2022 03:31:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 03:31:41 - INFO - __main__ - Starting training!
05/23/2022 03:31:45 - INFO - __main__ - Step 10 Global step 10 Train loss 23.717916 on epoch=0
05/23/2022 03:31:50 - INFO - __main__ - Step 20 Global step 20 Train loss 20.351223 on epoch=1
05/23/2022 03:31:55 - INFO - __main__ - Step 30 Global step 30 Train loss 18.236065 on epoch=2
05/23/2022 03:32:01 - INFO - __main__ - Step 40 Global step 40 Train loss 17.269472 on epoch=3
05/23/2022 03:32:06 - INFO - __main__ - Step 50 Global step 50 Train loss 16.040583 on epoch=4
05/23/2022 03:32:44 - INFO - __main__ - Global step 50 Train loss 19.123051 Classification-F1 0.0 on epoch=4
05/23/2022 03:32:49 - INFO - __main__ - Step 60 Global step 60 Train loss 15.564212 on epoch=4
05/23/2022 03:32:55 - INFO - __main__ - Step 70 Global step 70 Train loss 14.331274 on epoch=5
05/23/2022 03:33:00 - INFO - __main__ - Step 80 Global step 80 Train loss 13.740372 on epoch=6
05/23/2022 03:33:05 - INFO - __main__ - Step 90 Global step 90 Train loss 13.507919 on epoch=7
05/23/2022 03:33:10 - INFO - __main__ - Step 100 Global step 100 Train loss 11.981642 on epoch=8
05/23/2022 03:33:43 - INFO - __main__ - Global step 100 Train loss 13.825083 Classification-F1 0.0 on epoch=8
05/23/2022 03:33:48 - INFO - __main__ - Step 110 Global step 110 Train loss 11.250026 on epoch=9
05/23/2022 03:33:53 - INFO - __main__ - Step 120 Global step 120 Train loss 10.083825 on epoch=9
05/23/2022 03:33:58 - INFO - __main__ - Step 130 Global step 130 Train loss 6.938332 on epoch=10
05/23/2022 03:34:04 - INFO - __main__ - Step 140 Global step 140 Train loss 2.511932 on epoch=11
05/23/2022 03:34:09 - INFO - __main__ - Step 150 Global step 150 Train loss 0.817752 on epoch=12
05/23/2022 03:34:10 - INFO - __main__ - Global step 150 Train loss 6.320374 Classification-F1 0.49606299212598426 on epoch=12
05/23/2022 03:34:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.440842 on epoch=13
05/23/2022 03:34:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.567441 on epoch=14
05/23/2022 03:34:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.461479 on epoch=14
05/23/2022 03:34:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.359701 on epoch=15
05/23/2022 03:34:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.318799 on epoch=16
05/23/2022 03:34:38 - INFO - __main__ - Global step 200 Train loss 0.429652 Classification-F1 0.4980392156862745 on epoch=16
05/23/2022 03:34:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.307273 on epoch=17
05/23/2022 03:34:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.313716 on epoch=18
05/23/2022 03:34:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.280602 on epoch=19
05/23/2022 03:34:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.226490 on epoch=19
05/23/2022 03:35:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.368054 on epoch=20
05/23/2022 03:35:05 - INFO - __main__ - Global step 250 Train loss 0.299227 Classification-F1 0.36633663366336633 on epoch=20
05/23/2022 03:35:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.240454 on epoch=21
05/23/2022 03:35:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.161428 on epoch=22
05/23/2022 03:35:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.173756 on epoch=23
05/23/2022 03:35:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.179538 on epoch=24
05/23/2022 03:35:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.214880 on epoch=24
05/23/2022 03:35:32 - INFO - __main__ - Global step 300 Train loss 0.194011 Classification-F1 0.47540983606557374 on epoch=24
05/23/2022 03:35:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.229102 on epoch=25
05/23/2022 03:35:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.188184 on epoch=26
05/23/2022 03:35:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.161172 on epoch=27
05/23/2022 03:35:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.205794 on epoch=28
05/23/2022 03:35:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.210815 on epoch=29
05/23/2022 03:35:59 - INFO - __main__ - Global step 350 Train loss 0.199013 Classification-F1 0.459915611814346 on epoch=29
05/23/2022 03:36:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.320202 on epoch=29
05/23/2022 03:36:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.135784 on epoch=30
05/23/2022 03:36:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.167570 on epoch=31
05/23/2022 03:36:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.053235 on epoch=32
05/23/2022 03:36:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.133311 on epoch=33
05/23/2022 03:36:26 - INFO - __main__ - Global step 400 Train loss 0.162020 Classification-F1 0.4900398406374502 on epoch=33
05/23/2022 03:36:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.087688 on epoch=34
05/23/2022 03:36:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.073708 on epoch=34
05/23/2022 03:36:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.065143 on epoch=35
05/23/2022 03:36:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.078462 on epoch=36
05/23/2022 03:36:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.060089 on epoch=37
05/23/2022 03:36:53 - INFO - __main__ - Global step 450 Train loss 0.073018 Classification-F1 0.49407114624505927 on epoch=37
05/23/2022 03:36:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.090039 on epoch=38
05/23/2022 03:37:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.078693 on epoch=39
05/23/2022 03:37:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.074298 on epoch=39
05/23/2022 03:37:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.059250 on epoch=40
05/23/2022 03:37:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.039648 on epoch=41
05/23/2022 03:37:20 - INFO - __main__ - Global step 500 Train loss 0.068386 Classification-F1 0.4838709677419355 on epoch=41
05/23/2022 03:37:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.043904 on epoch=42
05/23/2022 03:37:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.029174 on epoch=43
05/23/2022 03:37:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.032551 on epoch=44
05/23/2022 03:37:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.030743 on epoch=44
05/23/2022 03:37:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.036568 on epoch=45
05/23/2022 03:37:47 - INFO - __main__ - Global step 550 Train loss 0.034588 Classification-F1 0.4796747967479675 on epoch=45
05/23/2022 03:37:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.018064 on epoch=46
05/23/2022 03:37:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.012676 on epoch=47
05/23/2022 03:38:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.032248 on epoch=48
05/23/2022 03:38:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.039081 on epoch=49
05/23/2022 03:38:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.037986 on epoch=49
05/23/2022 03:38:13 - INFO - __main__ - Global step 600 Train loss 0.028011 Classification-F1 0.47540983606557374 on epoch=49
05/23/2022 03:38:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.039699 on epoch=50
05/23/2022 03:38:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.044711 on epoch=51
05/23/2022 03:38:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.025977 on epoch=52
05/23/2022 03:38:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.019733 on epoch=53
05/23/2022 03:38:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.190377 on epoch=54
05/23/2022 03:38:40 - INFO - __main__ - Global step 650 Train loss 0.064099 Classification-F1 0.4817813765182186 on epoch=54
05/23/2022 03:38:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.004997 on epoch=54
05/23/2022 03:38:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.012074 on epoch=55
05/23/2022 03:38:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.011123 on epoch=56
05/23/2022 03:39:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.038469 on epoch=57
05/23/2022 03:39:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.013281 on epoch=58
05/23/2022 03:39:07 - INFO - __main__ - Global step 700 Train loss 0.015989 Classification-F1 0.4859437751004016 on epoch=58
05/23/2022 03:39:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.002641 on epoch=59
05/23/2022 03:39:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.007639 on epoch=59
05/23/2022 03:39:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.007987 on epoch=60
05/23/2022 03:39:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.012537 on epoch=61
05/23/2022 03:39:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.032659 on epoch=62
05/23/2022 03:39:34 - INFO - __main__ - Global step 750 Train loss 0.012693 Classification-F1 0.4838709677419355 on epoch=62
05/23/2022 03:39:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.012437 on epoch=63
05/23/2022 03:39:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.003187 on epoch=64
05/23/2022 03:39:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.054560 on epoch=64
05/23/2022 03:39:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.009975 on epoch=65
05/23/2022 03:39:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.002337 on epoch=66
05/23/2022 03:40:00 - INFO - __main__ - Global step 800 Train loss 0.016499 Classification-F1 0.4817813765182186 on epoch=66
05/23/2022 03:40:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.001197 on epoch=67
05/23/2022 03:40:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.013525 on epoch=68
05/23/2022 03:40:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.018261 on epoch=69
05/23/2022 03:40:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.003916 on epoch=69
05/23/2022 03:40:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.024412 on epoch=70
05/23/2022 03:40:26 - INFO - __main__ - Global step 850 Train loss 0.012262 Classification-F1 0.4796747967479675 on epoch=70
05/23/2022 03:40:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.018922 on epoch=71
05/23/2022 03:40:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.003104 on epoch=72
05/23/2022 03:40:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.004631 on epoch=73
05/23/2022 03:40:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.003513 on epoch=74
05/23/2022 03:40:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000680 on epoch=74
05/23/2022 03:40:53 - INFO - __main__ - Global step 900 Train loss 0.006170 Classification-F1 0.4817813765182186 on epoch=74
05/23/2022 03:40:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.026304 on epoch=75
05/23/2022 03:41:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.008799 on epoch=76
05/23/2022 03:41:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000966 on epoch=77
05/23/2022 03:41:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.007920 on epoch=78
05/23/2022 03:41:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.063668 on epoch=79
05/23/2022 03:41:19 - INFO - __main__ - Global step 950 Train loss 0.021532 Classification-F1 0.4817813765182186 on epoch=79
05/23/2022 03:41:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.011249 on epoch=79
05/23/2022 03:41:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.001594 on epoch=80
05/23/2022 03:41:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.001599 on epoch=81
05/23/2022 03:41:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000889 on epoch=82
05/23/2022 03:41:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000362 on epoch=83
05/23/2022 03:41:46 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 03:41:46 - INFO - __main__ - Printing 3 examples
05/23/2022 03:41:46 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 03:41:46 - INFO - __main__ - ['false']
05/23/2022 03:41:46 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
05/23/2022 03:41:46 - INFO - __main__ - ['false']
05/23/2022 03:41:46 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
05/23/2022 03:41:46 - INFO - __main__ - ['false']
05/23/2022 03:41:46 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:41:46 - INFO - __main__ - Global step 1000 Train loss 0.003139 Classification-F1 0.4796747967479675 on epoch=83
05/23/2022 03:41:46 - INFO - __main__ - save last model!
05/23/2022 03:41:46 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:41:46 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 03:41:46 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 03:41:46 - INFO - __main__ - Printing 3 examples
05/23/2022 03:41:46 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 03:41:46 - INFO - __main__ - ['false']
05/23/2022 03:41:46 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
05/23/2022 03:41:46 - INFO - __main__ - ['false']
05/23/2022 03:41:46 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
05/23/2022 03:41:46 - INFO - __main__ - ['false']
05/23/2022 03:41:46 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:41:46 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:41:46 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 03:41:53 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 03:41:54 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 03:41:54 - INFO - __main__ - Printing 3 examples
05/23/2022 03:41:54 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 03:41:54 - INFO - __main__ - ['true']
05/23/2022 03:41:54 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 03:41:54 - INFO - __main__ - ['false']
05/23/2022 03:41:54 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 03:41:54 - INFO - __main__ - ['false']
05/23/2022 03:41:54 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:41:54 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:41:54 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 03:41:56 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_21_0.0002_8_predictions.txt
05/23/2022 03:41:56 - INFO - __main__ - Classification-F1 on test data: 0.5111
05/23/2022 03:41:56 - INFO - __main__ - prefix=ethos-race_128_21, lr=0.0002, bsz=8, dev_performance=0.4980392156862745, test_performance=0.5110528287748219
05/23/2022 03:41:56 - INFO - __main__ - Running ... prefix=ethos-race_128_21, lr=0.0001, bsz=8 ...
05/23/2022 03:41:57 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 03:41:57 - INFO - __main__ - Printing 3 examples
05/23/2022 03:41:57 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 03:41:57 - INFO - __main__ - ['false']
05/23/2022 03:41:57 - INFO - __main__ -  [ethos-race] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
05/23/2022 03:41:57 - INFO - __main__ - ['false']
05/23/2022 03:41:57 - INFO - __main__ -  [ethos-race] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
05/23/2022 03:41:57 - INFO - __main__ - ['false']
05/23/2022 03:41:57 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:41:57 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:41:58 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 03:41:58 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 03:41:58 - INFO - __main__ - Printing 3 examples
05/23/2022 03:41:58 - INFO - __main__ -  [ethos-race] FUCK GAYS disgusting
05/23/2022 03:41:58 - INFO - __main__ - ['false']
05/23/2022 03:41:58 - INFO - __main__ -  [ethos-race] Disability centres all around the world. That’s were our money goes. Let’s take em down
05/23/2022 03:41:58 - INFO - __main__ - ['false']
05/23/2022 03:41:58 - INFO - __main__ -  [ethos-race] With no legs you are just a bag of meat in your household. Do them the favour and get dead already
05/23/2022 03:41:58 - INFO - __main__ - ['false']
05/23/2022 03:41:58 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:41:58 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:41:58 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 03:41:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 03:41:59 - INFO - __main__ - Starting training!
05/23/2022 03:42:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 03:42:08 - INFO - __main__ - Starting training!
05/23/2022 03:42:13 - INFO - __main__ - Step 10 Global step 10 Train loss 23.724041 on epoch=0
05/23/2022 03:42:17 - INFO - __main__ - Step 20 Global step 20 Train loss 20.833015 on epoch=1
05/23/2022 03:42:23 - INFO - __main__ - Step 30 Global step 30 Train loss 18.842693 on epoch=2
05/23/2022 03:42:28 - INFO - __main__ - Step 40 Global step 40 Train loss 18.296886 on epoch=3
05/23/2022 03:42:33 - INFO - __main__ - Step 50 Global step 50 Train loss 16.776995 on epoch=4
05/23/2022 03:43:09 - INFO - __main__ - Global step 50 Train loss 19.694727 Classification-F1 0.0 on epoch=4
05/23/2022 03:43:14 - INFO - __main__ - Step 60 Global step 60 Train loss 16.320419 on epoch=4
05/23/2022 03:43:19 - INFO - __main__ - Step 70 Global step 70 Train loss 16.606640 on epoch=5
05/23/2022 03:43:24 - INFO - __main__ - Step 80 Global step 80 Train loss 15.746964 on epoch=6
05/23/2022 03:43:29 - INFO - __main__ - Step 90 Global step 90 Train loss 15.246361 on epoch=7
05/23/2022 03:43:34 - INFO - __main__ - Step 100 Global step 100 Train loss 14.798788 on epoch=8
05/23/2022 03:44:10 - INFO - __main__ - Global step 100 Train loss 15.743834 Classification-F1 0.0 on epoch=8
05/23/2022 03:44:15 - INFO - __main__ - Step 110 Global step 110 Train loss 14.617755 on epoch=9
05/23/2022 03:44:20 - INFO - __main__ - Step 120 Global step 120 Train loss 14.013433 on epoch=9
05/23/2022 03:44:25 - INFO - __main__ - Step 130 Global step 130 Train loss 12.701332 on epoch=10
05/23/2022 03:44:30 - INFO - __main__ - Step 140 Global step 140 Train loss 13.028532 on epoch=11
05/23/2022 03:44:35 - INFO - __main__ - Step 150 Global step 150 Train loss 12.871869 on epoch=12
05/23/2022 03:45:10 - INFO - __main__ - Global step 150 Train loss 13.446585 Classification-F1 0.0 on epoch=12
05/23/2022 03:45:15 - INFO - __main__ - Step 160 Global step 160 Train loss 12.593769 on epoch=13
05/23/2022 03:45:20 - INFO - __main__ - Step 170 Global step 170 Train loss 11.547194 on epoch=14
05/23/2022 03:45:25 - INFO - __main__ - Step 180 Global step 180 Train loss 11.704561 on epoch=14
05/23/2022 03:45:30 - INFO - __main__ - Step 190 Global step 190 Train loss 11.831734 on epoch=15
05/23/2022 03:45:35 - INFO - __main__ - Step 200 Global step 200 Train loss 9.725641 on epoch=16
05/23/2022 03:46:09 - INFO - __main__ - Global step 200 Train loss 11.480579 Classification-F1 0.0 on epoch=16
05/23/2022 03:46:14 - INFO - __main__ - Step 210 Global step 210 Train loss 8.983239 on epoch=17
05/23/2022 03:46:19 - INFO - __main__ - Step 220 Global step 220 Train loss 6.183291 on epoch=18
05/23/2022 03:46:24 - INFO - __main__ - Step 230 Global step 230 Train loss 6.587007 on epoch=19
05/23/2022 03:46:29 - INFO - __main__ - Step 240 Global step 240 Train loss 4.562624 on epoch=19
05/23/2022 03:46:34 - INFO - __main__ - Step 250 Global step 250 Train loss 1.325053 on epoch=20
05/23/2022 03:46:35 - INFO - __main__ - Global step 250 Train loss 5.528244 Classification-F1 1.0 on epoch=20
05/23/2022 03:46:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.698898 on epoch=21
05/23/2022 03:46:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.640660 on epoch=22
05/23/2022 03:46:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.729577 on epoch=23
05/23/2022 03:46:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.443431 on epoch=24
05/23/2022 03:47:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.373879 on epoch=24
05/23/2022 03:47:02 - INFO - __main__ - Global step 300 Train loss 0.577289 Classification-F1 0.4336283185840708 on epoch=24
05/23/2022 03:47:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.569731 on epoch=25
05/23/2022 03:47:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.383890 on epoch=26
05/23/2022 03:47:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.451845 on epoch=27
05/23/2022 03:47:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.389045 on epoch=28
05/23/2022 03:47:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.441178 on epoch=29
05/23/2022 03:47:29 - INFO - __main__ - Global step 350 Train loss 0.447138 Classification-F1 0.49606299212598426 on epoch=29
05/23/2022 03:47:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.377376 on epoch=29
05/23/2022 03:47:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.328309 on epoch=30
05/23/2022 03:47:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.365318 on epoch=31
05/23/2022 03:47:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.331770 on epoch=32
05/23/2022 03:47:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.391983 on epoch=33
05/23/2022 03:47:55 - INFO - __main__ - Global step 400 Train loss 0.358951 Classification-F1 0.49407114624505927 on epoch=33
05/23/2022 03:48:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.369383 on epoch=34
05/23/2022 03:48:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.241720 on epoch=34
05/23/2022 03:48:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.280025 on epoch=35
05/23/2022 03:48:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.292796 on epoch=36
05/23/2022 03:48:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.229549 on epoch=37
05/23/2022 03:48:22 - INFO - __main__ - Global step 450 Train loss 0.282695 Classification-F1 0.4838709677419355 on epoch=37
05/23/2022 03:48:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.310381 on epoch=38
05/23/2022 03:48:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.232951 on epoch=39
05/23/2022 03:48:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.279589 on epoch=39
05/23/2022 03:48:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.261639 on epoch=40
05/23/2022 03:48:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.282574 on epoch=41
05/23/2022 03:48:49 - INFO - __main__ - Global step 500 Train loss 0.273427 Classification-F1 0.49606299212598426 on epoch=41
05/23/2022 03:48:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.239385 on epoch=42
05/23/2022 03:48:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.315085 on epoch=43
05/23/2022 03:49:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.226913 on epoch=44
05/23/2022 03:49:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.197391 on epoch=44
05/23/2022 03:49:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.180369 on epoch=45
05/23/2022 03:49:15 - INFO - __main__ - Global step 550 Train loss 0.231829 Classification-F1 0.4732510288065844 on epoch=45
05/23/2022 03:49:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.215591 on epoch=46
05/23/2022 03:49:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.153270 on epoch=47
05/23/2022 03:49:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.229679 on epoch=48
05/23/2022 03:49:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.219343 on epoch=49
05/23/2022 03:49:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.167442 on epoch=49
05/23/2022 03:49:42 - INFO - __main__ - Global step 600 Train loss 0.197065 Classification-F1 0.47107438016528924 on epoch=49
05/23/2022 03:49:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.188682 on epoch=50
05/23/2022 03:49:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.148638 on epoch=51
05/23/2022 03:49:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.143753 on epoch=52
05/23/2022 03:50:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.168938 on epoch=53
05/23/2022 03:50:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.188807 on epoch=54
05/23/2022 03:50:08 - INFO - __main__ - Global step 650 Train loss 0.167763 Classification-F1 0.4859437751004016 on epoch=54
05/23/2022 03:50:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.146967 on epoch=54
05/23/2022 03:50:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.114533 on epoch=55
05/23/2022 03:50:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.106624 on epoch=56
05/23/2022 03:50:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.158308 on epoch=57
05/23/2022 03:50:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.136595 on epoch=58
05/23/2022 03:50:35 - INFO - __main__ - Global step 700 Train loss 0.132605 Classification-F1 0.4859437751004016 on epoch=58
05/23/2022 03:50:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.154198 on epoch=59
05/23/2022 03:50:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.118968 on epoch=59
05/23/2022 03:50:50 - INFO - __main__ - Step 730 Global step 730 Train loss 0.128739 on epoch=60
05/23/2022 03:50:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.107439 on epoch=61
05/23/2022 03:51:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.112046 on epoch=62
05/23/2022 03:51:02 - INFO - __main__ - Global step 750 Train loss 0.124278 Classification-F1 0.4817813765182186 on epoch=62
05/23/2022 03:51:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.128066 on epoch=63
05/23/2022 03:51:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.126522 on epoch=64
05/23/2022 03:51:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.052270 on epoch=64
05/23/2022 03:51:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.164366 on epoch=65
05/23/2022 03:51:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.137081 on epoch=66
05/23/2022 03:51:28 - INFO - __main__ - Global step 800 Train loss 0.121661 Classification-F1 0.488 on epoch=66
05/23/2022 03:51:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.051541 on epoch=67
05/23/2022 03:51:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.136822 on epoch=68
05/23/2022 03:51:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.121704 on epoch=69
05/23/2022 03:51:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.085299 on epoch=69
05/23/2022 03:51:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.112076 on epoch=70
05/23/2022 03:51:55 - INFO - __main__ - Global step 850 Train loss 0.101488 Classification-F1 0.4859437751004016 on epoch=70
05/23/2022 03:52:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.079554 on epoch=71
05/23/2022 03:52:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.073627 on epoch=72
05/23/2022 03:52:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.093963 on epoch=73
05/23/2022 03:52:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.104101 on epoch=74
05/23/2022 03:52:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.081386 on epoch=74
05/23/2022 03:52:21 - INFO - __main__ - Global step 900 Train loss 0.086526 Classification-F1 0.4796747967479675 on epoch=74
05/23/2022 03:52:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.105803 on epoch=75
05/23/2022 03:52:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.052186 on epoch=76
05/23/2022 03:52:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.062827 on epoch=77
05/23/2022 03:52:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.068738 on epoch=78
05/23/2022 03:52:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.091315 on epoch=79
05/23/2022 03:52:48 - INFO - __main__ - Global step 950 Train loss 0.076174 Classification-F1 0.4838709677419355 on epoch=79
05/23/2022 03:52:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.103272 on epoch=79
05/23/2022 03:52:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.139847 on epoch=80
05/23/2022 03:53:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.082120 on epoch=81
05/23/2022 03:53:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.072448 on epoch=82
05/23/2022 03:53:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.064035 on epoch=83
05/23/2022 03:53:15 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 03:53:15 - INFO - __main__ - Printing 3 examples
05/23/2022 03:53:15 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
05/23/2022 03:53:15 - INFO - __main__ - ['false']
05/23/2022 03:53:15 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
05/23/2022 03:53:15 - INFO - __main__ - ['false']
05/23/2022 03:53:15 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
05/23/2022 03:53:15 - INFO - __main__ - ['false']
05/23/2022 03:53:15 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:53:15 - INFO - __main__ - Global step 1000 Train loss 0.092344 Classification-F1 0.488 on epoch=83
05/23/2022 03:53:15 - INFO - __main__ - save last model!
05/23/2022 03:53:15 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:53:15 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 03:53:15 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 03:53:15 - INFO - __main__ - Printing 3 examples
05/23/2022 03:53:15 - INFO - __main__ -  [ethos-race] They are laughing and their countries are poor evil murderes and all corrupted good job.
05/23/2022 03:53:15 - INFO - __main__ - ['false']
05/23/2022 03:53:15 - INFO - __main__ -  [ethos-race] Dalit? Is this a name of religion or an atom of poo composition
05/23/2022 03:53:15 - INFO - __main__ - ['false']
05/23/2022 03:53:15 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 03:53:15 - INFO - __main__ - ['false']
05/23/2022 03:53:15 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:53:15 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:53:15 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 03:53:22 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 03:53:22 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 03:53:22 - INFO - __main__ - Printing 3 examples
05/23/2022 03:53:22 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 03:53:22 - INFO - __main__ - ['true']
05/23/2022 03:53:22 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 03:53:22 - INFO - __main__ - ['false']
05/23/2022 03:53:22 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 03:53:22 - INFO - __main__ - ['false']
05/23/2022 03:53:22 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:53:22 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:53:22 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 03:53:24 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_21_0.0001_8_predictions.txt
05/23/2022 03:53:24 - INFO - __main__ - Classification-F1 on test data: 0.2972
05/23/2022 03:53:24 - INFO - __main__ - prefix=ethos-race_128_21, lr=0.0001, bsz=8, dev_performance=1.0, test_performance=0.29723991507430997
05/23/2022 03:53:24 - INFO - __main__ - Running ... prefix=ethos-race_128_42, lr=0.0005, bsz=8 ...
05/23/2022 03:53:25 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 03:53:25 - INFO - __main__ - Printing 3 examples
05/23/2022 03:53:25 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
05/23/2022 03:53:25 - INFO - __main__ - ['false']
05/23/2022 03:53:25 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
05/23/2022 03:53:25 - INFO - __main__ - ['false']
05/23/2022 03:53:25 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
05/23/2022 03:53:25 - INFO - __main__ - ['false']
05/23/2022 03:53:25 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:53:25 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:53:26 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 03:53:26 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 03:53:26 - INFO - __main__ - Printing 3 examples
05/23/2022 03:53:26 - INFO - __main__ -  [ethos-race] They are laughing and their countries are poor evil murderes and all corrupted good job.
05/23/2022 03:53:26 - INFO - __main__ - ['false']
05/23/2022 03:53:26 - INFO - __main__ -  [ethos-race] Dalit? Is this a name of religion or an atom of poo composition
05/23/2022 03:53:26 - INFO - __main__ - ['false']
05/23/2022 03:53:26 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 03:53:26 - INFO - __main__ - ['false']
05/23/2022 03:53:26 - INFO - __main__ - Tokenizing Input ...
05/23/2022 03:53:26 - INFO - __main__ - Tokenizing Output ...
05/23/2022 03:53:26 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 03:53:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 03:53:27 - INFO - __main__ - Starting training!
05/23/2022 03:53:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 03:53:36 - INFO - __main__ - Starting training!
05/23/2022 03:53:41 - INFO - __main__ - Step 10 Global step 10 Train loss 22.745636 on epoch=0
05/23/2022 03:53:46 - INFO - __main__ - Step 20 Global step 20 Train loss 17.550272 on epoch=1
05/23/2022 03:53:51 - INFO - __main__ - Step 30 Global step 30 Train loss 15.153387 on epoch=2
05/23/2022 03:53:56 - INFO - __main__ - Step 40 Global step 40 Train loss 12.506468 on epoch=3
05/23/2022 03:54:01 - INFO - __main__ - Step 50 Global step 50 Train loss 10.979432 on epoch=4
05/23/2022 03:54:02 - INFO - __main__ - Global step 50 Train loss 15.787039 Classification-F1 0.010256410256410256 on epoch=4
05/23/2022 03:54:08 - INFO - __main__ - Step 60 Global step 60 Train loss 6.440126 on epoch=4
05/23/2022 03:54:13 - INFO - __main__ - Step 70 Global step 70 Train loss 1.075212 on epoch=5
05/23/2022 03:54:18 - INFO - __main__ - Step 80 Global step 80 Train loss 1.044728 on epoch=6
05/23/2022 03:54:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.475208 on epoch=7
05/23/2022 03:54:28 - INFO - __main__ - Step 100 Global step 100 Train loss 0.369266 on epoch=8
05/23/2022 03:54:29 - INFO - __main__ - Global step 100 Train loss 1.880908 Classification-F1 0.46443514644351463 on epoch=8
05/23/2022 03:54:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.532330 on epoch=9
05/23/2022 03:54:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.318128 on epoch=9
05/23/2022 03:54:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.214783 on epoch=10
05/23/2022 03:54:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.257955 on epoch=11
05/23/2022 03:54:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.182160 on epoch=12
05/23/2022 03:54:57 - INFO - __main__ - Global step 150 Train loss 0.301071 Classification-F1 0.49206349206349204 on epoch=12
05/23/2022 03:55:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.216208 on epoch=13
05/23/2022 03:55:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.216652 on epoch=14
05/23/2022 03:55:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.108427 on epoch=14
05/23/2022 03:55:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.199717 on epoch=15
05/23/2022 03:55:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.117292 on epoch=16
05/23/2022 03:55:25 - INFO - __main__ - Global step 200 Train loss 0.171659 Classification-F1 0.488 on epoch=16
05/23/2022 03:55:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.153849 on epoch=17
05/23/2022 03:55:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.204156 on epoch=18
05/23/2022 03:55:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.199713 on epoch=19
05/23/2022 03:55:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.057226 on epoch=19
05/23/2022 03:55:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.095084 on epoch=20
05/23/2022 03:55:53 - INFO - __main__ - Global step 250 Train loss 0.142006 Classification-F1 0.46443514644351463 on epoch=20
05/23/2022 03:55:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.078819 on epoch=21
05/23/2022 03:56:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.062722 on epoch=22
05/23/2022 03:56:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.043343 on epoch=23
05/23/2022 03:56:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.017481 on epoch=24
05/23/2022 03:56:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.032327 on epoch=24
05/23/2022 03:56:20 - INFO - __main__ - Global step 300 Train loss 0.046938 Classification-F1 0.47540983606557374 on epoch=24
05/23/2022 03:56:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.022621 on epoch=25
05/23/2022 03:56:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.061987 on epoch=26
05/23/2022 03:56:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.034157 on epoch=27
05/23/2022 03:56:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.125661 on epoch=28
05/23/2022 03:56:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.009996 on epoch=29
05/23/2022 03:56:48 - INFO - __main__ - Global step 350 Train loss 0.050884 Classification-F1 0.4859437751004016 on epoch=29
05/23/2022 03:56:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.105321 on epoch=29
05/23/2022 03:56:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.032054 on epoch=30
05/23/2022 03:57:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.023766 on epoch=31
05/23/2022 03:57:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.013938 on epoch=32
05/23/2022 03:57:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.014000 on epoch=33
05/23/2022 03:57:15 - INFO - __main__ - Global step 400 Train loss 0.037816 Classification-F1 0.4732510288065844 on epoch=33
05/23/2022 03:57:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.007297 on epoch=34
05/23/2022 03:57:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.011250 on epoch=34
05/23/2022 03:57:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.004098 on epoch=35
05/23/2022 03:57:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.012827 on epoch=36
05/23/2022 03:57:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.005612 on epoch=37
05/23/2022 03:57:43 - INFO - __main__ - Global step 450 Train loss 0.008217 Classification-F1 0.4796747967479675 on epoch=37
05/23/2022 03:57:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001330 on epoch=38
05/23/2022 03:57:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.002527 on epoch=39
05/23/2022 03:57:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001402 on epoch=39
05/23/2022 03:58:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.016051 on epoch=40
05/23/2022 03:58:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.038272 on epoch=41
05/23/2022 03:58:10 - INFO - __main__ - Global step 500 Train loss 0.011916 Classification-F1 0.4775510204081633 on epoch=41
05/23/2022 03:58:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.025196 on epoch=42
05/23/2022 03:58:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.049509 on epoch=43
05/23/2022 03:58:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.009919 on epoch=44
05/23/2022 03:58:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.013723 on epoch=44
05/23/2022 03:58:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.009504 on epoch=45
05/23/2022 03:58:38 - INFO - __main__ - Global step 550 Train loss 0.021570 Classification-F1 0.4859437751004016 on epoch=45
05/23/2022 03:58:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.015437 on epoch=46
05/23/2022 03:58:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.007278 on epoch=47
05/23/2022 03:58:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000092 on epoch=48
05/23/2022 03:58:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000350 on epoch=49
05/23/2022 03:59:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.009085 on epoch=49
05/23/2022 03:59:05 - INFO - __main__ - Global step 600 Train loss 0.006448 Classification-F1 0.4796747967479675 on epoch=49
05/23/2022 03:59:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000611 on epoch=50
05/23/2022 03:59:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000225 on epoch=51
05/23/2022 03:59:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000455 on epoch=52
05/23/2022 03:59:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.013671 on epoch=53
05/23/2022 03:59:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.006284 on epoch=54
05/23/2022 03:59:33 - INFO - __main__ - Global step 650 Train loss 0.004249 Classification-F1 0.4838709677419355 on epoch=54
05/23/2022 03:59:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000215 on epoch=54
05/23/2022 03:59:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000209 on epoch=55
05/23/2022 03:59:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000448 on epoch=56
05/23/2022 03:59:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000337 on epoch=57
05/23/2022 03:59:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000245 on epoch=58
05/23/2022 04:00:00 - INFO - __main__ - Global step 700 Train loss 0.000291 Classification-F1 0.4817813765182186 on epoch=58
05/23/2022 04:00:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000088 on epoch=59
05/23/2022 04:00:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000035 on epoch=59
05/23/2022 04:00:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000417 on epoch=60
05/23/2022 04:00:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.036293 on epoch=61
05/23/2022 04:00:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000249 on epoch=62
05/23/2022 04:00:28 - INFO - __main__ - Global step 750 Train loss 0.007416 Classification-F1 0.4817813765182186 on epoch=62
05/23/2022 04:00:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000999 on epoch=63
05/23/2022 04:00:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000147 on epoch=64
05/23/2022 04:00:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000629 on epoch=64
05/23/2022 04:00:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000080 on epoch=65
05/23/2022 04:00:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000124 on epoch=66
05/23/2022 04:00:55 - INFO - __main__ - Global step 800 Train loss 0.000396 Classification-F1 0.4817813765182186 on epoch=66
05/23/2022 04:01:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000580 on epoch=67
05/23/2022 04:01:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000294 on epoch=68
05/23/2022 04:01:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.004045 on epoch=69
05/23/2022 04:01:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.003860 on epoch=69
05/23/2022 04:01:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000036 on epoch=70
05/23/2022 04:01:23 - INFO - __main__ - Global step 850 Train loss 0.001763 Classification-F1 0.4838709677419355 on epoch=70
05/23/2022 04:01:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000014 on epoch=71
05/23/2022 04:01:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000110 on epoch=72
05/23/2022 04:01:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000291 on epoch=73
05/23/2022 04:01:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000068 on epoch=74
05/23/2022 04:01:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000022 on epoch=74
05/23/2022 04:01:50 - INFO - __main__ - Global step 900 Train loss 0.000101 Classification-F1 0.4838709677419355 on epoch=74
05/23/2022 04:01:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000516 on epoch=75
05/23/2022 04:02:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000114 on epoch=76
05/23/2022 04:02:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000260 on epoch=77
05/23/2022 04:02:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000032 on epoch=78
05/23/2022 04:02:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000288 on epoch=79
05/23/2022 04:02:18 - INFO - __main__ - Global step 950 Train loss 0.000242 Classification-F1 0.4859437751004016 on epoch=79
05/23/2022 04:02:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000012 on epoch=79
05/23/2022 04:02:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000033 on epoch=80
05/23/2022 04:02:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000221 on epoch=81
05/23/2022 04:02:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000013 on epoch=82
05/23/2022 04:02:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000736 on epoch=83
05/23/2022 04:02:45 - INFO - __main__ - Global step 1000 Train loss 0.000203 Classification-F1 0.4775510204081633 on epoch=83
05/23/2022 04:02:45 - INFO - __main__ - save last model!
05/23/2022 04:02:45 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 04:02:45 - INFO - __main__ - Printing 3 examples
05/23/2022 04:02:45 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
05/23/2022 04:02:45 - INFO - __main__ - ['false']
05/23/2022 04:02:45 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
05/23/2022 04:02:45 - INFO - __main__ - ['false']
05/23/2022 04:02:45 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
05/23/2022 04:02:45 - INFO - __main__ - ['false']
05/23/2022 04:02:45 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:02:45 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:02:45 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 04:02:45 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 04:02:45 - INFO - __main__ - Printing 3 examples
05/23/2022 04:02:45 - INFO - __main__ -  [ethos-race] They are laughing and their countries are poor evil murderes and all corrupted good job.
05/23/2022 04:02:45 - INFO - __main__ - ['false']
05/23/2022 04:02:45 - INFO - __main__ -  [ethos-race] Dalit? Is this a name of religion or an atom of poo composition
05/23/2022 04:02:45 - INFO - __main__ - ['false']
05/23/2022 04:02:45 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 04:02:45 - INFO - __main__ - ['false']
05/23/2022 04:02:45 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:02:45 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:02:45 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 04:02:52 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 04:02:53 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 04:02:53 - INFO - __main__ - Printing 3 examples
05/23/2022 04:02:53 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 04:02:53 - INFO - __main__ - ['true']
05/23/2022 04:02:53 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 04:02:53 - INFO - __main__ - ['false']
05/23/2022 04:02:53 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 04:02:53 - INFO - __main__ - ['false']
05/23/2022 04:02:53 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:02:53 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:02:53 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 04:02:55 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_42_0.0005_8_predictions.txt
05/23/2022 04:02:55 - INFO - __main__ - Classification-F1 on test data: 0.9065
05/23/2022 04:02:55 - INFO - __main__ - prefix=ethos-race_128_42, lr=0.0005, bsz=8, dev_performance=0.49206349206349204, test_performance=0.9065119277885235
05/23/2022 04:02:55 - INFO - __main__ - Running ... prefix=ethos-race_128_42, lr=0.0003, bsz=8 ...
05/23/2022 04:02:56 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 04:02:56 - INFO - __main__ - Printing 3 examples
05/23/2022 04:02:56 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
05/23/2022 04:02:56 - INFO - __main__ - ['false']
05/23/2022 04:02:56 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
05/23/2022 04:02:56 - INFO - __main__ - ['false']
05/23/2022 04:02:56 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
05/23/2022 04:02:56 - INFO - __main__ - ['false']
05/23/2022 04:02:56 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:02:56 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:02:56 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 04:02:56 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 04:02:56 - INFO - __main__ - Printing 3 examples
05/23/2022 04:02:56 - INFO - __main__ -  [ethos-race] They are laughing and their countries are poor evil murderes and all corrupted good job.
05/23/2022 04:02:56 - INFO - __main__ - ['false']
05/23/2022 04:02:56 - INFO - __main__ -  [ethos-race] Dalit? Is this a name of religion or an atom of poo composition
05/23/2022 04:02:56 - INFO - __main__ - ['false']
05/23/2022 04:02:56 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 04:02:56 - INFO - __main__ - ['false']
05/23/2022 04:02:56 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:02:56 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:02:56 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 04:02:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 04:02:58 - INFO - __main__ - Starting training!
05/23/2022 04:03:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 04:03:09 - INFO - __main__ - Starting training!
05/23/2022 04:03:14 - INFO - __main__ - Step 10 Global step 10 Train loss 22.790607 on epoch=0
05/23/2022 04:03:19 - INFO - __main__ - Step 20 Global step 20 Train loss 19.191246 on epoch=1
05/23/2022 04:03:24 - INFO - __main__ - Step 30 Global step 30 Train loss 16.760624 on epoch=2
05/23/2022 04:03:29 - INFO - __main__ - Step 40 Global step 40 Train loss 15.196530 on epoch=3
05/23/2022 04:03:34 - INFO - __main__ - Step 50 Global step 50 Train loss 15.018260 on epoch=4
05/23/2022 04:04:03 - INFO - __main__ - Global step 50 Train loss 17.791454 Classification-F1 0.0 on epoch=4
05/23/2022 04:04:09 - INFO - __main__ - Step 60 Global step 60 Train loss 14.166005 on epoch=4
05/23/2022 04:04:14 - INFO - __main__ - Step 70 Global step 70 Train loss 12.359871 on epoch=5
05/23/2022 04:04:19 - INFO - __main__ - Step 80 Global step 80 Train loss 10.448657 on epoch=6
05/23/2022 04:04:24 - INFO - __main__ - Step 90 Global step 90 Train loss 7.663407 on epoch=7
05/23/2022 04:04:29 - INFO - __main__ - Step 100 Global step 100 Train loss 2.838187 on epoch=8
05/23/2022 04:05:09 - INFO - __main__ - Global step 100 Train loss 9.495225 Classification-F1 0.1414141414141414 on epoch=8
05/23/2022 04:05:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.748307 on epoch=9
05/23/2022 04:05:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.760350 on epoch=9
05/23/2022 04:05:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.453516 on epoch=10
05/23/2022 04:05:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.542901 on epoch=11
05/23/2022 04:05:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.470729 on epoch=12
05/23/2022 04:05:36 - INFO - __main__ - Global step 150 Train loss 0.595160 Classification-F1 1.0 on epoch=12
05/23/2022 04:05:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.390684 on epoch=13
05/23/2022 04:05:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.535667 on epoch=14
05/23/2022 04:05:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.374054 on epoch=14
05/23/2022 04:05:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.374975 on epoch=15
05/23/2022 04:06:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.366780 on epoch=16
05/23/2022 04:06:04 - INFO - __main__ - Global step 200 Train loss 0.408432 Classification-F1 1.0 on epoch=16
05/23/2022 04:06:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.402912 on epoch=17
05/23/2022 04:06:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.397773 on epoch=18
05/23/2022 04:06:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.496823 on epoch=19
05/23/2022 04:06:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.362110 on epoch=19
05/23/2022 04:06:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.341320 on epoch=20
05/23/2022 04:06:31 - INFO - __main__ - Global step 250 Train loss 0.400188 Classification-F1 0.39906103286384975 on epoch=20
05/23/2022 04:06:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.353139 on epoch=21
05/23/2022 04:06:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.368354 on epoch=22
05/23/2022 04:06:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.383337 on epoch=23
05/23/2022 04:06:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.416801 on epoch=24
05/23/2022 04:06:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.335408 on epoch=24
05/23/2022 04:06:58 - INFO - __main__ - Global step 300 Train loss 0.371408 Classification-F1 0.4046511627906977 on epoch=24
05/23/2022 04:07:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.378214 on epoch=25
05/23/2022 04:07:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.365800 on epoch=26
05/23/2022 04:07:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.343018 on epoch=27
05/23/2022 04:07:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.347695 on epoch=28
05/23/2022 04:07:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.375811 on epoch=29
05/23/2022 04:07:25 - INFO - __main__ - Global step 350 Train loss 0.362108 Classification-F1 1.0 on epoch=29
05/23/2022 04:07:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.359354 on epoch=29
05/23/2022 04:07:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.314702 on epoch=30
05/23/2022 04:07:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.329428 on epoch=31
05/23/2022 04:07:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.308186 on epoch=32
05/23/2022 04:07:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.264416 on epoch=33
05/23/2022 04:07:52 - INFO - __main__ - Global step 400 Train loss 0.315217 Classification-F1 0.4980392156862745 on epoch=33
05/23/2022 04:07:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.302525 on epoch=34
05/23/2022 04:08:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.295426 on epoch=34
05/23/2022 04:08:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.326172 on epoch=35
05/23/2022 04:08:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.284548 on epoch=36
05/23/2022 04:08:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.313166 on epoch=37
05/23/2022 04:08:20 - INFO - __main__ - Global step 450 Train loss 0.304367 Classification-F1 0.49606299212598426 on epoch=37
05/23/2022 04:08:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.256661 on epoch=38
05/23/2022 04:08:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.302208 on epoch=39
05/23/2022 04:08:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.306211 on epoch=39
05/23/2022 04:08:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.249917 on epoch=40
05/23/2022 04:08:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.251781 on epoch=41
05/23/2022 04:08:46 - INFO - __main__ - Global step 500 Train loss 0.273356 Classification-F1 0.4980392156862745 on epoch=41
05/23/2022 04:08:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.276512 on epoch=42
05/23/2022 04:08:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.223958 on epoch=43
05/23/2022 04:09:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.281841 on epoch=44
05/23/2022 04:09:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.223535 on epoch=44
05/23/2022 04:09:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.191658 on epoch=45
05/23/2022 04:09:13 - INFO - __main__ - Global step 550 Train loss 0.239501 Classification-F1 0.49407114624505927 on epoch=45
05/23/2022 04:09:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.175982 on epoch=46
05/23/2022 04:09:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.179547 on epoch=47
05/23/2022 04:09:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.228533 on epoch=48
05/23/2022 04:09:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.230371 on epoch=49
05/23/2022 04:09:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.186301 on epoch=49
05/23/2022 04:09:40 - INFO - __main__ - Global step 600 Train loss 0.200147 Classification-F1 0.488 on epoch=49
05/23/2022 04:09:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.147405 on epoch=50
05/23/2022 04:09:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.207903 on epoch=51
05/23/2022 04:09:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.229538 on epoch=52
05/23/2022 04:10:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.184208 on epoch=53
05/23/2022 04:10:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.144446 on epoch=54
05/23/2022 04:10:07 - INFO - __main__ - Global step 650 Train loss 0.182700 Classification-F1 0.49407114624505927 on epoch=54
05/23/2022 04:10:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.174792 on epoch=54
05/23/2022 04:10:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.131555 on epoch=55
05/23/2022 04:10:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.137613 on epoch=56
05/23/2022 04:10:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.160565 on epoch=57
05/23/2022 04:10:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.181494 on epoch=58
05/23/2022 04:10:34 - INFO - __main__ - Global step 700 Train loss 0.157204 Classification-F1 0.4900398406374502 on epoch=58
05/23/2022 04:10:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.162473 on epoch=59
05/23/2022 04:10:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.146364 on epoch=59
05/23/2022 04:10:50 - INFO - __main__ - Step 730 Global step 730 Train loss 0.150679 on epoch=60
05/23/2022 04:10:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.100857 on epoch=61
05/23/2022 04:11:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.142159 on epoch=62
05/23/2022 04:11:01 - INFO - __main__ - Global step 750 Train loss 0.140506 Classification-F1 0.4900398406374502 on epoch=62
05/23/2022 04:11:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.137355 on epoch=63
05/23/2022 04:11:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.184995 on epoch=64
05/23/2022 04:11:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.108033 on epoch=64
05/23/2022 04:11:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.149780 on epoch=65
05/23/2022 04:11:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.118606 on epoch=66
05/23/2022 04:11:28 - INFO - __main__ - Global step 800 Train loss 0.139754 Classification-F1 0.49206349206349204 on epoch=66
05/23/2022 04:11:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.190643 on epoch=67
05/23/2022 04:11:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.157137 on epoch=68
05/23/2022 04:11:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.121629 on epoch=69
05/23/2022 04:11:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.115473 on epoch=69
05/23/2022 04:11:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.129660 on epoch=70
05/23/2022 04:11:55 - INFO - __main__ - Global step 850 Train loss 0.142908 Classification-F1 0.4796747967479675 on epoch=70
05/23/2022 04:12:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.102345 on epoch=71
05/23/2022 04:12:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.070895 on epoch=72
05/23/2022 04:12:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.171358 on epoch=73
05/23/2022 04:12:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.089158 on epoch=74
05/23/2022 04:12:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.072873 on epoch=74
05/23/2022 04:12:22 - INFO - __main__ - Global step 900 Train loss 0.101326 Classification-F1 0.4859437751004016 on epoch=74
05/23/2022 04:12:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.103371 on epoch=75
05/23/2022 04:12:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.054554 on epoch=76
05/23/2022 04:12:38 - INFO - __main__ - Step 930 Global step 930 Train loss 0.068295 on epoch=77
05/23/2022 04:12:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.100088 on epoch=78
05/23/2022 04:12:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.110255 on epoch=79
05/23/2022 04:12:49 - INFO - __main__ - Global step 950 Train loss 0.087313 Classification-F1 0.49606299212598426 on epoch=79
05/23/2022 04:12:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.089366 on epoch=79
05/23/2022 04:13:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.066440 on epoch=80
05/23/2022 04:13:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.080206 on epoch=81
05/23/2022 04:13:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.094591 on epoch=82
05/23/2022 04:13:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.094191 on epoch=83
05/23/2022 04:13:16 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 04:13:16 - INFO - __main__ - Printing 3 examples
05/23/2022 04:13:16 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
05/23/2022 04:13:16 - INFO - __main__ - ['false']
05/23/2022 04:13:16 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
05/23/2022 04:13:16 - INFO - __main__ - ['false']
05/23/2022 04:13:16 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
05/23/2022 04:13:16 - INFO - __main__ - ['false']
05/23/2022 04:13:16 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:13:16 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:13:16 - INFO - __main__ - Global step 1000 Train loss 0.084959 Classification-F1 0.488 on epoch=83
05/23/2022 04:13:16 - INFO - __main__ - save last model!
05/23/2022 04:13:17 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 04:13:17 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 04:13:17 - INFO - __main__ - Printing 3 examples
05/23/2022 04:13:17 - INFO - __main__ -  [ethos-race] They are laughing and their countries are poor evil murderes and all corrupted good job.
05/23/2022 04:13:17 - INFO - __main__ - ['false']
05/23/2022 04:13:17 - INFO - __main__ -  [ethos-race] Dalit? Is this a name of religion or an atom of poo composition
05/23/2022 04:13:17 - INFO - __main__ - ['false']
05/23/2022 04:13:17 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 04:13:17 - INFO - __main__ - ['false']
05/23/2022 04:13:17 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:13:17 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:13:17 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 04:13:23 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 04:13:24 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 04:13:24 - INFO - __main__ - Printing 3 examples
05/23/2022 04:13:24 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 04:13:24 - INFO - __main__ - ['true']
05/23/2022 04:13:24 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 04:13:24 - INFO - __main__ - ['false']
05/23/2022 04:13:24 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 04:13:24 - INFO - __main__ - ['false']
05/23/2022 04:13:24 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:13:24 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:13:24 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 04:13:26 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_42_0.0003_8_predictions.txt
05/23/2022 04:13:26 - INFO - __main__ - Classification-F1 on test data: 0.4494
05/23/2022 04:13:26 - INFO - __main__ - prefix=ethos-race_128_42, lr=0.0003, bsz=8, dev_performance=1.0, test_performance=0.44936708860759494
05/23/2022 04:13:26 - INFO - __main__ - Running ... prefix=ethos-race_128_42, lr=0.0002, bsz=8 ...
05/23/2022 04:13:27 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 04:13:27 - INFO - __main__ - Printing 3 examples
05/23/2022 04:13:27 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
05/23/2022 04:13:27 - INFO - __main__ - ['false']
05/23/2022 04:13:27 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
05/23/2022 04:13:27 - INFO - __main__ - ['false']
05/23/2022 04:13:27 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
05/23/2022 04:13:27 - INFO - __main__ - ['false']
05/23/2022 04:13:27 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:13:27 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:13:28 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 04:13:28 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 04:13:28 - INFO - __main__ - Printing 3 examples
05/23/2022 04:13:28 - INFO - __main__ -  [ethos-race] They are laughing and their countries are poor evil murderes and all corrupted good job.
05/23/2022 04:13:28 - INFO - __main__ - ['false']
05/23/2022 04:13:28 - INFO - __main__ -  [ethos-race] Dalit? Is this a name of religion or an atom of poo composition
05/23/2022 04:13:28 - INFO - __main__ - ['false']
05/23/2022 04:13:28 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 04:13:28 - INFO - __main__ - ['false']
05/23/2022 04:13:28 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:13:28 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:13:28 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 04:13:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 04:13:28 - INFO - __main__ - Starting training!
05/23/2022 04:13:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 04:13:40 - INFO - __main__ - Starting training!
05/23/2022 04:13:45 - INFO - __main__ - Step 10 Global step 10 Train loss 23.125765 on epoch=0
05/23/2022 04:13:50 - INFO - __main__ - Step 20 Global step 20 Train loss 20.242828 on epoch=1
05/23/2022 04:13:55 - INFO - __main__ - Step 30 Global step 30 Train loss 17.518806 on epoch=2
05/23/2022 04:14:00 - INFO - __main__ - Step 40 Global step 40 Train loss 16.894133 on epoch=3
05/23/2022 04:14:06 - INFO - __main__ - Step 50 Global step 50 Train loss 15.799101 on epoch=4
05/23/2022 04:14:38 - INFO - __main__ - Global step 50 Train loss 18.716125 Classification-F1 0.0 on epoch=4
05/23/2022 04:14:43 - INFO - __main__ - Step 60 Global step 60 Train loss 14.735277 on epoch=4
05/23/2022 04:14:49 - INFO - __main__ - Step 70 Global step 70 Train loss 14.321692 on epoch=5
05/23/2022 04:14:54 - INFO - __main__ - Step 80 Global step 80 Train loss 13.786115 on epoch=6
05/23/2022 04:14:59 - INFO - __main__ - Step 90 Global step 90 Train loss 12.215558 on epoch=7
05/23/2022 04:15:04 - INFO - __main__ - Step 100 Global step 100 Train loss 11.807837 on epoch=8
05/23/2022 04:15:33 - INFO - __main__ - Global step 100 Train loss 13.373297 Classification-F1 0.0 on epoch=8
05/23/2022 04:15:38 - INFO - __main__ - Step 110 Global step 110 Train loss 9.953923 on epoch=9
05/23/2022 04:15:43 - INFO - __main__ - Step 120 Global step 120 Train loss 7.691627 on epoch=9
05/23/2022 04:15:48 - INFO - __main__ - Step 130 Global step 130 Train loss 4.897481 on epoch=10
05/23/2022 04:15:54 - INFO - __main__ - Step 140 Global step 140 Train loss 2.714375 on epoch=11
05/23/2022 04:15:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.968891 on epoch=12
05/23/2022 04:16:00 - INFO - __main__ - Global step 150 Train loss 5.245260 Classification-F1 1.0 on epoch=12
05/23/2022 04:16:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.744103 on epoch=13
05/23/2022 04:16:11 - INFO - __main__ - Step 170 Global step 170 Train loss 0.474086 on epoch=14
05/23/2022 04:16:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.343865 on epoch=14
05/23/2022 04:16:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.287586 on epoch=15
05/23/2022 04:16:27 - INFO - __main__ - Step 200 Global step 200 Train loss 0.301598 on epoch=16
05/23/2022 04:16:28 - INFO - __main__ - Global step 200 Train loss 0.430248 Classification-F1 0.4732510288065844 on epoch=16
05/23/2022 04:16:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.253096 on epoch=17
05/23/2022 04:16:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.213118 on epoch=18
05/23/2022 04:16:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.275812 on epoch=19
05/23/2022 04:16:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.192389 on epoch=19
05/23/2022 04:16:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.171652 on epoch=20
05/23/2022 04:16:55 - INFO - __main__ - Global step 250 Train loss 0.221214 Classification-F1 0.4576271186440678 on epoch=20
05/23/2022 04:17:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.255841 on epoch=21
05/23/2022 04:17:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.173482 on epoch=22
05/23/2022 04:17:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.164162 on epoch=23
05/23/2022 04:17:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.187341 on epoch=24
05/23/2022 04:17:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.148373 on epoch=24
05/23/2022 04:17:22 - INFO - __main__ - Global step 300 Train loss 0.185840 Classification-F1 0.4434782608695652 on epoch=24
05/23/2022 04:17:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.128837 on epoch=25
05/23/2022 04:17:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.118696 on epoch=26
05/23/2022 04:17:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.143752 on epoch=27
05/23/2022 04:17:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.124331 on epoch=28
05/23/2022 04:17:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.149251 on epoch=29
05/23/2022 04:17:49 - INFO - __main__ - Global step 350 Train loss 0.132973 Classification-F1 0.49407114624505927 on epoch=29
05/23/2022 04:17:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.124204 on epoch=29
05/23/2022 04:17:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.115373 on epoch=30
05/23/2022 04:18:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.070713 on epoch=31
05/23/2022 04:18:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.094915 on epoch=32
05/23/2022 04:18:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.091422 on epoch=33
05/23/2022 04:18:16 - INFO - __main__ - Global step 400 Train loss 0.099325 Classification-F1 0.4796747967479675 on epoch=33
05/23/2022 04:18:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.054024 on epoch=34
05/23/2022 04:18:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.073568 on epoch=34
05/23/2022 04:18:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.096464 on epoch=35
05/23/2022 04:18:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.058666 on epoch=36
05/23/2022 04:18:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.048491 on epoch=37
05/23/2022 04:18:43 - INFO - __main__ - Global step 450 Train loss 0.066242 Classification-F1 0.45064377682403434 on epoch=37
05/23/2022 04:18:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.075378 on epoch=38
05/23/2022 04:18:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.026057 on epoch=39
05/23/2022 04:18:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.073519 on epoch=39
05/23/2022 04:19:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.063964 on epoch=40
05/23/2022 04:19:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.087418 on epoch=41
05/23/2022 04:19:11 - INFO - __main__ - Global step 500 Train loss 0.065267 Classification-F1 0.4838709677419355 on epoch=41
05/23/2022 04:19:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.049020 on epoch=42
05/23/2022 04:19:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.070850 on epoch=43
05/23/2022 04:19:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.016079 on epoch=44
05/23/2022 04:19:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.049854 on epoch=44
05/23/2022 04:19:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.065258 on epoch=45
05/23/2022 04:19:38 - INFO - __main__ - Global step 550 Train loss 0.050212 Classification-F1 0.4666666666666667 on epoch=45
05/23/2022 04:19:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.064003 on epoch=46
05/23/2022 04:19:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.068904 on epoch=47
05/23/2022 04:19:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.060923 on epoch=48
05/23/2022 04:19:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.034073 on epoch=49
05/23/2022 04:20:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.029414 on epoch=49
05/23/2022 04:20:05 - INFO - __main__ - Global step 600 Train loss 0.051464 Classification-F1 0.4796747967479675 on epoch=49
05/23/2022 04:20:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.021579 on epoch=50
05/23/2022 04:20:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.021921 on epoch=51
05/23/2022 04:20:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.110459 on epoch=52
05/23/2022 04:20:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.045872 on epoch=53
05/23/2022 04:20:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.008801 on epoch=54
05/23/2022 04:20:32 - INFO - __main__ - Global step 650 Train loss 0.041727 Classification-F1 0.49407114624505927 on epoch=54
05/23/2022 04:20:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.081862 on epoch=54
05/23/2022 04:20:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.037057 on epoch=55
05/23/2022 04:20:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.154723 on epoch=56
05/23/2022 04:20:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.026572 on epoch=57
05/23/2022 04:20:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.054643 on epoch=58
05/23/2022 04:20:59 - INFO - __main__ - Global step 700 Train loss 0.070971 Classification-F1 0.4796747967479675 on epoch=58
05/23/2022 04:21:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.026739 on epoch=59
05/23/2022 04:21:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.075934 on epoch=59
05/23/2022 04:21:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.049242 on epoch=60
05/23/2022 04:21:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.035145 on epoch=61
05/23/2022 04:21:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.007165 on epoch=62
05/23/2022 04:21:27 - INFO - __main__ - Global step 750 Train loss 0.038845 Classification-F1 0.4900398406374502 on epoch=62
05/23/2022 04:21:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.084129 on epoch=63
05/23/2022 04:21:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.011618 on epoch=64
05/23/2022 04:21:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.014494 on epoch=64
05/23/2022 04:21:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.011286 on epoch=65
05/23/2022 04:21:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.011742 on epoch=66
05/23/2022 04:21:53 - INFO - __main__ - Global step 800 Train loss 0.026654 Classification-F1 0.47540983606557374 on epoch=66
05/23/2022 04:21:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.062808 on epoch=67
05/23/2022 04:22:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.032621 on epoch=68
05/23/2022 04:22:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.026459 on epoch=69
05/23/2022 04:22:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.012348 on epoch=69
05/23/2022 04:22:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.037552 on epoch=70
05/23/2022 04:22:21 - INFO - __main__ - Global step 850 Train loss 0.034358 Classification-F1 0.4775510204081633 on epoch=70
05/23/2022 04:22:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.042718 on epoch=71
05/23/2022 04:22:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.047649 on epoch=72
05/23/2022 04:22:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.032924 on epoch=73
05/23/2022 04:22:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.030138 on epoch=74
05/23/2022 04:22:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.066195 on epoch=74
05/23/2022 04:22:48 - INFO - __main__ - Global step 900 Train loss 0.043925 Classification-F1 0.4732510288065844 on epoch=74
05/23/2022 04:22:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.031381 on epoch=75
05/23/2022 04:22:58 - INFO - __main__ - Step 920 Global step 920 Train loss 0.002839 on epoch=76
05/23/2022 04:23:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.017128 on epoch=77
05/23/2022 04:23:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.042442 on epoch=78
05/23/2022 04:23:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.006183 on epoch=79
05/23/2022 04:23:15 - INFO - __main__ - Global step 950 Train loss 0.019994 Classification-F1 0.4838709677419355 on epoch=79
05/23/2022 04:23:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.033528 on epoch=79
05/23/2022 04:23:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.021991 on epoch=80
05/23/2022 04:23:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.018594 on epoch=81
05/23/2022 04:23:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.046932 on epoch=82
05/23/2022 04:23:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.049533 on epoch=83
05/23/2022 04:23:42 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 04:23:42 - INFO - __main__ - Printing 3 examples
05/23/2022 04:23:42 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
05/23/2022 04:23:42 - INFO - __main__ - ['false']
05/23/2022 04:23:42 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
05/23/2022 04:23:42 - INFO - __main__ - ['false']
05/23/2022 04:23:42 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
05/23/2022 04:23:42 - INFO - __main__ - ['false']
05/23/2022 04:23:42 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:23:42 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:23:42 - INFO - __main__ - Global step 1000 Train loss 0.034115 Classification-F1 0.4817813765182186 on epoch=83
05/23/2022 04:23:42 - INFO - __main__ - save last model!
05/23/2022 04:23:42 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 04:23:42 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 04:23:42 - INFO - __main__ - Printing 3 examples
05/23/2022 04:23:42 - INFO - __main__ -  [ethos-race] They are laughing and their countries are poor evil murderes and all corrupted good job.
05/23/2022 04:23:42 - INFO - __main__ - ['false']
05/23/2022 04:23:42 - INFO - __main__ -  [ethos-race] Dalit? Is this a name of religion or an atom of poo composition
05/23/2022 04:23:42 - INFO - __main__ - ['false']
05/23/2022 04:23:42 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 04:23:42 - INFO - __main__ - ['false']
05/23/2022 04:23:42 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:23:42 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:23:42 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 04:23:49 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 04:23:50 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 04:23:50 - INFO - __main__ - Printing 3 examples
05/23/2022 04:23:50 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 04:23:50 - INFO - __main__ - ['true']
05/23/2022 04:23:50 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 04:23:50 - INFO - __main__ - ['false']
05/23/2022 04:23:50 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 04:23:50 - INFO - __main__ - ['false']
05/23/2022 04:23:50 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:23:50 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:23:50 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 04:23:52 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_42_0.0002_8_predictions.txt
05/23/2022 04:23:52 - INFO - __main__ - Classification-F1 on test data: 0.4494
05/23/2022 04:23:52 - INFO - __main__ - prefix=ethos-race_128_42, lr=0.0002, bsz=8, dev_performance=1.0, test_performance=0.44936708860759494
05/23/2022 04:23:52 - INFO - __main__ - Running ... prefix=ethos-race_128_42, lr=0.0001, bsz=8 ...
05/23/2022 04:23:53 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 04:23:53 - INFO - __main__ - Printing 3 examples
05/23/2022 04:23:53 - INFO - __main__ -  [ethos-race] Women deserve to be abused, I guess.
05/23/2022 04:23:53 - INFO - __main__ - ['false']
05/23/2022 04:23:53 - INFO - __main__ -  [ethos-race] How can Europe allow millions of rapists animals to go there ?
05/23/2022 04:23:53 - INFO - __main__ - ['false']
05/23/2022 04:23:53 - INFO - __main__ -  [ethos-race] Most Americans are nazis or kkk members so they elected  Trump.
05/23/2022 04:23:53 - INFO - __main__ - ['false']
05/23/2022 04:23:53 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:23:53 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:23:53 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 04:23:53 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 04:23:53 - INFO - __main__ - Printing 3 examples
05/23/2022 04:23:53 - INFO - __main__ -  [ethos-race] They are laughing and their countries are poor evil murderes and all corrupted good job.
05/23/2022 04:23:53 - INFO - __main__ - ['false']
05/23/2022 04:23:53 - INFO - __main__ -  [ethos-race] Dalit? Is this a name of religion or an atom of poo composition
05/23/2022 04:23:53 - INFO - __main__ - ['false']
05/23/2022 04:23:53 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 04:23:53 - INFO - __main__ - ['false']
05/23/2022 04:23:53 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:23:53 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:23:53 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 04:23:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 04:23:55 - INFO - __main__ - Starting training!
05/23/2022 04:24:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 04:24:06 - INFO - __main__ - Starting training!
05/23/2022 04:24:10 - INFO - __main__ - Step 10 Global step 10 Train loss 23.358849 on epoch=0
05/23/2022 04:24:15 - INFO - __main__ - Step 20 Global step 20 Train loss 20.383106 on epoch=1
05/23/2022 04:24:21 - INFO - __main__ - Step 30 Global step 30 Train loss 17.686085 on epoch=2
05/23/2022 04:24:26 - INFO - __main__ - Step 40 Global step 40 Train loss 17.376963 on epoch=3
05/23/2022 04:24:31 - INFO - __main__ - Step 50 Global step 50 Train loss 16.669558 on epoch=4
05/23/2022 04:25:10 - INFO - __main__ - Global step 50 Train loss 19.094912 Classification-F1 0.0 on epoch=4
05/23/2022 04:25:16 - INFO - __main__ - Step 60 Global step 60 Train loss 16.637859 on epoch=4
05/23/2022 04:25:21 - INFO - __main__ - Step 70 Global step 70 Train loss 15.879137 on epoch=5
05/23/2022 04:25:27 - INFO - __main__ - Step 80 Global step 80 Train loss 15.602171 on epoch=6
05/23/2022 04:25:32 - INFO - __main__ - Step 90 Global step 90 Train loss 15.323819 on epoch=7
05/23/2022 04:25:37 - INFO - __main__ - Step 100 Global step 100 Train loss 15.368387 on epoch=8
05/23/2022 04:26:15 - INFO - __main__ - Global step 100 Train loss 15.762274 Classification-F1 0.0 on epoch=8
05/23/2022 04:26:20 - INFO - __main__ - Step 110 Global step 110 Train loss 14.665468 on epoch=9
05/23/2022 04:26:25 - INFO - __main__ - Step 120 Global step 120 Train loss 14.148028 on epoch=9
05/23/2022 04:26:30 - INFO - __main__ - Step 130 Global step 130 Train loss 13.726438 on epoch=10
05/23/2022 04:26:35 - INFO - __main__ - Step 140 Global step 140 Train loss 13.552236 on epoch=11
05/23/2022 04:26:41 - INFO - __main__ - Step 150 Global step 150 Train loss 14.059004 on epoch=12
05/23/2022 04:27:18 - INFO - __main__ - Global step 150 Train loss 14.030235 Classification-F1 0.0 on epoch=12
05/23/2022 04:27:23 - INFO - __main__ - Step 160 Global step 160 Train loss 13.358027 on epoch=13
05/23/2022 04:27:29 - INFO - __main__ - Step 170 Global step 170 Train loss 12.900681 on epoch=14
05/23/2022 04:27:34 - INFO - __main__ - Step 180 Global step 180 Train loss 12.609139 on epoch=14
05/23/2022 04:27:39 - INFO - __main__ - Step 190 Global step 190 Train loss 10.974663 on epoch=15
05/23/2022 04:27:44 - INFO - __main__ - Step 200 Global step 200 Train loss 10.884506 on epoch=16
05/23/2022 04:28:19 - INFO - __main__ - Global step 200 Train loss 12.145403 Classification-F1 0.0 on epoch=16
05/23/2022 04:28:25 - INFO - __main__ - Step 210 Global step 210 Train loss 10.279341 on epoch=17
05/23/2022 04:28:30 - INFO - __main__ - Step 220 Global step 220 Train loss 9.469111 on epoch=18
05/23/2022 04:28:35 - INFO - __main__ - Step 230 Global step 230 Train loss 7.700218 on epoch=19
05/23/2022 04:28:40 - INFO - __main__ - Step 240 Global step 240 Train loss 5.984633 on epoch=19
05/23/2022 04:28:45 - INFO - __main__ - Step 250 Global step 250 Train loss 3.599725 on epoch=20
05/23/2022 04:28:47 - INFO - __main__ - Global step 250 Train loss 7.406606 Classification-F1 0.4859437751004016 on epoch=20
05/23/2022 04:28:53 - INFO - __main__ - Step 260 Global step 260 Train loss 1.138070 on epoch=21
05/23/2022 04:28:58 - INFO - __main__ - Step 270 Global step 270 Train loss 1.007944 on epoch=22
05/23/2022 04:29:03 - INFO - __main__ - Step 280 Global step 280 Train loss 1.290619 on epoch=23
05/23/2022 04:29:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.738419 on epoch=24
05/23/2022 04:29:13 - INFO - __main__ - Step 300 Global step 300 Train loss 1.030838 on epoch=24
05/23/2022 04:29:15 - INFO - __main__ - Global step 300 Train loss 1.041178 Classification-F1 0.4838709677419355 on epoch=24
05/23/2022 04:29:20 - INFO - __main__ - Step 310 Global step 310 Train loss 1.565620 on epoch=25
05/23/2022 04:29:25 - INFO - __main__ - Step 320 Global step 320 Train loss 1.335803 on epoch=26
05/23/2022 04:29:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.547831 on epoch=27
05/23/2022 04:29:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.960067 on epoch=28
05/23/2022 04:29:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.600676 on epoch=29
05/23/2022 04:29:42 - INFO - __main__ - Global step 350 Train loss 1.002000 Classification-F1 0.488 on epoch=29
05/23/2022 04:29:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.752252 on epoch=29
05/23/2022 04:29:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.568566 on epoch=30
05/23/2022 04:29:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.823467 on epoch=31
05/23/2022 04:30:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.387993 on epoch=32
05/23/2022 04:30:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.964581 on epoch=33
05/23/2022 04:30:09 - INFO - __main__ - Global step 400 Train loss 0.699372 Classification-F1 0.4775510204081633 on epoch=33
05/23/2022 04:30:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.398546 on epoch=34
05/23/2022 04:30:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.430209 on epoch=34
05/23/2022 04:30:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.453099 on epoch=35
05/23/2022 04:30:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.352829 on epoch=36
05/23/2022 04:30:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.410168 on epoch=37
05/23/2022 04:30:37 - INFO - __main__ - Global step 450 Train loss 0.408970 Classification-F1 0.4980392156862745 on epoch=37
05/23/2022 04:30:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.418939 on epoch=38
05/23/2022 04:30:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.351930 on epoch=39
05/23/2022 04:30:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.349375 on epoch=39
05/23/2022 04:30:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.356021 on epoch=40
05/23/2022 04:31:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.297418 on epoch=41
05/23/2022 04:31:04 - INFO - __main__ - Global step 500 Train loss 0.354737 Classification-F1 1.0 on epoch=41
05/23/2022 04:31:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.439777 on epoch=42
05/23/2022 04:31:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.529861 on epoch=43
05/23/2022 04:31:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.337900 on epoch=44
05/23/2022 04:31:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.270136 on epoch=44
05/23/2022 04:31:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.375717 on epoch=45
05/23/2022 04:31:32 - INFO - __main__ - Global step 550 Train loss 0.390678 Classification-F1 0.4666666666666667 on epoch=45
05/23/2022 04:31:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.408197 on epoch=46
05/23/2022 04:31:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.562094 on epoch=47
05/23/2022 04:31:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.272652 on epoch=48
05/23/2022 04:31:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.462867 on epoch=49
05/23/2022 04:31:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.502304 on epoch=49
05/23/2022 04:31:59 - INFO - __main__ - Global step 600 Train loss 0.441623 Classification-F1 0.4980392156862745 on epoch=49
05/23/2022 04:32:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.326479 on epoch=50
05/23/2022 04:32:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.358305 on epoch=51
05/23/2022 04:32:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.365706 on epoch=52
05/23/2022 04:32:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.464961 on epoch=53
05/23/2022 04:32:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.344909 on epoch=54
05/23/2022 04:32:26 - INFO - __main__ - Global step 650 Train loss 0.372072 Classification-F1 1.0 on epoch=54
05/23/2022 04:32:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.505179 on epoch=54
05/23/2022 04:32:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.390068 on epoch=55
05/23/2022 04:32:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.376689 on epoch=56
05/23/2022 04:32:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.375071 on epoch=57
05/23/2022 04:32:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.480529 on epoch=58
05/23/2022 04:32:54 - INFO - __main__ - Global step 700 Train loss 0.425507 Classification-F1 0.4859437751004016 on epoch=58
05/23/2022 04:32:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.392030 on epoch=59
05/23/2022 04:33:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.319094 on epoch=59
05/23/2022 04:33:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.275005 on epoch=60
05/23/2022 04:33:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.336745 on epoch=61
05/23/2022 04:33:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.454713 on epoch=62
05/23/2022 04:33:21 - INFO - __main__ - Global step 750 Train loss 0.355517 Classification-F1 0.4980392156862745 on epoch=62
05/23/2022 04:33:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.302401 on epoch=63
05/23/2022 04:33:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.338747 on epoch=64
05/23/2022 04:33:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.290621 on epoch=64
05/23/2022 04:33:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.317986 on epoch=65
05/23/2022 04:33:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.340569 on epoch=66
05/23/2022 04:33:48 - INFO - __main__ - Global step 800 Train loss 0.318065 Classification-F1 0.4980392156862745 on epoch=66
05/23/2022 04:33:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.330961 on epoch=67
05/23/2022 04:33:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.315102 on epoch=68
05/23/2022 04:34:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.279400 on epoch=69
05/23/2022 04:34:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.345112 on epoch=69
05/23/2022 04:34:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.284362 on epoch=70
05/23/2022 04:34:16 - INFO - __main__ - Global step 850 Train loss 0.310987 Classification-F1 0.4817813765182186 on epoch=70
05/23/2022 04:34:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.325993 on epoch=71
05/23/2022 04:34:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.381757 on epoch=72
05/23/2022 04:34:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.361753 on epoch=73
05/23/2022 04:34:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.308780 on epoch=74
05/23/2022 04:34:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.308473 on epoch=74
05/23/2022 04:34:43 - INFO - __main__ - Global step 900 Train loss 0.337351 Classification-F1 0.4796747967479675 on epoch=74
05/23/2022 04:34:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.251935 on epoch=75
05/23/2022 04:34:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.303813 on epoch=76
05/23/2022 04:34:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.306953 on epoch=77
05/23/2022 04:35:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.445895 on epoch=78
05/23/2022 04:35:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.315630 on epoch=79
05/23/2022 04:35:10 - INFO - __main__ - Global step 950 Train loss 0.324845 Classification-F1 0.4980392156862745 on epoch=79
05/23/2022 04:35:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.310959 on epoch=79
05/23/2022 04:35:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.319125 on epoch=80
05/23/2022 04:35:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.287789 on epoch=81
05/23/2022 04:35:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.318062 on epoch=82
05/23/2022 04:35:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.342955 on epoch=83
05/23/2022 04:35:37 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 04:35:37 - INFO - __main__ - Printing 3 examples
05/23/2022 04:35:37 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
05/23/2022 04:35:37 - INFO - __main__ - ['false']
05/23/2022 04:35:37 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
05/23/2022 04:35:37 - INFO - __main__ - ['false']
05/23/2022 04:35:37 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
05/23/2022 04:35:37 - INFO - __main__ - ['false']
05/23/2022 04:35:37 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:35:37 - INFO - __main__ - Global step 1000 Train loss 0.315778 Classification-F1 0.4980392156862745 on epoch=83
05/23/2022 04:35:37 - INFO - __main__ - save last model!
05/23/2022 04:35:37 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:35:38 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 04:35:38 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 04:35:38 - INFO - __main__ - Printing 3 examples
05/23/2022 04:35:38 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 04:35:38 - INFO - __main__ - ['false']
05/23/2022 04:35:38 - INFO - __main__ -  [ethos-race] If you are from Germany I will call you Nazi or Hitler. Then ill protest to hang you
05/23/2022 04:35:38 - INFO - __main__ - ['false']
05/23/2022 04:35:38 - INFO - __main__ -  [ethos-race] stone throwing must be legal when against women
05/23/2022 04:35:38 - INFO - __main__ - ['false']
05/23/2022 04:35:38 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:35:38 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:35:38 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 04:35:45 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 04:35:45 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 04:35:45 - INFO - __main__ - Printing 3 examples
05/23/2022 04:35:45 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 04:35:45 - INFO - __main__ - ['true']
05/23/2022 04:35:45 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 04:35:45 - INFO - __main__ - ['false']
05/23/2022 04:35:45 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 04:35:45 - INFO - __main__ - ['false']
05/23/2022 04:35:45 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:35:45 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:35:45 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 04:35:47 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_42_0.0001_8_predictions.txt
05/23/2022 04:35:47 - INFO - __main__ - Classification-F1 on test data: 0.5111
05/23/2022 04:35:47 - INFO - __main__ - prefix=ethos-race_128_42, lr=0.0001, bsz=8, dev_performance=1.0, test_performance=0.5110528287748219
05/23/2022 04:35:47 - INFO - __main__ - Running ... prefix=ethos-race_128_87, lr=0.0005, bsz=8 ...
05/23/2022 04:35:48 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 04:35:48 - INFO - __main__ - Printing 3 examples
05/23/2022 04:35:48 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
05/23/2022 04:35:48 - INFO - __main__ - ['false']
05/23/2022 04:35:48 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
05/23/2022 04:35:48 - INFO - __main__ - ['false']
05/23/2022 04:35:48 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
05/23/2022 04:35:48 - INFO - __main__ - ['false']
05/23/2022 04:35:48 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:35:48 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:35:49 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 04:35:49 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 04:35:49 - INFO - __main__ - Printing 3 examples
05/23/2022 04:35:49 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 04:35:49 - INFO - __main__ - ['false']
05/23/2022 04:35:49 - INFO - __main__ -  [ethos-race] If you are from Germany I will call you Nazi or Hitler. Then ill protest to hang you
05/23/2022 04:35:49 - INFO - __main__ - ['false']
05/23/2022 04:35:49 - INFO - __main__ -  [ethos-race] stone throwing must be legal when against women
05/23/2022 04:35:49 - INFO - __main__ - ['false']
05/23/2022 04:35:49 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:35:49 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:35:49 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 04:35:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 04:35:50 - INFO - __main__ - Starting training!
05/23/2022 04:36:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 04:36:01 - INFO - __main__ - Starting training!
05/23/2022 04:36:06 - INFO - __main__ - Step 10 Global step 10 Train loss 22.729137 on epoch=0
05/23/2022 04:36:11 - INFO - __main__ - Step 20 Global step 20 Train loss 19.026020 on epoch=1
05/23/2022 04:36:16 - INFO - __main__ - Step 30 Global step 30 Train loss 15.558614 on epoch=2
05/23/2022 04:36:21 - INFO - __main__ - Step 40 Global step 40 Train loss 13.706430 on epoch=3
05/23/2022 04:36:26 - INFO - __main__ - Step 50 Global step 50 Train loss 11.570150 on epoch=4
05/23/2022 04:36:28 - INFO - __main__ - Global step 50 Train loss 16.518072 Classification-F1 0.0 on epoch=4
05/23/2022 04:36:33 - INFO - __main__ - Step 60 Global step 60 Train loss 9.298431 on epoch=4
05/23/2022 04:36:38 - INFO - __main__ - Step 70 Global step 70 Train loss 3.676568 on epoch=5
05/23/2022 04:36:44 - INFO - __main__ - Step 80 Global step 80 Train loss 2.214656 on epoch=6
05/23/2022 04:36:49 - INFO - __main__ - Step 90 Global step 90 Train loss 1.203693 on epoch=7
05/23/2022 04:36:54 - INFO - __main__ - Step 100 Global step 100 Train loss 1.093190 on epoch=8
05/23/2022 04:36:55 - INFO - __main__ - Global step 100 Train loss 3.497308 Classification-F1 0.4046511627906977 on epoch=8
05/23/2022 04:37:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.658504 on epoch=9
05/23/2022 04:37:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.820345 on epoch=9
05/23/2022 04:37:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.454928 on epoch=10
05/23/2022 04:37:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.480302 on epoch=11
05/23/2022 04:37:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.463113 on epoch=12
05/23/2022 04:37:23 - INFO - __main__ - Global step 150 Train loss 0.575438 Classification-F1 1.0 on epoch=12
05/23/2022 04:37:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.402458 on epoch=13
05/23/2022 04:37:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.469877 on epoch=14
05/23/2022 04:37:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.436381 on epoch=14
05/23/2022 04:37:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.314610 on epoch=15
05/23/2022 04:37:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.404159 on epoch=16
05/23/2022 04:37:51 - INFO - __main__ - Global step 200 Train loss 0.405497 Classification-F1 1.0 on epoch=16
05/23/2022 04:37:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.433620 on epoch=17
05/23/2022 04:38:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.368746 on epoch=18
05/23/2022 04:38:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.362200 on epoch=19
05/23/2022 04:38:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.410791 on epoch=19
05/23/2022 04:38:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.367938 on epoch=20
05/23/2022 04:38:18 - INFO - __main__ - Global step 250 Train loss 0.388659 Classification-F1 0.21951219512195122 on epoch=20
05/23/2022 04:38:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.362557 on epoch=21
05/23/2022 04:38:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.368050 on epoch=22
05/23/2022 04:38:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.411316 on epoch=23
05/23/2022 04:38:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.407865 on epoch=24
05/23/2022 04:38:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.375917 on epoch=24
05/23/2022 04:38:45 - INFO - __main__ - Global step 300 Train loss 0.385141 Classification-F1 0.4838709677419355 on epoch=24
05/23/2022 04:38:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.314026 on epoch=25
05/23/2022 04:38:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.401636 on epoch=26
05/23/2022 04:39:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.352779 on epoch=27
05/23/2022 04:39:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.363692 on epoch=28
05/23/2022 04:39:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.410980 on epoch=29
05/23/2022 04:39:13 - INFO - __main__ - Global step 350 Train loss 0.368623 Classification-F1 1.0 on epoch=29
05/23/2022 04:39:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.385395 on epoch=29
05/23/2022 04:39:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.357029 on epoch=30
05/23/2022 04:39:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.320434 on epoch=31
05/23/2022 04:39:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.378236 on epoch=32
05/23/2022 04:39:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.377750 on epoch=33
05/23/2022 04:39:40 - INFO - __main__ - Global step 400 Train loss 0.363769 Classification-F1 1.0 on epoch=33
05/23/2022 04:39:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.342146 on epoch=34
05/23/2022 04:39:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.367364 on epoch=34
05/23/2022 04:39:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.313530 on epoch=35
05/23/2022 04:40:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.334603 on epoch=36
05/23/2022 04:40:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.320473 on epoch=37
05/23/2022 04:40:07 - INFO - __main__ - Global step 450 Train loss 0.335623 Classification-F1 1.0 on epoch=37
05/23/2022 04:40:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.301232 on epoch=38
05/23/2022 04:40:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.335917 on epoch=39
05/23/2022 04:40:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.330703 on epoch=39
05/23/2022 04:40:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.287911 on epoch=40
05/23/2022 04:40:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.317088 on epoch=41
05/23/2022 04:40:34 - INFO - __main__ - Global step 500 Train loss 0.314570 Classification-F1 1.0 on epoch=41
05/23/2022 04:40:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.284945 on epoch=42
05/23/2022 04:40:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.303371 on epoch=43
05/23/2022 04:40:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.271715 on epoch=44
05/23/2022 04:40:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.265082 on epoch=44
05/23/2022 04:40:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.210147 on epoch=45
05/23/2022 04:41:01 - INFO - __main__ - Global step 550 Train loss 0.267052 Classification-F1 0.49206349206349204 on epoch=45
05/23/2022 04:41:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.300903 on epoch=46
05/23/2022 04:41:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.270430 on epoch=47
05/23/2022 04:41:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.274577 on epoch=48
05/23/2022 04:41:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.161435 on epoch=49
05/23/2022 04:41:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.127376 on epoch=49
05/23/2022 04:41:28 - INFO - __main__ - Global step 600 Train loss 0.226944 Classification-F1 0.4796747967479675 on epoch=49
05/23/2022 04:41:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.164800 on epoch=50
05/23/2022 04:41:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.197434 on epoch=51
05/23/2022 04:41:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.186961 on epoch=52
05/23/2022 04:41:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.199034 on epoch=53
05/23/2022 04:41:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.205842 on epoch=54
05/23/2022 04:41:55 - INFO - __main__ - Global step 650 Train loss 0.190814 Classification-F1 0.4796747967479675 on epoch=54
05/23/2022 04:42:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.132628 on epoch=54
05/23/2022 04:42:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.108597 on epoch=55
05/23/2022 04:42:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.176407 on epoch=56
05/23/2022 04:42:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.160301 on epoch=57
05/23/2022 04:42:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.148628 on epoch=58
05/23/2022 04:42:22 - INFO - __main__ - Global step 700 Train loss 0.145312 Classification-F1 0.4900398406374502 on epoch=58
05/23/2022 04:42:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.160784 on epoch=59
05/23/2022 04:42:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.171777 on epoch=59
05/23/2022 04:42:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.217987 on epoch=60
05/23/2022 04:42:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.263293 on epoch=61
05/23/2022 04:42:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.273654 on epoch=62
05/23/2022 04:42:50 - INFO - __main__ - Global step 750 Train loss 0.217499 Classification-F1 0.49606299212598426 on epoch=62
05/23/2022 04:42:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.203741 on epoch=63
05/23/2022 04:43:00 - INFO - __main__ - Step 770 Global step 770 Train loss 0.151485 on epoch=64
05/23/2022 04:43:05 - INFO - __main__ - Step 780 Global step 780 Train loss 0.131859 on epoch=64
05/23/2022 04:43:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.101196 on epoch=65
05/23/2022 04:43:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.236659 on epoch=66
05/23/2022 04:43:17 - INFO - __main__ - Global step 800 Train loss 0.164988 Classification-F1 0.49206349206349204 on epoch=66
05/23/2022 04:43:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.188642 on epoch=67
05/23/2022 04:43:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.117832 on epoch=68
05/23/2022 04:43:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.091572 on epoch=69
05/23/2022 04:43:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.167864 on epoch=69
05/23/2022 04:43:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.127730 on epoch=70
05/23/2022 04:43:44 - INFO - __main__ - Global step 850 Train loss 0.138728 Classification-F1 0.4775510204081633 on epoch=70
05/23/2022 04:43:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.123798 on epoch=71
05/23/2022 04:43:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.154113 on epoch=72
05/23/2022 04:43:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.154302 on epoch=73
05/23/2022 04:44:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.099894 on epoch=74
05/23/2022 04:44:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.108780 on epoch=74
05/23/2022 04:44:11 - INFO - __main__ - Global step 900 Train loss 0.128177 Classification-F1 0.4900398406374502 on epoch=74
05/23/2022 04:44:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.127469 on epoch=75
05/23/2022 04:44:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.117228 on epoch=76
05/23/2022 04:44:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.056334 on epoch=77
05/23/2022 04:44:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.166976 on epoch=78
05/23/2022 04:44:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.110156 on epoch=79
05/23/2022 04:44:38 - INFO - __main__ - Global step 950 Train loss 0.115632 Classification-F1 0.49606299212598426 on epoch=79
05/23/2022 04:44:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.092926 on epoch=79
05/23/2022 04:44:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.149500 on epoch=80
05/23/2022 04:44:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.079210 on epoch=81
05/23/2022 04:44:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.083926 on epoch=82
05/23/2022 04:45:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.099374 on epoch=83
05/23/2022 04:45:05 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 04:45:05 - INFO - __main__ - Printing 3 examples
05/23/2022 04:45:05 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
05/23/2022 04:45:05 - INFO - __main__ - ['false']
05/23/2022 04:45:05 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
05/23/2022 04:45:05 - INFO - __main__ - ['false']
05/23/2022 04:45:05 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
05/23/2022 04:45:05 - INFO - __main__ - ['false']
05/23/2022 04:45:05 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:45:05 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:45:05 - INFO - __main__ - Global step 1000 Train loss 0.100987 Classification-F1 0.49206349206349204 on epoch=83
05/23/2022 04:45:05 - INFO - __main__ - save last model!
05/23/2022 04:45:05 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 04:45:05 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 04:45:05 - INFO - __main__ - Printing 3 examples
05/23/2022 04:45:05 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 04:45:05 - INFO - __main__ - ['false']
05/23/2022 04:45:05 - INFO - __main__ -  [ethos-race] If you are from Germany I will call you Nazi or Hitler. Then ill protest to hang you
05/23/2022 04:45:05 - INFO - __main__ - ['false']
05/23/2022 04:45:05 - INFO - __main__ -  [ethos-race] stone throwing must be legal when against women
05/23/2022 04:45:05 - INFO - __main__ - ['false']
05/23/2022 04:45:05 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:45:05 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:45:06 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 04:45:12 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 04:45:13 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 04:45:13 - INFO - __main__ - Printing 3 examples
05/23/2022 04:45:13 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 04:45:13 - INFO - __main__ - ['true']
05/23/2022 04:45:13 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 04:45:13 - INFO - __main__ - ['false']
05/23/2022 04:45:13 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 04:45:13 - INFO - __main__ - ['false']
05/23/2022 04:45:13 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:45:13 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:45:13 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 04:45:15 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_87_0.0005_8_predictions.txt
05/23/2022 04:45:15 - INFO - __main__ - Classification-F1 on test data: 0.4494
05/23/2022 04:45:15 - INFO - __main__ - prefix=ethos-race_128_87, lr=0.0005, bsz=8, dev_performance=1.0, test_performance=0.44936708860759494
05/23/2022 04:45:15 - INFO - __main__ - Running ... prefix=ethos-race_128_87, lr=0.0003, bsz=8 ...
05/23/2022 04:45:16 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 04:45:16 - INFO - __main__ - Printing 3 examples
05/23/2022 04:45:16 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
05/23/2022 04:45:16 - INFO - __main__ - ['false']
05/23/2022 04:45:16 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
05/23/2022 04:45:16 - INFO - __main__ - ['false']
05/23/2022 04:45:16 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
05/23/2022 04:45:16 - INFO - __main__ - ['false']
05/23/2022 04:45:16 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:45:16 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:45:16 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 04:45:16 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 04:45:16 - INFO - __main__ - Printing 3 examples
05/23/2022 04:45:16 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 04:45:16 - INFO - __main__ - ['false']
05/23/2022 04:45:16 - INFO - __main__ -  [ethos-race] If you are from Germany I will call you Nazi or Hitler. Then ill protest to hang you
05/23/2022 04:45:16 - INFO - __main__ - ['false']
05/23/2022 04:45:16 - INFO - __main__ -  [ethos-race] stone throwing must be legal when against women
05/23/2022 04:45:16 - INFO - __main__ - ['false']
05/23/2022 04:45:16 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:45:16 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:45:16 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 04:45:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 04:45:18 - INFO - __main__ - Starting training!
05/23/2022 04:45:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 04:45:29 - INFO - __main__ - Starting training!
05/23/2022 04:45:33 - INFO - __main__ - Step 10 Global step 10 Train loss 24.401821 on epoch=0
05/23/2022 04:45:39 - INFO - __main__ - Step 20 Global step 20 Train loss 18.788025 on epoch=1
05/23/2022 04:45:44 - INFO - __main__ - Step 30 Global step 30 Train loss 16.583084 on epoch=2
05/23/2022 04:45:49 - INFO - __main__ - Step 40 Global step 40 Train loss 15.782146 on epoch=3
05/23/2022 04:45:54 - INFO - __main__ - Step 50 Global step 50 Train loss 14.153902 on epoch=4
05/23/2022 04:46:33 - INFO - __main__ - Global step 50 Train loss 17.941795 Classification-F1 0.0 on epoch=4
05/23/2022 04:46:39 - INFO - __main__ - Step 60 Global step 60 Train loss 12.580063 on epoch=4
05/23/2022 04:46:44 - INFO - __main__ - Step 70 Global step 70 Train loss 10.915892 on epoch=5
05/23/2022 04:46:50 - INFO - __main__ - Step 80 Global step 80 Train loss 7.488368 on epoch=6
05/23/2022 04:46:55 - INFO - __main__ - Step 90 Global step 90 Train loss 2.942572 on epoch=7
05/23/2022 04:47:00 - INFO - __main__ - Step 100 Global step 100 Train loss 2.807727 on epoch=8
05/23/2022 04:47:01 - INFO - __main__ - Global step 100 Train loss 7.346924 Classification-F1 1.0 on epoch=8
05/23/2022 04:47:07 - INFO - __main__ - Step 110 Global step 110 Train loss 1.296010 on epoch=9
05/23/2022 04:47:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.684684 on epoch=9
05/23/2022 04:47:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.512621 on epoch=10
05/23/2022 04:47:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.609795 on epoch=11
05/23/2022 04:47:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.484984 on epoch=12
05/23/2022 04:47:29 - INFO - __main__ - Global step 150 Train loss 0.717619 Classification-F1 1.0 on epoch=12
05/23/2022 04:47:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.310704 on epoch=13
05/23/2022 04:47:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.302475 on epoch=14
05/23/2022 04:47:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.268183 on epoch=14
05/23/2022 04:47:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.267993 on epoch=15
05/23/2022 04:47:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.256787 on epoch=16
05/23/2022 04:47:56 - INFO - __main__ - Global step 200 Train loss 0.281228 Classification-F1 0.4980392156862745 on epoch=16
05/23/2022 04:48:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.404180 on epoch=17
05/23/2022 04:48:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.369213 on epoch=18
05/23/2022 04:48:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.392735 on epoch=19
05/23/2022 04:48:17 - INFO - __main__ - Step 240 Global step 240 Train loss 1.864907 on epoch=19
05/23/2022 04:48:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.174074 on epoch=20
05/23/2022 04:48:23 - INFO - __main__ - Global step 250 Train loss 0.641022 Classification-F1 0.4576271186440678 on epoch=20
05/23/2022 04:48:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.260319 on epoch=21
05/23/2022 04:48:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.225912 on epoch=22
05/23/2022 04:48:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.142179 on epoch=23
05/23/2022 04:48:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.172248 on epoch=24
05/23/2022 04:48:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.132186 on epoch=24
05/23/2022 04:48:51 - INFO - __main__ - Global step 300 Train loss 0.186569 Classification-F1 0.4900398406374502 on epoch=24
05/23/2022 04:48:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.136896 on epoch=25
05/23/2022 04:49:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.090178 on epoch=26
05/23/2022 04:49:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.096307 on epoch=27
05/23/2022 04:49:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.078782 on epoch=28
05/23/2022 04:49:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.059386 on epoch=29
05/23/2022 04:49:18 - INFO - __main__ - Global step 350 Train loss 0.092310 Classification-F1 0.488 on epoch=29
05/23/2022 04:49:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.094879 on epoch=29
05/23/2022 04:49:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.097871 on epoch=30
05/23/2022 04:49:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.040153 on epoch=31
05/23/2022 04:49:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.067007 on epoch=32
05/23/2022 04:49:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.065953 on epoch=33
05/23/2022 04:49:45 - INFO - __main__ - Global step 400 Train loss 0.073173 Classification-F1 0.32396251673360105 on epoch=33
05/23/2022 04:49:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.028136 on epoch=34
05/23/2022 04:49:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.045515 on epoch=34
05/23/2022 04:50:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.066043 on epoch=35
05/23/2022 04:50:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.056249 on epoch=36
05/23/2022 04:50:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.082503 on epoch=37
05/23/2022 04:50:12 - INFO - __main__ - Global step 450 Train loss 0.055689 Classification-F1 0.49206349206349204 on epoch=37
05/23/2022 04:50:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.022041 on epoch=38
05/23/2022 04:50:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.046026 on epoch=39
05/23/2022 04:50:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.010883 on epoch=39
05/23/2022 04:50:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.031560 on epoch=40
05/23/2022 04:50:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.025835 on epoch=41
05/23/2022 04:50:39 - INFO - __main__ - Global step 500 Train loss 0.027269 Classification-F1 0.3225806451612903 on epoch=41
05/23/2022 04:50:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.051642 on epoch=42
05/23/2022 04:50:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.026277 on epoch=43
05/23/2022 04:50:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.026157 on epoch=44
05/23/2022 04:51:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.026979 on epoch=44
05/23/2022 04:51:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.031725 on epoch=45
05/23/2022 04:51:07 - INFO - __main__ - Global step 550 Train loss 0.032556 Classification-F1 0.3225806451612903 on epoch=45
05/23/2022 04:51:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.046507 on epoch=46
05/23/2022 04:51:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.054529 on epoch=47
05/23/2022 04:51:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.012714 on epoch=48
05/23/2022 04:51:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.007758 on epoch=49
05/23/2022 04:51:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.236998 on epoch=49
05/23/2022 04:51:34 - INFO - __main__ - Global step 600 Train loss 0.071701 Classification-F1 0.488 on epoch=49
05/23/2022 04:51:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.027925 on epoch=50
05/23/2022 04:51:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.101333 on epoch=51
05/23/2022 04:51:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.046117 on epoch=52
05/23/2022 04:51:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.084140 on epoch=53
05/23/2022 04:52:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.017705 on epoch=54
05/23/2022 04:52:01 - INFO - __main__ - Global step 650 Train loss 0.055444 Classification-F1 0.488 on epoch=54
05/23/2022 04:52:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.014000 on epoch=54
05/23/2022 04:52:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.040652 on epoch=55
05/23/2022 04:52:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.018130 on epoch=56
05/23/2022 04:52:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.052788 on epoch=57
05/23/2022 04:52:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.030540 on epoch=58
05/23/2022 04:52:28 - INFO - __main__ - Global step 700 Train loss 0.031222 Classification-F1 0.4900398406374502 on epoch=58
05/23/2022 04:52:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.011055 on epoch=59
05/23/2022 04:52:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.010926 on epoch=59
05/23/2022 04:52:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.022544 on epoch=60
05/23/2022 04:52:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.051227 on epoch=61
05/23/2022 04:52:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.007863 on epoch=62
05/23/2022 04:52:55 - INFO - __main__ - Global step 750 Train loss 0.020723 Classification-F1 0.31978319783197834 on epoch=62
05/23/2022 04:53:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.006897 on epoch=63
05/23/2022 04:53:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.030888 on epoch=64
05/23/2022 04:53:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.010720 on epoch=64
05/23/2022 04:53:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.002950 on epoch=65
05/23/2022 04:53:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.033751 on epoch=66
05/23/2022 04:53:22 - INFO - __main__ - Global step 800 Train loss 0.017041 Classification-F1 0.32118758434547906 on epoch=66
05/23/2022 04:53:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.007780 on epoch=67
05/23/2022 04:53:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.002036 on epoch=68
05/23/2022 04:53:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.006467 on epoch=69
05/23/2022 04:53:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.029922 on epoch=69
05/23/2022 04:53:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.012384 on epoch=70
05/23/2022 04:53:49 - INFO - __main__ - Global step 850 Train loss 0.011718 Classification-F1 0.4859437751004016 on epoch=70
05/23/2022 04:53:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.013526 on epoch=71
05/23/2022 04:54:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.001337 on epoch=72
05/23/2022 04:54:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.006996 on epoch=73
05/23/2022 04:54:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.002935 on epoch=74
05/23/2022 04:54:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000788 on epoch=74
05/23/2022 04:54:17 - INFO - __main__ - Global step 900 Train loss 0.005116 Classification-F1 0.488 on epoch=74
05/23/2022 04:54:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000451 on epoch=75
05/23/2022 04:54:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.001629 on epoch=76
05/23/2022 04:54:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.002471 on epoch=77
05/23/2022 04:54:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000489 on epoch=78
05/23/2022 04:54:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000869 on epoch=79
05/23/2022 04:54:44 - INFO - __main__ - Global step 950 Train loss 0.001182 Classification-F1 0.4900398406374502 on epoch=79
05/23/2022 04:54:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.001070 on epoch=79
05/23/2022 04:54:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000126 on epoch=80
05/23/2022 04:54:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000158 on epoch=81
05/23/2022 04:55:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.002428 on epoch=82
05/23/2022 04:55:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000314 on epoch=83
05/23/2022 04:55:11 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 04:55:11 - INFO - __main__ - Printing 3 examples
05/23/2022 04:55:11 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
05/23/2022 04:55:11 - INFO - __main__ - ['false']
05/23/2022 04:55:11 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
05/23/2022 04:55:11 - INFO - __main__ - ['false']
05/23/2022 04:55:11 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
05/23/2022 04:55:11 - INFO - __main__ - ['false']
05/23/2022 04:55:11 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:55:11 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:55:11 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 04:55:11 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 04:55:11 - INFO - __main__ - Printing 3 examples
05/23/2022 04:55:11 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 04:55:11 - INFO - __main__ - ['false']
05/23/2022 04:55:11 - INFO - __main__ -  [ethos-race] If you are from Germany I will call you Nazi or Hitler. Then ill protest to hang you
05/23/2022 04:55:11 - INFO - __main__ - ['false']
05/23/2022 04:55:11 - INFO - __main__ -  [ethos-race] stone throwing must be legal when against women
05/23/2022 04:55:11 - INFO - __main__ - ['false']
05/23/2022 04:55:11 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:55:11 - INFO - __main__ - Global step 1000 Train loss 0.000819 Classification-F1 0.3253333333333333 on epoch=83
05/23/2022 04:55:11 - INFO - __main__ - save last model!
05/23/2022 04:55:11 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:55:11 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 04:55:18 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 04:55:19 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 04:55:19 - INFO - __main__ - Printing 3 examples
05/23/2022 04:55:19 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 04:55:19 - INFO - __main__ - ['true']
05/23/2022 04:55:19 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 04:55:19 - INFO - __main__ - ['false']
05/23/2022 04:55:19 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 04:55:19 - INFO - __main__ - ['false']
05/23/2022 04:55:19 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:55:19 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:55:19 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 04:55:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_87_0.0003_8_predictions.txt
05/23/2022 04:55:20 - INFO - __main__ - Classification-F1 on test data: 0.4494
05/23/2022 04:55:21 - INFO - __main__ - prefix=ethos-race_128_87, lr=0.0003, bsz=8, dev_performance=1.0, test_performance=0.44936708860759494
05/23/2022 04:55:21 - INFO - __main__ - Running ... prefix=ethos-race_128_87, lr=0.0002, bsz=8 ...
05/23/2022 04:55:22 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 04:55:22 - INFO - __main__ - Printing 3 examples
05/23/2022 04:55:22 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
05/23/2022 04:55:22 - INFO - __main__ - ['false']
05/23/2022 04:55:22 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
05/23/2022 04:55:22 - INFO - __main__ - ['false']
05/23/2022 04:55:22 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
05/23/2022 04:55:22 - INFO - __main__ - ['false']
05/23/2022 04:55:22 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:55:22 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:55:22 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 04:55:22 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 04:55:22 - INFO - __main__ - Printing 3 examples
05/23/2022 04:55:22 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 04:55:22 - INFO - __main__ - ['false']
05/23/2022 04:55:22 - INFO - __main__ -  [ethos-race] If you are from Germany I will call you Nazi or Hitler. Then ill protest to hang you
05/23/2022 04:55:22 - INFO - __main__ - ['false']
05/23/2022 04:55:22 - INFO - __main__ -  [ethos-race] stone throwing must be legal when against women
05/23/2022 04:55:22 - INFO - __main__ - ['false']
05/23/2022 04:55:22 - INFO - __main__ - Tokenizing Input ...
05/23/2022 04:55:22 - INFO - __main__ - Tokenizing Output ...
05/23/2022 04:55:22 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 04:55:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 04:55:23 - INFO - __main__ - Starting training!
05/23/2022 04:55:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 04:55:35 - INFO - __main__ - Starting training!
05/23/2022 04:55:39 - INFO - __main__ - Step 10 Global step 10 Train loss 24.020554 on epoch=0
05/23/2022 04:55:44 - INFO - __main__ - Step 20 Global step 20 Train loss 19.536310 on epoch=1
05/23/2022 04:55:49 - INFO - __main__ - Step 30 Global step 30 Train loss 17.139156 on epoch=2
05/23/2022 04:55:54 - INFO - __main__ - Step 40 Global step 40 Train loss 18.522957 on epoch=3
05/23/2022 04:55:59 - INFO - __main__ - Step 50 Global step 50 Train loss 15.921722 on epoch=4
05/23/2022 04:56:24 - INFO - __main__ - Global step 50 Train loss 19.028139 Classification-F1 0.0 on epoch=4
05/23/2022 04:56:29 - INFO - __main__ - Step 60 Global step 60 Train loss 15.079966 on epoch=4
05/23/2022 04:56:35 - INFO - __main__ - Step 70 Global step 70 Train loss 14.274852 on epoch=5
05/23/2022 04:56:40 - INFO - __main__ - Step 80 Global step 80 Train loss 13.603444 on epoch=6
05/23/2022 04:56:45 - INFO - __main__ - Step 90 Global step 90 Train loss 12.848117 on epoch=7
05/23/2022 04:56:50 - INFO - __main__ - Step 100 Global step 100 Train loss 12.011584 on epoch=8
05/23/2022 04:56:51 - INFO - __main__ - Global step 100 Train loss 13.563593 Classification-F1 0.0 on epoch=8
05/23/2022 04:56:57 - INFO - __main__ - Step 110 Global step 110 Train loss 10.720175 on epoch=9
05/23/2022 04:57:02 - INFO - __main__ - Step 120 Global step 120 Train loss 9.580976 on epoch=9
05/23/2022 04:57:07 - INFO - __main__ - Step 130 Global step 130 Train loss 6.615062 on epoch=10
05/23/2022 04:57:12 - INFO - __main__ - Step 140 Global step 140 Train loss 4.226121 on epoch=11
05/23/2022 04:57:17 - INFO - __main__ - Step 150 Global step 150 Train loss 2.914620 on epoch=12
05/23/2022 04:57:19 - INFO - __main__ - Global step 150 Train loss 6.811391 Classification-F1 1.0 on epoch=12
05/23/2022 04:57:25 - INFO - __main__ - Step 160 Global step 160 Train loss 2.639268 on epoch=13
05/23/2022 04:57:30 - INFO - __main__ - Step 170 Global step 170 Train loss 2.583658 on epoch=14
05/23/2022 04:57:35 - INFO - __main__ - Step 180 Global step 180 Train loss 2.207050 on epoch=14
05/23/2022 04:57:40 - INFO - __main__ - Step 190 Global step 190 Train loss 1.756939 on epoch=15
05/23/2022 04:57:46 - INFO - __main__ - Step 200 Global step 200 Train loss 2.002715 on epoch=16
05/23/2022 04:57:47 - INFO - __main__ - Global step 200 Train loss 2.237926 Classification-F1 1.0 on epoch=16
05/23/2022 04:57:52 - INFO - __main__ - Step 210 Global step 210 Train loss 1.838806 on epoch=17
05/23/2022 04:57:57 - INFO - __main__ - Step 220 Global step 220 Train loss 1.905825 on epoch=18
05/23/2022 04:58:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.905195 on epoch=19
05/23/2022 04:58:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.590968 on epoch=19
05/23/2022 04:58:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.373922 on epoch=20
05/23/2022 04:58:14 - INFO - __main__ - Global step 250 Train loss 1.122943 Classification-F1 0.17419354838709677 on epoch=20
05/23/2022 04:58:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.495129 on epoch=21
05/23/2022 04:58:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.374553 on epoch=22
05/23/2022 04:58:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.427457 on epoch=23
05/23/2022 04:58:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.370120 on epoch=24
05/23/2022 04:58:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.370025 on epoch=24
05/23/2022 04:58:42 - INFO - __main__ - Global step 300 Train loss 0.407457 Classification-F1 0.4796747967479675 on epoch=24
05/23/2022 04:58:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.354041 on epoch=25
05/23/2022 04:58:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.285428 on epoch=26
05/23/2022 04:58:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.334684 on epoch=27
05/23/2022 04:59:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.323056 on epoch=28
05/23/2022 04:59:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.283523 on epoch=29
05/23/2022 04:59:09 - INFO - __main__ - Global step 350 Train loss 0.316146 Classification-F1 0.4859437751004016 on epoch=29
05/23/2022 04:59:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.304900 on epoch=29
05/23/2022 04:59:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.208661 on epoch=30
05/23/2022 04:59:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.266714 on epoch=31
05/23/2022 04:59:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.243140 on epoch=32
05/23/2022 04:59:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.260579 on epoch=33
05/23/2022 04:59:36 - INFO - __main__ - Global step 400 Train loss 0.256799 Classification-F1 0.49606299212598426 on epoch=33
05/23/2022 04:59:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.242418 on epoch=34
05/23/2022 04:59:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.186613 on epoch=34
05/23/2022 04:59:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.172940 on epoch=35
05/23/2022 04:59:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.179885 on epoch=36
05/23/2022 05:00:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.157465 on epoch=37
05/23/2022 05:00:04 - INFO - __main__ - Global step 450 Train loss 0.187864 Classification-F1 0.49407114624505927 on epoch=37
05/23/2022 05:00:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.177209 on epoch=38
05/23/2022 05:00:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.165207 on epoch=39
05/23/2022 05:00:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.120556 on epoch=39
05/23/2022 05:00:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.094102 on epoch=40
05/23/2022 05:00:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.101785 on epoch=41
05/23/2022 05:00:31 - INFO - __main__ - Global step 500 Train loss 0.131772 Classification-F1 0.49407114624505927 on epoch=41
05/23/2022 05:00:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.092600 on epoch=42
05/23/2022 05:00:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.067217 on epoch=43
05/23/2022 05:00:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.076527 on epoch=44
05/23/2022 05:00:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.097574 on epoch=44
05/23/2022 05:00:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.090713 on epoch=45
05/23/2022 05:00:58 - INFO - __main__ - Global step 550 Train loss 0.084926 Classification-F1 0.4900398406374502 on epoch=45
05/23/2022 05:01:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.184323 on epoch=46
05/23/2022 05:01:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.080748 on epoch=47
05/23/2022 05:01:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.070294 on epoch=48
05/23/2022 05:01:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.049698 on epoch=49
05/23/2022 05:01:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.058280 on epoch=49
05/23/2022 05:01:26 - INFO - __main__ - Global step 600 Train loss 0.088669 Classification-F1 0.49206349206349204 on epoch=49
05/23/2022 05:01:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.050115 on epoch=50
05/23/2022 05:01:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.036675 on epoch=51
05/23/2022 05:01:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.059934 on epoch=52
05/23/2022 05:01:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.033542 on epoch=53
05/23/2022 05:01:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.016776 on epoch=54
05/23/2022 05:01:53 - INFO - __main__ - Global step 650 Train loss 0.039408 Classification-F1 0.488 on epoch=54
05/23/2022 05:01:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.050316 on epoch=54
05/23/2022 05:02:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.069276 on epoch=55
05/23/2022 05:02:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.045157 on epoch=56
05/23/2022 05:02:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.057897 on epoch=57
05/23/2022 05:02:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.023651 on epoch=58
05/23/2022 05:02:20 - INFO - __main__ - Global step 700 Train loss 0.049259 Classification-F1 0.488 on epoch=58
05/23/2022 05:02:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.018665 on epoch=59
05/23/2022 05:02:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.008163 on epoch=59
05/23/2022 05:02:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.048070 on epoch=60
05/23/2022 05:02:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.007053 on epoch=61
05/23/2022 05:02:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.017413 on epoch=62
05/23/2022 05:02:47 - INFO - __main__ - Global step 750 Train loss 0.019873 Classification-F1 0.4817813765182186 on epoch=62
05/23/2022 05:02:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.018629 on epoch=63
05/23/2022 05:02:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.038228 on epoch=64
05/23/2022 05:03:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.009003 on epoch=64
05/23/2022 05:03:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.022612 on epoch=65
05/23/2022 05:03:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.008000 on epoch=66
05/23/2022 05:03:14 - INFO - __main__ - Global step 800 Train loss 0.019294 Classification-F1 0.4900398406374502 on epoch=66
05/23/2022 05:03:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.011979 on epoch=67
05/23/2022 05:03:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.015201 on epoch=68
05/23/2022 05:03:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.016008 on epoch=69
05/23/2022 05:03:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.012352 on epoch=69
05/23/2022 05:03:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.012433 on epoch=70
05/23/2022 05:03:42 - INFO - __main__ - Global step 850 Train loss 0.013595 Classification-F1 0.4817813765182186 on epoch=70
05/23/2022 05:03:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.011988 on epoch=71
05/23/2022 05:03:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.014314 on epoch=72
05/23/2022 05:03:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.010268 on epoch=73
05/23/2022 05:04:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.010682 on epoch=74
05/23/2022 05:04:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001398 on epoch=74
05/23/2022 05:04:09 - INFO - __main__ - Global step 900 Train loss 0.009730 Classification-F1 0.4900398406374502 on epoch=74
05/23/2022 05:04:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.022105 on epoch=75
05/23/2022 05:04:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.013335 on epoch=76
05/23/2022 05:04:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.003719 on epoch=77
05/23/2022 05:04:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.005670 on epoch=78
05/23/2022 05:04:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.002420 on epoch=79
05/23/2022 05:04:37 - INFO - __main__ - Global step 950 Train loss 0.009450 Classification-F1 0.488 on epoch=79
05/23/2022 05:04:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.002580 on epoch=79
05/23/2022 05:04:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.006555 on epoch=80
05/23/2022 05:04:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.004219 on epoch=81
05/23/2022 05:04:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.013524 on epoch=82
05/23/2022 05:05:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.005646 on epoch=83
05/23/2022 05:05:04 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 05:05:04 - INFO - __main__ - Printing 3 examples
05/23/2022 05:05:04 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
05/23/2022 05:05:04 - INFO - __main__ - ['false']
05/23/2022 05:05:04 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
05/23/2022 05:05:04 - INFO - __main__ - ['false']
05/23/2022 05:05:04 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
05/23/2022 05:05:04 - INFO - __main__ - ['false']
05/23/2022 05:05:04 - INFO - __main__ - Tokenizing Input ...
05/23/2022 05:05:04 - INFO - __main__ - Tokenizing Output ...
05/23/2022 05:05:04 - INFO - __main__ - Global step 1000 Train loss 0.006505 Classification-F1 0.4859437751004016 on epoch=83
05/23/2022 05:05:04 - INFO - __main__ - save last model!
05/23/2022 05:05:04 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 05:05:04 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 05:05:04 - INFO - __main__ - Printing 3 examples
05/23/2022 05:05:04 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 05:05:04 - INFO - __main__ - ['false']
05/23/2022 05:05:04 - INFO - __main__ -  [ethos-race] If you are from Germany I will call you Nazi or Hitler. Then ill protest to hang you
05/23/2022 05:05:04 - INFO - __main__ - ['false']
05/23/2022 05:05:04 - INFO - __main__ -  [ethos-race] stone throwing must be legal when against women
05/23/2022 05:05:04 - INFO - __main__ - ['false']
05/23/2022 05:05:04 - INFO - __main__ - Tokenizing Input ...
05/23/2022 05:05:04 - INFO - __main__ - Tokenizing Output ...
05/23/2022 05:05:04 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 05:05:10 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 05:05:11 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 05:05:11 - INFO - __main__ - Printing 3 examples
05/23/2022 05:05:11 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 05:05:11 - INFO - __main__ - ['true']
05/23/2022 05:05:11 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 05:05:11 - INFO - __main__ - ['false']
05/23/2022 05:05:11 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 05:05:11 - INFO - __main__ - ['false']
05/23/2022 05:05:11 - INFO - __main__ - Tokenizing Input ...
05/23/2022 05:05:11 - INFO - __main__ - Tokenizing Output ...
05/23/2022 05:05:11 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 05:05:13 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_87_0.0002_8_predictions.txt
05/23/2022 05:05:13 - INFO - __main__ - Classification-F1 on test data: 0.4494
05/23/2022 05:05:13 - INFO - __main__ - prefix=ethos-race_128_87, lr=0.0002, bsz=8, dev_performance=1.0, test_performance=0.44936708860759494
05/23/2022 05:05:13 - INFO - __main__ - Running ... prefix=ethos-race_128_87, lr=0.0001, bsz=8 ...
05/23/2022 05:05:14 - INFO - __main__ - Start tokenizing ... 188 instances
05/23/2022 05:05:14 - INFO - __main__ - Printing 3 examples
05/23/2022 05:05:14 - INFO - __main__ -  [ethos-race] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
05/23/2022 05:05:14 - INFO - __main__ - ['false']
05/23/2022 05:05:14 - INFO - __main__ -  [ethos-race] Honestly, go fuck yourself! bitch!
05/23/2022 05:05:14 - INFO - __main__ - ['false']
05/23/2022 05:05:14 - INFO - __main__ -  [ethos-race] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
05/23/2022 05:05:14 - INFO - __main__ - ['false']
05/23/2022 05:05:14 - INFO - __main__ - Tokenizing Input ...
05/23/2022 05:05:14 - INFO - __main__ - Tokenizing Output ...
05/23/2022 05:05:14 - INFO - __main__ - Loaded 188 examples from train data
05/23/2022 05:05:14 - INFO - __main__ - Start tokenizing ... 128 instances
05/23/2022 05:05:14 - INFO - __main__ - Printing 3 examples
05/23/2022 05:05:14 - INFO - __main__ -  [ethos-race] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
05/23/2022 05:05:14 - INFO - __main__ - ['false']
05/23/2022 05:05:14 - INFO - __main__ -  [ethos-race] If you are from Germany I will call you Nazi or Hitler. Then ill protest to hang you
05/23/2022 05:05:14 - INFO - __main__ - ['false']
05/23/2022 05:05:14 - INFO - __main__ -  [ethos-race] stone throwing must be legal when against women
05/23/2022 05:05:14 - INFO - __main__ - ['false']
05/23/2022 05:05:14 - INFO - __main__ - Tokenizing Input ...
05/23/2022 05:05:14 - INFO - __main__ - Tokenizing Output ...
05/23/2022 05:05:15 - INFO - __main__ - Loaded 128 examples from dev data
05/23/2022 05:05:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 05:05:15 - INFO - __main__ - Starting training!
05/23/2022 05:05:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/23/2022 05:05:27 - INFO - __main__ - Starting training!
05/23/2022 05:05:32 - INFO - __main__ - Step 10 Global step 10 Train loss 24.920088 on epoch=0
05/23/2022 05:05:37 - INFO - __main__ - Step 20 Global step 20 Train loss 21.027302 on epoch=1
05/23/2022 05:05:42 - INFO - __main__ - Step 30 Global step 30 Train loss 18.240084 on epoch=2
05/23/2022 05:05:47 - INFO - __main__ - Step 40 Global step 40 Train loss 16.613430 on epoch=3
05/23/2022 05:05:52 - INFO - __main__ - Step 50 Global step 50 Train loss 16.481100 on epoch=4
05/23/2022 05:06:26 - INFO - __main__ - Global step 50 Train loss 19.456402 Classification-F1 0.0 on epoch=4
05/23/2022 05:06:32 - INFO - __main__ - Step 60 Global step 60 Train loss 17.262325 on epoch=4
05/23/2022 05:06:37 - INFO - __main__ - Step 70 Global step 70 Train loss 16.967476 on epoch=5
05/23/2022 05:06:42 - INFO - __main__ - Step 80 Global step 80 Train loss 15.473969 on epoch=6
05/23/2022 05:06:47 - INFO - __main__ - Step 90 Global step 90 Train loss 14.885172 on epoch=7
05/23/2022 05:06:52 - INFO - __main__ - Step 100 Global step 100 Train loss 15.034925 on epoch=8
05/23/2022 05:07:10 - INFO - __main__ - Global step 100 Train loss 15.924773 Classification-F1 0.0 on epoch=8
05/23/2022 05:07:15 - INFO - __main__ - Step 110 Global step 110 Train loss 14.951979 on epoch=9
05/23/2022 05:07:20 - INFO - __main__ - Step 120 Global step 120 Train loss 14.672636 on epoch=9
05/23/2022 05:07:25 - INFO - __main__ - Step 130 Global step 130 Train loss 13.647406 on epoch=10
05/23/2022 05:07:30 - INFO - __main__ - Step 140 Global step 140 Train loss 14.724762 on epoch=11
05/23/2022 05:07:35 - INFO - __main__ - Step 150 Global step 150 Train loss 12.439880 on epoch=12
05/23/2022 05:07:47 - INFO - __main__ - Global step 150 Train loss 14.087333 Classification-F1 0.0 on epoch=12
05/23/2022 05:07:52 - INFO - __main__ - Step 160 Global step 160 Train loss 12.460267 on epoch=13
05/23/2022 05:07:57 - INFO - __main__ - Step 170 Global step 170 Train loss 12.352777 on epoch=14
05/23/2022 05:08:03 - INFO - __main__ - Step 180 Global step 180 Train loss 11.554910 on epoch=14
05/23/2022 05:08:08 - INFO - __main__ - Step 190 Global step 190 Train loss 11.153425 on epoch=15
05/23/2022 05:08:13 - INFO - __main__ - Step 200 Global step 200 Train loss 10.010737 on epoch=16
05/23/2022 05:08:15 - INFO - __main__ - Global step 200 Train loss 11.506423 Classification-F1 0.001594896331738437 on epoch=16
05/23/2022 05:08:21 - INFO - __main__ - Step 210 Global step 210 Train loss 8.976161 on epoch=17
05/23/2022 05:08:26 - INFO - __main__ - Step 220 Global step 220 Train loss 8.233240 on epoch=18
05/23/2022 05:08:31 - INFO - __main__ - Step 230 Global step 230 Train loss 5.005700 on epoch=19
05/23/2022 05:08:36 - INFO - __main__ - Step 240 Global step 240 Train loss 1.274955 on epoch=19
05/23/2022 05:08:41 - INFO - __main__ - Step 250 Global step 250 Train loss 1.079600 on epoch=20
05/23/2022 05:08:43 - INFO - __main__ - Global step 250 Train loss 4.913931 Classification-F1 0.09219858156028368 on epoch=20
05/23/2022 05:08:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.792765 on epoch=21
05/23/2022 05:08:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.553689 on epoch=22
05/23/2022 05:08:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.839011 on epoch=23
05/23/2022 05:09:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.717978 on epoch=24
05/23/2022 05:09:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.675541 on epoch=24
05/23/2022 05:09:10 - INFO - __main__ - Global step 300 Train loss 0.715797 Classification-F1 0.3402061855670103 on epoch=24
05/23/2022 05:09:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.335790 on epoch=25
05/23/2022 05:09:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.319037 on epoch=26
05/23/2022 05:09:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.337585 on epoch=27
05/23/2022 05:09:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.368819 on epoch=28
05/23/2022 05:09:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.317234 on epoch=29
05/23/2022 05:09:38 - INFO - __main__ - Global step 350 Train loss 0.335693 Classification-F1 0.488 on epoch=29
05/23/2022 05:09:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.375457 on epoch=29
05/23/2022 05:09:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.182414 on epoch=30
05/23/2022 05:09:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.258808 on epoch=31
05/23/2022 05:10:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.343101 on epoch=32
05/23/2022 05:10:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.284992 on epoch=33
05/23/2022 05:10:07 - INFO - __main__ - Global step 400 Train loss 0.288954 Classification-F1 0.49206349206349204 on epoch=33
05/23/2022 05:10:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.200714 on epoch=34
05/23/2022 05:10:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.190613 on epoch=34
05/23/2022 05:10:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.171653 on epoch=35
05/23/2022 05:10:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.189599 on epoch=36
05/23/2022 05:10:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.241186 on epoch=37
05/23/2022 05:10:35 - INFO - __main__ - Global step 450 Train loss 0.198753 Classification-F1 0.49407114624505927 on epoch=37
05/23/2022 05:10:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.202370 on epoch=38
05/23/2022 05:10:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.172511 on epoch=39
05/23/2022 05:10:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.184367 on epoch=39
05/23/2022 05:10:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.176534 on epoch=40
05/23/2022 05:11:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.163442 on epoch=41
05/23/2022 05:11:04 - INFO - __main__ - Global step 500 Train loss 0.179845 Classification-F1 0.49407114624505927 on epoch=41
05/23/2022 05:11:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.130588 on epoch=42
05/23/2022 05:11:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.148503 on epoch=43
05/23/2022 05:11:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.199215 on epoch=44
05/23/2022 05:11:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.114431 on epoch=44
05/23/2022 05:11:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.095979 on epoch=45
05/23/2022 05:11:31 - INFO - __main__ - Global step 550 Train loss 0.137743 Classification-F1 0.4838709677419355 on epoch=45
05/23/2022 05:11:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.110237 on epoch=46
05/23/2022 05:11:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.090255 on epoch=47
05/23/2022 05:11:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.125920 on epoch=48
05/23/2022 05:11:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.078793 on epoch=49
05/23/2022 05:11:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.084800 on epoch=49
05/23/2022 05:11:59 - INFO - __main__ - Global step 600 Train loss 0.098001 Classification-F1 0.4817813765182186 on epoch=49
05/23/2022 05:12:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.068206 on epoch=50
05/23/2022 05:12:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.083404 on epoch=51
05/23/2022 05:12:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.083421 on epoch=52
05/23/2022 05:12:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.067330 on epoch=53
05/23/2022 05:12:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.047864 on epoch=54
05/23/2022 05:12:27 - INFO - __main__ - Global step 650 Train loss 0.070045 Classification-F1 0.4838709677419355 on epoch=54
05/23/2022 05:12:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.052412 on epoch=54
05/23/2022 05:12:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.067028 on epoch=55
05/23/2022 05:12:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.045475 on epoch=56
05/23/2022 05:12:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.044115 on epoch=57
05/23/2022 05:12:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.029783 on epoch=58
05/23/2022 05:12:54 - INFO - __main__ - Global step 700 Train loss 0.047763 Classification-F1 0.4838709677419355 on epoch=58
05/23/2022 05:13:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.060154 on epoch=59
05/23/2022 05:13:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.024220 on epoch=59
05/23/2022 05:13:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.073224 on epoch=60
05/23/2022 05:13:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.036670 on epoch=61
05/23/2022 05:13:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.047333 on epoch=62
05/23/2022 05:13:22 - INFO - __main__ - Global step 750 Train loss 0.048320 Classification-F1 0.4838709677419355 on epoch=62
05/23/2022 05:13:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.021908 on epoch=63
05/23/2022 05:13:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.024513 on epoch=64
05/23/2022 05:13:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.034908 on epoch=64
05/23/2022 05:13:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.061367 on epoch=65
05/23/2022 05:13:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.040327 on epoch=66
05/23/2022 05:13:50 - INFO - __main__ - Global step 800 Train loss 0.036605 Classification-F1 0.4838709677419355 on epoch=66
05/23/2022 05:13:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.306721 on epoch=67
05/23/2022 05:14:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.346322 on epoch=68
05/23/2022 05:14:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.015609 on epoch=69
05/23/2022 05:14:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.008954 on epoch=69
05/23/2022 05:14:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.039984 on epoch=70
05/23/2022 05:14:17 - INFO - __main__ - Global step 850 Train loss 0.143518 Classification-F1 0.4900398406374502 on epoch=70
05/23/2022 05:14:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.014221 on epoch=71
05/23/2022 05:14:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.016686 on epoch=72
05/23/2022 05:14:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.011071 on epoch=73
05/23/2022 05:14:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.016671 on epoch=74
05/23/2022 05:14:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.120781 on epoch=74
05/23/2022 05:14:45 - INFO - __main__ - Global step 900 Train loss 0.035886 Classification-F1 0.4817813765182186 on epoch=74
05/23/2022 05:14:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.032352 on epoch=75
05/23/2022 05:14:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.011862 on epoch=76
05/23/2022 05:15:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.007777 on epoch=77
05/23/2022 05:15:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.022909 on epoch=78
05/23/2022 05:15:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.017597 on epoch=79
05/23/2022 05:15:13 - INFO - __main__ - Global step 950 Train loss 0.018500 Classification-F1 0.4817813765182186 on epoch=79
05/23/2022 05:15:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.004087 on epoch=79
05/23/2022 05:15:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.007418 on epoch=80
05/23/2022 05:15:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.092074 on epoch=81
05/23/2022 05:15:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.015978 on epoch=82
05/23/2022 05:15:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.013315 on epoch=83
05/23/2022 05:15:40 - INFO - __main__ - Global step 1000 Train loss 0.026574 Classification-F1 0.4859437751004016 on epoch=83
05/23/2022 05:15:40 - INFO - __main__ - save last model!
05/23/2022 05:15:47 - INFO - __main__ - Loading checkpoint on the fly
05/23/2022 05:15:48 - INFO - __main__ - Start tokenizing ... 87 instances
05/23/2022 05:15:48 - INFO - __main__ - Printing 3 examples
05/23/2022 05:15:48 - INFO - __main__ -  [ethos-race] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
05/23/2022 05:15:48 - INFO - __main__ - ['true']
05/23/2022 05:15:48 - INFO - __main__ -  [ethos-race] Looks like the gay poster boy with syphilis
05/23/2022 05:15:48 - INFO - __main__ - ['false']
05/23/2022 05:15:48 - INFO - __main__ -  [ethos-race] Jewish piece of shits like him should get hanged
05/23/2022 05:15:48 - INFO - __main__ - ['false']
05/23/2022 05:15:48 - INFO - __main__ - Tokenizing Input ...
05/23/2022 05:15:48 - INFO - __main__ - Tokenizing Output ...
05/23/2022 05:15:48 - INFO - __main__ - Loaded 87 examples from test data
05/23/2022 05:15:50 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-ethos-race/ethos-race_128_87_0.0001_8_predictions.txt
05/23/2022 05:15:50 - INFO - __main__ - Classification-F1 on test data: 0.8468
05/23/2022 05:15:50 - INFO - __main__ - prefix=ethos-race_128_87, lr=0.0001, bsz=8, dev_performance=0.49407114624505927, test_performance=0.846830985915493
