05/22/2022 11:17:28 - INFO - __main__ - Namespace(task_dir='data_128/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-ft-cls2cls-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/22/2022 11:17:28 - INFO - __main__ - models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity
05/22/2022 11:17:28 - INFO - __main__ - Namespace(task_dir='data_128/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-ft-cls2cls-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/22/2022 11:17:28 - INFO - __main__ - models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity
05/22/2022 11:17:30 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/22/2022 11:17:30 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/22/2022 11:17:30 - INFO - __main__ - args.device: cuda:0
05/22/2022 11:17:30 - INFO - __main__ - Using 2 gpus
05/22/2022 11:17:30 - INFO - __main__ - args.device: cuda:1
05/22/2022 11:17:30 - INFO - __main__ - Using 2 gpus
05/22/2022 11:17:30 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_128_100', 'amazon_polarity_128_13', 'amazon_polarity_128_21', 'amazon_polarity_128_42', 'amazon_polarity_128_87']
05/22/2022 11:17:30 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_128_100', 'amazon_polarity_128_13', 'amazon_polarity_128_21', 'amazon_polarity_128_42', 'amazon_polarity_128_87']
05/22/2022 11:17:34 - INFO - __main__ - Running ... prefix=amazon_polarity_128_100, lr=0.0005, bsz=8 ...
05/22/2022 11:17:35 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 11:17:35 - INFO - __main__ - Printing 3 examples
05/22/2022 11:17:35 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/22/2022 11:17:35 - INFO - __main__ - ['positive']
05/22/2022 11:17:35 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/22/2022 11:17:35 - INFO - __main__ - ['positive']
05/22/2022 11:17:35 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/22/2022 11:17:35 - INFO - __main__ - ['positive']
05/22/2022 11:17:35 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 11:17:35 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:17:35 - INFO - __main__ - Printing 3 examples
05/22/2022 11:17:35 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/22/2022 11:17:35 - INFO - __main__ - ['positive']
05/22/2022 11:17:35 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/22/2022 11:17:35 - INFO - __main__ - ['positive']
05/22/2022 11:17:35 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/22/2022 11:17:35 - INFO - __main__ - ['positive']
05/22/2022 11:17:35 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:17:35 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:17:35 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:17:36 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 11:17:36 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 11:17:36 - INFO - __main__ - Printing 3 examples
05/22/2022 11:17:36 - INFO - __main__ -  [amazon_polarity] title: Lowest cost organizer with keyboard and PC interface cable [SEP] content: Yes, the PC interface cable and (slow) backup and address entry software comes with the unit!The To Do list nicely sorts entries by alphanumeric priority, so you can organize by categories like B for Buy or M for Movies, as well as use priority levels 1 thru 9.Fits nicely in your pocket, with a keyboard which is quite usable for typical short entries.The calendar's monthly display shows morning and afternoon appointments at a glance.I'm completing software to speed up the slow backup and to allow importing and exporting entries. Email me if you would like to test and comment on the utility.
05/22/2022 11:17:36 - INFO - __main__ - ['positive']
05/22/2022 11:17:36 - INFO - __main__ -  [amazon_polarity] title: Listen Carefully! [SEP] content: Whoever thinks "Deep End" is about suicide should really listen carefully to the lyrics. Whoever thinks MadChild doesn't listen to bars should consider that maybe MadChild doesn't give a [damn] and considers that his style. I love Bad Dreams as well as every one of my Swollen CD's. Each MC's style is unique: Prevail is brainy. MadChild uses ancient and fantasy based ideas. Bad Dreams is the cd that you pop into the cd player in your car during a long midnight highway drive. Excellent.
05/22/2022 11:17:36 - INFO - __main__ - ['positive']
05/22/2022 11:17:36 - INFO - __main__ -  [amazon_polarity] title: Great Product.... [SEP] content: I really enjoy it..came just in time... and sorry it taken me so long to review it..
05/22/2022 11:17:36 - INFO - __main__ - ['positive']
05/22/2022 11:17:36 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:17:36 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 11:17:36 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 11:17:36 - INFO - __main__ - Printing 3 examples
05/22/2022 11:17:36 - INFO - __main__ -  [amazon_polarity] title: Lowest cost organizer with keyboard and PC interface cable [SEP] content: Yes, the PC interface cable and (slow) backup and address entry software comes with the unit!The To Do list nicely sorts entries by alphanumeric priority, so you can organize by categories like B for Buy or M for Movies, as well as use priority levels 1 thru 9.Fits nicely in your pocket, with a keyboard which is quite usable for typical short entries.The calendar's monthly display shows morning and afternoon appointments at a glance.I'm completing software to speed up the slow backup and to allow importing and exporting entries. Email me if you would like to test and comment on the utility.
05/22/2022 11:17:36 - INFO - __main__ - ['positive']
05/22/2022 11:17:36 - INFO - __main__ -  [amazon_polarity] title: Listen Carefully! [SEP] content: Whoever thinks "Deep End" is about suicide should really listen carefully to the lyrics. Whoever thinks MadChild doesn't listen to bars should consider that maybe MadChild doesn't give a [damn] and considers that his style. I love Bad Dreams as well as every one of my Swollen CD's. Each MC's style is unique: Prevail is brainy. MadChild uses ancient and fantasy based ideas. Bad Dreams is the cd that you pop into the cd player in your car during a long midnight highway drive. Excellent.
05/22/2022 11:17:36 - INFO - __main__ - ['positive']
05/22/2022 11:17:36 - INFO - __main__ -  [amazon_polarity] title: Great Product.... [SEP] content: I really enjoy it..came just in time... and sorry it taken me so long to review it..
05/22/2022 11:17:36 - INFO - __main__ - ['positive']
05/22/2022 11:17:36 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:17:36 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:17:36 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:17:36 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 11:17:36 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 11:17:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 11:17:49 - INFO - __main__ - Starting training!
05/22/2022 11:17:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 11:17:49 - INFO - __main__ - Starting training!
05/22/2022 11:17:55 - INFO - __main__ - Step 10 Global step 10 Train loss 22.988415 on epoch=0
05/22/2022 11:17:59 - INFO - __main__ - Step 20 Global step 20 Train loss 19.460871 on epoch=1
05/22/2022 11:18:04 - INFO - __main__ - Step 30 Global step 30 Train loss 16.553177 on epoch=1
05/22/2022 11:18:09 - INFO - __main__ - Step 40 Global step 40 Train loss 13.737552 on epoch=2
05/22/2022 11:18:14 - INFO - __main__ - Step 50 Global step 50 Train loss 11.652291 on epoch=3
05/22/2022 11:18:50 - INFO - __main__ - Global step 50 Train loss 16.878460 Classification-F1 0.0 on epoch=3
05/22/2022 11:18:57 - INFO - __main__ - Step 60 Global step 60 Train loss 7.221078 on epoch=3
05/22/2022 11:19:02 - INFO - __main__ - Step 70 Global step 70 Train loss 0.841426 on epoch=4
05/22/2022 11:19:07 - INFO - __main__ - Step 80 Global step 80 Train loss 0.222966 on epoch=4
05/22/2022 11:19:12 - INFO - __main__ - Step 90 Global step 90 Train loss 0.198627 on epoch=5
05/22/2022 11:19:17 - INFO - __main__ - Step 100 Global step 100 Train loss 0.187042 on epoch=6
05/22/2022 11:19:21 - INFO - __main__ - Global step 100 Train loss 1.734228 Classification-F1 0.9570305943389029 on epoch=6
05/22/2022 11:19:28 - INFO - __main__ - Step 110 Global step 110 Train loss 0.128699 on epoch=6
05/22/2022 11:19:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.157198 on epoch=7
05/22/2022 11:19:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.204215 on epoch=8
05/22/2022 11:19:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.049535 on epoch=8
05/22/2022 11:19:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.134014 on epoch=9
05/22/2022 11:19:53 - INFO - __main__ - Global step 150 Train loss 0.134732 Classification-F1 0.9648432135500115 on epoch=9
05/22/2022 11:19:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.135061 on epoch=9
05/22/2022 11:20:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.041530 on epoch=10
05/22/2022 11:20:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.056288 on epoch=11
05/22/2022 11:20:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.195544 on epoch=11
05/22/2022 11:20:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.017330 on epoch=12
05/22/2022 11:20:24 - INFO - __main__ - Global step 200 Train loss 0.089151 Classification-F1 0.9648174446836777 on epoch=12
05/22/2022 11:20:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.024239 on epoch=13
05/22/2022 11:20:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.047857 on epoch=13
05/22/2022 11:20:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.044708 on epoch=14
05/22/2022 11:20:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.026209 on epoch=14
05/22/2022 11:20:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.075951 on epoch=15
05/22/2022 11:20:53 - INFO - __main__ - Global step 250 Train loss 0.043793 Classification-F1 0.96875 on epoch=15
05/22/2022 11:20:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.086078 on epoch=16
05/22/2022 11:21:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.014898 on epoch=16
05/22/2022 11:21:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.005550 on epoch=17
05/22/2022 11:21:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.010350 on epoch=18
05/22/2022 11:21:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.025603 on epoch=18
05/22/2022 11:21:24 - INFO - __main__ - Global step 300 Train loss 0.028496 Classification-F1 0.9726558327611201 on epoch=18
05/22/2022 11:21:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.002527 on epoch=19
05/22/2022 11:21:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.011601 on epoch=19
05/22/2022 11:21:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000798 on epoch=20
05/22/2022 11:21:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000755 on epoch=21
05/22/2022 11:21:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.038709 on epoch=21
05/22/2022 11:21:54 - INFO - __main__ - Global step 350 Train loss 0.010878 Classification-F1 0.9765567765567766 on epoch=21
05/22/2022 11:22:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001742 on epoch=22
05/22/2022 11:22:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001055 on epoch=23
05/22/2022 11:22:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.004541 on epoch=23
05/22/2022 11:22:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.127057 on epoch=24
05/22/2022 11:22:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.041473 on epoch=24
05/22/2022 11:22:24 - INFO - __main__ - Global step 400 Train loss 0.035173 Classification-F1 0.9765610694012086 on epoch=24
05/22/2022 11:22:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.002775 on epoch=25
05/22/2022 11:22:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.003087 on epoch=26
05/22/2022 11:22:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.008547 on epoch=26
05/22/2022 11:22:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.003269 on epoch=27
05/22/2022 11:22:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.004619 on epoch=28
05/22/2022 11:22:55 - INFO - __main__ - Global step 450 Train loss 0.004460 Classification-F1 0.9373623684854416 on epoch=28
05/22/2022 11:23:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001013 on epoch=28
05/22/2022 11:23:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.033209 on epoch=29
05/22/2022 11:23:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.005153 on epoch=29
05/22/2022 11:23:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000673 on epoch=30
05/22/2022 11:23:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.047487 on epoch=31
05/22/2022 11:23:24 - INFO - __main__ - Global step 500 Train loss 0.017507 Classification-F1 0.9569990990578283 on epoch=31
05/22/2022 11:23:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.003813 on epoch=31
05/22/2022 11:23:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001379 on epoch=32
05/22/2022 11:23:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001383 on epoch=33
05/22/2022 11:23:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000118 on epoch=33
05/22/2022 11:23:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000128 on epoch=34
05/22/2022 11:23:54 - INFO - __main__ - Global step 550 Train loss 0.001364 Classification-F1 0.9687480925349448 on epoch=34
05/22/2022 11:23:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000233 on epoch=34
05/22/2022 11:24:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000199 on epoch=35
05/22/2022 11:24:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000260 on epoch=36
05/22/2022 11:24:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000552 on epoch=36
05/22/2022 11:24:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000422 on epoch=37
05/22/2022 11:24:23 - INFO - __main__ - Global step 600 Train loss 0.000333 Classification-F1 0.9452590420332356 on epoch=37
05/22/2022 11:24:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000142 on epoch=38
05/22/2022 11:24:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.003180 on epoch=38
05/22/2022 11:24:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000514 on epoch=39
05/22/2022 11:24:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000365 on epoch=39
05/22/2022 11:24:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001087 on epoch=40
05/22/2022 11:24:53 - INFO - __main__ - Global step 650 Train loss 0.001058 Classification-F1 0.960935115668681 on epoch=40
05/22/2022 11:24:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000518 on epoch=41
05/22/2022 11:25:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000254 on epoch=41
05/22/2022 11:25:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000212 on epoch=42
05/22/2022 11:25:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000257 on epoch=43
05/22/2022 11:25:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.008703 on epoch=43
05/22/2022 11:25:22 - INFO - __main__ - Global step 700 Train loss 0.001989 Classification-F1 0.9253701572688914 on epoch=43
05/22/2022 11:25:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000513 on epoch=44
05/22/2022 11:25:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000751 on epoch=44
05/22/2022 11:25:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000155 on epoch=45
05/22/2022 11:25:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000363 on epoch=46
05/22/2022 11:25:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000338 on epoch=46
05/22/2022 11:25:52 - INFO - __main__ - Global step 750 Train loss 0.000424 Classification-F1 0.9687328244274809 on epoch=46
05/22/2022 11:25:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000049 on epoch=47
05/22/2022 11:26:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000071 on epoch=48
05/22/2022 11:26:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000047 on epoch=48
05/22/2022 11:26:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000365 on epoch=49
05/22/2022 11:26:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000084 on epoch=49
05/22/2022 11:26:22 - INFO - __main__ - Global step 800 Train loss 0.000123 Classification-F1 0.9726458152066064 on epoch=49
05/22/2022 11:26:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000144 on epoch=50
05/22/2022 11:26:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000057 on epoch=51
05/22/2022 11:26:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000092 on epoch=51
05/22/2022 11:26:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000018 on epoch=52
05/22/2022 11:26:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000076 on epoch=53
05/22/2022 11:26:51 - INFO - __main__ - Global step 850 Train loss 0.000078 Classification-F1 0.9687423687423687 on epoch=53
05/22/2022 11:26:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000023 on epoch=53
05/22/2022 11:27:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000025 on epoch=54
05/22/2022 11:27:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000314 on epoch=54
05/22/2022 11:27:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000024 on epoch=55
05/22/2022 11:27:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000054 on epoch=56
05/22/2022 11:27:21 - INFO - __main__ - Global step 900 Train loss 0.000088 Classification-F1 0.9804684519722286 on epoch=56
05/22/2022 11:27:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000012 on epoch=56
05/22/2022 11:27:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.037479 on epoch=57
05/22/2022 11:27:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000235 on epoch=58
05/22/2022 11:27:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.042792 on epoch=58
05/22/2022 11:27:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.169480 on epoch=59
05/22/2022 11:27:51 - INFO - __main__ - Global step 950 Train loss 0.050000 Classification-F1 0.9726524943916248 on epoch=59
05/22/2022 11:27:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.066448 on epoch=59
05/22/2022 11:28:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.010432 on epoch=60
05/22/2022 11:28:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.001166 on epoch=61
05/22/2022 11:28:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000351 on epoch=61
05/22/2022 11:28:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000098 on epoch=62
05/22/2022 11:28:18 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 11:28:18 - INFO - __main__ - Printing 3 examples
05/22/2022 11:28:18 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/22/2022 11:28:18 - INFO - __main__ - ['positive']
05/22/2022 11:28:18 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/22/2022 11:28:18 - INFO - __main__ - ['positive']
05/22/2022 11:28:18 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/22/2022 11:28:18 - INFO - __main__ - ['positive']
05/22/2022 11:28:18 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:28:18 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:28:18 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 11:28:18 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 11:28:18 - INFO - __main__ - Printing 3 examples
05/22/2022 11:28:18 - INFO - __main__ -  [amazon_polarity] title: Lowest cost organizer with keyboard and PC interface cable [SEP] content: Yes, the PC interface cable and (slow) backup and address entry software comes with the unit!The To Do list nicely sorts entries by alphanumeric priority, so you can organize by categories like B for Buy or M for Movies, as well as use priority levels 1 thru 9.Fits nicely in your pocket, with a keyboard which is quite usable for typical short entries.The calendar's monthly display shows morning and afternoon appointments at a glance.I'm completing software to speed up the slow backup and to allow importing and exporting entries. Email me if you would like to test and comment on the utility.
05/22/2022 11:28:18 - INFO - __main__ - ['positive']
05/22/2022 11:28:18 - INFO - __main__ -  [amazon_polarity] title: Listen Carefully! [SEP] content: Whoever thinks "Deep End" is about suicide should really listen carefully to the lyrics. Whoever thinks MadChild doesn't listen to bars should consider that maybe MadChild doesn't give a [damn] and considers that his style. I love Bad Dreams as well as every one of my Swollen CD's. Each MC's style is unique: Prevail is brainy. MadChild uses ancient and fantasy based ideas. Bad Dreams is the cd that you pop into the cd player in your car during a long midnight highway drive. Excellent.
05/22/2022 11:28:18 - INFO - __main__ - ['positive']
05/22/2022 11:28:18 - INFO - __main__ -  [amazon_polarity] title: Great Product.... [SEP] content: I really enjoy it..came just in time... and sorry it taken me so long to review it..
05/22/2022 11:28:18 - INFO - __main__ - ['positive']
05/22/2022 11:28:18 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:28:19 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:28:19 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 11:28:21 - INFO - __main__ - Global step 1000 Train loss 0.015699 Classification-F1 0.9726524943916248 on epoch=62
05/22/2022 11:28:21 - INFO - __main__ - save last model!
05/22/2022 11:28:28 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 11:28:29 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 11:28:29 - INFO - __main__ - Printing 3 examples
05/22/2022 11:28:29 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 11:28:29 - INFO - __main__ - ['negative']
05/22/2022 11:28:29 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 11:28:29 - INFO - __main__ - ['negative']
05/22/2022 11:28:29 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 11:28:29 - INFO - __main__ - ['negative']
05/22/2022 11:28:29 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:28:29 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:28:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 11:28:30 - INFO - __main__ - Starting training!
05/22/2022 11:28:30 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 11:28:45 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_100_0.0005_8_predictions.txt
05/22/2022 11:28:45 - INFO - __main__ - Classification-F1 on test data: 0.9620
05/22/2022 11:28:46 - INFO - __main__ - prefix=amazon_polarity_128_100, lr=0.0005, bsz=8, dev_performance=0.9804684519722286, test_performance=0.9619975678443421
05/22/2022 11:28:46 - INFO - __main__ - Running ... prefix=amazon_polarity_128_100, lr=0.0003, bsz=8 ...
05/22/2022 11:28:47 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 11:28:47 - INFO - __main__ - Printing 3 examples
05/22/2022 11:28:47 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/22/2022 11:28:47 - INFO - __main__ - ['positive']
05/22/2022 11:28:47 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/22/2022 11:28:47 - INFO - __main__ - ['positive']
05/22/2022 11:28:47 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/22/2022 11:28:47 - INFO - __main__ - ['positive']
05/22/2022 11:28:47 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:28:47 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:28:47 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 11:28:47 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 11:28:47 - INFO - __main__ - Printing 3 examples
05/22/2022 11:28:47 - INFO - __main__ -  [amazon_polarity] title: Lowest cost organizer with keyboard and PC interface cable [SEP] content: Yes, the PC interface cable and (slow) backup and address entry software comes with the unit!The To Do list nicely sorts entries by alphanumeric priority, so you can organize by categories like B for Buy or M for Movies, as well as use priority levels 1 thru 9.Fits nicely in your pocket, with a keyboard which is quite usable for typical short entries.The calendar's monthly display shows morning and afternoon appointments at a glance.I'm completing software to speed up the slow backup and to allow importing and exporting entries. Email me if you would like to test and comment on the utility.
05/22/2022 11:28:47 - INFO - __main__ - ['positive']
05/22/2022 11:28:47 - INFO - __main__ -  [amazon_polarity] title: Listen Carefully! [SEP] content: Whoever thinks "Deep End" is about suicide should really listen carefully to the lyrics. Whoever thinks MadChild doesn't listen to bars should consider that maybe MadChild doesn't give a [damn] and considers that his style. I love Bad Dreams as well as every one of my Swollen CD's. Each MC's style is unique: Prevail is brainy. MadChild uses ancient and fantasy based ideas. Bad Dreams is the cd that you pop into the cd player in your car during a long midnight highway drive. Excellent.
05/22/2022 11:28:47 - INFO - __main__ - ['positive']
05/22/2022 11:28:47 - INFO - __main__ -  [amazon_polarity] title: Great Product.... [SEP] content: I really enjoy it..came just in time... and sorry it taken me so long to review it..
05/22/2022 11:28:47 - INFO - __main__ - ['positive']
05/22/2022 11:28:47 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:28:47 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:28:47 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 11:28:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 11:28:58 - INFO - __main__ - Starting training!
05/22/2022 11:29:03 - INFO - __main__ - Step 10 Global step 10 Train loss 23.398401 on epoch=0
05/22/2022 11:29:08 - INFO - __main__ - Step 20 Global step 20 Train loss 16.890875 on epoch=1
05/22/2022 11:29:13 - INFO - __main__ - Step 30 Global step 30 Train loss 16.476099 on epoch=1
05/22/2022 11:29:18 - INFO - __main__ - Step 40 Global step 40 Train loss 14.891851 on epoch=2
05/22/2022 11:29:23 - INFO - __main__ - Step 50 Global step 50 Train loss 13.802858 on epoch=3
05/22/2022 11:29:47 - INFO - __main__ - Global step 50 Train loss 17.092016 Classification-F1 0.0 on epoch=3
05/22/2022 11:29:53 - INFO - __main__ - Step 60 Global step 60 Train loss 12.522535 on epoch=3
05/22/2022 11:29:58 - INFO - __main__ - Step 70 Global step 70 Train loss 11.693800 on epoch=4
05/22/2022 11:30:03 - INFO - __main__ - Step 80 Global step 80 Train loss 10.286260 on epoch=4
05/22/2022 11:30:08 - INFO - __main__ - Step 90 Global step 90 Train loss 8.530597 on epoch=5
05/22/2022 11:30:13 - INFO - __main__ - Step 100 Global step 100 Train loss 4.287174 on epoch=6
05/22/2022 11:30:17 - INFO - __main__ - Global step 100 Train loss 9.464073 Classification-F1 0.4544122544122544 on epoch=6
05/22/2022 11:30:24 - INFO - __main__ - Step 110 Global step 110 Train loss 2.926594 on epoch=6
05/22/2022 11:30:29 - INFO - __main__ - Step 120 Global step 120 Train loss 1.082360 on epoch=7
05/22/2022 11:30:34 - INFO - __main__ - Step 130 Global step 130 Train loss 1.474342 on epoch=8
05/22/2022 11:30:39 - INFO - __main__ - Step 140 Global step 140 Train loss 1.063596 on epoch=8
05/22/2022 11:30:44 - INFO - __main__ - Step 150 Global step 150 Train loss 2.444153 on epoch=9
05/22/2022 11:30:48 - INFO - __main__ - Global step 150 Train loss 1.798209 Classification-F1 0.3333333333333333 on epoch=9
05/22/2022 11:30:53 - INFO - __main__ - Step 160 Global step 160 Train loss 1.422108 on epoch=9
05/22/2022 11:30:59 - INFO - __main__ - Step 170 Global step 170 Train loss 1.720471 on epoch=10
05/22/2022 11:31:04 - INFO - __main__ - Step 180 Global step 180 Train loss 1.415945 on epoch=11
05/22/2022 11:31:09 - INFO - __main__ - Step 190 Global step 190 Train loss 1.760902 on epoch=11
05/22/2022 11:31:14 - INFO - __main__ - Step 200 Global step 200 Train loss 1.537465 on epoch=12
05/22/2022 11:31:18 - INFO - __main__ - Global step 200 Train loss 1.571378 Classification-F1 0.3401530406766009 on epoch=12
05/22/2022 11:31:23 - INFO - __main__ - Step 210 Global step 210 Train loss 1.417868 on epoch=13
05/22/2022 11:31:28 - INFO - __main__ - Step 220 Global step 220 Train loss 1.118994 on epoch=13
05/22/2022 11:31:33 - INFO - __main__ - Step 230 Global step 230 Train loss 1.508189 on epoch=14
05/22/2022 11:31:38 - INFO - __main__ - Step 240 Global step 240 Train loss 1.599801 on epoch=14
05/22/2022 11:31:44 - INFO - __main__ - Step 250 Global step 250 Train loss 1.168912 on epoch=15
05/22/2022 11:31:47 - INFO - __main__ - Global step 250 Train loss 1.362753 Classification-F1 0.3333333333333333 on epoch=15
05/22/2022 11:31:53 - INFO - __main__ - Step 260 Global step 260 Train loss 1.420479 on epoch=16
05/22/2022 11:31:57 - INFO - __main__ - Step 270 Global step 270 Train loss 1.330381 on epoch=16
05/22/2022 11:32:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.709223 on epoch=17
05/22/2022 11:32:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.451689 on epoch=18
05/22/2022 11:32:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.474315 on epoch=18
05/22/2022 11:32:17 - INFO - __main__ - Global step 300 Train loss 0.877217 Classification-F1 0.3333333333333333 on epoch=18
05/22/2022 11:32:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.354940 on epoch=19
05/22/2022 11:32:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.384591 on epoch=19
05/22/2022 11:32:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.370588 on epoch=20
05/22/2022 11:32:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.425036 on epoch=21
05/22/2022 11:32:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.385371 on epoch=21
05/22/2022 11:32:46 - INFO - __main__ - Global step 350 Train loss 0.384105 Classification-F1 0.3813144709696433 on epoch=21
05/22/2022 11:32:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.414007 on epoch=22
05/22/2022 11:32:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.417297 on epoch=23
05/22/2022 11:33:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.341273 on epoch=23
05/22/2022 11:33:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.354005 on epoch=24
05/22/2022 11:33:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.375455 on epoch=24
05/22/2022 11:33:16 - INFO - __main__ - Global step 400 Train loss 0.380407 Classification-F1 0.3333333333333333 on epoch=24
05/22/2022 11:33:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.368638 on epoch=25
05/22/2022 11:33:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.407159 on epoch=26
05/22/2022 11:33:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.424227 on epoch=26
05/22/2022 11:33:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.384049 on epoch=27
05/22/2022 11:33:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.384196 on epoch=28
05/22/2022 11:33:45 - INFO - __main__ - Global step 450 Train loss 0.393654 Classification-F1 0.3333333333333333 on epoch=28
05/22/2022 11:33:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.345524 on epoch=28
05/22/2022 11:33:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.370180 on epoch=29
05/22/2022 11:34:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.376712 on epoch=29
05/22/2022 11:34:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.373632 on epoch=30
05/22/2022 11:34:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.366967 on epoch=31
05/22/2022 11:34:15 - INFO - __main__ - Global step 500 Train loss 0.366603 Classification-F1 0.3333333333333333 on epoch=31
05/22/2022 11:34:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.356850 on epoch=31
05/22/2022 11:34:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.368150 on epoch=32
05/22/2022 11:34:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.403470 on epoch=33
05/22/2022 11:34:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.340312 on epoch=33
05/22/2022 11:34:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.362839 on epoch=34
05/22/2022 11:34:45 - INFO - __main__ - Global step 550 Train loss 0.366324 Classification-F1 0.46562699466938084 on epoch=34
05/22/2022 11:34:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.380629 on epoch=34
05/22/2022 11:34:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.388084 on epoch=35
05/22/2022 11:35:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.332759 on epoch=36
05/22/2022 11:35:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.362918 on epoch=36
05/22/2022 11:35:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.368382 on epoch=37
05/22/2022 11:35:16 - INFO - __main__ - Global step 600 Train loss 0.366555 Classification-F1 0.3771289537712896 on epoch=37
05/22/2022 11:35:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.380966 on epoch=38
05/22/2022 11:35:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.352588 on epoch=38
05/22/2022 11:35:31 - INFO - __main__ - Step 630 Global step 630 Train loss 0.374255 on epoch=39
05/22/2022 11:35:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.354155 on epoch=39
05/22/2022 11:35:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.390198 on epoch=40
05/22/2022 11:35:45 - INFO - __main__ - Global step 650 Train loss 0.370432 Classification-F1 0.7069125564700787 on epoch=40
05/22/2022 11:35:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.368432 on epoch=41
05/22/2022 11:35:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.340537 on epoch=41
05/22/2022 11:36:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.385147 on epoch=42
05/22/2022 11:36:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.400647 on epoch=43
05/22/2022 11:36:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.624479 on epoch=43
05/22/2022 11:36:17 - INFO - __main__ - Global step 700 Train loss 0.423848 Classification-F1 0.3401530406766009 on epoch=43
05/22/2022 11:36:22 - INFO - __main__ - Step 710 Global step 710 Train loss 1.339063 on epoch=44
05/22/2022 11:36:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.984442 on epoch=44
05/22/2022 11:36:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.756654 on epoch=45
05/22/2022 11:36:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.942304 on epoch=46
05/22/2022 11:36:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.682901 on epoch=46
05/22/2022 11:36:46 - INFO - __main__ - Global step 750 Train loss 0.941073 Classification-F1 0.3401530406766009 on epoch=46
05/22/2022 11:36:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.860247 on epoch=47
05/22/2022 11:36:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.559002 on epoch=48
05/22/2022 11:37:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.546765 on epoch=48
05/22/2022 11:37:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.624698 on epoch=49
05/22/2022 11:37:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.450513 on epoch=49
05/22/2022 11:37:16 - INFO - __main__ - Global step 800 Train loss 0.608245 Classification-F1 0.4782235571765716 on epoch=49
05/22/2022 11:37:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.541774 on epoch=50
05/22/2022 11:37:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.471172 on epoch=51
05/22/2022 11:37:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.517175 on epoch=51
05/22/2022 11:37:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.505679 on epoch=52
05/22/2022 11:37:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.378114 on epoch=53
05/22/2022 11:37:45 - INFO - __main__ - Global step 850 Train loss 0.482783 Classification-F1 0.35693779904306216 on epoch=53
05/22/2022 11:37:50 - INFO - __main__ - Step 860 Global step 860 Train loss 0.442058 on epoch=53
05/22/2022 11:37:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.463405 on epoch=54
05/22/2022 11:38:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.434437 on epoch=54
05/22/2022 11:38:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.432333 on epoch=55
05/22/2022 11:38:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.431928 on epoch=56
05/22/2022 11:38:15 - INFO - __main__ - Global step 900 Train loss 0.440832 Classification-F1 0.604109568066635 on epoch=56
05/22/2022 11:38:20 - INFO - __main__ - Step 910 Global step 910 Train loss 0.427253 on epoch=56
05/22/2022 11:38:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.445335 on epoch=57
05/22/2022 11:38:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.409391 on epoch=58
05/22/2022 11:38:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.414459 on epoch=58
05/22/2022 11:38:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.365334 on epoch=59
05/22/2022 11:38:44 - INFO - __main__ - Global step 950 Train loss 0.412354 Classification-F1 0.5681181563534505 on epoch=59
05/22/2022 11:38:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.387437 on epoch=59
05/22/2022 11:38:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.404378 on epoch=60
05/22/2022 11:38:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.412467 on epoch=61
05/22/2022 11:39:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.351866 on epoch=61
05/22/2022 11:39:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.396539 on epoch=62
05/22/2022 11:39:11 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 11:39:11 - INFO - __main__ - Printing 3 examples
05/22/2022 11:39:11 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/22/2022 11:39:11 - INFO - __main__ - ['positive']
05/22/2022 11:39:11 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/22/2022 11:39:11 - INFO - __main__ - ['positive']
05/22/2022 11:39:11 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/22/2022 11:39:11 - INFO - __main__ - ['positive']
05/22/2022 11:39:11 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:39:11 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:39:11 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 11:39:11 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 11:39:11 - INFO - __main__ - Printing 3 examples
05/22/2022 11:39:11 - INFO - __main__ -  [amazon_polarity] title: Lowest cost organizer with keyboard and PC interface cable [SEP] content: Yes, the PC interface cable and (slow) backup and address entry software comes with the unit!The To Do list nicely sorts entries by alphanumeric priority, so you can organize by categories like B for Buy or M for Movies, as well as use priority levels 1 thru 9.Fits nicely in your pocket, with a keyboard which is quite usable for typical short entries.The calendar's monthly display shows morning and afternoon appointments at a glance.I'm completing software to speed up the slow backup and to allow importing and exporting entries. Email me if you would like to test and comment on the utility.
05/22/2022 11:39:11 - INFO - __main__ - ['positive']
05/22/2022 11:39:11 - INFO - __main__ -  [amazon_polarity] title: Listen Carefully! [SEP] content: Whoever thinks "Deep End" is about suicide should really listen carefully to the lyrics. Whoever thinks MadChild doesn't listen to bars should consider that maybe MadChild doesn't give a [damn] and considers that his style. I love Bad Dreams as well as every one of my Swollen CD's. Each MC's style is unique: Prevail is brainy. MadChild uses ancient and fantasy based ideas. Bad Dreams is the cd that you pop into the cd player in your car during a long midnight highway drive. Excellent.
05/22/2022 11:39:11 - INFO - __main__ - ['positive']
05/22/2022 11:39:11 - INFO - __main__ -  [amazon_polarity] title: Great Product.... [SEP] content: I really enjoy it..came just in time... and sorry it taken me so long to review it..
05/22/2022 11:39:11 - INFO - __main__ - ['positive']
05/22/2022 11:39:11 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:39:11 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:39:12 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 11:39:13 - INFO - __main__ - Global step 1000 Train loss 0.390537 Classification-F1 0.5928894207302673 on epoch=62
05/22/2022 11:39:13 - INFO - __main__ - save last model!
05/22/2022 11:39:21 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 11:39:21 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 11:39:21 - INFO - __main__ - Printing 3 examples
05/22/2022 11:39:21 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 11:39:21 - INFO - __main__ - ['negative']
05/22/2022 11:39:21 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 11:39:21 - INFO - __main__ - ['negative']
05/22/2022 11:39:21 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 11:39:21 - INFO - __main__ - ['negative']
05/22/2022 11:39:21 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:39:22 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:39:23 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 11:39:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 11:39:24 - INFO - __main__ - Starting training!
05/22/2022 11:39:38 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_100_0.0003_8_predictions.txt
05/22/2022 11:39:38 - INFO - __main__ - Classification-F1 on test data: 0.6528
05/22/2022 11:39:39 - INFO - __main__ - prefix=amazon_polarity_128_100, lr=0.0003, bsz=8, dev_performance=0.7069125564700787, test_performance=0.6528333328301932
05/22/2022 11:39:39 - INFO - __main__ - Running ... prefix=amazon_polarity_128_100, lr=0.0002, bsz=8 ...
05/22/2022 11:39:40 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 11:39:40 - INFO - __main__ - Printing 3 examples
05/22/2022 11:39:40 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/22/2022 11:39:40 - INFO - __main__ - ['positive']
05/22/2022 11:39:40 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/22/2022 11:39:40 - INFO - __main__ - ['positive']
05/22/2022 11:39:40 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/22/2022 11:39:40 - INFO - __main__ - ['positive']
05/22/2022 11:39:40 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:39:40 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:39:40 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 11:39:40 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 11:39:40 - INFO - __main__ - Printing 3 examples
05/22/2022 11:39:40 - INFO - __main__ -  [amazon_polarity] title: Lowest cost organizer with keyboard and PC interface cable [SEP] content: Yes, the PC interface cable and (slow) backup and address entry software comes with the unit!The To Do list nicely sorts entries by alphanumeric priority, so you can organize by categories like B for Buy or M for Movies, as well as use priority levels 1 thru 9.Fits nicely in your pocket, with a keyboard which is quite usable for typical short entries.The calendar's monthly display shows morning and afternoon appointments at a glance.I'm completing software to speed up the slow backup and to allow importing and exporting entries. Email me if you would like to test and comment on the utility.
05/22/2022 11:39:40 - INFO - __main__ - ['positive']
05/22/2022 11:39:40 - INFO - __main__ -  [amazon_polarity] title: Listen Carefully! [SEP] content: Whoever thinks "Deep End" is about suicide should really listen carefully to the lyrics. Whoever thinks MadChild doesn't listen to bars should consider that maybe MadChild doesn't give a [damn] and considers that his style. I love Bad Dreams as well as every one of my Swollen CD's. Each MC's style is unique: Prevail is brainy. MadChild uses ancient and fantasy based ideas. Bad Dreams is the cd that you pop into the cd player in your car during a long midnight highway drive. Excellent.
05/22/2022 11:39:40 - INFO - __main__ - ['positive']
05/22/2022 11:39:40 - INFO - __main__ -  [amazon_polarity] title: Great Product.... [SEP] content: I really enjoy it..came just in time... and sorry it taken me so long to review it..
05/22/2022 11:39:40 - INFO - __main__ - ['positive']
05/22/2022 11:39:40 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:39:40 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:39:40 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 11:39:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 11:39:53 - INFO - __main__ - Starting training!
05/22/2022 11:39:58 - INFO - __main__ - Step 10 Global step 10 Train loss 22.843876 on epoch=0
05/22/2022 11:40:03 - INFO - __main__ - Step 20 Global step 20 Train loss 19.285889 on epoch=1
05/22/2022 11:40:08 - INFO - __main__ - Step 30 Global step 30 Train loss 17.017172 on epoch=1
05/22/2022 11:40:13 - INFO - __main__ - Step 40 Global step 40 Train loss 15.886365 on epoch=2
05/22/2022 11:40:18 - INFO - __main__ - Step 50 Global step 50 Train loss 14.936014 on epoch=3
05/22/2022 11:40:41 - INFO - __main__ - Global step 50 Train loss 17.993864 Classification-F1 0.0 on epoch=3
05/22/2022 11:40:47 - INFO - __main__ - Step 60 Global step 60 Train loss 14.132948 on epoch=3
05/22/2022 11:40:52 - INFO - __main__ - Step 70 Global step 70 Train loss 13.020630 on epoch=4
05/22/2022 11:40:57 - INFO - __main__ - Step 80 Global step 80 Train loss 13.065516 on epoch=4
05/22/2022 11:41:02 - INFO - __main__ - Step 90 Global step 90 Train loss 11.977056 on epoch=5
05/22/2022 11:41:08 - INFO - __main__ - Step 100 Global step 100 Train loss 10.516057 on epoch=6
05/22/2022 11:41:19 - INFO - __main__ - Global step 100 Train loss 12.542441 Classification-F1 0.0 on epoch=6
05/22/2022 11:41:25 - INFO - __main__ - Step 110 Global step 110 Train loss 8.172470 on epoch=6
05/22/2022 11:41:30 - INFO - __main__ - Step 120 Global step 120 Train loss 3.852130 on epoch=7
05/22/2022 11:41:35 - INFO - __main__ - Step 130 Global step 130 Train loss 1.082480 on epoch=8
05/22/2022 11:41:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.557926 on epoch=8
05/22/2022 11:41:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.551810 on epoch=9
05/22/2022 11:41:49 - INFO - __main__ - Global step 150 Train loss 2.843363 Classification-F1 0.9765625 on epoch=9
05/22/2022 11:41:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.502984 on epoch=9
05/22/2022 11:42:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.821370 on epoch=10
05/22/2022 11:42:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.121509 on epoch=11
05/22/2022 11:42:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.156430 on epoch=11
05/22/2022 11:42:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.142651 on epoch=12
05/22/2022 11:42:20 - INFO - __main__ - Global step 200 Train loss 0.348989 Classification-F1 0.9726458152066064 on epoch=12
05/22/2022 11:42:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.177010 on epoch=13
05/22/2022 11:42:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.059963 on epoch=13
05/22/2022 11:42:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.085559 on epoch=14
05/22/2022 11:42:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.086673 on epoch=14
05/22/2022 11:42:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.049549 on epoch=15
05/22/2022 11:42:50 - INFO - __main__ - Global step 250 Train loss 0.091751 Classification-F1 0.9843740462674724 on epoch=15
05/22/2022 11:42:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.066793 on epoch=16
05/22/2022 11:43:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.050870 on epoch=16
05/22/2022 11:43:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.054910 on epoch=17
05/22/2022 11:43:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.079705 on epoch=18
05/22/2022 11:43:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.089845 on epoch=18
05/22/2022 11:43:20 - INFO - __main__ - Global step 300 Train loss 0.068425 Classification-F1 0.9648002444427469 on epoch=18
05/22/2022 11:43:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.135839 on epoch=19
05/22/2022 11:43:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.084224 on epoch=19
05/22/2022 11:43:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.012077 on epoch=20
05/22/2022 11:43:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.052726 on epoch=21
05/22/2022 11:43:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.021934 on epoch=21
05/22/2022 11:43:50 - INFO - __main__ - Global step 350 Train loss 0.061360 Classification-F1 0.9804660674225891 on epoch=21
05/22/2022 11:43:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.068409 on epoch=22
05/22/2022 11:44:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.055335 on epoch=23
05/22/2022 11:44:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.025647 on epoch=23
05/22/2022 11:44:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.050435 on epoch=24
05/22/2022 11:44:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.060945 on epoch=24
05/22/2022 11:44:20 - INFO - __main__ - Global step 400 Train loss 0.052154 Classification-F1 0.9843711843711844 on epoch=24
05/22/2022 11:44:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.016219 on epoch=25
05/22/2022 11:44:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.005388 on epoch=26
05/22/2022 11:44:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.050825 on epoch=26
05/22/2022 11:44:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.004448 on epoch=27
05/22/2022 11:44:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.011655 on epoch=28
05/22/2022 11:44:49 - INFO - __main__ - Global step 450 Train loss 0.017707 Classification-F1 0.9843711843711844 on epoch=28
05/22/2022 11:44:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.076919 on epoch=28
05/22/2022 11:45:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.045022 on epoch=29
05/22/2022 11:45:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.248955 on epoch=29
05/22/2022 11:45:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.204606 on epoch=30
05/22/2022 11:45:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.031719 on epoch=31
05/22/2022 11:45:19 - INFO - __main__ - Global step 500 Train loss 0.121444 Classification-F1 0.9804660674225891 on epoch=31
05/22/2022 11:45:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.107628 on epoch=31
05/22/2022 11:45:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.061170 on epoch=32
05/22/2022 11:45:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.017684 on epoch=33
05/22/2022 11:45:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.058980 on epoch=33
05/22/2022 11:45:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.004987 on epoch=34
05/22/2022 11:45:49 - INFO - __main__ - Global step 550 Train loss 0.050090 Classification-F1 0.984375 on epoch=34
05/22/2022 11:45:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.052355 on epoch=34
05/22/2022 11:46:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.078557 on epoch=35
05/22/2022 11:46:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.026461 on epoch=36
05/22/2022 11:46:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.139657 on epoch=36
05/22/2022 11:46:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.038886 on epoch=37
05/22/2022 11:46:20 - INFO - __main__ - Global step 600 Train loss 0.067183 Classification-F1 0.9726558327611201 on epoch=37
05/22/2022 11:46:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.012519 on epoch=38
05/22/2022 11:46:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.046737 on epoch=38
05/22/2022 11:46:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.001167 on epoch=39
05/22/2022 11:46:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.026402 on epoch=39
05/22/2022 11:46:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.088586 on epoch=40
05/22/2022 11:46:50 - INFO - __main__ - Global step 650 Train loss 0.035082 Classification-F1 0.9804660674225891 on epoch=40
05/22/2022 11:46:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.068998 on epoch=41
05/22/2022 11:47:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.036860 on epoch=41
05/22/2022 11:47:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.006916 on epoch=42
05/22/2022 11:47:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.004223 on epoch=43
05/22/2022 11:47:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.044372 on epoch=43
05/22/2022 11:47:19 - INFO - __main__ - Global step 700 Train loss 0.032274 Classification-F1 0.9765610694012086 on epoch=43
05/22/2022 11:47:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.006470 on epoch=44
05/22/2022 11:47:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.040105 on epoch=44
05/22/2022 11:47:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.011172 on epoch=45
05/22/2022 11:47:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.012803 on epoch=46
05/22/2022 11:47:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.009873 on epoch=46
05/22/2022 11:47:49 - INFO - __main__ - Global step 750 Train loss 0.016085 Classification-F1 0.9765610694012086 on epoch=46
05/22/2022 11:47:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.083633 on epoch=47
05/22/2022 11:47:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.044931 on epoch=48
05/22/2022 11:48:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.003184 on epoch=48
05/22/2022 11:48:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.005426 on epoch=49
05/22/2022 11:48:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000542 on epoch=49
05/22/2022 11:48:18 - INFO - __main__ - Global step 800 Train loss 0.027543 Classification-F1 0.9765610694012086 on epoch=49
05/22/2022 11:48:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.008339 on epoch=50
05/22/2022 11:48:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000139 on epoch=51
05/22/2022 11:48:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.005860 on epoch=51
05/22/2022 11:48:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.036450 on epoch=52
05/22/2022 11:48:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000955 on epoch=53
05/22/2022 11:48:48 - INFO - __main__ - Global step 850 Train loss 0.010349 Classification-F1 0.9804660674225891 on epoch=53
05/22/2022 11:48:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000329 on epoch=53
05/22/2022 11:48:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000536 on epoch=54
05/22/2022 11:49:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000856 on epoch=54
05/22/2022 11:49:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000326 on epoch=55
05/22/2022 11:49:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.002691 on epoch=56
05/22/2022 11:49:17 - INFO - __main__ - Global step 900 Train loss 0.000948 Classification-F1 0.984375 on epoch=56
05/22/2022 11:49:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000743 on epoch=56
05/22/2022 11:49:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000086 on epoch=57
05/22/2022 11:49:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000077 on epoch=58
05/22/2022 11:49:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.017295 on epoch=58
05/22/2022 11:49:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000050 on epoch=59
05/22/2022 11:49:46 - INFO - __main__ - Global step 950 Train loss 0.003650 Classification-F1 0.984375 on epoch=59
05/22/2022 11:49:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.001153 on epoch=59
05/22/2022 11:49:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000332 on epoch=60
05/22/2022 11:50:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000500 on epoch=61
05/22/2022 11:50:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000085 on epoch=61
05/22/2022 11:50:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000825 on epoch=62
05/22/2022 11:50:13 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 11:50:13 - INFO - __main__ - Printing 3 examples
05/22/2022 11:50:13 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/22/2022 11:50:13 - INFO - __main__ - ['positive']
05/22/2022 11:50:13 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/22/2022 11:50:13 - INFO - __main__ - ['positive']
05/22/2022 11:50:13 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/22/2022 11:50:13 - INFO - __main__ - ['positive']
05/22/2022 11:50:13 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:50:13 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:50:13 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 11:50:13 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 11:50:13 - INFO - __main__ - Printing 3 examples
05/22/2022 11:50:13 - INFO - __main__ -  [amazon_polarity] title: Lowest cost organizer with keyboard and PC interface cable [SEP] content: Yes, the PC interface cable and (slow) backup and address entry software comes with the unit!The To Do list nicely sorts entries by alphanumeric priority, so you can organize by categories like B for Buy or M for Movies, as well as use priority levels 1 thru 9.Fits nicely in your pocket, with a keyboard which is quite usable for typical short entries.The calendar's monthly display shows morning and afternoon appointments at a glance.I'm completing software to speed up the slow backup and to allow importing and exporting entries. Email me if you would like to test and comment on the utility.
05/22/2022 11:50:13 - INFO - __main__ - ['positive']
05/22/2022 11:50:13 - INFO - __main__ -  [amazon_polarity] title: Listen Carefully! [SEP] content: Whoever thinks "Deep End" is about suicide should really listen carefully to the lyrics. Whoever thinks MadChild doesn't listen to bars should consider that maybe MadChild doesn't give a [damn] and considers that his style. I love Bad Dreams as well as every one of my Swollen CD's. Each MC's style is unique: Prevail is brainy. MadChild uses ancient and fantasy based ideas. Bad Dreams is the cd that you pop into the cd player in your car during a long midnight highway drive. Excellent.
05/22/2022 11:50:13 - INFO - __main__ - ['positive']
05/22/2022 11:50:13 - INFO - __main__ -  [amazon_polarity] title: Great Product.... [SEP] content: I really enjoy it..came just in time... and sorry it taken me so long to review it..
05/22/2022 11:50:13 - INFO - __main__ - ['positive']
05/22/2022 11:50:13 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:50:14 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:50:14 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 11:50:16 - INFO - __main__ - Global step 1000 Train loss 0.000579 Classification-F1 0.9843740462674724 on epoch=62
05/22/2022 11:50:16 - INFO - __main__ - save last model!
05/22/2022 11:50:22 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 11:50:23 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 11:50:23 - INFO - __main__ - Printing 3 examples
05/22/2022 11:50:23 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 11:50:23 - INFO - __main__ - ['negative']
05/22/2022 11:50:23 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 11:50:23 - INFO - __main__ - ['negative']
05/22/2022 11:50:23 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 11:50:23 - INFO - __main__ - ['negative']
05/22/2022 11:50:23 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:50:24 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:50:25 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 11:50:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 11:50:25 - INFO - __main__ - Starting training!
05/22/2022 11:50:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_100_0.0002_8_predictions.txt
05/22/2022 11:50:40 - INFO - __main__ - Classification-F1 on test data: 0.9580
05/22/2022 11:50:40 - INFO - __main__ - prefix=amazon_polarity_128_100, lr=0.0002, bsz=8, dev_performance=0.984375, test_performance=0.9579863875895791
05/22/2022 11:50:40 - INFO - __main__ - Running ... prefix=amazon_polarity_128_100, lr=0.0001, bsz=8 ...
05/22/2022 11:50:41 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 11:50:41 - INFO - __main__ - Printing 3 examples
05/22/2022 11:50:41 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/22/2022 11:50:41 - INFO - __main__ - ['positive']
05/22/2022 11:50:41 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/22/2022 11:50:41 - INFO - __main__ - ['positive']
05/22/2022 11:50:41 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/22/2022 11:50:41 - INFO - __main__ - ['positive']
05/22/2022 11:50:41 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:50:41 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:50:41 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 11:50:41 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 11:50:41 - INFO - __main__ - Printing 3 examples
05/22/2022 11:50:41 - INFO - __main__ -  [amazon_polarity] title: Lowest cost organizer with keyboard and PC interface cable [SEP] content: Yes, the PC interface cable and (slow) backup and address entry software comes with the unit!The To Do list nicely sorts entries by alphanumeric priority, so you can organize by categories like B for Buy or M for Movies, as well as use priority levels 1 thru 9.Fits nicely in your pocket, with a keyboard which is quite usable for typical short entries.The calendar's monthly display shows morning and afternoon appointments at a glance.I'm completing software to speed up the slow backup and to allow importing and exporting entries. Email me if you would like to test and comment on the utility.
05/22/2022 11:50:41 - INFO - __main__ - ['positive']
05/22/2022 11:50:41 - INFO - __main__ -  [amazon_polarity] title: Listen Carefully! [SEP] content: Whoever thinks "Deep End" is about suicide should really listen carefully to the lyrics. Whoever thinks MadChild doesn't listen to bars should consider that maybe MadChild doesn't give a [damn] and considers that his style. I love Bad Dreams as well as every one of my Swollen CD's. Each MC's style is unique: Prevail is brainy. MadChild uses ancient and fantasy based ideas. Bad Dreams is the cd that you pop into the cd player in your car during a long midnight highway drive. Excellent.
05/22/2022 11:50:41 - INFO - __main__ - ['positive']
05/22/2022 11:50:41 - INFO - __main__ -  [amazon_polarity] title: Great Product.... [SEP] content: I really enjoy it..came just in time... and sorry it taken me so long to review it..
05/22/2022 11:50:41 - INFO - __main__ - ['positive']
05/22/2022 11:50:41 - INFO - __main__ - Tokenizing Input ...
05/22/2022 11:50:41 - INFO - __main__ - Tokenizing Output ...
05/22/2022 11:50:42 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 11:50:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 11:50:53 - INFO - __main__ - Starting training!
05/22/2022 11:50:57 - INFO - __main__ - Step 10 Global step 10 Train loss 23.119030 on epoch=0
05/22/2022 11:51:02 - INFO - __main__ - Step 20 Global step 20 Train loss 18.312012 on epoch=1
05/22/2022 11:51:08 - INFO - __main__ - Step 30 Global step 30 Train loss 17.888058 on epoch=1
05/22/2022 11:51:13 - INFO - __main__ - Step 40 Global step 40 Train loss 16.843901 on epoch=2
05/22/2022 11:51:18 - INFO - __main__ - Step 50 Global step 50 Train loss 16.300678 on epoch=3
05/22/2022 11:52:30 - INFO - __main__ - Global step 50 Train loss 18.492735 Classification-F1 0.0 on epoch=3
05/22/2022 11:52:36 - INFO - __main__ - Step 60 Global step 60 Train loss 16.178854 on epoch=3
05/22/2022 11:52:42 - INFO - __main__ - Step 70 Global step 70 Train loss 15.791925 on epoch=4
05/22/2022 11:52:47 - INFO - __main__ - Step 80 Global step 80 Train loss 15.463005 on epoch=4
05/22/2022 11:52:52 - INFO - __main__ - Step 90 Global step 90 Train loss 14.791097 on epoch=5
05/22/2022 11:52:57 - INFO - __main__ - Step 100 Global step 100 Train loss 13.816470 on epoch=6
05/22/2022 11:53:48 - INFO - __main__ - Global step 100 Train loss 15.208270 Classification-F1 0.0 on epoch=6
05/22/2022 11:53:53 - INFO - __main__ - Step 110 Global step 110 Train loss 13.849935 on epoch=6
05/22/2022 11:53:58 - INFO - __main__ - Step 120 Global step 120 Train loss 13.882263 on epoch=7
05/22/2022 11:54:04 - INFO - __main__ - Step 130 Global step 130 Train loss 13.394040 on epoch=8
05/22/2022 11:54:09 - INFO - __main__ - Step 140 Global step 140 Train loss 13.219521 on epoch=8
05/22/2022 11:54:14 - INFO - __main__ - Step 150 Global step 150 Train loss 12.447766 on epoch=9
05/22/2022 11:54:42 - INFO - __main__ - Global step 150 Train loss 13.358704 Classification-F1 0.0 on epoch=9
05/22/2022 11:54:47 - INFO - __main__ - Step 160 Global step 160 Train loss 12.445036 on epoch=9
05/22/2022 11:54:53 - INFO - __main__ - Step 170 Global step 170 Train loss 12.850519 on epoch=10
05/22/2022 11:54:58 - INFO - __main__ - Step 180 Global step 180 Train loss 11.688828 on epoch=11
05/22/2022 11:55:03 - INFO - __main__ - Step 190 Global step 190 Train loss 10.954531 on epoch=11
05/22/2022 11:55:09 - INFO - __main__ - Step 200 Global step 200 Train loss 10.114073 on epoch=12
05/22/2022 11:55:28 - INFO - __main__ - Global step 200 Train loss 11.610599 Classification-F1 0.0 on epoch=12
05/22/2022 11:55:33 - INFO - __main__ - Step 210 Global step 210 Train loss 9.294599 on epoch=13
05/22/2022 11:55:38 - INFO - __main__ - Step 220 Global step 220 Train loss 7.609192 on epoch=13
05/22/2022 11:55:43 - INFO - __main__ - Step 230 Global step 230 Train loss 6.333807 on epoch=14
05/22/2022 11:55:48 - INFO - __main__ - Step 240 Global step 240 Train loss 3.065453 on epoch=14
05/22/2022 11:55:54 - INFO - __main__ - Step 250 Global step 250 Train loss 1.950122 on epoch=15
05/22/2022 11:55:57 - INFO - __main__ - Global step 250 Train loss 5.650635 Classification-F1 0.3692115143929912 on epoch=15
05/22/2022 11:56:04 - INFO - __main__ - Step 260 Global step 260 Train loss 1.730204 on epoch=16
05/22/2022 11:56:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.995481 on epoch=16
05/22/2022 11:56:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.605394 on epoch=17
05/22/2022 11:56:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.757058 on epoch=18
05/22/2022 11:56:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.981967 on epoch=18
05/22/2022 11:56:29 - INFO - __main__ - Global step 300 Train loss 1.014021 Classification-F1 0.48538011695906436 on epoch=18
05/22/2022 11:56:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.645140 on epoch=19
05/22/2022 11:56:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.617385 on epoch=19
05/22/2022 11:56:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.503273 on epoch=20
05/22/2022 11:56:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.916456 on epoch=21
05/22/2022 11:56:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.617585 on epoch=21
05/22/2022 11:57:00 - INFO - __main__ - Global step 350 Train loss 0.659968 Classification-F1 0.6893957777238535 on epoch=21
05/22/2022 11:57:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.636629 on epoch=22
05/22/2022 11:57:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.455258 on epoch=23
05/22/2022 11:57:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.677133 on epoch=23
05/22/2022 11:57:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.474537 on epoch=24
05/22/2022 11:57:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.505146 on epoch=24
05/22/2022 11:57:32 - INFO - __main__ - Global step 400 Train loss 0.549741 Classification-F1 0.412425019769707 on epoch=24
05/22/2022 11:57:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.416706 on epoch=25
05/22/2022 11:57:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.381166 on epoch=26
05/22/2022 11:57:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.608456 on epoch=26
05/22/2022 11:57:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.533837 on epoch=27
05/22/2022 11:57:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.337348 on epoch=28
05/22/2022 11:58:02 - INFO - __main__ - Global step 450 Train loss 0.455503 Classification-F1 0.5544852075166558 on epoch=28
05/22/2022 11:58:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.669657 on epoch=28
05/22/2022 11:58:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.500706 on epoch=29
05/22/2022 11:58:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.352097 on epoch=29
05/22/2022 11:58:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.439088 on epoch=30
05/22/2022 11:58:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.424467 on epoch=31
05/22/2022 11:58:32 - INFO - __main__ - Global step 500 Train loss 0.477203 Classification-F1 0.6618867924528302 on epoch=31
05/22/2022 11:58:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.622109 on epoch=31
05/22/2022 11:58:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.662526 on epoch=32
05/22/2022 11:58:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.387752 on epoch=33
05/22/2022 11:58:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.578368 on epoch=33
05/22/2022 11:58:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.505695 on epoch=34
05/22/2022 11:59:03 - INFO - __main__ - Global step 550 Train loss 0.551290 Classification-F1 0.565365025466893 on epoch=34
05/22/2022 11:59:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.389977 on epoch=34
05/22/2022 11:59:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.426653 on epoch=35
05/22/2022 11:59:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.429973 on epoch=36
05/22/2022 11:59:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.343858 on epoch=36
05/22/2022 11:59:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.386047 on epoch=37
05/22/2022 11:59:33 - INFO - __main__ - Global step 600 Train loss 0.395302 Classification-F1 0.6129447339638423 on epoch=37
05/22/2022 11:59:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.389963 on epoch=38
05/22/2022 11:59:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.351124 on epoch=38
05/22/2022 11:59:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.455445 on epoch=39
05/22/2022 11:59:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.361852 on epoch=39
05/22/2022 12:00:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.368336 on epoch=40
05/22/2022 12:00:04 - INFO - __main__ - Global step 650 Train loss 0.385344 Classification-F1 0.6743451806213312 on epoch=40
05/22/2022 12:00:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.337493 on epoch=41
05/22/2022 12:00:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.390831 on epoch=41
05/22/2022 12:00:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.372793 on epoch=42
05/22/2022 12:00:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.579832 on epoch=43
05/22/2022 12:00:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.402893 on epoch=43
05/22/2022 12:00:34 - INFO - __main__ - Global step 700 Train loss 0.416768 Classification-F1 0.429800307219662 on epoch=43
05/22/2022 12:00:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.391059 on epoch=44
05/22/2022 12:00:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.358065 on epoch=44
05/22/2022 12:00:50 - INFO - __main__ - Step 730 Global step 730 Train loss 0.501515 on epoch=45
05/22/2022 12:00:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.427657 on epoch=46
05/22/2022 12:01:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.350241 on epoch=46
05/22/2022 12:01:05 - INFO - __main__ - Global step 750 Train loss 0.405707 Classification-F1 0.6401438378555083 on epoch=46
05/22/2022 12:01:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.382372 on epoch=47
05/22/2022 12:01:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.376589 on epoch=48
05/22/2022 12:01:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.452654 on epoch=48
05/22/2022 12:01:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.350923 on epoch=49
05/22/2022 12:01:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.508511 on epoch=49
05/22/2022 12:01:35 - INFO - __main__ - Global step 800 Train loss 0.414210 Classification-F1 0.5360904279178254 on epoch=49
05/22/2022 12:01:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.336337 on epoch=50
05/22/2022 12:01:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.362650 on epoch=51
05/22/2022 12:01:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.378815 on epoch=51
05/22/2022 12:01:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.366689 on epoch=52
05/22/2022 12:02:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.386474 on epoch=53
05/22/2022 12:02:05 - INFO - __main__ - Global step 850 Train loss 0.366193 Classification-F1 0.6115061298958173 on epoch=53
05/22/2022 12:02:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.327695 on epoch=53
05/22/2022 12:02:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.371045 on epoch=54
05/22/2022 12:02:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.363121 on epoch=54
05/22/2022 12:02:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.312517 on epoch=55
05/22/2022 12:02:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.353251 on epoch=56
05/22/2022 12:02:36 - INFO - __main__ - Global step 900 Train loss 0.345526 Classification-F1 0.6623560445216496 on epoch=56
05/22/2022 12:02:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.352747 on epoch=56
05/22/2022 12:02:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.333356 on epoch=57
05/22/2022 12:02:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.341277 on epoch=58
05/22/2022 12:02:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.350264 on epoch=58
05/22/2022 12:03:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.343547 on epoch=59
05/22/2022 12:03:06 - INFO - __main__ - Global step 950 Train loss 0.344238 Classification-F1 0.664771638454168 on epoch=59
05/22/2022 12:03:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.434483 on epoch=59
05/22/2022 12:03:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.325381 on epoch=60
05/22/2022 12:03:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.354910 on epoch=61
05/22/2022 12:03:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.338990 on epoch=61
05/22/2022 12:03:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.351994 on epoch=62
05/22/2022 12:03:34 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:03:34 - INFO - __main__ - Printing 3 examples
05/22/2022 12:03:34 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/22/2022 12:03:34 - INFO - __main__ - ['negative']
05/22/2022 12:03:34 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/22/2022 12:03:34 - INFO - __main__ - ['negative']
05/22/2022 12:03:34 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/22/2022 12:03:34 - INFO - __main__ - ['negative']
05/22/2022 12:03:34 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:03:34 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:03:35 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 12:03:35 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:03:35 - INFO - __main__ - Printing 3 examples
05/22/2022 12:03:35 - INFO - __main__ -  [amazon_polarity] title: a path of shadows [SEP] content: I'm glad a previous reviewer enjoyed this book and the previous one, but I did not. I too am a great fan of this series, but the last two books seemed flat to me--not very engaging. I certainly will continue to read Haney's books about Lieutenant Bak because when they are good(which they usually are) they are very very good.
05/22/2022 12:03:35 - INFO - __main__ - ['negative']
05/22/2022 12:03:35 - INFO - __main__ -  [amazon_polarity] title: WAY WAY WAY too much sex [SEP] content: I'm no prude, but when a book needs to rely on sex every page or 2 it gets really tedious. Please. It isn't necessary for the plot. The participants were married for heaven's sake.
05/22/2022 12:03:35 - INFO - __main__ - ['negative']
05/22/2022 12:03:35 - INFO - __main__ -  [amazon_polarity] title: Mind Game [SEP] content: I kept waiting for something to happen, the main character was anoying, things like this "she was so small she looked like a child" I ask just how sexy is that supposed to be and the guy was big etc etc. Nobe that this story gets a star is very generous of me.As I was reading this I kept thinking, is this over, somthing anything to bring this book around. (sigh) but we can't like them all.
05/22/2022 12:03:35 - INFO - __main__ - ['negative']
05/22/2022 12:03:35 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:03:35 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:03:35 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 12:03:37 - INFO - __main__ - Global step 1000 Train loss 0.361152 Classification-F1 0.6635897435897435 on epoch=62
05/22/2022 12:03:37 - INFO - __main__ - save last model!
05/22/2022 12:03:44 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 12:03:45 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 12:03:45 - INFO - __main__ - Printing 3 examples
05/22/2022 12:03:45 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 12:03:45 - INFO - __main__ - ['negative']
05/22/2022 12:03:45 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 12:03:45 - INFO - __main__ - ['negative']
05/22/2022 12:03:45 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 12:03:45 - INFO - __main__ - ['negative']
05/22/2022 12:03:45 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:03:45 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:03:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 12:03:46 - INFO - __main__ - Starting training!
05/22/2022 12:03:46 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 12:04:01 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_100_0.0001_8_predictions.txt
05/22/2022 12:04:01 - INFO - __main__ - Classification-F1 on test data: 0.4295
05/22/2022 12:04:02 - INFO - __main__ - prefix=amazon_polarity_128_100, lr=0.0001, bsz=8, dev_performance=0.6893957777238535, test_performance=0.42952328867013034
05/22/2022 12:04:02 - INFO - __main__ - Running ... prefix=amazon_polarity_128_13, lr=0.0005, bsz=8 ...
05/22/2022 12:04:03 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:04:03 - INFO - __main__ - Printing 3 examples
05/22/2022 12:04:03 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/22/2022 12:04:03 - INFO - __main__ - ['negative']
05/22/2022 12:04:03 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/22/2022 12:04:03 - INFO - __main__ - ['negative']
05/22/2022 12:04:03 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/22/2022 12:04:03 - INFO - __main__ - ['negative']
05/22/2022 12:04:03 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:04:03 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:04:03 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 12:04:03 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:04:03 - INFO - __main__ - Printing 3 examples
05/22/2022 12:04:03 - INFO - __main__ -  [amazon_polarity] title: a path of shadows [SEP] content: I'm glad a previous reviewer enjoyed this book and the previous one, but I did not. I too am a great fan of this series, but the last two books seemed flat to me--not very engaging. I certainly will continue to read Haney's books about Lieutenant Bak because when they are good(which they usually are) they are very very good.
05/22/2022 12:04:03 - INFO - __main__ - ['negative']
05/22/2022 12:04:03 - INFO - __main__ -  [amazon_polarity] title: WAY WAY WAY too much sex [SEP] content: I'm no prude, but when a book needs to rely on sex every page or 2 it gets really tedious. Please. It isn't necessary for the plot. The participants were married for heaven's sake.
05/22/2022 12:04:03 - INFO - __main__ - ['negative']
05/22/2022 12:04:03 - INFO - __main__ -  [amazon_polarity] title: Mind Game [SEP] content: I kept waiting for something to happen, the main character was anoying, things like this "she was so small she looked like a child" I ask just how sexy is that supposed to be and the guy was big etc etc. Nobe that this story gets a star is very generous of me.As I was reading this I kept thinking, is this over, somthing anything to bring this book around. (sigh) but we can't like them all.
05/22/2022 12:04:03 - INFO - __main__ - ['negative']
05/22/2022 12:04:03 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:04:03 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:04:03 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 12:04:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 12:04:16 - INFO - __main__ - Starting training!
05/22/2022 12:04:21 - INFO - __main__ - Step 10 Global step 10 Train loss 23.347706 on epoch=0
05/22/2022 12:04:26 - INFO - __main__ - Step 20 Global step 20 Train loss 17.486225 on epoch=1
05/22/2022 12:04:31 - INFO - __main__ - Step 30 Global step 30 Train loss 14.967226 on epoch=1
05/22/2022 12:04:37 - INFO - __main__ - Step 40 Global step 40 Train loss 13.040895 on epoch=2
05/22/2022 12:04:42 - INFO - __main__ - Step 50 Global step 50 Train loss 9.919265 on epoch=3
05/22/2022 12:04:48 - INFO - __main__ - Global step 50 Train loss 15.752263 Classification-F1 0.0 on epoch=3
05/22/2022 12:04:54 - INFO - __main__ - Step 60 Global step 60 Train loss 5.182992 on epoch=3
05/22/2022 12:04:59 - INFO - __main__ - Step 70 Global step 70 Train loss 2.824819 on epoch=4
05/22/2022 12:05:04 - INFO - __main__ - Step 80 Global step 80 Train loss 1.779459 on epoch=4
05/22/2022 12:05:10 - INFO - __main__ - Step 90 Global step 90 Train loss 1.980721 on epoch=5
05/22/2022 12:05:15 - INFO - __main__ - Step 100 Global step 100 Train loss 1.328871 on epoch=6
05/22/2022 12:05:19 - INFO - __main__ - Global step 100 Train loss 2.619372 Classification-F1 0.051864801864801864 on epoch=6
05/22/2022 12:05:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.900304 on epoch=6
05/22/2022 12:05:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.420480 on epoch=7
05/22/2022 12:05:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.444706 on epoch=8
05/22/2022 12:05:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.321579 on epoch=8
05/22/2022 12:05:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.242424 on epoch=9
05/22/2022 12:05:50 - INFO - __main__ - Global step 150 Train loss 0.465899 Classification-F1 0.8709972971734847 on epoch=9
05/22/2022 12:05:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.393363 on epoch=9
05/22/2022 12:06:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.249823 on epoch=10
05/22/2022 12:06:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.253613 on epoch=11
05/22/2022 12:06:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.144346 on epoch=11
05/22/2022 12:06:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.231110 on epoch=12
05/22/2022 12:06:21 - INFO - __main__ - Global step 200 Train loss 0.254451 Classification-F1 0.9295800476801761 on epoch=12
05/22/2022 12:06:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.191216 on epoch=13
05/22/2022 12:06:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.127102 on epoch=13
05/22/2022 12:06:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.091547 on epoch=14
05/22/2022 12:06:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.099090 on epoch=14
05/22/2022 12:06:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.055348 on epoch=15
05/22/2022 12:06:52 - INFO - __main__ - Global step 250 Train loss 0.112860 Classification-F1 0.9335684083589015 on epoch=15
05/22/2022 12:06:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.037193 on epoch=16
05/22/2022 12:07:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.071283 on epoch=16
05/22/2022 12:07:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.087801 on epoch=17
05/22/2022 12:07:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.066766 on epoch=18
05/22/2022 12:07:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.148618 on epoch=18
05/22/2022 12:07:22 - INFO - __main__ - Global step 300 Train loss 0.082332 Classification-F1 0.9452824427480916 on epoch=18
05/22/2022 12:07:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.024216 on epoch=19
05/22/2022 12:07:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.011636 on epoch=19
05/22/2022 12:07:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.029256 on epoch=20
05/22/2022 12:07:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.017573 on epoch=21
05/22/2022 12:07:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.014311 on epoch=21
05/22/2022 12:07:53 - INFO - __main__ - Global step 350 Train loss 0.019399 Classification-F1 0.9414053559166857 on epoch=21
05/22/2022 12:07:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.017565 on epoch=22
05/22/2022 12:08:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.011987 on epoch=23
05/22/2022 12:08:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.003499 on epoch=23
05/22/2022 12:08:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.028831 on epoch=24
05/22/2022 12:08:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.003100 on epoch=24
05/22/2022 12:08:23 - INFO - __main__ - Global step 400 Train loss 0.012996 Classification-F1 0.9413838897284426 on epoch=24
05/22/2022 12:08:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.023530 on epoch=25
05/22/2022 12:08:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.002953 on epoch=26
05/22/2022 12:08:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.002565 on epoch=26
05/22/2022 12:08:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001440 on epoch=27
05/22/2022 12:08:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000165 on epoch=28
05/22/2022 12:08:53 - INFO - __main__ - Global step 450 Train loss 0.006131 Classification-F1 0.9452991452991453 on epoch=28
05/22/2022 12:08:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000035 on epoch=28
05/22/2022 12:09:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.013079 on epoch=29
05/22/2022 12:09:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.035337 on epoch=29
05/22/2022 12:09:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.045925 on epoch=30
05/22/2022 12:09:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.011409 on epoch=31
05/22/2022 12:09:24 - INFO - __main__ - Global step 500 Train loss 0.021157 Classification-F1 0.8982822910935877 on epoch=31
05/22/2022 12:09:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000481 on epoch=31
05/22/2022 12:09:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000619 on epoch=32
05/22/2022 12:09:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000525 on epoch=33
05/22/2022 12:09:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.003377 on epoch=33
05/22/2022 12:09:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.017680 on epoch=34
05/22/2022 12:09:53 - INFO - __main__ - Global step 550 Train loss 0.004536 Classification-F1 0.8574169193638219 on epoch=34
05/22/2022 12:09:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.039946 on epoch=34
05/22/2022 12:10:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001660 on epoch=35
05/22/2022 12:10:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000484 on epoch=36
05/22/2022 12:10:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.016387 on epoch=36
05/22/2022 12:10:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000371 on epoch=37
05/22/2022 12:10:23 - INFO - __main__ - Global step 600 Train loss 0.011770 Classification-F1 0.9217986314760509 on epoch=37
05/22/2022 12:10:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000286 on epoch=38
05/22/2022 12:10:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000153 on epoch=38
05/22/2022 12:10:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.002679 on epoch=39
05/22/2022 12:10:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000250 on epoch=39
05/22/2022 12:10:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000553 on epoch=40
05/22/2022 12:10:53 - INFO - __main__ - Global step 650 Train loss 0.000784 Classification-F1 0.9217986314760509 on epoch=40
05/22/2022 12:10:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000364 on epoch=41
05/22/2022 12:11:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000053 on epoch=41
05/22/2022 12:11:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000140 on epoch=42
05/22/2022 12:11:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000091 on epoch=43
05/22/2022 12:11:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.001131 on epoch=43
05/22/2022 12:11:23 - INFO - __main__ - Global step 700 Train loss 0.000356 Classification-F1 0.9057842114948169 on epoch=43
05/22/2022 12:11:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.001523 on epoch=44
05/22/2022 12:11:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000038 on epoch=44
05/22/2022 12:11:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000619 on epoch=45
05/22/2022 12:11:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.003665 on epoch=46
05/22/2022 12:11:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000032 on epoch=46
05/22/2022 12:11:53 - INFO - __main__ - Global step 750 Train loss 0.001176 Classification-F1 0.9100450691314643 on epoch=46
05/22/2022 12:11:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000656 on epoch=47
05/22/2022 12:12:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000897 on epoch=48
05/22/2022 12:12:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000010 on epoch=48
05/22/2022 12:12:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000158 on epoch=49
05/22/2022 12:12:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000053 on epoch=49
05/22/2022 12:12:23 - INFO - __main__ - Global step 800 Train loss 0.000355 Classification-F1 0.9257529269893606 on epoch=49
05/22/2022 12:12:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000023 on epoch=50
05/22/2022 12:12:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000016 on epoch=51
05/22/2022 12:12:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000050 on epoch=51
05/22/2022 12:12:44 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000014 on epoch=52
05/22/2022 12:12:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000009 on epoch=53
05/22/2022 12:12:53 - INFO - __main__ - Global step 850 Train loss 0.000022 Classification-F1 0.9257529269893606 on epoch=53
05/22/2022 12:12:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000297 on epoch=53
05/22/2022 12:13:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000012 on epoch=54
05/22/2022 12:13:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000012 on epoch=54
05/22/2022 12:13:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000956 on epoch=55
05/22/2022 12:13:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000131 on epoch=56
05/22/2022 12:13:23 - INFO - __main__ - Global step 900 Train loss 0.000282 Classification-F1 0.9218320610687023 on epoch=56
05/22/2022 12:13:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.012675 on epoch=56
05/22/2022 12:13:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000051 on epoch=57
05/22/2022 12:13:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000121 on epoch=58
05/22/2022 12:13:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000021 on epoch=58
05/22/2022 12:13:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000046 on epoch=59
05/22/2022 12:13:53 - INFO - __main__ - Global step 950 Train loss 0.002583 Classification-F1 0.9060435527281625 on epoch=59
05/22/2022 12:13:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000028 on epoch=59
05/22/2022 12:14:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000025 on epoch=60
05/22/2022 12:14:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000074 on epoch=61
05/22/2022 12:14:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000009 on epoch=61
05/22/2022 12:14:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000031 on epoch=62
05/22/2022 12:14:20 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:14:20 - INFO - __main__ - Printing 3 examples
05/22/2022 12:14:20 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/22/2022 12:14:20 - INFO - __main__ - ['negative']
05/22/2022 12:14:20 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/22/2022 12:14:20 - INFO - __main__ - ['negative']
05/22/2022 12:14:20 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/22/2022 12:14:20 - INFO - __main__ - ['negative']
05/22/2022 12:14:20 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:14:20 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:14:20 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 12:14:20 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:14:20 - INFO - __main__ - Printing 3 examples
05/22/2022 12:14:20 - INFO - __main__ -  [amazon_polarity] title: a path of shadows [SEP] content: I'm glad a previous reviewer enjoyed this book and the previous one, but I did not. I too am a great fan of this series, but the last two books seemed flat to me--not very engaging. I certainly will continue to read Haney's books about Lieutenant Bak because when they are good(which they usually are) they are very very good.
05/22/2022 12:14:20 - INFO - __main__ - ['negative']
05/22/2022 12:14:20 - INFO - __main__ -  [amazon_polarity] title: WAY WAY WAY too much sex [SEP] content: I'm no prude, but when a book needs to rely on sex every page or 2 it gets really tedious. Please. It isn't necessary for the plot. The participants were married for heaven's sake.
05/22/2022 12:14:20 - INFO - __main__ - ['negative']
05/22/2022 12:14:20 - INFO - __main__ -  [amazon_polarity] title: Mind Game [SEP] content: I kept waiting for something to happen, the main character was anoying, things like this "she was so small she looked like a child" I ask just how sexy is that supposed to be and the guy was big etc etc. Nobe that this story gets a star is very generous of me.As I was reading this I kept thinking, is this over, somthing anything to bring this book around. (sigh) but we can't like them all.
05/22/2022 12:14:20 - INFO - __main__ - ['negative']
05/22/2022 12:14:20 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:14:20 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:14:21 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 12:14:22 - INFO - __main__ - Global step 1000 Train loss 0.000033 Classification-F1 0.9099900634411068 on epoch=62
05/22/2022 12:14:22 - INFO - __main__ - save last model!
05/22/2022 12:14:29 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 12:14:30 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 12:14:30 - INFO - __main__ - Printing 3 examples
05/22/2022 12:14:30 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 12:14:30 - INFO - __main__ - ['negative']
05/22/2022 12:14:30 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 12:14:30 - INFO - __main__ - ['negative']
05/22/2022 12:14:30 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 12:14:30 - INFO - __main__ - ['negative']
05/22/2022 12:14:30 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:14:30 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:14:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 12:14:31 - INFO - __main__ - Starting training!
05/22/2022 12:14:31 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 12:14:46 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_13_0.0005_8_predictions.txt
05/22/2022 12:14:46 - INFO - __main__ - Classification-F1 on test data: 0.9520
05/22/2022 12:14:47 - INFO - __main__ - prefix=amazon_polarity_128_13, lr=0.0005, bsz=8, dev_performance=0.9452991452991453, test_performance=0.9519767567502672
05/22/2022 12:14:47 - INFO - __main__ - Running ... prefix=amazon_polarity_128_13, lr=0.0003, bsz=8 ...
05/22/2022 12:14:48 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:14:48 - INFO - __main__ - Printing 3 examples
05/22/2022 12:14:48 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/22/2022 12:14:48 - INFO - __main__ - ['negative']
05/22/2022 12:14:48 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/22/2022 12:14:48 - INFO - __main__ - ['negative']
05/22/2022 12:14:48 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/22/2022 12:14:48 - INFO - __main__ - ['negative']
05/22/2022 12:14:48 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:14:48 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:14:48 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 12:14:48 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:14:48 - INFO - __main__ - Printing 3 examples
05/22/2022 12:14:48 - INFO - __main__ -  [amazon_polarity] title: a path of shadows [SEP] content: I'm glad a previous reviewer enjoyed this book and the previous one, but I did not. I too am a great fan of this series, but the last two books seemed flat to me--not very engaging. I certainly will continue to read Haney's books about Lieutenant Bak because when they are good(which they usually are) they are very very good.
05/22/2022 12:14:48 - INFO - __main__ - ['negative']
05/22/2022 12:14:48 - INFO - __main__ -  [amazon_polarity] title: WAY WAY WAY too much sex [SEP] content: I'm no prude, but when a book needs to rely on sex every page or 2 it gets really tedious. Please. It isn't necessary for the plot. The participants were married for heaven's sake.
05/22/2022 12:14:48 - INFO - __main__ - ['negative']
05/22/2022 12:14:48 - INFO - __main__ -  [amazon_polarity] title: Mind Game [SEP] content: I kept waiting for something to happen, the main character was anoying, things like this "she was so small she looked like a child" I ask just how sexy is that supposed to be and the guy was big etc etc. Nobe that this story gets a star is very generous of me.As I was reading this I kept thinking, is this over, somthing anything to bring this book around. (sigh) but we can't like them all.
05/22/2022 12:14:48 - INFO - __main__ - ['negative']
05/22/2022 12:14:48 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:14:48 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:14:49 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 12:15:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 12:15:01 - INFO - __main__ - Starting training!
05/22/2022 12:15:05 - INFO - __main__ - Step 10 Global step 10 Train loss 21.656094 on epoch=0
05/22/2022 12:15:10 - INFO - __main__ - Step 20 Global step 20 Train loss 17.854433 on epoch=1
05/22/2022 12:15:16 - INFO - __main__ - Step 30 Global step 30 Train loss 15.999428 on epoch=1
05/22/2022 12:15:21 - INFO - __main__ - Step 40 Global step 40 Train loss 15.147861 on epoch=2
05/22/2022 12:15:25 - INFO - __main__ - Step 50 Global step 50 Train loss 14.504721 on epoch=3
05/22/2022 12:16:35 - INFO - __main__ - Global step 50 Train loss 17.032507 Classification-F1 0.0001303780964797914 on epoch=3
05/22/2022 12:16:41 - INFO - __main__ - Step 60 Global step 60 Train loss 13.623416 on epoch=3
05/22/2022 12:16:46 - INFO - __main__ - Step 70 Global step 70 Train loss 12.698854 on epoch=4
05/22/2022 12:16:51 - INFO - __main__ - Step 80 Global step 80 Train loss 10.874751 on epoch=4
05/22/2022 12:16:56 - INFO - __main__ - Step 90 Global step 90 Train loss 8.547153 on epoch=5
05/22/2022 12:17:01 - INFO - __main__ - Step 100 Global step 100 Train loss 4.117274 on epoch=6
05/22/2022 12:17:05 - INFO - __main__ - Global step 100 Train loss 9.972289 Classification-F1 0.39659099032854356 on epoch=6
05/22/2022 12:17:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.709138 on epoch=6
05/22/2022 12:17:16 - INFO - __main__ - Step 120 Global step 120 Train loss 0.643919 on epoch=7
05/22/2022 12:17:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.445256 on epoch=8
05/22/2022 12:17:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.418417 on epoch=8
05/22/2022 12:17:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.423836 on epoch=9
05/22/2022 12:17:36 - INFO - __main__ - Global step 150 Train loss 0.528113 Classification-F1 0.437151179178803 on epoch=9
05/22/2022 12:17:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.402029 on epoch=9
05/22/2022 12:17:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.415218 on epoch=10
05/22/2022 12:17:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.392320 on epoch=11
05/22/2022 12:17:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.343715 on epoch=11
05/22/2022 12:18:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.319228 on epoch=12
05/22/2022 12:18:06 - INFO - __main__ - Global step 200 Train loss 0.374502 Classification-F1 0.8085674607413739 on epoch=12
05/22/2022 12:18:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.307163 on epoch=13
05/22/2022 12:18:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.246820 on epoch=13
05/22/2022 12:18:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.295786 on epoch=14
05/22/2022 12:18:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.226452 on epoch=14
05/22/2022 12:18:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.177608 on epoch=15
05/22/2022 12:18:37 - INFO - __main__ - Global step 250 Train loss 0.250766 Classification-F1 0.9218702313373619 on epoch=15
05/22/2022 12:18:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.154462 on epoch=16
05/22/2022 12:18:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.107840 on epoch=16
05/22/2022 12:18:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.089719 on epoch=17
05/22/2022 12:18:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.024594 on epoch=18
05/22/2022 12:19:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.060659 on epoch=18
05/22/2022 12:19:07 - INFO - __main__ - Global step 300 Train loss 0.087455 Classification-F1 0.9374656488549619 on epoch=18
05/22/2022 12:19:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.082644 on epoch=19
05/22/2022 12:19:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.035588 on epoch=19
05/22/2022 12:19:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.047521 on epoch=20
05/22/2022 12:19:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.006086 on epoch=21
05/22/2022 12:19:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.034796 on epoch=21
05/22/2022 12:19:38 - INFO - __main__ - Global step 350 Train loss 0.041327 Classification-F1 0.9414053559166857 on epoch=21
05/22/2022 12:19:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.093695 on epoch=22
05/22/2022 12:19:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.052065 on epoch=23
05/22/2022 12:19:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.025337 on epoch=23
05/22/2022 12:20:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.028565 on epoch=24
05/22/2022 12:20:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.014954 on epoch=24
05/22/2022 12:20:09 - INFO - __main__ - Global step 400 Train loss 0.042924 Classification-F1 0.9413838897284426 on epoch=24
05/22/2022 12:20:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.034694 on epoch=25
05/22/2022 12:20:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.029301 on epoch=26
05/22/2022 12:20:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.026821 on epoch=26
05/22/2022 12:20:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.031135 on epoch=27
05/22/2022 12:20:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.005907 on epoch=28
05/22/2022 12:20:39 - INFO - __main__ - Global step 450 Train loss 0.025571 Classification-F1 0.9413337407379114 on epoch=28
05/22/2022 12:20:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.008394 on epoch=28
05/22/2022 12:20:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.008911 on epoch=29
05/22/2022 12:20:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.011347 on epoch=29
05/22/2022 12:21:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002747 on epoch=30
05/22/2022 12:21:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.005559 on epoch=31
05/22/2022 12:21:09 - INFO - __main__ - Global step 500 Train loss 0.007392 Classification-F1 0.9531135531135531 on epoch=31
05/22/2022 12:21:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.009076 on epoch=31
05/22/2022 12:21:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.015884 on epoch=32
05/22/2022 12:21:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.003360 on epoch=33
05/22/2022 12:21:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.012237 on epoch=33
05/22/2022 12:21:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.025221 on epoch=34
05/22/2022 12:21:40 - INFO - __main__ - Global step 550 Train loss 0.013156 Classification-F1 0.9413624078061295 on epoch=34
05/22/2022 12:21:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.026082 on epoch=34
05/22/2022 12:21:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.018484 on epoch=35
05/22/2022 12:21:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001204 on epoch=36
05/22/2022 12:22:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.032690 on epoch=36
05/22/2022 12:22:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001287 on epoch=37
05/22/2022 12:22:10 - INFO - __main__ - Global step 600 Train loss 0.015949 Classification-F1 0.9374044868268232 on epoch=37
05/22/2022 12:22:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.001359 on epoch=38
05/22/2022 12:22:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.003278 on epoch=38
05/22/2022 12:22:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.009323 on epoch=39
05/22/2022 12:22:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.126019 on epoch=39
05/22/2022 12:22:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.292258 on epoch=40
05/22/2022 12:22:41 - INFO - __main__ - Global step 650 Train loss 0.086447 Classification-F1 0.6210724618179371 on epoch=40
05/22/2022 12:22:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.315580 on epoch=41
05/22/2022 12:22:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.368298 on epoch=41
05/22/2022 12:22:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.346073 on epoch=42
05/22/2022 12:23:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.350887 on epoch=43
05/22/2022 12:23:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.301892 on epoch=43
05/22/2022 12:23:11 - INFO - __main__ - Global step 700 Train loss 0.336546 Classification-F1 0.41470975742075483 on epoch=43
05/22/2022 12:23:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.319080 on epoch=44
05/22/2022 12:23:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.404693 on epoch=44
05/22/2022 12:23:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.286121 on epoch=45
05/22/2022 12:23:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.314040 on epoch=46
05/22/2022 12:23:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.290388 on epoch=46
05/22/2022 12:23:42 - INFO - __main__ - Global step 750 Train loss 0.322864 Classification-F1 0.6217494089834515 on epoch=46
05/22/2022 12:23:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.316459 on epoch=47
05/22/2022 12:23:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.326642 on epoch=48
05/22/2022 12:23:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.290802 on epoch=48
05/22/2022 12:24:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.315994 on epoch=49
05/22/2022 12:24:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.345915 on epoch=49
05/22/2022 12:24:12 - INFO - __main__ - Global step 800 Train loss 0.319162 Classification-F1 0.6610694910875743 on epoch=49
05/22/2022 12:24:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.305514 on epoch=50
05/22/2022 12:24:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.352148 on epoch=51
05/22/2022 12:24:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.284436 on epoch=51
05/22/2022 12:24:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.352616 on epoch=52
05/22/2022 12:24:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.330938 on epoch=53
05/22/2022 12:24:42 - INFO - __main__ - Global step 850 Train loss 0.325131 Classification-F1 0.6403008513100257 on epoch=53
05/22/2022 12:24:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.236168 on epoch=53
05/22/2022 12:24:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.290252 on epoch=54
05/22/2022 12:24:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.286352 on epoch=54
05/22/2022 12:25:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.260697 on epoch=55
05/22/2022 12:25:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.277402 on epoch=56
05/22/2022 12:25:12 - INFO - __main__ - Global step 900 Train loss 0.270174 Classification-F1 0.681566972650407 on epoch=56
05/22/2022 12:25:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.286065 on epoch=56
05/22/2022 12:25:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.308739 on epoch=57
05/22/2022 12:25:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.330069 on epoch=58
05/22/2022 12:25:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.286762 on epoch=58
05/22/2022 12:25:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.303507 on epoch=59
05/22/2022 12:25:41 - INFO - __main__ - Global step 950 Train loss 0.303028 Classification-F1 0.6788850174216028 on epoch=59
05/22/2022 12:25:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.348938 on epoch=59
05/22/2022 12:25:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.264558 on epoch=60
05/22/2022 12:25:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.310113 on epoch=61
05/22/2022 12:26:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.275075 on epoch=61
05/22/2022 12:26:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.302005 on epoch=62
05/22/2022 12:26:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:26:09 - INFO - __main__ - Printing 3 examples
05/22/2022 12:26:09 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/22/2022 12:26:09 - INFO - __main__ - ['negative']
05/22/2022 12:26:09 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/22/2022 12:26:09 - INFO - __main__ - ['negative']
05/22/2022 12:26:09 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/22/2022 12:26:09 - INFO - __main__ - ['negative']
05/22/2022 12:26:09 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:26:09 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:26:09 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 12:26:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:26:09 - INFO - __main__ - Printing 3 examples
05/22/2022 12:26:09 - INFO - __main__ -  [amazon_polarity] title: a path of shadows [SEP] content: I'm glad a previous reviewer enjoyed this book and the previous one, but I did not. I too am a great fan of this series, but the last two books seemed flat to me--not very engaging. I certainly will continue to read Haney's books about Lieutenant Bak because when they are good(which they usually are) they are very very good.
05/22/2022 12:26:09 - INFO - __main__ - ['negative']
05/22/2022 12:26:09 - INFO - __main__ -  [amazon_polarity] title: WAY WAY WAY too much sex [SEP] content: I'm no prude, but when a book needs to rely on sex every page or 2 it gets really tedious. Please. It isn't necessary for the plot. The participants were married for heaven's sake.
05/22/2022 12:26:09 - INFO - __main__ - ['negative']
05/22/2022 12:26:09 - INFO - __main__ -  [amazon_polarity] title: Mind Game [SEP] content: I kept waiting for something to happen, the main character was anoying, things like this "she was so small she looked like a child" I ask just how sexy is that supposed to be and the guy was big etc etc. Nobe that this story gets a star is very generous of me.As I was reading this I kept thinking, is this over, somthing anything to bring this book around. (sigh) but we can't like them all.
05/22/2022 12:26:09 - INFO - __main__ - ['negative']
05/22/2022 12:26:09 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:26:09 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:26:10 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 12:26:11 - INFO - __main__ - Global step 1000 Train loss 0.300138 Classification-F1 0.7157180024113206 on epoch=62
05/22/2022 12:26:11 - INFO - __main__ - save last model!
05/22/2022 12:26:18 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 12:26:19 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 12:26:19 - INFO - __main__ - Printing 3 examples
05/22/2022 12:26:19 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 12:26:19 - INFO - __main__ - ['negative']
05/22/2022 12:26:19 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 12:26:19 - INFO - __main__ - ['negative']
05/22/2022 12:26:19 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 12:26:19 - INFO - __main__ - ['negative']
05/22/2022 12:26:19 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:26:20 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:26:21 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 12:26:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 12:26:22 - INFO - __main__ - Starting training!
05/22/2022 12:26:36 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_13_0.0003_8_predictions.txt
05/22/2022 12:26:36 - INFO - __main__ - Classification-F1 on test data: 0.9550
05/22/2022 12:26:36 - INFO - __main__ - prefix=amazon_polarity_128_13, lr=0.0003, bsz=8, dev_performance=0.9531135531135531, test_performance=0.9549898727213623
05/22/2022 12:26:36 - INFO - __main__ - Running ... prefix=amazon_polarity_128_13, lr=0.0002, bsz=8 ...
05/22/2022 12:26:37 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:26:37 - INFO - __main__ - Printing 3 examples
05/22/2022 12:26:37 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/22/2022 12:26:37 - INFO - __main__ - ['negative']
05/22/2022 12:26:37 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/22/2022 12:26:37 - INFO - __main__ - ['negative']
05/22/2022 12:26:37 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/22/2022 12:26:37 - INFO - __main__ - ['negative']
05/22/2022 12:26:37 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:26:37 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:26:37 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 12:26:37 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:26:37 - INFO - __main__ - Printing 3 examples
05/22/2022 12:26:37 - INFO - __main__ -  [amazon_polarity] title: a path of shadows [SEP] content: I'm glad a previous reviewer enjoyed this book and the previous one, but I did not. I too am a great fan of this series, but the last two books seemed flat to me--not very engaging. I certainly will continue to read Haney's books about Lieutenant Bak because when they are good(which they usually are) they are very very good.
05/22/2022 12:26:37 - INFO - __main__ - ['negative']
05/22/2022 12:26:37 - INFO - __main__ -  [amazon_polarity] title: WAY WAY WAY too much sex [SEP] content: I'm no prude, but when a book needs to rely on sex every page or 2 it gets really tedious. Please. It isn't necessary for the plot. The participants were married for heaven's sake.
05/22/2022 12:26:37 - INFO - __main__ - ['negative']
05/22/2022 12:26:37 - INFO - __main__ -  [amazon_polarity] title: Mind Game [SEP] content: I kept waiting for something to happen, the main character was anoying, things like this "she was so small she looked like a child" I ask just how sexy is that supposed to be and the guy was big etc etc. Nobe that this story gets a star is very generous of me.As I was reading this I kept thinking, is this over, somthing anything to bring this book around. (sigh) but we can't like them all.
05/22/2022 12:26:37 - INFO - __main__ - ['negative']
05/22/2022 12:26:37 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:26:37 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:26:37 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 12:26:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 12:26:48 - INFO - __main__ - Starting training!
05/22/2022 12:26:53 - INFO - __main__ - Step 10 Global step 10 Train loss 23.097593 on epoch=0
05/22/2022 12:26:58 - INFO - __main__ - Step 20 Global step 20 Train loss 18.816084 on epoch=1
05/22/2022 12:27:03 - INFO - __main__ - Step 30 Global step 30 Train loss 17.681190 on epoch=1
05/22/2022 12:27:09 - INFO - __main__ - Step 40 Global step 40 Train loss 16.131718 on epoch=2
05/22/2022 12:27:14 - INFO - __main__ - Step 50 Global step 50 Train loss 14.925000 on epoch=3
05/22/2022 12:28:05 - INFO - __main__ - Global step 50 Train loss 18.130318 Classification-F1 0.0 on epoch=3
05/22/2022 12:28:11 - INFO - __main__ - Step 60 Global step 60 Train loss 14.449057 on epoch=3
05/22/2022 12:28:16 - INFO - __main__ - Step 70 Global step 70 Train loss 13.414560 on epoch=4
05/22/2022 12:28:21 - INFO - __main__ - Step 80 Global step 80 Train loss 12.429259 on epoch=4
05/22/2022 12:28:27 - INFO - __main__ - Step 90 Global step 90 Train loss 11.729604 on epoch=5
05/22/2022 12:28:32 - INFO - __main__ - Step 100 Global step 100 Train loss 11.500869 on epoch=6
05/22/2022 12:29:08 - INFO - __main__ - Global step 100 Train loss 12.704669 Classification-F1 0.0 on epoch=6
05/22/2022 12:29:13 - INFO - __main__ - Step 110 Global step 110 Train loss 9.790506 on epoch=6
05/22/2022 12:29:18 - INFO - __main__ - Step 120 Global step 120 Train loss 6.974440 on epoch=7
05/22/2022 12:29:23 - INFO - __main__ - Step 130 Global step 130 Train loss 1.162764 on epoch=8
05/22/2022 12:29:28 - INFO - __main__ - Step 140 Global step 140 Train loss 2.576060 on epoch=8
05/22/2022 12:29:33 - INFO - __main__ - Step 150 Global step 150 Train loss 2.172288 on epoch=9
05/22/2022 12:29:37 - INFO - __main__ - Global step 150 Train loss 4.535212 Classification-F1 0.48988842398884236 on epoch=9
05/22/2022 12:29:43 - INFO - __main__ - Step 160 Global step 160 Train loss 1.453865 on epoch=9
05/22/2022 12:29:49 - INFO - __main__ - Step 170 Global step 170 Train loss 1.430454 on epoch=10
05/22/2022 12:29:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.462003 on epoch=11
05/22/2022 12:29:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.458789 on epoch=11
05/22/2022 12:30:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.418918 on epoch=12
05/22/2022 12:30:08 - INFO - __main__ - Global step 200 Train loss 0.844806 Classification-F1 0.6835502922459444 on epoch=12
05/22/2022 12:30:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.471211 on epoch=13
05/22/2022 12:30:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.399916 on epoch=13
05/22/2022 12:30:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.442152 on epoch=14
05/22/2022 12:30:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.417916 on epoch=14
05/22/2022 12:30:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.412771 on epoch=15
05/22/2022 12:30:39 - INFO - __main__ - Global step 250 Train loss 0.428793 Classification-F1 0.6729479399552858 on epoch=15
05/22/2022 12:30:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.390164 on epoch=16
05/22/2022 12:30:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.394872 on epoch=16
05/22/2022 12:30:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.356800 on epoch=17
05/22/2022 12:31:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.373117 on epoch=18
05/22/2022 12:31:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.354068 on epoch=18
05/22/2022 12:31:09 - INFO - __main__ - Global step 300 Train loss 0.373804 Classification-F1 0.48409044687263114 on epoch=18
05/22/2022 12:31:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.381535 on epoch=19
05/22/2022 12:31:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.366495 on epoch=19
05/22/2022 12:31:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.357541 on epoch=20
05/22/2022 12:31:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.417123 on epoch=21
05/22/2022 12:31:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.333254 on epoch=21
05/22/2022 12:31:39 - INFO - __main__ - Global step 350 Train loss 0.371190 Classification-F1 0.5234842324026384 on epoch=21
05/22/2022 12:31:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.383896 on epoch=22
05/22/2022 12:31:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.384699 on epoch=23
05/22/2022 12:31:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.382863 on epoch=23
05/22/2022 12:32:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.374620 on epoch=24
05/22/2022 12:32:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.391129 on epoch=24
05/22/2022 12:32:09 - INFO - __main__ - Global step 400 Train loss 0.383442 Classification-F1 0.37532485567480706 on epoch=24
05/22/2022 12:32:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.335479 on epoch=25
05/22/2022 12:32:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.379128 on epoch=26
05/22/2022 12:32:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.371268 on epoch=26
05/22/2022 12:32:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.357184 on epoch=27
05/22/2022 12:32:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.376002 on epoch=28
05/22/2022 12:32:39 - INFO - __main__ - Global step 450 Train loss 0.363812 Classification-F1 0.5243403939056113 on epoch=28
05/22/2022 12:32:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.347472 on epoch=28
05/22/2022 12:32:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.347697 on epoch=29
05/22/2022 12:32:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.387498 on epoch=29
05/22/2022 12:33:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.342736 on epoch=30
05/22/2022 12:33:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.354268 on epoch=31
05/22/2022 12:33:09 - INFO - __main__ - Global step 500 Train loss 0.355934 Classification-F1 0.41996927803379414 on epoch=31
05/22/2022 12:33:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.355177 on epoch=31
05/22/2022 12:33:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.343218 on epoch=32
05/22/2022 12:33:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.376276 on epoch=33
05/22/2022 12:33:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.355783 on epoch=33
05/22/2022 12:33:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.353227 on epoch=34
05/22/2022 12:33:39 - INFO - __main__ - Global step 550 Train loss 0.356736 Classification-F1 0.4492753623188406 on epoch=34
05/22/2022 12:33:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.398500 on epoch=34
05/22/2022 12:33:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.336164 on epoch=35
05/22/2022 12:33:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.342182 on epoch=36
05/22/2022 12:34:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.341051 on epoch=36
05/22/2022 12:34:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.351042 on epoch=37
05/22/2022 12:34:09 - INFO - __main__ - Global step 600 Train loss 0.353788 Classification-F1 0.4420755396365153 on epoch=37
05/22/2022 12:34:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.372849 on epoch=38
05/22/2022 12:34:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.361537 on epoch=38
05/22/2022 12:34:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.359724 on epoch=39
05/22/2022 12:34:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.363168 on epoch=39
05/22/2022 12:34:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.355834 on epoch=40
05/22/2022 12:34:39 - INFO - __main__ - Global step 650 Train loss 0.362622 Classification-F1 0.683588921950103 on epoch=40
05/22/2022 12:34:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.357966 on epoch=41
05/22/2022 12:34:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.353931 on epoch=41
05/22/2022 12:34:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.347993 on epoch=42
05/22/2022 12:35:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.347140 on epoch=43
05/22/2022 12:35:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.343074 on epoch=43
05/22/2022 12:35:10 - INFO - __main__ - Global step 700 Train loss 0.350021 Classification-F1 0.350463149416029 on epoch=43
05/22/2022 12:35:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.371231 on epoch=44
05/22/2022 12:35:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.347956 on epoch=44
05/22/2022 12:35:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.326408 on epoch=45
05/22/2022 12:35:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.372017 on epoch=46
05/22/2022 12:35:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.329735 on epoch=46
05/22/2022 12:35:40 - INFO - __main__ - Global step 750 Train loss 0.349470 Classification-F1 0.49749591825503103 on epoch=46
05/22/2022 12:35:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.338574 on epoch=47
05/22/2022 12:35:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.347318 on epoch=48
05/22/2022 12:35:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.340972 on epoch=48
05/22/2022 12:36:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.347456 on epoch=49
05/22/2022 12:36:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.360621 on epoch=49
05/22/2022 12:36:10 - INFO - __main__ - Global step 800 Train loss 0.346988 Classification-F1 0.3671451355661882 on epoch=49
05/22/2022 12:36:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.335668 on epoch=50
05/22/2022 12:36:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.337558 on epoch=51
05/22/2022 12:36:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.368278 on epoch=51
05/22/2022 12:36:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.349670 on epoch=52
05/22/2022 12:36:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.343122 on epoch=53
05/22/2022 12:36:40 - INFO - __main__ - Global step 850 Train loss 0.346859 Classification-F1 0.3834004580273237 on epoch=53
05/22/2022 12:36:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.325379 on epoch=53
05/22/2022 12:36:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.338760 on epoch=54
05/22/2022 12:36:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.359304 on epoch=54
05/22/2022 12:37:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.325304 on epoch=55
05/22/2022 12:37:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.367690 on epoch=56
05/22/2022 12:37:10 - INFO - __main__ - Global step 900 Train loss 0.343288 Classification-F1 0.5142314990512334 on epoch=56
05/22/2022 12:37:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.357886 on epoch=56
05/22/2022 12:37:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.375650 on epoch=57
05/22/2022 12:37:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.349309 on epoch=58
05/22/2022 12:37:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.330621 on epoch=58
05/22/2022 12:37:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.348819 on epoch=59
05/22/2022 12:37:40 - INFO - __main__ - Global step 950 Train loss 0.352457 Classification-F1 0.6978864928655724 on epoch=59
05/22/2022 12:37:46 - INFO - __main__ - Step 960 Global step 960 Train loss 0.342997 on epoch=59
05/22/2022 12:37:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.316788 on epoch=60
05/22/2022 12:37:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.363949 on epoch=61
05/22/2022 12:38:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.345789 on epoch=61
05/22/2022 12:38:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.338899 on epoch=62
05/22/2022 12:38:08 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:38:08 - INFO - __main__ - Printing 3 examples
05/22/2022 12:38:08 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/22/2022 12:38:08 - INFO - __main__ - ['negative']
05/22/2022 12:38:08 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/22/2022 12:38:08 - INFO - __main__ - ['negative']
05/22/2022 12:38:08 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/22/2022 12:38:08 - INFO - __main__ - ['negative']
05/22/2022 12:38:08 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:38:08 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:38:09 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 12:38:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:38:09 - INFO - __main__ - Printing 3 examples
05/22/2022 12:38:09 - INFO - __main__ -  [amazon_polarity] title: a path of shadows [SEP] content: I'm glad a previous reviewer enjoyed this book and the previous one, but I did not. I too am a great fan of this series, but the last two books seemed flat to me--not very engaging. I certainly will continue to read Haney's books about Lieutenant Bak because when they are good(which they usually are) they are very very good.
05/22/2022 12:38:09 - INFO - __main__ - ['negative']
05/22/2022 12:38:09 - INFO - __main__ -  [amazon_polarity] title: WAY WAY WAY too much sex [SEP] content: I'm no prude, but when a book needs to rely on sex every page or 2 it gets really tedious. Please. It isn't necessary for the plot. The participants were married for heaven's sake.
05/22/2022 12:38:09 - INFO - __main__ - ['negative']
05/22/2022 12:38:09 - INFO - __main__ -  [amazon_polarity] title: Mind Game [SEP] content: I kept waiting for something to happen, the main character was anoying, things like this "she was so small she looked like a child" I ask just how sexy is that supposed to be and the guy was big etc etc. Nobe that this story gets a star is very generous of me.As I was reading this I kept thinking, is this over, somthing anything to bring this book around. (sigh) but we can't like them all.
05/22/2022 12:38:09 - INFO - __main__ - ['negative']
05/22/2022 12:38:09 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:38:09 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:38:09 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 12:38:11 - INFO - __main__ - Global step 1000 Train loss 0.341684 Classification-F1 0.6658448150833938 on epoch=62
05/22/2022 12:38:11 - INFO - __main__ - save last model!
05/22/2022 12:38:18 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 12:38:18 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 12:38:18 - INFO - __main__ - Printing 3 examples
05/22/2022 12:38:18 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 12:38:18 - INFO - __main__ - ['negative']
05/22/2022 12:38:18 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 12:38:18 - INFO - __main__ - ['negative']
05/22/2022 12:38:18 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 12:38:18 - INFO - __main__ - ['negative']
05/22/2022 12:38:18 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:38:19 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:38:20 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 12:38:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 12:38:22 - INFO - __main__ - Starting training!
05/22/2022 12:38:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_13_0.0002_8_predictions.txt
05/22/2022 12:38:35 - INFO - __main__ - Classification-F1 on test data: 0.7328
05/22/2022 12:38:35 - INFO - __main__ - prefix=amazon_polarity_128_13, lr=0.0002, bsz=8, dev_performance=0.6978864928655724, test_performance=0.7327872108934084
05/22/2022 12:38:35 - INFO - __main__ - Running ... prefix=amazon_polarity_128_13, lr=0.0001, bsz=8 ...
05/22/2022 12:38:36 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:38:36 - INFO - __main__ - Printing 3 examples
05/22/2022 12:38:36 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/22/2022 12:38:36 - INFO - __main__ - ['negative']
05/22/2022 12:38:36 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/22/2022 12:38:36 - INFO - __main__ - ['negative']
05/22/2022 12:38:36 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/22/2022 12:38:36 - INFO - __main__ - ['negative']
05/22/2022 12:38:36 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:38:36 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:38:37 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 12:38:37 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:38:37 - INFO - __main__ - Printing 3 examples
05/22/2022 12:38:37 - INFO - __main__ -  [amazon_polarity] title: a path of shadows [SEP] content: I'm glad a previous reviewer enjoyed this book and the previous one, but I did not. I too am a great fan of this series, but the last two books seemed flat to me--not very engaging. I certainly will continue to read Haney's books about Lieutenant Bak because when they are good(which they usually are) they are very very good.
05/22/2022 12:38:37 - INFO - __main__ - ['negative']
05/22/2022 12:38:37 - INFO - __main__ -  [amazon_polarity] title: WAY WAY WAY too much sex [SEP] content: I'm no prude, but when a book needs to rely on sex every page or 2 it gets really tedious. Please. It isn't necessary for the plot. The participants were married for heaven's sake.
05/22/2022 12:38:37 - INFO - __main__ - ['negative']
05/22/2022 12:38:37 - INFO - __main__ -  [amazon_polarity] title: Mind Game [SEP] content: I kept waiting for something to happen, the main character was anoying, things like this "she was so small she looked like a child" I ask just how sexy is that supposed to be and the guy was big etc etc. Nobe that this story gets a star is very generous of me.As I was reading this I kept thinking, is this over, somthing anything to bring this book around. (sigh) but we can't like them all.
05/22/2022 12:38:37 - INFO - __main__ - ['negative']
05/22/2022 12:38:37 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:38:37 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:38:37 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 12:38:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 12:38:48 - INFO - __main__ - Starting training!
05/22/2022 12:38:53 - INFO - __main__ - Step 10 Global step 10 Train loss 24.342745 on epoch=0
05/22/2022 12:38:58 - INFO - __main__ - Step 20 Global step 20 Train loss 18.941031 on epoch=1
05/22/2022 12:39:03 - INFO - __main__ - Step 30 Global step 30 Train loss 17.711590 on epoch=1
05/22/2022 12:39:08 - INFO - __main__ - Step 40 Global step 40 Train loss 17.417639 on epoch=2
05/22/2022 12:39:13 - INFO - __main__ - Step 50 Global step 50 Train loss 16.033295 on epoch=3
05/22/2022 12:40:20 - INFO - __main__ - Global step 50 Train loss 18.889259 Classification-F1 0.0 on epoch=3
05/22/2022 12:40:26 - INFO - __main__ - Step 60 Global step 60 Train loss 16.189333 on epoch=3
05/22/2022 12:40:31 - INFO - __main__ - Step 70 Global step 70 Train loss 15.330790 on epoch=4
05/22/2022 12:40:37 - INFO - __main__ - Step 80 Global step 80 Train loss 15.099974 on epoch=4
05/22/2022 12:40:42 - INFO - __main__ - Step 90 Global step 90 Train loss 14.951663 on epoch=5
05/22/2022 12:40:47 - INFO - __main__ - Step 100 Global step 100 Train loss 14.071118 on epoch=6
05/22/2022 12:41:50 - INFO - __main__ - Global step 100 Train loss 15.128574 Classification-F1 0.0 on epoch=6
05/22/2022 12:41:56 - INFO - __main__ - Step 110 Global step 110 Train loss 13.654508 on epoch=6
05/22/2022 12:42:01 - INFO - __main__ - Step 120 Global step 120 Train loss 13.753398 on epoch=7
05/22/2022 12:42:06 - INFO - __main__ - Step 130 Global step 130 Train loss 13.038736 on epoch=8
05/22/2022 12:42:11 - INFO - __main__ - Step 140 Global step 140 Train loss 13.191591 on epoch=8
05/22/2022 12:42:16 - INFO - __main__ - Step 150 Global step 150 Train loss 12.766891 on epoch=9
05/22/2022 12:43:06 - INFO - __main__ - Global step 150 Train loss 13.281025 Classification-F1 0.0 on epoch=9
05/22/2022 12:43:12 - INFO - __main__ - Step 160 Global step 160 Train loss 11.853357 on epoch=9
05/22/2022 12:43:17 - INFO - __main__ - Step 170 Global step 170 Train loss 10.921986 on epoch=10
05/22/2022 12:43:22 - INFO - __main__ - Step 180 Global step 180 Train loss 9.688601 on epoch=11
05/22/2022 12:43:27 - INFO - __main__ - Step 190 Global step 190 Train loss 6.794140 on epoch=11
05/22/2022 12:43:32 - INFO - __main__ - Step 200 Global step 200 Train loss 2.064239 on epoch=12
05/22/2022 12:43:36 - INFO - __main__ - Global step 200 Train loss 8.264464 Classification-F1 0.6623560445216496 on epoch=12
05/22/2022 12:43:42 - INFO - __main__ - Step 210 Global step 210 Train loss 1.609864 on epoch=13
05/22/2022 12:43:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.914574 on epoch=13
05/22/2022 12:43:52 - INFO - __main__ - Step 230 Global step 230 Train loss 1.225507 on epoch=14
05/22/2022 12:43:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.622975 on epoch=14
05/22/2022 12:44:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.565017 on epoch=15
05/22/2022 12:44:07 - INFO - __main__ - Global step 250 Train loss 0.987587 Classification-F1 0.9101219642502786 on epoch=15
05/22/2022 12:44:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.636644 on epoch=16
05/22/2022 12:44:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.605870 on epoch=16
05/22/2022 12:44:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.234174 on epoch=17
05/22/2022 12:44:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.358816 on epoch=18
05/22/2022 12:44:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.131447 on epoch=18
05/22/2022 12:44:37 - INFO - __main__ - Global step 300 Train loss 0.393390 Classification-F1 0.929648854961832 on epoch=18
05/22/2022 12:44:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.222763 on epoch=19
05/22/2022 12:44:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.317065 on epoch=19
05/22/2022 12:44:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.143933 on epoch=20
05/22/2022 12:44:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.130467 on epoch=21
05/22/2022 12:45:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.095829 on epoch=21
05/22/2022 12:45:08 - INFO - __main__ - Global step 350 Train loss 0.182012 Classification-F1 0.9218702313373619 on epoch=21
05/22/2022 12:45:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.303770 on epoch=22
05/22/2022 12:45:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.196389 on epoch=23
05/22/2022 12:45:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.170983 on epoch=23
05/22/2022 12:45:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.251382 on epoch=24
05/22/2022 12:45:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.153067 on epoch=24
05/22/2022 12:45:37 - INFO - __main__ - Global step 400 Train loss 0.215118 Classification-F1 0.9296187683284458 on epoch=24
05/22/2022 12:45:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.189109 on epoch=25
05/22/2022 12:45:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.071466 on epoch=26
05/22/2022 12:45:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.070070 on epoch=26
05/22/2022 12:45:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.082615 on epoch=27
05/22/2022 12:46:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.064111 on epoch=28
05/22/2022 12:46:08 - INFO - __main__ - Global step 450 Train loss 0.095474 Classification-F1 0.9179073709285812 on epoch=28
05/22/2022 12:46:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.065950 on epoch=28
05/22/2022 12:46:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.100322 on epoch=29
05/22/2022 12:46:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.063432 on epoch=29
05/22/2022 12:46:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.049321 on epoch=30
05/22/2022 12:46:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.039386 on epoch=31
05/22/2022 12:46:37 - INFO - __main__ - Global step 500 Train loss 0.063682 Classification-F1 0.9335846292368031 on epoch=31
05/22/2022 12:46:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.039993 on epoch=31
05/22/2022 12:46:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.061166 on epoch=32
05/22/2022 12:46:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.029300 on epoch=33
05/22/2022 12:46:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.024988 on epoch=33
05/22/2022 12:47:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.107153 on epoch=34
05/22/2022 12:47:08 - INFO - __main__ - Global step 550 Train loss 0.052520 Classification-F1 0.925689404934688 on epoch=34
05/22/2022 12:47:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.038689 on epoch=34
05/22/2022 12:47:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.292798 on epoch=35
05/22/2022 12:47:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.049839 on epoch=36
05/22/2022 12:47:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.060712 on epoch=36
05/22/2022 12:47:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.086405 on epoch=37
05/22/2022 12:47:38 - INFO - __main__ - Global step 600 Train loss 0.105689 Classification-F1 0.921755608533529 on epoch=37
05/22/2022 12:47:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.138143 on epoch=38
05/22/2022 12:47:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.012221 on epoch=38
05/22/2022 12:47:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.024897 on epoch=39
05/22/2022 12:47:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.017864 on epoch=39
05/22/2022 12:48:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.051913 on epoch=40
05/22/2022 12:48:08 - INFO - __main__ - Global step 650 Train loss 0.049008 Classification-F1 0.9413982022677675 on epoch=40
05/22/2022 12:48:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.018956 on epoch=41
05/22/2022 12:48:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.012453 on epoch=41
05/22/2022 12:48:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.106611 on epoch=42
05/22/2022 12:48:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.014189 on epoch=43
05/22/2022 12:48:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.016328 on epoch=43
05/22/2022 12:48:38 - INFO - __main__ - Global step 700 Train loss 0.033708 Classification-F1 0.9414053559166857 on epoch=43
05/22/2022 12:48:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.021269 on epoch=44
05/22/2022 12:48:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.016486 on epoch=44
05/22/2022 12:48:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.007754 on epoch=45
05/22/2022 12:49:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.004279 on epoch=46
05/22/2022 12:49:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.009301 on epoch=46
05/22/2022 12:49:09 - INFO - __main__ - Global step 750 Train loss 0.011818 Classification-F1 0.9374961850698895 on epoch=46
05/22/2022 12:49:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.018111 on epoch=47
05/22/2022 12:49:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.005549 on epoch=48
05/22/2022 12:49:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.003552 on epoch=48
05/22/2022 12:49:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.006818 on epoch=49
05/22/2022 12:49:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.012549 on epoch=49
05/22/2022 12:49:39 - INFO - __main__ - Global step 800 Train loss 0.009316 Classification-F1 0.9374961850698895 on epoch=49
05/22/2022 12:49:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.006101 on epoch=50
05/22/2022 12:49:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.014002 on epoch=51
05/22/2022 12:49:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.001486 on epoch=51
05/22/2022 12:50:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.005506 on epoch=52
05/22/2022 12:50:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.012404 on epoch=53
05/22/2022 12:50:09 - INFO - __main__ - Global step 850 Train loss 0.007900 Classification-F1 0.9453125 on epoch=53
05/22/2022 12:50:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.006156 on epoch=53
05/22/2022 12:50:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.036189 on epoch=54
05/22/2022 12:50:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001524 on epoch=54
05/22/2022 12:50:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001038 on epoch=55
05/22/2022 12:50:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.009404 on epoch=56
05/22/2022 12:50:40 - INFO - __main__ - Global step 900 Train loss 0.010862 Classification-F1 0.9531221388024171 on epoch=56
05/22/2022 12:50:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.001490 on epoch=56
05/22/2022 12:50:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.007631 on epoch=57
05/22/2022 12:50:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.009106 on epoch=58
05/22/2022 12:51:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.001404 on epoch=58
05/22/2022 12:51:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000579 on epoch=59
05/22/2022 12:51:11 - INFO - __main__ - Global step 950 Train loss 0.004042 Classification-F1 0.9452991452991453 on epoch=59
05/22/2022 12:51:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.055598 on epoch=59
05/22/2022 12:51:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.001993 on epoch=60
05/22/2022 12:51:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.004449 on epoch=61
05/22/2022 12:51:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.003629 on epoch=61
05/22/2022 12:51:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.007336 on epoch=62
05/22/2022 12:51:38 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:51:38 - INFO - __main__ - Printing 3 examples
05/22/2022 12:51:38 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/22/2022 12:51:38 - INFO - __main__ - ['positive']
05/22/2022 12:51:38 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/22/2022 12:51:38 - INFO - __main__ - ['positive']
05/22/2022 12:51:38 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/22/2022 12:51:38 - INFO - __main__ - ['positive']
05/22/2022 12:51:38 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:51:38 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:51:38 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 12:51:38 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:51:38 - INFO - __main__ - Printing 3 examples
05/22/2022 12:51:38 - INFO - __main__ -  [amazon_polarity] title: Funny, Poignant and Thought-provoking [SEP] content: "Pappy Jack" Holloway discovers a new breed of mammals on the planet Zarathustra. He feels they are sapient but the company that controls the planet, and will lose their rights to the planet if the "Fuzzies" are sapient, will stop at nothing to prove him wrong. A very good book, that I highly recommend.
05/22/2022 12:51:38 - INFO - __main__ - ['positive']
05/22/2022 12:51:38 - INFO - __main__ -  [amazon_polarity] title: One of the best cookbooks ever! [SEP] content: This is one of my favorite cookbooks, which I've used for about 20 years. In fact, I used it so much that it was falling apart so I ordered a new one for myself, and then one for a friend as a gift.What I like so much: It's arranged as complete meals, usually 2-3 recipes per meal. Then there are directions for what order to do things in, so everything gets done at about the same time. Mostly very healthy recipes, with calorie counts. And in addition to all that, every meal either takes less than 30 minutes, or you can make it ahead and freeze it.Marian Burros rocks!
05/22/2022 12:51:38 - INFO - __main__ - ['positive']
05/22/2022 12:51:38 - INFO - __main__ -  [amazon_polarity] title: Guitar delights from the Philippines [SEP] content: Tipanan - A Celebration of the Philippine GuitarYoung Filipino guitarist Florante Aguilar presents a delightful programme of popular music from the Philippines, some traditional, some well-loved popular music from the middle of the 20th century, and some more modern items of his own composition. Don't expect classical fireworks, just simple relaxing melodies played beautifully with great sensitivity. The more modern items are, to me at any rate, not so pleasing as Florante's interpretation of the older melodies and traditional music, very little of which is known outside of the Philippines. With his consumate musicianship and most attractive demeanour Florante makes a fine ambassador for the music of his homeland, and deserves to be known much better outside the Philippines.
05/22/2022 12:51:38 - INFO - __main__ - ['positive']
05/22/2022 12:51:38 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:51:39 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:51:39 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 12:51:40 - INFO - __main__ - Global step 1000 Train loss 0.014601 Classification-F1 0.9413982022677675 on epoch=62
05/22/2022 12:51:40 - INFO - __main__ - save last model!
05/22/2022 12:51:47 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 12:51:48 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 12:51:48 - INFO - __main__ - Printing 3 examples
05/22/2022 12:51:48 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 12:51:48 - INFO - __main__ - ['negative']
05/22/2022 12:51:48 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 12:51:48 - INFO - __main__ - ['negative']
05/22/2022 12:51:48 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 12:51:48 - INFO - __main__ - ['negative']
05/22/2022 12:51:48 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:51:49 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:51:50 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 12:51:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 12:51:52 - INFO - __main__ - Starting training!
05/22/2022 12:52:05 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_13_0.0001_8_predictions.txt
05/22/2022 12:52:05 - INFO - __main__ - Classification-F1 on test data: 0.9620
05/22/2022 12:52:05 - INFO - __main__ - prefix=amazon_polarity_128_13, lr=0.0001, bsz=8, dev_performance=0.9531221388024171, test_performance=0.961996199619962
05/22/2022 12:52:05 - INFO - __main__ - Running ... prefix=amazon_polarity_128_21, lr=0.0005, bsz=8 ...
05/22/2022 12:52:06 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:52:06 - INFO - __main__ - Printing 3 examples
05/22/2022 12:52:06 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/22/2022 12:52:06 - INFO - __main__ - ['positive']
05/22/2022 12:52:06 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/22/2022 12:52:06 - INFO - __main__ - ['positive']
05/22/2022 12:52:06 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/22/2022 12:52:06 - INFO - __main__ - ['positive']
05/22/2022 12:52:06 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:52:06 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:52:06 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 12:52:06 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 12:52:06 - INFO - __main__ - Printing 3 examples
05/22/2022 12:52:06 - INFO - __main__ -  [amazon_polarity] title: Funny, Poignant and Thought-provoking [SEP] content: "Pappy Jack" Holloway discovers a new breed of mammals on the planet Zarathustra. He feels they are sapient but the company that controls the planet, and will lose their rights to the planet if the "Fuzzies" are sapient, will stop at nothing to prove him wrong. A very good book, that I highly recommend.
05/22/2022 12:52:06 - INFO - __main__ - ['positive']
05/22/2022 12:52:06 - INFO - __main__ -  [amazon_polarity] title: One of the best cookbooks ever! [SEP] content: This is one of my favorite cookbooks, which I've used for about 20 years. In fact, I used it so much that it was falling apart so I ordered a new one for myself, and then one for a friend as a gift.What I like so much: It's arranged as complete meals, usually 2-3 recipes per meal. Then there are directions for what order to do things in, so everything gets done at about the same time. Mostly very healthy recipes, with calorie counts. And in addition to all that, every meal either takes less than 30 minutes, or you can make it ahead and freeze it.Marian Burros rocks!
05/22/2022 12:52:06 - INFO - __main__ - ['positive']
05/22/2022 12:52:06 - INFO - __main__ -  [amazon_polarity] title: Guitar delights from the Philippines [SEP] content: Tipanan - A Celebration of the Philippine GuitarYoung Filipino guitarist Florante Aguilar presents a delightful programme of popular music from the Philippines, some traditional, some well-loved popular music from the middle of the 20th century, and some more modern items of his own composition. Don't expect classical fireworks, just simple relaxing melodies played beautifully with great sensitivity. The more modern items are, to me at any rate, not so pleasing as Florante's interpretation of the older melodies and traditional music, very little of which is known outside of the Philippines. With his consumate musicianship and most attractive demeanour Florante makes a fine ambassador for the music of his homeland, and deserves to be known much better outside the Philippines.
05/22/2022 12:52:06 - INFO - __main__ - ['positive']
05/22/2022 12:52:06 - INFO - __main__ - Tokenizing Input ...
05/22/2022 12:52:07 - INFO - __main__ - Tokenizing Output ...
05/22/2022 12:52:07 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 12:52:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 12:52:18 - INFO - __main__ - Starting training!
05/22/2022 12:52:22 - INFO - __main__ - Step 10 Global step 10 Train loss 23.111982 on epoch=0
05/22/2022 12:52:27 - INFO - __main__ - Step 20 Global step 20 Train loss 16.837769 on epoch=1
05/22/2022 12:52:33 - INFO - __main__ - Step 30 Global step 30 Train loss 16.509806 on epoch=1
05/22/2022 12:52:38 - INFO - __main__ - Step 40 Global step 40 Train loss 13.253130 on epoch=2
05/22/2022 12:52:43 - INFO - __main__ - Step 50 Global step 50 Train loss 11.832850 on epoch=3
05/22/2022 12:52:47 - INFO - __main__ - Global step 50 Train loss 16.309107 Classification-F1 0.0 on epoch=3
05/22/2022 12:52:53 - INFO - __main__ - Step 60 Global step 60 Train loss 8.061682 on epoch=3
05/22/2022 12:52:58 - INFO - __main__ - Step 70 Global step 70 Train loss 4.972303 on epoch=4
05/22/2022 12:53:03 - INFO - __main__ - Step 80 Global step 80 Train loss 1.665475 on epoch=4
05/22/2022 12:53:08 - INFO - __main__ - Step 90 Global step 90 Train loss 0.461500 on epoch=5
05/22/2022 12:53:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.354392 on epoch=6
05/22/2022 12:53:17 - INFO - __main__ - Global step 100 Train loss 3.103070 Classification-F1 0.8581280788177339 on epoch=6
05/22/2022 12:53:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.237860 on epoch=6
05/22/2022 12:53:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.137716 on epoch=7
05/22/2022 12:53:33 - INFO - __main__ - Step 130 Global step 130 Train loss 0.038899 on epoch=8
05/22/2022 12:53:39 - INFO - __main__ - Step 140 Global step 140 Train loss 0.214627 on epoch=8
05/22/2022 12:53:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.076810 on epoch=9
05/22/2022 12:53:48 - INFO - __main__ - Global step 150 Train loss 0.141182 Classification-F1 0.9296703296703297 on epoch=9
05/22/2022 12:53:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.057257 on epoch=9
05/22/2022 12:54:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.569157 on epoch=10
05/22/2022 12:54:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.238741 on epoch=11
05/22/2022 12:54:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.469218 on epoch=11
05/22/2022 12:54:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.810477 on epoch=12
05/22/2022 12:54:21 - INFO - __main__ - Global step 200 Train loss 0.428970 Classification-F1 0.3333333333333333 on epoch=12
05/22/2022 12:54:26 - INFO - __main__ - Step 210 Global step 210 Train loss 1.724525 on epoch=13
05/22/2022 12:54:31 - INFO - __main__ - Step 220 Global step 220 Train loss 1.549143 on epoch=13
05/22/2022 12:54:36 - INFO - __main__ - Step 230 Global step 230 Train loss 1.333198 on epoch=14
05/22/2022 12:54:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.257055 on epoch=14
05/22/2022 12:54:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.060295 on epoch=15
05/22/2022 12:54:51 - INFO - __main__ - Global step 250 Train loss 0.984843 Classification-F1 0.9414053559166857 on epoch=15
05/22/2022 12:54:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.089139 on epoch=16
05/22/2022 12:55:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.067136 on epoch=16
05/22/2022 12:55:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.067501 on epoch=17
05/22/2022 12:55:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.024003 on epoch=18
05/22/2022 12:55:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.011154 on epoch=18
05/22/2022 12:55:22 - INFO - __main__ - Global step 300 Train loss 0.051787 Classification-F1 0.9492179751277943 on epoch=18
05/22/2022 12:55:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.009694 on epoch=19
05/22/2022 12:55:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.070652 on epoch=19
05/22/2022 12:55:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.024550 on epoch=20
05/22/2022 12:55:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.477610 on epoch=21
05/22/2022 12:55:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.110558 on epoch=21
05/22/2022 12:55:53 - INFO - __main__ - Global step 350 Train loss 0.138613 Classification-F1 0.6220238095238095 on epoch=21
05/22/2022 12:55:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.030753 on epoch=22
05/22/2022 12:56:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.016443 on epoch=23
05/22/2022 12:56:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.020460 on epoch=23
05/22/2022 12:56:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.001081 on epoch=24
05/22/2022 12:56:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.021393 on epoch=24
05/22/2022 12:56:23 - INFO - __main__ - Global step 400 Train loss 0.018026 Classification-F1 0.9452991452991453 on epoch=24
05/22/2022 12:56:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.014214 on epoch=25
05/22/2022 12:56:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.010681 on epoch=26
05/22/2022 12:56:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.014794 on epoch=26
05/22/2022 12:56:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.004315 on epoch=27
05/22/2022 12:56:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.001486 on epoch=28
05/22/2022 12:56:53 - INFO - __main__ - Global step 450 Train loss 0.009098 Classification-F1 0.9530992366412214 on epoch=28
05/22/2022 12:56:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.030653 on epoch=28
05/22/2022 12:57:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000664 on epoch=29
05/22/2022 12:57:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.006693 on epoch=29
05/22/2022 12:57:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002300 on epoch=30
05/22/2022 12:57:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.013310 on epoch=31
05/22/2022 12:57:23 - INFO - __main__ - Global step 500 Train loss 0.010724 Classification-F1 0.9452991452991453 on epoch=31
05/22/2022 12:57:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.050438 on epoch=31
05/22/2022 12:57:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003525 on epoch=32
05/22/2022 12:57:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.002533 on epoch=33
05/22/2022 12:57:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.004266 on epoch=33
05/22/2022 12:57:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.009314 on epoch=34
05/22/2022 12:57:53 - INFO - __main__ - Global step 550 Train loss 0.014015 Classification-F1 0.9335115728362997 on epoch=34
05/22/2022 12:57:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001385 on epoch=34
05/22/2022 12:58:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000604 on epoch=35
05/22/2022 12:58:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000435 on epoch=36
05/22/2022 12:58:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000883 on epoch=36
05/22/2022 12:58:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000655 on epoch=37
05/22/2022 12:58:22 - INFO - __main__ - Global step 600 Train loss 0.000792 Classification-F1 0.9452991452991453 on epoch=37
05/22/2022 12:58:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000147 on epoch=38
05/22/2022 12:58:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000385 on epoch=38
05/22/2022 12:58:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000139 on epoch=39
05/22/2022 12:58:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000151 on epoch=39
05/22/2022 12:58:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.002009 on epoch=40
05/22/2022 12:58:53 - INFO - __main__ - Global step 650 Train loss 0.000566 Classification-F1 0.9218320610687023 on epoch=40
05/22/2022 12:58:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000084 on epoch=41
05/22/2022 12:59:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000316 on epoch=41
05/22/2022 12:59:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000048 on epoch=42
05/22/2022 12:59:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000276 on epoch=43
05/22/2022 12:59:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.007377 on epoch=43
05/22/2022 12:59:22 - INFO - __main__ - Global step 700 Train loss 0.001620 Classification-F1 0.894064094900915 on epoch=43
05/22/2022 12:59:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.007480 on epoch=44
05/22/2022 12:59:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.012788 on epoch=44
05/22/2022 12:59:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.001149 on epoch=45
05/22/2022 12:59:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000428 on epoch=46
05/22/2022 12:59:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.002123 on epoch=46
05/22/2022 12:59:52 - INFO - __main__ - Global step 750 Train loss 0.004794 Classification-F1 0.9414053559166857 on epoch=46
05/22/2022 12:59:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000204 on epoch=47
05/22/2022 13:00:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000291 on epoch=48
05/22/2022 13:00:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.001056 on epoch=48
05/22/2022 13:00:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000236 on epoch=49
05/22/2022 13:00:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000080 on epoch=49
05/22/2022 13:00:22 - INFO - __main__ - Global step 800 Train loss 0.000374 Classification-F1 0.9452991452991453 on epoch=49
05/22/2022 13:00:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000405 on epoch=50
05/22/2022 13:00:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000033 on epoch=51
05/22/2022 13:00:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000541 on epoch=51
05/22/2022 13:00:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000764 on epoch=52
05/22/2022 13:00:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000048 on epoch=53
05/22/2022 13:00:52 - INFO - __main__ - Global step 850 Train loss 0.000358 Classification-F1 0.9413337407379114 on epoch=53
05/22/2022 13:00:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.443319 on epoch=53
05/22/2022 13:01:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.328555 on epoch=54
05/22/2022 13:01:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.046632 on epoch=54
05/22/2022 13:01:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.037930 on epoch=55
05/22/2022 13:01:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.070354 on epoch=56
05/22/2022 13:01:22 - INFO - __main__ - Global step 900 Train loss 0.185358 Classification-F1 0.9570148524675245 on epoch=56
05/22/2022 13:01:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.016135 on epoch=56
05/22/2022 13:01:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.024305 on epoch=57
05/22/2022 13:01:38 - INFO - __main__ - Step 930 Global step 930 Train loss 0.070564 on epoch=58
05/22/2022 13:01:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.091269 on epoch=58
05/22/2022 13:01:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.044518 on epoch=59
05/22/2022 13:01:52 - INFO - __main__ - Global step 950 Train loss 0.049358 Classification-F1 0.9175143843498275 on epoch=59
05/22/2022 13:01:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.017928 on epoch=59
05/22/2022 13:02:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.001070 on epoch=60
05/22/2022 13:02:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000439 on epoch=61
05/22/2022 13:02:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000714 on epoch=61
05/22/2022 13:02:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.023235 on epoch=62
05/22/2022 13:02:20 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:02:20 - INFO - __main__ - Printing 3 examples
05/22/2022 13:02:20 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/22/2022 13:02:20 - INFO - __main__ - ['positive']
05/22/2022 13:02:20 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/22/2022 13:02:20 - INFO - __main__ - ['positive']
05/22/2022 13:02:20 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/22/2022 13:02:20 - INFO - __main__ - ['positive']
05/22/2022 13:02:20 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:02:20 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:02:20 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 13:02:20 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:02:20 - INFO - __main__ - Printing 3 examples
05/22/2022 13:02:20 - INFO - __main__ -  [amazon_polarity] title: Funny, Poignant and Thought-provoking [SEP] content: "Pappy Jack" Holloway discovers a new breed of mammals on the planet Zarathustra. He feels they are sapient but the company that controls the planet, and will lose their rights to the planet if the "Fuzzies" are sapient, will stop at nothing to prove him wrong. A very good book, that I highly recommend.
05/22/2022 13:02:20 - INFO - __main__ - ['positive']
05/22/2022 13:02:20 - INFO - __main__ -  [amazon_polarity] title: One of the best cookbooks ever! [SEP] content: This is one of my favorite cookbooks, which I've used for about 20 years. In fact, I used it so much that it was falling apart so I ordered a new one for myself, and then one for a friend as a gift.What I like so much: It's arranged as complete meals, usually 2-3 recipes per meal. Then there are directions for what order to do things in, so everything gets done at about the same time. Mostly very healthy recipes, with calorie counts. And in addition to all that, every meal either takes less than 30 minutes, or you can make it ahead and freeze it.Marian Burros rocks!
05/22/2022 13:02:20 - INFO - __main__ - ['positive']
05/22/2022 13:02:20 - INFO - __main__ -  [amazon_polarity] title: Guitar delights from the Philippines [SEP] content: Tipanan - A Celebration of the Philippine GuitarYoung Filipino guitarist Florante Aguilar presents a delightful programme of popular music from the Philippines, some traditional, some well-loved popular music from the middle of the 20th century, and some more modern items of his own composition. Don't expect classical fireworks, just simple relaxing melodies played beautifully with great sensitivity. The more modern items are, to me at any rate, not so pleasing as Florante's interpretation of the older melodies and traditional music, very little of which is known outside of the Philippines. With his consumate musicianship and most attractive demeanour Florante makes a fine ambassador for the music of his homeland, and deserves to be known much better outside the Philippines.
05/22/2022 13:02:20 - INFO - __main__ - ['positive']
05/22/2022 13:02:20 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:02:20 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:02:20 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 13:02:22 - INFO - __main__ - Global step 1000 Train loss 0.008677 Classification-F1 0.9492179751277943 on epoch=62
05/22/2022 13:02:22 - INFO - __main__ - save last model!
05/22/2022 13:02:29 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 13:02:30 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 13:02:30 - INFO - __main__ - Printing 3 examples
05/22/2022 13:02:30 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 13:02:30 - INFO - __main__ - ['negative']
05/22/2022 13:02:30 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 13:02:30 - INFO - __main__ - ['negative']
05/22/2022 13:02:30 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 13:02:30 - INFO - __main__ - ['negative']
05/22/2022 13:02:30 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:02:30 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:02:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 13:02:31 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 13:02:31 - INFO - __main__ - Starting training!
05/22/2022 13:02:46 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_21_0.0005_8_predictions.txt
05/22/2022 13:02:47 - INFO - __main__ - Classification-F1 on test data: 0.9520
05/22/2022 13:02:47 - INFO - __main__ - prefix=amazon_polarity_128_21, lr=0.0005, bsz=8, dev_performance=0.9570148524675245, test_performance=0.9519982719377897
05/22/2022 13:02:47 - INFO - __main__ - Running ... prefix=amazon_polarity_128_21, lr=0.0003, bsz=8 ...
05/22/2022 13:02:48 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:02:48 - INFO - __main__ - Printing 3 examples
05/22/2022 13:02:48 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/22/2022 13:02:48 - INFO - __main__ - ['positive']
05/22/2022 13:02:48 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/22/2022 13:02:48 - INFO - __main__ - ['positive']
05/22/2022 13:02:48 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/22/2022 13:02:48 - INFO - __main__ - ['positive']
05/22/2022 13:02:48 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:02:48 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:02:48 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 13:02:48 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:02:48 - INFO - __main__ - Printing 3 examples
05/22/2022 13:02:48 - INFO - __main__ -  [amazon_polarity] title: Funny, Poignant and Thought-provoking [SEP] content: "Pappy Jack" Holloway discovers a new breed of mammals on the planet Zarathustra. He feels they are sapient but the company that controls the planet, and will lose their rights to the planet if the "Fuzzies" are sapient, will stop at nothing to prove him wrong. A very good book, that I highly recommend.
05/22/2022 13:02:48 - INFO - __main__ - ['positive']
05/22/2022 13:02:48 - INFO - __main__ -  [amazon_polarity] title: One of the best cookbooks ever! [SEP] content: This is one of my favorite cookbooks, which I've used for about 20 years. In fact, I used it so much that it was falling apart so I ordered a new one for myself, and then one for a friend as a gift.What I like so much: It's arranged as complete meals, usually 2-3 recipes per meal. Then there are directions for what order to do things in, so everything gets done at about the same time. Mostly very healthy recipes, with calorie counts. And in addition to all that, every meal either takes less than 30 minutes, or you can make it ahead and freeze it.Marian Burros rocks!
05/22/2022 13:02:48 - INFO - __main__ - ['positive']
05/22/2022 13:02:48 - INFO - __main__ -  [amazon_polarity] title: Guitar delights from the Philippines [SEP] content: Tipanan - A Celebration of the Philippine GuitarYoung Filipino guitarist Florante Aguilar presents a delightful programme of popular music from the Philippines, some traditional, some well-loved popular music from the middle of the 20th century, and some more modern items of his own composition. Don't expect classical fireworks, just simple relaxing melodies played beautifully with great sensitivity. The more modern items are, to me at any rate, not so pleasing as Florante's interpretation of the older melodies and traditional music, very little of which is known outside of the Philippines. With his consumate musicianship and most attractive demeanour Florante makes a fine ambassador for the music of his homeland, and deserves to be known much better outside the Philippines.
05/22/2022 13:02:48 - INFO - __main__ - ['positive']
05/22/2022 13:02:48 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:02:48 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:02:49 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 13:03:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 13:03:00 - INFO - __main__ - Starting training!
05/22/2022 13:03:04 - INFO - __main__ - Step 10 Global step 10 Train loss 22.658413 on epoch=0
05/22/2022 13:03:09 - INFO - __main__ - Step 20 Global step 20 Train loss 17.387867 on epoch=1
05/22/2022 13:03:14 - INFO - __main__ - Step 30 Global step 30 Train loss 15.810738 on epoch=1
05/22/2022 13:03:20 - INFO - __main__ - Step 40 Global step 40 Train loss 15.153737 on epoch=2
05/22/2022 13:03:25 - INFO - __main__ - Step 50 Global step 50 Train loss 13.443186 on epoch=3
05/22/2022 13:04:41 - INFO - __main__ - Global step 50 Train loss 16.890785 Classification-F1 6.666888896296543e-05 on epoch=3
05/22/2022 13:04:47 - INFO - __main__ - Step 60 Global step 60 Train loss 12.856503 on epoch=3
05/22/2022 13:04:52 - INFO - __main__ - Step 70 Global step 70 Train loss 11.084401 on epoch=4
05/22/2022 13:04:57 - INFO - __main__ - Step 80 Global step 80 Train loss 8.841731 on epoch=4
05/22/2022 13:05:02 - INFO - __main__ - Step 90 Global step 90 Train loss 1.186208 on epoch=5
05/22/2022 13:05:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.452473 on epoch=6
05/22/2022 13:05:11 - INFO - __main__ - Global step 100 Train loss 6.884263 Classification-F1 0.9062271062271061 on epoch=6
05/22/2022 13:05:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.175620 on epoch=6
05/22/2022 13:05:23 - INFO - __main__ - Step 120 Global step 120 Train loss 0.149363 on epoch=7
05/22/2022 13:05:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.247158 on epoch=8
05/22/2022 13:05:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.108846 on epoch=8
05/22/2022 13:05:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.096547 on epoch=9
05/22/2022 13:05:42 - INFO - __main__ - Global step 150 Train loss 0.155507 Classification-F1 0.9413838897284426 on epoch=9
05/22/2022 13:05:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.098572 on epoch=9
05/22/2022 13:05:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.070550 on epoch=10
05/22/2022 13:05:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.103607 on epoch=11
05/22/2022 13:06:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.048221 on epoch=11
05/22/2022 13:06:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.054161 on epoch=12
05/22/2022 13:06:13 - INFO - __main__ - Global step 200 Train loss 0.075022 Classification-F1 0.9413337407379114 on epoch=12
05/22/2022 13:06:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.069746 on epoch=13
05/22/2022 13:06:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.015483 on epoch=13
05/22/2022 13:06:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.331000 on epoch=14
05/22/2022 13:06:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.573748 on epoch=14
05/22/2022 13:06:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.393089 on epoch=15
05/22/2022 13:06:42 - INFO - __main__ - Global step 250 Train loss 0.276613 Classification-F1 0.6125214998280014 on epoch=15
05/22/2022 13:06:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.515180 on epoch=16
05/22/2022 13:06:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.362922 on epoch=16
05/22/2022 13:06:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.392019 on epoch=17
05/22/2022 13:07:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.334236 on epoch=18
05/22/2022 13:07:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.328360 on epoch=18
05/22/2022 13:07:12 - INFO - __main__ - Global step 300 Train loss 0.386543 Classification-F1 0.601328903654485 on epoch=18
05/22/2022 13:07:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.353044 on epoch=19
05/22/2022 13:07:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.276935 on epoch=19
05/22/2022 13:07:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.311282 on epoch=20
05/22/2022 13:07:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.242776 on epoch=21
05/22/2022 13:07:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.246162 on epoch=21
05/22/2022 13:07:42 - INFO - __main__ - Global step 350 Train loss 0.286040 Classification-F1 0.8617433301958122 on epoch=21
05/22/2022 13:07:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.119521 on epoch=22
05/22/2022 13:07:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.068652 on epoch=23
05/22/2022 13:07:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.034795 on epoch=23
05/22/2022 13:08:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.051730 on epoch=24
05/22/2022 13:08:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.035348 on epoch=24
05/22/2022 13:08:12 - INFO - __main__ - Global step 400 Train loss 0.062009 Classification-F1 0.953125 on epoch=24
05/22/2022 13:08:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.036396 on epoch=25
05/22/2022 13:08:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.002667 on epoch=26
05/22/2022 13:08:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.009111 on epoch=26
05/22/2022 13:08:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.043052 on epoch=27
05/22/2022 13:08:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.032897 on epoch=28
05/22/2022 13:08:43 - INFO - __main__ - Global step 450 Train loss 0.024824 Classification-F1 0.960935115668681 on epoch=28
05/22/2022 13:08:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.146453 on epoch=28
05/22/2022 13:08:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.152317 on epoch=29
05/22/2022 13:08:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.348590 on epoch=29
05/22/2022 13:09:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.322156 on epoch=30
05/22/2022 13:09:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.125632 on epoch=31
05/22/2022 13:09:13 - INFO - __main__ - Global step 500 Train loss 0.219029 Classification-F1 0.8322411533420707 on epoch=31
05/22/2022 13:09:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.147068 on epoch=31
05/22/2022 13:09:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.233778 on epoch=32
05/22/2022 13:09:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.155017 on epoch=33
05/22/2022 13:09:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.186770 on epoch=33
05/22/2022 13:09:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.312203 on epoch=34
05/22/2022 13:09:43 - INFO - __main__ - Global step 550 Train loss 0.206967 Classification-F1 0.7548008227738785 on epoch=34
05/22/2022 13:09:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.227840 on epoch=34
05/22/2022 13:09:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.258660 on epoch=35
05/22/2022 13:09:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.246585 on epoch=36
05/22/2022 13:10:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.247284 on epoch=36
05/22/2022 13:10:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.174200 on epoch=37
05/22/2022 13:10:13 - INFO - __main__ - Global step 600 Train loss 0.230914 Classification-F1 0.8783698867380876 on epoch=37
05/22/2022 13:10:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.247931 on epoch=38
05/22/2022 13:10:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.260435 on epoch=38
05/22/2022 13:10:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.144808 on epoch=39
05/22/2022 13:10:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.069358 on epoch=39
05/22/2022 13:10:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.045003 on epoch=40
05/22/2022 13:10:43 - INFO - __main__ - Global step 650 Train loss 0.153507 Classification-F1 0.9453091619361533 on epoch=40
05/22/2022 13:10:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.091061 on epoch=41
05/22/2022 13:10:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.074073 on epoch=41
05/22/2022 13:10:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.292372 on epoch=42
05/22/2022 13:11:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.274126 on epoch=43
05/22/2022 13:11:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.272800 on epoch=43
05/22/2022 13:11:12 - INFO - __main__ - Global step 700 Train loss 0.200887 Classification-F1 0.5327369657540435 on epoch=43
05/22/2022 13:11:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.293895 on epoch=44
05/22/2022 13:11:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.265097 on epoch=44
05/22/2022 13:11:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.323352 on epoch=45
05/22/2022 13:11:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.255195 on epoch=46
05/22/2022 13:11:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.384048 on epoch=46
05/22/2022 13:11:42 - INFO - __main__ - Global step 750 Train loss 0.304317 Classification-F1 0.42752983181433807 on epoch=46
05/22/2022 13:11:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.398618 on epoch=47
05/22/2022 13:11:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.296086 on epoch=48
05/22/2022 13:11:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.315105 on epoch=48
05/22/2022 13:12:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.325256 on epoch=49
05/22/2022 13:12:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.292714 on epoch=49
05/22/2022 13:12:12 - INFO - __main__ - Global step 800 Train loss 0.325556 Classification-F1 0.34195559333697656 on epoch=49
05/22/2022 13:12:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.300263 on epoch=50
05/22/2022 13:12:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.309721 on epoch=51
05/22/2022 13:12:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.255427 on epoch=51
05/22/2022 13:12:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.342149 on epoch=52
05/22/2022 13:12:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.264079 on epoch=53
05/22/2022 13:12:42 - INFO - __main__ - Global step 850 Train loss 0.294328 Classification-F1 0.5724858051927505 on epoch=53
05/22/2022 13:12:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.325601 on epoch=53
05/22/2022 13:12:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.253256 on epoch=54
05/22/2022 13:12:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.256871 on epoch=54
05/22/2022 13:13:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.310993 on epoch=55
05/22/2022 13:13:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.228111 on epoch=56
05/22/2022 13:13:12 - INFO - __main__ - Global step 900 Train loss 0.274966 Classification-F1 0.36516753625488524 on epoch=56
05/22/2022 13:13:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.273799 on epoch=56
05/22/2022 13:13:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.292516 on epoch=57
05/22/2022 13:13:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.264908 on epoch=58
05/22/2022 13:13:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.270847 on epoch=58
05/22/2022 13:13:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.289999 on epoch=59
05/22/2022 13:13:41 - INFO - __main__ - Global step 950 Train loss 0.278414 Classification-F1 0.45827145827145827 on epoch=59
05/22/2022 13:13:46 - INFO - __main__ - Step 960 Global step 960 Train loss 0.231444 on epoch=59
05/22/2022 13:13:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.262073 on epoch=60
05/22/2022 13:13:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.234822 on epoch=61
05/22/2022 13:14:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.247089 on epoch=61
05/22/2022 13:14:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.235865 on epoch=62
05/22/2022 13:14:08 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:14:08 - INFO - __main__ - Printing 3 examples
05/22/2022 13:14:08 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/22/2022 13:14:08 - INFO - __main__ - ['positive']
05/22/2022 13:14:08 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/22/2022 13:14:08 - INFO - __main__ - ['positive']
05/22/2022 13:14:08 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/22/2022 13:14:08 - INFO - __main__ - ['positive']
05/22/2022 13:14:08 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:14:08 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:14:09 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 13:14:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:14:09 - INFO - __main__ - Printing 3 examples
05/22/2022 13:14:09 - INFO - __main__ -  [amazon_polarity] title: Funny, Poignant and Thought-provoking [SEP] content: "Pappy Jack" Holloway discovers a new breed of mammals on the planet Zarathustra. He feels they are sapient but the company that controls the planet, and will lose their rights to the planet if the "Fuzzies" are sapient, will stop at nothing to prove him wrong. A very good book, that I highly recommend.
05/22/2022 13:14:09 - INFO - __main__ - ['positive']
05/22/2022 13:14:09 - INFO - __main__ -  [amazon_polarity] title: One of the best cookbooks ever! [SEP] content: This is one of my favorite cookbooks, which I've used for about 20 years. In fact, I used it so much that it was falling apart so I ordered a new one for myself, and then one for a friend as a gift.What I like so much: It's arranged as complete meals, usually 2-3 recipes per meal. Then there are directions for what order to do things in, so everything gets done at about the same time. Mostly very healthy recipes, with calorie counts. And in addition to all that, every meal either takes less than 30 minutes, or you can make it ahead and freeze it.Marian Burros rocks!
05/22/2022 13:14:09 - INFO - __main__ - ['positive']
05/22/2022 13:14:09 - INFO - __main__ -  [amazon_polarity] title: Guitar delights from the Philippines [SEP] content: Tipanan - A Celebration of the Philippine GuitarYoung Filipino guitarist Florante Aguilar presents a delightful programme of popular music from the Philippines, some traditional, some well-loved popular music from the middle of the 20th century, and some more modern items of his own composition. Don't expect classical fireworks, just simple relaxing melodies played beautifully with great sensitivity. The more modern items are, to me at any rate, not so pleasing as Florante's interpretation of the older melodies and traditional music, very little of which is known outside of the Philippines. With his consumate musicianship and most attractive demeanour Florante makes a fine ambassador for the music of his homeland, and deserves to be known much better outside the Philippines.
05/22/2022 13:14:09 - INFO - __main__ - ['positive']
05/22/2022 13:14:09 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:14:09 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:14:09 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 13:14:11 - INFO - __main__ - Global step 1000 Train loss 0.242259 Classification-F1 0.787192118226601 on epoch=62
05/22/2022 13:14:11 - INFO - __main__ - save last model!
05/22/2022 13:14:18 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 13:14:19 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 13:14:19 - INFO - __main__ - Printing 3 examples
05/22/2022 13:14:19 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 13:14:19 - INFO - __main__ - ['negative']
05/22/2022 13:14:19 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 13:14:19 - INFO - __main__ - ['negative']
05/22/2022 13:14:19 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 13:14:19 - INFO - __main__ - ['negative']
05/22/2022 13:14:19 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:14:19 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:14:20 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 13:14:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 13:14:22 - INFO - __main__ - Starting training!
05/22/2022 13:14:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_21_0.0003_8_predictions.txt
05/22/2022 13:14:35 - INFO - __main__ - Classification-F1 on test data: 0.9480
05/22/2022 13:14:36 - INFO - __main__ - prefix=amazon_polarity_128_21, lr=0.0003, bsz=8, dev_performance=0.960935115668681, test_performance=0.9479748198127895
05/22/2022 13:14:36 - INFO - __main__ - Running ... prefix=amazon_polarity_128_21, lr=0.0002, bsz=8 ...
05/22/2022 13:14:36 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:14:36 - INFO - __main__ - Printing 3 examples
05/22/2022 13:14:36 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/22/2022 13:14:36 - INFO - __main__ - ['positive']
05/22/2022 13:14:36 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/22/2022 13:14:36 - INFO - __main__ - ['positive']
05/22/2022 13:14:36 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/22/2022 13:14:36 - INFO - __main__ - ['positive']
05/22/2022 13:14:36 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:14:37 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:14:37 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 13:14:37 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:14:37 - INFO - __main__ - Printing 3 examples
05/22/2022 13:14:37 - INFO - __main__ -  [amazon_polarity] title: Funny, Poignant and Thought-provoking [SEP] content: "Pappy Jack" Holloway discovers a new breed of mammals on the planet Zarathustra. He feels they are sapient but the company that controls the planet, and will lose their rights to the planet if the "Fuzzies" are sapient, will stop at nothing to prove him wrong. A very good book, that I highly recommend.
05/22/2022 13:14:37 - INFO - __main__ - ['positive']
05/22/2022 13:14:37 - INFO - __main__ -  [amazon_polarity] title: One of the best cookbooks ever! [SEP] content: This is one of my favorite cookbooks, which I've used for about 20 years. In fact, I used it so much that it was falling apart so I ordered a new one for myself, and then one for a friend as a gift.What I like so much: It's arranged as complete meals, usually 2-3 recipes per meal. Then there are directions for what order to do things in, so everything gets done at about the same time. Mostly very healthy recipes, with calorie counts. And in addition to all that, every meal either takes less than 30 minutes, or you can make it ahead and freeze it.Marian Burros rocks!
05/22/2022 13:14:37 - INFO - __main__ - ['positive']
05/22/2022 13:14:37 - INFO - __main__ -  [amazon_polarity] title: Guitar delights from the Philippines [SEP] content: Tipanan - A Celebration of the Philippine GuitarYoung Filipino guitarist Florante Aguilar presents a delightful programme of popular music from the Philippines, some traditional, some well-loved popular music from the middle of the 20th century, and some more modern items of his own composition. Don't expect classical fireworks, just simple relaxing melodies played beautifully with great sensitivity. The more modern items are, to me at any rate, not so pleasing as Florante's interpretation of the older melodies and traditional music, very little of which is known outside of the Philippines. With his consumate musicianship and most attractive demeanour Florante makes a fine ambassador for the music of his homeland, and deserves to be known much better outside the Philippines.
05/22/2022 13:14:37 - INFO - __main__ - ['positive']
05/22/2022 13:14:37 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:14:37 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:14:37 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 13:14:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 13:14:48 - INFO - __main__ - Starting training!
05/22/2022 13:14:53 - INFO - __main__ - Step 10 Global step 10 Train loss 23.733322 on epoch=0
05/22/2022 13:14:58 - INFO - __main__ - Step 20 Global step 20 Train loss 17.064602 on epoch=1
05/22/2022 13:15:03 - INFO - __main__ - Step 30 Global step 30 Train loss 17.319220 on epoch=1
05/22/2022 13:15:08 - INFO - __main__ - Step 40 Global step 40 Train loss 15.946531 on epoch=2
05/22/2022 13:15:14 - INFO - __main__ - Step 50 Global step 50 Train loss 15.319385 on epoch=3
05/22/2022 13:15:50 - INFO - __main__ - Global step 50 Train loss 17.876612 Classification-F1 0.0 on epoch=3
05/22/2022 13:15:56 - INFO - __main__ - Step 60 Global step 60 Train loss 13.411730 on epoch=3
05/22/2022 13:16:01 - INFO - __main__ - Step 70 Global step 70 Train loss 13.611218 on epoch=4
05/22/2022 13:16:06 - INFO - __main__ - Step 80 Global step 80 Train loss 11.828562 on epoch=4
05/22/2022 13:16:11 - INFO - __main__ - Step 90 Global step 90 Train loss 10.733568 on epoch=5
05/22/2022 13:16:17 - INFO - __main__ - Step 100 Global step 100 Train loss 10.374080 on epoch=6
05/22/2022 13:16:28 - INFO - __main__ - Global step 100 Train loss 11.991833 Classification-F1 0.0030888491353607632 on epoch=6
05/22/2022 13:16:34 - INFO - __main__ - Step 110 Global step 110 Train loss 5.912659 on epoch=6
05/22/2022 13:16:39 - INFO - __main__ - Step 120 Global step 120 Train loss 3.092333 on epoch=7
05/22/2022 13:16:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.378152 on epoch=8
05/22/2022 13:16:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.359957 on epoch=8
05/22/2022 13:16:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.206562 on epoch=9
05/22/2022 13:16:58 - INFO - __main__ - Global step 150 Train loss 1.989933 Classification-F1 0.8856280137418928 on epoch=9
05/22/2022 13:17:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.238902 on epoch=9
05/22/2022 13:17:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.228934 on epoch=10
05/22/2022 13:17:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.062713 on epoch=11
05/22/2022 13:17:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.107662 on epoch=11
05/22/2022 13:17:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.056982 on epoch=12
05/22/2022 13:17:29 - INFO - __main__ - Global step 200 Train loss 0.139039 Classification-F1 0.9570253483296962 on epoch=12
05/22/2022 13:17:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.136374 on epoch=13
05/22/2022 13:17:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.101018 on epoch=13
05/22/2022 13:17:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.062004 on epoch=14
05/22/2022 13:17:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.082491 on epoch=14
05/22/2022 13:17:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.061928 on epoch=15
05/22/2022 13:17:59 - INFO - __main__ - Global step 250 Train loss 0.088763 Classification-F1 0.960927960927961 on epoch=15
05/22/2022 13:18:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.013677 on epoch=16
05/22/2022 13:18:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.106038 on epoch=16
05/22/2022 13:18:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.027623 on epoch=17
05/22/2022 13:18:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.005034 on epoch=18
05/22/2022 13:18:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.014253 on epoch=18
05/22/2022 13:18:29 - INFO - __main__ - Global step 300 Train loss 0.033325 Classification-F1 0.9570253483296962 on epoch=18
05/22/2022 13:18:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.035870 on epoch=19
05/22/2022 13:18:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.113691 on epoch=19
05/22/2022 13:18:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.092092 on epoch=20
05/22/2022 13:18:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.034103 on epoch=21
05/22/2022 13:18:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.081387 on epoch=21
05/22/2022 13:18:59 - INFO - __main__ - Global step 350 Train loss 0.071429 Classification-F1 0.9570305943389029 on epoch=21
05/22/2022 13:19:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.023588 on epoch=22
05/22/2022 13:19:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.005955 on epoch=23
05/22/2022 13:19:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.029006 on epoch=23
05/22/2022 13:19:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.032518 on epoch=24
05/22/2022 13:19:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.012535 on epoch=24
05/22/2022 13:19:28 - INFO - __main__ - Global step 400 Train loss 0.020720 Classification-F1 0.9687423687423687 on epoch=24
05/22/2022 13:19:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.035005 on epoch=25
05/22/2022 13:19:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000670 on epoch=26
05/22/2022 13:19:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.006838 on epoch=26
05/22/2022 13:19:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.051882 on epoch=27
05/22/2022 13:19:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.008225 on epoch=28
05/22/2022 13:19:58 - INFO - __main__ - Global step 450 Train loss 0.020524 Classification-F1 0.9570253483296962 on epoch=28
05/22/2022 13:20:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.069465 on epoch=28
05/22/2022 13:20:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.002434 on epoch=29
05/22/2022 13:20:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.007956 on epoch=29
05/22/2022 13:20:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001154 on epoch=30
05/22/2022 13:20:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000394 on epoch=31
05/22/2022 13:20:28 - INFO - __main__ - Global step 500 Train loss 0.016280 Classification-F1 0.9570253483296962 on epoch=31
05/22/2022 13:20:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.171158 on epoch=31
05/22/2022 13:20:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.021690 on epoch=32
05/22/2022 13:20:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.020600 on epoch=33
05/22/2022 13:20:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.002334 on epoch=33
05/22/2022 13:20:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.005698 on epoch=34
05/22/2022 13:20:57 - INFO - __main__ - Global step 550 Train loss 0.044296 Classification-F1 0.9648389213606605 on epoch=34
05/22/2022 13:21:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.139055 on epoch=34
05/22/2022 13:21:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.021879 on epoch=35
05/22/2022 13:21:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.002671 on epoch=36
05/22/2022 13:21:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.025622 on epoch=36
05/22/2022 13:21:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.009602 on epoch=37
05/22/2022 13:21:26 - INFO - __main__ - Global step 600 Train loss 0.039766 Classification-F1 0.960935115668681 on epoch=37
05/22/2022 13:21:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.001192 on epoch=38
05/22/2022 13:21:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.001983 on epoch=38
05/22/2022 13:21:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000300 on epoch=39
05/22/2022 13:21:47 - INFO - __main__ - Step 640 Global step 640 Train loss 0.078861 on epoch=39
05/22/2022 13:21:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001410 on epoch=40
05/22/2022 13:21:56 - INFO - __main__ - Global step 650 Train loss 0.016749 Classification-F1 0.960935115668681 on epoch=40
05/22/2022 13:22:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000351 on epoch=41
05/22/2022 13:22:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.062518 on epoch=41
05/22/2022 13:22:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.011253 on epoch=42
05/22/2022 13:22:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.005777 on epoch=43
05/22/2022 13:22:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.202062 on epoch=43
05/22/2022 13:22:25 - INFO - __main__ - Global step 700 Train loss 0.056392 Classification-F1 0.9491993710979836 on epoch=43
05/22/2022 13:22:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.098686 on epoch=44
05/22/2022 13:22:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.075372 on epoch=44
05/22/2022 13:22:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.343380 on epoch=45
05/22/2022 13:22:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.424164 on epoch=46
05/22/2022 13:22:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.157418 on epoch=46
05/22/2022 13:22:55 - INFO - __main__ - Global step 750 Train loss 0.219804 Classification-F1 0.898132843587389 on epoch=46
05/22/2022 13:23:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.195248 on epoch=47
05/22/2022 13:23:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.186391 on epoch=48
05/22/2022 13:23:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.231153 on epoch=48
05/22/2022 13:23:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.309238 on epoch=49
05/22/2022 13:23:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.249934 on epoch=49
05/22/2022 13:23:24 - INFO - __main__ - Global step 800 Train loss 0.234393 Classification-F1 0.8533178474641889 on epoch=49
05/22/2022 13:23:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.275205 on epoch=50
05/22/2022 13:23:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.294958 on epoch=51
05/22/2022 13:23:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.312636 on epoch=51
05/22/2022 13:23:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.265356 on epoch=52
05/22/2022 13:23:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.236423 on epoch=53
05/22/2022 13:23:54 - INFO - __main__ - Global step 850 Train loss 0.276916 Classification-F1 0.8593406593406593 on epoch=53
05/22/2022 13:23:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.254532 on epoch=53
05/22/2022 13:24:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.228637 on epoch=54
05/22/2022 13:24:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.146366 on epoch=54
05/22/2022 13:24:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.209112 on epoch=55
05/22/2022 13:24:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.108581 on epoch=56
05/22/2022 13:24:23 - INFO - __main__ - Global step 900 Train loss 0.189446 Classification-F1 0.9374656488549619 on epoch=56
05/22/2022 13:24:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.144878 on epoch=56
05/22/2022 13:24:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.133377 on epoch=57
05/22/2022 13:24:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.226005 on epoch=58
05/22/2022 13:24:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.222171 on epoch=58
05/22/2022 13:24:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.180563 on epoch=59
05/22/2022 13:24:53 - INFO - __main__ - Global step 950 Train loss 0.181399 Classification-F1 0.7844161014224249 on epoch=59
05/22/2022 13:24:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.153421 on epoch=59
05/22/2022 13:25:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.170880 on epoch=60
05/22/2022 13:25:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.108532 on epoch=61
05/22/2022 13:25:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.192364 on epoch=61
05/22/2022 13:25:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.273586 on epoch=62
05/22/2022 13:25:20 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:25:20 - INFO - __main__ - Printing 3 examples
05/22/2022 13:25:20 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/22/2022 13:25:20 - INFO - __main__ - ['positive']
05/22/2022 13:25:20 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/22/2022 13:25:20 - INFO - __main__ - ['positive']
05/22/2022 13:25:20 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/22/2022 13:25:20 - INFO - __main__ - ['positive']
05/22/2022 13:25:20 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:25:20 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:25:21 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 13:25:21 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:25:21 - INFO - __main__ - Printing 3 examples
05/22/2022 13:25:21 - INFO - __main__ -  [amazon_polarity] title: Funny, Poignant and Thought-provoking [SEP] content: "Pappy Jack" Holloway discovers a new breed of mammals on the planet Zarathustra. He feels they are sapient but the company that controls the planet, and will lose their rights to the planet if the "Fuzzies" are sapient, will stop at nothing to prove him wrong. A very good book, that I highly recommend.
05/22/2022 13:25:21 - INFO - __main__ - ['positive']
05/22/2022 13:25:21 - INFO - __main__ -  [amazon_polarity] title: One of the best cookbooks ever! [SEP] content: This is one of my favorite cookbooks, which I've used for about 20 years. In fact, I used it so much that it was falling apart so I ordered a new one for myself, and then one for a friend as a gift.What I like so much: It's arranged as complete meals, usually 2-3 recipes per meal. Then there are directions for what order to do things in, so everything gets done at about the same time. Mostly very healthy recipes, with calorie counts. And in addition to all that, every meal either takes less than 30 minutes, or you can make it ahead and freeze it.Marian Burros rocks!
05/22/2022 13:25:21 - INFO - __main__ - ['positive']
05/22/2022 13:25:21 - INFO - __main__ -  [amazon_polarity] title: Guitar delights from the Philippines [SEP] content: Tipanan - A Celebration of the Philippine GuitarYoung Filipino guitarist Florante Aguilar presents a delightful programme of popular music from the Philippines, some traditional, some well-loved popular music from the middle of the 20th century, and some more modern items of his own composition. Don't expect classical fireworks, just simple relaxing melodies played beautifully with great sensitivity. The more modern items are, to me at any rate, not so pleasing as Florante's interpretation of the older melodies and traditional music, very little of which is known outside of the Philippines. With his consumate musicianship and most attractive demeanour Florante makes a fine ambassador for the music of his homeland, and deserves to be known much better outside the Philippines.
05/22/2022 13:25:21 - INFO - __main__ - ['positive']
05/22/2022 13:25:21 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:25:21 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:25:21 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 13:25:23 - INFO - __main__ - Global step 1000 Train loss 0.179756 Classification-F1 0.6391225152451097 on epoch=62
05/22/2022 13:25:23 - INFO - __main__ - save last model!
05/22/2022 13:25:30 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 13:25:31 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 13:25:31 - INFO - __main__ - Printing 3 examples
05/22/2022 13:25:31 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 13:25:31 - INFO - __main__ - ['negative']
05/22/2022 13:25:31 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 13:25:31 - INFO - __main__ - ['negative']
05/22/2022 13:25:31 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 13:25:31 - INFO - __main__ - ['negative']
05/22/2022 13:25:31 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:25:31 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:25:32 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 13:25:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 13:25:34 - INFO - __main__ - Starting training!
05/22/2022 13:25:47 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_21_0.0002_8_predictions.txt
05/22/2022 13:25:47 - INFO - __main__ - Classification-F1 on test data: 0.9530
05/22/2022 13:25:47 - INFO - __main__ - prefix=amazon_polarity_128_21, lr=0.0002, bsz=8, dev_performance=0.9687423687423687, test_performance=0.9529830268727011
05/22/2022 13:25:47 - INFO - __main__ - Running ... prefix=amazon_polarity_128_21, lr=0.0001, bsz=8 ...
05/22/2022 13:25:48 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:25:48 - INFO - __main__ - Printing 3 examples
05/22/2022 13:25:48 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/22/2022 13:25:48 - INFO - __main__ - ['positive']
05/22/2022 13:25:48 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/22/2022 13:25:48 - INFO - __main__ - ['positive']
05/22/2022 13:25:48 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/22/2022 13:25:48 - INFO - __main__ - ['positive']
05/22/2022 13:25:48 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:25:49 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:25:49 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 13:25:49 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:25:49 - INFO - __main__ - Printing 3 examples
05/22/2022 13:25:49 - INFO - __main__ -  [amazon_polarity] title: Funny, Poignant and Thought-provoking [SEP] content: "Pappy Jack" Holloway discovers a new breed of mammals on the planet Zarathustra. He feels they are sapient but the company that controls the planet, and will lose their rights to the planet if the "Fuzzies" are sapient, will stop at nothing to prove him wrong. A very good book, that I highly recommend.
05/22/2022 13:25:49 - INFO - __main__ - ['positive']
05/22/2022 13:25:49 - INFO - __main__ -  [amazon_polarity] title: One of the best cookbooks ever! [SEP] content: This is one of my favorite cookbooks, which I've used for about 20 years. In fact, I used it so much that it was falling apart so I ordered a new one for myself, and then one for a friend as a gift.What I like so much: It's arranged as complete meals, usually 2-3 recipes per meal. Then there are directions for what order to do things in, so everything gets done at about the same time. Mostly very healthy recipes, with calorie counts. And in addition to all that, every meal either takes less than 30 minutes, or you can make it ahead and freeze it.Marian Burros rocks!
05/22/2022 13:25:49 - INFO - __main__ - ['positive']
05/22/2022 13:25:49 - INFO - __main__ -  [amazon_polarity] title: Guitar delights from the Philippines [SEP] content: Tipanan - A Celebration of the Philippine GuitarYoung Filipino guitarist Florante Aguilar presents a delightful programme of popular music from the Philippines, some traditional, some well-loved popular music from the middle of the 20th century, and some more modern items of his own composition. Don't expect classical fireworks, just simple relaxing melodies played beautifully with great sensitivity. The more modern items are, to me at any rate, not so pleasing as Florante's interpretation of the older melodies and traditional music, very little of which is known outside of the Philippines. With his consumate musicianship and most attractive demeanour Florante makes a fine ambassador for the music of his homeland, and deserves to be known much better outside the Philippines.
05/22/2022 13:25:49 - INFO - __main__ - ['positive']
05/22/2022 13:25:49 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:25:49 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:25:49 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 13:26:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 13:26:02 - INFO - __main__ - Starting training!
05/22/2022 13:26:07 - INFO - __main__ - Step 10 Global step 10 Train loss 21.926107 on epoch=0
05/22/2022 13:26:12 - INFO - __main__ - Step 20 Global step 20 Train loss 19.537006 on epoch=1
05/22/2022 13:26:17 - INFO - __main__ - Step 30 Global step 30 Train loss 18.343849 on epoch=1
05/22/2022 13:26:22 - INFO - __main__ - Step 40 Global step 40 Train loss 17.165289 on epoch=2
05/22/2022 13:26:28 - INFO - __main__ - Step 50 Global step 50 Train loss 16.840303 on epoch=3
05/22/2022 13:27:09 - INFO - __main__ - Global step 50 Train loss 18.762510 Classification-F1 0.0 on epoch=3
05/22/2022 13:27:15 - INFO - __main__ - Step 60 Global step 60 Train loss 16.334173 on epoch=3
05/22/2022 13:27:20 - INFO - __main__ - Step 70 Global step 70 Train loss 15.487585 on epoch=4
05/22/2022 13:27:26 - INFO - __main__ - Step 80 Global step 80 Train loss 15.515004 on epoch=4
05/22/2022 13:27:31 - INFO - __main__ - Step 90 Global step 90 Train loss 14.902341 on epoch=5
05/22/2022 13:27:36 - INFO - __main__ - Step 100 Global step 100 Train loss 14.561701 on epoch=6
05/22/2022 13:27:55 - INFO - __main__ - Global step 100 Train loss 15.360160 Classification-F1 0.0 on epoch=6
05/22/2022 13:28:00 - INFO - __main__ - Step 110 Global step 110 Train loss 14.082594 on epoch=6
05/22/2022 13:28:06 - INFO - __main__ - Step 120 Global step 120 Train loss 13.894107 on epoch=7
05/22/2022 13:28:11 - INFO - __main__ - Step 130 Global step 130 Train loss 13.568380 on epoch=8
05/22/2022 13:28:17 - INFO - __main__ - Step 140 Global step 140 Train loss 12.606871 on epoch=8
05/22/2022 13:28:22 - INFO - __main__ - Step 150 Global step 150 Train loss 12.716108 on epoch=9
05/22/2022 13:28:32 - INFO - __main__ - Global step 150 Train loss 13.373612 Classification-F1 0.0 on epoch=9
05/22/2022 13:28:37 - INFO - __main__ - Step 160 Global step 160 Train loss 12.458753 on epoch=9
05/22/2022 13:28:42 - INFO - __main__ - Step 170 Global step 170 Train loss 10.900981 on epoch=10
05/22/2022 13:28:47 - INFO - __main__ - Step 180 Global step 180 Train loss 10.583385 on epoch=11
05/22/2022 13:28:53 - INFO - __main__ - Step 190 Global step 190 Train loss 9.885721 on epoch=11
05/22/2022 13:28:58 - INFO - __main__ - Step 200 Global step 200 Train loss 8.189267 on epoch=12
05/22/2022 13:29:06 - INFO - __main__ - Global step 200 Train loss 10.403622 Classification-F1 0.0 on epoch=12
05/22/2022 13:29:11 - INFO - __main__ - Step 210 Global step 210 Train loss 6.185091 on epoch=13
05/22/2022 13:29:16 - INFO - __main__ - Step 220 Global step 220 Train loss 5.225177 on epoch=13
05/22/2022 13:29:22 - INFO - __main__ - Step 230 Global step 230 Train loss 4.646325 on epoch=14
05/22/2022 13:29:27 - INFO - __main__ - Step 240 Global step 240 Train loss 2.288947 on epoch=14
05/22/2022 13:29:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.750647 on epoch=15
05/22/2022 13:29:36 - INFO - __main__ - Global step 250 Train loss 3.819238 Classification-F1 0.9452991452991453 on epoch=15
05/22/2022 13:29:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.923910 on epoch=16
05/22/2022 13:29:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.569817 on epoch=16
05/22/2022 13:29:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.786759 on epoch=17
05/22/2022 13:29:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.443050 on epoch=18
05/22/2022 13:30:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.197595 on epoch=18
05/22/2022 13:30:07 - INFO - __main__ - Global step 300 Train loss 0.584226 Classification-F1 0.9491559086395234 on epoch=18
05/22/2022 13:30:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.271156 on epoch=19
05/22/2022 13:30:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.127538 on epoch=19
05/22/2022 13:30:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.203415 on epoch=20
05/22/2022 13:30:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.240880 on epoch=21
05/22/2022 13:30:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.101412 on epoch=21
05/22/2022 13:30:38 - INFO - __main__ - Global step 350 Train loss 0.188880 Classification-F1 0.9452824427480916 on epoch=21
05/22/2022 13:30:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.151590 on epoch=22
05/22/2022 13:30:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.054311 on epoch=23
05/22/2022 13:30:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.139848 on epoch=23
05/22/2022 13:30:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.120922 on epoch=24
05/22/2022 13:31:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.064738 on epoch=24
05/22/2022 13:31:08 - INFO - __main__ - Global step 400 Train loss 0.106282 Classification-F1 0.9531135531135531 on epoch=24
05/22/2022 13:31:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.086127 on epoch=25
05/22/2022 13:31:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.092342 on epoch=26
05/22/2022 13:31:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.166018 on epoch=26
05/22/2022 13:31:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.097306 on epoch=27
05/22/2022 13:31:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.077468 on epoch=28
05/22/2022 13:31:39 - INFO - __main__ - Global step 450 Train loss 0.103852 Classification-F1 0.9609160305343512 on epoch=28
05/22/2022 13:31:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.123947 on epoch=28
05/22/2022 13:31:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.044514 on epoch=29
05/22/2022 13:31:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.058057 on epoch=29
05/22/2022 13:32:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.023434 on epoch=30
05/22/2022 13:32:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.025313 on epoch=31
05/22/2022 13:32:09 - INFO - __main__ - Global step 500 Train loss 0.055053 Classification-F1 0.9531221388024171 on epoch=31
05/22/2022 13:32:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.074933 on epoch=31
05/22/2022 13:32:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.107244 on epoch=32
05/22/2022 13:32:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.035935 on epoch=33
05/22/2022 13:32:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.027577 on epoch=33
05/22/2022 13:32:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.014293 on epoch=34
05/22/2022 13:32:39 - INFO - __main__ - Global step 550 Train loss 0.051996 Classification-F1 0.960935115668681 on epoch=34
05/22/2022 13:32:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.053608 on epoch=34
05/22/2022 13:32:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.003887 on epoch=35
05/22/2022 13:32:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.022984 on epoch=36
05/22/2022 13:33:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.031963 on epoch=36
05/22/2022 13:33:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.009955 on epoch=37
05/22/2022 13:33:10 - INFO - __main__ - Global step 600 Train loss 0.024480 Classification-F1 0.9570253483296962 on epoch=37
05/22/2022 13:33:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.006332 on epoch=38
05/22/2022 13:33:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.019348 on epoch=38
05/22/2022 13:33:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.042525 on epoch=39
05/22/2022 13:33:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.010393 on epoch=39
05/22/2022 13:33:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.004005 on epoch=40
05/22/2022 13:33:39 - INFO - __main__ - Global step 650 Train loss 0.016521 Classification-F1 0.9648432135500115 on epoch=40
05/22/2022 13:33:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.002324 on epoch=41
05/22/2022 13:33:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.018772 on epoch=41
05/22/2022 13:33:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.005658 on epoch=42
05/22/2022 13:34:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.005533 on epoch=43
05/22/2022 13:34:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.003395 on epoch=43
05/22/2022 13:34:10 - INFO - __main__ - Global step 700 Train loss 0.007136 Classification-F1 0.9570253483296962 on epoch=43
05/22/2022 13:34:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.003161 on epoch=44
05/22/2022 13:34:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.007052 on epoch=44
05/22/2022 13:34:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.004798 on epoch=45
05/22/2022 13:34:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.003378 on epoch=46
05/22/2022 13:34:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.005726 on epoch=46
05/22/2022 13:34:40 - INFO - __main__ - Global step 750 Train loss 0.004823 Classification-F1 0.9609375 on epoch=46
05/22/2022 13:34:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.006438 on epoch=47
05/22/2022 13:34:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.005407 on epoch=48
05/22/2022 13:34:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.005058 on epoch=48
05/22/2022 13:35:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000788 on epoch=49
05/22/2022 13:35:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.014756 on epoch=49
05/22/2022 13:35:10 - INFO - __main__ - Global step 800 Train loss 0.006489 Classification-F1 0.9648389213606605 on epoch=49
05/22/2022 13:35:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.007872 on epoch=50
05/22/2022 13:35:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.014205 on epoch=51
05/22/2022 13:35:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.109216 on epoch=51
05/22/2022 13:35:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.018213 on epoch=52
05/22/2022 13:35:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.008737 on epoch=53
05/22/2022 13:35:40 - INFO - __main__ - Global step 850 Train loss 0.031648 Classification-F1 0.9570305943389029 on epoch=53
05/22/2022 13:35:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.002145 on epoch=53
05/22/2022 13:35:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.006307 on epoch=54
05/22/2022 13:35:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000825 on epoch=54
05/22/2022 13:36:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001963 on epoch=55
05/22/2022 13:36:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000204 on epoch=56
05/22/2022 13:36:10 - INFO - __main__ - Global step 900 Train loss 0.002289 Classification-F1 0.960935115668681 on epoch=56
05/22/2022 13:36:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.001107 on epoch=56
05/22/2022 13:36:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.001980 on epoch=57
05/22/2022 13:36:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.001366 on epoch=58
05/22/2022 13:36:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.014911 on epoch=58
05/22/2022 13:36:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.001067 on epoch=59
05/22/2022 13:36:40 - INFO - __main__ - Global step 950 Train loss 0.004086 Classification-F1 0.9570253483296962 on epoch=59
05/22/2022 13:36:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.015013 on epoch=59
05/22/2022 13:36:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.020305 on epoch=60
05/22/2022 13:36:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.001552 on epoch=61
05/22/2022 13:37:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.018517 on epoch=61
05/22/2022 13:37:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.006057 on epoch=62
05/22/2022 13:37:07 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:37:07 - INFO - __main__ - Printing 3 examples
05/22/2022 13:37:07 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/22/2022 13:37:07 - INFO - __main__ - ['negative']
05/22/2022 13:37:07 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/22/2022 13:37:07 - INFO - __main__ - ['negative']
05/22/2022 13:37:07 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/22/2022 13:37:07 - INFO - __main__ - ['negative']
05/22/2022 13:37:07 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:37:07 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:37:08 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 13:37:08 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:37:08 - INFO - __main__ - Printing 3 examples
05/22/2022 13:37:08 - INFO - __main__ -  [amazon_polarity] title: cheap quality [SEP] content: The metal that was used for this necklace looks cheap and left stains on my skin. I returned the product immediately but I have not received my refund yet after a month.
05/22/2022 13:37:08 - INFO - __main__ - ['negative']
05/22/2022 13:37:08 - INFO - __main__ -  [amazon_polarity] title: Awful. [SEP] content: Why can't Weezer release another cool album like "Pinkerton" or "The blue album".This is even worse than their last "green" album that contained the annoying hash pipe.Do not buy this, its crap!
05/22/2022 13:37:08 - INFO - __main__ - ['negative']
05/22/2022 13:37:08 - INFO - __main__ -  [amazon_polarity] title: Not what you think it is! [SEP] content: Although this documentary may be very good...it is NOT a colorized version of the original Sink the Bismarck movie! We bought it as a gift for someone who had asked for the movie and were VERY disappointed!
05/22/2022 13:37:08 - INFO - __main__ - ['negative']
05/22/2022 13:37:08 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:37:08 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:37:08 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 13:37:10 - INFO - __main__ - Global step 1000 Train loss 0.012289 Classification-F1 0.960935115668681 on epoch=62
05/22/2022 13:37:10 - INFO - __main__ - save last model!
05/22/2022 13:37:17 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 13:37:17 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 13:37:17 - INFO - __main__ - Printing 3 examples
05/22/2022 13:37:17 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 13:37:17 - INFO - __main__ - ['negative']
05/22/2022 13:37:17 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 13:37:17 - INFO - __main__ - ['negative']
05/22/2022 13:37:17 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 13:37:17 - INFO - __main__ - ['negative']
05/22/2022 13:37:17 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:37:18 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:37:19 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 13:37:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 13:37:21 - INFO - __main__ - Starting training!
05/22/2022 13:37:34 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_21_0.0001_8_predictions.txt
05/22/2022 13:37:34 - INFO - __main__ - Classification-F1 on test data: 0.9560
05/22/2022 13:37:34 - INFO - __main__ - prefix=amazon_polarity_128_21, lr=0.0001, bsz=8, dev_performance=0.9648432135500115, test_performance=0.9559992959887358
05/22/2022 13:37:34 - INFO - __main__ - Running ... prefix=amazon_polarity_128_42, lr=0.0005, bsz=8 ...
05/22/2022 13:37:35 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:37:35 - INFO - __main__ - Printing 3 examples
05/22/2022 13:37:35 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/22/2022 13:37:35 - INFO - __main__ - ['negative']
05/22/2022 13:37:35 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/22/2022 13:37:35 - INFO - __main__ - ['negative']
05/22/2022 13:37:35 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/22/2022 13:37:35 - INFO - __main__ - ['negative']
05/22/2022 13:37:35 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:37:35 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:37:35 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 13:37:35 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:37:35 - INFO - __main__ - Printing 3 examples
05/22/2022 13:37:35 - INFO - __main__ -  [amazon_polarity] title: cheap quality [SEP] content: The metal that was used for this necklace looks cheap and left stains on my skin. I returned the product immediately but I have not received my refund yet after a month.
05/22/2022 13:37:35 - INFO - __main__ - ['negative']
05/22/2022 13:37:35 - INFO - __main__ -  [amazon_polarity] title: Awful. [SEP] content: Why can't Weezer release another cool album like "Pinkerton" or "The blue album".This is even worse than their last "green" album that contained the annoying hash pipe.Do not buy this, its crap!
05/22/2022 13:37:35 - INFO - __main__ - ['negative']
05/22/2022 13:37:35 - INFO - __main__ -  [amazon_polarity] title: Not what you think it is! [SEP] content: Although this documentary may be very good...it is NOT a colorized version of the original Sink the Bismarck movie! We bought it as a gift for someone who had asked for the movie and were VERY disappointed!
05/22/2022 13:37:35 - INFO - __main__ - ['negative']
05/22/2022 13:37:35 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:37:36 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:37:36 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 13:37:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 13:37:47 - INFO - __main__ - Starting training!
05/22/2022 13:37:52 - INFO - __main__ - Step 10 Global step 10 Train loss 23.264471 on epoch=0
05/22/2022 13:37:57 - INFO - __main__ - Step 20 Global step 20 Train loss 18.043818 on epoch=1
05/22/2022 13:38:02 - INFO - __main__ - Step 30 Global step 30 Train loss 14.943977 on epoch=1
05/22/2022 13:38:07 - INFO - __main__ - Step 40 Global step 40 Train loss 13.607309 on epoch=2
05/22/2022 13:38:12 - INFO - __main__ - Step 50 Global step 50 Train loss 11.529192 on epoch=3
05/22/2022 13:38:15 - INFO - __main__ - Global step 50 Train loss 16.277752 Classification-F1 0.0 on epoch=3
05/22/2022 13:38:21 - INFO - __main__ - Step 60 Global step 60 Train loss 9.152909 on epoch=3
05/22/2022 13:38:26 - INFO - __main__ - Step 70 Global step 70 Train loss 4.719117 on epoch=4
05/22/2022 13:38:31 - INFO - __main__ - Step 80 Global step 80 Train loss 2.068048 on epoch=4
05/22/2022 13:38:36 - INFO - __main__ - Step 90 Global step 90 Train loss 1.648475 on epoch=5
05/22/2022 13:38:42 - INFO - __main__ - Step 100 Global step 100 Train loss 1.441796 on epoch=6
05/22/2022 13:38:45 - INFO - __main__ - Global step 100 Train loss 3.806069 Classification-F1 0.412425019769707 on epoch=6
05/22/2022 13:38:51 - INFO - __main__ - Step 110 Global step 110 Train loss 1.521625 on epoch=6
05/22/2022 13:38:57 - INFO - __main__ - Step 120 Global step 120 Train loss 1.134752 on epoch=7
05/22/2022 13:39:02 - INFO - __main__ - Step 130 Global step 130 Train loss 2.045021 on epoch=8
05/22/2022 13:39:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.823414 on epoch=8
05/22/2022 13:39:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.337634 on epoch=9
05/22/2022 13:39:16 - INFO - __main__ - Global step 150 Train loss 1.172489 Classification-F1 0.7644520591408085 on epoch=9
05/22/2022 13:39:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.387351 on epoch=9
05/22/2022 13:39:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.284130 on epoch=10
05/22/2022 13:39:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.268620 on epoch=11
05/22/2022 13:39:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.214255 on epoch=11
05/22/2022 13:39:42 - INFO - __main__ - Step 200 Global step 200 Train loss 0.220724 on epoch=12
05/22/2022 13:39:46 - INFO - __main__ - Global step 200 Train loss 0.275016 Classification-F1 0.9452991452991453 on epoch=12
05/22/2022 13:39:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.158868 on epoch=13
05/22/2022 13:39:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.079022 on epoch=13
05/22/2022 13:40:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.263396 on epoch=14
05/22/2022 13:40:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.271504 on epoch=14
05/22/2022 13:40:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.363678 on epoch=15
05/22/2022 13:40:16 - INFO - __main__ - Global step 250 Train loss 0.227293 Classification-F1 0.41470975742075483 on epoch=15
05/22/2022 13:40:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.503283 on epoch=16
05/22/2022 13:40:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.346296 on epoch=16
05/22/2022 13:40:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.318013 on epoch=17
05/22/2022 13:40:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.311974 on epoch=18
05/22/2022 13:40:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.128379 on epoch=18
05/22/2022 13:40:46 - INFO - __main__ - Global step 300 Train loss 0.321589 Classification-F1 0.9452991452991453 on epoch=18
05/22/2022 13:40:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.050885 on epoch=19
05/22/2022 13:40:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.068077 on epoch=19
05/22/2022 13:41:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.306877 on epoch=20
05/22/2022 13:41:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.327549 on epoch=21
05/22/2022 13:41:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.275580 on epoch=21
05/22/2022 13:41:14 - INFO - __main__ - Global step 350 Train loss 0.205794 Classification-F1 0.6180480526830272 on epoch=21
05/22/2022 13:41:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.387452 on epoch=22
05/22/2022 13:41:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.354043 on epoch=23
05/22/2022 13:41:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.289929 on epoch=23
05/22/2022 13:41:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.316702 on epoch=24
05/22/2022 13:41:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.331114 on epoch=24
05/22/2022 13:41:44 - INFO - __main__ - Global step 400 Train loss 0.335848 Classification-F1 0.48409044687263114 on epoch=24
05/22/2022 13:41:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.321889 on epoch=25
05/22/2022 13:41:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.288266 on epoch=26
05/22/2022 13:41:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.237054 on epoch=26
05/22/2022 13:42:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.340068 on epoch=27
05/22/2022 13:42:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.355845 on epoch=28
05/22/2022 13:42:13 - INFO - __main__ - Global step 450 Train loss 0.308624 Classification-F1 0.8032343356084364 on epoch=28
05/22/2022 13:42:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.190307 on epoch=28
05/22/2022 13:42:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.192615 on epoch=29
05/22/2022 13:42:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.232513 on epoch=29
05/22/2022 13:42:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.226495 on epoch=30
05/22/2022 13:42:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.251604 on epoch=31
05/22/2022 13:42:43 - INFO - __main__ - Global step 500 Train loss 0.218707 Classification-F1 0.8082397003745319 on epoch=31
05/22/2022 13:42:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.210030 on epoch=31
05/22/2022 13:42:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.243610 on epoch=32
05/22/2022 13:42:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.233730 on epoch=33
05/22/2022 13:43:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.170396 on epoch=33
05/22/2022 13:43:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.243005 on epoch=34
05/22/2022 13:43:12 - INFO - __main__ - Global step 550 Train loss 0.220154 Classification-F1 0.7327935222672065 on epoch=34
05/22/2022 13:43:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.224954 on epoch=34
05/22/2022 13:43:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.170246 on epoch=35
05/22/2022 13:43:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.150362 on epoch=36
05/22/2022 13:43:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.166075 on epoch=36
05/22/2022 13:43:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.167985 on epoch=37
05/22/2022 13:43:42 - INFO - __main__ - Global step 600 Train loss 0.175924 Classification-F1 0.7839662447257384 on epoch=37
05/22/2022 13:43:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.087870 on epoch=38
05/22/2022 13:43:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.093696 on epoch=38
05/22/2022 13:43:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.249093 on epoch=39
05/22/2022 13:44:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.158366 on epoch=39
05/22/2022 13:44:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.123209 on epoch=40
05/22/2022 13:44:11 - INFO - __main__ - Global step 650 Train loss 0.142447 Classification-F1 0.8117647058823529 on epoch=40
05/22/2022 13:44:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.109409 on epoch=41
05/22/2022 13:44:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.143785 on epoch=41
05/22/2022 13:44:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.166698 on epoch=42
05/22/2022 13:44:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.073559 on epoch=43
05/22/2022 13:44:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.059729 on epoch=43
05/22/2022 13:44:41 - INFO - __main__ - Global step 700 Train loss 0.110636 Classification-F1 0.777177149663292 on epoch=43
05/22/2022 13:44:46 - INFO - __main__ - Step 710 Global step 710 Train loss 1.053281 on epoch=44
05/22/2022 13:44:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.144207 on epoch=44
05/22/2022 13:44:56 - INFO - __main__ - Step 730 Global step 730 Train loss 0.147455 on epoch=45
05/22/2022 13:45:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.514902 on epoch=46
05/22/2022 13:45:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.364336 on epoch=46
05/22/2022 13:45:11 - INFO - __main__ - Global step 750 Train loss 0.444836 Classification-F1 0.736986301369863 on epoch=46
05/22/2022 13:45:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.198694 on epoch=47
05/22/2022 13:45:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.319605 on epoch=48
05/22/2022 13:45:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.136650 on epoch=48
05/22/2022 13:45:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.148105 on epoch=49
05/22/2022 13:45:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.167494 on epoch=49
05/22/2022 13:45:40 - INFO - __main__ - Global step 800 Train loss 0.194110 Classification-F1 0.6705912629479509 on epoch=49
05/22/2022 13:45:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.124380 on epoch=50
05/22/2022 13:45:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.208726 on epoch=51
05/22/2022 13:45:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.133382 on epoch=51
05/22/2022 13:46:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.135468 on epoch=52
05/22/2022 13:46:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.114530 on epoch=53
05/22/2022 13:46:10 - INFO - __main__ - Global step 850 Train loss 0.143297 Classification-F1 0.7497319457584358 on epoch=53
05/22/2022 13:46:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.153975 on epoch=53
05/22/2022 13:46:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.152237 on epoch=54
05/22/2022 13:46:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.129720 on epoch=54
05/22/2022 13:46:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.106225 on epoch=55
05/22/2022 13:46:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.095452 on epoch=56
05/22/2022 13:46:39 - INFO - __main__ - Global step 900 Train loss 0.127522 Classification-F1 0.7326974924805585 on epoch=56
05/22/2022 13:46:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.070053 on epoch=56
05/22/2022 13:46:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.153113 on epoch=57
05/22/2022 13:46:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.090461 on epoch=58
05/22/2022 13:47:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.119289 on epoch=58
05/22/2022 13:47:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.072350 on epoch=59
05/22/2022 13:47:09 - INFO - __main__ - Global step 950 Train loss 0.101053 Classification-F1 0.8007052250767046 on epoch=59
05/22/2022 13:47:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.086983 on epoch=59
05/22/2022 13:47:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.049952 on epoch=60
05/22/2022 13:47:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.085609 on epoch=61
05/22/2022 13:47:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.094075 on epoch=61
05/22/2022 13:47:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.062541 on epoch=62
05/22/2022 13:47:37 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:47:37 - INFO - __main__ - Printing 3 examples
05/22/2022 13:47:37 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/22/2022 13:47:37 - INFO - __main__ - ['negative']
05/22/2022 13:47:37 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/22/2022 13:47:37 - INFO - __main__ - ['negative']
05/22/2022 13:47:37 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/22/2022 13:47:37 - INFO - __main__ - ['negative']
05/22/2022 13:47:37 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:47:37 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:47:37 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 13:47:37 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:47:37 - INFO - __main__ - Printing 3 examples
05/22/2022 13:47:37 - INFO - __main__ -  [amazon_polarity] title: cheap quality [SEP] content: The metal that was used for this necklace looks cheap and left stains on my skin. I returned the product immediately but I have not received my refund yet after a month.
05/22/2022 13:47:37 - INFO - __main__ - ['negative']
05/22/2022 13:47:37 - INFO - __main__ -  [amazon_polarity] title: Awful. [SEP] content: Why can't Weezer release another cool album like "Pinkerton" or "The blue album".This is even worse than their last "green" album that contained the annoying hash pipe.Do not buy this, its crap!
05/22/2022 13:47:37 - INFO - __main__ - ['negative']
05/22/2022 13:47:37 - INFO - __main__ -  [amazon_polarity] title: Not what you think it is! [SEP] content: Although this documentary may be very good...it is NOT a colorized version of the original Sink the Bismarck movie! We bought it as a gift for someone who had asked for the movie and were VERY disappointed!
05/22/2022 13:47:37 - INFO - __main__ - ['negative']
05/22/2022 13:47:37 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:47:37 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:47:38 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 13:47:39 - INFO - __main__ - Global step 1000 Train loss 0.075832 Classification-F1 0.7534638086362224 on epoch=62
05/22/2022 13:47:39 - INFO - __main__ - save last model!
05/22/2022 13:47:46 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 13:47:47 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 13:47:47 - INFO - __main__ - Printing 3 examples
05/22/2022 13:47:47 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 13:47:47 - INFO - __main__ - ['negative']
05/22/2022 13:47:47 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 13:47:47 - INFO - __main__ - ['negative']
05/22/2022 13:47:47 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 13:47:47 - INFO - __main__ - ['negative']
05/22/2022 13:47:47 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:47:47 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:47:48 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 13:47:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 13:47:49 - INFO - __main__ - Starting training!
05/22/2022 13:48:03 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_42_0.0005_8_predictions.txt
05/22/2022 13:48:03 - INFO - __main__ - Classification-F1 on test data: 0.9350
05/22/2022 13:48:04 - INFO - __main__ - prefix=amazon_polarity_128_42, lr=0.0005, bsz=8, dev_performance=0.9452991452991453, test_performance=0.9349890131432212
05/22/2022 13:48:04 - INFO - __main__ - Running ... prefix=amazon_polarity_128_42, lr=0.0003, bsz=8 ...
05/22/2022 13:48:05 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:48:05 - INFO - __main__ - Printing 3 examples
05/22/2022 13:48:05 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/22/2022 13:48:05 - INFO - __main__ - ['negative']
05/22/2022 13:48:05 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/22/2022 13:48:05 - INFO - __main__ - ['negative']
05/22/2022 13:48:05 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/22/2022 13:48:05 - INFO - __main__ - ['negative']
05/22/2022 13:48:05 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:48:05 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:48:05 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 13:48:05 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:48:05 - INFO - __main__ - Printing 3 examples
05/22/2022 13:48:05 - INFO - __main__ -  [amazon_polarity] title: cheap quality [SEP] content: The metal that was used for this necklace looks cheap and left stains on my skin. I returned the product immediately but I have not received my refund yet after a month.
05/22/2022 13:48:05 - INFO - __main__ - ['negative']
05/22/2022 13:48:05 - INFO - __main__ -  [amazon_polarity] title: Awful. [SEP] content: Why can't Weezer release another cool album like "Pinkerton" or "The blue album".This is even worse than their last "green" album that contained the annoying hash pipe.Do not buy this, its crap!
05/22/2022 13:48:05 - INFO - __main__ - ['negative']
05/22/2022 13:48:05 - INFO - __main__ -  [amazon_polarity] title: Not what you think it is! [SEP] content: Although this documentary may be very good...it is NOT a colorized version of the original Sink the Bismarck movie! We bought it as a gift for someone who had asked for the movie and were VERY disappointed!
05/22/2022 13:48:05 - INFO - __main__ - ['negative']
05/22/2022 13:48:05 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:48:05 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:48:05 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 13:48:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 13:48:18 - INFO - __main__ - Starting training!
05/22/2022 13:48:23 - INFO - __main__ - Step 10 Global step 10 Train loss 22.907839 on epoch=0
05/22/2022 13:48:28 - INFO - __main__ - Step 20 Global step 20 Train loss 18.048229 on epoch=1
05/22/2022 13:48:33 - INFO - __main__ - Step 30 Global step 30 Train loss 16.307632 on epoch=1
05/22/2022 13:48:39 - INFO - __main__ - Step 40 Global step 40 Train loss 16.375423 on epoch=2
05/22/2022 13:48:44 - INFO - __main__ - Step 50 Global step 50 Train loss 13.612780 on epoch=3
05/22/2022 13:48:49 - INFO - __main__ - Global step 50 Train loss 17.450380 Classification-F1 0.0 on epoch=3
05/22/2022 13:48:55 - INFO - __main__ - Step 60 Global step 60 Train loss 13.537497 on epoch=3
05/22/2022 13:49:00 - INFO - __main__ - Step 70 Global step 70 Train loss 12.392191 on epoch=4
05/22/2022 13:49:05 - INFO - __main__ - Step 80 Global step 80 Train loss 10.856233 on epoch=4
05/22/2022 13:49:10 - INFO - __main__ - Step 90 Global step 90 Train loss 9.349854 on epoch=5
05/22/2022 13:49:15 - INFO - __main__ - Step 100 Global step 100 Train loss 6.033034 on epoch=6
05/22/2022 13:49:19 - INFO - __main__ - Global step 100 Train loss 10.433763 Classification-F1 0.005128205128205128 on epoch=6
05/22/2022 13:49:25 - INFO - __main__ - Step 110 Global step 110 Train loss 2.073972 on epoch=6
05/22/2022 13:49:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.821374 on epoch=7
05/22/2022 13:49:35 - INFO - __main__ - Step 130 Global step 130 Train loss 0.423850 on epoch=8
05/22/2022 13:49:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.367809 on epoch=8
05/22/2022 13:49:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.319892 on epoch=9
05/22/2022 13:49:49 - INFO - __main__ - Global step 150 Train loss 0.801379 Classification-F1 0.9335684083589015 on epoch=9
05/22/2022 13:49:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.316206 on epoch=9
05/22/2022 13:49:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.180508 on epoch=10
05/22/2022 13:50:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.195022 on epoch=11
05/22/2022 13:50:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.095110 on epoch=11
05/22/2022 13:50:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.129270 on epoch=12
05/22/2022 13:50:19 - INFO - __main__ - Global step 200 Train loss 0.183223 Classification-F1 0.9648432135500115 on epoch=12
05/22/2022 13:50:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.111823 on epoch=13
05/22/2022 13:50:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.102891 on epoch=13
05/22/2022 13:50:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.127157 on epoch=14
05/22/2022 13:50:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.064277 on epoch=14
05/22/2022 13:50:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.079996 on epoch=15
05/22/2022 13:50:49 - INFO - __main__ - Global step 250 Train loss 0.097229 Classification-F1 0.9530791788856304 on epoch=15
05/22/2022 13:50:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.050363 on epoch=16
05/22/2022 13:50:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.045927 on epoch=16
05/22/2022 13:51:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.045509 on epoch=17
05/22/2022 13:51:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.053510 on epoch=18
05/22/2022 13:51:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.037886 on epoch=18
05/22/2022 13:51:18 - INFO - __main__ - Global step 300 Train loss 0.046639 Classification-F1 0.9648432135500115 on epoch=18
05/22/2022 13:51:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.024227 on epoch=19
05/22/2022 13:51:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.106998 on epoch=19
05/22/2022 13:51:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.057914 on epoch=20
05/22/2022 13:51:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.021456 on epoch=21
05/22/2022 13:51:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.028246 on epoch=21
05/22/2022 13:51:48 - INFO - __main__ - Global step 350 Train loss 0.047768 Classification-F1 0.9531135531135531 on epoch=21
05/22/2022 13:51:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.014693 on epoch=22
05/22/2022 13:51:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.020769 on epoch=23
05/22/2022 13:52:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.007236 on epoch=23
05/22/2022 13:52:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.014198 on epoch=24
05/22/2022 13:52:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.005817 on epoch=24
05/22/2022 13:52:18 - INFO - __main__ - Global step 400 Train loss 0.012543 Classification-F1 0.9569990990578283 on epoch=24
05/22/2022 13:52:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.014030 on epoch=25
05/22/2022 13:52:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.223136 on epoch=26
05/22/2022 13:52:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.029518 on epoch=26
05/22/2022 13:52:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.011846 on epoch=27
05/22/2022 13:52:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.024374 on epoch=28
05/22/2022 13:52:47 - INFO - __main__ - Global step 450 Train loss 0.060581 Classification-F1 0.9687328244274809 on epoch=28
05/22/2022 13:52:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.022599 on epoch=28
05/22/2022 13:52:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.017835 on epoch=29
05/22/2022 13:53:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.006464 on epoch=29
05/22/2022 13:53:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001169 on epoch=30
05/22/2022 13:53:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.025413 on epoch=31
05/22/2022 13:53:17 - INFO - __main__ - Global step 500 Train loss 0.014696 Classification-F1 0.960935115668681 on epoch=31
05/22/2022 13:53:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.015857 on epoch=31
05/22/2022 13:53:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.016602 on epoch=32
05/22/2022 13:53:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.026451 on epoch=33
05/22/2022 13:53:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.029141 on epoch=33
05/22/2022 13:53:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001141 on epoch=34
05/22/2022 13:53:47 - INFO - __main__ - Global step 550 Train loss 0.017838 Classification-F1 0.9648432135500115 on epoch=34
05/22/2022 13:53:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001301 on epoch=34
05/22/2022 13:53:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.004529 on epoch=35
05/22/2022 13:54:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.004213 on epoch=36
05/22/2022 13:54:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.014463 on epoch=36
05/22/2022 13:54:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.008532 on epoch=37
05/22/2022 13:54:16 - INFO - __main__ - Global step 600 Train loss 0.006608 Classification-F1 0.9294117647058824 on epoch=37
05/22/2022 13:54:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.032735 on epoch=38
05/22/2022 13:54:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.037068 on epoch=38
05/22/2022 13:54:31 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000929 on epoch=39
05/22/2022 13:54:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000576 on epoch=39
05/22/2022 13:54:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000752 on epoch=40
05/22/2022 13:54:45 - INFO - __main__ - Global step 650 Train loss 0.014412 Classification-F1 0.9648389213606605 on epoch=40
05/22/2022 13:54:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.002406 on epoch=41
05/22/2022 13:54:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000297 on epoch=41
05/22/2022 13:55:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.001009 on epoch=42
05/22/2022 13:55:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.001113 on epoch=43
05/22/2022 13:55:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000574 on epoch=43
05/22/2022 13:55:14 - INFO - __main__ - Global step 700 Train loss 0.001080 Classification-F1 0.9687423687423687 on epoch=43
05/22/2022 13:55:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.003424 on epoch=44
05/22/2022 13:55:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.004246 on epoch=44
05/22/2022 13:55:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.022464 on epoch=45
05/22/2022 13:55:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000483 on epoch=46
05/22/2022 13:55:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000809 on epoch=46
05/22/2022 13:55:44 - INFO - __main__ - Global step 750 Train loss 0.006285 Classification-F1 0.9804684519722286 on epoch=46
05/22/2022 13:55:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000634 on epoch=47
05/22/2022 13:55:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000296 on epoch=48
05/22/2022 13:56:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000555 on epoch=48
05/22/2022 13:56:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.003857 on epoch=49
05/22/2022 13:56:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000402 on epoch=49
05/22/2022 13:56:14 - INFO - __main__ - Global step 800 Train loss 0.001149 Classification-F1 0.9765567765567766 on epoch=49
05/22/2022 13:56:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000510 on epoch=50
05/22/2022 13:56:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000174 on epoch=51
05/22/2022 13:56:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000133 on epoch=51
05/22/2022 13:56:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.008326 on epoch=52
05/22/2022 13:56:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.003236 on epoch=53
05/22/2022 13:56:43 - INFO - __main__ - Global step 850 Train loss 0.002476 Classification-F1 0.9765567765567766 on epoch=53
05/22/2022 13:56:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.006574 on epoch=53
05/22/2022 13:56:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.002213 on epoch=54
05/22/2022 13:56:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000083 on epoch=54
05/22/2022 13:57:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001668 on epoch=55
05/22/2022 13:57:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000964 on epoch=56
05/22/2022 13:57:13 - INFO - __main__ - Global step 900 Train loss 0.002300 Classification-F1 0.9530217763640813 on epoch=56
05/22/2022 13:57:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.001968 on epoch=56
05/22/2022 13:57:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.006003 on epoch=57
05/22/2022 13:57:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000499 on epoch=58
05/22/2022 13:57:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.005797 on epoch=58
05/22/2022 13:57:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.001004 on epoch=59
05/22/2022 13:57:42 - INFO - __main__ - Global step 950 Train loss 0.003054 Classification-F1 0.9609160305343512 on epoch=59
05/22/2022 13:57:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000657 on epoch=59
05/22/2022 13:57:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000085 on epoch=60
05/22/2022 13:57:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000112 on epoch=61
05/22/2022 13:58:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000132 on epoch=61
05/22/2022 13:58:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000100 on epoch=62
05/22/2022 13:58:08 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:58:08 - INFO - __main__ - Printing 3 examples
05/22/2022 13:58:08 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/22/2022 13:58:08 - INFO - __main__ - ['negative']
05/22/2022 13:58:08 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/22/2022 13:58:08 - INFO - __main__ - ['negative']
05/22/2022 13:58:08 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/22/2022 13:58:08 - INFO - __main__ - ['negative']
05/22/2022 13:58:08 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:58:09 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:58:09 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 13:58:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:58:09 - INFO - __main__ - Printing 3 examples
05/22/2022 13:58:09 - INFO - __main__ -  [amazon_polarity] title: cheap quality [SEP] content: The metal that was used for this necklace looks cheap and left stains on my skin. I returned the product immediately but I have not received my refund yet after a month.
05/22/2022 13:58:09 - INFO - __main__ - ['negative']
05/22/2022 13:58:09 - INFO - __main__ -  [amazon_polarity] title: Awful. [SEP] content: Why can't Weezer release another cool album like "Pinkerton" or "The blue album".This is even worse than their last "green" album that contained the annoying hash pipe.Do not buy this, its crap!
05/22/2022 13:58:09 - INFO - __main__ - ['negative']
05/22/2022 13:58:09 - INFO - __main__ -  [amazon_polarity] title: Not what you think it is! [SEP] content: Although this documentary may be very good...it is NOT a colorized version of the original Sink the Bismarck movie! We bought it as a gift for someone who had asked for the movie and were VERY disappointed!
05/22/2022 13:58:09 - INFO - __main__ - ['negative']
05/22/2022 13:58:09 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:58:09 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:58:09 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 13:58:11 - INFO - __main__ - Global step 1000 Train loss 0.000217 Classification-F1 0.9687423687423687 on epoch=62
05/22/2022 13:58:11 - INFO - __main__ - save last model!
05/22/2022 13:58:18 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 13:58:19 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 13:58:19 - INFO - __main__ - Printing 3 examples
05/22/2022 13:58:19 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 13:58:19 - INFO - __main__ - ['negative']
05/22/2022 13:58:19 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 13:58:19 - INFO - __main__ - ['negative']
05/22/2022 13:58:19 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 13:58:19 - INFO - __main__ - ['negative']
05/22/2022 13:58:19 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:58:19 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:58:20 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 13:58:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 13:58:22 - INFO - __main__ - Starting training!
05/22/2022 13:58:35 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_42_0.0003_8_predictions.txt
05/22/2022 13:58:35 - INFO - __main__ - Classification-F1 on test data: 0.9520
05/22/2022 13:58:35 - INFO - __main__ - prefix=amazon_polarity_128_42, lr=0.0003, bsz=8, dev_performance=0.9804684519722286, test_performance=0.9519905901556706
05/22/2022 13:58:35 - INFO - __main__ - Running ... prefix=amazon_polarity_128_42, lr=0.0002, bsz=8 ...
05/22/2022 13:58:36 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:58:36 - INFO - __main__ - Printing 3 examples
05/22/2022 13:58:36 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/22/2022 13:58:36 - INFO - __main__ - ['negative']
05/22/2022 13:58:36 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/22/2022 13:58:36 - INFO - __main__ - ['negative']
05/22/2022 13:58:36 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/22/2022 13:58:36 - INFO - __main__ - ['negative']
05/22/2022 13:58:36 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:58:36 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:58:37 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 13:58:37 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 13:58:37 - INFO - __main__ - Printing 3 examples
05/22/2022 13:58:37 - INFO - __main__ -  [amazon_polarity] title: cheap quality [SEP] content: The metal that was used for this necklace looks cheap and left stains on my skin. I returned the product immediately but I have not received my refund yet after a month.
05/22/2022 13:58:37 - INFO - __main__ - ['negative']
05/22/2022 13:58:37 - INFO - __main__ -  [amazon_polarity] title: Awful. [SEP] content: Why can't Weezer release another cool album like "Pinkerton" or "The blue album".This is even worse than their last "green" album that contained the annoying hash pipe.Do not buy this, its crap!
05/22/2022 13:58:37 - INFO - __main__ - ['negative']
05/22/2022 13:58:37 - INFO - __main__ -  [amazon_polarity] title: Not what you think it is! [SEP] content: Although this documentary may be very good...it is NOT a colorized version of the original Sink the Bismarck movie! We bought it as a gift for someone who had asked for the movie and were VERY disappointed!
05/22/2022 13:58:37 - INFO - __main__ - ['negative']
05/22/2022 13:58:37 - INFO - __main__ - Tokenizing Input ...
05/22/2022 13:58:37 - INFO - __main__ - Tokenizing Output ...
05/22/2022 13:58:37 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 13:58:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 13:58:50 - INFO - __main__ - Starting training!
05/22/2022 13:58:54 - INFO - __main__ - Step 10 Global step 10 Train loss 22.876057 on epoch=0
05/22/2022 13:58:59 - INFO - __main__ - Step 20 Global step 20 Train loss 18.110352 on epoch=1
05/22/2022 13:59:05 - INFO - __main__ - Step 30 Global step 30 Train loss 17.561443 on epoch=1
05/22/2022 13:59:10 - INFO - __main__ - Step 40 Global step 40 Train loss 16.386248 on epoch=2
05/22/2022 13:59:15 - INFO - __main__ - Step 50 Global step 50 Train loss 14.365483 on epoch=3
05/22/2022 13:59:19 - INFO - __main__ - Global step 50 Train loss 17.859917 Classification-F1 0.0 on epoch=3
05/22/2022 13:59:25 - INFO - __main__ - Step 60 Global step 60 Train loss 15.475168 on epoch=3
05/22/2022 13:59:30 - INFO - __main__ - Step 70 Global step 70 Train loss 13.974726 on epoch=4
05/22/2022 13:59:35 - INFO - __main__ - Step 80 Global step 80 Train loss 13.378993 on epoch=4
05/22/2022 13:59:40 - INFO - __main__ - Step 90 Global step 90 Train loss 12.354490 on epoch=5
05/22/2022 13:59:46 - INFO - __main__ - Step 100 Global step 100 Train loss 11.637511 on epoch=6
05/22/2022 13:59:50 - INFO - __main__ - Global step 100 Train loss 13.364179 Classification-F1 0.0 on epoch=6
05/22/2022 13:59:55 - INFO - __main__ - Step 110 Global step 110 Train loss 10.553357 on epoch=6
05/22/2022 14:00:00 - INFO - __main__ - Step 120 Global step 120 Train loss 8.320776 on epoch=7
05/22/2022 14:00:05 - INFO - __main__ - Step 130 Global step 130 Train loss 3.203553 on epoch=8
05/22/2022 14:00:10 - INFO - __main__ - Step 140 Global step 140 Train loss 2.579198 on epoch=8
05/22/2022 14:00:16 - INFO - __main__ - Step 150 Global step 150 Train loss 1.267383 on epoch=9
05/22/2022 14:00:19 - INFO - __main__ - Global step 150 Train loss 5.184853 Classification-F1 0.516338062772557 on epoch=9
05/22/2022 14:00:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.571655 on epoch=9
05/22/2022 14:00:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.364163 on epoch=10
05/22/2022 14:00:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.600834 on epoch=11
05/22/2022 14:00:40 - INFO - __main__ - Step 190 Global step 190 Train loss 1.692953 on epoch=11
05/22/2022 14:00:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.688011 on epoch=12
05/22/2022 14:00:49 - INFO - __main__ - Global step 200 Train loss 0.783523 Classification-F1 0.21647398843930635 on epoch=12
05/22/2022 14:00:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.519445 on epoch=13
05/22/2022 14:00:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.470091 on epoch=13
05/22/2022 14:01:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.404720 on epoch=14
05/22/2022 14:01:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.492744 on epoch=14
05/22/2022 14:01:15 - INFO - __main__ - Step 250 Global step 250 Train loss 1.119645 on epoch=15
05/22/2022 14:01:19 - INFO - __main__ - Global step 250 Train loss 0.601329 Classification-F1 0.4151889947624156 on epoch=15
05/22/2022 14:01:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.373100 on epoch=16
05/22/2022 14:01:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.380074 on epoch=16
05/22/2022 14:01:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.366093 on epoch=17
05/22/2022 14:01:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.353534 on epoch=18
05/22/2022 14:01:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.373553 on epoch=18
05/22/2022 14:01:48 - INFO - __main__ - Global step 300 Train loss 0.369271 Classification-F1 0.4957260323484232 on epoch=18
05/22/2022 14:01:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.367477 on epoch=19
05/22/2022 14:01:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.399020 on epoch=19
05/22/2022 14:02:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.366572 on epoch=20
05/22/2022 14:02:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.392174 on epoch=21
05/22/2022 14:02:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.382663 on epoch=21
05/22/2022 14:02:18 - INFO - __main__ - Global step 350 Train loss 0.381581 Classification-F1 0.18723200574380244 on epoch=21
05/22/2022 14:02:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.407813 on epoch=22
05/22/2022 14:02:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.410392 on epoch=23
05/22/2022 14:02:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.384993 on epoch=23
05/22/2022 14:02:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.410656 on epoch=24
05/22/2022 14:02:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.403233 on epoch=24
05/22/2022 14:02:48 - INFO - __main__ - Global step 400 Train loss 0.403417 Classification-F1 0.18126099824028155 on epoch=24
05/22/2022 14:02:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.377986 on epoch=25
05/22/2022 14:02:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.344445 on epoch=26
05/22/2022 14:03:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.359353 on epoch=26
05/22/2022 14:03:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.363632 on epoch=27
05/22/2022 14:03:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.398405 on epoch=28
05/22/2022 14:03:18 - INFO - __main__ - Global step 450 Train loss 0.368764 Classification-F1 0.2188711064640707 on epoch=28
05/22/2022 14:03:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.365170 on epoch=28
05/22/2022 14:03:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.350659 on epoch=29
05/22/2022 14:03:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.396126 on epoch=29
05/22/2022 14:03:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.396075 on epoch=30
05/22/2022 14:03:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.361279 on epoch=31
05/22/2022 14:03:47 - INFO - __main__ - Global step 500 Train loss 0.373862 Classification-F1 0.4251066696609028 on epoch=31
05/22/2022 14:03:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.358993 on epoch=31
05/22/2022 14:03:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.374736 on epoch=32
05/22/2022 14:04:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.398906 on epoch=33
05/22/2022 14:04:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.337709 on epoch=33
05/22/2022 14:04:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.357913 on epoch=34
05/22/2022 14:04:17 - INFO - __main__ - Global step 550 Train loss 0.365651 Classification-F1 0.4799390553580498 on epoch=34
05/22/2022 14:04:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.362274 on epoch=34
05/22/2022 14:04:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.374675 on epoch=35
05/22/2022 14:04:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.365102 on epoch=36
05/22/2022 14:04:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.343184 on epoch=36
05/22/2022 14:04:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.344829 on epoch=37
05/22/2022 14:04:47 - INFO - __main__ - Global step 600 Train loss 0.358013 Classification-F1 0.5797215655371684 on epoch=37
05/22/2022 14:04:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.356878 on epoch=38
05/22/2022 14:04:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.348816 on epoch=38
05/22/2022 14:05:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.383445 on epoch=39
05/22/2022 14:05:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.397408 on epoch=39
05/22/2022 14:05:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.360415 on epoch=40
05/22/2022 14:05:17 - INFO - __main__ - Global step 650 Train loss 0.369392 Classification-F1 0.6216352201257862 on epoch=40
05/22/2022 14:05:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.362135 on epoch=41
05/22/2022 14:05:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.354176 on epoch=41
05/22/2022 14:05:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.352157 on epoch=42
05/22/2022 14:05:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.375231 on epoch=43
05/22/2022 14:05:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.348086 on epoch=43
05/22/2022 14:05:48 - INFO - __main__ - Global step 700 Train loss 0.358357 Classification-F1 0.34195559333697656 on epoch=43
05/22/2022 14:05:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.337364 on epoch=44
05/22/2022 14:05:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.379645 on epoch=44
05/22/2022 14:06:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.334551 on epoch=45
05/22/2022 14:06:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.328384 on epoch=46
05/22/2022 14:06:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.356597 on epoch=46
05/22/2022 14:06:17 - INFO - __main__ - Global step 750 Train loss 0.347308 Classification-F1 0.5021607605877269 on epoch=46
05/22/2022 14:06:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.352087 on epoch=47
05/22/2022 14:06:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.379614 on epoch=48
05/22/2022 14:06:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.318657 on epoch=48
05/22/2022 14:06:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.340992 on epoch=49
05/22/2022 14:06:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.331375 on epoch=49
05/22/2022 14:06:47 - INFO - __main__ - Global step 800 Train loss 0.344545 Classification-F1 0.7174257330709795 on epoch=49
05/22/2022 14:06:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.340146 on epoch=50
05/22/2022 14:06:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.353783 on epoch=51
05/22/2022 14:07:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.313539 on epoch=51
05/22/2022 14:07:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.347261 on epoch=52
05/22/2022 14:07:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.331611 on epoch=53
05/22/2022 14:07:18 - INFO - __main__ - Global step 850 Train loss 0.337268 Classification-F1 0.7059613362452479 on epoch=53
05/22/2022 14:07:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.327527 on epoch=53
05/22/2022 14:07:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.348395 on epoch=54
05/22/2022 14:07:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.371973 on epoch=54
05/22/2022 14:07:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.333319 on epoch=55
05/22/2022 14:07:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.333882 on epoch=56
05/22/2022 14:07:47 - INFO - __main__ - Global step 900 Train loss 0.343019 Classification-F1 0.7135807010283999 on epoch=56
05/22/2022 14:07:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.352934 on epoch=56
05/22/2022 14:07:58 - INFO - __main__ - Step 920 Global step 920 Train loss 0.344640 on epoch=57
05/22/2022 14:08:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.335394 on epoch=58
05/22/2022 14:08:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.320250 on epoch=58
05/22/2022 14:08:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.331659 on epoch=59
05/22/2022 14:08:17 - INFO - __main__ - Global step 950 Train loss 0.336976 Classification-F1 0.7655677655677655 on epoch=59
05/22/2022 14:08:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.372336 on epoch=59
05/22/2022 14:08:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.350471 on epoch=60
05/22/2022 14:08:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.363017 on epoch=61
05/22/2022 14:08:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.330831 on epoch=61
05/22/2022 14:08:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.332486 on epoch=62
05/22/2022 14:08:45 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:08:45 - INFO - __main__ - Printing 3 examples
05/22/2022 14:08:45 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/22/2022 14:08:45 - INFO - __main__ - ['negative']
05/22/2022 14:08:45 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/22/2022 14:08:45 - INFO - __main__ - ['negative']
05/22/2022 14:08:45 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/22/2022 14:08:45 - INFO - __main__ - ['negative']
05/22/2022 14:08:45 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:08:45 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:08:45 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 14:08:45 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:08:45 - INFO - __main__ - Printing 3 examples
05/22/2022 14:08:45 - INFO - __main__ -  [amazon_polarity] title: cheap quality [SEP] content: The metal that was used for this necklace looks cheap and left stains on my skin. I returned the product immediately but I have not received my refund yet after a month.
05/22/2022 14:08:45 - INFO - __main__ - ['negative']
05/22/2022 14:08:45 - INFO - __main__ -  [amazon_polarity] title: Awful. [SEP] content: Why can't Weezer release another cool album like "Pinkerton" or "The blue album".This is even worse than their last "green" album that contained the annoying hash pipe.Do not buy this, its crap!
05/22/2022 14:08:45 - INFO - __main__ - ['negative']
05/22/2022 14:08:45 - INFO - __main__ -  [amazon_polarity] title: Not what you think it is! [SEP] content: Although this documentary may be very good...it is NOT a colorized version of the original Sink the Bismarck movie! We bought it as a gift for someone who had asked for the movie and were VERY disappointed!
05/22/2022 14:08:45 - INFO - __main__ - ['negative']
05/22/2022 14:08:45 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:08:46 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:08:46 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 14:08:48 - INFO - __main__ - Global step 1000 Train loss 0.349828 Classification-F1 0.7837007450649052 on epoch=62
05/22/2022 14:08:48 - INFO - __main__ - save last model!
05/22/2022 14:08:55 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 14:08:56 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 14:08:56 - INFO - __main__ - Printing 3 examples
05/22/2022 14:08:56 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 14:08:56 - INFO - __main__ - ['negative']
05/22/2022 14:08:56 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 14:08:56 - INFO - __main__ - ['negative']
05/22/2022 14:08:56 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 14:08:56 - INFO - __main__ - ['negative']
05/22/2022 14:08:56 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:08:56 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:08:57 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 14:08:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 14:08:59 - INFO - __main__ - Starting training!
05/22/2022 14:09:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_42_0.0002_8_predictions.txt
05/22/2022 14:09:12 - INFO - __main__ - Classification-F1 on test data: 0.6897
05/22/2022 14:09:13 - INFO - __main__ - prefix=amazon_polarity_128_42, lr=0.0002, bsz=8, dev_performance=0.7837007450649052, test_performance=0.6897370547137952
05/22/2022 14:09:13 - INFO - __main__ - Running ... prefix=amazon_polarity_128_42, lr=0.0001, bsz=8 ...
05/22/2022 14:09:13 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:09:13 - INFO - __main__ - Printing 3 examples
05/22/2022 14:09:13 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/22/2022 14:09:13 - INFO - __main__ - ['negative']
05/22/2022 14:09:13 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/22/2022 14:09:13 - INFO - __main__ - ['negative']
05/22/2022 14:09:13 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/22/2022 14:09:13 - INFO - __main__ - ['negative']
05/22/2022 14:09:13 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:09:14 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:09:14 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 14:09:14 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:09:14 - INFO - __main__ - Printing 3 examples
05/22/2022 14:09:14 - INFO - __main__ -  [amazon_polarity] title: cheap quality [SEP] content: The metal that was used for this necklace looks cheap and left stains on my skin. I returned the product immediately but I have not received my refund yet after a month.
05/22/2022 14:09:14 - INFO - __main__ - ['negative']
05/22/2022 14:09:14 - INFO - __main__ -  [amazon_polarity] title: Awful. [SEP] content: Why can't Weezer release another cool album like "Pinkerton" or "The blue album".This is even worse than their last "green" album that contained the annoying hash pipe.Do not buy this, its crap!
05/22/2022 14:09:14 - INFO - __main__ - ['negative']
05/22/2022 14:09:14 - INFO - __main__ -  [amazon_polarity] title: Not what you think it is! [SEP] content: Although this documentary may be very good...it is NOT a colorized version of the original Sink the Bismarck movie! We bought it as a gift for someone who had asked for the movie and were VERY disappointed!
05/22/2022 14:09:14 - INFO - __main__ - ['negative']
05/22/2022 14:09:14 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:09:14 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:09:14 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 14:09:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 14:09:25 - INFO - __main__ - Starting training!
05/22/2022 14:09:30 - INFO - __main__ - Step 10 Global step 10 Train loss 22.842709 on epoch=0
05/22/2022 14:09:35 - INFO - __main__ - Step 20 Global step 20 Train loss 18.732420 on epoch=1
05/22/2022 14:09:40 - INFO - __main__ - Step 30 Global step 30 Train loss 17.963360 on epoch=1
05/22/2022 14:09:45 - INFO - __main__ - Step 40 Global step 40 Train loss 16.860695 on epoch=2
05/22/2022 14:09:51 - INFO - __main__ - Step 50 Global step 50 Train loss 16.876265 on epoch=3
05/22/2022 14:11:11 - INFO - __main__ - Global step 50 Train loss 18.655090 Classification-F1 0.0 on epoch=3
05/22/2022 14:11:17 - INFO - __main__ - Step 60 Global step 60 Train loss 16.023777 on epoch=3
05/22/2022 14:11:22 - INFO - __main__ - Step 70 Global step 70 Train loss 15.048315 on epoch=4
05/22/2022 14:11:28 - INFO - __main__ - Step 80 Global step 80 Train loss 15.191028 on epoch=4
05/22/2022 14:11:33 - INFO - __main__ - Step 90 Global step 90 Train loss 14.995207 on epoch=5
05/22/2022 14:11:38 - INFO - __main__ - Step 100 Global step 100 Train loss 14.821576 on epoch=6
05/22/2022 14:12:06 - INFO - __main__ - Global step 100 Train loss 15.215981 Classification-F1 0.0 on epoch=6
05/22/2022 14:12:11 - INFO - __main__ - Step 110 Global step 110 Train loss 14.023285 on epoch=6
05/22/2022 14:12:16 - INFO - __main__ - Step 120 Global step 120 Train loss 14.005368 on epoch=7
05/22/2022 14:12:21 - INFO - __main__ - Step 130 Global step 130 Train loss 13.342329 on epoch=8
05/22/2022 14:12:27 - INFO - __main__ - Step 140 Global step 140 Train loss 12.533120 on epoch=8
05/22/2022 14:12:32 - INFO - __main__ - Step 150 Global step 150 Train loss 12.890429 on epoch=9
05/22/2022 14:12:57 - INFO - __main__ - Global step 150 Train loss 13.358906 Classification-F1 0.0 on epoch=9
05/22/2022 14:13:02 - INFO - __main__ - Step 160 Global step 160 Train loss 12.448153 on epoch=9
05/22/2022 14:13:07 - INFO - __main__ - Step 170 Global step 170 Train loss 12.148921 on epoch=10
05/22/2022 14:13:12 - INFO - __main__ - Step 180 Global step 180 Train loss 11.376488 on epoch=11
05/22/2022 14:13:17 - INFO - __main__ - Step 190 Global step 190 Train loss 10.602554 on epoch=11
05/22/2022 14:13:23 - INFO - __main__ - Step 200 Global step 200 Train loss 10.283929 on epoch=12
05/22/2022 14:13:33 - INFO - __main__ - Global step 200 Train loss 11.372008 Classification-F1 0.00442282176028306 on epoch=12
05/22/2022 14:13:39 - INFO - __main__ - Step 210 Global step 210 Train loss 8.940211 on epoch=13
05/22/2022 14:13:44 - INFO - __main__ - Step 220 Global step 220 Train loss 6.791377 on epoch=13
05/22/2022 14:13:49 - INFO - __main__ - Step 230 Global step 230 Train loss 5.014991 on epoch=14
05/22/2022 14:13:54 - INFO - __main__ - Step 240 Global step 240 Train loss 2.616793 on epoch=14
05/22/2022 14:13:59 - INFO - __main__ - Step 250 Global step 250 Train loss 2.414710 on epoch=15
05/22/2022 14:14:03 - INFO - __main__ - Global step 250 Train loss 5.155616 Classification-F1 0.34485289741504155 on epoch=15
05/22/2022 14:14:09 - INFO - __main__ - Step 260 Global step 260 Train loss 2.324238 on epoch=16
05/22/2022 14:14:14 - INFO - __main__ - Step 270 Global step 270 Train loss 1.653898 on epoch=16
05/22/2022 14:14:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.761042 on epoch=17
05/22/2022 14:14:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.444608 on epoch=18
05/22/2022 14:14:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.313052 on epoch=18
05/22/2022 14:14:33 - INFO - __main__ - Global step 300 Train loss 1.099367 Classification-F1 0.9374044868268232 on epoch=18
05/22/2022 14:14:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.217862 on epoch=19
05/22/2022 14:14:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.249812 on epoch=19
05/22/2022 14:14:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.168683 on epoch=20
05/22/2022 14:14:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.101922 on epoch=21
05/22/2022 14:14:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.146608 on epoch=21
05/22/2022 14:15:03 - INFO - __main__ - Global step 350 Train loss 0.176977 Classification-F1 0.9334220631205348 on epoch=21
05/22/2022 14:15:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.123966 on epoch=22
05/22/2022 14:15:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.119750 on epoch=23
05/22/2022 14:15:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.108944 on epoch=23
05/22/2022 14:15:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.083907 on epoch=24
05/22/2022 14:15:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.149619 on epoch=24
05/22/2022 14:15:32 - INFO - __main__ - Global step 400 Train loss 0.117237 Classification-F1 0.9569990990578283 on epoch=24
05/22/2022 14:15:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.035157 on epoch=25
05/22/2022 14:15:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.115459 on epoch=26
05/22/2022 14:15:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.114576 on epoch=26
05/22/2022 14:15:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.121057 on epoch=27
05/22/2022 14:15:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.156471 on epoch=28
05/22/2022 14:16:03 - INFO - __main__ - Global step 450 Train loss 0.108544 Classification-F1 0.9765610694012086 on epoch=28
05/22/2022 14:16:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.073860 on epoch=28
05/22/2022 14:16:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.054559 on epoch=29
05/22/2022 14:16:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.083610 on epoch=29
05/22/2022 14:16:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.044733 on epoch=30
05/22/2022 14:16:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.071709 on epoch=31
05/22/2022 14:16:33 - INFO - __main__ - Global step 500 Train loss 0.065694 Classification-F1 0.9765625 on epoch=31
05/22/2022 14:16:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.032319 on epoch=31
05/22/2022 14:16:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.044711 on epoch=32
05/22/2022 14:16:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.048846 on epoch=33
05/22/2022 14:16:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.035805 on epoch=33
05/22/2022 14:17:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.040646 on epoch=34
05/22/2022 14:17:03 - INFO - __main__ - Global step 550 Train loss 0.040465 Classification-F1 0.9530533651201173 on epoch=34
05/22/2022 14:17:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.040390 on epoch=34
05/22/2022 14:17:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.039758 on epoch=35
05/22/2022 14:17:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.035323 on epoch=36
05/22/2022 14:17:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.027773 on epoch=36
05/22/2022 14:17:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.045211 on epoch=37
05/22/2022 14:17:33 - INFO - __main__ - Global step 600 Train loss 0.037691 Classification-F1 0.9726558327611201 on epoch=37
05/22/2022 14:17:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.013337 on epoch=38
05/22/2022 14:17:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.010329 on epoch=38
05/22/2022 14:17:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.045090 on epoch=39
05/22/2022 14:17:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.009370 on epoch=39
05/22/2022 14:17:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.007149 on epoch=40
05/22/2022 14:18:02 - INFO - __main__ - Global step 650 Train loss 0.017055 Classification-F1 0.9687480925349448 on epoch=40
05/22/2022 14:18:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.021711 on epoch=41
05/22/2022 14:18:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.024658 on epoch=41
05/22/2022 14:18:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.027233 on epoch=42
05/22/2022 14:18:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.023250 on epoch=43
05/22/2022 14:18:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.009447 on epoch=43
05/22/2022 14:18:32 - INFO - __main__ - Global step 700 Train loss 0.021260 Classification-F1 0.9726524943916248 on epoch=43
05/22/2022 14:18:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.008933 on epoch=44
05/22/2022 14:18:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.019855 on epoch=44
05/22/2022 14:18:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.012836 on epoch=45
05/22/2022 14:18:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.014540 on epoch=46
05/22/2022 14:18:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001749 on epoch=46
05/22/2022 14:19:01 - INFO - __main__ - Global step 750 Train loss 0.011583 Classification-F1 0.9765496183206106 on epoch=46
05/22/2022 14:19:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.018657 on epoch=47
05/22/2022 14:19:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.027497 on epoch=48
05/22/2022 14:19:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.014284 on epoch=48
05/22/2022 14:19:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.015746 on epoch=49
05/22/2022 14:19:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.004525 on epoch=49
05/22/2022 14:19:31 - INFO - __main__ - Global step 800 Train loss 0.016142 Classification-F1 0.960935115668681 on epoch=49
05/22/2022 14:19:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.001967 on epoch=50
05/22/2022 14:19:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.002868 on epoch=51
05/22/2022 14:19:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.002311 on epoch=51
05/22/2022 14:19:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.008381 on epoch=52
05/22/2022 14:19:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001399 on epoch=53
05/22/2022 14:20:00 - INFO - __main__ - Global step 850 Train loss 0.003385 Classification-F1 0.9726524943916248 on epoch=53
05/22/2022 14:20:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.010761 on epoch=53
05/22/2022 14:20:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.009067 on epoch=54
05/22/2022 14:20:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.008487 on epoch=54
05/22/2022 14:20:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001089 on epoch=55
05/22/2022 14:20:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000634 on epoch=56
05/22/2022 14:20:30 - INFO - __main__ - Global step 900 Train loss 0.006008 Classification-F1 0.9726558327611201 on epoch=56
05/22/2022 14:20:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.055401 on epoch=56
05/22/2022 14:20:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.011686 on epoch=57
05/22/2022 14:20:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.003735 on epoch=58
05/22/2022 14:20:51 - INFO - __main__ - Step 940 Global step 940 Train loss 0.023911 on epoch=58
05/22/2022 14:20:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.005665 on epoch=59
05/22/2022 14:21:00 - INFO - __main__ - Global step 950 Train loss 0.020079 Classification-F1 0.9726524943916248 on epoch=59
05/22/2022 14:21:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000479 on epoch=59
05/22/2022 14:21:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000631 on epoch=60
05/22/2022 14:21:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000560 on epoch=61
05/22/2022 14:21:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.006660 on epoch=61
05/22/2022 14:21:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000452 on epoch=62
05/22/2022 14:21:28 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:21:28 - INFO - __main__ - Printing 3 examples
05/22/2022 14:21:28 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/22/2022 14:21:28 - INFO - __main__ - ['negative']
05/22/2022 14:21:28 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/22/2022 14:21:28 - INFO - __main__ - ['negative']
05/22/2022 14:21:28 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/22/2022 14:21:28 - INFO - __main__ - ['negative']
05/22/2022 14:21:28 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:21:28 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:21:28 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 14:21:28 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:21:28 - INFO - __main__ - Printing 3 examples
05/22/2022 14:21:28 - INFO - __main__ -  [amazon_polarity] title: Awful and Disappointing [SEP] content: I was waiting for this movie for what seemed like a very long time.I was extremely disappointed - so much I had to turn the movie OFF!Awful direction - the world already has enough darkness - I thought a movie including quantum physics may include more light.When the 'good guy' isn't really, there is no one to root for.I don't know how it ended, but it wouldn't matter from where I could see and feel the writer was taking us.This is a movie where I didn't care for not one of the characters, which is when a story dies.I have only turned off a handfull of movies in my 40+ years and this was one of them.
05/22/2022 14:21:28 - INFO - __main__ - ['negative']
05/22/2022 14:21:28 - INFO - __main__ -  [amazon_polarity] title: Faulty electronics in a nice design [SEP] content: There have been frequent failures of this mouse of the DOA kind. Many DOA mice do not communicate with their dongle and the Logitech reset procedure does not help. It isn't clear whether this problem is a design or a production problem.There have even been many DOAs of the same kind with so-called refurb MX Revolutions recently sold by WOOT.For details check either Logitech's customer blog or WOOT's.Note also that Logitech's reset procedure involves six button actions and a switch flip, since there is no reset button on this mouse to reconnect the wireless, a seemingly clear design defect.This is a huge black eye for Logitech, heretofore a company held in high respect.
05/22/2022 14:21:28 - INFO - __main__ - ['negative']
05/22/2022 14:21:28 - INFO - __main__ -  [amazon_polarity] title: CISCO should be imbarassed to put their name on this POS! [SEP] content: THIS ROUTER DESERVES ZERO STARS!!!I've set up many routers. THis think is maddening.So i get the new router and automatically upgrade it to the latest software release before any configuration ( v4.2.1.02 (Jan 18 2012 14:10:55))Then I have one simple task - Forward incoming port 41790 to 192.168.0.50.That's it - the simplest feature on any $70 router on the planet. Guess what 3 hours later and the friggin thing still cannon do a simple port forward. I disabled the firewall, set up aUPNP service, set the software DMZ to the IP everything.Still doesnt work - Then I search online and find this has been a probelem with this router since 2007 thru three hardware revs and countless firmware revs. What crap.I WILL NEVER EVER BUY ANOTHER CISCO ROUTER - THEY HAVE BURNED ME OF THE BRAND FOREVER>BEWARE _ THIS ROUTER WILL NOT WORK CORRECTLY _ EVER!!!
05/22/2022 14:21:28 - INFO - __main__ - ['negative']
05/22/2022 14:21:28 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:21:28 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:21:29 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 14:21:30 - INFO - __main__ - Global step 1000 Train loss 0.001756 Classification-F1 0.9843740462674724 on epoch=62
05/22/2022 14:21:31 - INFO - __main__ - save last model!
05/22/2022 14:21:38 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 14:21:39 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 14:21:39 - INFO - __main__ - Printing 3 examples
05/22/2022 14:21:39 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 14:21:39 - INFO - __main__ - ['negative']
05/22/2022 14:21:39 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 14:21:39 - INFO - __main__ - ['negative']
05/22/2022 14:21:39 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 14:21:39 - INFO - __main__ - ['negative']
05/22/2022 14:21:39 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:21:40 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:21:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 14:21:40 - INFO - __main__ - Starting training!
05/22/2022 14:21:41 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 14:21:55 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_42_0.0001_8_predictions.txt
05/22/2022 14:21:55 - INFO - __main__ - Classification-F1 on test data: 0.9570
05/22/2022 14:21:56 - INFO - __main__ - prefix=amazon_polarity_128_42, lr=0.0001, bsz=8, dev_performance=0.9843740462674724, test_performance=0.9569996129965168
05/22/2022 14:21:56 - INFO - __main__ - Running ... prefix=amazon_polarity_128_87, lr=0.0005, bsz=8 ...
05/22/2022 14:21:57 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:21:57 - INFO - __main__ - Printing 3 examples
05/22/2022 14:21:57 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/22/2022 14:21:57 - INFO - __main__ - ['negative']
05/22/2022 14:21:57 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/22/2022 14:21:57 - INFO - __main__ - ['negative']
05/22/2022 14:21:57 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/22/2022 14:21:57 - INFO - __main__ - ['negative']
05/22/2022 14:21:57 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:21:57 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:21:57 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 14:21:57 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:21:57 - INFO - __main__ - Printing 3 examples
05/22/2022 14:21:57 - INFO - __main__ -  [amazon_polarity] title: Awful and Disappointing [SEP] content: I was waiting for this movie for what seemed like a very long time.I was extremely disappointed - so much I had to turn the movie OFF!Awful direction - the world already has enough darkness - I thought a movie including quantum physics may include more light.When the 'good guy' isn't really, there is no one to root for.I don't know how it ended, but it wouldn't matter from where I could see and feel the writer was taking us.This is a movie where I didn't care for not one of the characters, which is when a story dies.I have only turned off a handfull of movies in my 40+ years and this was one of them.
05/22/2022 14:21:57 - INFO - __main__ - ['negative']
05/22/2022 14:21:57 - INFO - __main__ -  [amazon_polarity] title: Faulty electronics in a nice design [SEP] content: There have been frequent failures of this mouse of the DOA kind. Many DOA mice do not communicate with their dongle and the Logitech reset procedure does not help. It isn't clear whether this problem is a design or a production problem.There have even been many DOAs of the same kind with so-called refurb MX Revolutions recently sold by WOOT.For details check either Logitech's customer blog or WOOT's.Note also that Logitech's reset procedure involves six button actions and a switch flip, since there is no reset button on this mouse to reconnect the wireless, a seemingly clear design defect.This is a huge black eye for Logitech, heretofore a company held in high respect.
05/22/2022 14:21:57 - INFO - __main__ - ['negative']
05/22/2022 14:21:57 - INFO - __main__ -  [amazon_polarity] title: CISCO should be imbarassed to put their name on this POS! [SEP] content: THIS ROUTER DESERVES ZERO STARS!!!I've set up many routers. THis think is maddening.So i get the new router and automatically upgrade it to the latest software release before any configuration ( v4.2.1.02 (Jan 18 2012 14:10:55))Then I have one simple task - Forward incoming port 41790 to 192.168.0.50.That's it - the simplest feature on any $70 router on the planet. Guess what 3 hours later and the friggin thing still cannon do a simple port forward. I disabled the firewall, set up aUPNP service, set the software DMZ to the IP everything.Still doesnt work - Then I search online and find this has been a probelem with this router since 2007 thru three hardware revs and countless firmware revs. What crap.I WILL NEVER EVER BUY ANOTHER CISCO ROUTER - THEY HAVE BURNED ME OF THE BRAND FOREVER>BEWARE _ THIS ROUTER WILL NOT WORK CORRECTLY _ EVER!!!
05/22/2022 14:21:57 - INFO - __main__ - ['negative']
05/22/2022 14:21:57 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:21:57 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:21:58 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 14:22:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 14:22:10 - INFO - __main__ - Starting training!
05/22/2022 14:22:15 - INFO - __main__ - Step 10 Global step 10 Train loss 22.897573 on epoch=0
05/22/2022 14:22:20 - INFO - __main__ - Step 20 Global step 20 Train loss 17.824001 on epoch=1
05/22/2022 14:22:25 - INFO - __main__ - Step 30 Global step 30 Train loss 15.048523 on epoch=1
05/22/2022 14:22:30 - INFO - __main__ - Step 40 Global step 40 Train loss 13.271116 on epoch=2
05/22/2022 14:22:35 - INFO - __main__ - Step 50 Global step 50 Train loss 8.692795 on epoch=3
05/22/2022 14:22:40 - INFO - __main__ - Global step 50 Train loss 15.546801 Classification-F1 0.217351598173516 on epoch=3
05/22/2022 14:22:45 - INFO - __main__ - Step 60 Global step 60 Train loss 4.025661 on epoch=3
05/22/2022 14:22:50 - INFO - __main__ - Step 70 Global step 70 Train loss 1.989850 on epoch=4
05/22/2022 14:22:55 - INFO - __main__ - Step 80 Global step 80 Train loss 0.606669 on epoch=4
05/22/2022 14:23:01 - INFO - __main__ - Step 90 Global step 90 Train loss 0.400208 on epoch=5
05/22/2022 14:23:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.456149 on epoch=6
05/22/2022 14:23:09 - INFO - __main__ - Global step 100 Train loss 1.495707 Classification-F1 0.5773296094557686 on epoch=6
05/22/2022 14:23:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.385019 on epoch=6
05/22/2022 14:23:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.459219 on epoch=7
05/22/2022 14:23:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.362319 on epoch=8
05/22/2022 14:23:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.354697 on epoch=8
05/22/2022 14:23:37 - INFO - __main__ - Step 150 Global step 150 Train loss 0.373581 on epoch=9
05/22/2022 14:23:40 - INFO - __main__ - Global step 150 Train loss 0.386967 Classification-F1 0.350463149416029 on epoch=9
05/22/2022 14:23:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.395099 on epoch=9
05/22/2022 14:23:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.394847 on epoch=10
05/22/2022 14:23:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.378932 on epoch=11
05/22/2022 14:24:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.419621 on epoch=11
05/22/2022 14:24:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.409158 on epoch=12
05/22/2022 14:24:10 - INFO - __main__ - Global step 200 Train loss 0.399532 Classification-F1 0.3333333333333333 on epoch=12
05/22/2022 14:24:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.438485 on epoch=13
05/22/2022 14:24:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.344241 on epoch=13
05/22/2022 14:24:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.386542 on epoch=14
05/22/2022 14:24:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.366850 on epoch=14
05/22/2022 14:24:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.400004 on epoch=15
05/22/2022 14:24:40 - INFO - __main__ - Global step 250 Train loss 0.387224 Classification-F1 0.3333333333333333 on epoch=15
05/22/2022 14:24:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.390919 on epoch=16
05/22/2022 14:24:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.393437 on epoch=16
05/22/2022 14:24:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.375788 on epoch=17
05/22/2022 14:25:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.394780 on epoch=18
05/22/2022 14:25:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.351184 on epoch=18
05/22/2022 14:25:10 - INFO - __main__ - Global step 300 Train loss 0.381222 Classification-F1 0.3333333333333333 on epoch=18
05/22/2022 14:25:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.395435 on epoch=19
05/22/2022 14:25:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.395625 on epoch=19
05/22/2022 14:25:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.373920 on epoch=20
05/22/2022 14:25:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.391374 on epoch=21
05/22/2022 14:25:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.369885 on epoch=21
05/22/2022 14:25:40 - INFO - __main__ - Global step 350 Train loss 0.385248 Classification-F1 0.5607885352889725 on epoch=21
05/22/2022 14:25:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.379994 on epoch=22
05/22/2022 14:25:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.373339 on epoch=23
05/22/2022 14:25:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.345463 on epoch=23
05/22/2022 14:26:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.356904 on epoch=24
05/22/2022 14:26:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.372022 on epoch=24
05/22/2022 14:26:10 - INFO - __main__ - Global step 400 Train loss 0.365544 Classification-F1 0.3333333333333333 on epoch=24
05/22/2022 14:26:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.370670 on epoch=25
05/22/2022 14:26:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.375499 on epoch=26
05/22/2022 14:26:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.343100 on epoch=26
05/22/2022 14:26:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.356139 on epoch=27
05/22/2022 14:26:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.393229 on epoch=28
05/22/2022 14:26:39 - INFO - __main__ - Global step 450 Train loss 0.367727 Classification-F1 0.3333333333333333 on epoch=28
05/22/2022 14:26:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.347539 on epoch=28
05/22/2022 14:26:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.345646 on epoch=29
05/22/2022 14:26:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.348285 on epoch=29
05/22/2022 14:27:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.358248 on epoch=30
05/22/2022 14:27:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.338339 on epoch=31
05/22/2022 14:27:09 - INFO - __main__ - Global step 500 Train loss 0.347612 Classification-F1 0.6267232237539766 on epoch=31
05/22/2022 14:27:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.297631 on epoch=31
05/22/2022 14:27:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.312154 on epoch=32
05/22/2022 14:27:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.307586 on epoch=33
05/22/2022 14:27:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.301660 on epoch=33
05/22/2022 14:27:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.292432 on epoch=34
05/22/2022 14:27:39 - INFO - __main__ - Global step 550 Train loss 0.302293 Classification-F1 0.7430847393840556 on epoch=34
05/22/2022 14:27:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.273839 on epoch=34
05/22/2022 14:27:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.304119 on epoch=35
05/22/2022 14:27:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.329752 on epoch=36
05/22/2022 14:28:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.299048 on epoch=36
05/22/2022 14:28:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.319199 on epoch=37
05/22/2022 14:28:10 - INFO - __main__ - Global step 600 Train loss 0.305191 Classification-F1 0.761416589002796 on epoch=37
05/22/2022 14:28:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.327076 on epoch=38
05/22/2022 14:28:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.240872 on epoch=38
05/22/2022 14:28:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.275884 on epoch=39
05/22/2022 14:28:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.222937 on epoch=39
05/22/2022 14:28:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.270218 on epoch=40
05/22/2022 14:28:40 - INFO - __main__ - Global step 650 Train loss 0.267397 Classification-F1 0.786125379045733 on epoch=40
05/22/2022 14:28:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.202334 on epoch=41
05/22/2022 14:28:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.240154 on epoch=41
05/22/2022 14:28:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.303368 on epoch=42
05/22/2022 14:29:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.256832 on epoch=43
05/22/2022 14:29:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.245378 on epoch=43
05/22/2022 14:29:11 - INFO - __main__ - Global step 700 Train loss 0.249613 Classification-F1 0.4492753623188406 on epoch=43
05/22/2022 14:29:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.228816 on epoch=44
05/22/2022 14:29:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.252890 on epoch=44
05/22/2022 14:29:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.851226 on epoch=45
05/22/2022 14:29:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.329161 on epoch=46
05/22/2022 14:29:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.246398 on epoch=46
05/22/2022 14:29:41 - INFO - __main__ - Global step 750 Train loss 0.381698 Classification-F1 0.7408906882591093 on epoch=46
05/22/2022 14:29:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.273239 on epoch=47
05/22/2022 14:29:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.213171 on epoch=48
05/22/2022 14:29:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.245745 on epoch=48
05/22/2022 14:30:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.295722 on epoch=49
05/22/2022 14:30:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.178537 on epoch=49
05/22/2022 14:30:11 - INFO - __main__ - Global step 800 Train loss 0.241283 Classification-F1 0.7262893081761006 on epoch=49
05/22/2022 14:30:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.218049 on epoch=50
05/22/2022 14:30:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.224654 on epoch=51
05/22/2022 14:30:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.191345 on epoch=51
05/22/2022 14:30:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.190080 on epoch=52
05/22/2022 14:30:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.175501 on epoch=53
05/22/2022 14:30:40 - INFO - __main__ - Global step 850 Train loss 0.199926 Classification-F1 0.715349286922891 on epoch=53
05/22/2022 14:30:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.269560 on epoch=53
05/22/2022 14:30:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.231162 on epoch=54
05/22/2022 14:30:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.230041 on epoch=54
05/22/2022 14:31:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.190178 on epoch=55
05/22/2022 14:31:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.226408 on epoch=56
05/22/2022 14:31:10 - INFO - __main__ - Global step 900 Train loss 0.229470 Classification-F1 0.8006321865408401 on epoch=56
05/22/2022 14:31:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.229352 on epoch=56
05/22/2022 14:31:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.165157 on epoch=57
05/22/2022 14:31:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.197236 on epoch=58
05/22/2022 14:31:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.133480 on epoch=58
05/22/2022 14:31:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.150300 on epoch=59
05/22/2022 14:31:40 - INFO - __main__ - Global step 950 Train loss 0.175105 Classification-F1 0.796078431372549 on epoch=59
05/22/2022 14:31:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.148602 on epoch=59
05/22/2022 14:31:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.102662 on epoch=60
05/22/2022 14:31:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.187054 on epoch=61
05/22/2022 14:32:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.117707 on epoch=61
05/22/2022 14:32:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.154080 on epoch=62
05/22/2022 14:32:07 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:32:07 - INFO - __main__ - Printing 3 examples
05/22/2022 14:32:07 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/22/2022 14:32:07 - INFO - __main__ - ['negative']
05/22/2022 14:32:07 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/22/2022 14:32:07 - INFO - __main__ - ['negative']
05/22/2022 14:32:07 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/22/2022 14:32:07 - INFO - __main__ - ['negative']
05/22/2022 14:32:07 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:32:08 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:32:08 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 14:32:08 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:32:08 - INFO - __main__ - Printing 3 examples
05/22/2022 14:32:08 - INFO - __main__ -  [amazon_polarity] title: Awful and Disappointing [SEP] content: I was waiting for this movie for what seemed like a very long time.I was extremely disappointed - so much I had to turn the movie OFF!Awful direction - the world already has enough darkness - I thought a movie including quantum physics may include more light.When the 'good guy' isn't really, there is no one to root for.I don't know how it ended, but it wouldn't matter from where I could see and feel the writer was taking us.This is a movie where I didn't care for not one of the characters, which is when a story dies.I have only turned off a handfull of movies in my 40+ years and this was one of them.
05/22/2022 14:32:08 - INFO - __main__ - ['negative']
05/22/2022 14:32:08 - INFO - __main__ -  [amazon_polarity] title: Faulty electronics in a nice design [SEP] content: There have been frequent failures of this mouse of the DOA kind. Many DOA mice do not communicate with their dongle and the Logitech reset procedure does not help. It isn't clear whether this problem is a design or a production problem.There have even been many DOAs of the same kind with so-called refurb MX Revolutions recently sold by WOOT.For details check either Logitech's customer blog or WOOT's.Note also that Logitech's reset procedure involves six button actions and a switch flip, since there is no reset button on this mouse to reconnect the wireless, a seemingly clear design defect.This is a huge black eye for Logitech, heretofore a company held in high respect.
05/22/2022 14:32:08 - INFO - __main__ - ['negative']
05/22/2022 14:32:08 - INFO - __main__ -  [amazon_polarity] title: CISCO should be imbarassed to put their name on this POS! [SEP] content: THIS ROUTER DESERVES ZERO STARS!!!I've set up many routers. THis think is maddening.So i get the new router and automatically upgrade it to the latest software release before any configuration ( v4.2.1.02 (Jan 18 2012 14:10:55))Then I have one simple task - Forward incoming port 41790 to 192.168.0.50.That's it - the simplest feature on any $70 router on the planet. Guess what 3 hours later and the friggin thing still cannon do a simple port forward. I disabled the firewall, set up aUPNP service, set the software DMZ to the IP everything.Still doesnt work - Then I search online and find this has been a probelem with this router since 2007 thru three hardware revs and countless firmware revs. What crap.I WILL NEVER EVER BUY ANOTHER CISCO ROUTER - THEY HAVE BURNED ME OF THE BRAND FOREVER>BEWARE _ THIS ROUTER WILL NOT WORK CORRECTLY _ EVER!!!
05/22/2022 14:32:08 - INFO - __main__ - ['negative']
05/22/2022 14:32:08 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:32:08 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:32:08 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 14:32:10 - INFO - __main__ - Global step 1000 Train loss 0.142021 Classification-F1 0.7842046377611231 on epoch=62
05/22/2022 14:32:10 - INFO - __main__ - save last model!
05/22/2022 14:32:17 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 14:32:17 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 14:32:17 - INFO - __main__ - Printing 3 examples
05/22/2022 14:32:17 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 14:32:17 - INFO - __main__ - ['negative']
05/22/2022 14:32:17 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 14:32:17 - INFO - __main__ - ['negative']
05/22/2022 14:32:17 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 14:32:17 - INFO - __main__ - ['negative']
05/22/2022 14:32:17 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:32:18 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:32:19 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 14:32:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 14:32:19 - INFO - __main__ - Starting training!
05/22/2022 14:32:33 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_87_0.0005_8_predictions.txt
05/22/2022 14:32:33 - INFO - __main__ - Classification-F1 on test data: 0.7758
05/22/2022 14:32:34 - INFO - __main__ - prefix=amazon_polarity_128_87, lr=0.0005, bsz=8, dev_performance=0.8006321865408401, test_performance=0.7757982183965568
05/22/2022 14:32:34 - INFO - __main__ - Running ... prefix=amazon_polarity_128_87, lr=0.0003, bsz=8 ...
05/22/2022 14:32:35 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:32:35 - INFO - __main__ - Printing 3 examples
05/22/2022 14:32:35 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/22/2022 14:32:35 - INFO - __main__ - ['negative']
05/22/2022 14:32:35 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/22/2022 14:32:35 - INFO - __main__ - ['negative']
05/22/2022 14:32:35 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/22/2022 14:32:35 - INFO - __main__ - ['negative']
05/22/2022 14:32:35 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:32:35 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:32:35 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 14:32:35 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:32:35 - INFO - __main__ - Printing 3 examples
05/22/2022 14:32:35 - INFO - __main__ -  [amazon_polarity] title: Awful and Disappointing [SEP] content: I was waiting for this movie for what seemed like a very long time.I was extremely disappointed - so much I had to turn the movie OFF!Awful direction - the world already has enough darkness - I thought a movie including quantum physics may include more light.When the 'good guy' isn't really, there is no one to root for.I don't know how it ended, but it wouldn't matter from where I could see and feel the writer was taking us.This is a movie where I didn't care for not one of the characters, which is when a story dies.I have only turned off a handfull of movies in my 40+ years and this was one of them.
05/22/2022 14:32:35 - INFO - __main__ - ['negative']
05/22/2022 14:32:35 - INFO - __main__ -  [amazon_polarity] title: Faulty electronics in a nice design [SEP] content: There have been frequent failures of this mouse of the DOA kind. Many DOA mice do not communicate with their dongle and the Logitech reset procedure does not help. It isn't clear whether this problem is a design or a production problem.There have even been many DOAs of the same kind with so-called refurb MX Revolutions recently sold by WOOT.For details check either Logitech's customer blog or WOOT's.Note also that Logitech's reset procedure involves six button actions and a switch flip, since there is no reset button on this mouse to reconnect the wireless, a seemingly clear design defect.This is a huge black eye for Logitech, heretofore a company held in high respect.
05/22/2022 14:32:35 - INFO - __main__ - ['negative']
05/22/2022 14:32:35 - INFO - __main__ -  [amazon_polarity] title: CISCO should be imbarassed to put their name on this POS! [SEP] content: THIS ROUTER DESERVES ZERO STARS!!!I've set up many routers. THis think is maddening.So i get the new router and automatically upgrade it to the latest software release before any configuration ( v4.2.1.02 (Jan 18 2012 14:10:55))Then I have one simple task - Forward incoming port 41790 to 192.168.0.50.That's it - the simplest feature on any $70 router on the planet. Guess what 3 hours later and the friggin thing still cannon do a simple port forward. I disabled the firewall, set up aUPNP service, set the software DMZ to the IP everything.Still doesnt work - Then I search online and find this has been a probelem with this router since 2007 thru three hardware revs and countless firmware revs. What crap.I WILL NEVER EVER BUY ANOTHER CISCO ROUTER - THEY HAVE BURNED ME OF THE BRAND FOREVER>BEWARE _ THIS ROUTER WILL NOT WORK CORRECTLY _ EVER!!!
05/22/2022 14:32:35 - INFO - __main__ - ['negative']
05/22/2022 14:32:35 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:32:35 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:32:36 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 14:32:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 14:32:48 - INFO - __main__ - Starting training!
05/22/2022 14:32:53 - INFO - __main__ - Step 10 Global step 10 Train loss 22.306343 on epoch=0
05/22/2022 14:32:58 - INFO - __main__ - Step 20 Global step 20 Train loss 17.116373 on epoch=1
05/22/2022 14:33:03 - INFO - __main__ - Step 30 Global step 30 Train loss 15.008365 on epoch=1
05/22/2022 14:33:08 - INFO - __main__ - Step 40 Global step 40 Train loss 14.150495 on epoch=2
05/22/2022 14:33:14 - INFO - __main__ - Step 50 Global step 50 Train loss 13.523361 on epoch=3
05/22/2022 14:33:26 - INFO - __main__ - Global step 50 Train loss 16.420988 Classification-F1 0.0026087673072014437 on epoch=3
05/22/2022 14:33:32 - INFO - __main__ - Step 60 Global step 60 Train loss 11.995941 on epoch=3
05/22/2022 14:33:37 - INFO - __main__ - Step 70 Global step 70 Train loss 6.464616 on epoch=4
05/22/2022 14:33:42 - INFO - __main__ - Step 80 Global step 80 Train loss 1.416387 on epoch=4
05/22/2022 14:33:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.898187 on epoch=5
05/22/2022 14:33:51 - INFO - __main__ - Step 100 Global step 100 Train loss 2.138511 on epoch=6
05/22/2022 14:33:55 - INFO - __main__ - Global step 100 Train loss 4.582728 Classification-F1 0.9335684083589015 on epoch=6
05/22/2022 14:34:00 - INFO - __main__ - Step 110 Global step 110 Train loss 1.562691 on epoch=6
05/22/2022 14:34:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.382436 on epoch=7
05/22/2022 14:34:11 - INFO - __main__ - Step 130 Global step 130 Train loss 1.411630 on epoch=8
05/22/2022 14:34:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.378948 on epoch=8
05/22/2022 14:34:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.443419 on epoch=9
05/22/2022 14:34:25 - INFO - __main__ - Global step 150 Train loss 0.835825 Classification-F1 0.6050306232175418 on epoch=9
05/22/2022 14:34:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.540708 on epoch=9
05/22/2022 14:34:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.486520 on epoch=10
05/22/2022 14:34:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.364855 on epoch=11
05/22/2022 14:34:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.392547 on epoch=11
05/22/2022 14:34:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.395878 on epoch=12
05/22/2022 14:34:55 - INFO - __main__ - Global step 200 Train loss 0.436102 Classification-F1 0.34195559333697656 on epoch=12
05/22/2022 14:35:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.404402 on epoch=13
05/22/2022 14:35:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.410921 on epoch=13
05/22/2022 14:35:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.396259 on epoch=14
05/22/2022 14:35:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.461323 on epoch=14
05/22/2022 14:35:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.361518 on epoch=15
05/22/2022 14:35:25 - INFO - __main__ - Global step 250 Train loss 0.406884 Classification-F1 0.3712545436683367 on epoch=15
05/22/2022 14:35:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.407059 on epoch=16
05/22/2022 14:35:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.374346 on epoch=16
05/22/2022 14:35:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.377669 on epoch=17
05/22/2022 14:35:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.392674 on epoch=18
05/22/2022 14:35:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.342220 on epoch=18
05/22/2022 14:35:55 - INFO - __main__ - Global step 300 Train loss 0.378794 Classification-F1 0.3333333333333333 on epoch=18
05/22/2022 14:36:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.377203 on epoch=19
05/22/2022 14:36:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.379957 on epoch=19
05/22/2022 14:36:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.381804 on epoch=20
05/22/2022 14:36:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.397478 on epoch=21
05/22/2022 14:36:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.354827 on epoch=21
05/22/2022 14:36:25 - INFO - __main__ - Global step 350 Train loss 0.378254 Classification-F1 0.7304646372167545 on epoch=21
05/22/2022 14:36:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.378850 on epoch=22
05/22/2022 14:36:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.364536 on epoch=23
05/22/2022 14:36:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.341617 on epoch=23
05/22/2022 14:36:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.340663 on epoch=24
05/22/2022 14:36:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.386870 on epoch=24
05/22/2022 14:36:55 - INFO - __main__ - Global step 400 Train loss 0.362507 Classification-F1 0.3333333333333333 on epoch=24
05/22/2022 14:37:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.371269 on epoch=25
05/22/2022 14:37:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.398137 on epoch=26
05/22/2022 14:37:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.350029 on epoch=26
05/22/2022 14:37:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.370195 on epoch=27
05/22/2022 14:37:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.376042 on epoch=28
05/22/2022 14:37:25 - INFO - __main__ - Global step 450 Train loss 0.373135 Classification-F1 0.3333333333333333 on epoch=28
05/22/2022 14:37:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.367046 on epoch=28
05/22/2022 14:37:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.326166 on epoch=29
05/22/2022 14:37:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.375161 on epoch=29
05/22/2022 14:37:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.384807 on epoch=30
05/22/2022 14:37:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.389219 on epoch=31
05/22/2022 14:37:55 - INFO - __main__ - Global step 500 Train loss 0.368480 Classification-F1 0.6760572760572761 on epoch=31
05/22/2022 14:38:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.344489 on epoch=31
05/22/2022 14:38:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.356796 on epoch=32
05/22/2022 14:38:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.382527 on epoch=33
05/22/2022 14:38:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.336418 on epoch=33
05/22/2022 14:38:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.351620 on epoch=34
05/22/2022 14:38:25 - INFO - __main__ - Global step 550 Train loss 0.354370 Classification-F1 0.5239934205821355 on epoch=34
05/22/2022 14:38:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.376882 on epoch=34
05/22/2022 14:38:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.367260 on epoch=35
05/22/2022 14:38:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.375547 on epoch=36
05/22/2022 14:38:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.346584 on epoch=36
05/22/2022 14:38:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.365175 on epoch=37
05/22/2022 14:38:55 - INFO - __main__ - Global step 600 Train loss 0.366290 Classification-F1 0.34195559333697656 on epoch=37
05/22/2022 14:39:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.387593 on epoch=38
05/22/2022 14:39:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.352381 on epoch=38
05/22/2022 14:39:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.340678 on epoch=39
05/22/2022 14:39:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.393067 on epoch=39
05/22/2022 14:39:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.343123 on epoch=40
05/22/2022 14:39:26 - INFO - __main__ - Global step 650 Train loss 0.363369 Classification-F1 0.8827480916030535 on epoch=40
05/22/2022 14:39:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.339470 on epoch=41
05/22/2022 14:39:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.361598 on epoch=41
05/22/2022 14:39:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.362352 on epoch=42
05/22/2022 14:39:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.366358 on epoch=43
05/22/2022 14:39:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.355463 on epoch=43
05/22/2022 14:39:56 - INFO - __main__ - Global step 700 Train loss 0.357048 Classification-F1 0.3333333333333333 on epoch=43
05/22/2022 14:40:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.363563 on epoch=44
05/22/2022 14:40:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.383600 on epoch=44
05/22/2022 14:40:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.345095 on epoch=45
05/22/2022 14:40:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.359989 on epoch=46
05/22/2022 14:40:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.357914 on epoch=46
05/22/2022 14:40:27 - INFO - __main__ - Global step 750 Train loss 0.362032 Classification-F1 0.6487282463186077 on epoch=46
05/22/2022 14:40:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.370743 on epoch=47
05/22/2022 14:40:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.369816 on epoch=48
05/22/2022 14:40:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.343958 on epoch=48
05/22/2022 14:40:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.362141 on epoch=49
05/22/2022 14:40:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.364634 on epoch=49
05/22/2022 14:40:57 - INFO - __main__ - Global step 800 Train loss 0.362258 Classification-F1 0.4372120586492325 on epoch=49
05/22/2022 14:41:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.359748 on epoch=50
05/22/2022 14:41:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.353748 on epoch=51
05/22/2022 14:41:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.350688 on epoch=51
05/22/2022 14:41:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.318198 on epoch=52
05/22/2022 14:41:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.363588 on epoch=53
05/22/2022 14:41:28 - INFO - __main__ - Global step 850 Train loss 0.349194 Classification-F1 0.572115049794554 on epoch=53
05/22/2022 14:41:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.304231 on epoch=53
05/22/2022 14:41:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.341410 on epoch=54
05/22/2022 14:41:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.360005 on epoch=54
05/22/2022 14:41:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.327634 on epoch=55
05/22/2022 14:41:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.342030 on epoch=56
05/22/2022 14:41:58 - INFO - __main__ - Global step 900 Train loss 0.335062 Classification-F1 0.7688667496886674 on epoch=56
05/22/2022 14:42:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.341314 on epoch=56
05/22/2022 14:42:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.339097 on epoch=57
05/22/2022 14:42:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.336538 on epoch=58
05/22/2022 14:42:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.313784 on epoch=58
05/22/2022 14:42:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.333728 on epoch=59
05/22/2022 14:42:29 - INFO - __main__ - Global step 950 Train loss 0.332892 Classification-F1 0.7717518293057861 on epoch=59
05/22/2022 14:42:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.347847 on epoch=59
05/22/2022 14:42:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.334834 on epoch=60
05/22/2022 14:42:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.342257 on epoch=61
05/22/2022 14:42:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.317277 on epoch=61
05/22/2022 14:42:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.337222 on epoch=62
05/22/2022 14:42:57 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:42:57 - INFO - __main__ - Printing 3 examples
05/22/2022 14:42:57 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/22/2022 14:42:57 - INFO - __main__ - ['negative']
05/22/2022 14:42:57 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/22/2022 14:42:57 - INFO - __main__ - ['negative']
05/22/2022 14:42:57 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/22/2022 14:42:57 - INFO - __main__ - ['negative']
05/22/2022 14:42:57 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:42:57 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:42:57 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 14:42:57 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:42:57 - INFO - __main__ - Printing 3 examples
05/22/2022 14:42:57 - INFO - __main__ -  [amazon_polarity] title: Awful and Disappointing [SEP] content: I was waiting for this movie for what seemed like a very long time.I was extremely disappointed - so much I had to turn the movie OFF!Awful direction - the world already has enough darkness - I thought a movie including quantum physics may include more light.When the 'good guy' isn't really, there is no one to root for.I don't know how it ended, but it wouldn't matter from where I could see and feel the writer was taking us.This is a movie where I didn't care for not one of the characters, which is when a story dies.I have only turned off a handfull of movies in my 40+ years and this was one of them.
05/22/2022 14:42:57 - INFO - __main__ - ['negative']
05/22/2022 14:42:57 - INFO - __main__ -  [amazon_polarity] title: Faulty electronics in a nice design [SEP] content: There have been frequent failures of this mouse of the DOA kind. Many DOA mice do not communicate with their dongle and the Logitech reset procedure does not help. It isn't clear whether this problem is a design or a production problem.There have even been many DOAs of the same kind with so-called refurb MX Revolutions recently sold by WOOT.For details check either Logitech's customer blog or WOOT's.Note also that Logitech's reset procedure involves six button actions and a switch flip, since there is no reset button on this mouse to reconnect the wireless, a seemingly clear design defect.This is a huge black eye for Logitech, heretofore a company held in high respect.
05/22/2022 14:42:57 - INFO - __main__ - ['negative']
05/22/2022 14:42:57 - INFO - __main__ -  [amazon_polarity] title: CISCO should be imbarassed to put their name on this POS! [SEP] content: THIS ROUTER DESERVES ZERO STARS!!!I've set up many routers. THis think is maddening.So i get the new router and automatically upgrade it to the latest software release before any configuration ( v4.2.1.02 (Jan 18 2012 14:10:55))Then I have one simple task - Forward incoming port 41790 to 192.168.0.50.That's it - the simplest feature on any $70 router on the planet. Guess what 3 hours later and the friggin thing still cannon do a simple port forward. I disabled the firewall, set up aUPNP service, set the software DMZ to the IP everything.Still doesnt work - Then I search online and find this has been a probelem with this router since 2007 thru three hardware revs and countless firmware revs. What crap.I WILL NEVER EVER BUY ANOTHER CISCO ROUTER - THEY HAVE BURNED ME OF THE BRAND FOREVER>BEWARE _ THIS ROUTER WILL NOT WORK CORRECTLY _ EVER!!!
05/22/2022 14:42:57 - INFO - __main__ - ['negative']
05/22/2022 14:42:57 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:42:57 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:42:58 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 14:42:59 - INFO - __main__ - Global step 1000 Train loss 0.335887 Classification-F1 0.7621829289414433 on epoch=62
05/22/2022 14:42:59 - INFO - __main__ - save last model!
05/22/2022 14:43:06 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 14:43:07 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 14:43:07 - INFO - __main__ - Printing 3 examples
05/22/2022 14:43:07 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 14:43:07 - INFO - __main__ - ['negative']
05/22/2022 14:43:07 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 14:43:07 - INFO - __main__ - ['negative']
05/22/2022 14:43:07 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 14:43:07 - INFO - __main__ - ['negative']
05/22/2022 14:43:07 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:43:08 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:43:09 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 14:43:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 14:43:10 - INFO - __main__ - Starting training!
05/22/2022 14:43:23 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_87_0.0003_8_predictions.txt
05/22/2022 14:43:23 - INFO - __main__ - Classification-F1 on test data: 0.9230
05/22/2022 14:43:23 - INFO - __main__ - prefix=amazon_polarity_128_87, lr=0.0003, bsz=8, dev_performance=0.9335684083589015, test_performance=0.9229937624947621
05/22/2022 14:43:23 - INFO - __main__ - Running ... prefix=amazon_polarity_128_87, lr=0.0002, bsz=8 ...
05/22/2022 14:43:24 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:43:24 - INFO - __main__ - Printing 3 examples
05/22/2022 14:43:24 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/22/2022 14:43:24 - INFO - __main__ - ['negative']
05/22/2022 14:43:24 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/22/2022 14:43:24 - INFO - __main__ - ['negative']
05/22/2022 14:43:24 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/22/2022 14:43:24 - INFO - __main__ - ['negative']
05/22/2022 14:43:24 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:43:24 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:43:25 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 14:43:25 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:43:25 - INFO - __main__ - Printing 3 examples
05/22/2022 14:43:25 - INFO - __main__ -  [amazon_polarity] title: Awful and Disappointing [SEP] content: I was waiting for this movie for what seemed like a very long time.I was extremely disappointed - so much I had to turn the movie OFF!Awful direction - the world already has enough darkness - I thought a movie including quantum physics may include more light.When the 'good guy' isn't really, there is no one to root for.I don't know how it ended, but it wouldn't matter from where I could see and feel the writer was taking us.This is a movie where I didn't care for not one of the characters, which is when a story dies.I have only turned off a handfull of movies in my 40+ years and this was one of them.
05/22/2022 14:43:25 - INFO - __main__ - ['negative']
05/22/2022 14:43:25 - INFO - __main__ -  [amazon_polarity] title: Faulty electronics in a nice design [SEP] content: There have been frequent failures of this mouse of the DOA kind. Many DOA mice do not communicate with their dongle and the Logitech reset procedure does not help. It isn't clear whether this problem is a design or a production problem.There have even been many DOAs of the same kind with so-called refurb MX Revolutions recently sold by WOOT.For details check either Logitech's customer blog or WOOT's.Note also that Logitech's reset procedure involves six button actions and a switch flip, since there is no reset button on this mouse to reconnect the wireless, a seemingly clear design defect.This is a huge black eye for Logitech, heretofore a company held in high respect.
05/22/2022 14:43:25 - INFO - __main__ - ['negative']
05/22/2022 14:43:25 - INFO - __main__ -  [amazon_polarity] title: CISCO should be imbarassed to put their name on this POS! [SEP] content: THIS ROUTER DESERVES ZERO STARS!!!I've set up many routers. THis think is maddening.So i get the new router and automatically upgrade it to the latest software release before any configuration ( v4.2.1.02 (Jan 18 2012 14:10:55))Then I have one simple task - Forward incoming port 41790 to 192.168.0.50.That's it - the simplest feature on any $70 router on the planet. Guess what 3 hours later and the friggin thing still cannon do a simple port forward. I disabled the firewall, set up aUPNP service, set the software DMZ to the IP everything.Still doesnt work - Then I search online and find this has been a probelem with this router since 2007 thru three hardware revs and countless firmware revs. What crap.I WILL NEVER EVER BUY ANOTHER CISCO ROUTER - THEY HAVE BURNED ME OF THE BRAND FOREVER>BEWARE _ THIS ROUTER WILL NOT WORK CORRECTLY _ EVER!!!
05/22/2022 14:43:25 - INFO - __main__ - ['negative']
05/22/2022 14:43:25 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:43:25 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:43:25 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 14:43:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 14:43:36 - INFO - __main__ - Starting training!
05/22/2022 14:43:41 - INFO - __main__ - Step 10 Global step 10 Train loss 23.769831 on epoch=0
05/22/2022 14:43:46 - INFO - __main__ - Step 20 Global step 20 Train loss 18.180996 on epoch=1
05/22/2022 14:43:51 - INFO - __main__ - Step 30 Global step 30 Train loss 16.744511 on epoch=1
05/22/2022 14:43:56 - INFO - __main__ - Step 40 Global step 40 Train loss 16.446447 on epoch=2
05/22/2022 14:44:01 - INFO - __main__ - Step 50 Global step 50 Train loss 14.625467 on epoch=3
05/22/2022 14:44:54 - INFO - __main__ - Global step 50 Train loss 17.953449 Classification-F1 0.0 on epoch=3
05/22/2022 14:45:00 - INFO - __main__ - Step 60 Global step 60 Train loss 13.918981 on epoch=3
05/22/2022 14:45:06 - INFO - __main__ - Step 70 Global step 70 Train loss 13.383911 on epoch=4
05/22/2022 14:45:11 - INFO - __main__ - Step 80 Global step 80 Train loss 13.188225 on epoch=4
05/22/2022 14:45:16 - INFO - __main__ - Step 90 Global step 90 Train loss 11.915453 on epoch=5
05/22/2022 14:45:22 - INFO - __main__ - Step 100 Global step 100 Train loss 10.281250 on epoch=6
05/22/2022 14:45:52 - INFO - __main__ - Global step 100 Train loss 12.537562 Classification-F1 0.0008263054774682682 on epoch=6
05/22/2022 14:45:59 - INFO - __main__ - Step 110 Global step 110 Train loss 8.365509 on epoch=6
05/22/2022 14:46:04 - INFO - __main__ - Step 120 Global step 120 Train loss 3.621386 on epoch=7
05/22/2022 14:46:09 - INFO - __main__ - Step 130 Global step 130 Train loss 1.601890 on epoch=8
05/22/2022 14:46:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.464725 on epoch=8
05/22/2022 14:46:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.145481 on epoch=9
05/22/2022 14:46:23 - INFO - __main__ - Global step 150 Train loss 2.839798 Classification-F1 0.9177566662077195 on epoch=9
05/22/2022 14:46:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.233226 on epoch=9
05/22/2022 14:46:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.138785 on epoch=10
05/22/2022 14:46:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.163667 on epoch=11
05/22/2022 14:46:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.060428 on epoch=11
05/22/2022 14:46:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.088369 on epoch=12
05/22/2022 14:46:54 - INFO - __main__ - Global step 200 Train loss 0.136895 Classification-F1 0.9452590420332356 on epoch=12
05/22/2022 14:47:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.032454 on epoch=13
05/22/2022 14:47:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.114110 on epoch=13
05/22/2022 14:47:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.043447 on epoch=14
05/22/2022 14:47:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.017202 on epoch=14
05/22/2022 14:47:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.073741 on epoch=15
05/22/2022 14:47:25 - INFO - __main__ - Global step 250 Train loss 0.056191 Classification-F1 0.9648432135500115 on epoch=15
05/22/2022 14:47:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.019674 on epoch=16
05/22/2022 14:47:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.020102 on epoch=16
05/22/2022 14:47:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.047359 on epoch=17
05/22/2022 14:47:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.026107 on epoch=18
05/22/2022 14:47:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.045366 on epoch=18
05/22/2022 14:47:55 - INFO - __main__ - Global step 300 Train loss 0.031722 Classification-F1 0.9531221388024171 on epoch=18
05/22/2022 14:48:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.010941 on epoch=19
05/22/2022 14:48:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.011334 on epoch=19
05/22/2022 14:48:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.011561 on epoch=20
05/22/2022 14:48:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.031778 on epoch=21
05/22/2022 14:48:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.016138 on epoch=21
05/22/2022 14:48:25 - INFO - __main__ - Global step 350 Train loss 0.016350 Classification-F1 0.9570305943389029 on epoch=21
05/22/2022 14:48:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.005817 on epoch=22
05/22/2022 14:48:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001227 on epoch=23
05/22/2022 14:48:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.004129 on epoch=23
05/22/2022 14:48:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.006551 on epoch=24
05/22/2022 14:48:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.003863 on epoch=24
05/22/2022 14:48:55 - INFO - __main__ - Global step 400 Train loss 0.004317 Classification-F1 0.953125 on epoch=24
05/22/2022 14:49:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.001264 on epoch=25
05/22/2022 14:49:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.005741 on epoch=26
05/22/2022 14:49:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.006220 on epoch=26
05/22/2022 14:49:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.002378 on epoch=27
05/22/2022 14:49:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.027419 on epoch=28
05/22/2022 14:49:25 - INFO - __main__ - Global step 450 Train loss 0.008604 Classification-F1 0.9213952345860967 on epoch=28
05/22/2022 14:49:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.035025 on epoch=28
05/22/2022 14:49:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.010227 on epoch=29
05/22/2022 14:49:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.024669 on epoch=29
05/22/2022 14:49:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.002511 on epoch=30
05/22/2022 14:49:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000878 on epoch=31
05/22/2022 14:49:54 - INFO - __main__ - Global step 500 Train loss 0.014662 Classification-F1 0.9609375 on epoch=31
05/22/2022 14:50:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.027403 on epoch=31
05/22/2022 14:50:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.001995 on epoch=32
05/22/2022 14:50:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000448 on epoch=33
05/22/2022 14:50:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.001080 on epoch=33
05/22/2022 14:50:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000383 on epoch=34
05/22/2022 14:50:24 - INFO - __main__ - Global step 550 Train loss 0.006262 Classification-F1 0.9609375 on epoch=34
05/22/2022 14:50:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000716 on epoch=34
05/22/2022 14:50:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000495 on epoch=35
05/22/2022 14:50:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001545 on epoch=36
05/22/2022 14:50:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000818 on epoch=36
05/22/2022 14:50:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000243 on epoch=37
05/22/2022 14:50:54 - INFO - __main__ - Global step 600 Train loss 0.000763 Classification-F1 0.9609375 on epoch=37
05/22/2022 14:50:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000225 on epoch=38
05/22/2022 14:51:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000157 on epoch=38
05/22/2022 14:51:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.045128 on epoch=39
05/22/2022 14:51:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.004203 on epoch=39
05/22/2022 14:51:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.003831 on epoch=40
05/22/2022 14:51:23 - INFO - __main__ - Global step 650 Train loss 0.010709 Classification-F1 0.9570305943389029 on epoch=40
05/22/2022 14:51:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000420 on epoch=41
05/22/2022 14:51:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.002013 on epoch=41
05/22/2022 14:51:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000838 on epoch=42
05/22/2022 14:51:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000463 on epoch=43
05/22/2022 14:51:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.003293 on epoch=43
05/22/2022 14:51:53 - INFO - __main__ - Global step 700 Train loss 0.001405 Classification-F1 0.9570253483296962 on epoch=43
05/22/2022 14:51:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.015372 on epoch=44
05/22/2022 14:52:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000086 on epoch=44
05/22/2022 14:52:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000285 on epoch=45
05/22/2022 14:52:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000378 on epoch=46
05/22/2022 14:52:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000039 on epoch=46
05/22/2022 14:52:23 - INFO - __main__ - Global step 750 Train loss 0.003232 Classification-F1 0.9648389213606605 on epoch=46
05/22/2022 14:52:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000448 on epoch=47
05/22/2022 14:52:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.003374 on epoch=48
05/22/2022 14:52:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000146 on epoch=48
05/22/2022 14:52:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000090 on epoch=49
05/22/2022 14:52:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.003622 on epoch=49
05/22/2022 14:52:53 - INFO - __main__ - Global step 800 Train loss 0.001536 Classification-F1 0.9335115728362997 on epoch=49
05/22/2022 14:52:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000990 on epoch=50
05/22/2022 14:53:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000062 on epoch=51
05/22/2022 14:53:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000089 on epoch=51
05/22/2022 14:53:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000048 on epoch=52
05/22/2022 14:53:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000033 on epoch=53
05/22/2022 14:53:22 - INFO - __main__ - Global step 850 Train loss 0.000244 Classification-F1 0.960935115668681 on epoch=53
05/22/2022 14:53:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000248 on epoch=53
05/22/2022 14:53:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000121 on epoch=54
05/22/2022 14:53:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000025 on epoch=54
05/22/2022 14:53:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000065 on epoch=55
05/22/2022 14:53:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000113 on epoch=56
05/22/2022 14:53:52 - INFO - __main__ - Global step 900 Train loss 0.000114 Classification-F1 0.9570305943389029 on epoch=56
05/22/2022 14:53:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000047 on epoch=56
05/22/2022 14:54:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000032 on epoch=57
05/22/2022 14:54:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000022 on epoch=58
05/22/2022 14:54:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.003108 on epoch=58
05/22/2022 14:54:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000201 on epoch=59
05/22/2022 14:54:22 - INFO - __main__ - Global step 950 Train loss 0.000682 Classification-F1 0.9452590420332356 on epoch=59
05/22/2022 14:54:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000236 on epoch=59
05/22/2022 14:54:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000126 on epoch=60
05/22/2022 14:54:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000141 on epoch=61
05/22/2022 14:54:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000537 on epoch=61
05/22/2022 14:54:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000641 on epoch=62
05/22/2022 14:54:49 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:54:49 - INFO - __main__ - Printing 3 examples
05/22/2022 14:54:49 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/22/2022 14:54:49 - INFO - __main__ - ['negative']
05/22/2022 14:54:49 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/22/2022 14:54:49 - INFO - __main__ - ['negative']
05/22/2022 14:54:49 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/22/2022 14:54:49 - INFO - __main__ - ['negative']
05/22/2022 14:54:49 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:54:49 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:54:50 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 14:54:50 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:54:50 - INFO - __main__ - Printing 3 examples
05/22/2022 14:54:50 - INFO - __main__ -  [amazon_polarity] title: Awful and Disappointing [SEP] content: I was waiting for this movie for what seemed like a very long time.I was extremely disappointed - so much I had to turn the movie OFF!Awful direction - the world already has enough darkness - I thought a movie including quantum physics may include more light.When the 'good guy' isn't really, there is no one to root for.I don't know how it ended, but it wouldn't matter from where I could see and feel the writer was taking us.This is a movie where I didn't care for not one of the characters, which is when a story dies.I have only turned off a handfull of movies in my 40+ years and this was one of them.
05/22/2022 14:54:50 - INFO - __main__ - ['negative']
05/22/2022 14:54:50 - INFO - __main__ -  [amazon_polarity] title: Faulty electronics in a nice design [SEP] content: There have been frequent failures of this mouse of the DOA kind. Many DOA mice do not communicate with their dongle and the Logitech reset procedure does not help. It isn't clear whether this problem is a design or a production problem.There have even been many DOAs of the same kind with so-called refurb MX Revolutions recently sold by WOOT.For details check either Logitech's customer blog or WOOT's.Note also that Logitech's reset procedure involves six button actions and a switch flip, since there is no reset button on this mouse to reconnect the wireless, a seemingly clear design defect.This is a huge black eye for Logitech, heretofore a company held in high respect.
05/22/2022 14:54:50 - INFO - __main__ - ['negative']
05/22/2022 14:54:50 - INFO - __main__ -  [amazon_polarity] title: CISCO should be imbarassed to put their name on this POS! [SEP] content: THIS ROUTER DESERVES ZERO STARS!!!I've set up many routers. THis think is maddening.So i get the new router and automatically upgrade it to the latest software release before any configuration ( v4.2.1.02 (Jan 18 2012 14:10:55))Then I have one simple task - Forward incoming port 41790 to 192.168.0.50.That's it - the simplest feature on any $70 router on the planet. Guess what 3 hours later and the friggin thing still cannon do a simple port forward. I disabled the firewall, set up aUPNP service, set the software DMZ to the IP everything.Still doesnt work - Then I search online and find this has been a probelem with this router since 2007 thru three hardware revs and countless firmware revs. What crap.I WILL NEVER EVER BUY ANOTHER CISCO ROUTER - THEY HAVE BURNED ME OF THE BRAND FOREVER>BEWARE _ THIS ROUTER WILL NOT WORK CORRECTLY _ EVER!!!
05/22/2022 14:54:50 - INFO - __main__ - ['negative']
05/22/2022 14:54:50 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:54:50 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:54:50 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 14:54:52 - INFO - __main__ - Global step 1000 Train loss 0.000336 Classification-F1 0.9570253483296962 on epoch=62
05/22/2022 14:54:52 - INFO - __main__ - save last model!
05/22/2022 14:54:59 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 14:55:00 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 14:55:00 - INFO - __main__ - Printing 3 examples
05/22/2022 14:55:00 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 14:55:00 - INFO - __main__ - ['negative']
05/22/2022 14:55:00 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 14:55:00 - INFO - __main__ - ['negative']
05/22/2022 14:55:00 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 14:55:00 - INFO - __main__ - ['negative']
05/22/2022 14:55:00 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:55:00 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:55:01 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 14:55:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 14:55:03 - INFO - __main__ - Starting training!
05/22/2022 14:55:16 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_87_0.0002_8_predictions.txt
05/22/2022 14:55:16 - INFO - __main__ - Classification-F1 on test data: 0.9540
05/22/2022 14:55:16 - INFO - __main__ - prefix=amazon_polarity_128_87, lr=0.0002, bsz=8, dev_performance=0.9648432135500115, test_performance=0.9539909822325175
05/22/2022 14:55:16 - INFO - __main__ - Running ... prefix=amazon_polarity_128_87, lr=0.0001, bsz=8 ...
05/22/2022 14:55:17 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:55:17 - INFO - __main__ - Printing 3 examples
05/22/2022 14:55:17 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/22/2022 14:55:17 - INFO - __main__ - ['negative']
05/22/2022 14:55:17 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/22/2022 14:55:17 - INFO - __main__ - ['negative']
05/22/2022 14:55:17 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/22/2022 14:55:17 - INFO - __main__ - ['negative']
05/22/2022 14:55:17 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:55:17 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:55:18 - INFO - __main__ - Loaded 256 examples from train data
05/22/2022 14:55:18 - INFO - __main__ - Start tokenizing ... 256 instances
05/22/2022 14:55:18 - INFO - __main__ - Printing 3 examples
05/22/2022 14:55:18 - INFO - __main__ -  [amazon_polarity] title: Awful and Disappointing [SEP] content: I was waiting for this movie for what seemed like a very long time.I was extremely disappointed - so much I had to turn the movie OFF!Awful direction - the world already has enough darkness - I thought a movie including quantum physics may include more light.When the 'good guy' isn't really, there is no one to root for.I don't know how it ended, but it wouldn't matter from where I could see and feel the writer was taking us.This is a movie where I didn't care for not one of the characters, which is when a story dies.I have only turned off a handfull of movies in my 40+ years and this was one of them.
05/22/2022 14:55:18 - INFO - __main__ - ['negative']
05/22/2022 14:55:18 - INFO - __main__ -  [amazon_polarity] title: Faulty electronics in a nice design [SEP] content: There have been frequent failures of this mouse of the DOA kind. Many DOA mice do not communicate with their dongle and the Logitech reset procedure does not help. It isn't clear whether this problem is a design or a production problem.There have even been many DOAs of the same kind with so-called refurb MX Revolutions recently sold by WOOT.For details check either Logitech's customer blog or WOOT's.Note also that Logitech's reset procedure involves six button actions and a switch flip, since there is no reset button on this mouse to reconnect the wireless, a seemingly clear design defect.This is a huge black eye for Logitech, heretofore a company held in high respect.
05/22/2022 14:55:18 - INFO - __main__ - ['negative']
05/22/2022 14:55:18 - INFO - __main__ -  [amazon_polarity] title: CISCO should be imbarassed to put their name on this POS! [SEP] content: THIS ROUTER DESERVES ZERO STARS!!!I've set up many routers. THis think is maddening.So i get the new router and automatically upgrade it to the latest software release before any configuration ( v4.2.1.02 (Jan 18 2012 14:10:55))Then I have one simple task - Forward incoming port 41790 to 192.168.0.50.That's it - the simplest feature on any $70 router on the planet. Guess what 3 hours later and the friggin thing still cannon do a simple port forward. I disabled the firewall, set up aUPNP service, set the software DMZ to the IP everything.Still doesnt work - Then I search online and find this has been a probelem with this router since 2007 thru three hardware revs and countless firmware revs. What crap.I WILL NEVER EVER BUY ANOTHER CISCO ROUTER - THEY HAVE BURNED ME OF THE BRAND FOREVER>BEWARE _ THIS ROUTER WILL NOT WORK CORRECTLY _ EVER!!!
05/22/2022 14:55:18 - INFO - __main__ - ['negative']
05/22/2022 14:55:18 - INFO - __main__ - Tokenizing Input ...
05/22/2022 14:55:18 - INFO - __main__ - Tokenizing Output ...
05/22/2022 14:55:18 - INFO - __main__ - Loaded 256 examples from dev data
05/22/2022 14:55:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
05/22/2022 14:55:29 - INFO - __main__ - Starting training!
05/22/2022 14:55:34 - INFO - __main__ - Step 10 Global step 10 Train loss 22.974781 on epoch=0
05/22/2022 14:55:39 - INFO - __main__ - Step 20 Global step 20 Train loss 19.515131 on epoch=1
05/22/2022 14:55:44 - INFO - __main__ - Step 30 Global step 30 Train loss 18.010960 on epoch=1
05/22/2022 14:55:49 - INFO - __main__ - Step 40 Global step 40 Train loss 17.208385 on epoch=2
05/22/2022 14:55:55 - INFO - __main__ - Step 50 Global step 50 Train loss 16.959120 on epoch=3
05/22/2022 14:57:09 - INFO - __main__ - Global step 50 Train loss 18.933674 Classification-F1 0.0 on epoch=3
05/22/2022 14:57:15 - INFO - __main__ - Step 60 Global step 60 Train loss 16.894138 on epoch=3
05/22/2022 14:57:20 - INFO - __main__ - Step 70 Global step 70 Train loss 16.277502 on epoch=4
05/22/2022 14:57:25 - INFO - __main__ - Step 80 Global step 80 Train loss 14.624601 on epoch=4
05/22/2022 14:57:31 - INFO - __main__ - Step 90 Global step 90 Train loss 14.713878 on epoch=5
05/22/2022 14:57:36 - INFO - __main__ - Step 100 Global step 100 Train loss 14.630191 on epoch=6
05/22/2022 14:58:41 - INFO - __main__ - Global step 100 Train loss 15.428061 Classification-F1 0.0 on epoch=6
05/22/2022 14:58:46 - INFO - __main__ - Step 110 Global step 110 Train loss 14.585382 on epoch=6
05/22/2022 14:58:51 - INFO - __main__ - Step 120 Global step 120 Train loss 13.629496 on epoch=7
05/22/2022 14:58:57 - INFO - __main__ - Step 130 Global step 130 Train loss 13.406079 on epoch=8
05/22/2022 14:59:02 - INFO - __main__ - Step 140 Global step 140 Train loss 13.483478 on epoch=8
05/22/2022 14:59:07 - INFO - __main__ - Step 150 Global step 150 Train loss 12.899632 on epoch=9
05/22/2022 14:59:54 - INFO - __main__ - Global step 150 Train loss 13.600814 Classification-F1 0.0 on epoch=9
05/22/2022 14:59:59 - INFO - __main__ - Step 160 Global step 160 Train loss 11.933111 on epoch=9
05/22/2022 15:00:04 - INFO - __main__ - Step 170 Global step 170 Train loss 11.878497 on epoch=10
05/22/2022 15:00:09 - INFO - __main__ - Step 180 Global step 180 Train loss 11.469875 on epoch=11
05/22/2022 15:00:15 - INFO - __main__ - Step 190 Global step 190 Train loss 10.313991 on epoch=11
05/22/2022 15:00:20 - INFO - __main__ - Step 200 Global step 200 Train loss 8.647047 on epoch=12
05/22/2022 15:00:24 - INFO - __main__ - Global step 200 Train loss 10.848505 Classification-F1 0.018554517724779923 on epoch=12
05/22/2022 15:00:30 - INFO - __main__ - Step 210 Global step 210 Train loss 6.291747 on epoch=13
05/22/2022 15:00:36 - INFO - __main__ - Step 220 Global step 220 Train loss 4.680584 on epoch=13
05/22/2022 15:00:41 - INFO - __main__ - Step 230 Global step 230 Train loss 3.008505 on epoch=14
05/22/2022 15:00:46 - INFO - __main__ - Step 240 Global step 240 Train loss 1.337691 on epoch=14
05/22/2022 15:00:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.467223 on epoch=15
05/22/2022 15:00:54 - INFO - __main__ - Global step 250 Train loss 3.157150 Classification-F1 0.5272277227722771 on epoch=15
05/22/2022 15:01:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.357573 on epoch=16
05/22/2022 15:01:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.252240 on epoch=16
05/22/2022 15:01:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.234378 on epoch=17
05/22/2022 15:01:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.133720 on epoch=18
05/22/2022 15:01:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.243473 on epoch=18
05/22/2022 15:01:25 - INFO - __main__ - Global step 300 Train loss 0.244277 Classification-F1 0.9453091619361533 on epoch=18
05/22/2022 15:01:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.201032 on epoch=19
05/22/2022 15:01:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.065141 on epoch=19
05/22/2022 15:01:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.083649 on epoch=20
05/22/2022 15:01:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.076795 on epoch=21
05/22/2022 15:01:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.110560 on epoch=21
05/22/2022 15:01:56 - INFO - __main__ - Global step 350 Train loss 0.107435 Classification-F1 0.9492117752987319 on epoch=21
05/22/2022 15:02:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.160452 on epoch=22
05/22/2022 15:02:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.128926 on epoch=23
05/22/2022 15:02:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.138711 on epoch=23
05/22/2022 15:02:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.099667 on epoch=24
05/22/2022 15:02:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.025183 on epoch=24
05/22/2022 15:02:26 - INFO - __main__ - Global step 400 Train loss 0.110588 Classification-F1 0.9413838897284426 on epoch=24
05/22/2022 15:02:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.094556 on epoch=25
05/22/2022 15:02:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.027186 on epoch=26
05/22/2022 15:02:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.057171 on epoch=26
05/22/2022 15:02:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.011650 on epoch=27
05/22/2022 15:02:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.022880 on epoch=28
05/22/2022 15:02:56 - INFO - __main__ - Global step 450 Train loss 0.042688 Classification-F1 0.9374656488549619 on epoch=28
05/22/2022 15:03:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.044946 on epoch=28
05/22/2022 15:03:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.018527 on epoch=29
05/22/2022 15:03:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.009659 on epoch=29
05/22/2022 15:03:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.017351 on epoch=30
05/22/2022 15:03:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.012057 on epoch=31
05/22/2022 15:03:26 - INFO - __main__ - Global step 500 Train loss 0.020508 Classification-F1 0.9374389051808407 on epoch=31
05/22/2022 15:03:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.008517 on epoch=31
05/22/2022 15:03:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.007831 on epoch=32
05/22/2022 15:03:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.007460 on epoch=33
05/22/2022 15:03:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.066785 on epoch=33
05/22/2022 15:03:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.022415 on epoch=34
05/22/2022 15:03:56 - INFO - __main__ - Global step 550 Train loss 0.022602 Classification-F1 0.9492117752987319 on epoch=34
05/22/2022 15:04:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.038505 on epoch=34
05/22/2022 15:04:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.022234 on epoch=35
05/22/2022 15:04:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.038772 on epoch=36
05/22/2022 15:04:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.002783 on epoch=36
05/22/2022 15:04:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.041505 on epoch=37
05/22/2022 15:04:26 - INFO - __main__ - Global step 600 Train loss 0.028760 Classification-F1 0.9256439654513491 on epoch=37
05/22/2022 15:04:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.045753 on epoch=38
05/22/2022 15:04:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.053308 on epoch=38
05/22/2022 15:04:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.002191 on epoch=39
05/22/2022 15:04:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.031066 on epoch=39
05/22/2022 15:04:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.002891 on epoch=40
05/22/2022 15:04:55 - INFO - __main__ - Global step 650 Train loss 0.027042 Classification-F1 0.9491993710979836 on epoch=40
05/22/2022 15:05:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.001966 on epoch=41
05/22/2022 15:05:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.014066 on epoch=41
05/22/2022 15:05:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.084155 on epoch=42
05/22/2022 15:05:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.010225 on epoch=43
05/22/2022 15:05:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.057788 on epoch=43
05/22/2022 15:05:25 - INFO - __main__ - Global step 700 Train loss 0.033640 Classification-F1 0.9413982022677675 on epoch=43
05/22/2022 15:05:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.016339 on epoch=44
05/22/2022 15:05:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.003393 on epoch=44
05/22/2022 15:05:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.028992 on epoch=45
05/22/2022 15:05:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.014918 on epoch=46
05/22/2022 15:05:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.002264 on epoch=46
05/22/2022 15:05:55 - INFO - __main__ - Global step 750 Train loss 0.013181 Classification-F1 0.9491993710979836 on epoch=46
05/22/2022 15:06:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.001932 on epoch=47
05/22/2022 15:06:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.003518 on epoch=48
05/22/2022 15:06:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.001043 on epoch=48
05/22/2022 15:06:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.002759 on epoch=49
05/22/2022 15:06:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.088933 on epoch=49
05/22/2022 15:06:24 - INFO - __main__ - Global step 800 Train loss 0.019637 Classification-F1 0.9531135531135531 on epoch=49
05/22/2022 15:06:30 - INFO - __main__ - Step 810 Global step 810 Train loss 0.119568 on epoch=50
05/22/2022 15:06:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.171657 on epoch=51
05/22/2022 15:06:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.001031 on epoch=51
05/22/2022 15:06:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000863 on epoch=52
05/22/2022 15:06:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000512 on epoch=53
05/22/2022 15:06:55 - INFO - __main__ - Global step 850 Train loss 0.058726 Classification-F1 0.9570253483296962 on epoch=53
05/22/2022 15:07:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.077135 on epoch=53
05/22/2022 15:07:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.002384 on epoch=54
05/22/2022 15:07:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.133960 on epoch=54
05/22/2022 15:07:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.010121 on epoch=55
05/22/2022 15:07:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.007123 on epoch=56
05/22/2022 15:07:25 - INFO - __main__ - Global step 900 Train loss 0.046145 Classification-F1 0.9491993710979836 on epoch=56
05/22/2022 15:07:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000526 on epoch=56
05/22/2022 15:07:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.001979 on epoch=57
05/22/2022 15:07:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000273 on epoch=58
05/22/2022 15:07:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.039057 on epoch=58
05/22/2022 15:07:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000742 on epoch=59
05/22/2022 15:07:55 - INFO - __main__ - Global step 950 Train loss 0.008516 Classification-F1 0.9492179751277943 on epoch=59
05/22/2022 15:08:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.082784 on epoch=59
05/22/2022 15:08:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000752 on epoch=60
05/22/2022 15:08:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.115332 on epoch=61
05/22/2022 15:08:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000896 on epoch=61
05/22/2022 15:08:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.029685 on epoch=62
05/22/2022 15:08:25 - INFO - __main__ - Global step 1000 Train loss 0.045890 Classification-F1 0.953125 on epoch=62
05/22/2022 15:08:25 - INFO - __main__ - save last model!
05/22/2022 15:08:32 - INFO - __main__ - Loading checkpoint on the fly
05/22/2022 15:08:33 - INFO - __main__ - Start tokenizing ... 1000 instances
05/22/2022 15:08:33 - INFO - __main__ - Printing 3 examples
05/22/2022 15:08:33 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/22/2022 15:08:33 - INFO - __main__ - ['negative']
05/22/2022 15:08:33 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/22/2022 15:08:33 - INFO - __main__ - ['negative']
05/22/2022 15:08:33 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/22/2022 15:08:33 - INFO - __main__ - ['negative']
05/22/2022 15:08:33 - INFO - __main__ - Tokenizing Input ...
05/22/2022 15:08:33 - INFO - __main__ - Tokenizing Output ...
05/22/2022 15:08:34 - INFO - __main__ - Loaded 1000 examples from test data
05/22/2022 15:08:49 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-amazon_polarity/amazon_polarity_128_87_0.0001_8_predictions.txt
05/22/2022 15:08:49 - INFO - __main__ - Classification-F1 on test data: 0.9650
05/22/2022 15:08:49 - INFO - __main__ - prefix=amazon_polarity_128_87, lr=0.0001, bsz=8, dev_performance=0.9570253483296962, test_performance=0.9649996849971649
