05/17/2022 23:41:03 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-base-multitask-cls2cls-5e-1-4-20', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-base-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='4,5')
05/17/2022 23:41:03 - INFO - __main__ - models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo
05/17/2022 23:41:03 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-base-multitask-cls2cls-5e-1-4-20', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-base-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='4,5')
05/17/2022 23:41:03 - INFO - __main__ - models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo
05/17/2022 23:41:04 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/17/2022 23:41:04 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/17/2022 23:41:04 - INFO - __main__ - args.device: cuda:1
05/17/2022 23:41:04 - INFO - __main__ - args.device: cuda:0
05/17/2022 23:41:04 - INFO - __main__ - Using 2 gpus
05/17/2022 23:41:04 - INFO - __main__ - Using 2 gpus
05/17/2022 23:41:04 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/17/2022 23:41:04 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/17/2022 23:41:09 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
05/17/2022 23:41:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 23:41:10 - INFO - __main__ - Printing 3 examples
05/17/2022 23:41:10 - INFO - __main__ -  [emo] how cause yes am listening
05/17/2022 23:41:10 - INFO - __main__ - ['others']
05/17/2022 23:41:10 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/17/2022 23:41:10 - INFO - __main__ - ['others']
05/17/2022 23:41:10 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/17/2022 23:41:10 - INFO - __main__ - ['others']
05/17/2022 23:41:10 - INFO - __main__ - Tokenizing Input ...
05/17/2022 23:41:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 23:41:10 - INFO - __main__ - Printing 3 examples
05/17/2022 23:41:10 - INFO - __main__ -  [emo] how cause yes am listening
05/17/2022 23:41:10 - INFO - __main__ - ['others']
05/17/2022 23:41:10 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/17/2022 23:41:10 - INFO - __main__ - ['others']
05/17/2022 23:41:10 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/17/2022 23:41:10 - INFO - __main__ - ['others']
05/17/2022 23:41:10 - INFO - __main__ - Tokenizing Input ...
05/17/2022 23:41:10 - INFO - __main__ - Tokenizing Output ...
05/17/2022 23:41:10 - INFO - __main__ - Tokenizing Output ...
05/17/2022 23:41:10 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 23:41:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 23:41:10 - INFO - __main__ - Printing 3 examples
05/17/2022 23:41:10 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/17/2022 23:41:10 - INFO - __main__ - ['others']
05/17/2022 23:41:10 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/17/2022 23:41:10 - INFO - __main__ - ['others']
05/17/2022 23:41:10 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/17/2022 23:41:10 - INFO - __main__ - ['others']
05/17/2022 23:41:10 - INFO - __main__ - Tokenizing Input ...
05/17/2022 23:41:10 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 23:41:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 23:41:10 - INFO - __main__ - Printing 3 examples
05/17/2022 23:41:10 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/17/2022 23:41:10 - INFO - __main__ - ['others']
05/17/2022 23:41:10 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/17/2022 23:41:10 - INFO - __main__ - ['others']
05/17/2022 23:41:10 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/17/2022 23:41:10 - INFO - __main__ - ['others']
05/17/2022 23:41:10 - INFO - __main__ - Tokenizing Input ...
05/17/2022 23:41:10 - INFO - __main__ - Tokenizing Output ...
05/17/2022 23:41:10 - INFO - __main__ - Tokenizing Output ...
05/17/2022 23:41:10 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 23:41:10 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 23:41:16 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 23:41:16 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 23:41:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 23:41:17 - INFO - __main__ - Starting training!
05/17/2022 23:41:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 23:41:24 - INFO - __main__ - Starting training!
05/17/2022 23:41:26 - INFO - __main__ - Step 10 Global step 10 Train loss 3.97 on epoch=2
05/17/2022 23:41:27 - INFO - __main__ - Step 20 Global step 20 Train loss 3.30 on epoch=4
05/17/2022 23:41:29 - INFO - __main__ - Step 30 Global step 30 Train loss 2.67 on epoch=7
05/17/2022 23:41:30 - INFO - __main__ - Step 40 Global step 40 Train loss 2.06 on epoch=9
05/17/2022 23:41:31 - INFO - __main__ - Step 50 Global step 50 Train loss 1.78 on epoch=12
05/17/2022 23:41:32 - INFO - __main__ - Global step 50 Train loss 2.76 Classification-F1 0.1 on epoch=12
05/17/2022 23:41:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/17/2022 23:41:33 - INFO - __main__ - Step 60 Global step 60 Train loss 1.35 on epoch=14
05/17/2022 23:41:34 - INFO - __main__ - Step 70 Global step 70 Train loss 1.34 on epoch=17
05/17/2022 23:41:35 - INFO - __main__ - Step 80 Global step 80 Train loss 1.06 on epoch=19
05/17/2022 23:41:36 - INFO - __main__ - Step 90 Global step 90 Train loss 1.09 on epoch=22
05/17/2022 23:41:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.93 on epoch=24
05/17/2022 23:41:38 - INFO - __main__ - Global step 100 Train loss 1.15 Classification-F1 0.16138763197586728 on epoch=24
05/17/2022 23:41:38 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.16138763197586728 on epoch=24, global_step=100
05/17/2022 23:41:39 - INFO - __main__ - Step 110 Global step 110 Train loss 1.01 on epoch=27
05/17/2022 23:41:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.99 on epoch=29
05/17/2022 23:41:42 - INFO - __main__ - Step 130 Global step 130 Train loss 1.01 on epoch=32
05/17/2022 23:41:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.94 on epoch=34
05/17/2022 23:41:44 - INFO - __main__ - Step 150 Global step 150 Train loss 1.12 on epoch=37
05/17/2022 23:41:45 - INFO - __main__ - Global step 150 Train loss 1.01 Classification-F1 0.1 on epoch=37
05/17/2022 23:41:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.93 on epoch=39
05/17/2022 23:41:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.93 on epoch=42
05/17/2022 23:41:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.90 on epoch=44
05/17/2022 23:41:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.99 on epoch=47
05/17/2022 23:41:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.90 on epoch=49
05/17/2022 23:41:51 - INFO - __main__ - Global step 200 Train loss 0.93 Classification-F1 0.1758862352861128 on epoch=49
05/17/2022 23:41:51 - INFO - __main__ - Saving model with best Classification-F1: 0.16138763197586728 -> 0.1758862352861128 on epoch=49, global_step=200
05/17/2022 23:41:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.96 on epoch=52
05/17/2022 23:41:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.90 on epoch=54
05/17/2022 23:41:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.90 on epoch=57
05/17/2022 23:41:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.92 on epoch=59
05/17/2022 23:41:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.94 on epoch=62
05/17/2022 23:41:58 - INFO - __main__ - Global step 250 Train loss 0.92 Classification-F1 0.1546731022378236 on epoch=62
05/17/2022 23:41:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=64
05/17/2022 23:42:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.95 on epoch=67
05/17/2022 23:42:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.85 on epoch=69
05/17/2022 23:42:02 - INFO - __main__ - Step 290 Global step 290 Train loss 0.90 on epoch=72
05/17/2022 23:42:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.81 on epoch=74
05/17/2022 23:42:04 - INFO - __main__ - Global step 300 Train loss 0.87 Classification-F1 0.16944444444444445 on epoch=74
05/17/2022 23:42:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.92 on epoch=77
05/17/2022 23:42:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.79 on epoch=79
05/17/2022 23:42:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.89 on epoch=82
05/17/2022 23:42:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.91 on epoch=84
05/17/2022 23:42:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.85 on epoch=87
05/17/2022 23:42:11 - INFO - __main__ - Global step 350 Train loss 0.87 Classification-F1 0.1974891774891775 on epoch=87
05/17/2022 23:42:11 - INFO - __main__ - Saving model with best Classification-F1: 0.1758862352861128 -> 0.1974891774891775 on epoch=87, global_step=350
05/17/2022 23:42:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.88 on epoch=89
05/17/2022 23:42:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.82 on epoch=92
05/17/2022 23:42:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.83 on epoch=94
05/17/2022 23:42:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=97
05/17/2022 23:42:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.88 on epoch=99
05/17/2022 23:42:17 - INFO - __main__ - Global step 400 Train loss 0.86 Classification-F1 0.12499999999999999 on epoch=99
05/17/2022 23:42:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.81 on epoch=102
05/17/2022 23:42:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.95 on epoch=104
05/17/2022 23:42:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.92 on epoch=107
05/17/2022 23:42:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.92 on epoch=109
05/17/2022 23:42:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.87 on epoch=112
05/17/2022 23:42:24 - INFO - __main__ - Global step 450 Train loss 0.89 Classification-F1 0.1743014200641319 on epoch=112
05/17/2022 23:42:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.81 on epoch=114
05/17/2022 23:42:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.91 on epoch=117
05/17/2022 23:42:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.83 on epoch=119
05/17/2022 23:42:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.86 on epoch=122
05/17/2022 23:42:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.90 on epoch=124
05/17/2022 23:42:30 - INFO - __main__ - Global step 500 Train loss 0.86 Classification-F1 0.156201749871333 on epoch=124
05/17/2022 23:42:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.89 on epoch=127
05/17/2022 23:42:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.85 on epoch=129
05/17/2022 23:42:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.82 on epoch=132
05/17/2022 23:42:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.80 on epoch=134
05/17/2022 23:42:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.86 on epoch=137
05/17/2022 23:42:37 - INFO - __main__ - Global step 550 Train loss 0.84 Classification-F1 0.11285714285714285 on epoch=137
05/17/2022 23:42:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.81 on epoch=139
05/17/2022 23:42:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.78 on epoch=142
05/17/2022 23:42:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.84 on epoch=144
05/17/2022 23:42:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.79 on epoch=147
05/17/2022 23:42:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.83 on epoch=149
05/17/2022 23:42:43 - INFO - __main__ - Global step 600 Train loss 0.81 Classification-F1 0.1412763767370046 on epoch=149
05/17/2022 23:42:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.83 on epoch=152
05/17/2022 23:42:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.80 on epoch=154
05/17/2022 23:42:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.82 on epoch=157
05/17/2022 23:42:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.80 on epoch=159
05/17/2022 23:42:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.77 on epoch=162
05/17/2022 23:42:50 - INFO - __main__ - Global step 650 Train loss 0.80 Classification-F1 0.1912442396313364 on epoch=162
05/17/2022 23:42:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.96 on epoch=164
05/17/2022 23:42:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.79 on epoch=167
05/17/2022 23:42:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.85 on epoch=169
05/17/2022 23:42:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.86 on epoch=172
05/17/2022 23:42:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.84 on epoch=174
05/17/2022 23:42:56 - INFO - __main__ - Global step 700 Train loss 0.86 Classification-F1 0.15015015015015015 on epoch=174
05/17/2022 23:42:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.80 on epoch=177
05/17/2022 23:42:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.81 on epoch=179
05/17/2022 23:43:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.86 on epoch=182
05/17/2022 23:43:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.83 on epoch=184
05/17/2022 23:43:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.85 on epoch=187
05/17/2022 23:43:03 - INFO - __main__ - Global step 750 Train loss 0.83 Classification-F1 0.16059379217273953 on epoch=187
05/17/2022 23:43:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.77 on epoch=189
05/17/2022 23:43:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.82 on epoch=192
05/17/2022 23:43:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.81 on epoch=194
05/17/2022 23:43:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.81 on epoch=197
05/17/2022 23:43:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.79 on epoch=199
05/17/2022 23:43:09 - INFO - __main__ - Global step 800 Train loss 0.80 Classification-F1 0.10757575757575757 on epoch=199
05/17/2022 23:43:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.84 on epoch=202
05/17/2022 23:43:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.83 on epoch=204
05/17/2022 23:43:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.84 on epoch=207
05/17/2022 23:43:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.79 on epoch=209
05/17/2022 23:43:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.86 on epoch=212
05/17/2022 23:43:16 - INFO - __main__ - Global step 850 Train loss 0.83 Classification-F1 0.08904109589041095 on epoch=212
05/17/2022 23:43:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.79 on epoch=214
05/17/2022 23:43:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.82 on epoch=217
05/17/2022 23:43:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.81 on epoch=219
05/17/2022 23:43:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.81 on epoch=222
05/17/2022 23:43:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.81 on epoch=224
05/17/2022 23:43:22 - INFO - __main__ - Global step 900 Train loss 0.81 Classification-F1 0.1390013495276653 on epoch=224
05/17/2022 23:43:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.84 on epoch=227
05/17/2022 23:43:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.85 on epoch=229
05/17/2022 23:43:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.81 on epoch=232
05/17/2022 23:43:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.86 on epoch=234
05/17/2022 23:43:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.80 on epoch=237
05/17/2022 23:43:29 - INFO - __main__ - Global step 950 Train loss 0.83 Classification-F1 0.1853146853146853 on epoch=237
05/17/2022 23:43:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.78 on epoch=239
05/17/2022 23:43:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.85 on epoch=242
05/17/2022 23:43:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.83 on epoch=244
05/17/2022 23:43:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.77 on epoch=247
05/17/2022 23:43:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.76 on epoch=249
05/17/2022 23:43:35 - INFO - __main__ - Global step 1000 Train loss 0.80 Classification-F1 0.13149768399382397 on epoch=249
05/17/2022 23:43:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.80 on epoch=252
05/17/2022 23:43:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.72 on epoch=254
05/17/2022 23:43:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.79 on epoch=257
05/17/2022 23:43:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.78 on epoch=259
05/17/2022 23:43:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.84 on epoch=262
05/17/2022 23:43:42 - INFO - __main__ - Global step 1050 Train loss 0.79 Classification-F1 0.13233835200746966 on epoch=262
05/17/2022 23:43:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.76 on epoch=264
05/17/2022 23:43:44 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.78 on epoch=267
05/17/2022 23:43:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.75 on epoch=269
05/17/2022 23:43:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.82 on epoch=272
05/17/2022 23:43:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.76 on epoch=274
05/17/2022 23:43:49 - INFO - __main__ - Global step 1100 Train loss 0.77 Classification-F1 0.15192359429647564 on epoch=274
05/17/2022 23:43:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.80 on epoch=277
05/17/2022 23:43:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.78 on epoch=279
05/17/2022 23:43:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.81 on epoch=282
05/17/2022 23:43:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.81 on epoch=284
05/17/2022 23:43:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.80 on epoch=287
05/17/2022 23:43:55 - INFO - __main__ - Global step 1150 Train loss 0.80 Classification-F1 0.16105095884507648 on epoch=287
05/17/2022 23:43:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.77 on epoch=289
05/17/2022 23:43:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.83 on epoch=292
05/17/2022 23:43:59 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.75 on epoch=294
05/17/2022 23:44:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.76 on epoch=297
05/17/2022 23:44:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.83 on epoch=299
05/17/2022 23:44:02 - INFO - __main__ - Global step 1200 Train loss 0.79 Classification-F1 0.13035714285714284 on epoch=299
05/17/2022 23:44:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.74 on epoch=302
05/17/2022 23:44:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.78 on epoch=304
05/17/2022 23:44:05 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.71 on epoch=307
05/17/2022 23:44:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.81 on epoch=309
05/17/2022 23:44:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.73 on epoch=312
05/17/2022 23:44:08 - INFO - __main__ - Global step 1250 Train loss 0.76 Classification-F1 0.10892857142857143 on epoch=312
05/17/2022 23:44:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.70 on epoch=314
05/17/2022 23:44:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.75 on epoch=317
05/17/2022 23:44:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.77 on epoch=319
05/17/2022 23:44:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.81 on epoch=322
05/17/2022 23:44:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.84 on epoch=324
05/17/2022 23:44:15 - INFO - __main__ - Global step 1300 Train loss 0.77 Classification-F1 0.1247628083491461 on epoch=324
05/17/2022 23:44:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.78 on epoch=327
05/17/2022 23:44:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.72 on epoch=329
05/17/2022 23:44:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.83 on epoch=332
05/17/2022 23:44:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.82 on epoch=334
05/17/2022 23:44:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.79 on epoch=337
05/17/2022 23:44:21 - INFO - __main__ - Global step 1350 Train loss 0.79 Classification-F1 0.13275613275613274 on epoch=337
05/17/2022 23:44:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.77 on epoch=339
05/17/2022 23:44:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.83 on epoch=342
05/17/2022 23:44:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.74 on epoch=344
05/17/2022 23:44:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.73 on epoch=347
05/17/2022 23:44:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.70 on epoch=349
05/17/2022 23:44:28 - INFO - __main__ - Global step 1400 Train loss 0.75 Classification-F1 0.11657231085949563 on epoch=349
05/17/2022 23:44:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.75 on epoch=352
05/17/2022 23:44:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.75 on epoch=354
05/17/2022 23:44:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.83 on epoch=357
05/17/2022 23:44:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.81 on epoch=359
05/17/2022 23:44:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.77 on epoch=362
05/17/2022 23:44:34 - INFO - __main__ - Global step 1450 Train loss 0.78 Classification-F1 0.125 on epoch=362
05/17/2022 23:44:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.83 on epoch=364
05/17/2022 23:44:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.73 on epoch=367
05/17/2022 23:44:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.79 on epoch=369
05/17/2022 23:44:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.75 on epoch=372
05/17/2022 23:44:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.74 on epoch=374
05/17/2022 23:44:41 - INFO - __main__ - Global step 1500 Train loss 0.77 Classification-F1 0.14284543248737622 on epoch=374
05/17/2022 23:44:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.77 on epoch=377
05/17/2022 23:44:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.76 on epoch=379
05/17/2022 23:44:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.75 on epoch=382
05/17/2022 23:44:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.76 on epoch=384
05/17/2022 23:44:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.80 on epoch=387
05/17/2022 23:44:47 - INFO - __main__ - Global step 1550 Train loss 0.77 Classification-F1 0.12008547008547008 on epoch=387
05/17/2022 23:44:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.75 on epoch=389
05/17/2022 23:44:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.79 on epoch=392
05/17/2022 23:44:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.80 on epoch=394
05/17/2022 23:44:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.77 on epoch=397
05/17/2022 23:44:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.77 on epoch=399
05/17/2022 23:44:54 - INFO - __main__ - Global step 1600 Train loss 0.78 Classification-F1 0.1176046176046176 on epoch=399
05/17/2022 23:44:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.80 on epoch=402
05/17/2022 23:44:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.74 on epoch=404
05/17/2022 23:44:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.74 on epoch=407
05/17/2022 23:44:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.76 on epoch=409
05/17/2022 23:45:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.82 on epoch=412
05/17/2022 23:45:00 - INFO - __main__ - Global step 1650 Train loss 0.77 Classification-F1 0.09999999999999999 on epoch=412
05/17/2022 23:45:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.79 on epoch=414
05/17/2022 23:45:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.77 on epoch=417
05/17/2022 23:45:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.78 on epoch=419
05/17/2022 23:45:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.77 on epoch=422
05/17/2022 23:45:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.75 on epoch=424
05/17/2022 23:45:07 - INFO - __main__ - Global step 1700 Train loss 0.77 Classification-F1 0.14777327935222673 on epoch=424
05/17/2022 23:45:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.79 on epoch=427
05/17/2022 23:45:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.79 on epoch=429
05/17/2022 23:45:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.81 on epoch=432
05/17/2022 23:45:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.80 on epoch=434
05/17/2022 23:45:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.72 on epoch=437
05/17/2022 23:45:13 - INFO - __main__ - Global step 1750 Train loss 0.78 Classification-F1 0.13883847549909256 on epoch=437
05/17/2022 23:45:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.71 on epoch=439
05/17/2022 23:45:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.80 on epoch=442
05/17/2022 23:45:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.73 on epoch=444
05/17/2022 23:45:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.74 on epoch=447
05/17/2022 23:45:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.77 on epoch=449
05/17/2022 23:45:20 - INFO - __main__ - Global step 1800 Train loss 0.75 Classification-F1 0.10074441687344912 on epoch=449
05/17/2022 23:45:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.76 on epoch=452
05/17/2022 23:45:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.72 on epoch=454
05/17/2022 23:45:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.73 on epoch=457
05/17/2022 23:45:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.84 on epoch=459
05/17/2022 23:45:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.72 on epoch=462
05/17/2022 23:45:26 - INFO - __main__ - Global step 1850 Train loss 0.75 Classification-F1 0.13300248138957815 on epoch=462
05/17/2022 23:45:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.78 on epoch=464
05/17/2022 23:45:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.73 on epoch=467
05/17/2022 23:45:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.80 on epoch=469
05/17/2022 23:45:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.76 on epoch=472
05/17/2022 23:45:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.73 on epoch=474
05/17/2022 23:45:33 - INFO - __main__ - Global step 1900 Train loss 0.76 Classification-F1 0.09285714285714285 on epoch=474
05/17/2022 23:45:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.76 on epoch=477
05/17/2022 23:45:35 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.75 on epoch=479
05/17/2022 23:45:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.72 on epoch=482
05/17/2022 23:45:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.71 on epoch=484
05/17/2022 23:45:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.78 on epoch=487
05/17/2022 23:45:40 - INFO - __main__ - Global step 1950 Train loss 0.75 Classification-F1 0.1181214421252372 on epoch=487
05/17/2022 23:45:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.71 on epoch=489
05/17/2022 23:45:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.77 on epoch=492
05/17/2022 23:45:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.74 on epoch=494
05/17/2022 23:45:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.72 on epoch=497
05/17/2022 23:45:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.73 on epoch=499
05/17/2022 23:45:46 - INFO - __main__ - Global step 2000 Train loss 0.74 Classification-F1 0.11662763466042156 on epoch=499
05/17/2022 23:45:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.78 on epoch=502
05/17/2022 23:45:49 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.69 on epoch=504
05/17/2022 23:45:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.75 on epoch=507
05/17/2022 23:45:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.74 on epoch=509
05/17/2022 23:45:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.79 on epoch=512
05/17/2022 23:45:53 - INFO - __main__ - Global step 2050 Train loss 0.75 Classification-F1 0.121895773234667 on epoch=512
05/17/2022 23:45:54 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.80 on epoch=514
05/17/2022 23:45:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.76 on epoch=517
05/17/2022 23:45:56 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.72 on epoch=519
05/17/2022 23:45:58 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.71 on epoch=522
05/17/2022 23:45:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.76 on epoch=524
05/17/2022 23:45:59 - INFO - __main__ - Global step 2100 Train loss 0.75 Classification-F1 0.10277777777777777 on epoch=524
05/17/2022 23:46:00 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.73 on epoch=527
05/17/2022 23:46:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.70 on epoch=529
05/17/2022 23:46:03 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.80 on epoch=532
05/17/2022 23:46:04 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.71 on epoch=534
05/17/2022 23:46:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.75 on epoch=537
05/17/2022 23:46:06 - INFO - __main__ - Global step 2150 Train loss 0.74 Classification-F1 0.12590299277605782 on epoch=537
05/17/2022 23:46:07 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.73 on epoch=539
05/17/2022 23:46:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.77 on epoch=542
05/17/2022 23:46:09 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.73 on epoch=544
05/17/2022 23:46:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.80 on epoch=547
05/17/2022 23:46:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.75 on epoch=549
05/17/2022 23:46:12 - INFO - __main__ - Global step 2200 Train loss 0.76 Classification-F1 0.11177278973889145 on epoch=549
05/17/2022 23:46:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.80 on epoch=552
05/17/2022 23:46:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.74 on epoch=554
05/17/2022 23:46:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.70 on epoch=557
05/17/2022 23:46:17 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.77 on epoch=559
05/17/2022 23:46:18 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.75 on epoch=562
05/17/2022 23:46:19 - INFO - __main__ - Global step 2250 Train loss 0.75 Classification-F1 0.10547504025764896 on epoch=562
05/17/2022 23:46:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.74 on epoch=564
05/17/2022 23:46:21 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.73 on epoch=567
05/17/2022 23:46:23 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.78 on epoch=569
05/17/2022 23:46:24 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.80 on epoch=572
05/17/2022 23:46:25 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.76 on epoch=574
05/17/2022 23:46:26 - INFO - __main__ - Global step 2300 Train loss 0.76 Classification-F1 0.18679201886033003 on epoch=574
05/17/2022 23:46:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.81 on epoch=577
05/17/2022 23:46:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.69 on epoch=579
05/17/2022 23:46:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.72 on epoch=582
05/17/2022 23:46:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.77 on epoch=584
05/17/2022 23:46:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.78 on epoch=587
05/17/2022 23:46:32 - INFO - __main__ - Global step 2350 Train loss 0.76 Classification-F1 0.13022941970310392 on epoch=587
05/17/2022 23:46:33 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.72 on epoch=589
05/17/2022 23:46:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.73 on epoch=592
05/17/2022 23:46:36 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.74 on epoch=594
05/17/2022 23:46:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.76 on epoch=597
05/17/2022 23:46:38 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.71 on epoch=599
05/17/2022 23:46:39 - INFO - __main__ - Global step 2400 Train loss 0.73 Classification-F1 0.18869731800766282 on epoch=599
05/17/2022 23:46:40 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.76 on epoch=602
05/17/2022 23:46:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.75 on epoch=604
05/17/2022 23:46:42 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.72 on epoch=607
05/17/2022 23:46:44 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.74 on epoch=609
05/17/2022 23:46:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.69 on epoch=612
05/17/2022 23:46:45 - INFO - __main__ - Global step 2450 Train loss 0.73 Classification-F1 0.17978504820610086 on epoch=612
05/17/2022 23:46:46 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.75 on epoch=614
05/17/2022 23:46:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.69 on epoch=617
05/17/2022 23:46:49 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.78 on epoch=619
05/17/2022 23:46:50 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.78 on epoch=622
05/17/2022 23:46:51 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.73 on epoch=624
05/17/2022 23:46:52 - INFO - __main__ - Global step 2500 Train loss 0.74 Classification-F1 0.1911402329749104 on epoch=624
05/17/2022 23:46:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.70 on epoch=627
05/17/2022 23:46:54 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.75 on epoch=629
05/17/2022 23:46:55 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.72 on epoch=632
05/17/2022 23:46:57 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.67 on epoch=634
05/17/2022 23:46:58 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.75 on epoch=637
05/17/2022 23:46:58 - INFO - __main__ - Global step 2550 Train loss 0.72 Classification-F1 0.1911402329749104 on epoch=637
05/17/2022 23:47:00 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.70 on epoch=639
05/17/2022 23:47:01 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.73 on epoch=642
05/17/2022 23:47:02 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.73 on epoch=644
05/17/2022 23:47:03 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.73 on epoch=647
05/17/2022 23:47:04 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.70 on epoch=649
05/17/2022 23:47:05 - INFO - __main__ - Global step 2600 Train loss 0.72 Classification-F1 0.15620915032679739 on epoch=649
05/17/2022 23:47:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.74 on epoch=652
05/17/2022 23:47:07 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.72 on epoch=654
05/17/2022 23:47:08 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.68 on epoch=657
05/17/2022 23:47:10 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.68 on epoch=659
05/17/2022 23:47:11 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.74 on epoch=662
05/17/2022 23:47:11 - INFO - __main__ - Global step 2650 Train loss 0.71 Classification-F1 0.24071207430340558 on epoch=662
05/17/2022 23:47:11 - INFO - __main__ - Saving model with best Classification-F1: 0.1974891774891775 -> 0.24071207430340558 on epoch=662, global_step=2650
05/17/2022 23:47:13 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.77 on epoch=664
05/17/2022 23:47:14 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.81 on epoch=667
05/17/2022 23:47:15 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.76 on epoch=669
05/17/2022 23:47:16 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.74 on epoch=672
05/17/2022 23:47:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.73 on epoch=674
05/17/2022 23:47:18 - INFO - __main__ - Global step 2700 Train loss 0.76 Classification-F1 0.14734961781322697 on epoch=674
05/17/2022 23:47:19 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.82 on epoch=677
05/17/2022 23:47:20 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.70 on epoch=679
05/17/2022 23:47:22 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.75 on epoch=682
05/17/2022 23:47:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.71 on epoch=684
05/17/2022 23:47:24 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.79 on epoch=687
05/17/2022 23:47:25 - INFO - __main__ - Global step 2750 Train loss 0.75 Classification-F1 0.15582419408441345 on epoch=687
05/17/2022 23:47:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.72 on epoch=689
05/17/2022 23:47:27 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.69 on epoch=692
05/17/2022 23:47:28 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.70 on epoch=694
05/17/2022 23:47:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.73 on epoch=697
05/17/2022 23:47:31 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.78 on epoch=699
05/17/2022 23:47:31 - INFO - __main__ - Global step 2800 Train loss 0.73 Classification-F1 0.21250000000000002 on epoch=699
05/17/2022 23:47:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.75 on epoch=702
05/17/2022 23:47:34 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.77 on epoch=704
05/17/2022 23:47:35 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.72 on epoch=707
05/17/2022 23:47:36 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.75 on epoch=709
05/17/2022 23:47:37 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.75 on epoch=712
05/17/2022 23:47:38 - INFO - __main__ - Global step 2850 Train loss 0.75 Classification-F1 0.20093703148425784 on epoch=712
05/17/2022 23:47:39 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.71 on epoch=714
05/17/2022 23:47:40 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.74 on epoch=717
05/17/2022 23:47:41 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.73 on epoch=719
05/17/2022 23:47:43 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.71 on epoch=722
05/17/2022 23:47:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.73 on epoch=724
05/17/2022 23:47:44 - INFO - __main__ - Global step 2900 Train loss 0.72 Classification-F1 0.24104774535809018 on epoch=724
05/17/2022 23:47:44 - INFO - __main__ - Saving model with best Classification-F1: 0.24071207430340558 -> 0.24104774535809018 on epoch=724, global_step=2900
05/17/2022 23:47:46 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.70 on epoch=727
05/17/2022 23:47:47 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.73 on epoch=729
05/17/2022 23:47:48 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.76 on epoch=732
05/17/2022 23:47:49 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.70 on epoch=734
05/17/2022 23:47:50 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.69 on epoch=737
05/17/2022 23:47:51 - INFO - __main__ - Global step 2950 Train loss 0.72 Classification-F1 0.19004879301489472 on epoch=737
05/17/2022 23:47:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.74 on epoch=739
05/17/2022 23:47:53 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.73 on epoch=742
05/17/2022 23:47:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.69 on epoch=744
05/17/2022 23:47:56 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.77 on epoch=747
05/17/2022 23:47:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.69 on epoch=749
05/17/2022 23:47:58 - INFO - __main__ - Global step 3000 Train loss 0.73 Classification-F1 0.1935185185185185 on epoch=749
05/17/2022 23:47:58 - INFO - __main__ - save last model!
05/17/2022 23:47:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 23:47:58 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 23:47:58 - INFO - __main__ - Printing 3 examples
05/17/2022 23:47:58 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 23:47:58 - INFO - __main__ - ['others']
05/17/2022 23:47:58 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 23:47:58 - INFO - __main__ - ['others']
05/17/2022 23:47:58 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 23:47:58 - INFO - __main__ - ['others']
05/17/2022 23:47:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 23:47:58 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 23:47:58 - INFO - __main__ - Printing 3 examples
05/17/2022 23:47:58 - INFO - __main__ -  [emo] how cause yes am listening
05/17/2022 23:47:58 - INFO - __main__ - ['others']
05/17/2022 23:47:58 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/17/2022 23:47:58 - INFO - __main__ - ['others']
05/17/2022 23:47:58 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/17/2022 23:47:58 - INFO - __main__ - ['others']
05/17/2022 23:47:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 23:47:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 23:47:58 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 23:47:58 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 23:47:58 - INFO - __main__ - Printing 3 examples
05/17/2022 23:47:58 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/17/2022 23:47:58 - INFO - __main__ - ['others']
05/17/2022 23:47:58 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/17/2022 23:47:58 - INFO - __main__ - ['others']
05/17/2022 23:47:58 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/17/2022 23:47:58 - INFO - __main__ - ['others']
05/17/2022 23:47:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 23:47:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 23:47:58 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 23:48:00 - INFO - __main__ - Tokenizing Output ...
05/17/2022 23:48:04 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 23:48:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 23:48:04 - INFO - __main__ - Starting training!
05/17/2022 23:48:05 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 23:48:47 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_100_0.5_8_predictions.txt
05/17/2022 23:48:47 - INFO - __main__ - Classification-F1 on test data: 0.1098
05/17/2022 23:48:47 - INFO - __main__ - prefix=emo_16_100, lr=0.5, bsz=8, dev_performance=0.24104774535809018, test_performance=0.10980573185676215
05/17/2022 23:48:47 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.4, bsz=8 ...
05/17/2022 23:48:48 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 23:48:48 - INFO - __main__ - Printing 3 examples
05/17/2022 23:48:48 - INFO - __main__ -  [emo] how cause yes am listening
05/17/2022 23:48:48 - INFO - __main__ - ['others']
05/17/2022 23:48:48 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/17/2022 23:48:48 - INFO - __main__ - ['others']
05/17/2022 23:48:48 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/17/2022 23:48:48 - INFO - __main__ - ['others']
05/17/2022 23:48:48 - INFO - __main__ - Tokenizing Input ...
05/17/2022 23:48:48 - INFO - __main__ - Tokenizing Output ...
05/17/2022 23:48:48 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 23:48:48 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 23:48:48 - INFO - __main__ - Printing 3 examples
05/17/2022 23:48:48 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/17/2022 23:48:48 - INFO - __main__ - ['others']
05/17/2022 23:48:48 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/17/2022 23:48:48 - INFO - __main__ - ['others']
05/17/2022 23:48:48 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/17/2022 23:48:48 - INFO - __main__ - ['others']
05/17/2022 23:48:48 - INFO - __main__ - Tokenizing Input ...
05/17/2022 23:48:48 - INFO - __main__ - Tokenizing Output ...
05/17/2022 23:48:48 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 23:48:54 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 23:48:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 23:48:54 - INFO - __main__ - Starting training!
05/17/2022 23:48:56 - INFO - __main__ - Step 10 Global step 10 Train loss 4.05 on epoch=2
05/17/2022 23:48:57 - INFO - __main__ - Step 20 Global step 20 Train loss 3.59 on epoch=4
05/17/2022 23:48:58 - INFO - __main__ - Step 30 Global step 30 Train loss 2.90 on epoch=7
05/17/2022 23:48:59 - INFO - __main__ - Step 40 Global step 40 Train loss 2.42 on epoch=9
05/17/2022 23:49:01 - INFO - __main__ - Step 50 Global step 50 Train loss 2.05 on epoch=12
05/17/2022 23:49:01 - INFO - __main__ - Global step 50 Train loss 3.00 Classification-F1 0.1 on epoch=12
05/17/2022 23:49:01 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/17/2022 23:49:02 - INFO - __main__ - Step 60 Global step 60 Train loss 1.66 on epoch=14
05/17/2022 23:49:04 - INFO - __main__ - Step 70 Global step 70 Train loss 1.55 on epoch=17
05/17/2022 23:49:05 - INFO - __main__ - Step 80 Global step 80 Train loss 1.33 on epoch=19
05/17/2022 23:49:06 - INFO - __main__ - Step 90 Global step 90 Train loss 1.28 on epoch=22
05/17/2022 23:49:07 - INFO - __main__ - Step 100 Global step 100 Train loss 1.02 on epoch=24
05/17/2022 23:49:08 - INFO - __main__ - Global step 100 Train loss 1.37 Classification-F1 0.1609907120743034 on epoch=24
05/17/2022 23:49:08 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1609907120743034 on epoch=24, global_step=100
05/17/2022 23:49:09 - INFO - __main__ - Step 110 Global step 110 Train loss 1.11 on epoch=27
05/17/2022 23:49:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.92 on epoch=29
05/17/2022 23:49:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.97 on epoch=32
05/17/2022 23:49:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.97 on epoch=34
05/17/2022 23:49:14 - INFO - __main__ - Step 150 Global step 150 Train loss 1.06 on epoch=37
05/17/2022 23:49:14 - INFO - __main__ - Global step 150 Train loss 1.01 Classification-F1 0.1 on epoch=37
05/17/2022 23:49:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.96 on epoch=39
05/17/2022 23:49:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.98 on epoch=42
05/17/2022 23:49:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.96 on epoch=44
05/17/2022 23:49:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.88 on epoch=47
05/17/2022 23:49:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.84 on epoch=49
05/17/2022 23:49:21 - INFO - __main__ - Global step 200 Train loss 0.92 Classification-F1 0.17890810170221935 on epoch=49
05/17/2022 23:49:21 - INFO - __main__ - Saving model with best Classification-F1: 0.1609907120743034 -> 0.17890810170221935 on epoch=49, global_step=200
05/17/2022 23:49:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.97 on epoch=52
05/17/2022 23:49:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.94 on epoch=54
05/17/2022 23:49:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.90 on epoch=57
05/17/2022 23:49:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.86 on epoch=59
05/17/2022 23:49:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.85 on epoch=62
05/17/2022 23:49:27 - INFO - __main__ - Global step 250 Train loss 0.90 Classification-F1 0.12407862407862408 on epoch=62
05/17/2022 23:49:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.97 on epoch=64
05/17/2022 23:49:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.98 on epoch=67
05/17/2022 23:49:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.89 on epoch=69
05/17/2022 23:49:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.83 on epoch=72
05/17/2022 23:49:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.92 on epoch=74
05/17/2022 23:49:34 - INFO - __main__ - Global step 300 Train loss 0.92 Classification-F1 0.11762954139368673 on epoch=74
05/17/2022 23:49:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.93 on epoch=77
05/17/2022 23:49:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.93 on epoch=79
05/17/2022 23:49:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.90 on epoch=82
05/17/2022 23:49:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.89 on epoch=84
05/17/2022 23:49:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.91 on epoch=87
05/17/2022 23:49:40 - INFO - __main__ - Global step 350 Train loss 0.91 Classification-F1 0.13067758749069247 on epoch=87
05/17/2022 23:49:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.86 on epoch=89
05/17/2022 23:49:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.82 on epoch=92
05/17/2022 23:49:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.95 on epoch=94
05/17/2022 23:49:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.91 on epoch=97
05/17/2022 23:49:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.90 on epoch=99
05/17/2022 23:49:47 - INFO - __main__ - Global step 400 Train loss 0.89 Classification-F1 0.19791666666666669 on epoch=99
05/17/2022 23:49:47 - INFO - __main__ - Saving model with best Classification-F1: 0.17890810170221935 -> 0.19791666666666669 on epoch=99, global_step=400
05/17/2022 23:49:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.94 on epoch=102
05/17/2022 23:49:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.89 on epoch=104
05/17/2022 23:49:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.89 on epoch=107
05/17/2022 23:49:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.89 on epoch=109
05/17/2022 23:49:53 - INFO - __main__ - Step 450 Global step 450 Train loss 1.03 on epoch=112
05/17/2022 23:49:54 - INFO - __main__ - Global step 450 Train loss 0.93 Classification-F1 0.10126582278481013 on epoch=112
05/17/2022 23:49:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.90 on epoch=114
05/17/2022 23:49:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.82 on epoch=117
05/17/2022 23:49:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.98 on epoch=119
05/17/2022 23:49:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.89 on epoch=122
05/17/2022 23:49:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.82 on epoch=124
05/17/2022 23:50:00 - INFO - __main__ - Global step 500 Train loss 0.88 Classification-F1 0.19957983193277312 on epoch=124
05/17/2022 23:50:00 - INFO - __main__ - Saving model with best Classification-F1: 0.19791666666666669 -> 0.19957983193277312 on epoch=124, global_step=500
05/17/2022 23:50:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.91 on epoch=127
05/17/2022 23:50:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.84 on epoch=129
05/17/2022 23:50:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.94 on epoch=132
05/17/2022 23:50:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.79 on epoch=134
05/17/2022 23:50:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.86 on epoch=137
05/17/2022 23:50:06 - INFO - __main__ - Global step 550 Train loss 0.87 Classification-F1 0.12499999999999999 on epoch=137
05/17/2022 23:50:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.80 on epoch=139
05/17/2022 23:50:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.83 on epoch=142
05/17/2022 23:50:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.83 on epoch=144
05/17/2022 23:50:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.81 on epoch=147
05/17/2022 23:50:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.89 on epoch=149
05/17/2022 23:50:13 - INFO - __main__ - Global step 600 Train loss 0.83 Classification-F1 0.10389610389610389 on epoch=149
05/17/2022 23:50:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.81 on epoch=152
05/17/2022 23:50:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.75 on epoch=154
05/17/2022 23:50:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.83 on epoch=157
05/17/2022 23:50:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.82 on epoch=159
05/17/2022 23:50:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.83 on epoch=162
05/17/2022 23:50:20 - INFO - __main__ - Global step 650 Train loss 0.81 Classification-F1 0.09615384615384615 on epoch=162
05/17/2022 23:50:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.85 on epoch=164
05/17/2022 23:50:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.83 on epoch=167
05/17/2022 23:50:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.77 on epoch=169
05/17/2022 23:50:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.81 on epoch=172
05/17/2022 23:50:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.86 on epoch=174
05/17/2022 23:50:26 - INFO - __main__ - Global step 700 Train loss 0.83 Classification-F1 0.17573497147871872 on epoch=174
05/17/2022 23:50:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.88 on epoch=177
05/17/2022 23:50:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.78 on epoch=179
05/17/2022 23:50:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.86 on epoch=182
05/17/2022 23:50:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.88 on epoch=184
05/17/2022 23:50:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.87 on epoch=187
05/17/2022 23:50:33 - INFO - __main__ - Global step 750 Train loss 0.86 Classification-F1 0.1784090909090909 on epoch=187
05/17/2022 23:50:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.89 on epoch=189
05/17/2022 23:50:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.90 on epoch=192
05/17/2022 23:50:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.85 on epoch=194
05/17/2022 23:50:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.88 on epoch=197
05/17/2022 23:50:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.87 on epoch=199
05/17/2022 23:50:39 - INFO - __main__ - Global step 800 Train loss 0.88 Classification-F1 0.1412763767370046 on epoch=199
05/17/2022 23:50:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.84 on epoch=202
05/17/2022 23:50:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.86 on epoch=204
05/17/2022 23:50:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.80 on epoch=207
05/17/2022 23:50:44 - INFO - __main__ - Step 840 Global step 840 Train loss 0.81 on epoch=209
05/17/2022 23:50:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.79 on epoch=212
05/17/2022 23:50:46 - INFO - __main__ - Global step 850 Train loss 0.82 Classification-F1 0.2048611111111111 on epoch=212
05/17/2022 23:50:46 - INFO - __main__ - Saving model with best Classification-F1: 0.19957983193277312 -> 0.2048611111111111 on epoch=212, global_step=850
05/17/2022 23:50:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.85 on epoch=214
05/17/2022 23:50:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.79 on epoch=217
05/17/2022 23:50:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.78 on epoch=219
05/17/2022 23:50:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.79 on epoch=222
05/17/2022 23:50:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.81 on epoch=224
05/17/2022 23:50:52 - INFO - __main__ - Global step 900 Train loss 0.80 Classification-F1 0.19068450849202268 on epoch=224
05/17/2022 23:50:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.80 on epoch=227
05/17/2022 23:50:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.80 on epoch=229
05/17/2022 23:50:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.73 on epoch=232
05/17/2022 23:50:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.79 on epoch=234
05/17/2022 23:50:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.87 on epoch=237
05/17/2022 23:50:59 - INFO - __main__ - Global step 950 Train loss 0.80 Classification-F1 0.18218623481781374 on epoch=237
05/17/2022 23:51:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.82 on epoch=239
05/17/2022 23:51:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.78 on epoch=242
05/17/2022 23:51:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.76 on epoch=244
05/17/2022 23:51:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.85 on epoch=247
05/17/2022 23:51:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.79 on epoch=249
05/17/2022 23:51:05 - INFO - __main__ - Global step 1000 Train loss 0.80 Classification-F1 0.1570048309178744 on epoch=249
05/17/2022 23:51:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.77 on epoch=252
05/17/2022 23:51:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.79 on epoch=254
05/17/2022 23:51:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.78 on epoch=257
05/17/2022 23:51:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.83 on epoch=259
05/17/2022 23:51:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.81 on epoch=262
05/17/2022 23:51:12 - INFO - __main__ - Global step 1050 Train loss 0.79 Classification-F1 0.15 on epoch=262
05/17/2022 23:51:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.84 on epoch=264
05/17/2022 23:51:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.80 on epoch=267
05/17/2022 23:51:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.86 on epoch=269
05/17/2022 23:51:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.82 on epoch=272
05/17/2022 23:51:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.79 on epoch=274
05/17/2022 23:51:18 - INFO - __main__ - Global step 1100 Train loss 0.82 Classification-F1 0.2065573770491803 on epoch=274
05/17/2022 23:51:18 - INFO - __main__ - Saving model with best Classification-F1: 0.2048611111111111 -> 0.2065573770491803 on epoch=274, global_step=1100
05/17/2022 23:51:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.81 on epoch=277
05/17/2022 23:51:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.77 on epoch=279
05/17/2022 23:51:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.77 on epoch=282
05/17/2022 23:51:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.80 on epoch=284
05/17/2022 23:51:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.79 on epoch=287
05/17/2022 23:51:25 - INFO - __main__ - Global step 1150 Train loss 0.79 Classification-F1 0.09722222222222222 on epoch=287
05/17/2022 23:51:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.82 on epoch=289
05/17/2022 23:51:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.72 on epoch=292
05/17/2022 23:51:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.82 on epoch=294
05/17/2022 23:51:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.80 on epoch=297
05/17/2022 23:51:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.77 on epoch=299
05/17/2022 23:51:32 - INFO - __main__ - Global step 1200 Train loss 0.78 Classification-F1 0.16666666666666666 on epoch=299
05/17/2022 23:51:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.79 on epoch=302
05/17/2022 23:51:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.84 on epoch=304
05/17/2022 23:51:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.82 on epoch=307
05/17/2022 23:51:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.86 on epoch=309
05/17/2022 23:51:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.80 on epoch=312
05/17/2022 23:51:38 - INFO - __main__ - Global step 1250 Train loss 0.82 Classification-F1 0.10206653225806452 on epoch=312
05/17/2022 23:51:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.75 on epoch=314
05/17/2022 23:51:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.75 on epoch=317
05/17/2022 23:51:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.81 on epoch=319
05/17/2022 23:51:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.74 on epoch=322
05/17/2022 23:51:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.86 on epoch=324
05/17/2022 23:51:45 - INFO - __main__ - Global step 1300 Train loss 0.78 Classification-F1 0.13300248138957815 on epoch=324
05/17/2022 23:51:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.81 on epoch=327
05/17/2022 23:51:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.80 on epoch=329
05/17/2022 23:51:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.80 on epoch=332
05/17/2022 23:51:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.78 on epoch=334
05/17/2022 23:51:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.83 on epoch=337
05/17/2022 23:51:51 - INFO - __main__ - Global step 1350 Train loss 0.80 Classification-F1 0.11078022632519356 on epoch=337
05/17/2022 23:51:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.76 on epoch=339
05/17/2022 23:51:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.88 on epoch=342
05/17/2022 23:51:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.76 on epoch=344
05/17/2022 23:51:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.80 on epoch=347
05/17/2022 23:51:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.73 on epoch=349
05/17/2022 23:51:58 - INFO - __main__ - Global step 1400 Train loss 0.79 Classification-F1 0.09615384615384615 on epoch=349
05/17/2022 23:51:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.80 on epoch=352
05/17/2022 23:52:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.73 on epoch=354
05/17/2022 23:52:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.76 on epoch=357
05/17/2022 23:52:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.83 on epoch=359
05/17/2022 23:52:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.84 on epoch=362
05/17/2022 23:52:04 - INFO - __main__ - Global step 1450 Train loss 0.79 Classification-F1 0.1542857142857143 on epoch=362
05/17/2022 23:52:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.82 on epoch=364
05/17/2022 23:52:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.83 on epoch=367
05/17/2022 23:52:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.77 on epoch=369
05/17/2022 23:52:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.81 on epoch=372
05/17/2022 23:52:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.82 on epoch=374
05/17/2022 23:52:11 - INFO - __main__ - Global step 1500 Train loss 0.81 Classification-F1 0.10609243697478991 on epoch=374
05/17/2022 23:52:12 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.78 on epoch=377
05/17/2022 23:52:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.79 on epoch=379
05/17/2022 23:52:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.79 on epoch=382
05/17/2022 23:52:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.77 on epoch=384
05/17/2022 23:52:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.81 on epoch=387
05/17/2022 23:52:18 - INFO - __main__ - Global step 1550 Train loss 0.79 Classification-F1 0.1111111111111111 on epoch=387
05/17/2022 23:52:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.78 on epoch=389
05/17/2022 23:52:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.82 on epoch=392
05/17/2022 23:52:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.78 on epoch=394
05/17/2022 23:52:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.78 on epoch=397
05/17/2022 23:52:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.73 on epoch=399
05/17/2022 23:52:24 - INFO - __main__ - Global step 1600 Train loss 0.78 Classification-F1 0.12368421052631579 on epoch=399
05/17/2022 23:52:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.77 on epoch=402
05/17/2022 23:52:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.75 on epoch=404
05/17/2022 23:52:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.75 on epoch=407
05/17/2022 23:52:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.86 on epoch=409
05/17/2022 23:52:30 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.81 on epoch=412
05/17/2022 23:52:31 - INFO - __main__ - Global step 1650 Train loss 0.79 Classification-F1 0.11666666666666667 on epoch=412
05/17/2022 23:52:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.75 on epoch=414
05/17/2022 23:52:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.74 on epoch=417
05/17/2022 23:52:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.84 on epoch=419
05/17/2022 23:52:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.76 on epoch=422
05/17/2022 23:52:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.79 on epoch=424
05/17/2022 23:52:37 - INFO - __main__ - Global step 1700 Train loss 0.78 Classification-F1 0.12368421052631579 on epoch=424
05/17/2022 23:52:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.81 on epoch=427
05/17/2022 23:52:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.74 on epoch=429
05/17/2022 23:52:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.77 on epoch=432
05/17/2022 23:52:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.76 on epoch=434
05/17/2022 23:52:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.75 on epoch=437
05/17/2022 23:52:44 - INFO - __main__ - Global step 1750 Train loss 0.76 Classification-F1 0.11208791208791208 on epoch=437
05/17/2022 23:52:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.79 on epoch=439
05/17/2022 23:52:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.79 on epoch=442
05/17/2022 23:52:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.79 on epoch=444
05/17/2022 23:52:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.81 on epoch=447
05/17/2022 23:52:50 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.85 on epoch=449
05/17/2022 23:52:50 - INFO - __main__ - Global step 1800 Train loss 0.80 Classification-F1 0.1111111111111111 on epoch=449
05/17/2022 23:52:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.82 on epoch=452
05/17/2022 23:52:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.77 on epoch=454
05/17/2022 23:52:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.82 on epoch=457
05/17/2022 23:52:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.80 on epoch=459
05/17/2022 23:52:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.82 on epoch=462
05/17/2022 23:52:57 - INFO - __main__ - Global step 1850 Train loss 0.80 Classification-F1 0.12499999999999999 on epoch=462
05/17/2022 23:52:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.84 on epoch=464
05/17/2022 23:52:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.77 on epoch=467
05/17/2022 23:53:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.80 on epoch=469
05/17/2022 23:53:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.84 on epoch=472
05/17/2022 23:53:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.81 on epoch=474
05/17/2022 23:53:03 - INFO - __main__ - Global step 1900 Train loss 0.81 Classification-F1 0.15859154929577468 on epoch=474
05/17/2022 23:53:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.75 on epoch=477
05/17/2022 23:53:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.80 on epoch=479
05/17/2022 23:53:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.75 on epoch=482
05/17/2022 23:53:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.77 on epoch=484
05/17/2022 23:53:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.81 on epoch=487
05/17/2022 23:53:10 - INFO - __main__ - Global step 1950 Train loss 0.78 Classification-F1 0.177928916191312 on epoch=487
05/17/2022 23:53:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.72 on epoch=489
05/17/2022 23:53:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.73 on epoch=492
05/17/2022 23:53:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.77 on epoch=494
05/17/2022 23:53:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.72 on epoch=497
05/17/2022 23:53:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.80 on epoch=499
05/17/2022 23:53:17 - INFO - __main__ - Global step 2000 Train loss 0.75 Classification-F1 0.13846153846153847 on epoch=499
05/17/2022 23:53:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.74 on epoch=502
05/17/2022 23:53:19 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.80 on epoch=504
05/17/2022 23:53:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.80 on epoch=507
05/17/2022 23:53:22 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.73 on epoch=509
05/17/2022 23:53:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.77 on epoch=512
05/17/2022 23:53:23 - INFO - __main__ - Global step 2050 Train loss 0.77 Classification-F1 0.12198332602018429 on epoch=512
05/17/2022 23:53:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.81 on epoch=514
05/17/2022 23:53:26 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.76 on epoch=517
05/17/2022 23:53:27 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.77 on epoch=519
05/17/2022 23:53:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.80 on epoch=522
05/17/2022 23:53:29 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.77 on epoch=524
05/17/2022 23:53:30 - INFO - __main__ - Global step 2100 Train loss 0.78 Classification-F1 0.10843920145190562 on epoch=524
05/17/2022 23:53:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.75 on epoch=527
05/17/2022 23:53:32 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.70 on epoch=529
05/17/2022 23:53:34 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.77 on epoch=532
05/17/2022 23:53:35 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.84 on epoch=534
05/17/2022 23:53:36 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.80 on epoch=537
05/17/2022 23:53:36 - INFO - __main__ - Global step 2150 Train loss 0.77 Classification-F1 0.1111111111111111 on epoch=537
05/17/2022 23:53:38 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.74 on epoch=539
05/17/2022 23:53:39 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.82 on epoch=542
05/17/2022 23:53:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.74 on epoch=544
05/17/2022 23:53:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.79 on epoch=547
05/17/2022 23:53:43 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.80 on epoch=549
05/17/2022 23:53:43 - INFO - __main__ - Global step 2200 Train loss 0.78 Classification-F1 0.17171945701357466 on epoch=549
05/17/2022 23:53:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.78 on epoch=552
05/17/2022 23:53:45 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.71 on epoch=554
05/17/2022 23:53:47 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.78 on epoch=557
05/17/2022 23:53:48 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.77 on epoch=559
05/17/2022 23:53:49 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.78 on epoch=562
05/17/2022 23:53:50 - INFO - __main__ - Global step 2250 Train loss 0.76 Classification-F1 0.11657231085949563 on epoch=562
05/17/2022 23:53:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.78 on epoch=564
05/17/2022 23:53:52 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.78 on epoch=567
05/17/2022 23:53:53 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.76 on epoch=569
05/17/2022 23:53:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.76 on epoch=572
05/17/2022 23:53:56 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.81 on epoch=574
05/17/2022 23:53:56 - INFO - __main__ - Global step 2300 Train loss 0.78 Classification-F1 0.11053864168618267 on epoch=574
05/17/2022 23:53:57 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.80 on epoch=577
05/17/2022 23:53:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.75 on epoch=579
05/17/2022 23:54:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.74 on epoch=582
05/17/2022 23:54:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.78 on epoch=584
05/17/2022 23:54:02 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.83 on epoch=587
05/17/2022 23:54:03 - INFO - __main__ - Global step 2350 Train loss 0.78 Classification-F1 0.12482435597189696 on epoch=587
05/17/2022 23:54:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.76 on epoch=589
05/17/2022 23:54:05 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.83 on epoch=592
05/17/2022 23:54:07 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.77 on epoch=594
05/17/2022 23:54:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.75 on epoch=597
05/17/2022 23:54:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.79 on epoch=599
05/17/2022 23:54:09 - INFO - __main__ - Global step 2400 Train loss 0.78 Classification-F1 0.12169312169312169 on epoch=599
05/17/2022 23:54:11 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.77 on epoch=602
05/17/2022 23:54:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.77 on epoch=604
05/17/2022 23:54:13 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.75 on epoch=607
05/17/2022 23:54:14 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.75 on epoch=609
05/17/2022 23:54:15 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.74 on epoch=612
05/17/2022 23:54:16 - INFO - __main__ - Global step 2450 Train loss 0.75 Classification-F1 0.11714285714285715 on epoch=612
05/17/2022 23:54:17 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.76 on epoch=614
05/17/2022 23:54:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.74 on epoch=617
05/17/2022 23:54:20 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.77 on epoch=619
05/17/2022 23:54:21 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.78 on epoch=622
05/17/2022 23:54:22 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.72 on epoch=624
05/17/2022 23:54:23 - INFO - __main__ - Global step 2500 Train loss 0.75 Classification-F1 0.09493670886075949 on epoch=624
05/17/2022 23:54:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.75 on epoch=627
05/17/2022 23:54:25 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.79 on epoch=629
05/17/2022 23:54:26 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.83 on epoch=632
05/17/2022 23:54:27 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.79 on epoch=634
05/17/2022 23:54:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.75 on epoch=637
05/17/2022 23:54:29 - INFO - __main__ - Global step 2550 Train loss 0.78 Classification-F1 0.1313186813186813 on epoch=637
05/17/2022 23:54:30 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.78 on epoch=639
05/17/2022 23:54:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.74 on epoch=642
05/17/2022 23:54:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.77 on epoch=644
05/17/2022 23:54:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.73 on epoch=647
05/17/2022 23:54:35 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.69 on epoch=649
05/17/2022 23:54:36 - INFO - __main__ - Global step 2600 Train loss 0.74 Classification-F1 0.10256410256410256 on epoch=649
05/17/2022 23:54:37 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.77 on epoch=652
05/17/2022 23:54:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.70 on epoch=654
05/17/2022 23:54:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.75 on epoch=657
05/17/2022 23:54:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.69 on epoch=659
05/17/2022 23:54:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.82 on epoch=662
05/17/2022 23:54:42 - INFO - __main__ - Global step 2650 Train loss 0.75 Classification-F1 0.09090909090909091 on epoch=662
05/17/2022 23:54:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.77 on epoch=664
05/17/2022 23:54:45 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.76 on epoch=667
05/17/2022 23:54:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.74 on epoch=669
05/17/2022 23:54:47 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.76 on epoch=672
05/17/2022 23:54:49 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.69 on epoch=674
05/17/2022 23:54:49 - INFO - __main__ - Global step 2700 Train loss 0.75 Classification-F1 0.1238095238095238 on epoch=674
05/17/2022 23:54:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.73 on epoch=677
05/17/2022 23:54:51 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.76 on epoch=679
05/17/2022 23:54:53 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.74 on epoch=682
05/17/2022 23:54:54 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.78 on epoch=684
05/17/2022 23:54:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.74 on epoch=687
05/17/2022 23:54:56 - INFO - __main__ - Global step 2750 Train loss 0.75 Classification-F1 0.109375 on epoch=687
05/17/2022 23:54:57 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.68 on epoch=689
05/17/2022 23:54:58 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.74 on epoch=692
05/17/2022 23:54:59 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.74 on epoch=694
05/17/2022 23:55:01 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.77 on epoch=697
05/17/2022 23:55:02 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.75 on epoch=699
05/17/2022 23:55:02 - INFO - __main__ - Global step 2800 Train loss 0.74 Classification-F1 0.09589041095890412 on epoch=699
05/17/2022 23:55:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.78 on epoch=702
05/17/2022 23:55:05 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.69 on epoch=704
05/17/2022 23:55:06 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.72 on epoch=707
05/17/2022 23:55:07 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.75 on epoch=709
05/17/2022 23:55:08 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.73 on epoch=712
05/17/2022 23:55:09 - INFO - __main__ - Global step 2850 Train loss 0.73 Classification-F1 0.14583333333333331 on epoch=712
05/17/2022 23:55:10 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.73 on epoch=714
05/17/2022 23:55:11 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.83 on epoch=717
05/17/2022 23:55:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.75 on epoch=719
05/17/2022 23:55:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.72 on epoch=722
05/17/2022 23:55:15 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.73 on epoch=724
05/17/2022 23:55:15 - INFO - __main__ - Global step 2900 Train loss 0.75 Classification-F1 0.10494505494505495 on epoch=724
05/17/2022 23:55:17 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.75 on epoch=727
05/17/2022 23:55:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.73 on epoch=729
05/17/2022 23:55:19 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.76 on epoch=732
05/17/2022 23:55:20 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.75 on epoch=734
05/17/2022 23:55:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.71 on epoch=737
05/17/2022 23:55:22 - INFO - __main__ - Global step 2950 Train loss 0.74 Classification-F1 0.09305210918114144 on epoch=737
05/17/2022 23:55:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.76 on epoch=739
05/17/2022 23:55:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.71 on epoch=742
05/17/2022 23:55:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.70 on epoch=744
05/17/2022 23:55:27 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.76 on epoch=747
05/17/2022 23:55:28 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.71 on epoch=749
05/17/2022 23:55:29 - INFO - __main__ - Global step 3000 Train loss 0.73 Classification-F1 0.13021778584392013 on epoch=749
05/17/2022 23:55:29 - INFO - __main__ - save last model!
05/17/2022 23:55:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/17/2022 23:55:29 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 23:55:29 - INFO - __main__ - Printing 3 examples
05/17/2022 23:55:29 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 23:55:29 - INFO - __main__ - ['others']
05/17/2022 23:55:29 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 23:55:29 - INFO - __main__ - ['others']
05/17/2022 23:55:29 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 23:55:29 - INFO - __main__ - ['others']
05/17/2022 23:55:29 - INFO - __main__ - Tokenizing Input ...
05/17/2022 23:55:29 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 23:55:29 - INFO - __main__ - Printing 3 examples
05/17/2022 23:55:29 - INFO - __main__ -  [emo] how cause yes am listening
05/17/2022 23:55:29 - INFO - __main__ - ['others']
05/17/2022 23:55:29 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/17/2022 23:55:29 - INFO - __main__ - ['others']
05/17/2022 23:55:29 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/17/2022 23:55:29 - INFO - __main__ - ['others']
05/17/2022 23:55:29 - INFO - __main__ - Tokenizing Input ...
05/17/2022 23:55:29 - INFO - __main__ - Tokenizing Output ...
05/17/2022 23:55:29 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 23:55:29 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 23:55:29 - INFO - __main__ - Printing 3 examples
05/17/2022 23:55:29 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/17/2022 23:55:29 - INFO - __main__ - ['others']
05/17/2022 23:55:29 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/17/2022 23:55:29 - INFO - __main__ - ['others']
05/17/2022 23:55:29 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/17/2022 23:55:29 - INFO - __main__ - ['others']
05/17/2022 23:55:29 - INFO - __main__ - Tokenizing Input ...
05/17/2022 23:55:29 - INFO - __main__ - Tokenizing Output ...
05/17/2022 23:55:29 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 23:55:31 - INFO - __main__ - Tokenizing Output ...
05/17/2022 23:55:35 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 23:55:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 23:55:35 - INFO - __main__ - Starting training!
05/17/2022 23:55:36 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 23:56:18 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_100_0.4_8_predictions.txt
05/17/2022 23:56:18 - INFO - __main__ - Classification-F1 on test data: 0.0513
05/17/2022 23:56:18 - INFO - __main__ - prefix=emo_16_100, lr=0.4, bsz=8, dev_performance=0.2065573770491803, test_performance=0.05126280658379339
05/17/2022 23:56:18 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.3, bsz=8 ...
05/17/2022 23:56:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 23:56:19 - INFO - __main__ - Printing 3 examples
05/17/2022 23:56:19 - INFO - __main__ -  [emo] how cause yes am listening
05/17/2022 23:56:19 - INFO - __main__ - ['others']
05/17/2022 23:56:19 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/17/2022 23:56:19 - INFO - __main__ - ['others']
05/17/2022 23:56:19 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/17/2022 23:56:19 - INFO - __main__ - ['others']
05/17/2022 23:56:19 - INFO - __main__ - Tokenizing Input ...
05/17/2022 23:56:19 - INFO - __main__ - Tokenizing Output ...
05/17/2022 23:56:19 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 23:56:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 23:56:19 - INFO - __main__ - Printing 3 examples
05/17/2022 23:56:19 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/17/2022 23:56:19 - INFO - __main__ - ['others']
05/17/2022 23:56:19 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/17/2022 23:56:19 - INFO - __main__ - ['others']
05/17/2022 23:56:19 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/17/2022 23:56:19 - INFO - __main__ - ['others']
05/17/2022 23:56:19 - INFO - __main__ - Tokenizing Input ...
05/17/2022 23:56:19 - INFO - __main__ - Tokenizing Output ...
05/17/2022 23:56:20 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 23:56:25 - INFO - __main__ - load prompt embedding from ckpt
05/17/2022 23:56:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/17/2022 23:56:25 - INFO - __main__ - Starting training!
05/17/2022 23:56:27 - INFO - __main__ - Step 10 Global step 10 Train loss 4.12 on epoch=2
05/17/2022 23:56:28 - INFO - __main__ - Step 20 Global step 20 Train loss 3.65 on epoch=4
05/17/2022 23:56:29 - INFO - __main__ - Step 30 Global step 30 Train loss 3.46 on epoch=7
05/17/2022 23:56:30 - INFO - __main__ - Step 40 Global step 40 Train loss 2.98 on epoch=9
05/17/2022 23:56:31 - INFO - __main__ - Step 50 Global step 50 Train loss 2.64 on epoch=12
05/17/2022 23:56:32 - INFO - __main__ - Global step 50 Train loss 3.37 Classification-F1 0.1 on epoch=12
05/17/2022 23:56:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/17/2022 23:56:33 - INFO - __main__ - Step 60 Global step 60 Train loss 2.10 on epoch=14
05/17/2022 23:56:35 - INFO - __main__ - Step 70 Global step 70 Train loss 1.86 on epoch=17
05/17/2022 23:56:36 - INFO - __main__ - Step 80 Global step 80 Train loss 1.66 on epoch=19
05/17/2022 23:56:37 - INFO - __main__ - Step 90 Global step 90 Train loss 1.51 on epoch=22
05/17/2022 23:56:39 - INFO - __main__ - Step 100 Global step 100 Train loss 1.30 on epoch=24
05/17/2022 23:56:39 - INFO - __main__ - Global step 100 Train loss 1.68 Classification-F1 0.1 on epoch=24
05/17/2022 23:56:40 - INFO - __main__ - Step 110 Global step 110 Train loss 1.20 on epoch=27
05/17/2022 23:56:41 - INFO - __main__ - Step 120 Global step 120 Train loss 1.17 on epoch=29
05/17/2022 23:56:43 - INFO - __main__ - Step 130 Global step 130 Train loss 1.23 on epoch=32
05/17/2022 23:56:44 - INFO - __main__ - Step 140 Global step 140 Train loss 1.03 on epoch=34
05/17/2022 23:56:45 - INFO - __main__ - Step 150 Global step 150 Train loss 1.09 on epoch=37
05/17/2022 23:56:46 - INFO - __main__ - Global step 150 Train loss 1.14 Classification-F1 0.1982859531772575 on epoch=37
05/17/2022 23:56:46 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1982859531772575 on epoch=37, global_step=150
05/17/2022 23:56:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.97 on epoch=39
05/17/2022 23:56:48 - INFO - __main__ - Step 170 Global step 170 Train loss 1.01 on epoch=42
05/17/2022 23:56:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.92 on epoch=44
05/17/2022 23:56:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.95 on epoch=47
05/17/2022 23:56:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.93 on epoch=49
05/17/2022 23:56:52 - INFO - __main__ - Global step 200 Train loss 0.96 Classification-F1 0.10126582278481013 on epoch=49
05/17/2022 23:56:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.90 on epoch=52
05/17/2022 23:56:54 - INFO - __main__ - Step 220 Global step 220 Train loss 1.00 on epoch=54
05/17/2022 23:56:56 - INFO - __main__ - Step 230 Global step 230 Train loss 1.04 on epoch=57
05/17/2022 23:56:57 - INFO - __main__ - Step 240 Global step 240 Train loss 1.00 on epoch=59
05/17/2022 23:56:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.93 on epoch=62
05/17/2022 23:56:59 - INFO - __main__ - Global step 250 Train loss 0.98 Classification-F1 0.10833715071003205 on epoch=62
05/17/2022 23:57:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.92 on epoch=64
05/17/2022 23:57:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.96 on epoch=67
05/17/2022 23:57:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.93 on epoch=69
05/17/2022 23:57:03 - INFO - __main__ - Step 290 Global step 290 Train loss 1.04 on epoch=72
05/17/2022 23:57:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.98 on epoch=74
05/17/2022 23:57:05 - INFO - __main__ - Global step 300 Train loss 0.97 Classification-F1 0.16865079365079366 on epoch=74
05/17/2022 23:57:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.91 on epoch=77
05/17/2022 23:57:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.94 on epoch=79
05/17/2022 23:57:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.97 on epoch=82
05/17/2022 23:57:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.93 on epoch=84
05/17/2022 23:57:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.92 on epoch=87
05/17/2022 23:57:12 - INFO - __main__ - Global step 350 Train loss 0.93 Classification-F1 0.16223908918406071 on epoch=87
05/17/2022 23:57:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.89 on epoch=89
05/17/2022 23:57:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.87 on epoch=92
05/17/2022 23:57:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.92 on epoch=94
05/17/2022 23:57:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.96 on epoch=97
05/17/2022 23:57:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.85 on epoch=99
05/17/2022 23:57:18 - INFO - __main__ - Global step 400 Train loss 0.90 Classification-F1 0.10126582278481013 on epoch=99
05/17/2022 23:57:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.87 on epoch=102
05/17/2022 23:57:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.92 on epoch=104
05/17/2022 23:57:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.84 on epoch=107
05/17/2022 23:57:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.91 on epoch=109
05/17/2022 23:57:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.90 on epoch=112
05/17/2022 23:57:24 - INFO - __main__ - Global step 450 Train loss 0.89 Classification-F1 0.14383875400824553 on epoch=112
05/17/2022 23:57:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.87 on epoch=114
05/17/2022 23:57:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.79 on epoch=117
05/17/2022 23:57:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.82 on epoch=119
05/17/2022 23:57:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.80 on epoch=122
05/17/2022 23:57:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.96 on epoch=124
05/17/2022 23:57:31 - INFO - __main__ - Global step 500 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=124
05/17/2022 23:57:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.95 on epoch=127
05/17/2022 23:57:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.87 on epoch=129
05/17/2022 23:57:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.86 on epoch=132
05/17/2022 23:57:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.81 on epoch=134
05/17/2022 23:57:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.85 on epoch=137
05/17/2022 23:57:37 - INFO - __main__ - Global step 550 Train loss 0.87 Classification-F1 0.12403499742665978 on epoch=137
05/17/2022 23:57:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.87 on epoch=139
05/17/2022 23:57:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.78 on epoch=142
05/17/2022 23:57:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.87 on epoch=144
05/17/2022 23:57:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.83 on epoch=147
05/17/2022 23:57:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.93 on epoch=149
05/17/2022 23:57:44 - INFO - __main__ - Global step 600 Train loss 0.86 Classification-F1 0.13194444444444445 on epoch=149
05/17/2022 23:57:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.87 on epoch=152
05/17/2022 23:57:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.79 on epoch=154
05/17/2022 23:57:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.80 on epoch=157
05/17/2022 23:57:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.82 on epoch=159
05/17/2022 23:57:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.80 on epoch=162
05/17/2022 23:57:50 - INFO - __main__ - Global step 650 Train loss 0.81 Classification-F1 0.17744755244755245 on epoch=162
05/17/2022 23:57:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.87 on epoch=164
05/17/2022 23:57:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.91 on epoch=167
05/17/2022 23:57:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.83 on epoch=169
05/17/2022 23:57:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.85 on epoch=172
05/17/2022 23:57:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.78 on epoch=174
05/17/2022 23:57:57 - INFO - __main__ - Global step 700 Train loss 0.85 Classification-F1 0.11666666666666667 on epoch=174
05/17/2022 23:57:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.91 on epoch=177
05/17/2022 23:57:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.91 on epoch=179
05/17/2022 23:58:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.80 on epoch=182
05/17/2022 23:58:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.78 on epoch=184
05/17/2022 23:58:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.85 on epoch=187
05/17/2022 23:58:03 - INFO - __main__ - Global step 750 Train loss 0.85 Classification-F1 0.11687344913151365 on epoch=187
05/17/2022 23:58:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.87 on epoch=189
05/17/2022 23:58:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.89 on epoch=192
05/17/2022 23:58:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.83 on epoch=194
05/17/2022 23:58:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.81 on epoch=197
05/17/2022 23:58:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.78 on epoch=199
05/17/2022 23:58:10 - INFO - __main__ - Global step 800 Train loss 0.84 Classification-F1 0.09210526315789473 on epoch=199
05/17/2022 23:58:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.86 on epoch=202
05/17/2022 23:58:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.86 on epoch=204
05/17/2022 23:58:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.81 on epoch=207
05/17/2022 23:58:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.79 on epoch=209
05/17/2022 23:58:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.81 on epoch=212
05/17/2022 23:58:16 - INFO - __main__ - Global step 850 Train loss 0.83 Classification-F1 0.13333333333333333 on epoch=212
05/17/2022 23:58:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.84 on epoch=214
05/17/2022 23:58:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.84 on epoch=217
05/17/2022 23:58:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.93 on epoch=219
05/17/2022 23:58:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.78 on epoch=222
05/17/2022 23:58:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.85 on epoch=224
05/17/2022 23:58:23 - INFO - __main__ - Global step 900 Train loss 0.85 Classification-F1 0.13035714285714284 on epoch=224
05/17/2022 23:58:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.84 on epoch=227
05/17/2022 23:58:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.87 on epoch=229
05/17/2022 23:58:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.84 on epoch=232
05/17/2022 23:58:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.83 on epoch=234
05/17/2022 23:58:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.90 on epoch=237
05/17/2022 23:58:29 - INFO - __main__ - Global step 950 Train loss 0.85 Classification-F1 0.10476190476190475 on epoch=237
05/17/2022 23:58:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.89 on epoch=239
05/17/2022 23:58:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.82 on epoch=242
05/17/2022 23:58:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.83 on epoch=244
05/17/2022 23:58:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.85 on epoch=247
05/17/2022 23:58:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.79 on epoch=249
05/17/2022 23:58:36 - INFO - __main__ - Global step 1000 Train loss 0.84 Classification-F1 0.12394957983193278 on epoch=249
05/17/2022 23:58:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.97 on epoch=252
05/17/2022 23:58:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.95 on epoch=254
05/17/2022 23:58:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.79 on epoch=257
05/17/2022 23:58:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.86 on epoch=259
05/17/2022 23:58:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.86 on epoch=262
05/17/2022 23:58:42 - INFO - __main__ - Global step 1050 Train loss 0.89 Classification-F1 0.10234192037470727 on epoch=262
05/17/2022 23:58:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.85 on epoch=264
05/17/2022 23:58:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.87 on epoch=267
05/17/2022 23:58:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.80 on epoch=269
05/17/2022 23:58:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.81 on epoch=272
05/17/2022 23:58:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.85 on epoch=274
05/17/2022 23:58:49 - INFO - __main__ - Global step 1100 Train loss 0.84 Classification-F1 0.10675381263616557 on epoch=274
05/17/2022 23:58:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.89 on epoch=277
05/17/2022 23:58:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.84 on epoch=279
05/17/2022 23:58:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.83 on epoch=282
05/17/2022 23:58:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.83 on epoch=284
05/17/2022 23:58:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.78 on epoch=287
05/17/2022 23:58:55 - INFO - __main__ - Global step 1150 Train loss 0.83 Classification-F1 0.1253101736972705 on epoch=287
05/17/2022 23:58:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.87 on epoch=289
05/17/2022 23:58:58 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.89 on epoch=292
05/17/2022 23:58:59 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.80 on epoch=294
05/17/2022 23:59:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.78 on epoch=297
05/17/2022 23:59:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.81 on epoch=299
05/17/2022 23:59:02 - INFO - __main__ - Global step 1200 Train loss 0.83 Classification-F1 0.12394957983193278 on epoch=299
05/17/2022 23:59:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.78 on epoch=302
05/17/2022 23:59:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.85 on epoch=304
05/17/2022 23:59:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.81 on epoch=307
05/17/2022 23:59:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.83 on epoch=309
05/17/2022 23:59:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.73 on epoch=312
05/17/2022 23:59:08 - INFO - __main__ - Global step 1250 Train loss 0.80 Classification-F1 0.08 on epoch=312
05/17/2022 23:59:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.87 on epoch=314
05/17/2022 23:59:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.85 on epoch=317
05/17/2022 23:59:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.82 on epoch=319
05/17/2022 23:59:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.79 on epoch=322
05/17/2022 23:59:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.75 on epoch=324
05/17/2022 23:59:15 - INFO - __main__ - Global step 1300 Train loss 0.82 Classification-F1 0.08333333333333333 on epoch=324
05/17/2022 23:59:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.84 on epoch=327
05/17/2022 23:59:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.76 on epoch=329
05/17/2022 23:59:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.84 on epoch=332
05/17/2022 23:59:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.82 on epoch=334
05/17/2022 23:59:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.86 on epoch=337
05/17/2022 23:59:21 - INFO - __main__ - Global step 1350 Train loss 0.82 Classification-F1 0.1347521402927368 on epoch=337
05/17/2022 23:59:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.80 on epoch=339
05/17/2022 23:59:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.80 on epoch=342
05/17/2022 23:59:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.85 on epoch=344
05/17/2022 23:59:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.82 on epoch=347
05/17/2022 23:59:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.80 on epoch=349
05/17/2022 23:59:28 - INFO - __main__ - Global step 1400 Train loss 0.82 Classification-F1 0.09090909090909091 on epoch=349
05/17/2022 23:59:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.77 on epoch=352
05/17/2022 23:59:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.81 on epoch=354
05/17/2022 23:59:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.83 on epoch=357
05/17/2022 23:59:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.78 on epoch=359
05/17/2022 23:59:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.84 on epoch=362
05/17/2022 23:59:34 - INFO - __main__ - Global step 1450 Train loss 0.81 Classification-F1 0.1328125 on epoch=362
05/17/2022 23:59:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.84 on epoch=364
05/17/2022 23:59:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.73 on epoch=367
05/17/2022 23:59:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.74 on epoch=369
05/17/2022 23:59:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.79 on epoch=372
05/17/2022 23:59:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.80 on epoch=374
05/17/2022 23:59:41 - INFO - __main__ - Global step 1500 Train loss 0.78 Classification-F1 0.0974025974025974 on epoch=374
05/17/2022 23:59:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.84 on epoch=377
05/17/2022 23:59:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.80 on epoch=379
05/17/2022 23:59:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.79 on epoch=382
05/17/2022 23:59:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.72 on epoch=384
05/17/2022 23:59:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.77 on epoch=387
05/17/2022 23:59:47 - INFO - __main__ - Global step 1550 Train loss 0.79 Classification-F1 0.1313186813186813 on epoch=387
05/17/2022 23:59:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.84 on epoch=389
05/17/2022 23:59:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.81 on epoch=392
05/17/2022 23:59:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.85 on epoch=394
05/17/2022 23:59:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.80 on epoch=397
05/17/2022 23:59:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.74 on epoch=399
05/17/2022 23:59:54 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.11732186732186733 on epoch=399
05/17/2022 23:59:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.77 on epoch=402
05/17/2022 23:59:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.75 on epoch=404
05/17/2022 23:59:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.77 on epoch=407
05/17/2022 23:59:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.85 on epoch=409
05/18/2022 00:00:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.74 on epoch=412
05/18/2022 00:00:00 - INFO - __main__ - Global step 1650 Train loss 0.78 Classification-F1 0.11208791208791208 on epoch=412
05/18/2022 00:00:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.80 on epoch=414
05/18/2022 00:00:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.84 on epoch=417
05/18/2022 00:00:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.73 on epoch=419
05/18/2022 00:00:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.84 on epoch=422
05/18/2022 00:00:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.81 on epoch=424
05/18/2022 00:00:07 - INFO - __main__ - Global step 1700 Train loss 0.80 Classification-F1 0.11078022632519356 on epoch=424
05/18/2022 00:00:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.79 on epoch=427
05/18/2022 00:00:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.84 on epoch=429
05/18/2022 00:00:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.82 on epoch=432
05/18/2022 00:00:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.80 on epoch=434
05/18/2022 00:00:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.83 on epoch=437
05/18/2022 00:00:13 - INFO - __main__ - Global step 1750 Train loss 0.81 Classification-F1 0.11666666666666667 on epoch=437
05/18/2022 00:00:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.76 on epoch=439
05/18/2022 00:00:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.77 on epoch=442
05/18/2022 00:00:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.80 on epoch=444
05/18/2022 00:00:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.78 on epoch=447
05/18/2022 00:00:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.79 on epoch=449
05/18/2022 00:00:20 - INFO - __main__ - Global step 1800 Train loss 0.78 Classification-F1 0.0945945945945946 on epoch=449
05/18/2022 00:00:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.80 on epoch=452
05/18/2022 00:00:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.76 on epoch=454
05/18/2022 00:00:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.83 on epoch=457
05/18/2022 00:00:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.77 on epoch=459
05/18/2022 00:00:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.77 on epoch=462
05/18/2022 00:00:26 - INFO - __main__ - Global step 1850 Train loss 0.79 Classification-F1 0.11056511056511056 on epoch=462
05/18/2022 00:00:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.77 on epoch=464
05/18/2022 00:00:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.75 on epoch=467
05/18/2022 00:00:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.78 on epoch=469
05/18/2022 00:00:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.79 on epoch=472
05/18/2022 00:00:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.86 on epoch=474
05/18/2022 00:00:33 - INFO - __main__ - Global step 1900 Train loss 0.79 Classification-F1 0.0945945945945946 on epoch=474
05/18/2022 00:00:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.81 on epoch=477
05/18/2022 00:00:35 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.72 on epoch=479
05/18/2022 00:00:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.73 on epoch=482
05/18/2022 00:00:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.80 on epoch=484
05/18/2022 00:00:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.76 on epoch=487
05/18/2022 00:00:40 - INFO - __main__ - Global step 1950 Train loss 0.76 Classification-F1 0.13275613275613274 on epoch=487
05/18/2022 00:00:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.82 on epoch=489
05/18/2022 00:00:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.80 on epoch=492
05/18/2022 00:00:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.72 on epoch=494
05/18/2022 00:00:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.76 on epoch=497
05/18/2022 00:00:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.82 on epoch=499
05/18/2022 00:00:46 - INFO - __main__ - Global step 2000 Train loss 0.78 Classification-F1 0.09333333333333334 on epoch=499
05/18/2022 00:00:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.81 on epoch=502
05/18/2022 00:00:49 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.76 on epoch=504
05/18/2022 00:00:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.77 on epoch=507
05/18/2022 00:00:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.85 on epoch=509
05/18/2022 00:00:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.75 on epoch=512
05/18/2022 00:00:53 - INFO - __main__ - Global step 2050 Train loss 0.79 Classification-F1 0.09305210918114144 on epoch=512
05/18/2022 00:00:54 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.75 on epoch=514
05/18/2022 00:00:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.76 on epoch=517
05/18/2022 00:00:56 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.74 on epoch=519
05/18/2022 00:00:58 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.74 on epoch=522
05/18/2022 00:00:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.78 on epoch=524
05/18/2022 00:00:59 - INFO - __main__ - Global step 2100 Train loss 0.75 Classification-F1 0.11944444444444444 on epoch=524
05/18/2022 00:01:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.75 on epoch=527
05/18/2022 00:01:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.76 on epoch=529
05/18/2022 00:01:03 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.77 on epoch=532
05/18/2022 00:01:04 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.80 on epoch=534
05/18/2022 00:01:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.72 on epoch=537
05/18/2022 00:01:06 - INFO - __main__ - Global step 2150 Train loss 0.76 Classification-F1 0.11208791208791208 on epoch=537
05/18/2022 00:01:07 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.73 on epoch=539
05/18/2022 00:01:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.84 on epoch=542
05/18/2022 00:01:10 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.80 on epoch=544
05/18/2022 00:01:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.73 on epoch=547
05/18/2022 00:01:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.79 on epoch=549
05/18/2022 00:01:13 - INFO - __main__ - Global step 2200 Train loss 0.78 Classification-F1 0.09999999999999999 on epoch=549
05/18/2022 00:01:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.77 on epoch=552
05/18/2022 00:01:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.77 on epoch=554
05/18/2022 00:01:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.73 on epoch=557
05/18/2022 00:01:17 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.71 on epoch=559
05/18/2022 00:01:19 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.71 on epoch=562
05/18/2022 00:01:19 - INFO - __main__ - Global step 2250 Train loss 0.74 Classification-F1 0.14279379157427938 on epoch=562
05/18/2022 00:01:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.83 on epoch=564
05/18/2022 00:01:22 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.74 on epoch=567
05/18/2022 00:01:23 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.79 on epoch=569
05/18/2022 00:01:24 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.74 on epoch=572
05/18/2022 00:01:25 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.73 on epoch=574
05/18/2022 00:01:26 - INFO - __main__ - Global step 2300 Train loss 0.77 Classification-F1 0.11272141706924316 on epoch=574
05/18/2022 00:01:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.80 on epoch=577
05/18/2022 00:01:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.79 on epoch=579
05/18/2022 00:01:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.79 on epoch=582
05/18/2022 00:01:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.78 on epoch=584
05/18/2022 00:01:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.79 on epoch=587
05/18/2022 00:01:32 - INFO - __main__ - Global step 2350 Train loss 0.79 Classification-F1 0.1176046176046176 on epoch=587
05/18/2022 00:01:34 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.76 on epoch=589
05/18/2022 00:01:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.77 on epoch=592
05/18/2022 00:01:36 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.78 on epoch=594
05/18/2022 00:01:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.78 on epoch=597
05/18/2022 00:01:38 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.78 on epoch=599
05/18/2022 00:01:39 - INFO - __main__ - Global step 2400 Train loss 0.77 Classification-F1 0.10757575757575757 on epoch=599
05/18/2022 00:01:40 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.77 on epoch=602
05/18/2022 00:01:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.78 on epoch=604
05/18/2022 00:01:43 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.80 on epoch=607
05/18/2022 00:01:44 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.78 on epoch=609
05/18/2022 00:01:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.75 on epoch=612
05/18/2022 00:01:45 - INFO - __main__ - Global step 2450 Train loss 0.77 Classification-F1 0.1451048951048951 on epoch=612
05/18/2022 00:01:47 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.82 on epoch=614
05/18/2022 00:01:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.78 on epoch=617
05/18/2022 00:01:49 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.75 on epoch=619
05/18/2022 00:01:50 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.79 on epoch=622
05/18/2022 00:01:52 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.78 on epoch=624
05/18/2022 00:01:52 - INFO - __main__ - Global step 2500 Train loss 0.78 Classification-F1 0.13022941970310392 on epoch=624
05/18/2022 00:01:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.83 on epoch=627
05/18/2022 00:01:54 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.72 on epoch=629
05/18/2022 00:01:56 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.79 on epoch=632
05/18/2022 00:01:57 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.70 on epoch=634
05/18/2022 00:01:58 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.77 on epoch=637
05/18/2022 00:01:59 - INFO - __main__ - Global step 2550 Train loss 0.76 Classification-F1 0.1437908496732026 on epoch=637
05/18/2022 00:02:00 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.75 on epoch=639
05/18/2022 00:02:01 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.75 on epoch=642
05/18/2022 00:02:02 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.71 on epoch=644
05/18/2022 00:02:03 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.78 on epoch=647
05/18/2022 00:02:05 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.74 on epoch=649
05/18/2022 00:02:05 - INFO - __main__ - Global step 2600 Train loss 0.75 Classification-F1 0.13032524049473201 on epoch=649
05/18/2022 00:02:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.78 on epoch=652
05/18/2022 00:02:08 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.80 on epoch=654
05/18/2022 00:02:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.74 on epoch=657
05/18/2022 00:02:10 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.81 on epoch=659
05/18/2022 00:02:11 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.76 on epoch=662
05/18/2022 00:02:12 - INFO - __main__ - Global step 2650 Train loss 0.78 Classification-F1 0.1337028824833703 on epoch=662
05/18/2022 00:02:13 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.68 on epoch=664
05/18/2022 00:02:14 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.74 on epoch=667
05/18/2022 00:02:16 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.79 on epoch=669
05/18/2022 00:02:17 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.76 on epoch=672
05/18/2022 00:02:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.77 on epoch=674
05/18/2022 00:02:19 - INFO - __main__ - Global step 2700 Train loss 0.75 Classification-F1 0.14285714285714285 on epoch=674
05/18/2022 00:02:20 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.78 on epoch=677
05/18/2022 00:02:21 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.74 on epoch=679
05/18/2022 00:02:23 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.82 on epoch=682
05/18/2022 00:02:24 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.72 on epoch=684
05/18/2022 00:02:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.76 on epoch=687
05/18/2022 00:02:26 - INFO - __main__ - Global step 2750 Train loss 0.76 Classification-F1 0.14285714285714285 on epoch=687
05/18/2022 00:02:27 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.70 on epoch=689
05/18/2022 00:02:28 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.76 on epoch=692
05/18/2022 00:02:29 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.78 on epoch=694
05/18/2022 00:02:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.77 on epoch=697
05/18/2022 00:02:32 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.73 on epoch=699
05/18/2022 00:02:32 - INFO - __main__ - Global step 2800 Train loss 0.75 Classification-F1 0.13022941970310392 on epoch=699
05/18/2022 00:02:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.81 on epoch=702
05/18/2022 00:02:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.79 on epoch=704
05/18/2022 00:02:36 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.77 on epoch=707
05/18/2022 00:02:37 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.73 on epoch=709
05/18/2022 00:02:38 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.83 on epoch=712
05/18/2022 00:02:39 - INFO - __main__ - Global step 2850 Train loss 0.79 Classification-F1 0.14337568058076222 on epoch=712
05/18/2022 00:02:40 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.77 on epoch=714
05/18/2022 00:02:41 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.72 on epoch=717
05/18/2022 00:02:42 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.72 on epoch=719
05/18/2022 00:02:44 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.80 on epoch=722
05/18/2022 00:02:45 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.73 on epoch=724
05/18/2022 00:02:45 - INFO - __main__ - Global step 2900 Train loss 0.75 Classification-F1 0.16988795518207284 on epoch=724
05/18/2022 00:02:47 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.75 on epoch=727
05/18/2022 00:02:48 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.75 on epoch=729
05/18/2022 00:02:49 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.78 on epoch=732
05/18/2022 00:02:50 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.74 on epoch=734
05/18/2022 00:02:51 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.74 on epoch=737
05/18/2022 00:02:52 - INFO - __main__ - Global step 2950 Train loss 0.75 Classification-F1 0.12403499742665978 on epoch=737
05/18/2022 00:02:53 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.68 on epoch=739
05/18/2022 00:02:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.70 on epoch=742
05/18/2022 00:02:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.76 on epoch=744
05/18/2022 00:02:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.76 on epoch=747
05/18/2022 00:02:58 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.67 on epoch=749
05/18/2022 00:02:58 - INFO - __main__ - Global step 3000 Train loss 0.71 Classification-F1 0.11208791208791208 on epoch=749
05/18/2022 00:02:58 - INFO - __main__ - save last model!
05/18/2022 00:02:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 00:02:58 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 00:02:58 - INFO - __main__ - Printing 3 examples
05/18/2022 00:02:58 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 00:02:58 - INFO - __main__ - ['others']
05/18/2022 00:02:58 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 00:02:58 - INFO - __main__ - ['others']
05/18/2022 00:02:58 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 00:02:58 - INFO - __main__ - ['others']
05/18/2022 00:02:58 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:02:59 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:02:59 - INFO - __main__ - Printing 3 examples
05/18/2022 00:02:59 - INFO - __main__ -  [emo] how cause yes am listening
05/18/2022 00:02:59 - INFO - __main__ - ['others']
05/18/2022 00:02:59 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/18/2022 00:02:59 - INFO - __main__ - ['others']
05/18/2022 00:02:59 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/18/2022 00:02:59 - INFO - __main__ - ['others']
05/18/2022 00:02:59 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:02:59 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:02:59 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 00:02:59 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:02:59 - INFO - __main__ - Printing 3 examples
05/18/2022 00:02:59 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/18/2022 00:02:59 - INFO - __main__ - ['others']
05/18/2022 00:02:59 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/18/2022 00:02:59 - INFO - __main__ - ['others']
05/18/2022 00:02:59 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/18/2022 00:02:59 - INFO - __main__ - ['others']
05/18/2022 00:02:59 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:02:59 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:02:59 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 00:03:01 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:03:05 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 00:03:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 00:03:05 - INFO - __main__ - Starting training!
05/18/2022 00:03:06 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 00:03:48 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_100_0.3_8_predictions.txt
05/18/2022 00:03:48 - INFO - __main__ - Classification-F1 on test data: 0.0575
05/18/2022 00:03:48 - INFO - __main__ - prefix=emo_16_100, lr=0.3, bsz=8, dev_performance=0.1982859531772575, test_performance=0.057494644158672614
05/18/2022 00:03:48 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.2, bsz=8 ...
05/18/2022 00:03:49 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:03:49 - INFO - __main__ - Printing 3 examples
05/18/2022 00:03:49 - INFO - __main__ -  [emo] how cause yes am listening
05/18/2022 00:03:49 - INFO - __main__ - ['others']
05/18/2022 00:03:49 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/18/2022 00:03:49 - INFO - __main__ - ['others']
05/18/2022 00:03:49 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/18/2022 00:03:49 - INFO - __main__ - ['others']
05/18/2022 00:03:49 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:03:49 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:03:49 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 00:03:49 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:03:49 - INFO - __main__ - Printing 3 examples
05/18/2022 00:03:49 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/18/2022 00:03:49 - INFO - __main__ - ['others']
05/18/2022 00:03:49 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/18/2022 00:03:49 - INFO - __main__ - ['others']
05/18/2022 00:03:49 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/18/2022 00:03:49 - INFO - __main__ - ['others']
05/18/2022 00:03:49 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:03:49 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:03:49 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 00:03:55 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 00:03:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 00:03:55 - INFO - __main__ - Starting training!
05/18/2022 00:03:57 - INFO - __main__ - Step 10 Global step 10 Train loss 4.06 on epoch=2
05/18/2022 00:03:58 - INFO - __main__ - Step 20 Global step 20 Train loss 3.82 on epoch=4
05/18/2022 00:03:59 - INFO - __main__ - Step 30 Global step 30 Train loss 3.60 on epoch=7
05/18/2022 00:04:00 - INFO - __main__ - Step 40 Global step 40 Train loss 3.21 on epoch=9
05/18/2022 00:04:02 - INFO - __main__ - Step 50 Global step 50 Train loss 2.99 on epoch=12
05/18/2022 00:04:02 - INFO - __main__ - Global step 50 Train loss 3.54 Classification-F1 0.0 on epoch=12
05/18/2022 00:04:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
05/18/2022 00:04:04 - INFO - __main__ - Step 60 Global step 60 Train loss 2.74 on epoch=14
05/18/2022 00:04:05 - INFO - __main__ - Step 70 Global step 70 Train loss 2.63 on epoch=17
05/18/2022 00:04:06 - INFO - __main__ - Step 80 Global step 80 Train loss 2.25 on epoch=19
05/18/2022 00:04:07 - INFO - __main__ - Step 90 Global step 90 Train loss 2.20 on epoch=22
05/18/2022 00:04:08 - INFO - __main__ - Step 100 Global step 100 Train loss 1.87 on epoch=24
05/18/2022 00:04:09 - INFO - __main__ - Global step 100 Train loss 2.34 Classification-F1 0.1 on epoch=24
05/18/2022 00:04:09 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.1 on epoch=24, global_step=100
05/18/2022 00:04:10 - INFO - __main__ - Step 110 Global step 110 Train loss 1.85 on epoch=27
05/18/2022 00:04:12 - INFO - __main__ - Step 120 Global step 120 Train loss 1.66 on epoch=29
05/18/2022 00:04:13 - INFO - __main__ - Step 130 Global step 130 Train loss 1.65 on epoch=32
05/18/2022 00:04:14 - INFO - __main__ - Step 140 Global step 140 Train loss 1.33 on epoch=34
05/18/2022 00:04:15 - INFO - __main__ - Step 150 Global step 150 Train loss 1.26 on epoch=37
05/18/2022 00:04:16 - INFO - __main__ - Global step 150 Train loss 1.55 Classification-F1 0.1778584392014519 on epoch=37
05/18/2022 00:04:16 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1778584392014519 on epoch=37, global_step=150
05/18/2022 00:04:17 - INFO - __main__ - Step 160 Global step 160 Train loss 1.27 on epoch=39
05/18/2022 00:04:18 - INFO - __main__ - Step 170 Global step 170 Train loss 1.23 on epoch=42
05/18/2022 00:04:20 - INFO - __main__ - Step 180 Global step 180 Train loss 1.05 on epoch=44
05/18/2022 00:04:21 - INFO - __main__ - Step 190 Global step 190 Train loss 1.12 on epoch=47
05/18/2022 00:04:22 - INFO - __main__ - Step 200 Global step 200 Train loss 1.11 on epoch=49
05/18/2022 00:04:23 - INFO - __main__ - Global step 200 Train loss 1.15 Classification-F1 0.1 on epoch=49
05/18/2022 00:04:24 - INFO - __main__ - Step 210 Global step 210 Train loss 1.08 on epoch=52
05/18/2022 00:04:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.97 on epoch=54
05/18/2022 00:04:26 - INFO - __main__ - Step 230 Global step 230 Train loss 1.03 on epoch=57
05/18/2022 00:04:28 - INFO - __main__ - Step 240 Global step 240 Train loss 1.09 on epoch=59
05/18/2022 00:04:29 - INFO - __main__ - Step 250 Global step 250 Train loss 1.05 on epoch=62
05/18/2022 00:04:29 - INFO - __main__ - Global step 250 Train loss 1.04 Classification-F1 0.09493670886075949 on epoch=62
05/18/2022 00:04:31 - INFO - __main__ - Step 260 Global step 260 Train loss 1.01 on epoch=64
05/18/2022 00:04:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.96 on epoch=67
05/18/2022 00:04:33 - INFO - __main__ - Step 280 Global step 280 Train loss 1.00 on epoch=69
05/18/2022 00:04:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.92 on epoch=72
05/18/2022 00:04:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.96 on epoch=74
05/18/2022 00:04:36 - INFO - __main__ - Global step 300 Train loss 0.97 Classification-F1 0.1 on epoch=74
05/18/2022 00:04:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.98 on epoch=77
05/18/2022 00:04:38 - INFO - __main__ - Step 320 Global step 320 Train loss 1.06 on epoch=79
05/18/2022 00:04:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.91 on epoch=82
05/18/2022 00:04:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.89 on epoch=84
05/18/2022 00:04:42 - INFO - __main__ - Step 350 Global step 350 Train loss 1.05 on epoch=87
05/18/2022 00:04:43 - INFO - __main__ - Global step 350 Train loss 0.98 Classification-F1 0.16138763197586728 on epoch=87
05/18/2022 00:04:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.96 on epoch=89
05/18/2022 00:04:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.99 on epoch=92
05/18/2022 00:04:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.93 on epoch=94
05/18/2022 00:04:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.93 on epoch=97
05/18/2022 00:04:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.97 on epoch=99
05/18/2022 00:04:49 - INFO - __main__ - Global step 400 Train loss 0.96 Classification-F1 0.14069478908188585 on epoch=99
05/18/2022 00:04:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.87 on epoch=102
05/18/2022 00:04:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.95 on epoch=104
05/18/2022 00:04:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.95 on epoch=107
05/18/2022 00:04:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.97 on epoch=109
05/18/2022 00:04:56 - INFO - __main__ - Step 450 Global step 450 Train loss 1.01 on epoch=112
05/18/2022 00:04:56 - INFO - __main__ - Global step 450 Train loss 0.95 Classification-F1 0.16451612903225807 on epoch=112
05/18/2022 00:04:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.91 on epoch=114
05/18/2022 00:04:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.95 on epoch=117
05/18/2022 00:05:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.82 on epoch=119
05/18/2022 00:05:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.89 on epoch=122
05/18/2022 00:05:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.94 on epoch=124
05/18/2022 00:05:03 - INFO - __main__ - Global step 500 Train loss 0.90 Classification-F1 0.1 on epoch=124
05/18/2022 00:05:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.87 on epoch=127
05/18/2022 00:05:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.95 on epoch=129
05/18/2022 00:05:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.97 on epoch=132
05/18/2022 00:05:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.95 on epoch=134
05/18/2022 00:05:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=137
05/18/2022 00:05:10 - INFO - __main__ - Global step 550 Train loss 0.92 Classification-F1 0.13997113997113997 on epoch=137
05/18/2022 00:05:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.94 on epoch=139
05/18/2022 00:05:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.85 on epoch=142
05/18/2022 00:05:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.91 on epoch=144
05/18/2022 00:05:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.82 on epoch=147
05/18/2022 00:05:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.91 on epoch=149
05/18/2022 00:05:16 - INFO - __main__ - Global step 600 Train loss 0.89 Classification-F1 0.11666666666666667 on epoch=149
05/18/2022 00:05:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.90 on epoch=152
05/18/2022 00:05:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.91 on epoch=154
05/18/2022 00:05:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.87 on epoch=157
05/18/2022 00:05:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.90 on epoch=159
05/18/2022 00:05:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.95 on epoch=162
05/18/2022 00:05:23 - INFO - __main__ - Global step 650 Train loss 0.90 Classification-F1 0.1 on epoch=162
05/18/2022 00:05:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.79 on epoch=164
05/18/2022 00:05:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.91 on epoch=167
05/18/2022 00:05:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.85 on epoch=169
05/18/2022 00:05:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.91 on epoch=172
05/18/2022 00:05:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.85 on epoch=174
05/18/2022 00:05:30 - INFO - __main__ - Global step 700 Train loss 0.86 Classification-F1 0.13381369016984046 on epoch=174
05/18/2022 00:05:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.85 on epoch=177
05/18/2022 00:05:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.85 on epoch=179
05/18/2022 00:05:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.90 on epoch=182
05/18/2022 00:05:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.88 on epoch=184
05/18/2022 00:05:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.80 on epoch=187
05/18/2022 00:05:36 - INFO - __main__ - Global step 750 Train loss 0.86 Classification-F1 0.13879981676591846 on epoch=187
05/18/2022 00:05:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.90 on epoch=189
05/18/2022 00:05:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.90 on epoch=192
05/18/2022 00:05:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.88 on epoch=194
05/18/2022 00:05:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.91 on epoch=197
05/18/2022 00:05:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.88 on epoch=199
05/18/2022 00:05:43 - INFO - __main__ - Global step 800 Train loss 0.89 Classification-F1 0.14069478908188585 on epoch=199
05/18/2022 00:05:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.90 on epoch=202
05/18/2022 00:05:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.91 on epoch=204
05/18/2022 00:05:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.88 on epoch=207
05/18/2022 00:05:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.82 on epoch=209
05/18/2022 00:05:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.88 on epoch=212
05/18/2022 00:05:50 - INFO - __main__ - Global step 850 Train loss 0.88 Classification-F1 0.12368421052631579 on epoch=212
05/18/2022 00:05:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.86 on epoch=214
05/18/2022 00:05:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.93 on epoch=217
05/18/2022 00:05:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.83 on epoch=219
05/18/2022 00:05:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.86 on epoch=222
05/18/2022 00:05:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.92 on epoch=224
05/18/2022 00:05:56 - INFO - __main__ - Global step 900 Train loss 0.88 Classification-F1 0.16005291005291006 on epoch=224
05/18/2022 00:05:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.85 on epoch=227
05/18/2022 00:05:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.90 on epoch=229
05/18/2022 00:06:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.79 on epoch=232
05/18/2022 00:06:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.84 on epoch=234
05/18/2022 00:06:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.91 on epoch=237
05/18/2022 00:06:03 - INFO - __main__ - Global step 950 Train loss 0.86 Classification-F1 0.1 on epoch=237
05/18/2022 00:06:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.87 on epoch=239
05/18/2022 00:06:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.81 on epoch=242
05/18/2022 00:06:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.86 on epoch=244
05/18/2022 00:06:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.85 on epoch=247
05/18/2022 00:06:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.82 on epoch=249
05/18/2022 00:06:10 - INFO - __main__ - Global step 1000 Train loss 0.84 Classification-F1 0.08974358974358974 on epoch=249
05/18/2022 00:06:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.80 on epoch=252
05/18/2022 00:06:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.84 on epoch=254
05/18/2022 00:06:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.81 on epoch=257
05/18/2022 00:06:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.81 on epoch=259
05/18/2022 00:06:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.84 on epoch=262
05/18/2022 00:06:17 - INFO - __main__ - Global step 1050 Train loss 0.82 Classification-F1 0.13475499092558985 on epoch=262
05/18/2022 00:06:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.80 on epoch=264
05/18/2022 00:06:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.81 on epoch=267
05/18/2022 00:06:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.71 on epoch=269
05/18/2022 00:06:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.88 on epoch=272
05/18/2022 00:06:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.80 on epoch=274
05/18/2022 00:06:23 - INFO - __main__ - Global step 1100 Train loss 0.80 Classification-F1 0.10135135135135136 on epoch=274
05/18/2022 00:06:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.83 on epoch=277
05/18/2022 00:06:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.79 on epoch=279
05/18/2022 00:06:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.83 on epoch=282
05/18/2022 00:06:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.82 on epoch=284
05/18/2022 00:06:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.77 on epoch=287
05/18/2022 00:06:30 - INFO - __main__ - Global step 1150 Train loss 0.81 Classification-F1 0.10679361811631496 on epoch=287
05/18/2022 00:06:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.93 on epoch=289
05/18/2022 00:06:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.76 on epoch=292
05/18/2022 00:06:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.79 on epoch=294
05/18/2022 00:06:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.84 on epoch=297
05/18/2022 00:06:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.81 on epoch=299
05/18/2022 00:06:37 - INFO - __main__ - Global step 1200 Train loss 0.83 Classification-F1 0.1402116402116402 on epoch=299
05/18/2022 00:06:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.80 on epoch=302
05/18/2022 00:06:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.93 on epoch=304
05/18/2022 00:06:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.89 on epoch=307
05/18/2022 00:06:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.76 on epoch=309
05/18/2022 00:06:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.87 on epoch=312
05/18/2022 00:06:43 - INFO - __main__ - Global step 1250 Train loss 0.85 Classification-F1 0.13251935675997617 on epoch=312
05/18/2022 00:06:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.92 on epoch=314
05/18/2022 00:06:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.81 on epoch=317
05/18/2022 00:06:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.76 on epoch=319
05/18/2022 00:06:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.82 on epoch=322
05/18/2022 00:06:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.84 on epoch=324
05/18/2022 00:06:50 - INFO - __main__ - Global step 1300 Train loss 0.83 Classification-F1 0.1409090909090909 on epoch=324
05/18/2022 00:06:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.85 on epoch=327
05/18/2022 00:06:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.81 on epoch=329
05/18/2022 00:06:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.86 on epoch=332
05/18/2022 00:06:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.85 on epoch=334
05/18/2022 00:06:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.84 on epoch=337
05/18/2022 00:06:56 - INFO - __main__ - Global step 1350 Train loss 0.84 Classification-F1 0.12287581699346406 on epoch=337
05/18/2022 00:06:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.93 on epoch=339
05/18/2022 00:06:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.74 on epoch=342
05/18/2022 00:07:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.72 on epoch=344
05/18/2022 00:07:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.86 on epoch=347
05/18/2022 00:07:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.78 on epoch=349
05/18/2022 00:07:03 - INFO - __main__ - Global step 1400 Train loss 0.81 Classification-F1 0.13398692810457516 on epoch=349
05/18/2022 00:07:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.83 on epoch=352
05/18/2022 00:07:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.86 on epoch=354
05/18/2022 00:07:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.84 on epoch=357
05/18/2022 00:07:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.74 on epoch=359
05/18/2022 00:07:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.84 on epoch=362
05/18/2022 00:07:10 - INFO - __main__ - Global step 1450 Train loss 0.82 Classification-F1 0.14521739130434783 on epoch=362
05/18/2022 00:07:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.85 on epoch=364
05/18/2022 00:07:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.86 on epoch=367
05/18/2022 00:07:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.86 on epoch=369
05/18/2022 00:07:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.86 on epoch=372
05/18/2022 00:07:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.86 on epoch=374
05/18/2022 00:07:16 - INFO - __main__ - Global step 1500 Train loss 0.86 Classification-F1 0.10843672456575681 on epoch=374
05/18/2022 00:07:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.85 on epoch=377
05/18/2022 00:07:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.82 on epoch=379
05/18/2022 00:07:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.82 on epoch=382
05/18/2022 00:07:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.89 on epoch=384
05/18/2022 00:07:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.78 on epoch=387
05/18/2022 00:07:23 - INFO - __main__ - Global step 1550 Train loss 0.83 Classification-F1 0.09090909090909091 on epoch=387
05/18/2022 00:07:24 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.85 on epoch=389
05/18/2022 00:07:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.76 on epoch=392
05/18/2022 00:07:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.77 on epoch=394
05/18/2022 00:07:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.70 on epoch=397
05/18/2022 00:07:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.86 on epoch=399
05/18/2022 00:07:30 - INFO - __main__ - Global step 1600 Train loss 0.79 Classification-F1 0.14285714285714285 on epoch=399
05/18/2022 00:07:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.80 on epoch=402
05/18/2022 00:07:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.78 on epoch=404
05/18/2022 00:07:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.85 on epoch=407
05/18/2022 00:07:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.80 on epoch=409
05/18/2022 00:07:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.85 on epoch=412
05/18/2022 00:07:37 - INFO - __main__ - Global step 1650 Train loss 0.82 Classification-F1 0.1392857142857143 on epoch=412
05/18/2022 00:07:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.75 on epoch=414
05/18/2022 00:07:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.84 on epoch=417
05/18/2022 00:07:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.92 on epoch=419
05/18/2022 00:07:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.81 on epoch=422
05/18/2022 00:07:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.76 on epoch=424
05/18/2022 00:07:44 - INFO - __main__ - Global step 1700 Train loss 0.82 Classification-F1 0.14304519526107942 on epoch=424
05/18/2022 00:07:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.77 on epoch=427
05/18/2022 00:07:46 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.81 on epoch=429
05/18/2022 00:07:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.81 on epoch=432
05/18/2022 00:07:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.83 on epoch=434
05/18/2022 00:07:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.84 on epoch=437
05/18/2022 00:07:50 - INFO - __main__ - Global step 1750 Train loss 0.81 Classification-F1 0.13482414242292662 on epoch=437
05/18/2022 00:07:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.78 on epoch=439
05/18/2022 00:07:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.82 on epoch=442
05/18/2022 00:07:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.82 on epoch=444
05/18/2022 00:07:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.76 on epoch=447
05/18/2022 00:07:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.83 on epoch=449
05/18/2022 00:07:57 - INFO - __main__ - Global step 1800 Train loss 0.80 Classification-F1 0.09333333333333334 on epoch=449
05/18/2022 00:07:58 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.80 on epoch=452
05/18/2022 00:08:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.82 on epoch=454
05/18/2022 00:08:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.88 on epoch=457
05/18/2022 00:08:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.82 on epoch=459
05/18/2022 00:08:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.85 on epoch=462
05/18/2022 00:08:04 - INFO - __main__ - Global step 1850 Train loss 0.83 Classification-F1 0.11005692599620492 on epoch=462
05/18/2022 00:08:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.79 on epoch=464
05/18/2022 00:08:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.77 on epoch=467
05/18/2022 00:08:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.85 on epoch=469
05/18/2022 00:08:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.77 on epoch=472
05/18/2022 00:08:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.84 on epoch=474
05/18/2022 00:08:11 - INFO - __main__ - Global step 1900 Train loss 0.81 Classification-F1 0.11705989110707803 on epoch=474
05/18/2022 00:08:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.78 on epoch=477
05/18/2022 00:08:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.84 on epoch=479
05/18/2022 00:08:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.84 on epoch=482
05/18/2022 00:08:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.83 on epoch=484
05/18/2022 00:08:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.77 on epoch=487
05/18/2022 00:08:17 - INFO - __main__ - Global step 1950 Train loss 0.81 Classification-F1 0.13361123299692845 on epoch=487
05/18/2022 00:08:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.81 on epoch=489
05/18/2022 00:08:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.80 on epoch=492
05/18/2022 00:08:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.84 on epoch=494
05/18/2022 00:08:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.84 on epoch=497
05/18/2022 00:08:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.80 on epoch=499
05/18/2022 00:08:24 - INFO - __main__ - Global step 2000 Train loss 0.82 Classification-F1 0.13398692810457516 on epoch=499
05/18/2022 00:08:26 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.80 on epoch=502
05/18/2022 00:08:27 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.76 on epoch=504
05/18/2022 00:08:28 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.82 on epoch=507
05/18/2022 00:08:29 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.80 on epoch=509
05/18/2022 00:08:31 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.80 on epoch=512
05/18/2022 00:08:31 - INFO - __main__ - Global step 2050 Train loss 0.80 Classification-F1 0.12456575682382134 on epoch=512
05/18/2022 00:08:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.82 on epoch=514
05/18/2022 00:08:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.77 on epoch=517
05/18/2022 00:08:35 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.80 on epoch=519
05/18/2022 00:08:36 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.79 on epoch=522
05/18/2022 00:08:38 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.74 on epoch=524
05/18/2022 00:08:38 - INFO - __main__ - Global step 2100 Train loss 0.78 Classification-F1 0.1171875 on epoch=524
05/18/2022 00:08:39 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.87 on epoch=527
05/18/2022 00:08:41 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.91 on epoch=529
05/18/2022 00:08:42 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.81 on epoch=532
05/18/2022 00:08:43 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.80 on epoch=534
05/18/2022 00:08:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.76 on epoch=537
05/18/2022 00:08:45 - INFO - __main__ - Global step 2150 Train loss 0.83 Classification-F1 0.12424242424242424 on epoch=537
05/18/2022 00:08:46 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.83 on epoch=539
05/18/2022 00:08:48 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.82 on epoch=542
05/18/2022 00:08:49 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.78 on epoch=544
05/18/2022 00:08:50 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.78 on epoch=547
05/18/2022 00:08:51 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.78 on epoch=549
05/18/2022 00:08:52 - INFO - __main__ - Global step 2200 Train loss 0.80 Classification-F1 0.1392857142857143 on epoch=549
05/18/2022 00:08:53 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.80 on epoch=552
05/18/2022 00:08:54 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.79 on epoch=554
05/18/2022 00:08:56 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.81 on epoch=557
05/18/2022 00:08:57 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.84 on epoch=559
05/18/2022 00:08:58 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.77 on epoch=562
05/18/2022 00:08:59 - INFO - __main__ - Global step 2250 Train loss 0.80 Classification-F1 0.1392857142857143 on epoch=562
05/18/2022 00:09:00 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.82 on epoch=564
05/18/2022 00:09:01 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.83 on epoch=567
05/18/2022 00:09:02 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.75 on epoch=569
05/18/2022 00:09:04 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.81 on epoch=572
05/18/2022 00:09:05 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.80 on epoch=574
05/18/2022 00:09:05 - INFO - __main__ - Global step 2300 Train loss 0.80 Classification-F1 0.09589041095890412 on epoch=574
05/18/2022 00:09:07 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.80 on epoch=577
05/18/2022 00:09:08 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.76 on epoch=579
05/18/2022 00:09:09 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.78 on epoch=582
05/18/2022 00:09:10 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.79 on epoch=584
05/18/2022 00:09:12 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.75 on epoch=587
05/18/2022 00:09:12 - INFO - __main__ - Global step 2350 Train loss 0.78 Classification-F1 0.1390013495276653 on epoch=587
05/18/2022 00:09:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.77 on epoch=589
05/18/2022 00:09:15 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.82 on epoch=592
05/18/2022 00:09:16 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.75 on epoch=594
05/18/2022 00:09:17 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.80 on epoch=597
05/18/2022 00:09:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.78 on epoch=599
05/18/2022 00:09:19 - INFO - __main__ - Global step 2400 Train loss 0.79 Classification-F1 0.1333333333333333 on epoch=599
05/18/2022 00:09:20 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.81 on epoch=602
05/18/2022 00:09:22 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.84 on epoch=604
05/18/2022 00:09:23 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.75 on epoch=607
05/18/2022 00:09:24 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.77 on epoch=609
05/18/2022 00:09:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.79 on epoch=612
05/18/2022 00:09:26 - INFO - __main__ - Global step 2450 Train loss 0.79 Classification-F1 0.1333333333333333 on epoch=612
05/18/2022 00:09:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.79 on epoch=614
05/18/2022 00:09:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.75 on epoch=617
05/18/2022 00:09:30 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.74 on epoch=619
05/18/2022 00:09:31 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.79 on epoch=622
05/18/2022 00:09:32 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.85 on epoch=624
05/18/2022 00:09:33 - INFO - __main__ - Global step 2500 Train loss 0.79 Classification-F1 0.09493670886075949 on epoch=624
05/18/2022 00:09:34 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.80 on epoch=627
05/18/2022 00:09:35 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.80 on epoch=629
05/18/2022 00:09:36 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.81 on epoch=632
05/18/2022 00:09:38 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.80 on epoch=634
05/18/2022 00:09:39 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.80 on epoch=637
05/18/2022 00:09:39 - INFO - __main__ - Global step 2550 Train loss 0.80 Classification-F1 0.1134453781512605 on epoch=637
05/18/2022 00:09:41 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.77 on epoch=639
05/18/2022 00:09:42 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.86 on epoch=642
05/18/2022 00:09:43 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.83 on epoch=644
05/18/2022 00:09:45 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.85 on epoch=647
05/18/2022 00:09:46 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.76 on epoch=649
05/18/2022 00:09:46 - INFO - __main__ - Global step 2600 Train loss 0.81 Classification-F1 0.12618595825426943 on epoch=649
05/18/2022 00:09:48 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.77 on epoch=652
05/18/2022 00:09:49 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.80 on epoch=654
05/18/2022 00:09:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.89 on epoch=657
05/18/2022 00:09:51 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.83 on epoch=659
05/18/2022 00:09:53 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.79 on epoch=662
05/18/2022 00:09:53 - INFO - __main__ - Global step 2650 Train loss 0.82 Classification-F1 0.11176470588235293 on epoch=662
05/18/2022 00:09:54 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.78 on epoch=664
05/18/2022 00:09:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.77 on epoch=667
05/18/2022 00:09:57 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.74 on epoch=669
05/18/2022 00:09:58 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.80 on epoch=672
05/18/2022 00:10:00 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.79 on epoch=674
05/18/2022 00:10:00 - INFO - __main__ - Global step 2700 Train loss 0.77 Classification-F1 0.13968957871396895 on epoch=674
05/18/2022 00:10:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.78 on epoch=677
05/18/2022 00:10:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.76 on epoch=679
05/18/2022 00:10:04 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.82 on epoch=682
05/18/2022 00:10:05 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.81 on epoch=684
05/18/2022 00:10:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.82 on epoch=687
05/18/2022 00:10:07 - INFO - __main__ - Global step 2750 Train loss 0.80 Classification-F1 0.12198332602018429 on epoch=687
05/18/2022 00:10:08 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.82 on epoch=689
05/18/2022 00:10:10 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.80 on epoch=692
05/18/2022 00:10:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.81 on epoch=694
05/18/2022 00:10:12 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.79 on epoch=697
05/18/2022 00:10:13 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.78 on epoch=699
05/18/2022 00:10:14 - INFO - __main__ - Global step 2800 Train loss 0.80 Classification-F1 0.12424242424242424 on epoch=699
05/18/2022 00:10:15 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.80 on epoch=702
05/18/2022 00:10:16 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.75 on epoch=704
05/18/2022 00:10:18 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.73 on epoch=707
05/18/2022 00:10:19 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.77 on epoch=709
05/18/2022 00:10:20 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.82 on epoch=712
05/18/2022 00:10:21 - INFO - __main__ - Global step 2850 Train loss 0.78 Classification-F1 0.1472743930371049 on epoch=712
05/18/2022 00:10:22 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.77 on epoch=714
05/18/2022 00:10:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.75 on epoch=717
05/18/2022 00:10:25 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.79 on epoch=719
05/18/2022 00:10:26 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.86 on epoch=722
05/18/2022 00:10:27 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.83 on epoch=724
05/18/2022 00:10:28 - INFO - __main__ - Global step 2900 Train loss 0.80 Classification-F1 0.12499999999999999 on epoch=724
05/18/2022 00:10:29 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.77 on epoch=727
05/18/2022 00:10:30 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.76 on epoch=729
05/18/2022 00:10:32 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.82 on epoch=732
05/18/2022 00:10:33 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.79 on epoch=734
05/18/2022 00:10:34 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.80 on epoch=737
05/18/2022 00:10:35 - INFO - __main__ - Global step 2950 Train loss 0.79 Classification-F1 0.11740890688259109 on epoch=737
05/18/2022 00:10:36 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.81 on epoch=739
05/18/2022 00:10:37 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.75 on epoch=742
05/18/2022 00:10:38 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.74 on epoch=744
05/18/2022 00:10:40 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.74 on epoch=747
05/18/2022 00:10:41 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.74 on epoch=749
05/18/2022 00:10:41 - INFO - __main__ - Global step 3000 Train loss 0.76 Classification-F1 0.10277777777777777 on epoch=749
05/18/2022 00:10:41 - INFO - __main__ - save last model!
05/18/2022 00:10:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 00:10:41 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 00:10:41 - INFO - __main__ - Printing 3 examples
05/18/2022 00:10:41 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 00:10:41 - INFO - __main__ - ['others']
05/18/2022 00:10:41 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 00:10:41 - INFO - __main__ - ['others']
05/18/2022 00:10:41 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 00:10:41 - INFO - __main__ - ['others']
05/18/2022 00:10:41 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:10:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:10:42 - INFO - __main__ - Printing 3 examples
05/18/2022 00:10:42 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/18/2022 00:10:42 - INFO - __main__ - ['others']
05/18/2022 00:10:42 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/18/2022 00:10:42 - INFO - __main__ - ['others']
05/18/2022 00:10:42 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/18/2022 00:10:42 - INFO - __main__ - ['others']
05/18/2022 00:10:42 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:10:42 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:10:42 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 00:10:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:10:42 - INFO - __main__ - Printing 3 examples
05/18/2022 00:10:42 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/18/2022 00:10:42 - INFO - __main__ - ['others']
05/18/2022 00:10:42 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/18/2022 00:10:42 - INFO - __main__ - ['others']
05/18/2022 00:10:42 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/18/2022 00:10:42 - INFO - __main__ - ['others']
05/18/2022 00:10:42 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:10:42 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:10:42 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 00:10:44 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:10:48 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 00:10:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 00:10:48 - INFO - __main__ - Starting training!
05/18/2022 00:10:49 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 00:11:33 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_100_0.2_8_predictions.txt
05/18/2022 00:11:33 - INFO - __main__ - Classification-F1 on test data: 0.0446
05/18/2022 00:11:33 - INFO - __main__ - prefix=emo_16_100, lr=0.2, bsz=8, dev_performance=0.1778584392014519, test_performance=0.044621152681889875
05/18/2022 00:11:33 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.5, bsz=8 ...
05/18/2022 00:11:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:11:34 - INFO - __main__ - Printing 3 examples
05/18/2022 00:11:34 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/18/2022 00:11:34 - INFO - __main__ - ['others']
05/18/2022 00:11:34 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/18/2022 00:11:34 - INFO - __main__ - ['others']
05/18/2022 00:11:34 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/18/2022 00:11:34 - INFO - __main__ - ['others']
05/18/2022 00:11:34 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:11:34 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:11:34 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 00:11:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:11:34 - INFO - __main__ - Printing 3 examples
05/18/2022 00:11:34 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/18/2022 00:11:34 - INFO - __main__ - ['others']
05/18/2022 00:11:34 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/18/2022 00:11:34 - INFO - __main__ - ['others']
05/18/2022 00:11:34 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/18/2022 00:11:34 - INFO - __main__ - ['others']
05/18/2022 00:11:34 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:11:34 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:11:34 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 00:11:40 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 00:11:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 00:11:40 - INFO - __main__ - Starting training!
05/18/2022 00:11:41 - INFO - __main__ - Step 10 Global step 10 Train loss 4.19 on epoch=2
05/18/2022 00:11:42 - INFO - __main__ - Step 20 Global step 20 Train loss 3.51 on epoch=4
05/18/2022 00:11:44 - INFO - __main__ - Step 30 Global step 30 Train loss 3.04 on epoch=7
05/18/2022 00:11:45 - INFO - __main__ - Step 40 Global step 40 Train loss 2.34 on epoch=9
05/18/2022 00:11:46 - INFO - __main__ - Step 50 Global step 50 Train loss 2.03 on epoch=12
05/18/2022 00:11:48 - INFO - __main__ - Global step 50 Train loss 3.02 Classification-F1 0.1 on epoch=12
05/18/2022 00:11:48 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/18/2022 00:11:49 - INFO - __main__ - Step 60 Global step 60 Train loss 1.62 on epoch=14
05/18/2022 00:11:50 - INFO - __main__ - Step 70 Global step 70 Train loss 1.43 on epoch=17
05/18/2022 00:11:51 - INFO - __main__ - Step 80 Global step 80 Train loss 1.24 on epoch=19
05/18/2022 00:11:53 - INFO - __main__ - Step 90 Global step 90 Train loss 1.17 on epoch=22
05/18/2022 00:11:54 - INFO - __main__ - Step 100 Global step 100 Train loss 1.05 on epoch=24
05/18/2022 00:11:54 - INFO - __main__ - Global step 100 Train loss 1.30 Classification-F1 0.2188339455073735 on epoch=24
05/18/2022 00:11:54 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.2188339455073735 on epoch=24, global_step=100
05/18/2022 00:11:56 - INFO - __main__ - Step 110 Global step 110 Train loss 1.03 on epoch=27
05/18/2022 00:11:57 - INFO - __main__ - Step 120 Global step 120 Train loss 1.06 on epoch=29
05/18/2022 00:11:58 - INFO - __main__ - Step 130 Global step 130 Train loss 1.04 on epoch=32
05/18/2022 00:11:59 - INFO - __main__ - Step 140 Global step 140 Train loss 1.03 on epoch=34
05/18/2022 00:12:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.96 on epoch=37
05/18/2022 00:12:01 - INFO - __main__ - Global step 150 Train loss 1.03 Classification-F1 0.18854427736006682 on epoch=37
05/18/2022 00:12:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.99 on epoch=39
05/18/2022 00:12:03 - INFO - __main__ - Step 170 Global step 170 Train loss 1.03 on epoch=42
05/18/2022 00:12:05 - INFO - __main__ - Step 180 Global step 180 Train loss 1.04 on epoch=44
05/18/2022 00:12:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.93 on epoch=47
05/18/2022 00:12:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.98 on epoch=49
05/18/2022 00:12:08 - INFO - __main__ - Global step 200 Train loss 1.00 Classification-F1 0.1 on epoch=49
05/18/2022 00:12:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.96 on epoch=52
05/18/2022 00:12:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.89 on epoch=54
05/18/2022 00:12:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.89 on epoch=57
05/18/2022 00:12:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.98 on epoch=59
05/18/2022 00:12:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.93 on epoch=62
05/18/2022 00:12:14 - INFO - __main__ - Global step 250 Train loss 0.93 Classification-F1 0.1 on epoch=62
05/18/2022 00:12:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.96 on epoch=64
05/18/2022 00:12:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.77 on epoch=67
05/18/2022 00:12:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.82 on epoch=69
05/18/2022 00:12:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.98 on epoch=72
05/18/2022 00:12:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.92 on epoch=74
05/18/2022 00:12:21 - INFO - __main__ - Global step 300 Train loss 0.89 Classification-F1 0.1 on epoch=74
05/18/2022 00:12:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.97 on epoch=77
05/18/2022 00:12:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.83 on epoch=79
05/18/2022 00:12:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.82 on epoch=82
05/18/2022 00:12:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.98 on epoch=84
05/18/2022 00:12:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.87 on epoch=87
05/18/2022 00:12:27 - INFO - __main__ - Global step 350 Train loss 0.89 Classification-F1 0.1 on epoch=87
05/18/2022 00:12:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.85 on epoch=89
05/18/2022 00:12:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.93 on epoch=92
05/18/2022 00:12:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.89 on epoch=94
05/18/2022 00:12:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.83 on epoch=97
05/18/2022 00:12:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.99 on epoch=99
05/18/2022 00:12:34 - INFO - __main__ - Global step 400 Train loss 0.90 Classification-F1 0.09615384615384615 on epoch=99
05/18/2022 00:12:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.96 on epoch=102
05/18/2022 00:12:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.84 on epoch=104
05/18/2022 00:12:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.90 on epoch=107
05/18/2022 00:12:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.87 on epoch=109
05/18/2022 00:12:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.88 on epoch=112
05/18/2022 00:12:40 - INFO - __main__ - Global step 450 Train loss 0.89 Classification-F1 0.1 on epoch=112
05/18/2022 00:12:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.88 on epoch=114
05/18/2022 00:12:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.87 on epoch=117
05/18/2022 00:12:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.90 on epoch=119
05/18/2022 00:12:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.88 on epoch=122
05/18/2022 00:12:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.83 on epoch=124
05/18/2022 00:12:47 - INFO - __main__ - Global step 500 Train loss 0.87 Classification-F1 0.09868421052631579 on epoch=124
05/18/2022 00:12:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.83 on epoch=127
05/18/2022 00:12:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.91 on epoch=129
05/18/2022 00:12:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.80 on epoch=132
05/18/2022 00:12:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.81 on epoch=134
05/18/2022 00:12:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=137
05/18/2022 00:12:53 - INFO - __main__ - Global step 550 Train loss 0.83 Classification-F1 0.1 on epoch=137
05/18/2022 00:12:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.91 on epoch=139
05/18/2022 00:12:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.87 on epoch=142
05/18/2022 00:12:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.81 on epoch=144
05/18/2022 00:12:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.87 on epoch=147
05/18/2022 00:12:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.90 on epoch=149
05/18/2022 00:13:00 - INFO - __main__ - Global step 600 Train loss 0.87 Classification-F1 0.14210526315789473 on epoch=149
05/18/2022 00:13:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.87 on epoch=152
05/18/2022 00:13:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.91 on epoch=154
05/18/2022 00:13:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.89 on epoch=157
05/18/2022 00:13:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.82 on epoch=159
05/18/2022 00:13:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.80 on epoch=162
05/18/2022 00:13:06 - INFO - __main__ - Global step 650 Train loss 0.86 Classification-F1 0.1 on epoch=162
05/18/2022 00:13:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.89 on epoch=164
05/18/2022 00:13:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.86 on epoch=167
05/18/2022 00:13:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.82 on epoch=169
05/18/2022 00:13:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.90 on epoch=172
05/18/2022 00:13:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.77 on epoch=174
05/18/2022 00:13:13 - INFO - __main__ - Global step 700 Train loss 0.85 Classification-F1 0.1476190476190476 on epoch=174
05/18/2022 00:13:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.83 on epoch=177
05/18/2022 00:13:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.87 on epoch=179
05/18/2022 00:13:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.79 on epoch=182
05/18/2022 00:13:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.79 on epoch=184
05/18/2022 00:13:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.83 on epoch=187
05/18/2022 00:13:20 - INFO - __main__ - Global step 750 Train loss 0.82 Classification-F1 0.1255656108597285 on epoch=187
05/18/2022 00:13:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.86 on epoch=189
05/18/2022 00:13:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.89 on epoch=192
05/18/2022 00:13:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.90 on epoch=194
05/18/2022 00:13:25 - INFO - __main__ - Step 790 Global step 790 Train loss 0.83 on epoch=197
05/18/2022 00:13:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.78 on epoch=199
05/18/2022 00:13:26 - INFO - __main__ - Global step 800 Train loss 0.85 Classification-F1 0.13154929577464788 on epoch=199
05/18/2022 00:13:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.82 on epoch=202
05/18/2022 00:13:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.89 on epoch=204
05/18/2022 00:13:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.79 on epoch=207
05/18/2022 00:13:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.79 on epoch=209
05/18/2022 00:13:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.89 on epoch=212
05/18/2022 00:13:33 - INFO - __main__ - Global step 850 Train loss 0.84 Classification-F1 0.1875 on epoch=212
05/18/2022 00:13:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.80 on epoch=214
05/18/2022 00:13:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.74 on epoch=217
05/18/2022 00:13:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.77 on epoch=219
05/18/2022 00:13:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.77 on epoch=222
05/18/2022 00:13:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.83 on epoch=224
05/18/2022 00:13:40 - INFO - __main__ - Global step 900 Train loss 0.78 Classification-F1 0.09868421052631579 on epoch=224
05/18/2022 00:13:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.77 on epoch=227
05/18/2022 00:13:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.78 on epoch=229
05/18/2022 00:13:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.83 on epoch=232
05/18/2022 00:13:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.83 on epoch=234
05/18/2022 00:13:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.85 on epoch=237
05/18/2022 00:13:46 - INFO - __main__ - Global step 950 Train loss 0.81 Classification-F1 0.1 on epoch=237
05/18/2022 00:13:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.77 on epoch=239
05/18/2022 00:13:49 - INFO - __main__ - Step 970 Global step 970 Train loss 0.84 on epoch=242
05/18/2022 00:13:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.87 on epoch=244
05/18/2022 00:13:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.78 on epoch=247
05/18/2022 00:13:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.85 on epoch=249
05/18/2022 00:13:53 - INFO - __main__ - Global step 1000 Train loss 0.82 Classification-F1 0.1515492957746479 on epoch=249
05/18/2022 00:13:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.79 on epoch=252
05/18/2022 00:13:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.85 on epoch=254
05/18/2022 00:13:57 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.84 on epoch=257
05/18/2022 00:13:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.78 on epoch=259
05/18/2022 00:13:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.82 on epoch=262
05/18/2022 00:14:00 - INFO - __main__ - Global step 1050 Train loss 0.82 Classification-F1 0.09493670886075949 on epoch=262
05/18/2022 00:14:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.81 on epoch=264
05/18/2022 00:14:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.84 on epoch=267
05/18/2022 00:14:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.76 on epoch=269
05/18/2022 00:14:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.78 on epoch=272
05/18/2022 00:14:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.81 on epoch=274
05/18/2022 00:14:06 - INFO - __main__ - Global step 1100 Train loss 0.80 Classification-F1 0.11722488038277512 on epoch=274
05/18/2022 00:14:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.81 on epoch=277
05/18/2022 00:14:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.86 on epoch=279
05/18/2022 00:14:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.86 on epoch=282
05/18/2022 00:14:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.83 on epoch=284
05/18/2022 00:14:13 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.81 on epoch=287
05/18/2022 00:14:13 - INFO - __main__ - Global step 1150 Train loss 0.83 Classification-F1 0.11714285714285715 on epoch=287
05/18/2022 00:14:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.78 on epoch=289
05/18/2022 00:14:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.81 on epoch=292
05/18/2022 00:14:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.85 on epoch=294
05/18/2022 00:14:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.84 on epoch=297
05/18/2022 00:14:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.75 on epoch=299
05/18/2022 00:14:20 - INFO - __main__ - Global step 1200 Train loss 0.81 Classification-F1 0.1337028824833703 on epoch=299
05/18/2022 00:14:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.78 on epoch=302
05/18/2022 00:14:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.77 on epoch=304
05/18/2022 00:14:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.82 on epoch=307
05/18/2022 00:14:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.77 on epoch=309
05/18/2022 00:14:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.82 on epoch=312
05/18/2022 00:14:26 - INFO - __main__ - Global step 1250 Train loss 0.79 Classification-F1 0.1332923832923833 on epoch=312
05/18/2022 00:14:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.82 on epoch=314
05/18/2022 00:14:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.81 on epoch=317
05/18/2022 00:14:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.76 on epoch=319
05/18/2022 00:14:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.81 on epoch=322
05/18/2022 00:14:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.81 on epoch=324
05/18/2022 00:14:33 - INFO - __main__ - Global step 1300 Train loss 0.80 Classification-F1 0.09615384615384615 on epoch=324
05/18/2022 00:14:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.85 on epoch=327
05/18/2022 00:14:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.84 on epoch=329
05/18/2022 00:14:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.79 on epoch=332
05/18/2022 00:14:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.76 on epoch=334
05/18/2022 00:14:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.74 on epoch=337
05/18/2022 00:14:39 - INFO - __main__ - Global step 1350 Train loss 0.80 Classification-F1 0.2491341991341991 on epoch=337
05/18/2022 00:14:40 - INFO - __main__ - Saving model with best Classification-F1: 0.2188339455073735 -> 0.2491341991341991 on epoch=337, global_step=1350
05/18/2022 00:14:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.79 on epoch=339
05/18/2022 00:14:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.82 on epoch=342
05/18/2022 00:14:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.81 on epoch=344
05/18/2022 00:14:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.82 on epoch=347
05/18/2022 00:14:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.76 on epoch=349
05/18/2022 00:14:46 - INFO - __main__ - Global step 1400 Train loss 0.80 Classification-F1 0.12450704225352113 on epoch=349
05/18/2022 00:14:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.77 on epoch=352
05/18/2022 00:14:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.80 on epoch=354
05/18/2022 00:14:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.87 on epoch=357
05/18/2022 00:14:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.78 on epoch=359
05/18/2022 00:14:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.78 on epoch=362
05/18/2022 00:14:53 - INFO - __main__ - Global step 1450 Train loss 0.80 Classification-F1 0.18999453253143794 on epoch=362
05/18/2022 00:14:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.74 on epoch=364
05/18/2022 00:14:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.76 on epoch=367
05/18/2022 00:14:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.77 on epoch=369
05/18/2022 00:14:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.85 on epoch=372
05/18/2022 00:14:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.84 on epoch=374
05/18/2022 00:14:59 - INFO - __main__ - Global step 1500 Train loss 0.79 Classification-F1 0.16373636148007592 on epoch=374
05/18/2022 00:15:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.82 on epoch=377
05/18/2022 00:15:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.83 on epoch=379
05/18/2022 00:15:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.85 on epoch=382
05/18/2022 00:15:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.75 on epoch=384
05/18/2022 00:15:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.84 on epoch=387
05/18/2022 00:15:06 - INFO - __main__ - Global step 1550 Train loss 0.82 Classification-F1 0.11969993476842793 on epoch=387
05/18/2022 00:15:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.77 on epoch=389
05/18/2022 00:15:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.82 on epoch=392
05/18/2022 00:15:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.72 on epoch=394
05/18/2022 00:15:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.74 on epoch=397
05/18/2022 00:15:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.86 on epoch=399
05/18/2022 00:15:13 - INFO - __main__ - Global step 1600 Train loss 0.78 Classification-F1 0.11674718196457327 on epoch=399
05/18/2022 00:15:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.79 on epoch=402
05/18/2022 00:15:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.85 on epoch=404
05/18/2022 00:15:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.85 on epoch=407
05/18/2022 00:15:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.80 on epoch=409
05/18/2022 00:15:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.83 on epoch=412
05/18/2022 00:15:19 - INFO - __main__ - Global step 1650 Train loss 0.82 Classification-F1 0.20907738095238096 on epoch=412
05/18/2022 00:15:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.84 on epoch=414
05/18/2022 00:15:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.81 on epoch=417
05/18/2022 00:15:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.81 on epoch=419
05/18/2022 00:15:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.78 on epoch=422
05/18/2022 00:15:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.80 on epoch=424
05/18/2022 00:15:26 - INFO - __main__ - Global step 1700 Train loss 0.81 Classification-F1 0.17011224779766979 on epoch=424
05/18/2022 00:15:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.78 on epoch=427
05/18/2022 00:15:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.74 on epoch=429
05/18/2022 00:15:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.79 on epoch=432
05/18/2022 00:15:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.79 on epoch=434
05/18/2022 00:15:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.82 on epoch=437
05/18/2022 00:15:32 - INFO - __main__ - Global step 1750 Train loss 0.78 Classification-F1 0.2294764358071388 on epoch=437
05/18/2022 00:15:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.82 on epoch=439
05/18/2022 00:15:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.75 on epoch=442
05/18/2022 00:15:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.78 on epoch=444
05/18/2022 00:15:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.77 on epoch=447
05/18/2022 00:15:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.79 on epoch=449
05/18/2022 00:15:39 - INFO - __main__ - Global step 1800 Train loss 0.78 Classification-F1 0.1478328173374613 on epoch=449
05/18/2022 00:15:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.80 on epoch=452
05/18/2022 00:15:41 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.75 on epoch=454
05/18/2022 00:15:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.81 on epoch=457
05/18/2022 00:15:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.79 on epoch=459
05/18/2022 00:15:45 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.77 on epoch=462
05/18/2022 00:15:46 - INFO - __main__ - Global step 1850 Train loss 0.78 Classification-F1 0.2267949767949768 on epoch=462
05/18/2022 00:15:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.74 on epoch=464
05/18/2022 00:15:48 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.77 on epoch=467
05/18/2022 00:15:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.80 on epoch=469
05/18/2022 00:15:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.81 on epoch=472
05/18/2022 00:15:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.81 on epoch=474
05/18/2022 00:15:52 - INFO - __main__ - Global step 1900 Train loss 0.78 Classification-F1 0.11274509803921567 on epoch=474
05/18/2022 00:15:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.82 on epoch=477
05/18/2022 00:15:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.81 on epoch=479
05/18/2022 00:15:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.84 on epoch=482
05/18/2022 00:15:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.74 on epoch=484
05/18/2022 00:15:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.76 on epoch=487
05/18/2022 00:15:59 - INFO - __main__ - Global step 1950 Train loss 0.79 Classification-F1 0.11415362731152205 on epoch=487
05/18/2022 00:16:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.75 on epoch=489
05/18/2022 00:16:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.76 on epoch=492
05/18/2022 00:16:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.70 on epoch=494
05/18/2022 00:16:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.77 on epoch=497
05/18/2022 00:16:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.78 on epoch=499
05/18/2022 00:16:05 - INFO - __main__ - Global step 2000 Train loss 0.75 Classification-F1 0.1480294483920592 on epoch=499
05/18/2022 00:16:07 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.78 on epoch=502
05/18/2022 00:16:08 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.78 on epoch=504
05/18/2022 00:16:09 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.75 on epoch=507
05/18/2022 00:16:10 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.78 on epoch=509
05/18/2022 00:16:11 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.81 on epoch=512
05/18/2022 00:16:12 - INFO - __main__ - Global step 2050 Train loss 0.78 Classification-F1 0.10126582278481013 on epoch=512
05/18/2022 00:16:13 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.83 on epoch=514
05/18/2022 00:16:15 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.82 on epoch=517
05/18/2022 00:16:16 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.81 on epoch=519
05/18/2022 00:16:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.76 on epoch=522
05/18/2022 00:16:18 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.78 on epoch=524
05/18/2022 00:16:19 - INFO - __main__ - Global step 2100 Train loss 0.80 Classification-F1 0.154093567251462 on epoch=524
05/18/2022 00:16:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.78 on epoch=527
05/18/2022 00:16:21 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.73 on epoch=529
05/18/2022 00:16:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.82 on epoch=532
05/18/2022 00:16:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.72 on epoch=534
05/18/2022 00:16:25 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.80 on epoch=537
05/18/2022 00:16:25 - INFO - __main__ - Global step 2150 Train loss 0.77 Classification-F1 0.19667767369768852 on epoch=537
05/18/2022 00:16:26 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.80 on epoch=539
05/18/2022 00:16:28 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.79 on epoch=542
05/18/2022 00:16:29 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.78 on epoch=544
05/18/2022 00:16:30 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.76 on epoch=547
05/18/2022 00:16:31 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.90 on epoch=549
05/18/2022 00:16:32 - INFO - __main__ - Global step 2200 Train loss 0.81 Classification-F1 0.12518037518037517 on epoch=549
05/18/2022 00:16:33 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.77 on epoch=552
05/18/2022 00:16:34 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.75 on epoch=554
05/18/2022 00:16:36 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.81 on epoch=557
05/18/2022 00:16:37 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.76 on epoch=559
05/18/2022 00:16:38 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.81 on epoch=562
05/18/2022 00:16:39 - INFO - __main__ - Global step 2250 Train loss 0.78 Classification-F1 0.2268728004022122 on epoch=562
05/18/2022 00:16:40 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.75 on epoch=564
05/18/2022 00:16:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.81 on epoch=567
05/18/2022 00:16:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.78 on epoch=569
05/18/2022 00:16:43 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.79 on epoch=572
05/18/2022 00:16:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.78 on epoch=574
05/18/2022 00:16:45 - INFO - __main__ - Global step 2300 Train loss 0.78 Classification-F1 0.18581399735898887 on epoch=574
05/18/2022 00:16:46 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.72 on epoch=577
05/18/2022 00:16:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.73 on epoch=579
05/18/2022 00:16:49 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.71 on epoch=582
05/18/2022 00:16:50 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.78 on epoch=584
05/18/2022 00:16:51 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.76 on epoch=587
05/18/2022 00:16:52 - INFO - __main__ - Global step 2350 Train loss 0.74 Classification-F1 0.18301435406698563 on epoch=587
05/18/2022 00:16:53 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.86 on epoch=589
05/18/2022 00:16:54 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.68 on epoch=592
05/18/2022 00:16:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.77 on epoch=594
05/18/2022 00:16:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.77 on epoch=597
05/18/2022 00:16:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.74 on epoch=599
05/18/2022 00:16:58 - INFO - __main__ - Global step 2400 Train loss 0.76 Classification-F1 0.15441176470588236 on epoch=599
05/18/2022 00:17:00 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.82 on epoch=602
05/18/2022 00:17:01 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.73 on epoch=604
05/18/2022 00:17:02 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.77 on epoch=607
05/18/2022 00:17:03 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.75 on epoch=609
05/18/2022 00:17:05 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.70 on epoch=612
05/18/2022 00:17:05 - INFO - __main__ - Global step 2450 Train loss 0.75 Classification-F1 0.17484393757503003 on epoch=612
05/18/2022 00:17:06 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.76 on epoch=614
05/18/2022 00:17:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.83 on epoch=617
05/18/2022 00:17:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.78 on epoch=619
05/18/2022 00:17:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.86 on epoch=622
05/18/2022 00:17:11 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.75 on epoch=624
05/18/2022 00:17:12 - INFO - __main__ - Global step 2500 Train loss 0.80 Classification-F1 0.12368421052631579 on epoch=624
05/18/2022 00:17:13 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.72 on epoch=627
05/18/2022 00:17:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.77 on epoch=629
05/18/2022 00:17:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.77 on epoch=632
05/18/2022 00:17:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.75 on epoch=634
05/18/2022 00:17:18 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.74 on epoch=637
05/18/2022 00:17:18 - INFO - __main__ - Global step 2550 Train loss 0.75 Classification-F1 0.12275641025641025 on epoch=637
05/18/2022 00:17:20 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.83 on epoch=639
05/18/2022 00:17:21 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.78 on epoch=642
05/18/2022 00:17:22 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.74 on epoch=644
05/18/2022 00:17:23 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.73 on epoch=647
05/18/2022 00:17:24 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.74 on epoch=649
05/18/2022 00:17:25 - INFO - __main__ - Global step 2600 Train loss 0.77 Classification-F1 0.13497442455242964 on epoch=649
05/18/2022 00:17:26 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.85 on epoch=652
05/18/2022 00:17:27 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.82 on epoch=654
05/18/2022 00:17:29 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.72 on epoch=657
05/18/2022 00:17:30 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.78 on epoch=659
05/18/2022 00:17:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.76 on epoch=662
05/18/2022 00:17:32 - INFO - __main__ - Global step 2650 Train loss 0.78 Classification-F1 0.2961301912069176 on epoch=662
05/18/2022 00:17:32 - INFO - __main__ - Saving model with best Classification-F1: 0.2491341991341991 -> 0.2961301912069176 on epoch=662, global_step=2650
05/18/2022 00:17:33 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.77 on epoch=664
05/18/2022 00:17:34 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.76 on epoch=667
05/18/2022 00:17:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.76 on epoch=669
05/18/2022 00:17:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.78 on epoch=672
05/18/2022 00:17:38 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.76 on epoch=674
05/18/2022 00:17:38 - INFO - __main__ - Global step 2700 Train loss 0.77 Classification-F1 0.16630681818181817 on epoch=674
05/18/2022 00:17:40 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.71 on epoch=677
05/18/2022 00:17:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.71 on epoch=679
05/18/2022 00:17:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.75 on epoch=682
05/18/2022 00:17:43 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.74 on epoch=684
05/18/2022 00:17:44 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.78 on epoch=687
05/18/2022 00:17:45 - INFO - __main__ - Global step 2750 Train loss 0.74 Classification-F1 0.22233201581027665 on epoch=687
05/18/2022 00:17:46 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.74 on epoch=689
05/18/2022 00:17:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.73 on epoch=692
05/18/2022 00:17:49 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.72 on epoch=694
05/18/2022 00:17:50 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.80 on epoch=697
05/18/2022 00:17:51 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.73 on epoch=699
05/18/2022 00:17:52 - INFO - __main__ - Global step 2800 Train loss 0.75 Classification-F1 0.19318181818181815 on epoch=699
05/18/2022 00:17:53 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.68 on epoch=702
05/18/2022 00:17:54 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.82 on epoch=704
05/18/2022 00:17:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.72 on epoch=707
05/18/2022 00:17:57 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.81 on epoch=709
05/18/2022 00:17:58 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.76 on epoch=712
05/18/2022 00:17:58 - INFO - __main__ - Global step 2850 Train loss 0.76 Classification-F1 0.2578099838969404 on epoch=712
05/18/2022 00:18:00 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.76 on epoch=714
05/18/2022 00:18:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.72 on epoch=717
05/18/2022 00:18:02 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.73 on epoch=719
05/18/2022 00:18:03 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.72 on epoch=722
05/18/2022 00:18:04 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.76 on epoch=724
05/18/2022 00:18:05 - INFO - __main__ - Global step 2900 Train loss 0.74 Classification-F1 0.21735578564114233 on epoch=724
05/18/2022 00:18:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.71 on epoch=727
05/18/2022 00:18:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.65 on epoch=729
05/18/2022 00:18:09 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.83 on epoch=732
05/18/2022 00:18:10 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.79 on epoch=734
05/18/2022 00:18:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.70 on epoch=737
05/18/2022 00:18:12 - INFO - __main__ - Global step 2950 Train loss 0.74 Classification-F1 0.25049261083743846 on epoch=737
05/18/2022 00:18:13 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.79 on epoch=739
05/18/2022 00:18:14 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.75 on epoch=742
05/18/2022 00:18:15 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.81 on epoch=744
05/18/2022 00:18:17 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.75 on epoch=747
05/18/2022 00:18:18 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.71 on epoch=749
05/18/2022 00:18:18 - INFO - __main__ - Global step 3000 Train loss 0.76 Classification-F1 0.2299218897044984 on epoch=749
05/18/2022 00:18:18 - INFO - __main__ - save last model!
05/18/2022 00:18:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 00:18:19 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 00:18:19 - INFO - __main__ - Printing 3 examples
05/18/2022 00:18:19 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 00:18:19 - INFO - __main__ - ['others']
05/18/2022 00:18:19 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 00:18:19 - INFO - __main__ - ['others']
05/18/2022 00:18:19 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 00:18:19 - INFO - __main__ - ['others']
05/18/2022 00:18:19 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:18:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:18:19 - INFO - __main__ - Printing 3 examples
05/18/2022 00:18:19 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/18/2022 00:18:19 - INFO - __main__ - ['others']
05/18/2022 00:18:19 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/18/2022 00:18:19 - INFO - __main__ - ['others']
05/18/2022 00:18:19 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/18/2022 00:18:19 - INFO - __main__ - ['others']
05/18/2022 00:18:19 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:18:20 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:18:20 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 00:18:20 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:18:20 - INFO - __main__ - Printing 3 examples
05/18/2022 00:18:20 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/18/2022 00:18:20 - INFO - __main__ - ['others']
05/18/2022 00:18:20 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/18/2022 00:18:20 - INFO - __main__ - ['others']
05/18/2022 00:18:20 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/18/2022 00:18:20 - INFO - __main__ - ['others']
05/18/2022 00:18:20 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:18:20 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:18:20 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 00:18:21 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:18:26 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 00:18:26 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 00:18:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 00:18:26 - INFO - __main__ - Starting training!
05/18/2022 00:19:09 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_13_0.5_8_predictions.txt
05/18/2022 00:19:10 - INFO - __main__ - Classification-F1 on test data: 0.1207
05/18/2022 00:19:10 - INFO - __main__ - prefix=emo_16_13, lr=0.5, bsz=8, dev_performance=0.2961301912069176, test_performance=0.12066386332039226
05/18/2022 00:19:10 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.4, bsz=8 ...
05/18/2022 00:19:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:19:11 - INFO - __main__ - Printing 3 examples
05/18/2022 00:19:11 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/18/2022 00:19:11 - INFO - __main__ - ['others']
05/18/2022 00:19:11 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/18/2022 00:19:11 - INFO - __main__ - ['others']
05/18/2022 00:19:11 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/18/2022 00:19:11 - INFO - __main__ - ['others']
05/18/2022 00:19:11 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:19:11 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:19:11 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 00:19:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:19:11 - INFO - __main__ - Printing 3 examples
05/18/2022 00:19:11 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/18/2022 00:19:11 - INFO - __main__ - ['others']
05/18/2022 00:19:11 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/18/2022 00:19:11 - INFO - __main__ - ['others']
05/18/2022 00:19:11 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/18/2022 00:19:11 - INFO - __main__ - ['others']
05/18/2022 00:19:11 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:19:11 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:19:11 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 00:19:16 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 00:19:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 00:19:17 - INFO - __main__ - Starting training!
05/18/2022 00:19:19 - INFO - __main__ - Step 10 Global step 10 Train loss 4.33 on epoch=2
05/18/2022 00:19:20 - INFO - __main__ - Step 20 Global step 20 Train loss 3.67 on epoch=4
05/18/2022 00:19:22 - INFO - __main__ - Step 30 Global step 30 Train loss 3.34 on epoch=7
05/18/2022 00:19:23 - INFO - __main__ - Step 40 Global step 40 Train loss 2.56 on epoch=9
05/18/2022 00:19:24 - INFO - __main__ - Step 50 Global step 50 Train loss 2.22 on epoch=12
05/18/2022 00:19:25 - INFO - __main__ - Global step 50 Train loss 3.23 Classification-F1 0.1 on epoch=12
05/18/2022 00:19:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/18/2022 00:19:26 - INFO - __main__ - Step 60 Global step 60 Train loss 1.75 on epoch=14
05/18/2022 00:19:27 - INFO - __main__ - Step 70 Global step 70 Train loss 1.64 on epoch=17
05/18/2022 00:19:28 - INFO - __main__ - Step 80 Global step 80 Train loss 1.34 on epoch=19
05/18/2022 00:19:30 - INFO - __main__ - Step 90 Global step 90 Train loss 1.38 on epoch=22
05/18/2022 00:19:31 - INFO - __main__ - Step 100 Global step 100 Train loss 1.19 on epoch=24
05/18/2022 00:19:31 - INFO - __main__ - Global step 100 Train loss 1.46 Classification-F1 0.11805555555555555 on epoch=24
05/18/2022 00:19:31 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.11805555555555555 on epoch=24, global_step=100
05/18/2022 00:19:33 - INFO - __main__ - Step 110 Global step 110 Train loss 1.09 on epoch=27
05/18/2022 00:19:34 - INFO - __main__ - Step 120 Global step 120 Train loss 1.08 on epoch=29
05/18/2022 00:19:35 - INFO - __main__ - Step 130 Global step 130 Train loss 1.15 on epoch=32
05/18/2022 00:19:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.98 on epoch=34
05/18/2022 00:19:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.89 on epoch=37
05/18/2022 00:19:38 - INFO - __main__ - Global step 150 Train loss 1.04 Classification-F1 0.09615384615384615 on epoch=37
05/18/2022 00:19:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.96 on epoch=39
05/18/2022 00:19:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.95 on epoch=42
05/18/2022 00:19:42 - INFO - __main__ - Step 180 Global step 180 Train loss 1.08 on epoch=44
05/18/2022 00:19:43 - INFO - __main__ - Step 190 Global step 190 Train loss 0.94 on epoch=47
05/18/2022 00:19:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.93 on epoch=49
05/18/2022 00:19:45 - INFO - __main__ - Global step 200 Train loss 0.97 Classification-F1 0.1659804426145136 on epoch=49
05/18/2022 00:19:45 - INFO - __main__ - Saving model with best Classification-F1: 0.11805555555555555 -> 0.1659804426145136 on epoch=49, global_step=200
05/18/2022 00:19:46 - INFO - __main__ - Step 210 Global step 210 Train loss 1.05 on epoch=52
05/18/2022 00:19:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.90 on epoch=54
05/18/2022 00:19:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.95 on epoch=57
05/18/2022 00:19:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.95 on epoch=59
05/18/2022 00:19:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.93 on epoch=62
05/18/2022 00:19:52 - INFO - __main__ - Global step 250 Train loss 0.96 Classification-F1 0.1 on epoch=62
05/18/2022 00:19:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.90 on epoch=64
05/18/2022 00:19:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.94 on epoch=67
05/18/2022 00:19:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.91 on epoch=69
05/18/2022 00:19:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.94 on epoch=72
05/18/2022 00:19:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.97 on epoch=74
05/18/2022 00:19:58 - INFO - __main__ - Global step 300 Train loss 0.93 Classification-F1 0.11507936507936507 on epoch=74
05/18/2022 00:20:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.89 on epoch=77
05/18/2022 00:20:01 - INFO - __main__ - Step 320 Global step 320 Train loss 1.00 on epoch=79
05/18/2022 00:20:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.95 on epoch=82
05/18/2022 00:20:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.96 on epoch=84
05/18/2022 00:20:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.85 on epoch=87
05/18/2022 00:20:05 - INFO - __main__ - Global step 350 Train loss 0.93 Classification-F1 0.1 on epoch=87
05/18/2022 00:20:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.90 on epoch=89
05/18/2022 00:20:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.91 on epoch=92
05/18/2022 00:20:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.83 on epoch=94
05/18/2022 00:20:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.79 on epoch=97
05/18/2022 00:20:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.91 on epoch=99
05/18/2022 00:20:12 - INFO - __main__ - Global step 400 Train loss 0.87 Classification-F1 0.1 on epoch=99
05/18/2022 00:20:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.91 on epoch=102
05/18/2022 00:20:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.94 on epoch=104
05/18/2022 00:20:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.93 on epoch=107
05/18/2022 00:20:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.85 on epoch=109
05/18/2022 00:20:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.83 on epoch=112
05/18/2022 00:20:19 - INFO - __main__ - Global step 450 Train loss 0.89 Classification-F1 0.1 on epoch=112
05/18/2022 00:20:20 - INFO - __main__ - Step 460 Global step 460 Train loss 1.01 on epoch=114
05/18/2022 00:20:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.92 on epoch=117
05/18/2022 00:20:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.92 on epoch=119
05/18/2022 00:20:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.90 on epoch=122
05/18/2022 00:20:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.91 on epoch=124
05/18/2022 00:20:25 - INFO - __main__ - Global step 500 Train loss 0.93 Classification-F1 0.10256410256410256 on epoch=124
05/18/2022 00:20:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.98 on epoch=127
05/18/2022 00:20:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.91 on epoch=129
05/18/2022 00:20:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.88 on epoch=132
05/18/2022 00:20:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.82 on epoch=134
05/18/2022 00:20:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=137
05/18/2022 00:20:32 - INFO - __main__ - Global step 550 Train loss 0.88 Classification-F1 0.1 on epoch=137
05/18/2022 00:20:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.84 on epoch=139
05/18/2022 00:20:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.92 on epoch=142
05/18/2022 00:20:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.84 on epoch=144
05/18/2022 00:20:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.84 on epoch=147
05/18/2022 00:20:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.85 on epoch=149
05/18/2022 00:20:39 - INFO - __main__ - Global step 600 Train loss 0.86 Classification-F1 0.1 on epoch=149
05/18/2022 00:20:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.89 on epoch=152
05/18/2022 00:20:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.84 on epoch=154
05/18/2022 00:20:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.81 on epoch=157
05/18/2022 00:20:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.82 on epoch=159
05/18/2022 00:20:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.79 on epoch=162
05/18/2022 00:20:46 - INFO - __main__ - Global step 650 Train loss 0.83 Classification-F1 0.1 on epoch=162
05/18/2022 00:20:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.85 on epoch=164
05/18/2022 00:20:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.89 on epoch=167
05/18/2022 00:20:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.79 on epoch=169
05/18/2022 00:20:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.86 on epoch=172
05/18/2022 00:20:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.79 on epoch=174
05/18/2022 00:20:52 - INFO - __main__ - Global step 700 Train loss 0.84 Classification-F1 0.1 on epoch=174
05/18/2022 00:20:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.85 on epoch=177
05/18/2022 00:20:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.89 on epoch=179
05/18/2022 00:20:56 - INFO - __main__ - Step 730 Global step 730 Train loss 0.83 on epoch=182
05/18/2022 00:20:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.78 on epoch=184
05/18/2022 00:20:59 - INFO - __main__ - Step 750 Global step 750 Train loss 0.93 on epoch=187
05/18/2022 00:20:59 - INFO - __main__ - Global step 750 Train loss 0.86 Classification-F1 0.12590579710144928 on epoch=187
05/18/2022 00:21:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.85 on epoch=189
05/18/2022 00:21:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.83 on epoch=192
05/18/2022 00:21:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.83 on epoch=194
05/18/2022 00:21:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.91 on epoch=197
05/18/2022 00:21:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.82 on epoch=199
05/18/2022 00:21:06 - INFO - __main__ - Global step 800 Train loss 0.85 Classification-F1 0.1 on epoch=199
05/18/2022 00:21:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.82 on epoch=202
05/18/2022 00:21:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.76 on epoch=204
05/18/2022 00:21:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.83 on epoch=207
05/18/2022 00:21:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.82 on epoch=209
05/18/2022 00:21:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.89 on epoch=212
05/18/2022 00:21:13 - INFO - __main__ - Global step 850 Train loss 0.82 Classification-F1 0.1 on epoch=212
05/18/2022 00:21:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.81 on epoch=214
05/18/2022 00:21:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.75 on epoch=217
05/18/2022 00:21:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.84 on epoch=219
05/18/2022 00:21:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.78 on epoch=222
05/18/2022 00:21:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.82 on epoch=224
05/18/2022 00:21:19 - INFO - __main__ - Global step 900 Train loss 0.80 Classification-F1 0.1 on epoch=224
05/18/2022 00:21:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.83 on epoch=227
05/18/2022 00:21:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.81 on epoch=229
05/18/2022 00:21:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.82 on epoch=232
05/18/2022 00:21:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.81 on epoch=234
05/18/2022 00:21:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.85 on epoch=237
05/18/2022 00:21:26 - INFO - __main__ - Global step 950 Train loss 0.82 Classification-F1 0.1 on epoch=237
05/18/2022 00:21:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.89 on epoch=239
05/18/2022 00:21:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.85 on epoch=242
05/18/2022 00:21:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.81 on epoch=244
05/18/2022 00:21:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.75 on epoch=247
05/18/2022 00:21:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.84 on epoch=249
05/18/2022 00:21:33 - INFO - __main__ - Global step 1000 Train loss 0.83 Classification-F1 0.08974358974358974 on epoch=249
05/18/2022 00:21:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.77 on epoch=252
05/18/2022 00:21:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.87 on epoch=254
05/18/2022 00:21:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.80 on epoch=257
05/18/2022 00:21:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.78 on epoch=259
05/18/2022 00:21:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.81 on epoch=262
05/18/2022 00:21:39 - INFO - __main__ - Global step 1050 Train loss 0.81 Classification-F1 0.1 on epoch=262
05/18/2022 00:21:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.83 on epoch=264
05/18/2022 00:21:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.79 on epoch=267
05/18/2022 00:21:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.77 on epoch=269
05/18/2022 00:21:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.83 on epoch=272
05/18/2022 00:21:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.84 on epoch=274
05/18/2022 00:21:46 - INFO - __main__ - Global step 1100 Train loss 0.81 Classification-F1 0.1 on epoch=274
05/18/2022 00:21:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.76 on epoch=277
05/18/2022 00:21:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.82 on epoch=279
05/18/2022 00:21:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.83 on epoch=282
05/18/2022 00:21:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.82 on epoch=284
05/18/2022 00:21:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.82 on epoch=287
05/18/2022 00:21:53 - INFO - __main__ - Global step 1150 Train loss 0.81 Classification-F1 0.1238095238095238 on epoch=287
05/18/2022 00:21:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.75 on epoch=289
05/18/2022 00:21:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.73 on epoch=292
05/18/2022 00:21:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.82 on epoch=294
05/18/2022 00:21:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.80 on epoch=297
05/18/2022 00:21:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.85 on epoch=299
05/18/2022 00:22:00 - INFO - __main__ - Global step 1200 Train loss 0.79 Classification-F1 0.1 on epoch=299
05/18/2022 00:22:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.83 on epoch=302
05/18/2022 00:22:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.81 on epoch=304
05/18/2022 00:22:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.88 on epoch=307
05/18/2022 00:22:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.83 on epoch=309
05/18/2022 00:22:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.84 on epoch=312
05/18/2022 00:22:06 - INFO - __main__ - Global step 1250 Train loss 0.84 Classification-F1 0.17979490646833446 on epoch=312
05/18/2022 00:22:06 - INFO - __main__ - Saving model with best Classification-F1: 0.1659804426145136 -> 0.17979490646833446 on epoch=312, global_step=1250
05/18/2022 00:22:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.82 on epoch=314
05/18/2022 00:22:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.81 on epoch=317
05/18/2022 00:22:10 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.79 on epoch=319
05/18/2022 00:22:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.86 on epoch=322
05/18/2022 00:22:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.79 on epoch=324
05/18/2022 00:22:13 - INFO - __main__ - Global step 1300 Train loss 0.81 Classification-F1 0.16695652173913045 on epoch=324
05/18/2022 00:22:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.89 on epoch=327
05/18/2022 00:22:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.82 on epoch=329
05/18/2022 00:22:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.77 on epoch=332
05/18/2022 00:22:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.88 on epoch=334
05/18/2022 00:22:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.78 on epoch=337
05/18/2022 00:22:20 - INFO - __main__ - Global step 1350 Train loss 0.83 Classification-F1 0.19732679541161074 on epoch=337
05/18/2022 00:22:20 - INFO - __main__ - Saving model with best Classification-F1: 0.17979490646833446 -> 0.19732679541161074 on epoch=337, global_step=1350
05/18/2022 00:22:21 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.78 on epoch=339
05/18/2022 00:22:22 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.81 on epoch=342
05/18/2022 00:22:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.78 on epoch=344
05/18/2022 00:22:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.80 on epoch=347
05/18/2022 00:22:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.87 on epoch=349
05/18/2022 00:22:27 - INFO - __main__ - Global step 1400 Train loss 0.81 Classification-F1 0.10833333333333331 on epoch=349
05/18/2022 00:22:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.75 on epoch=352
05/18/2022 00:22:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.76 on epoch=354
05/18/2022 00:22:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.85 on epoch=357
05/18/2022 00:22:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.77 on epoch=359
05/18/2022 00:22:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.76 on epoch=362
05/18/2022 00:22:33 - INFO - __main__ - Global step 1450 Train loss 0.78 Classification-F1 0.15585364580614855 on epoch=362
05/18/2022 00:22:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.80 on epoch=364
05/18/2022 00:22:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.79 on epoch=367
05/18/2022 00:22:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.81 on epoch=369
05/18/2022 00:22:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.81 on epoch=372
05/18/2022 00:22:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.80 on epoch=374
05/18/2022 00:22:40 - INFO - __main__ - Global step 1500 Train loss 0.80 Classification-F1 0.16637984981226533 on epoch=374
05/18/2022 00:22:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.87 on epoch=377
05/18/2022 00:22:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.78 on epoch=379
05/18/2022 00:22:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.82 on epoch=382
05/18/2022 00:22:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.83 on epoch=384
05/18/2022 00:22:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.85 on epoch=387
05/18/2022 00:22:47 - INFO - __main__ - Global step 1550 Train loss 0.83 Classification-F1 0.15572755417956655 on epoch=387
05/18/2022 00:22:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.85 on epoch=389
05/18/2022 00:22:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.80 on epoch=392
05/18/2022 00:22:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.82 on epoch=394
05/18/2022 00:22:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.84 on epoch=397
05/18/2022 00:22:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.72 on epoch=399
05/18/2022 00:22:54 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.15582419408441342 on epoch=399
05/18/2022 00:22:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.89 on epoch=402
05/18/2022 00:22:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.87 on epoch=404
05/18/2022 00:22:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.84 on epoch=407
05/18/2022 00:22:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.75 on epoch=409
05/18/2022 00:23:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.83 on epoch=412
05/18/2022 00:23:01 - INFO - __main__ - Global step 1650 Train loss 0.83 Classification-F1 0.11868686868686869 on epoch=412
05/18/2022 00:23:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.82 on epoch=414
05/18/2022 00:23:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.73 on epoch=417
05/18/2022 00:23:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.84 on epoch=419
05/18/2022 00:23:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.81 on epoch=422
05/18/2022 00:23:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.84 on epoch=424
05/18/2022 00:23:07 - INFO - __main__ - Global step 1700 Train loss 0.81 Classification-F1 0.13654401154401155 on epoch=424
05/18/2022 00:23:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.83 on epoch=427
05/18/2022 00:23:10 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.86 on epoch=429
05/18/2022 00:23:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.78 on epoch=432
05/18/2022 00:23:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.81 on epoch=434
05/18/2022 00:23:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.87 on epoch=437
05/18/2022 00:23:14 - INFO - __main__ - Global step 1750 Train loss 0.83 Classification-F1 0.20383986928104575 on epoch=437
05/18/2022 00:23:14 - INFO - __main__ - Saving model with best Classification-F1: 0.19732679541161074 -> 0.20383986928104575 on epoch=437, global_step=1750
05/18/2022 00:23:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.84 on epoch=439
05/18/2022 00:23:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.75 on epoch=442
05/18/2022 00:23:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.80 on epoch=444
05/18/2022 00:23:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.86 on epoch=447
05/18/2022 00:23:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.83 on epoch=449
05/18/2022 00:23:21 - INFO - __main__ - Global step 1800 Train loss 0.82 Classification-F1 0.13067758749069247 on epoch=449
05/18/2022 00:23:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.76 on epoch=452
05/18/2022 00:23:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.83 on epoch=454
05/18/2022 00:23:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.85 on epoch=457
05/18/2022 00:23:26 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.75 on epoch=459
05/18/2022 00:23:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.78 on epoch=462
05/18/2022 00:23:27 - INFO - __main__ - Global step 1850 Train loss 0.79 Classification-F1 0.2332516339869281 on epoch=462
05/18/2022 00:23:28 - INFO - __main__ - Saving model with best Classification-F1: 0.20383986928104575 -> 0.2332516339869281 on epoch=462, global_step=1850
05/18/2022 00:23:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.77 on epoch=464
05/18/2022 00:23:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.80 on epoch=467
05/18/2022 00:23:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.73 on epoch=469
05/18/2022 00:23:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.74 on epoch=472
05/18/2022 00:23:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.79 on epoch=474
05/18/2022 00:23:34 - INFO - __main__ - Global step 1900 Train loss 0.76 Classification-F1 0.14367690058479532 on epoch=474
05/18/2022 00:23:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.81 on epoch=477
05/18/2022 00:23:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.80 on epoch=479
05/18/2022 00:23:38 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.77 on epoch=482
05/18/2022 00:23:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.80 on epoch=484
05/18/2022 00:23:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.80 on epoch=487
05/18/2022 00:23:41 - INFO - __main__ - Global step 1950 Train loss 0.79 Classification-F1 0.19916582540778918 on epoch=487
05/18/2022 00:23:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.84 on epoch=489
05/18/2022 00:23:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.83 on epoch=492
05/18/2022 00:23:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.78 on epoch=494
05/18/2022 00:23:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.82 on epoch=497
05/18/2022 00:23:47 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.78 on epoch=499
05/18/2022 00:23:48 - INFO - __main__ - Global step 2000 Train loss 0.81 Classification-F1 0.18721318314559865 on epoch=499
05/18/2022 00:23:49 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.78 on epoch=502
05/18/2022 00:23:50 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.80 on epoch=504
05/18/2022 00:23:52 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.78 on epoch=507
05/18/2022 00:23:53 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.80 on epoch=509
05/18/2022 00:23:54 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.80 on epoch=512
05/18/2022 00:23:55 - INFO - __main__ - Global step 2050 Train loss 0.79 Classification-F1 0.2227947491105386 on epoch=512
05/18/2022 00:23:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.77 on epoch=514
05/18/2022 00:23:57 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.85 on epoch=517
05/18/2022 00:23:58 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.75 on epoch=519
05/18/2022 00:24:00 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.87 on epoch=522
05/18/2022 00:24:01 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.85 on epoch=524
05/18/2022 00:24:01 - INFO - __main__ - Global step 2100 Train loss 0.82 Classification-F1 0.16677723721067372 on epoch=524
05/18/2022 00:24:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.83 on epoch=527
05/18/2022 00:24:04 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.80 on epoch=529
05/18/2022 00:24:05 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.76 on epoch=532
05/18/2022 00:24:06 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.76 on epoch=534
05/18/2022 00:24:08 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.75 on epoch=537
05/18/2022 00:24:08 - INFO - __main__ - Global step 2150 Train loss 0.78 Classification-F1 0.2216672534178255 on epoch=537
05/18/2022 00:24:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.79 on epoch=539
05/18/2022 00:24:11 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.82 on epoch=542
05/18/2022 00:24:12 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.87 on epoch=544
05/18/2022 00:24:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.76 on epoch=547
05/18/2022 00:24:14 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.79 on epoch=549
05/18/2022 00:24:15 - INFO - __main__ - Global step 2200 Train loss 0.81 Classification-F1 0.21457426221577164 on epoch=549
05/18/2022 00:24:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.77 on epoch=552
05/18/2022 00:24:17 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.74 on epoch=554
05/18/2022 00:24:18 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.83 on epoch=557
05/18/2022 00:24:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.80 on epoch=559
05/18/2022 00:24:21 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.74 on epoch=562
05/18/2022 00:24:21 - INFO - __main__ - Global step 2250 Train loss 0.77 Classification-F1 0.20022629600125832 on epoch=562
05/18/2022 00:24:23 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.84 on epoch=564
05/18/2022 00:24:24 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.81 on epoch=567
05/18/2022 00:24:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.87 on epoch=569
05/18/2022 00:24:26 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.80 on epoch=572
05/18/2022 00:24:28 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.81 on epoch=574
05/18/2022 00:24:28 - INFO - __main__ - Global step 2300 Train loss 0.83 Classification-F1 0.20502364066193854 on epoch=574
05/18/2022 00:24:30 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.76 on epoch=577
05/18/2022 00:24:31 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.82 on epoch=579
05/18/2022 00:24:32 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.79 on epoch=582
05/18/2022 00:24:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.73 on epoch=584
05/18/2022 00:24:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.81 on epoch=587
05/18/2022 00:24:35 - INFO - __main__ - Global step 2350 Train loss 0.78 Classification-F1 0.265245995423341 on epoch=587
05/18/2022 00:24:35 - INFO - __main__ - Saving model with best Classification-F1: 0.2332516339869281 -> 0.265245995423341 on epoch=587, global_step=2350
05/18/2022 00:24:36 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.75 on epoch=589
05/18/2022 00:24:37 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.71 on epoch=592
05/18/2022 00:24:39 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.76 on epoch=594
05/18/2022 00:24:40 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.81 on epoch=597
05/18/2022 00:24:41 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.78 on epoch=599
05/18/2022 00:24:42 - INFO - __main__ - Global step 2400 Train loss 0.76 Classification-F1 0.18087833219412167 on epoch=599
05/18/2022 00:24:43 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.77 on epoch=602
05/18/2022 00:24:44 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.72 on epoch=604
05/18/2022 00:24:45 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.72 on epoch=607
05/18/2022 00:24:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.82 on epoch=609
05/18/2022 00:24:48 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.77 on epoch=612
05/18/2022 00:24:48 - INFO - __main__ - Global step 2450 Train loss 0.76 Classification-F1 0.2575890175890176 on epoch=612
05/18/2022 00:24:50 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.78 on epoch=614
05/18/2022 00:24:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.76 on epoch=617
05/18/2022 00:24:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.78 on epoch=619
05/18/2022 00:24:53 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.76 on epoch=622
05/18/2022 00:24:55 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.74 on epoch=624
05/18/2022 00:24:55 - INFO - __main__ - Global step 2500 Train loss 0.76 Classification-F1 0.17883964582077788 on epoch=624
05/18/2022 00:24:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.77 on epoch=627
05/18/2022 00:24:58 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.82 on epoch=629
05/18/2022 00:24:59 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.80 on epoch=632
05/18/2022 00:25:00 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.79 on epoch=634
05/18/2022 00:25:01 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.75 on epoch=637
05/18/2022 00:25:02 - INFO - __main__ - Global step 2550 Train loss 0.79 Classification-F1 0.17718715393133996 on epoch=637
05/18/2022 00:25:03 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.72 on epoch=639
05/18/2022 00:25:04 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.73 on epoch=642
05/18/2022 00:25:06 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.79 on epoch=644
05/18/2022 00:25:07 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.82 on epoch=647
05/18/2022 00:25:08 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.76 on epoch=649
05/18/2022 00:25:09 - INFO - __main__ - Global step 2600 Train loss 0.76 Classification-F1 0.1982115923292394 on epoch=649
05/18/2022 00:25:10 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.73 on epoch=652
05/18/2022 00:25:11 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.77 on epoch=654
05/18/2022 00:25:12 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.82 on epoch=657
05/18/2022 00:25:13 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.78 on epoch=659
05/18/2022 00:25:15 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.76 on epoch=662
05/18/2022 00:25:15 - INFO - __main__ - Global step 2650 Train loss 0.77 Classification-F1 0.20945532435740513 on epoch=662
05/18/2022 00:25:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.77 on epoch=664
05/18/2022 00:25:18 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.78 on epoch=667
05/18/2022 00:25:19 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.73 on epoch=669
05/18/2022 00:25:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.83 on epoch=672
05/18/2022 00:25:21 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.70 on epoch=674
05/18/2022 00:25:22 - INFO - __main__ - Global step 2700 Train loss 0.76 Classification-F1 0.20604979979979976 on epoch=674
05/18/2022 00:25:23 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.78 on epoch=677
05/18/2022 00:25:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.74 on epoch=679
05/18/2022 00:25:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.78 on epoch=682
05/18/2022 00:25:27 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.80 on epoch=684
05/18/2022 00:25:28 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.79 on epoch=687
05/18/2022 00:25:29 - INFO - __main__ - Global step 2750 Train loss 0.78 Classification-F1 0.22188384397686722 on epoch=687
05/18/2022 00:25:30 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.73 on epoch=689
05/18/2022 00:25:31 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.78 on epoch=692
05/18/2022 00:25:32 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.83 on epoch=694
05/18/2022 00:25:34 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.74 on epoch=697
05/18/2022 00:25:35 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.78 on epoch=699
05/18/2022 00:25:35 - INFO - __main__ - Global step 2800 Train loss 0.77 Classification-F1 0.1821196196196196 on epoch=699
05/18/2022 00:25:37 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.80 on epoch=702
05/18/2022 00:25:38 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.77 on epoch=704
05/18/2022 00:25:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.78 on epoch=707
05/18/2022 00:25:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.72 on epoch=709
05/18/2022 00:25:42 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.75 on epoch=712
05/18/2022 00:25:42 - INFO - __main__ - Global step 2850 Train loss 0.76 Classification-F1 0.2619230769230769 on epoch=712
05/18/2022 00:25:43 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.80 on epoch=714
05/18/2022 00:25:45 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.79 on epoch=717
05/18/2022 00:25:46 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.81 on epoch=719
05/18/2022 00:25:47 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.79 on epoch=722
05/18/2022 00:25:48 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.79 on epoch=724
05/18/2022 00:25:49 - INFO - __main__ - Global step 2900 Train loss 0.80 Classification-F1 0.215625 on epoch=724
05/18/2022 00:25:50 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.74 on epoch=727
05/18/2022 00:25:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.75 on epoch=729
05/18/2022 00:25:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.81 on epoch=732
05/18/2022 00:25:54 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.76 on epoch=734
05/18/2022 00:25:55 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.73 on epoch=737
05/18/2022 00:25:56 - INFO - __main__ - Global step 2950 Train loss 0.76 Classification-F1 0.2617998796570225 on epoch=737
05/18/2022 00:25:57 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.75 on epoch=739
05/18/2022 00:25:58 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.72 on epoch=742
05/18/2022 00:25:59 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.75 on epoch=744
05/18/2022 00:26:01 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.74 on epoch=747
05/18/2022 00:26:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.81 on epoch=749
05/18/2022 00:26:02 - INFO - __main__ - Global step 3000 Train loss 0.75 Classification-F1 0.2167172450191318 on epoch=749
05/18/2022 00:26:02 - INFO - __main__ - save last model!
05/18/2022 00:26:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 00:26:02 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 00:26:02 - INFO - __main__ - Printing 3 examples
05/18/2022 00:26:02 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 00:26:02 - INFO - __main__ - ['others']
05/18/2022 00:26:02 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 00:26:02 - INFO - __main__ - ['others']
05/18/2022 00:26:02 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 00:26:02 - INFO - __main__ - ['others']
05/18/2022 00:26:02 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:26:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:26:03 - INFO - __main__ - Printing 3 examples
05/18/2022 00:26:03 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/18/2022 00:26:03 - INFO - __main__ - ['others']
05/18/2022 00:26:03 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/18/2022 00:26:03 - INFO - __main__ - ['others']
05/18/2022 00:26:03 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/18/2022 00:26:03 - INFO - __main__ - ['others']
05/18/2022 00:26:03 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:26:03 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:26:03 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 00:26:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:26:03 - INFO - __main__ - Printing 3 examples
05/18/2022 00:26:03 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/18/2022 00:26:03 - INFO - __main__ - ['others']
05/18/2022 00:26:03 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/18/2022 00:26:03 - INFO - __main__ - ['others']
05/18/2022 00:26:03 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/18/2022 00:26:03 - INFO - __main__ - ['others']
05/18/2022 00:26:03 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:26:03 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:26:03 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 00:26:05 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:26:09 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 00:26:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 00:26:09 - INFO - __main__ - Starting training!
05/18/2022 00:26:10 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 00:26:54 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_13_0.4_8_predictions.txt
05/18/2022 00:26:54 - INFO - __main__ - Classification-F1 on test data: 0.1027
05/18/2022 00:26:54 - INFO - __main__ - prefix=emo_16_13, lr=0.4, bsz=8, dev_performance=0.265245995423341, test_performance=0.10273527639977276
05/18/2022 00:26:54 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.3, bsz=8 ...
05/18/2022 00:26:55 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:26:55 - INFO - __main__ - Printing 3 examples
05/18/2022 00:26:55 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/18/2022 00:26:55 - INFO - __main__ - ['others']
05/18/2022 00:26:55 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/18/2022 00:26:55 - INFO - __main__ - ['others']
05/18/2022 00:26:55 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/18/2022 00:26:55 - INFO - __main__ - ['others']
05/18/2022 00:26:55 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:26:55 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:26:55 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 00:26:55 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:26:55 - INFO - __main__ - Printing 3 examples
05/18/2022 00:26:55 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/18/2022 00:26:55 - INFO - __main__ - ['others']
05/18/2022 00:26:55 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/18/2022 00:26:55 - INFO - __main__ - ['others']
05/18/2022 00:26:55 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/18/2022 00:26:55 - INFO - __main__ - ['others']
05/18/2022 00:26:55 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:26:55 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:26:55 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 00:27:01 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 00:27:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 00:27:01 - INFO - __main__ - Starting training!
05/18/2022 00:27:02 - INFO - __main__ - Step 10 Global step 10 Train loss 4.34 on epoch=2
05/18/2022 00:27:04 - INFO - __main__ - Step 20 Global step 20 Train loss 3.89 on epoch=4
05/18/2022 00:27:05 - INFO - __main__ - Step 30 Global step 30 Train loss 3.60 on epoch=7
05/18/2022 00:27:06 - INFO - __main__ - Step 40 Global step 40 Train loss 3.09 on epoch=9
05/18/2022 00:27:07 - INFO - __main__ - Step 50 Global step 50 Train loss 2.76 on epoch=12
05/18/2022 00:27:08 - INFO - __main__ - Global step 50 Train loss 3.54 Classification-F1 0.1 on epoch=12
05/18/2022 00:27:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/18/2022 00:27:09 - INFO - __main__ - Step 60 Global step 60 Train loss 2.28 on epoch=14
05/18/2022 00:27:11 - INFO - __main__ - Step 70 Global step 70 Train loss 2.16 on epoch=17
05/18/2022 00:27:12 - INFO - __main__ - Step 80 Global step 80 Train loss 1.75 on epoch=19
05/18/2022 00:27:13 - INFO - __main__ - Step 90 Global step 90 Train loss 1.57 on epoch=22
05/18/2022 00:27:14 - INFO - __main__ - Step 100 Global step 100 Train loss 1.30 on epoch=24
05/18/2022 00:27:15 - INFO - __main__ - Global step 100 Train loss 1.81 Classification-F1 0.10126582278481013 on epoch=24
05/18/2022 00:27:15 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.10126582278481013 on epoch=24, global_step=100
05/18/2022 00:27:16 - INFO - __main__ - Step 110 Global step 110 Train loss 1.39 on epoch=27
05/18/2022 00:27:17 - INFO - __main__ - Step 120 Global step 120 Train loss 1.16 on epoch=29
05/18/2022 00:27:19 - INFO - __main__ - Step 130 Global step 130 Train loss 1.13 on epoch=32
05/18/2022 00:27:20 - INFO - __main__ - Step 140 Global step 140 Train loss 1.06 on epoch=34
05/18/2022 00:27:21 - INFO - __main__ - Step 150 Global step 150 Train loss 1.06 on epoch=37
05/18/2022 00:27:22 - INFO - __main__ - Global step 150 Train loss 1.16 Classification-F1 0.1 on epoch=37
05/18/2022 00:27:23 - INFO - __main__ - Step 160 Global step 160 Train loss 1.12 on epoch=39
05/18/2022 00:27:24 - INFO - __main__ - Step 170 Global step 170 Train loss 1.07 on epoch=42
05/18/2022 00:27:25 - INFO - __main__ - Step 180 Global step 180 Train loss 1.06 on epoch=44
05/18/2022 00:27:26 - INFO - __main__ - Step 190 Global step 190 Train loss 1.07 on epoch=47
05/18/2022 00:27:28 - INFO - __main__ - Step 200 Global step 200 Train loss 1.14 on epoch=49
05/18/2022 00:27:28 - INFO - __main__ - Global step 200 Train loss 1.09 Classification-F1 0.1 on epoch=49
05/18/2022 00:27:29 - INFO - __main__ - Step 210 Global step 210 Train loss 1.04 on epoch=52
05/18/2022 00:27:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.93 on epoch=54
05/18/2022 00:27:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.98 on epoch=57
05/18/2022 00:27:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.98 on epoch=59
05/18/2022 00:27:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.94 on epoch=62
05/18/2022 00:27:35 - INFO - __main__ - Global step 250 Train loss 0.97 Classification-F1 0.1 on epoch=62
05/18/2022 00:27:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.92 on epoch=64
05/18/2022 00:27:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.92 on epoch=67
05/18/2022 00:27:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.93 on epoch=69
05/18/2022 00:27:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.93 on epoch=72
05/18/2022 00:27:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.88 on epoch=74
05/18/2022 00:27:41 - INFO - __main__ - Global step 300 Train loss 0.92 Classification-F1 0.1 on epoch=74
05/18/2022 00:27:42 - INFO - __main__ - Step 310 Global step 310 Train loss 1.01 on epoch=77
05/18/2022 00:27:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.99 on epoch=79
05/18/2022 00:27:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.94 on epoch=82
05/18/2022 00:27:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.91 on epoch=84
05/18/2022 00:27:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.93 on epoch=87
05/18/2022 00:27:48 - INFO - __main__ - Global step 350 Train loss 0.96 Classification-F1 0.1 on epoch=87
05/18/2022 00:27:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.88 on epoch=89
05/18/2022 00:27:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.86 on epoch=92
05/18/2022 00:27:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.78 on epoch=94
05/18/2022 00:27:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.87 on epoch=97
05/18/2022 00:27:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.91 on epoch=99
05/18/2022 00:27:54 - INFO - __main__ - Global step 400 Train loss 0.86 Classification-F1 0.09615384615384615 on epoch=99
05/18/2022 00:27:55 - INFO - __main__ - Step 410 Global step 410 Train loss 1.00 on epoch=102
05/18/2022 00:27:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.88 on epoch=104
05/18/2022 00:27:58 - INFO - __main__ - Step 430 Global step 430 Train loss 1.02 on epoch=107
05/18/2022 00:27:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.96 on epoch=109
05/18/2022 00:28:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.98 on epoch=112
05/18/2022 00:28:01 - INFO - __main__ - Global step 450 Train loss 0.97 Classification-F1 0.10256410256410256 on epoch=112
05/18/2022 00:28:01 - INFO - __main__ - Saving model with best Classification-F1: 0.10126582278481013 -> 0.10256410256410256 on epoch=112, global_step=450
05/18/2022 00:28:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.90 on epoch=114
05/18/2022 00:28:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.80 on epoch=117
05/18/2022 00:28:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.87 on epoch=119
05/18/2022 00:28:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.92 on epoch=122
05/18/2022 00:28:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.82 on epoch=124
05/18/2022 00:28:07 - INFO - __main__ - Global step 500 Train loss 0.86 Classification-F1 0.0974025974025974 on epoch=124
05/18/2022 00:28:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.88 on epoch=127
05/18/2022 00:28:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.86 on epoch=129
05/18/2022 00:28:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.88 on epoch=132
05/18/2022 00:28:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.89 on epoch=134
05/18/2022 00:28:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.84 on epoch=137
05/18/2022 00:28:14 - INFO - __main__ - Global step 550 Train loss 0.87 Classification-F1 0.13067758749069247 on epoch=137
05/18/2022 00:28:14 - INFO - __main__ - Saving model with best Classification-F1: 0.10256410256410256 -> 0.13067758749069247 on epoch=137, global_step=550
05/18/2022 00:28:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.91 on epoch=139
05/18/2022 00:28:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.94 on epoch=142
05/18/2022 00:28:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.91 on epoch=144
05/18/2022 00:28:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.90 on epoch=147
05/18/2022 00:28:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.95 on epoch=149
05/18/2022 00:28:20 - INFO - __main__ - Global step 600 Train loss 0.92 Classification-F1 0.10126582278481013 on epoch=149
05/18/2022 00:28:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.84 on epoch=152
05/18/2022 00:28:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.79 on epoch=154
05/18/2022 00:28:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.95 on epoch=157
05/18/2022 00:28:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.86 on epoch=159
05/18/2022 00:28:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.84 on epoch=162
05/18/2022 00:28:27 - INFO - __main__ - Global step 650 Train loss 0.86 Classification-F1 0.1 on epoch=162
05/18/2022 00:28:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.85 on epoch=164
05/18/2022 00:28:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.84 on epoch=167
05/18/2022 00:28:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.87 on epoch=169
05/18/2022 00:28:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.86 on epoch=172
05/18/2022 00:28:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.86 on epoch=174
05/18/2022 00:28:33 - INFO - __main__ - Global step 700 Train loss 0.86 Classification-F1 0.1237183868762816 on epoch=174
05/18/2022 00:28:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.83 on epoch=177
05/18/2022 00:28:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.89 on epoch=179
05/18/2022 00:28:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.85 on epoch=182
05/18/2022 00:28:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.87 on epoch=184
05/18/2022 00:28:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.90 on epoch=187
05/18/2022 00:28:40 - INFO - __main__ - Global step 750 Train loss 0.87 Classification-F1 0.1 on epoch=187
05/18/2022 00:28:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.89 on epoch=189
05/18/2022 00:28:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.80 on epoch=192
05/18/2022 00:28:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.83 on epoch=194
05/18/2022 00:28:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.84 on epoch=197
05/18/2022 00:28:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.83 on epoch=199
05/18/2022 00:28:46 - INFO - __main__ - Global step 800 Train loss 0.84 Classification-F1 0.1302118933697881 on epoch=199
05/18/2022 00:28:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.83 on epoch=202
05/18/2022 00:28:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.91 on epoch=204
05/18/2022 00:28:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.86 on epoch=207
05/18/2022 00:28:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.87 on epoch=209
05/18/2022 00:28:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.81 on epoch=212
05/18/2022 00:28:53 - INFO - __main__ - Global step 850 Train loss 0.86 Classification-F1 0.10126582278481013 on epoch=212
05/18/2022 00:28:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.84 on epoch=214
05/18/2022 00:28:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.87 on epoch=217
05/18/2022 00:28:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.76 on epoch=219
05/18/2022 00:28:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.90 on epoch=222
05/18/2022 00:28:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.89 on epoch=224
05/18/2022 00:28:59 - INFO - __main__ - Global step 900 Train loss 0.85 Classification-F1 0.1 on epoch=224
05/18/2022 00:29:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.84 on epoch=227
05/18/2022 00:29:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.75 on epoch=229
05/18/2022 00:29:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.86 on epoch=232
05/18/2022 00:29:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.92 on epoch=234
05/18/2022 00:29:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.91 on epoch=237
05/18/2022 00:29:06 - INFO - __main__ - Global step 950 Train loss 0.86 Classification-F1 0.1 on epoch=237
05/18/2022 00:29:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.81 on epoch=239
05/18/2022 00:29:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.81 on epoch=242
05/18/2022 00:29:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.81 on epoch=244
05/18/2022 00:29:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.79 on epoch=247
05/18/2022 00:29:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.81 on epoch=249
05/18/2022 00:29:12 - INFO - __main__ - Global step 1000 Train loss 0.81 Classification-F1 0.1 on epoch=249
05/18/2022 00:29:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.80 on epoch=252
05/18/2022 00:29:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.92 on epoch=254
05/18/2022 00:29:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.80 on epoch=257
05/18/2022 00:29:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.84 on epoch=259
05/18/2022 00:29:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.84 on epoch=262
05/18/2022 00:29:19 - INFO - __main__ - Global step 1050 Train loss 0.84 Classification-F1 0.13067758749069247 on epoch=262
05/18/2022 00:29:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.85 on epoch=264
05/18/2022 00:29:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.80 on epoch=267
05/18/2022 00:29:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.82 on epoch=269
05/18/2022 00:29:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.79 on epoch=272
05/18/2022 00:29:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.83 on epoch=274
05/18/2022 00:29:25 - INFO - __main__ - Global step 1100 Train loss 0.82 Classification-F1 0.13067758749069247 on epoch=274
05/18/2022 00:29:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.89 on epoch=277
05/18/2022 00:29:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.83 on epoch=279
05/18/2022 00:29:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.90 on epoch=282
05/18/2022 00:29:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.82 on epoch=284
05/18/2022 00:29:32 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.82 on epoch=287
05/18/2022 00:29:32 - INFO - __main__ - Global step 1150 Train loss 0.85 Classification-F1 0.1238095238095238 on epoch=287
05/18/2022 00:29:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.79 on epoch=289
05/18/2022 00:29:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.80 on epoch=292
05/18/2022 00:29:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.77 on epoch=294
05/18/2022 00:29:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.86 on epoch=297
05/18/2022 00:29:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.87 on epoch=299
05/18/2022 00:29:39 - INFO - __main__ - Global step 1200 Train loss 0.82 Classification-F1 0.1486842105263158 on epoch=299
05/18/2022 00:29:39 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.1486842105263158 on epoch=299, global_step=1200
05/18/2022 00:29:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.81 on epoch=302
05/18/2022 00:29:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.80 on epoch=304
05/18/2022 00:29:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.87 on epoch=307
05/18/2022 00:29:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.87 on epoch=309
05/18/2022 00:29:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.80 on epoch=312
05/18/2022 00:29:45 - INFO - __main__ - Global step 1250 Train loss 0.83 Classification-F1 0.1 on epoch=312
05/18/2022 00:29:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.75 on epoch=314
05/18/2022 00:29:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.86 on epoch=317
05/18/2022 00:29:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.85 on epoch=319
05/18/2022 00:29:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.86 on epoch=322
05/18/2022 00:29:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.88 on epoch=324
05/18/2022 00:29:52 - INFO - __main__ - Global step 1300 Train loss 0.84 Classification-F1 0.16277641277641278 on epoch=324
05/18/2022 00:29:52 - INFO - __main__ - Saving model with best Classification-F1: 0.1486842105263158 -> 0.16277641277641278 on epoch=324, global_step=1300
05/18/2022 00:29:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.85 on epoch=327
05/18/2022 00:29:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.83 on epoch=329
05/18/2022 00:29:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.81 on epoch=332
05/18/2022 00:29:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.80 on epoch=334
05/18/2022 00:29:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.88 on epoch=337
05/18/2022 00:29:58 - INFO - __main__ - Global step 1350 Train loss 0.83 Classification-F1 0.1 on epoch=337
05/18/2022 00:29:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.78 on epoch=339
05/18/2022 00:30:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.87 on epoch=342
05/18/2022 00:30:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.93 on epoch=344
05/18/2022 00:30:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.87 on epoch=347
05/18/2022 00:30:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.82 on epoch=349
05/18/2022 00:30:05 - INFO - __main__ - Global step 1400 Train loss 0.85 Classification-F1 0.15809312638580933 on epoch=349
05/18/2022 00:30:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.83 on epoch=352
05/18/2022 00:30:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.84 on epoch=354
05/18/2022 00:30:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.83 on epoch=357
05/18/2022 00:30:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.89 on epoch=359
05/18/2022 00:30:11 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.81 on epoch=362
05/18/2022 00:30:11 - INFO - __main__ - Global step 1450 Train loss 0.84 Classification-F1 0.1 on epoch=362
05/18/2022 00:30:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.88 on epoch=364
05/18/2022 00:30:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.85 on epoch=367
05/18/2022 00:30:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.74 on epoch=369
05/18/2022 00:30:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.86 on epoch=372
05/18/2022 00:30:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.76 on epoch=374
05/18/2022 00:30:18 - INFO - __main__ - Global step 1500 Train loss 0.82 Classification-F1 0.12681436210847974 on epoch=374
05/18/2022 00:30:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.83 on epoch=377
05/18/2022 00:30:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.83 on epoch=379
05/18/2022 00:30:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.85 on epoch=382
05/18/2022 00:30:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.76 on epoch=384
05/18/2022 00:30:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.82 on epoch=387
05/18/2022 00:30:25 - INFO - __main__ - Global step 1550 Train loss 0.82 Classification-F1 0.12393162393162392 on epoch=387
05/18/2022 00:30:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.79 on epoch=389
05/18/2022 00:30:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.84 on epoch=392
05/18/2022 00:30:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.89 on epoch=394
05/18/2022 00:30:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.87 on epoch=397
05/18/2022 00:30:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.81 on epoch=399
05/18/2022 00:30:32 - INFO - __main__ - Global step 1600 Train loss 0.84 Classification-F1 0.09493670886075949 on epoch=399
05/18/2022 00:30:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.80 on epoch=402
05/18/2022 00:30:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.80 on epoch=404
05/18/2022 00:30:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.82 on epoch=407
05/18/2022 00:30:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.84 on epoch=409
05/18/2022 00:30:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.79 on epoch=412
05/18/2022 00:30:38 - INFO - __main__ - Global step 1650 Train loss 0.81 Classification-F1 0.11176836861768369 on epoch=412
05/18/2022 00:30:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.81 on epoch=414
05/18/2022 00:30:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.74 on epoch=417
05/18/2022 00:30:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.85 on epoch=419
05/18/2022 00:30:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.78 on epoch=422
05/18/2022 00:30:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.78 on epoch=424
05/18/2022 00:30:44 - INFO - __main__ - Global step 1700 Train loss 0.80 Classification-F1 0.13067758749069247 on epoch=424
05/18/2022 00:30:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.85 on epoch=427
05/18/2022 00:30:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.79 on epoch=429
05/18/2022 00:30:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.77 on epoch=432
05/18/2022 00:30:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.79 on epoch=434
05/18/2022 00:30:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.81 on epoch=437
05/18/2022 00:30:51 - INFO - __main__ - Global step 1750 Train loss 0.80 Classification-F1 0.1 on epoch=437
05/18/2022 00:30:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.77 on epoch=439
05/18/2022 00:30:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.83 on epoch=442
05/18/2022 00:30:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.84 on epoch=444
05/18/2022 00:30:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.81 on epoch=447
05/18/2022 00:30:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.75 on epoch=449
05/18/2022 00:30:57 - INFO - __main__ - Global step 1800 Train loss 0.80 Classification-F1 0.15459213988625753 on epoch=449
05/18/2022 00:30:58 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.81 on epoch=452
05/18/2022 00:30:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.77 on epoch=454
05/18/2022 00:31:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.83 on epoch=457
05/18/2022 00:31:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.75 on epoch=459
05/18/2022 00:31:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.89 on epoch=462
05/18/2022 00:31:03 - INFO - __main__ - Global step 1850 Train loss 0.81 Classification-F1 0.22972385301152423 on epoch=462
05/18/2022 00:31:03 - INFO - __main__ - Saving model with best Classification-F1: 0.16277641277641278 -> 0.22972385301152423 on epoch=462, global_step=1850
05/18/2022 00:31:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.76 on epoch=464
05/18/2022 00:31:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.85 on epoch=467
05/18/2022 00:31:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.83 on epoch=469
05/18/2022 00:31:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.87 on epoch=472
05/18/2022 00:31:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.83 on epoch=474
05/18/2022 00:31:10 - INFO - __main__ - Global step 1900 Train loss 0.83 Classification-F1 0.13067758749069247 on epoch=474
05/18/2022 00:31:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.79 on epoch=477
05/18/2022 00:31:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.88 on epoch=479
05/18/2022 00:31:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.86 on epoch=482
05/18/2022 00:31:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.83 on epoch=484
05/18/2022 00:31:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.85 on epoch=487
05/18/2022 00:31:16 - INFO - __main__ - Global step 1950 Train loss 0.84 Classification-F1 0.13067758749069247 on epoch=487
05/18/2022 00:31:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.77 on epoch=489
05/18/2022 00:31:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.85 on epoch=492
05/18/2022 00:31:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.83 on epoch=494
05/18/2022 00:31:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.74 on epoch=497
05/18/2022 00:31:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.80 on epoch=499
05/18/2022 00:31:22 - INFO - __main__ - Global step 2000 Train loss 0.80 Classification-F1 0.1715492957746479 on epoch=499
05/18/2022 00:31:24 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.88 on epoch=502
05/18/2022 00:31:25 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.80 on epoch=504
05/18/2022 00:31:26 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.75 on epoch=507
05/18/2022 00:31:27 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.86 on epoch=509
05/18/2022 00:31:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.77 on epoch=512
05/18/2022 00:31:29 - INFO - __main__ - Global step 2050 Train loss 0.81 Classification-F1 0.17320261437908496 on epoch=512
05/18/2022 00:31:30 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.80 on epoch=514
05/18/2022 00:31:31 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.84 on epoch=517
05/18/2022 00:31:32 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.80 on epoch=519
05/18/2022 00:31:34 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.75 on epoch=522
05/18/2022 00:31:35 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.82 on epoch=524
05/18/2022 00:31:35 - INFO - __main__ - Global step 2100 Train loss 0.80 Classification-F1 0.18130697094891468 on epoch=524
05/18/2022 00:31:36 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.84 on epoch=527
05/18/2022 00:31:38 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.81 on epoch=529
05/18/2022 00:31:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.79 on epoch=532
05/18/2022 00:31:40 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.77 on epoch=534
05/18/2022 00:31:41 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.84 on epoch=537
05/18/2022 00:31:42 - INFO - __main__ - Global step 2150 Train loss 0.81 Classification-F1 0.19923240614075088 on epoch=537
05/18/2022 00:31:43 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.80 on epoch=539
05/18/2022 00:31:44 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.85 on epoch=542
05/18/2022 00:31:45 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.82 on epoch=544
05/18/2022 00:31:47 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.83 on epoch=547
05/18/2022 00:31:48 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.76 on epoch=549
05/18/2022 00:31:48 - INFO - __main__ - Global step 2200 Train loss 0.81 Classification-F1 0.1437908496732026 on epoch=549
05/18/2022 00:31:50 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.75 on epoch=552
05/18/2022 00:31:51 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.78 on epoch=554
05/18/2022 00:31:52 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.76 on epoch=557
05/18/2022 00:31:53 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.80 on epoch=559
05/18/2022 00:31:54 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.80 on epoch=562
05/18/2022 00:31:55 - INFO - __main__ - Global step 2250 Train loss 0.78 Classification-F1 0.15560224089635855 on epoch=562
05/18/2022 00:31:56 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.80 on epoch=564
05/18/2022 00:31:57 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.79 on epoch=567
05/18/2022 00:31:58 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.87 on epoch=569
05/18/2022 00:32:00 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.81 on epoch=572
05/18/2022 00:32:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.78 on epoch=574
05/18/2022 00:32:01 - INFO - __main__ - Global step 2300 Train loss 0.81 Classification-F1 0.18721318314559865 on epoch=574
05/18/2022 00:32:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.84 on epoch=577
05/18/2022 00:32:04 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.81 on epoch=579
05/18/2022 00:32:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.82 on epoch=582
05/18/2022 00:32:06 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.82 on epoch=584
05/18/2022 00:32:07 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.78 on epoch=587
05/18/2022 00:32:08 - INFO - __main__ - Global step 2350 Train loss 0.82 Classification-F1 0.18888888888888888 on epoch=587
05/18/2022 00:32:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.77 on epoch=589
05/18/2022 00:32:10 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.82 on epoch=592
05/18/2022 00:32:11 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.80 on epoch=594
05/18/2022 00:32:13 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.75 on epoch=597
05/18/2022 00:32:14 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.79 on epoch=599
05/18/2022 00:32:14 - INFO - __main__ - Global step 2400 Train loss 0.79 Classification-F1 0.14553934781459754 on epoch=599
05/18/2022 00:32:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.77 on epoch=602
05/18/2022 00:32:17 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.78 on epoch=604
05/18/2022 00:32:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.79 on epoch=607
05/18/2022 00:32:19 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.73 on epoch=609
05/18/2022 00:32:20 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.83 on epoch=612
05/18/2022 00:32:21 - INFO - __main__ - Global step 2450 Train loss 0.78 Classification-F1 0.17547753564702717 on epoch=612
05/18/2022 00:32:22 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.77 on epoch=614
05/18/2022 00:32:23 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.76 on epoch=617
05/18/2022 00:32:24 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.81 on epoch=619
05/18/2022 00:32:26 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.79 on epoch=622
05/18/2022 00:32:27 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.80 on epoch=624
05/18/2022 00:32:27 - INFO - __main__ - Global step 2500 Train loss 0.78 Classification-F1 0.1090909090909091 on epoch=624
05/18/2022 00:32:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.73 on epoch=627
05/18/2022 00:32:30 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.81 on epoch=629
05/18/2022 00:32:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.72 on epoch=632
05/18/2022 00:32:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.83 on epoch=634
05/18/2022 00:32:33 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.75 on epoch=637
05/18/2022 00:32:34 - INFO - __main__ - Global step 2550 Train loss 0.77 Classification-F1 0.08675464320625612 on epoch=637
05/18/2022 00:32:35 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.80 on epoch=639
05/18/2022 00:32:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.78 on epoch=642
05/18/2022 00:32:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.84 on epoch=644
05/18/2022 00:32:39 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.83 on epoch=647
05/18/2022 00:32:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.76 on epoch=649
05/18/2022 00:32:40 - INFO - __main__ - Global step 2600 Train loss 0.80 Classification-F1 0.17369852369852368 on epoch=649
05/18/2022 00:32:41 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.84 on epoch=652
05/18/2022 00:32:43 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.78 on epoch=654
05/18/2022 00:32:44 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.84 on epoch=657
05/18/2022 00:32:45 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.74 on epoch=659
05/18/2022 00:32:46 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.76 on epoch=662
05/18/2022 00:32:47 - INFO - __main__ - Global step 2650 Train loss 0.79 Classification-F1 0.1313362812769629 on epoch=662
05/18/2022 00:32:48 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.77 on epoch=664
05/18/2022 00:32:49 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.79 on epoch=667
05/18/2022 00:32:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.81 on epoch=669
05/18/2022 00:32:51 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.83 on epoch=672
05/18/2022 00:32:53 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.79 on epoch=674
05/18/2022 00:32:53 - INFO - __main__ - Global step 2700 Train loss 0.80 Classification-F1 0.16123850670211587 on epoch=674
05/18/2022 00:32:55 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.78 on epoch=677
05/18/2022 00:32:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.72 on epoch=679
05/18/2022 00:32:57 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.73 on epoch=682
05/18/2022 00:32:58 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.79 on epoch=684
05/18/2022 00:32:59 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.83 on epoch=687
05/18/2022 00:33:00 - INFO - __main__ - Global step 2750 Train loss 0.77 Classification-F1 0.0831043956043956 on epoch=687
05/18/2022 00:33:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.79 on epoch=689
05/18/2022 00:33:02 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.80 on epoch=692
05/18/2022 00:33:04 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.80 on epoch=694
05/18/2022 00:33:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.75 on epoch=697
05/18/2022 00:33:06 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.83 on epoch=699
05/18/2022 00:33:07 - INFO - __main__ - Global step 2800 Train loss 0.79 Classification-F1 0.10199240986717267 on epoch=699
05/18/2022 00:33:08 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.80 on epoch=702
05/18/2022 00:33:09 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.74 on epoch=704
05/18/2022 00:33:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.73 on epoch=707
05/18/2022 00:33:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.78 on epoch=709
05/18/2022 00:33:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.76 on epoch=712
05/18/2022 00:33:13 - INFO - __main__ - Global step 2850 Train loss 0.76 Classification-F1 0.07950191570881227 on epoch=712
05/18/2022 00:33:14 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.81 on epoch=714
05/18/2022 00:33:15 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.82 on epoch=717
05/18/2022 00:33:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.80 on epoch=719
05/18/2022 00:33:18 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.77 on epoch=722
05/18/2022 00:33:19 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.75 on epoch=724
05/18/2022 00:33:20 - INFO - __main__ - Global step 2900 Train loss 0.79 Classification-F1 0.08232118758434548 on epoch=724
05/18/2022 00:33:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.76 on epoch=727
05/18/2022 00:33:22 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.78 on epoch=729
05/18/2022 00:33:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.79 on epoch=732
05/18/2022 00:33:24 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.77 on epoch=734
05/18/2022 00:33:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.74 on epoch=737
05/18/2022 00:33:26 - INFO - __main__ - Global step 2950 Train loss 0.77 Classification-F1 0.10667903525046382 on epoch=737
05/18/2022 00:33:27 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.76 on epoch=739
05/18/2022 00:33:28 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.79 on epoch=742
05/18/2022 00:33:30 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.83 on epoch=744
05/18/2022 00:33:31 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.74 on epoch=747
05/18/2022 00:33:32 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.76 on epoch=749
05/18/2022 00:33:33 - INFO - __main__ - Global step 3000 Train loss 0.77 Classification-F1 0.15606107984156767 on epoch=749
05/18/2022 00:33:33 - INFO - __main__ - save last model!
05/18/2022 00:33:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 00:33:33 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 00:33:33 - INFO - __main__ - Printing 3 examples
05/18/2022 00:33:33 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 00:33:33 - INFO - __main__ - ['others']
05/18/2022 00:33:33 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 00:33:33 - INFO - __main__ - ['others']
05/18/2022 00:33:33 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 00:33:33 - INFO - __main__ - ['others']
05/18/2022 00:33:33 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:33:33 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:33:33 - INFO - __main__ - Printing 3 examples
05/18/2022 00:33:33 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/18/2022 00:33:33 - INFO - __main__ - ['others']
05/18/2022 00:33:33 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/18/2022 00:33:33 - INFO - __main__ - ['others']
05/18/2022 00:33:33 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/18/2022 00:33:33 - INFO - __main__ - ['others']
05/18/2022 00:33:33 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:33:33 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:33:33 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 00:33:33 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:33:33 - INFO - __main__ - Printing 3 examples
05/18/2022 00:33:33 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/18/2022 00:33:33 - INFO - __main__ - ['others']
05/18/2022 00:33:33 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/18/2022 00:33:33 - INFO - __main__ - ['others']
05/18/2022 00:33:33 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/18/2022 00:33:33 - INFO - __main__ - ['others']
05/18/2022 00:33:33 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:33:33 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:33:33 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 00:33:35 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:33:39 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 00:33:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 00:33:39 - INFO - __main__ - Starting training!
05/18/2022 00:33:40 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 00:34:23 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_13_0.3_8_predictions.txt
05/18/2022 00:34:23 - INFO - __main__ - Classification-F1 on test data: 0.0631
05/18/2022 00:34:23 - INFO - __main__ - prefix=emo_16_13, lr=0.3, bsz=8, dev_performance=0.22972385301152423, test_performance=0.06308911141687151
05/18/2022 00:34:23 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.2, bsz=8 ...
05/18/2022 00:34:24 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:34:24 - INFO - __main__ - Printing 3 examples
05/18/2022 00:34:24 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/18/2022 00:34:24 - INFO - __main__ - ['others']
05/18/2022 00:34:24 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/18/2022 00:34:24 - INFO - __main__ - ['others']
05/18/2022 00:34:24 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/18/2022 00:34:24 - INFO - __main__ - ['others']
05/18/2022 00:34:24 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:34:24 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:34:24 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 00:34:24 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:34:24 - INFO - __main__ - Printing 3 examples
05/18/2022 00:34:24 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/18/2022 00:34:24 - INFO - __main__ - ['others']
05/18/2022 00:34:24 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/18/2022 00:34:24 - INFO - __main__ - ['others']
05/18/2022 00:34:24 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/18/2022 00:34:24 - INFO - __main__ - ['others']
05/18/2022 00:34:24 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:34:24 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:34:25 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 00:34:30 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 00:34:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 00:34:30 - INFO - __main__ - Starting training!
05/18/2022 00:34:32 - INFO - __main__ - Step 10 Global step 10 Train loss 4.46 on epoch=2
05/18/2022 00:34:33 - INFO - __main__ - Step 20 Global step 20 Train loss 4.11 on epoch=4
05/18/2022 00:34:34 - INFO - __main__ - Step 30 Global step 30 Train loss 3.94 on epoch=7
05/18/2022 00:34:35 - INFO - __main__ - Step 40 Global step 40 Train loss 3.52 on epoch=9
05/18/2022 00:34:36 - INFO - __main__ - Step 50 Global step 50 Train loss 3.44 on epoch=12
05/18/2022 00:34:37 - INFO - __main__ - Global step 50 Train loss 3.89 Classification-F1 0.0 on epoch=12
05/18/2022 00:34:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
05/18/2022 00:34:38 - INFO - __main__ - Step 60 Global step 60 Train loss 2.98 on epoch=14
05/18/2022 00:34:40 - INFO - __main__ - Step 70 Global step 70 Train loss 2.84 on epoch=17
05/18/2022 00:34:41 - INFO - __main__ - Step 80 Global step 80 Train loss 2.53 on epoch=19
05/18/2022 00:34:42 - INFO - __main__ - Step 90 Global step 90 Train loss 2.49 on epoch=22
05/18/2022 00:34:43 - INFO - __main__ - Step 100 Global step 100 Train loss 2.11 on epoch=24
05/18/2022 00:34:44 - INFO - __main__ - Global step 100 Train loss 2.59 Classification-F1 0.1 on epoch=24
05/18/2022 00:34:44 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.1 on epoch=24, global_step=100
05/18/2022 00:34:45 - INFO - __main__ - Step 110 Global step 110 Train loss 2.18 on epoch=27
05/18/2022 00:34:46 - INFO - __main__ - Step 120 Global step 120 Train loss 1.75 on epoch=29
05/18/2022 00:34:48 - INFO - __main__ - Step 130 Global step 130 Train loss 1.75 on epoch=32
05/18/2022 00:34:49 - INFO - __main__ - Step 140 Global step 140 Train loss 1.58 on epoch=34
05/18/2022 00:34:50 - INFO - __main__ - Step 150 Global step 150 Train loss 1.51 on epoch=37
05/18/2022 00:34:51 - INFO - __main__ - Global step 150 Train loss 1.75 Classification-F1 0.13034188034188032 on epoch=37
05/18/2022 00:34:51 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.13034188034188032 on epoch=37, global_step=150
05/18/2022 00:34:52 - INFO - __main__ - Step 160 Global step 160 Train loss 1.41 on epoch=39
05/18/2022 00:34:53 - INFO - __main__ - Step 170 Global step 170 Train loss 1.46 on epoch=42
05/18/2022 00:34:54 - INFO - __main__ - Step 180 Global step 180 Train loss 1.18 on epoch=44
05/18/2022 00:34:56 - INFO - __main__ - Step 190 Global step 190 Train loss 1.33 on epoch=47
05/18/2022 00:34:57 - INFO - __main__ - Step 200 Global step 200 Train loss 1.18 on epoch=49
05/18/2022 00:34:57 - INFO - __main__ - Global step 200 Train loss 1.31 Classification-F1 0.1237183868762816 on epoch=49
05/18/2022 00:34:59 - INFO - __main__ - Step 210 Global step 210 Train loss 1.24 on epoch=52
05/18/2022 00:35:00 - INFO - __main__ - Step 220 Global step 220 Train loss 1.15 on epoch=54
05/18/2022 00:35:01 - INFO - __main__ - Step 230 Global step 230 Train loss 1.11 on epoch=57
05/18/2022 00:35:02 - INFO - __main__ - Step 240 Global step 240 Train loss 1.13 on epoch=59
05/18/2022 00:35:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.97 on epoch=62
05/18/2022 00:35:04 - INFO - __main__ - Global step 250 Train loss 1.12 Classification-F1 0.1 on epoch=62
05/18/2022 00:35:05 - INFO - __main__ - Step 260 Global step 260 Train loss 1.21 on epoch=64
05/18/2022 00:35:06 - INFO - __main__ - Step 270 Global step 270 Train loss 1.20 on epoch=67
05/18/2022 00:35:08 - INFO - __main__ - Step 280 Global step 280 Train loss 1.06 on epoch=69
05/18/2022 00:35:09 - INFO - __main__ - Step 290 Global step 290 Train loss 1.13 on epoch=72
05/18/2022 00:35:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.94 on epoch=74
05/18/2022 00:35:11 - INFO - __main__ - Global step 300 Train loss 1.11 Classification-F1 0.1 on epoch=74
05/18/2022 00:35:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.99 on epoch=77
05/18/2022 00:35:13 - INFO - __main__ - Step 320 Global step 320 Train loss 1.01 on epoch=79
05/18/2022 00:35:14 - INFO - __main__ - Step 330 Global step 330 Train loss 1.04 on epoch=82
05/18/2022 00:35:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.95 on epoch=84
05/18/2022 00:35:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.98 on epoch=87
05/18/2022 00:35:17 - INFO - __main__ - Global step 350 Train loss 0.99 Classification-F1 0.1 on epoch=87
05/18/2022 00:35:18 - INFO - __main__ - Step 360 Global step 360 Train loss 1.06 on epoch=89
05/18/2022 00:35:20 - INFO - __main__ - Step 370 Global step 370 Train loss 1.03 on epoch=92
05/18/2022 00:35:21 - INFO - __main__ - Step 380 Global step 380 Train loss 1.01 on epoch=94
05/18/2022 00:35:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.96 on epoch=97
05/18/2022 00:35:23 - INFO - __main__ - Step 400 Global step 400 Train loss 1.07 on epoch=99
05/18/2022 00:35:24 - INFO - __main__ - Global step 400 Train loss 1.02 Classification-F1 0.1 on epoch=99
05/18/2022 00:35:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.95 on epoch=102
05/18/2022 00:35:26 - INFO - __main__ - Step 420 Global step 420 Train loss 1.03 on epoch=104
05/18/2022 00:35:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.96 on epoch=107
05/18/2022 00:35:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.99 on epoch=109
05/18/2022 00:35:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.86 on epoch=112
05/18/2022 00:35:31 - INFO - __main__ - Global step 450 Train loss 0.96 Classification-F1 0.1 on epoch=112
05/18/2022 00:35:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.92 on epoch=114
05/18/2022 00:35:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.85 on epoch=117
05/18/2022 00:35:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.89 on epoch=119
05/18/2022 00:35:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.96 on epoch=122
05/18/2022 00:35:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.94 on epoch=124
05/18/2022 00:35:37 - INFO - __main__ - Global step 500 Train loss 0.91 Classification-F1 0.1 on epoch=124
05/18/2022 00:35:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.98 on epoch=127
05/18/2022 00:35:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.94 on epoch=129
05/18/2022 00:35:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.89 on epoch=132
05/18/2022 00:35:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.93 on epoch=134
05/18/2022 00:35:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.95 on epoch=137
05/18/2022 00:35:44 - INFO - __main__ - Global step 550 Train loss 0.94 Classification-F1 0.09493670886075949 on epoch=137
05/18/2022 00:35:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.97 on epoch=139
05/18/2022 00:35:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.96 on epoch=142
05/18/2022 00:35:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.88 on epoch=144
05/18/2022 00:35:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.93 on epoch=147
05/18/2022 00:35:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.91 on epoch=149
05/18/2022 00:35:51 - INFO - __main__ - Global step 600 Train loss 0.93 Classification-F1 0.09493670886075949 on epoch=149
05/18/2022 00:35:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.92 on epoch=152
05/18/2022 00:35:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.89 on epoch=154
05/18/2022 00:35:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.91 on epoch=157
05/18/2022 00:35:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.89 on epoch=159
05/18/2022 00:35:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.84 on epoch=162
05/18/2022 00:35:58 - INFO - __main__ - Global step 650 Train loss 0.89 Classification-F1 0.09615384615384615 on epoch=162
05/18/2022 00:35:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.91 on epoch=164
05/18/2022 00:36:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.96 on epoch=167
05/18/2022 00:36:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.86 on epoch=169
05/18/2022 00:36:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.93 on epoch=172
05/18/2022 00:36:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.84 on epoch=174
05/18/2022 00:36:04 - INFO - __main__ - Global step 700 Train loss 0.90 Classification-F1 0.1 on epoch=174
05/18/2022 00:36:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.93 on epoch=177
05/18/2022 00:36:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.85 on epoch=179
05/18/2022 00:36:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.83 on epoch=182
05/18/2022 00:36:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.98 on epoch=184
05/18/2022 00:36:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.83 on epoch=187
05/18/2022 00:36:11 - INFO - __main__ - Global step 750 Train loss 0.88 Classification-F1 0.1 on epoch=187
05/18/2022 00:36:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.86 on epoch=189
05/18/2022 00:36:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.93 on epoch=192
05/18/2022 00:36:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.90 on epoch=194
05/18/2022 00:36:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.91 on epoch=197
05/18/2022 00:36:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.90 on epoch=199
05/18/2022 00:36:17 - INFO - __main__ - Global step 800 Train loss 0.90 Classification-F1 0.1 on epoch=199
05/18/2022 00:36:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.83 on epoch=202
05/18/2022 00:36:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.95 on epoch=204
05/18/2022 00:36:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.93 on epoch=207
05/18/2022 00:36:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.89 on epoch=209
05/18/2022 00:36:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.94 on epoch=212
05/18/2022 00:36:24 - INFO - __main__ - Global step 850 Train loss 0.91 Classification-F1 0.1 on epoch=212
05/18/2022 00:36:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.88 on epoch=214
05/18/2022 00:36:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.85 on epoch=217
05/18/2022 00:36:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.83 on epoch=219
05/18/2022 00:36:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.92 on epoch=222
05/18/2022 00:36:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.86 on epoch=224
05/18/2022 00:36:31 - INFO - __main__ - Global step 900 Train loss 0.87 Classification-F1 0.09615384615384615 on epoch=224
05/18/2022 00:36:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.95 on epoch=227
05/18/2022 00:36:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.93 on epoch=229
05/18/2022 00:36:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.89 on epoch=232
05/18/2022 00:36:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.84 on epoch=234
05/18/2022 00:36:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.90 on epoch=237
05/18/2022 00:36:37 - INFO - __main__ - Global step 950 Train loss 0.90 Classification-F1 0.1 on epoch=237
05/18/2022 00:36:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.86 on epoch=239
05/18/2022 00:36:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.86 on epoch=242
05/18/2022 00:36:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.86 on epoch=244
05/18/2022 00:36:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.94 on epoch=247
05/18/2022 00:36:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.77 on epoch=249
05/18/2022 00:36:44 - INFO - __main__ - Global step 1000 Train loss 0.86 Classification-F1 0.0974025974025974 on epoch=249
05/18/2022 00:36:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.90 on epoch=252
05/18/2022 00:36:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.81 on epoch=254
05/18/2022 00:36:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.81 on epoch=257
05/18/2022 00:36:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.88 on epoch=259
05/18/2022 00:36:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.80 on epoch=262
05/18/2022 00:36:51 - INFO - __main__ - Global step 1050 Train loss 0.84 Classification-F1 0.09210526315789473 on epoch=262
05/18/2022 00:36:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.91 on epoch=264
05/18/2022 00:36:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.90 on epoch=267
05/18/2022 00:36:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.90 on epoch=269
05/18/2022 00:36:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.87 on epoch=272
05/18/2022 00:36:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.82 on epoch=274
05/18/2022 00:36:57 - INFO - __main__ - Global step 1100 Train loss 0.88 Classification-F1 0.1 on epoch=274
05/18/2022 00:36:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.90 on epoch=277
05/18/2022 00:37:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.88 on epoch=279
05/18/2022 00:37:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.93 on epoch=282
05/18/2022 00:37:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.88 on epoch=284
05/18/2022 00:37:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.93 on epoch=287
05/18/2022 00:37:04 - INFO - __main__ - Global step 1150 Train loss 0.90 Classification-F1 0.1 on epoch=287
05/18/2022 00:37:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.85 on epoch=289
05/18/2022 00:37:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.81 on epoch=292
05/18/2022 00:37:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.88 on epoch=294
05/18/2022 00:37:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.83 on epoch=297
05/18/2022 00:37:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.87 on epoch=299
05/18/2022 00:37:11 - INFO - __main__ - Global step 1200 Train loss 0.85 Classification-F1 0.1 on epoch=299
05/18/2022 00:37:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.84 on epoch=302
05/18/2022 00:37:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.88 on epoch=304
05/18/2022 00:37:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.80 on epoch=307
05/18/2022 00:37:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.83 on epoch=309
05/18/2022 00:37:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.85 on epoch=312
05/18/2022 00:37:17 - INFO - __main__ - Global step 1250 Train loss 0.84 Classification-F1 0.1 on epoch=312
05/18/2022 00:37:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.86 on epoch=314
05/18/2022 00:37:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.84 on epoch=317
05/18/2022 00:37:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.88 on epoch=319
05/18/2022 00:37:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.93 on epoch=322
05/18/2022 00:37:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.86 on epoch=324
05/18/2022 00:37:24 - INFO - __main__ - Global step 1300 Train loss 0.88 Classification-F1 0.10126582278481013 on epoch=324
05/18/2022 00:37:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.83 on epoch=327
05/18/2022 00:37:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.89 on epoch=329
05/18/2022 00:37:28 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.86 on epoch=332
05/18/2022 00:37:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.80 on epoch=334
05/18/2022 00:37:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.83 on epoch=337
05/18/2022 00:37:31 - INFO - __main__ - Global step 1350 Train loss 0.84 Classification-F1 0.1 on epoch=337
05/18/2022 00:37:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.89 on epoch=339
05/18/2022 00:37:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.88 on epoch=342
05/18/2022 00:37:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.78 on epoch=344
05/18/2022 00:37:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.85 on epoch=347
05/18/2022 00:37:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.77 on epoch=349
05/18/2022 00:37:37 - INFO - __main__ - Global step 1400 Train loss 0.83 Classification-F1 0.1 on epoch=349
05/18/2022 00:37:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.90 on epoch=352
05/18/2022 00:37:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.78 on epoch=354
05/18/2022 00:37:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.87 on epoch=357
05/18/2022 00:37:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.91 on epoch=359
05/18/2022 00:37:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.82 on epoch=362
05/18/2022 00:37:44 - INFO - __main__ - Global step 1450 Train loss 0.86 Classification-F1 0.1 on epoch=362
05/18/2022 00:37:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.81 on epoch=364
05/18/2022 00:37:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.89 on epoch=367
05/18/2022 00:37:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.85 on epoch=369
05/18/2022 00:37:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.85 on epoch=372
05/18/2022 00:37:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.79 on epoch=374
05/18/2022 00:37:51 - INFO - __main__ - Global step 1500 Train loss 0.84 Classification-F1 0.09493670886075949 on epoch=374
05/18/2022 00:37:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.84 on epoch=377
05/18/2022 00:37:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.90 on epoch=379
05/18/2022 00:37:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.87 on epoch=382
05/18/2022 00:37:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.82 on epoch=384
05/18/2022 00:37:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.84 on epoch=387
05/18/2022 00:37:57 - INFO - __main__ - Global step 1550 Train loss 0.85 Classification-F1 0.1 on epoch=387
05/18/2022 00:37:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.74 on epoch=389
05/18/2022 00:38:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.82 on epoch=392
05/18/2022 00:38:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.80 on epoch=394
05/18/2022 00:38:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.85 on epoch=397
05/18/2022 00:38:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.85 on epoch=399
05/18/2022 00:38:04 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.1 on epoch=399
05/18/2022 00:38:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.86 on epoch=402
05/18/2022 00:38:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.81 on epoch=404
05/18/2022 00:38:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.85 on epoch=407
05/18/2022 00:38:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.76 on epoch=409
05/18/2022 00:38:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.86 on epoch=412
05/18/2022 00:38:11 - INFO - __main__ - Global step 1650 Train loss 0.83 Classification-F1 0.1 on epoch=412
05/18/2022 00:38:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.84 on epoch=414
05/18/2022 00:38:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.86 on epoch=417
05/18/2022 00:38:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.85 on epoch=419
05/18/2022 00:38:16 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.79 on epoch=422
05/18/2022 00:38:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.87 on epoch=424
05/18/2022 00:38:17 - INFO - __main__ - Global step 1700 Train loss 0.84 Classification-F1 0.1 on epoch=424
05/18/2022 00:38:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.83 on epoch=427
05/18/2022 00:38:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.85 on epoch=429
05/18/2022 00:38:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.71 on epoch=432
05/18/2022 00:38:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.82 on epoch=434
05/18/2022 00:38:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.84 on epoch=437
05/18/2022 00:38:24 - INFO - __main__ - Global step 1750 Train loss 0.81 Classification-F1 0.1 on epoch=437
05/18/2022 00:38:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.72 on epoch=439
05/18/2022 00:38:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.81 on epoch=442
05/18/2022 00:38:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.82 on epoch=444
05/18/2022 00:38:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.83 on epoch=447
05/18/2022 00:38:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.76 on epoch=449
05/18/2022 00:38:31 - INFO - __main__ - Global step 1800 Train loss 0.79 Classification-F1 0.14915966386554622 on epoch=449
05/18/2022 00:38:31 - INFO - __main__ - Saving model with best Classification-F1: 0.13034188034188032 -> 0.14915966386554622 on epoch=449, global_step=1800
05/18/2022 00:38:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.87 on epoch=452
05/18/2022 00:38:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.78 on epoch=454
05/18/2022 00:38:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.88 on epoch=457
05/18/2022 00:38:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.80 on epoch=459
05/18/2022 00:38:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.84 on epoch=462
05/18/2022 00:38:38 - INFO - __main__ - Global step 1850 Train loss 0.84 Classification-F1 0.1 on epoch=462
05/18/2022 00:38:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.81 on epoch=464
05/18/2022 00:38:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.82 on epoch=467
05/18/2022 00:38:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.84 on epoch=469
05/18/2022 00:38:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.89 on epoch=472
05/18/2022 00:38:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.93 on epoch=474
05/18/2022 00:38:44 - INFO - __main__ - Global step 1900 Train loss 0.86 Classification-F1 0.09333333333333334 on epoch=474
05/18/2022 00:38:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.79 on epoch=477
05/18/2022 00:38:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.83 on epoch=479
05/18/2022 00:38:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.80 on epoch=482
05/18/2022 00:38:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.82 on epoch=484
05/18/2022 00:38:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.78 on epoch=487
05/18/2022 00:38:51 - INFO - __main__ - Global step 1950 Train loss 0.80 Classification-F1 0.12518037518037517 on epoch=487
05/18/2022 00:38:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.81 on epoch=489
05/18/2022 00:38:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.85 on epoch=492
05/18/2022 00:38:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.85 on epoch=494
05/18/2022 00:38:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.76 on epoch=497
05/18/2022 00:38:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.81 on epoch=499
05/18/2022 00:38:58 - INFO - __main__ - Global step 2000 Train loss 0.82 Classification-F1 0.1 on epoch=499
05/18/2022 00:38:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.84 on epoch=502
05/18/2022 00:39:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.82 on epoch=504
05/18/2022 00:39:01 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.82 on epoch=507
05/18/2022 00:39:02 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.82 on epoch=509
05/18/2022 00:39:04 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.84 on epoch=512
05/18/2022 00:39:04 - INFO - __main__ - Global step 2050 Train loss 0.83 Classification-F1 0.12393162393162392 on epoch=512
05/18/2022 00:39:05 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.84 on epoch=514
05/18/2022 00:39:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.80 on epoch=517
05/18/2022 00:39:08 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.88 on epoch=519
05/18/2022 00:39:09 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.81 on epoch=522
05/18/2022 00:39:10 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.79 on epoch=524
05/18/2022 00:39:11 - INFO - __main__ - Global step 2100 Train loss 0.83 Classification-F1 0.1 on epoch=524
05/18/2022 00:39:12 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.89 on epoch=527
05/18/2022 00:39:13 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.78 on epoch=529
05/18/2022 00:39:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.85 on epoch=532
05/18/2022 00:39:15 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.82 on epoch=534
05/18/2022 00:39:17 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.83 on epoch=537
05/18/2022 00:39:17 - INFO - __main__ - Global step 2150 Train loss 0.83 Classification-F1 0.09493670886075949 on epoch=537
05/18/2022 00:39:18 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.79 on epoch=539
05/18/2022 00:39:20 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.78 on epoch=542
05/18/2022 00:39:21 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.82 on epoch=544
05/18/2022 00:39:22 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.81 on epoch=547
05/18/2022 00:39:23 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.85 on epoch=549
05/18/2022 00:39:24 - INFO - __main__ - Global step 2200 Train loss 0.81 Classification-F1 0.1 on epoch=549
05/18/2022 00:39:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.87 on epoch=552
05/18/2022 00:39:26 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.85 on epoch=554
05/18/2022 00:39:27 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.83 on epoch=557
05/18/2022 00:39:29 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.79 on epoch=559
05/18/2022 00:39:30 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.82 on epoch=562
05/18/2022 00:39:30 - INFO - __main__ - Global step 2250 Train loss 0.83 Classification-F1 0.1 on epoch=562
05/18/2022 00:39:32 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.85 on epoch=564
05/18/2022 00:39:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.78 on epoch=567
05/18/2022 00:39:34 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.82 on epoch=569
05/18/2022 00:39:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.75 on epoch=572
05/18/2022 00:39:37 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.79 on epoch=574
05/18/2022 00:39:37 - INFO - __main__ - Global step 2300 Train loss 0.80 Classification-F1 0.10126582278481013 on epoch=574
05/18/2022 00:39:38 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.78 on epoch=577
05/18/2022 00:39:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.77 on epoch=579
05/18/2022 00:39:41 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.83 on epoch=582
05/18/2022 00:39:42 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.89 on epoch=584
05/18/2022 00:39:43 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.83 on epoch=587
05/18/2022 00:39:44 - INFO - __main__ - Global step 2350 Train loss 0.82 Classification-F1 0.10126582278481013 on epoch=587
05/18/2022 00:39:45 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.72 on epoch=589
05/18/2022 00:39:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.89 on epoch=592
05/18/2022 00:39:47 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.81 on epoch=594
05/18/2022 00:39:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.80 on epoch=597
05/18/2022 00:39:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.83 on epoch=599
05/18/2022 00:39:50 - INFO - __main__ - Global step 2400 Train loss 0.81 Classification-F1 0.09493670886075949 on epoch=599
05/18/2022 00:39:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.79 on epoch=602
05/18/2022 00:39:53 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.79 on epoch=604
05/18/2022 00:39:54 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.81 on epoch=607
05/18/2022 00:39:55 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.81 on epoch=609
05/18/2022 00:39:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.90 on epoch=612
05/18/2022 00:39:57 - INFO - __main__ - Global step 2450 Train loss 0.82 Classification-F1 0.09210526315789473 on epoch=612
05/18/2022 00:39:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.80 on epoch=614
05/18/2022 00:39:59 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.78 on epoch=617
05/18/2022 00:40:00 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.72 on epoch=619
05/18/2022 00:40:02 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.83 on epoch=622
05/18/2022 00:40:03 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.79 on epoch=624
05/18/2022 00:40:03 - INFO - __main__ - Global step 2500 Train loss 0.78 Classification-F1 0.1586672879776328 on epoch=624
05/18/2022 00:40:03 - INFO - __main__ - Saving model with best Classification-F1: 0.14915966386554622 -> 0.1586672879776328 on epoch=624, global_step=2500
05/18/2022 00:40:05 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.81 on epoch=627
05/18/2022 00:40:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.80 on epoch=629
05/18/2022 00:40:07 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.85 on epoch=632
05/18/2022 00:40:08 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.81 on epoch=634
05/18/2022 00:40:10 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.81 on epoch=637
05/18/2022 00:40:10 - INFO - __main__ - Global step 2550 Train loss 0.82 Classification-F1 0.09090909090909091 on epoch=637
05/18/2022 00:40:11 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.79 on epoch=639
05/18/2022 00:40:13 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.76 on epoch=642
05/18/2022 00:40:14 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.79 on epoch=644
05/18/2022 00:40:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.74 on epoch=647
05/18/2022 00:40:16 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.78 on epoch=649
05/18/2022 00:40:17 - INFO - __main__ - Global step 2600 Train loss 0.77 Classification-F1 0.14766081871345027 on epoch=649
05/18/2022 00:40:18 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.82 on epoch=652
05/18/2022 00:40:19 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.79 on epoch=654
05/18/2022 00:40:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.80 on epoch=657
05/18/2022 00:40:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.83 on epoch=659
05/18/2022 00:40:23 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.87 on epoch=662
05/18/2022 00:40:23 - INFO - __main__ - Global step 2650 Train loss 0.82 Classification-F1 0.12151702786377708 on epoch=662
05/18/2022 00:40:25 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.86 on epoch=664
05/18/2022 00:40:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.85 on epoch=667
05/18/2022 00:40:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.78 on epoch=669
05/18/2022 00:40:28 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.82 on epoch=672
05/18/2022 00:40:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.76 on epoch=674
05/18/2022 00:40:30 - INFO - __main__ - Global step 2700 Train loss 0.81 Classification-F1 0.11166666666666666 on epoch=674
05/18/2022 00:40:31 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.77 on epoch=677
05/18/2022 00:40:32 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.78 on epoch=679
05/18/2022 00:40:33 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.90 on epoch=682
05/18/2022 00:40:35 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.83 on epoch=684
05/18/2022 00:40:36 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.83 on epoch=687
05/18/2022 00:40:36 - INFO - __main__ - Global step 2750 Train loss 0.82 Classification-F1 0.12518037518037517 on epoch=687
05/18/2022 00:40:38 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.81 on epoch=689
05/18/2022 00:40:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.89 on epoch=692
05/18/2022 00:40:40 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.76 on epoch=694
05/18/2022 00:40:41 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.81 on epoch=697
05/18/2022 00:40:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.75 on epoch=699
05/18/2022 00:40:43 - INFO - __main__ - Global step 2800 Train loss 0.80 Classification-F1 0.16666666666666666 on epoch=699
05/18/2022 00:40:43 - INFO - __main__ - Saving model with best Classification-F1: 0.1586672879776328 -> 0.16666666666666666 on epoch=699, global_step=2800
05/18/2022 00:40:44 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.82 on epoch=702
05/18/2022 00:40:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.79 on epoch=704
05/18/2022 00:40:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.82 on epoch=707
05/18/2022 00:40:48 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.84 on epoch=709
05/18/2022 00:40:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.78 on epoch=712
05/18/2022 00:40:50 - INFO - __main__ - Global step 2850 Train loss 0.81 Classification-F1 0.10996955859969558 on epoch=712
05/18/2022 00:40:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.85 on epoch=714
05/18/2022 00:40:52 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.80 on epoch=717
05/18/2022 00:40:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.81 on epoch=719
05/18/2022 00:40:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.76 on epoch=722
05/18/2022 00:40:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.75 on epoch=724
05/18/2022 00:40:56 - INFO - __main__ - Global step 2900 Train loss 0.79 Classification-F1 0.11913145539906103 on epoch=724
05/18/2022 00:40:57 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.82 on epoch=727
05/18/2022 00:40:59 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.86 on epoch=729
05/18/2022 00:41:00 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.77 on epoch=732
05/18/2022 00:41:01 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.76 on epoch=734
05/18/2022 00:41:02 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.80 on epoch=737
05/18/2022 00:41:03 - INFO - __main__ - Global step 2950 Train loss 0.80 Classification-F1 0.10126582278481013 on epoch=737
05/18/2022 00:41:04 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.73 on epoch=739
05/18/2022 00:41:05 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.80 on epoch=742
05/18/2022 00:41:07 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.79 on epoch=744
05/18/2022 00:41:08 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.75 on epoch=747
05/18/2022 00:41:09 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.80 on epoch=749
05/18/2022 00:41:09 - INFO - __main__ - Global step 3000 Train loss 0.77 Classification-F1 0.15409356725146198 on epoch=749
05/18/2022 00:41:09 - INFO - __main__ - save last model!
05/18/2022 00:41:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 00:41:09 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 00:41:09 - INFO - __main__ - Printing 3 examples
05/18/2022 00:41:09 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 00:41:09 - INFO - __main__ - ['others']
05/18/2022 00:41:09 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 00:41:09 - INFO - __main__ - ['others']
05/18/2022 00:41:09 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 00:41:09 - INFO - __main__ - ['others']
05/18/2022 00:41:09 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:41:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:41:10 - INFO - __main__ - Printing 3 examples
05/18/2022 00:41:10 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/18/2022 00:41:10 - INFO - __main__ - ['sad']
05/18/2022 00:41:10 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/18/2022 00:41:10 - INFO - __main__ - ['sad']
05/18/2022 00:41:10 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/18/2022 00:41:10 - INFO - __main__ - ['sad']
05/18/2022 00:41:10 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:41:10 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:41:10 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 00:41:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:41:10 - INFO - __main__ - Printing 3 examples
05/18/2022 00:41:10 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/18/2022 00:41:10 - INFO - __main__ - ['sad']
05/18/2022 00:41:10 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/18/2022 00:41:10 - INFO - __main__ - ['sad']
05/18/2022 00:41:10 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/18/2022 00:41:10 - INFO - __main__ - ['sad']
05/18/2022 00:41:10 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:41:10 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:41:10 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 00:41:12 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:41:16 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 00:41:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 00:41:17 - INFO - __main__ - Starting training!
05/18/2022 00:41:17 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 00:42:00 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_13_0.2_8_predictions.txt
05/18/2022 00:42:00 - INFO - __main__ - Classification-F1 on test data: 0.0646
05/18/2022 00:42:01 - INFO - __main__ - prefix=emo_16_13, lr=0.2, bsz=8, dev_performance=0.16666666666666666, test_performance=0.0645849793808082
05/18/2022 00:42:01 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.5, bsz=8 ...
05/18/2022 00:42:02 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:42:02 - INFO - __main__ - Printing 3 examples
05/18/2022 00:42:02 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/18/2022 00:42:02 - INFO - __main__ - ['sad']
05/18/2022 00:42:02 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/18/2022 00:42:02 - INFO - __main__ - ['sad']
05/18/2022 00:42:02 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/18/2022 00:42:02 - INFO - __main__ - ['sad']
05/18/2022 00:42:02 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:42:02 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:42:02 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 00:42:02 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:42:02 - INFO - __main__ - Printing 3 examples
05/18/2022 00:42:02 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/18/2022 00:42:02 - INFO - __main__ - ['sad']
05/18/2022 00:42:02 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/18/2022 00:42:02 - INFO - __main__ - ['sad']
05/18/2022 00:42:02 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/18/2022 00:42:02 - INFO - __main__ - ['sad']
05/18/2022 00:42:02 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:42:02 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:42:02 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 00:42:09 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 00:42:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 00:42:09 - INFO - __main__ - Starting training!
05/18/2022 00:42:10 - INFO - __main__ - Step 10 Global step 10 Train loss 4.43 on epoch=2
05/18/2022 00:42:12 - INFO - __main__ - Step 20 Global step 20 Train loss 3.79 on epoch=4
05/18/2022 00:42:13 - INFO - __main__ - Step 30 Global step 30 Train loss 3.22 on epoch=7
05/18/2022 00:42:14 - INFO - __main__ - Step 40 Global step 40 Train loss 2.74 on epoch=9
05/18/2022 00:42:15 - INFO - __main__ - Step 50 Global step 50 Train loss 2.22 on epoch=12
05/18/2022 00:42:16 - INFO - __main__ - Global step 50 Train loss 3.28 Classification-F1 0.1 on epoch=12
05/18/2022 00:42:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/18/2022 00:42:17 - INFO - __main__ - Step 60 Global step 60 Train loss 1.77 on epoch=14
05/18/2022 00:42:19 - INFO - __main__ - Step 70 Global step 70 Train loss 1.66 on epoch=17
05/18/2022 00:42:20 - INFO - __main__ - Step 80 Global step 80 Train loss 1.30 on epoch=19
05/18/2022 00:42:21 - INFO - __main__ - Step 90 Global step 90 Train loss 1.07 on epoch=22
05/18/2022 00:42:22 - INFO - __main__ - Step 100 Global step 100 Train loss 1.10 on epoch=24
05/18/2022 00:42:23 - INFO - __main__ - Global step 100 Train loss 1.38 Classification-F1 0.1 on epoch=24
05/18/2022 00:42:24 - INFO - __main__ - Step 110 Global step 110 Train loss 1.03 on epoch=27
05/18/2022 00:42:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.97 on epoch=29
05/18/2022 00:42:27 - INFO - __main__ - Step 130 Global step 130 Train loss 1.00 on epoch=32
05/18/2022 00:42:28 - INFO - __main__ - Step 140 Global step 140 Train loss 1.02 on epoch=34
05/18/2022 00:42:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.91 on epoch=37
05/18/2022 00:42:30 - INFO - __main__ - Global step 150 Train loss 0.99 Classification-F1 0.1 on epoch=37
05/18/2022 00:42:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.90 on epoch=39
05/18/2022 00:42:32 - INFO - __main__ - Step 170 Global step 170 Train loss 1.01 on epoch=42
05/18/2022 00:42:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.98 on epoch=44
05/18/2022 00:42:35 - INFO - __main__ - Step 190 Global step 190 Train loss 1.01 on epoch=47
05/18/2022 00:42:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.91 on epoch=49
05/18/2022 00:42:36 - INFO - __main__ - Global step 200 Train loss 0.96 Classification-F1 0.09493670886075949 on epoch=49
05/18/2022 00:42:38 - INFO - __main__ - Step 210 Global step 210 Train loss 1.00 on epoch=52
05/18/2022 00:42:39 - INFO - __main__ - Step 220 Global step 220 Train loss 1.02 on epoch=54
05/18/2022 00:42:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.93 on epoch=57
05/18/2022 00:42:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.93 on epoch=59
05/18/2022 00:42:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.85 on epoch=62
05/18/2022 00:42:43 - INFO - __main__ - Global step 250 Train loss 0.95 Classification-F1 0.0974025974025974 on epoch=62
05/18/2022 00:42:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.89 on epoch=64
05/18/2022 00:42:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.94 on epoch=67
05/18/2022 00:42:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.83 on epoch=69
05/18/2022 00:42:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.85 on epoch=72
05/18/2022 00:42:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.92 on epoch=74
05/18/2022 00:42:50 - INFO - __main__ - Global step 300 Train loss 0.89 Classification-F1 0.1 on epoch=74
05/18/2022 00:42:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.93 on epoch=77
05/18/2022 00:42:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.86 on epoch=79
05/18/2022 00:42:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.87 on epoch=82
05/18/2022 00:42:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.90 on epoch=84
05/18/2022 00:42:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.95 on epoch=87
05/18/2022 00:42:57 - INFO - __main__ - Global step 350 Train loss 0.90 Classification-F1 0.11762954139368673 on epoch=87
05/18/2022 00:42:57 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.11762954139368673 on epoch=87, global_step=350
05/18/2022 00:42:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.85 on epoch=89
05/18/2022 00:42:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.90 on epoch=92
05/18/2022 00:43:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.92 on epoch=94
05/18/2022 00:43:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=97
05/18/2022 00:43:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.89 on epoch=99
05/18/2022 00:43:04 - INFO - __main__ - Global step 400 Train loss 0.89 Classification-F1 0.1 on epoch=99
05/18/2022 00:43:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.83 on epoch=102
05/18/2022 00:43:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.85 on epoch=104
05/18/2022 00:43:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.84 on epoch=107
05/18/2022 00:43:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.79 on epoch=109
05/18/2022 00:43:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.79 on epoch=112
05/18/2022 00:43:10 - INFO - __main__ - Global step 450 Train loss 0.82 Classification-F1 0.08783783783783784 on epoch=112
05/18/2022 00:43:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.80 on epoch=114
05/18/2022 00:43:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.89 on epoch=117
05/18/2022 00:43:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.83 on epoch=119
05/18/2022 00:43:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.85 on epoch=122
05/18/2022 00:43:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.86 on epoch=124
05/18/2022 00:43:17 - INFO - __main__ - Global step 500 Train loss 0.85 Classification-F1 0.1 on epoch=124
05/18/2022 00:43:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.78 on epoch=127
05/18/2022 00:43:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.81 on epoch=129
05/18/2022 00:43:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.85 on epoch=132
05/18/2022 00:43:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.82 on epoch=134
05/18/2022 00:43:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.89 on epoch=137
05/18/2022 00:43:24 - INFO - __main__ - Global step 550 Train loss 0.83 Classification-F1 0.09090909090909091 on epoch=137
05/18/2022 00:43:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.89 on epoch=139
05/18/2022 00:43:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.84 on epoch=142
05/18/2022 00:43:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.87 on epoch=144
05/18/2022 00:43:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.85 on epoch=147
05/18/2022 00:43:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.82 on epoch=149
05/18/2022 00:43:31 - INFO - __main__ - Global step 600 Train loss 0.85 Classification-F1 0.1 on epoch=149
05/18/2022 00:43:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.86 on epoch=152
05/18/2022 00:43:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.79 on epoch=154
05/18/2022 00:43:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.82 on epoch=157
05/18/2022 00:43:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.78 on epoch=159
05/18/2022 00:43:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.85 on epoch=162
05/18/2022 00:43:37 - INFO - __main__ - Global step 650 Train loss 0.82 Classification-F1 0.09615384615384615 on epoch=162
05/18/2022 00:43:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.79 on epoch=164
05/18/2022 00:43:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.86 on epoch=167
05/18/2022 00:43:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.82 on epoch=169
05/18/2022 00:43:42 - INFO - __main__ - Step 690 Global step 690 Train loss 0.89 on epoch=172
05/18/2022 00:43:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.88 on epoch=174
05/18/2022 00:43:44 - INFO - __main__ - Global step 700 Train loss 0.85 Classification-F1 0.09493670886075949 on epoch=174
05/18/2022 00:43:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.88 on epoch=177
05/18/2022 00:43:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.80 on epoch=179
05/18/2022 00:43:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.88 on epoch=182
05/18/2022 00:43:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.75 on epoch=184
05/18/2022 00:43:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.80 on epoch=187
05/18/2022 00:43:51 - INFO - __main__ - Global step 750 Train loss 0.82 Classification-F1 0.1659804426145136 on epoch=187
05/18/2022 00:43:51 - INFO - __main__ - Saving model with best Classification-F1: 0.11762954139368673 -> 0.1659804426145136 on epoch=187, global_step=750
05/18/2022 00:43:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.83 on epoch=189
05/18/2022 00:43:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.75 on epoch=192
05/18/2022 00:43:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.76 on epoch=194
05/18/2022 00:43:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.83 on epoch=197
05/18/2022 00:43:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.77 on epoch=199
05/18/2022 00:43:58 - INFO - __main__ - Global step 800 Train loss 0.79 Classification-F1 0.2201454340075541 on epoch=199
05/18/2022 00:43:58 - INFO - __main__ - Saving model with best Classification-F1: 0.1659804426145136 -> 0.2201454340075541 on epoch=199, global_step=800
05/18/2022 00:43:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.76 on epoch=202
05/18/2022 00:44:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.77 on epoch=204
05/18/2022 00:44:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.83 on epoch=207
05/18/2022 00:44:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.80 on epoch=209
05/18/2022 00:44:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.78 on epoch=212
05/18/2022 00:44:04 - INFO - __main__ - Global step 850 Train loss 0.79 Classification-F1 0.23790322580645162 on epoch=212
05/18/2022 00:44:04 - INFO - __main__ - Saving model with best Classification-F1: 0.2201454340075541 -> 0.23790322580645162 on epoch=212, global_step=850
05/18/2022 00:44:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.82 on epoch=214
05/18/2022 00:44:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.84 on epoch=217
05/18/2022 00:44:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.75 on epoch=219
05/18/2022 00:44:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.76 on epoch=222
05/18/2022 00:44:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.82 on epoch=224
05/18/2022 00:44:11 - INFO - __main__ - Global step 900 Train loss 0.80 Classification-F1 0.4600563457706315 on epoch=224
05/18/2022 00:44:11 - INFO - __main__ - Saving model with best Classification-F1: 0.23790322580645162 -> 0.4600563457706315 on epoch=224, global_step=900
05/18/2022 00:44:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.72 on epoch=227
05/18/2022 00:44:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.72 on epoch=229
05/18/2022 00:44:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.74 on epoch=232
05/18/2022 00:44:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.79 on epoch=234
05/18/2022 00:44:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.75 on epoch=237
05/18/2022 00:44:18 - INFO - __main__ - Global step 950 Train loss 0.74 Classification-F1 0.4319101950680898 on epoch=237
05/18/2022 00:44:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.70 on epoch=239
05/18/2022 00:44:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.72 on epoch=242
05/18/2022 00:44:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.83 on epoch=244
05/18/2022 00:44:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.73 on epoch=247
05/18/2022 00:44:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.76 on epoch=249
05/18/2022 00:44:25 - INFO - __main__ - Global step 1000 Train loss 0.75 Classification-F1 0.5321621621621622 on epoch=249
05/18/2022 00:44:25 - INFO - __main__ - Saving model with best Classification-F1: 0.4600563457706315 -> 0.5321621621621622 on epoch=249, global_step=1000
05/18/2022 00:44:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.77 on epoch=252
05/18/2022 00:44:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.74 on epoch=254
05/18/2022 00:44:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.80 on epoch=257
05/18/2022 00:44:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.70 on epoch=259
05/18/2022 00:44:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.69 on epoch=262
05/18/2022 00:44:31 - INFO - __main__ - Global step 1050 Train loss 0.74 Classification-F1 0.517786561264822 on epoch=262
05/18/2022 00:44:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.68 on epoch=264
05/18/2022 00:44:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.73 on epoch=267
05/18/2022 00:44:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.70 on epoch=269
05/18/2022 00:44:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.70 on epoch=272
05/18/2022 00:44:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.68 on epoch=274
05/18/2022 00:44:38 - INFO - __main__ - Global step 1100 Train loss 0.70 Classification-F1 0.46862073883896727 on epoch=274
05/18/2022 00:44:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.69 on epoch=277
05/18/2022 00:44:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.70 on epoch=279
05/18/2022 00:44:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.64 on epoch=282
05/18/2022 00:44:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.71 on epoch=284
05/18/2022 00:44:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.72 on epoch=287
05/18/2022 00:44:45 - INFO - __main__ - Global step 1150 Train loss 0.69 Classification-F1 0.5271083505866114 on epoch=287
05/18/2022 00:44:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.64 on epoch=289
05/18/2022 00:44:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.72 on epoch=292
05/18/2022 00:44:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.71 on epoch=294
05/18/2022 00:44:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.71 on epoch=297
05/18/2022 00:44:51 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.64 on epoch=299
05/18/2022 00:44:51 - INFO - __main__ - Global step 1200 Train loss 0.68 Classification-F1 0.547620494988916 on epoch=299
05/18/2022 00:44:51 - INFO - __main__ - Saving model with best Classification-F1: 0.5321621621621622 -> 0.547620494988916 on epoch=299, global_step=1200
05/18/2022 00:44:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.62 on epoch=302
05/18/2022 00:44:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.67 on epoch=304
05/18/2022 00:44:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.75 on epoch=307
05/18/2022 00:44:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.64 on epoch=309
05/18/2022 00:44:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.65 on epoch=312
05/18/2022 00:44:58 - INFO - __main__ - Global step 1250 Train loss 0.67 Classification-F1 0.5177569254701742 on epoch=312
05/18/2022 00:44:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.62 on epoch=314
05/18/2022 00:45:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.67 on epoch=317
05/18/2022 00:45:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.70 on epoch=319
05/18/2022 00:45:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.58 on epoch=322
05/18/2022 00:45:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.66 on epoch=324
05/18/2022 00:45:05 - INFO - __main__ - Global step 1300 Train loss 0.65 Classification-F1 0.5617438867438866 on epoch=324
05/18/2022 00:45:05 - INFO - __main__ - Saving model with best Classification-F1: 0.547620494988916 -> 0.5617438867438866 on epoch=324, global_step=1300
05/18/2022 00:45:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.78 on epoch=327
05/18/2022 00:45:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.70 on epoch=329
05/18/2022 00:45:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.70 on epoch=332
05/18/2022 00:45:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.56 on epoch=334
05/18/2022 00:45:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.72 on epoch=337
05/18/2022 00:45:11 - INFO - __main__ - Global step 1350 Train loss 0.69 Classification-F1 0.5311258916522075 on epoch=337
05/18/2022 00:45:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.61 on epoch=339
05/18/2022 00:45:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.66 on epoch=342
05/18/2022 00:45:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.62 on epoch=344
05/18/2022 00:45:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.65 on epoch=347
05/18/2022 00:45:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.58 on epoch=349
05/18/2022 00:45:18 - INFO - __main__ - Global step 1400 Train loss 0.63 Classification-F1 0.5001569593354583 on epoch=349
05/18/2022 00:45:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.65 on epoch=352
05/18/2022 00:45:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.61 on epoch=354
05/18/2022 00:45:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.66 on epoch=357
05/18/2022 00:45:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.59 on epoch=359
05/18/2022 00:45:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.67 on epoch=362
05/18/2022 00:45:25 - INFO - __main__ - Global step 1450 Train loss 0.63 Classification-F1 0.6007635175282234 on epoch=362
05/18/2022 00:45:25 - INFO - __main__ - Saving model with best Classification-F1: 0.5617438867438866 -> 0.6007635175282234 on epoch=362, global_step=1450
05/18/2022 00:45:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.54 on epoch=364
05/18/2022 00:45:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.64 on epoch=367
05/18/2022 00:45:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.56 on epoch=369
05/18/2022 00:45:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.63 on epoch=372
05/18/2022 00:45:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.60 on epoch=374
05/18/2022 00:45:32 - INFO - __main__ - Global step 1500 Train loss 0.59 Classification-F1 0.4748803827751196 on epoch=374
05/18/2022 00:45:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.63 on epoch=377
05/18/2022 00:45:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.66 on epoch=379
05/18/2022 00:45:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.54 on epoch=382
05/18/2022 00:45:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.57 on epoch=384
05/18/2022 00:45:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.55 on epoch=387
05/18/2022 00:45:38 - INFO - __main__ - Global step 1550 Train loss 0.59 Classification-F1 0.5712074303405573 on epoch=387
05/18/2022 00:45:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.53 on epoch=389
05/18/2022 00:45:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.60 on epoch=392
05/18/2022 00:45:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.59 on epoch=394
05/18/2022 00:45:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.57 on epoch=397
05/18/2022 00:45:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.55 on epoch=399
05/18/2022 00:45:45 - INFO - __main__ - Global step 1600 Train loss 0.57 Classification-F1 0.46859349800526273 on epoch=399
05/18/2022 00:45:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.60 on epoch=402
05/18/2022 00:45:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.55 on epoch=404
05/18/2022 00:45:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.60 on epoch=407
05/18/2022 00:45:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.55 on epoch=409
05/18/2022 00:45:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.58 on epoch=412
05/18/2022 00:45:52 - INFO - __main__ - Global step 1650 Train loss 0.58 Classification-F1 0.5299242424242424 on epoch=412
05/18/2022 00:45:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.55 on epoch=414
05/18/2022 00:45:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.60 on epoch=417
05/18/2022 00:45:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.52 on epoch=419
05/18/2022 00:45:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.54 on epoch=422
05/18/2022 00:45:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.65 on epoch=424
05/18/2022 00:45:59 - INFO - __main__ - Global step 1700 Train loss 0.57 Classification-F1 0.472436974789916 on epoch=424
05/18/2022 00:46:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.47 on epoch=427
05/18/2022 00:46:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.58 on epoch=429
05/18/2022 00:46:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.62 on epoch=432
05/18/2022 00:46:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.52 on epoch=434
05/18/2022 00:46:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.60 on epoch=437
05/18/2022 00:46:05 - INFO - __main__ - Global step 1750 Train loss 0.56 Classification-F1 0.6154761904761905 on epoch=437
05/18/2022 00:46:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6007635175282234 -> 0.6154761904761905 on epoch=437, global_step=1750
05/18/2022 00:46:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.53 on epoch=439
05/18/2022 00:46:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.57 on epoch=442
05/18/2022 00:46:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.54 on epoch=444
05/18/2022 00:46:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.53 on epoch=447
05/18/2022 00:46:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.53 on epoch=449
05/18/2022 00:46:12 - INFO - __main__ - Global step 1800 Train loss 0.54 Classification-F1 0.4994926641985466 on epoch=449
05/18/2022 00:46:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.58 on epoch=452
05/18/2022 00:46:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.56 on epoch=454
05/18/2022 00:46:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.54 on epoch=457
05/18/2022 00:46:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.53 on epoch=459
05/18/2022 00:46:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.56 on epoch=462
05/18/2022 00:46:19 - INFO - __main__ - Global step 1850 Train loss 0.55 Classification-F1 0.7003708133971291 on epoch=462
05/18/2022 00:46:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6154761904761905 -> 0.7003708133971291 on epoch=462, global_step=1850
05/18/2022 00:46:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.54 on epoch=464
05/18/2022 00:46:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.58 on epoch=467
05/18/2022 00:46:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.51 on epoch=469
05/18/2022 00:46:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.52 on epoch=472
05/18/2022 00:46:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.49 on epoch=474
05/18/2022 00:46:26 - INFO - __main__ - Global step 1900 Train loss 0.53 Classification-F1 0.5125272331154684 on epoch=474
05/18/2022 00:46:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.53 on epoch=477
05/18/2022 00:46:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.53 on epoch=479
05/18/2022 00:46:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.54 on epoch=482
05/18/2022 00:46:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=484
05/18/2022 00:46:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.53 on epoch=487
05/18/2022 00:46:33 - INFO - __main__ - Global step 1950 Train loss 0.51 Classification-F1 0.686032800216861 on epoch=487
05/18/2022 00:46:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=489
05/18/2022 00:46:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.59 on epoch=492
05/18/2022 00:46:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=494
05/18/2022 00:46:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.44 on epoch=497
05/18/2022 00:46:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.52 on epoch=499
05/18/2022 00:46:39 - INFO - __main__ - Global step 2000 Train loss 0.49 Classification-F1 0.5123725348725349 on epoch=499
05/18/2022 00:46:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.54 on epoch=502
05/18/2022 00:46:42 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.44 on epoch=504
05/18/2022 00:46:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.49 on epoch=507
05/18/2022 00:46:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.42 on epoch=509
05/18/2022 00:46:46 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.49 on epoch=512
05/18/2022 00:46:46 - INFO - __main__ - Global step 2050 Train loss 0.48 Classification-F1 0.6726190476190477 on epoch=512
05/18/2022 00:46:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.47 on epoch=514
05/18/2022 00:46:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.55 on epoch=517
05/18/2022 00:46:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.47 on epoch=519
05/18/2022 00:46:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.37 on epoch=522
05/18/2022 00:46:52 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.47 on epoch=524
05/18/2022 00:46:53 - INFO - __main__ - Global step 2100 Train loss 0.47 Classification-F1 0.6713269178639198 on epoch=524
05/18/2022 00:46:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.53 on epoch=527
05/18/2022 00:46:55 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.45 on epoch=529
05/18/2022 00:46:57 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.57 on epoch=532
05/18/2022 00:46:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.36 on epoch=534
05/18/2022 00:46:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.48 on epoch=537
05/18/2022 00:47:00 - INFO - __main__ - Global step 2150 Train loss 0.48 Classification-F1 0.6880684420598051 on epoch=537
05/18/2022 00:47:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.43 on epoch=539
05/18/2022 00:47:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.41 on epoch=542
05/18/2022 00:47:04 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.47 on epoch=544
05/18/2022 00:47:05 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.52 on epoch=547
05/18/2022 00:47:06 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.42 on epoch=549
05/18/2022 00:47:07 - INFO - __main__ - Global step 2200 Train loss 0.45 Classification-F1 0.7352954913438785 on epoch=549
05/18/2022 00:47:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7003708133971291 -> 0.7352954913438785 on epoch=549, global_step=2200
05/18/2022 00:47:08 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.46 on epoch=552
05/18/2022 00:47:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.48 on epoch=554
05/18/2022 00:47:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.40 on epoch=557
05/18/2022 00:47:12 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.45 on epoch=559
05/18/2022 00:47:13 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.50 on epoch=562
05/18/2022 00:47:13 - INFO - __main__ - Global step 2250 Train loss 0.46 Classification-F1 0.7336124896608768 on epoch=562
05/18/2022 00:47:15 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.50 on epoch=564
05/18/2022 00:47:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.46 on epoch=567
05/18/2022 00:47:17 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.41 on epoch=569
05/18/2022 00:47:18 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.41 on epoch=572
05/18/2022 00:47:20 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.40 on epoch=574
05/18/2022 00:47:20 - INFO - __main__ - Global step 2300 Train loss 0.44 Classification-F1 0.7191295546558705 on epoch=574
05/18/2022 00:47:22 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.39 on epoch=577
05/18/2022 00:47:23 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.42 on epoch=579
05/18/2022 00:47:24 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.41 on epoch=582
05/18/2022 00:47:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.44 on epoch=584
05/18/2022 00:47:27 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.36 on epoch=587
05/18/2022 00:47:27 - INFO - __main__ - Global step 2350 Train loss 0.41 Classification-F1 0.6554388422035482 on epoch=587
05/18/2022 00:47:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.43 on epoch=589
05/18/2022 00:47:30 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.45 on epoch=592
05/18/2022 00:47:31 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.42 on epoch=594
05/18/2022 00:47:32 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.36 on epoch=597
05/18/2022 00:47:33 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.40 on epoch=599
05/18/2022 00:47:34 - INFO - __main__ - Global step 2400 Train loss 0.41 Classification-F1 0.7180707805707804 on epoch=599
05/18/2022 00:47:35 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.42 on epoch=602
05/18/2022 00:47:36 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.36 on epoch=604
05/18/2022 00:47:38 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.39 on epoch=607
05/18/2022 00:47:39 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.44 on epoch=609
05/18/2022 00:47:40 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.42 on epoch=612
05/18/2022 00:47:41 - INFO - __main__ - Global step 2450 Train loss 0.41 Classification-F1 0.7180707805707804 on epoch=612
05/18/2022 00:47:42 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.37 on epoch=614
05/18/2022 00:47:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.44 on epoch=617
05/18/2022 00:47:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.35 on epoch=619
05/18/2022 00:47:46 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.44 on epoch=622
05/18/2022 00:47:47 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.45 on epoch=624
05/18/2022 00:47:48 - INFO - __main__ - Global step 2500 Train loss 0.41 Classification-F1 0.6668859649122807 on epoch=624
05/18/2022 00:47:49 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.35 on epoch=627
05/18/2022 00:47:50 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.35 on epoch=629
05/18/2022 00:47:51 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.42 on epoch=632
05/18/2022 00:47:53 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.38 on epoch=634
05/18/2022 00:47:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.35 on epoch=637
05/18/2022 00:47:54 - INFO - __main__ - Global step 2550 Train loss 0.37 Classification-F1 0.6733343875184482 on epoch=637
05/18/2022 00:47:56 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.34 on epoch=639
05/18/2022 00:47:57 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.35 on epoch=642
05/18/2022 00:47:58 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.40 on epoch=644
05/18/2022 00:47:59 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.44 on epoch=647
05/18/2022 00:48:01 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.34 on epoch=649
05/18/2022 00:48:01 - INFO - __main__ - Global step 2600 Train loss 0.37 Classification-F1 0.6510100884294433 on epoch=649
05/18/2022 00:48:02 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.41 on epoch=652
05/18/2022 00:48:04 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.45 on epoch=654
05/18/2022 00:48:05 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.39 on epoch=657
05/18/2022 00:48:06 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.38 on epoch=659
05/18/2022 00:48:08 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.44 on epoch=662
05/18/2022 00:48:08 - INFO - __main__ - Global step 2650 Train loss 0.42 Classification-F1 0.7502253775073248 on epoch=662
05/18/2022 00:48:08 - INFO - __main__ - Saving model with best Classification-F1: 0.7352954913438785 -> 0.7502253775073248 on epoch=662, global_step=2650
05/18/2022 00:48:09 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.37 on epoch=664
05/18/2022 00:48:11 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.40 on epoch=667
05/18/2022 00:48:12 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.33 on epoch=669
05/18/2022 00:48:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.37 on epoch=672
05/18/2022 00:48:14 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.35 on epoch=674
05/18/2022 00:48:15 - INFO - __main__ - Global step 2700 Train loss 0.36 Classification-F1 0.645691975285243 on epoch=674
05/18/2022 00:48:16 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.43 on epoch=677
05/18/2022 00:48:17 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.26 on epoch=679
05/18/2022 00:48:19 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.34 on epoch=682
05/18/2022 00:48:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.28 on epoch=684
05/18/2022 00:48:21 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.31 on epoch=687
05/18/2022 00:48:22 - INFO - __main__ - Global step 2750 Train loss 0.32 Classification-F1 0.751259115066123 on epoch=687
05/18/2022 00:48:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7502253775073248 -> 0.751259115066123 on epoch=687, global_step=2750
05/18/2022 00:48:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.30 on epoch=689
05/18/2022 00:48:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.31 on epoch=692
05/18/2022 00:48:26 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.35 on epoch=694
05/18/2022 00:48:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.35 on epoch=697
05/18/2022 00:48:28 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.32 on epoch=699
05/18/2022 00:48:29 - INFO - __main__ - Global step 2800 Train loss 0.32 Classification-F1 0.698724716144071 on epoch=699
05/18/2022 00:48:30 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.29 on epoch=702
05/18/2022 00:48:31 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.31 on epoch=704
05/18/2022 00:48:32 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.37 on epoch=707
05/18/2022 00:48:34 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.30 on epoch=709
05/18/2022 00:48:35 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.33 on epoch=712
05/18/2022 00:48:35 - INFO - __main__ - Global step 2850 Train loss 0.32 Classification-F1 0.7172916666666667 on epoch=712
05/18/2022 00:48:37 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.31 on epoch=714
05/18/2022 00:48:38 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.43 on epoch=717
05/18/2022 00:48:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.34 on epoch=719
05/18/2022 00:48:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.29 on epoch=722
05/18/2022 00:48:42 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.27 on epoch=724
05/18/2022 00:48:42 - INFO - __main__ - Global step 2900 Train loss 0.33 Classification-F1 0.7168181818181818 on epoch=724
05/18/2022 00:48:44 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.36 on epoch=727
05/18/2022 00:48:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.35 on epoch=729
05/18/2022 00:48:46 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.33 on epoch=732
05/18/2022 00:48:47 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.29 on epoch=734
05/18/2022 00:48:49 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.30 on epoch=737
05/18/2022 00:48:49 - INFO - __main__ - Global step 2950 Train loss 0.33 Classification-F1 0.6860272913403844 on epoch=737
05/18/2022 00:48:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.42 on epoch=739
05/18/2022 00:48:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.27 on epoch=742
05/18/2022 00:48:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.27 on epoch=744
05/18/2022 00:48:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.37 on epoch=747
05/18/2022 00:48:55 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.29 on epoch=749
05/18/2022 00:48:56 - INFO - __main__ - Global step 3000 Train loss 0.32 Classification-F1 0.6882114789235533 on epoch=749
05/18/2022 00:48:56 - INFO - __main__ - save last model!
05/18/2022 00:48:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 00:48:56 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 00:48:56 - INFO - __main__ - Printing 3 examples
05/18/2022 00:48:56 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 00:48:56 - INFO - __main__ - ['others']
05/18/2022 00:48:56 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 00:48:56 - INFO - __main__ - ['others']
05/18/2022 00:48:56 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 00:48:56 - INFO - __main__ - ['others']
05/18/2022 00:48:56 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:48:57 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:48:57 - INFO - __main__ - Printing 3 examples
05/18/2022 00:48:57 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/18/2022 00:48:57 - INFO - __main__ - ['sad']
05/18/2022 00:48:57 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/18/2022 00:48:57 - INFO - __main__ - ['sad']
05/18/2022 00:48:57 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/18/2022 00:48:57 - INFO - __main__ - ['sad']
05/18/2022 00:48:57 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:48:57 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:48:57 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 00:48:57 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:48:57 - INFO - __main__ - Printing 3 examples
05/18/2022 00:48:57 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/18/2022 00:48:57 - INFO - __main__ - ['sad']
05/18/2022 00:48:57 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/18/2022 00:48:57 - INFO - __main__ - ['sad']
05/18/2022 00:48:57 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/18/2022 00:48:57 - INFO - __main__ - ['sad']
05/18/2022 00:48:57 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:48:57 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:48:57 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 00:48:58 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:49:03 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 00:49:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 00:49:03 - INFO - __main__ - Starting training!
05/18/2022 00:49:04 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 00:49:48 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_21_0.5_8_predictions.txt
05/18/2022 00:49:48 - INFO - __main__ - Classification-F1 on test data: 0.1296
05/18/2022 00:49:49 - INFO - __main__ - prefix=emo_16_21, lr=0.5, bsz=8, dev_performance=0.751259115066123, test_performance=0.1295880342062252
05/18/2022 00:49:49 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.4, bsz=8 ...
05/18/2022 00:49:50 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:49:50 - INFO - __main__ - Printing 3 examples
05/18/2022 00:49:50 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/18/2022 00:49:50 - INFO - __main__ - ['sad']
05/18/2022 00:49:50 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/18/2022 00:49:50 - INFO - __main__ - ['sad']
05/18/2022 00:49:50 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/18/2022 00:49:50 - INFO - __main__ - ['sad']
05/18/2022 00:49:50 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:49:50 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:49:50 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 00:49:50 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:49:50 - INFO - __main__ - Printing 3 examples
05/18/2022 00:49:50 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/18/2022 00:49:50 - INFO - __main__ - ['sad']
05/18/2022 00:49:50 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/18/2022 00:49:50 - INFO - __main__ - ['sad']
05/18/2022 00:49:50 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/18/2022 00:49:50 - INFO - __main__ - ['sad']
05/18/2022 00:49:50 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:49:50 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:49:50 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 00:49:56 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 00:49:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 00:49:56 - INFO - __main__ - Starting training!
05/18/2022 00:49:58 - INFO - __main__ - Step 10 Global step 10 Train loss 4.49 on epoch=2
05/18/2022 00:49:59 - INFO - __main__ - Step 20 Global step 20 Train loss 3.88 on epoch=4
05/18/2022 00:50:00 - INFO - __main__ - Step 30 Global step 30 Train loss 3.47 on epoch=7
05/18/2022 00:50:01 - INFO - __main__ - Step 40 Global step 40 Train loss 2.98 on epoch=9
05/18/2022 00:50:03 - INFO - __main__ - Step 50 Global step 50 Train loss 2.57 on epoch=12
05/18/2022 00:50:03 - INFO - __main__ - Global step 50 Train loss 3.48 Classification-F1 0.1 on epoch=12
05/18/2022 00:50:03 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/18/2022 00:50:05 - INFO - __main__ - Step 60 Global step 60 Train loss 2.19 on epoch=14
05/18/2022 00:50:06 - INFO - __main__ - Step 70 Global step 70 Train loss 1.94 on epoch=17
05/18/2022 00:50:07 - INFO - __main__ - Step 80 Global step 80 Train loss 1.69 on epoch=19
05/18/2022 00:50:09 - INFO - __main__ - Step 90 Global step 90 Train loss 1.53 on epoch=22
05/18/2022 00:50:10 - INFO - __main__ - Step 100 Global step 100 Train loss 1.30 on epoch=24
05/18/2022 00:50:10 - INFO - __main__ - Global step 100 Train loss 1.73 Classification-F1 0.14211309523809523 on epoch=24
05/18/2022 00:50:10 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.14211309523809523 on epoch=24, global_step=100
05/18/2022 00:50:12 - INFO - __main__ - Step 110 Global step 110 Train loss 1.43 on epoch=27
05/18/2022 00:50:13 - INFO - __main__ - Step 120 Global step 120 Train loss 1.19 on epoch=29
05/18/2022 00:50:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.99 on epoch=32
05/18/2022 00:50:15 - INFO - __main__ - Step 140 Global step 140 Train loss 1.09 on epoch=34
05/18/2022 00:50:17 - INFO - __main__ - Step 150 Global step 150 Train loss 1.15 on epoch=37
05/18/2022 00:50:17 - INFO - __main__ - Global step 150 Train loss 1.17 Classification-F1 0.14180672268907563 on epoch=37
05/18/2022 00:50:18 - INFO - __main__ - Step 160 Global step 160 Train loss 1.00 on epoch=39
05/18/2022 00:50:20 - INFO - __main__ - Step 170 Global step 170 Train loss 1.03 on epoch=42
05/18/2022 00:50:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.91 on epoch=44
05/18/2022 00:50:22 - INFO - __main__ - Step 190 Global step 190 Train loss 1.03 on epoch=47
05/18/2022 00:50:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.94 on epoch=49
05/18/2022 00:50:24 - INFO - __main__ - Global step 200 Train loss 0.98 Classification-F1 0.10126582278481013 on epoch=49
05/18/2022 00:50:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.96 on epoch=52
05/18/2022 00:50:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.87 on epoch=54
05/18/2022 00:50:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.94 on epoch=57
05/18/2022 00:50:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.96 on epoch=59
05/18/2022 00:50:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.80 on epoch=62
05/18/2022 00:50:30 - INFO - __main__ - Global step 250 Train loss 0.91 Classification-F1 0.19836065573770492 on epoch=62
05/18/2022 00:50:31 - INFO - __main__ - Saving model with best Classification-F1: 0.14211309523809523 -> 0.19836065573770492 on epoch=62, global_step=250
05/18/2022 00:50:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=64
05/18/2022 00:50:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.95 on epoch=67
05/18/2022 00:50:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.93 on epoch=69
05/18/2022 00:50:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.89 on epoch=72
05/18/2022 00:50:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.91 on epoch=74
05/18/2022 00:50:37 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.10126582278481013 on epoch=74
05/18/2022 00:50:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.92 on epoch=77
05/18/2022 00:50:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.93 on epoch=79
05/18/2022 00:50:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.94 on epoch=82
05/18/2022 00:50:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.96 on epoch=84
05/18/2022 00:50:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.96 on epoch=87
05/18/2022 00:50:44 - INFO - __main__ - Global step 350 Train loss 0.94 Classification-F1 0.14782608695652172 on epoch=87
05/18/2022 00:50:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.91 on epoch=89
05/18/2022 00:50:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.90 on epoch=92
05/18/2022 00:50:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.94 on epoch=94
05/18/2022 00:50:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=97
05/18/2022 00:50:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.91 on epoch=99
05/18/2022 00:50:50 - INFO - __main__ - Global step 400 Train loss 0.91 Classification-F1 0.10126582278481013 on epoch=99
05/18/2022 00:50:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.96 on epoch=102
05/18/2022 00:50:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.88 on epoch=104
05/18/2022 00:50:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.89 on epoch=107
05/18/2022 00:50:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.79 on epoch=109
05/18/2022 00:50:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.81 on epoch=112
05/18/2022 00:50:57 - INFO - __main__ - Global step 450 Train loss 0.87 Classification-F1 0.14095238095238094 on epoch=112
05/18/2022 00:50:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.89 on epoch=114
05/18/2022 00:51:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.96 on epoch=117
05/18/2022 00:51:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.87 on epoch=119
05/18/2022 00:51:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.89 on epoch=122
05/18/2022 00:51:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.90 on epoch=124
05/18/2022 00:51:04 - INFO - __main__ - Global step 500 Train loss 0.90 Classification-F1 0.10126582278481013 on epoch=124
05/18/2022 00:51:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.80 on epoch=127
05/18/2022 00:51:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.79 on epoch=129
05/18/2022 00:51:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.93 on epoch=132
05/18/2022 00:51:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.85 on epoch=134
05/18/2022 00:51:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.89 on epoch=137
05/18/2022 00:51:10 - INFO - __main__ - Global step 550 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=137
05/18/2022 00:51:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.74 on epoch=139
05/18/2022 00:51:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.84 on epoch=142
05/18/2022 00:51:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.87 on epoch=144
05/18/2022 00:51:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.92 on epoch=147
05/18/2022 00:51:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.84 on epoch=149
05/18/2022 00:51:17 - INFO - __main__ - Global step 600 Train loss 0.84 Classification-F1 0.13067758749069247 on epoch=149
05/18/2022 00:51:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.83 on epoch=152
05/18/2022 00:51:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.82 on epoch=154
05/18/2022 00:51:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.93 on epoch=157
05/18/2022 00:51:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.83 on epoch=159
05/18/2022 00:51:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.82 on epoch=162
05/18/2022 00:51:24 - INFO - __main__ - Global step 650 Train loss 0.85 Classification-F1 0.1332923832923833 on epoch=162
05/18/2022 00:51:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.84 on epoch=164
05/18/2022 00:51:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.76 on epoch=167
05/18/2022 00:51:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.82 on epoch=169
05/18/2022 00:51:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.86 on epoch=172
05/18/2022 00:51:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.84 on epoch=174
05/18/2022 00:51:30 - INFO - __main__ - Global step 700 Train loss 0.82 Classification-F1 0.1 on epoch=174
05/18/2022 00:51:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.86 on epoch=177
05/18/2022 00:51:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.92 on epoch=179
05/18/2022 00:51:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.82 on epoch=182
05/18/2022 00:51:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.86 on epoch=184
05/18/2022 00:51:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.85 on epoch=187
05/18/2022 00:51:37 - INFO - __main__ - Global step 750 Train loss 0.86 Classification-F1 0.1 on epoch=187
05/18/2022 00:51:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.82 on epoch=189
05/18/2022 00:51:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.89 on epoch=192
05/18/2022 00:51:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.79 on epoch=194
05/18/2022 00:51:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.79 on epoch=197
05/18/2022 00:51:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.73 on epoch=199
05/18/2022 00:51:44 - INFO - __main__ - Global step 800 Train loss 0.80 Classification-F1 0.13067758749069247 on epoch=199
05/18/2022 00:51:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.77 on epoch=202
05/18/2022 00:51:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.81 on epoch=204
05/18/2022 00:51:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=207
05/18/2022 00:51:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.81 on epoch=209
05/18/2022 00:51:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.84 on epoch=212
05/18/2022 00:51:50 - INFO - __main__ - Global step 850 Train loss 0.81 Classification-F1 0.14046941678520625 on epoch=212
05/18/2022 00:51:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.77 on epoch=214
05/18/2022 00:51:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.82 on epoch=217
05/18/2022 00:51:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.85 on epoch=219
05/18/2022 00:51:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.80 on epoch=222
05/18/2022 00:51:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.92 on epoch=224
05/18/2022 00:51:57 - INFO - __main__ - Global step 900 Train loss 0.83 Classification-F1 0.13067758749069247 on epoch=224
05/18/2022 00:51:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.85 on epoch=227
05/18/2022 00:51:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.84 on epoch=229
05/18/2022 00:52:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.89 on epoch=232
05/18/2022 00:52:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.89 on epoch=234
05/18/2022 00:52:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.81 on epoch=237
05/18/2022 00:52:04 - INFO - __main__ - Global step 950 Train loss 0.86 Classification-F1 0.13067758749069247 on epoch=237
05/18/2022 00:52:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.81 on epoch=239
05/18/2022 00:52:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.87 on epoch=242
05/18/2022 00:52:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.83 on epoch=244
05/18/2022 00:52:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.80 on epoch=247
05/18/2022 00:52:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.84 on epoch=249
05/18/2022 00:52:10 - INFO - __main__ - Global step 1000 Train loss 0.83 Classification-F1 0.1 on epoch=249
05/18/2022 00:52:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.79 on epoch=252
05/18/2022 00:52:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.78 on epoch=254
05/18/2022 00:52:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.80 on epoch=257
05/18/2022 00:52:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.79 on epoch=259
05/18/2022 00:52:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.86 on epoch=262
05/18/2022 00:52:17 - INFO - __main__ - Global step 1050 Train loss 0.80 Classification-F1 0.13067758749069247 on epoch=262
05/18/2022 00:52:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.79 on epoch=264
05/18/2022 00:52:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.87 on epoch=267
05/18/2022 00:52:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.77 on epoch=269
05/18/2022 00:52:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.85 on epoch=272
05/18/2022 00:52:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.80 on epoch=274
05/18/2022 00:52:24 - INFO - __main__ - Global step 1100 Train loss 0.82 Classification-F1 0.12393162393162392 on epoch=274
05/18/2022 00:52:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.84 on epoch=277
05/18/2022 00:52:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.85 on epoch=279
05/18/2022 00:52:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.79 on epoch=282
05/18/2022 00:52:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.81 on epoch=284
05/18/2022 00:52:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.87 on epoch=287
05/18/2022 00:52:30 - INFO - __main__ - Global step 1150 Train loss 0.83 Classification-F1 0.13067758749069247 on epoch=287
05/18/2022 00:52:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.81 on epoch=289
05/18/2022 00:52:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.84 on epoch=292
05/18/2022 00:52:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.78 on epoch=294
05/18/2022 00:52:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.78 on epoch=297
05/18/2022 00:52:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.78 on epoch=299
05/18/2022 00:52:37 - INFO - __main__ - Global step 1200 Train loss 0.80 Classification-F1 0.13067758749069247 on epoch=299
05/18/2022 00:52:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.74 on epoch=302
05/18/2022 00:52:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.81 on epoch=304
05/18/2022 00:52:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.77 on epoch=307
05/18/2022 00:52:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.81 on epoch=309
05/18/2022 00:52:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.88 on epoch=312
05/18/2022 00:52:44 - INFO - __main__ - Global step 1250 Train loss 0.80 Classification-F1 0.12393162393162392 on epoch=312
05/18/2022 00:52:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.82 on epoch=314
05/18/2022 00:52:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.84 on epoch=317
05/18/2022 00:52:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.86 on epoch=319
05/18/2022 00:52:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.84 on epoch=322
05/18/2022 00:52:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.80 on epoch=324
05/18/2022 00:52:50 - INFO - __main__ - Global step 1300 Train loss 0.83 Classification-F1 0.12393162393162392 on epoch=324
05/18/2022 00:52:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.83 on epoch=327
05/18/2022 00:52:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.75 on epoch=329
05/18/2022 00:52:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.81 on epoch=332
05/18/2022 00:52:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.78 on epoch=334
05/18/2022 00:52:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.83 on epoch=337
05/18/2022 00:52:57 - INFO - __main__ - Global step 1350 Train loss 0.80 Classification-F1 0.22588872947553001 on epoch=337
05/18/2022 00:52:57 - INFO - __main__ - Saving model with best Classification-F1: 0.19836065573770492 -> 0.22588872947553001 on epoch=337, global_step=1350
05/18/2022 00:52:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.74 on epoch=339
05/18/2022 00:52:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.78 on epoch=342
05/18/2022 00:53:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.72 on epoch=344
05/18/2022 00:53:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.77 on epoch=347
05/18/2022 00:53:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.79 on epoch=349
05/18/2022 00:53:03 - INFO - __main__ - Global step 1400 Train loss 0.76 Classification-F1 0.2604150413481042 on epoch=349
05/18/2022 00:53:04 - INFO - __main__ - Saving model with best Classification-F1: 0.22588872947553001 -> 0.2604150413481042 on epoch=349, global_step=1400
05/18/2022 00:53:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.83 on epoch=352
05/18/2022 00:53:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.78 on epoch=354
05/18/2022 00:53:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.79 on epoch=357
05/18/2022 00:53:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.77 on epoch=359
05/18/2022 00:53:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.76 on epoch=362
05/18/2022 00:53:10 - INFO - __main__ - Global step 1450 Train loss 0.79 Classification-F1 0.2668669618384989 on epoch=362
05/18/2022 00:53:10 - INFO - __main__ - Saving model with best Classification-F1: 0.2604150413481042 -> 0.2668669618384989 on epoch=362, global_step=1450
05/18/2022 00:53:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.80 on epoch=364
05/18/2022 00:53:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.87 on epoch=367
05/18/2022 00:53:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.75 on epoch=369
05/18/2022 00:53:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.78 on epoch=372
05/18/2022 00:53:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.78 on epoch=374
05/18/2022 00:53:17 - INFO - __main__ - Global step 1500 Train loss 0.80 Classification-F1 0.2879319510898458 on epoch=374
05/18/2022 00:53:17 - INFO - __main__ - Saving model with best Classification-F1: 0.2668669618384989 -> 0.2879319510898458 on epoch=374, global_step=1500
05/18/2022 00:53:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.77 on epoch=377
05/18/2022 00:53:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.76 on epoch=379
05/18/2022 00:53:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.76 on epoch=382
05/18/2022 00:53:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.74 on epoch=384
05/18/2022 00:53:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.80 on epoch=387
05/18/2022 00:53:23 - INFO - __main__ - Global step 1550 Train loss 0.76 Classification-F1 0.3248538011695906 on epoch=387
05/18/2022 00:53:23 - INFO - __main__ - Saving model with best Classification-F1: 0.2879319510898458 -> 0.3248538011695906 on epoch=387, global_step=1550
05/18/2022 00:53:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.80 on epoch=389
05/18/2022 00:53:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.79 on epoch=392
05/18/2022 00:53:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.74 on epoch=394
05/18/2022 00:53:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.75 on epoch=397
05/18/2022 00:53:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.76 on epoch=399
05/18/2022 00:53:30 - INFO - __main__ - Global step 1600 Train loss 0.77 Classification-F1 0.3490767490009148 on epoch=399
05/18/2022 00:53:30 - INFO - __main__ - Saving model with best Classification-F1: 0.3248538011695906 -> 0.3490767490009148 on epoch=399, global_step=1600
05/18/2022 00:53:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.82 on epoch=402
05/18/2022 00:53:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.71 on epoch=404
05/18/2022 00:53:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.74 on epoch=407
05/18/2022 00:53:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.73 on epoch=409
05/18/2022 00:53:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.80 on epoch=412
05/18/2022 00:53:37 - INFO - __main__ - Global step 1650 Train loss 0.76 Classification-F1 0.33942414174972313 on epoch=412
05/18/2022 00:53:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.76 on epoch=414
05/18/2022 00:53:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.68 on epoch=417
05/18/2022 00:53:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.69 on epoch=419
05/18/2022 00:53:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.74 on epoch=422
05/18/2022 00:53:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.77 on epoch=424
05/18/2022 00:53:43 - INFO - __main__ - Global step 1700 Train loss 0.73 Classification-F1 0.46497584541062803 on epoch=424
05/18/2022 00:53:43 - INFO - __main__ - Saving model with best Classification-F1: 0.3490767490009148 -> 0.46497584541062803 on epoch=424, global_step=1700
05/18/2022 00:53:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.77 on epoch=427
05/18/2022 00:53:46 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.69 on epoch=429
05/18/2022 00:53:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.78 on epoch=432
05/18/2022 00:53:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.74 on epoch=434
05/18/2022 00:53:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.70 on epoch=437
05/18/2022 00:53:50 - INFO - __main__ - Global step 1750 Train loss 0.74 Classification-F1 0.4398809523809524 on epoch=437
05/18/2022 00:53:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.75 on epoch=439
05/18/2022 00:53:52 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.66 on epoch=442
05/18/2022 00:53:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.66 on epoch=444
05/18/2022 00:53:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.65 on epoch=447
05/18/2022 00:53:56 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.72 on epoch=449
05/18/2022 00:53:57 - INFO - __main__ - Global step 1800 Train loss 0.69 Classification-F1 0.4546267011692543 on epoch=449
05/18/2022 00:53:58 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.70 on epoch=452
05/18/2022 00:53:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.63 on epoch=454
05/18/2022 00:54:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.77 on epoch=457
05/18/2022 00:54:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.73 on epoch=459
05/18/2022 00:54:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.74 on epoch=462
05/18/2022 00:54:03 - INFO - __main__ - Global step 1850 Train loss 0.71 Classification-F1 0.4301932367149759 on epoch=462
05/18/2022 00:54:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.62 on epoch=464
05/18/2022 00:54:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.76 on epoch=467
05/18/2022 00:54:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.70 on epoch=469
05/18/2022 00:54:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.66 on epoch=472
05/18/2022 00:54:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.67 on epoch=474
05/18/2022 00:54:10 - INFO - __main__ - Global step 1900 Train loss 0.68 Classification-F1 0.4301725162709144 on epoch=474
05/18/2022 00:54:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.68 on epoch=477
05/18/2022 00:54:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.70 on epoch=479
05/18/2022 00:54:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.60 on epoch=482
05/18/2022 00:54:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.72 on epoch=484
05/18/2022 00:54:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.70 on epoch=487
05/18/2022 00:54:16 - INFO - __main__ - Global step 1950 Train loss 0.68 Classification-F1 0.4181016644174539 on epoch=487
05/18/2022 00:54:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.72 on epoch=489
05/18/2022 00:54:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.67 on epoch=492
05/18/2022 00:54:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.68 on epoch=494
05/18/2022 00:54:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.68 on epoch=497
05/18/2022 00:54:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.68 on epoch=499
05/18/2022 00:54:23 - INFO - __main__ - Global step 2000 Train loss 0.69 Classification-F1 0.4770188375078366 on epoch=499
05/18/2022 00:54:23 - INFO - __main__ - Saving model with best Classification-F1: 0.46497584541062803 -> 0.4770188375078366 on epoch=499, global_step=2000
05/18/2022 00:54:24 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.66 on epoch=502
05/18/2022 00:54:25 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.66 on epoch=504
05/18/2022 00:54:27 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.72 on epoch=507
05/18/2022 00:54:28 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.66 on epoch=509
05/18/2022 00:54:29 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.67 on epoch=512
05/18/2022 00:54:30 - INFO - __main__ - Global step 2050 Train loss 0.67 Classification-F1 0.5249716949716949 on epoch=512
05/18/2022 00:54:30 - INFO - __main__ - Saving model with best Classification-F1: 0.4770188375078366 -> 0.5249716949716949 on epoch=512, global_step=2050
05/18/2022 00:54:31 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.65 on epoch=514
05/18/2022 00:54:32 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.61 on epoch=517
05/18/2022 00:54:33 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.64 on epoch=519
05/18/2022 00:54:35 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.65 on epoch=522
05/18/2022 00:54:36 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.63 on epoch=524
05/18/2022 00:54:36 - INFO - __main__ - Global step 2100 Train loss 0.64 Classification-F1 0.39282051282051283 on epoch=524
05/18/2022 00:54:38 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.75 on epoch=527
05/18/2022 00:54:39 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.64 on epoch=529
05/18/2022 00:54:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.66 on epoch=532
05/18/2022 00:54:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.59 on epoch=534
05/18/2022 00:54:42 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.53 on epoch=537
05/18/2022 00:54:43 - INFO - __main__ - Global step 2150 Train loss 0.63 Classification-F1 0.39959950338419636 on epoch=537
05/18/2022 00:54:44 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.58 on epoch=539
05/18/2022 00:54:45 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.67 on epoch=542
05/18/2022 00:54:47 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.56 on epoch=544
05/18/2022 00:54:48 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.64 on epoch=547
05/18/2022 00:54:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.56 on epoch=549
05/18/2022 00:54:50 - INFO - __main__ - Global step 2200 Train loss 0.60 Classification-F1 0.5238738738738739 on epoch=549
05/18/2022 00:54:51 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.54 on epoch=552
05/18/2022 00:54:52 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.66 on epoch=554
05/18/2022 00:54:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.56 on epoch=557
05/18/2022 00:54:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.61 on epoch=559
05/18/2022 00:54:56 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.60 on epoch=562
05/18/2022 00:54:56 - INFO - __main__ - Global step 2250 Train loss 0.60 Classification-F1 0.5003417634996582 on epoch=562
05/18/2022 00:54:57 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.57 on epoch=564
05/18/2022 00:54:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.59 on epoch=567
05/18/2022 00:55:00 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.58 on epoch=569
05/18/2022 00:55:01 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.60 on epoch=572
05/18/2022 00:55:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.55 on epoch=574
05/18/2022 00:55:03 - INFO - __main__ - Global step 2300 Train loss 0.58 Classification-F1 0.43747644846043005 on epoch=574
05/18/2022 00:55:04 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.63 on epoch=577
05/18/2022 00:55:05 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.55 on epoch=579
05/18/2022 00:55:06 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.68 on epoch=582
05/18/2022 00:55:08 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.52 on epoch=584
05/18/2022 00:55:09 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.68 on epoch=587
05/18/2022 00:55:09 - INFO - __main__ - Global step 2350 Train loss 0.61 Classification-F1 0.5706043956043956 on epoch=587
05/18/2022 00:55:09 - INFO - __main__ - Saving model with best Classification-F1: 0.5249716949716949 -> 0.5706043956043956 on epoch=587, global_step=2350
05/18/2022 00:55:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.53 on epoch=589
05/18/2022 00:55:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.74 on epoch=592
05/18/2022 00:55:13 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.62 on epoch=594
05/18/2022 00:55:14 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.63 on epoch=597
05/18/2022 00:55:16 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.61 on epoch=599
05/18/2022 00:55:16 - INFO - __main__ - Global step 2400 Train loss 0.63 Classification-F1 0.5357051282051282 on epoch=599
05/18/2022 00:55:17 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.66 on epoch=602
05/18/2022 00:55:19 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.59 on epoch=604
05/18/2022 00:55:20 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.64 on epoch=607
05/18/2022 00:55:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.56 on epoch=609
05/18/2022 00:55:22 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.56 on epoch=612
05/18/2022 00:55:23 - INFO - __main__ - Global step 2450 Train loss 0.60 Classification-F1 0.45407407407407413 on epoch=612
05/18/2022 00:55:24 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.61 on epoch=614
05/18/2022 00:55:25 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.54 on epoch=617
05/18/2022 00:55:26 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.55 on epoch=619
05/18/2022 00:55:28 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.63 on epoch=622
05/18/2022 00:55:29 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.58 on epoch=624
05/18/2022 00:55:29 - INFO - __main__ - Global step 2500 Train loss 0.58 Classification-F1 0.4551766513056835 on epoch=624
05/18/2022 00:55:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.62 on epoch=627
05/18/2022 00:55:32 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.58 on epoch=629
05/18/2022 00:55:33 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.62 on epoch=632
05/18/2022 00:55:34 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.55 on epoch=634
05/18/2022 00:55:35 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.59 on epoch=637
05/18/2022 00:55:36 - INFO - __main__ - Global step 2550 Train loss 0.59 Classification-F1 0.4304080923028291 on epoch=637
05/18/2022 00:55:37 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.59 on epoch=639
05/18/2022 00:55:38 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.63 on epoch=642
05/18/2022 00:55:39 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.65 on epoch=644
05/18/2022 00:55:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.53 on epoch=647
05/18/2022 00:55:41 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.54 on epoch=649
05/18/2022 00:55:42 - INFO - __main__ - Global step 2600 Train loss 0.59 Classification-F1 0.4365934065934066 on epoch=649
05/18/2022 00:55:43 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.52 on epoch=652
05/18/2022 00:55:44 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.61 on epoch=654
05/18/2022 00:55:46 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.65 on epoch=657
05/18/2022 00:55:47 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.61 on epoch=659
05/18/2022 00:55:48 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.57 on epoch=662
05/18/2022 00:55:48 - INFO - __main__ - Global step 2650 Train loss 0.59 Classification-F1 0.4716227916227916 on epoch=662
05/18/2022 00:55:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.63 on epoch=664
05/18/2022 00:55:51 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.54 on epoch=667
05/18/2022 00:55:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.63 on epoch=669
05/18/2022 00:55:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.57 on epoch=672
05/18/2022 00:55:54 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.54 on epoch=674
05/18/2022 00:55:55 - INFO - __main__ - Global step 2700 Train loss 0.58 Classification-F1 0.4287063385908059 on epoch=674
05/18/2022 00:55:56 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.58 on epoch=677
05/18/2022 00:55:57 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.52 on epoch=679
05/18/2022 00:55:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.58 on epoch=682
05/18/2022 00:55:59 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.49 on epoch=684
05/18/2022 00:56:01 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.56 on epoch=687
05/18/2022 00:56:01 - INFO - __main__ - Global step 2750 Train loss 0.55 Classification-F1 0.5202055442866802 on epoch=687
05/18/2022 00:56:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.55 on epoch=689
05/18/2022 00:56:03 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.56 on epoch=692
05/18/2022 00:56:05 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.51 on epoch=694
05/18/2022 00:56:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.51 on epoch=697
05/18/2022 00:56:07 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.58 on epoch=699
05/18/2022 00:56:07 - INFO - __main__ - Global step 2800 Train loss 0.54 Classification-F1 0.43096653214300273 on epoch=699
05/18/2022 00:56:09 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.57 on epoch=702
05/18/2022 00:56:10 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.60 on epoch=704
05/18/2022 00:56:11 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.53 on epoch=707
05/18/2022 00:56:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.54 on epoch=709
05/18/2022 00:56:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.57 on epoch=712
05/18/2022 00:56:14 - INFO - __main__ - Global step 2850 Train loss 0.56 Classification-F1 0.44761904761904764 on epoch=712
05/18/2022 00:56:15 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.47 on epoch=714
05/18/2022 00:56:16 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.52 on epoch=717
05/18/2022 00:56:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.52 on epoch=719
05/18/2022 00:56:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.64 on epoch=722
05/18/2022 00:56:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.49 on epoch=724
05/18/2022 00:56:20 - INFO - __main__ - Global step 2900 Train loss 0.53 Classification-F1 0.4617733990147784 on epoch=724
05/18/2022 00:56:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.51 on epoch=727
05/18/2022 00:56:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.64 on epoch=729
05/18/2022 00:56:24 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.55 on epoch=732
05/18/2022 00:56:25 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.52 on epoch=734
05/18/2022 00:56:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.53 on epoch=737
05/18/2022 00:56:27 - INFO - __main__ - Global step 2950 Train loss 0.55 Classification-F1 0.48672349226428924 on epoch=737
05/18/2022 00:56:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.51 on epoch=739
05/18/2022 00:56:29 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.59 on epoch=742
05/18/2022 00:56:30 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.43 on epoch=744
05/18/2022 00:56:31 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.56 on epoch=747
05/18/2022 00:56:32 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.43 on epoch=749
05/18/2022 00:56:33 - INFO - __main__ - Global step 3000 Train loss 0.50 Classification-F1 0.4708660540239487 on epoch=749
05/18/2022 00:56:33 - INFO - __main__ - save last model!
05/18/2022 00:56:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 00:56:33 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 00:56:33 - INFO - __main__ - Printing 3 examples
05/18/2022 00:56:33 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 00:56:33 - INFO - __main__ - ['others']
05/18/2022 00:56:33 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 00:56:33 - INFO - __main__ - ['others']
05/18/2022 00:56:33 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 00:56:33 - INFO - __main__ - ['others']
05/18/2022 00:56:33 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:56:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:56:34 - INFO - __main__ - Printing 3 examples
05/18/2022 00:56:34 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/18/2022 00:56:34 - INFO - __main__ - ['sad']
05/18/2022 00:56:34 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/18/2022 00:56:34 - INFO - __main__ - ['sad']
05/18/2022 00:56:34 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/18/2022 00:56:34 - INFO - __main__ - ['sad']
05/18/2022 00:56:34 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:56:34 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:56:34 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 00:56:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:56:34 - INFO - __main__ - Printing 3 examples
05/18/2022 00:56:34 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/18/2022 00:56:34 - INFO - __main__ - ['sad']
05/18/2022 00:56:34 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/18/2022 00:56:34 - INFO - __main__ - ['sad']
05/18/2022 00:56:34 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/18/2022 00:56:34 - INFO - __main__ - ['sad']
05/18/2022 00:56:34 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:56:34 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:56:34 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 00:56:35 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:56:39 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 00:56:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 00:56:39 - INFO - __main__ - Starting training!
05/18/2022 00:56:40 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 00:57:23 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_21_0.4_8_predictions.txt
05/18/2022 00:57:23 - INFO - __main__ - Classification-F1 on test data: 0.0850
05/18/2022 00:57:24 - INFO - __main__ - prefix=emo_16_21, lr=0.4, bsz=8, dev_performance=0.5706043956043956, test_performance=0.08497876232863517
05/18/2022 00:57:24 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.3, bsz=8 ...
05/18/2022 00:57:24 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:57:24 - INFO - __main__ - Printing 3 examples
05/18/2022 00:57:24 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/18/2022 00:57:24 - INFO - __main__ - ['sad']
05/18/2022 00:57:24 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/18/2022 00:57:24 - INFO - __main__ - ['sad']
05/18/2022 00:57:24 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/18/2022 00:57:24 - INFO - __main__ - ['sad']
05/18/2022 00:57:24 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:57:25 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:57:25 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 00:57:25 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 00:57:25 - INFO - __main__ - Printing 3 examples
05/18/2022 00:57:25 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/18/2022 00:57:25 - INFO - __main__ - ['sad']
05/18/2022 00:57:25 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/18/2022 00:57:25 - INFO - __main__ - ['sad']
05/18/2022 00:57:25 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/18/2022 00:57:25 - INFO - __main__ - ['sad']
05/18/2022 00:57:25 - INFO - __main__ - Tokenizing Input ...
05/18/2022 00:57:25 - INFO - __main__ - Tokenizing Output ...
05/18/2022 00:57:25 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 00:57:31 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 00:57:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 00:57:31 - INFO - __main__ - Starting training!
05/18/2022 00:57:32 - INFO - __main__ - Step 10 Global step 10 Train loss 4.41 on epoch=2
05/18/2022 00:57:34 - INFO - __main__ - Step 20 Global step 20 Train loss 3.98 on epoch=4
05/18/2022 00:57:35 - INFO - __main__ - Step 30 Global step 30 Train loss 3.73 on epoch=7
05/18/2022 00:57:36 - INFO - __main__ - Step 40 Global step 40 Train loss 3.40 on epoch=9
05/18/2022 00:57:38 - INFO - __main__ - Step 50 Global step 50 Train loss 2.94 on epoch=12
05/18/2022 00:57:38 - INFO - __main__ - Global step 50 Train loss 3.69 Classification-F1 0.1 on epoch=12
05/18/2022 00:57:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/18/2022 00:57:40 - INFO - __main__ - Step 60 Global step 60 Train loss 2.65 on epoch=14
05/18/2022 00:57:41 - INFO - __main__ - Step 70 Global step 70 Train loss 2.38 on epoch=17
05/18/2022 00:57:42 - INFO - __main__ - Step 80 Global step 80 Train loss 2.06 on epoch=19
05/18/2022 00:57:43 - INFO - __main__ - Step 90 Global step 90 Train loss 2.05 on epoch=22
05/18/2022 00:57:45 - INFO - __main__ - Step 100 Global step 100 Train loss 1.61 on epoch=24
05/18/2022 00:57:45 - INFO - __main__ - Global step 100 Train loss 2.15 Classification-F1 0.1302118933697881 on epoch=24
05/18/2022 00:57:45 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1302118933697881 on epoch=24, global_step=100
05/18/2022 00:57:46 - INFO - __main__ - Step 110 Global step 110 Train loss 1.60 on epoch=27
05/18/2022 00:57:48 - INFO - __main__ - Step 120 Global step 120 Train loss 1.33 on epoch=29
05/18/2022 00:57:49 - INFO - __main__ - Step 130 Global step 130 Train loss 1.27 on epoch=32
05/18/2022 00:57:50 - INFO - __main__ - Step 140 Global step 140 Train loss 1.12 on epoch=34
05/18/2022 00:57:51 - INFO - __main__ - Step 150 Global step 150 Train loss 1.20 on epoch=37
05/18/2022 00:57:52 - INFO - __main__ - Global step 150 Train loss 1.31 Classification-F1 0.13653308480894688 on epoch=37
05/18/2022 00:57:52 - INFO - __main__ - Saving model with best Classification-F1: 0.1302118933697881 -> 0.13653308480894688 on epoch=37, global_step=150
05/18/2022 00:57:53 - INFO - __main__ - Step 160 Global step 160 Train loss 1.06 on epoch=39
05/18/2022 00:57:54 - INFO - __main__ - Step 170 Global step 170 Train loss 1.08 on epoch=42
05/18/2022 00:57:56 - INFO - __main__ - Step 180 Global step 180 Train loss 1.04 on epoch=44
05/18/2022 00:57:57 - INFO - __main__ - Step 190 Global step 190 Train loss 1.02 on epoch=47
05/18/2022 00:57:58 - INFO - __main__ - Step 200 Global step 200 Train loss 1.07 on epoch=49
05/18/2022 00:57:58 - INFO - __main__ - Global step 200 Train loss 1.05 Classification-F1 0.15423976608187134 on epoch=49
05/18/2022 00:57:59 - INFO - __main__ - Saving model with best Classification-F1: 0.13653308480894688 -> 0.15423976608187134 on epoch=49, global_step=200
05/18/2022 00:58:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.96 on epoch=52
05/18/2022 00:58:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.97 on epoch=54
05/18/2022 00:58:02 - INFO - __main__ - Step 230 Global step 230 Train loss 1.06 on epoch=57
05/18/2022 00:58:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.99 on epoch=59
05/18/2022 00:58:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.86 on epoch=62
05/18/2022 00:58:05 - INFO - __main__ - Global step 250 Train loss 0.97 Classification-F1 0.09210526315789473 on epoch=62
05/18/2022 00:58:06 - INFO - __main__ - Step 260 Global step 260 Train loss 1.01 on epoch=64
05/18/2022 00:58:08 - INFO - __main__ - Step 270 Global step 270 Train loss 1.00 on epoch=67
05/18/2022 00:58:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.94 on epoch=69
05/18/2022 00:58:10 - INFO - __main__ - Step 290 Global step 290 Train loss 1.04 on epoch=72
05/18/2022 00:58:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.93 on epoch=74
05/18/2022 00:58:12 - INFO - __main__ - Global step 300 Train loss 0.98 Classification-F1 0.10126582278481013 on epoch=74
05/18/2022 00:58:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.91 on epoch=77
05/18/2022 00:58:14 - INFO - __main__ - Step 320 Global step 320 Train loss 1.03 on epoch=79
05/18/2022 00:58:16 - INFO - __main__ - Step 330 Global step 330 Train loss 1.01 on epoch=82
05/18/2022 00:58:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.98 on epoch=84
05/18/2022 00:58:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.94 on epoch=87
05/18/2022 00:58:19 - INFO - __main__ - Global step 350 Train loss 0.97 Classification-F1 0.10126582278481013 on epoch=87
05/18/2022 00:58:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.95 on epoch=89
05/18/2022 00:58:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.98 on epoch=92
05/18/2022 00:58:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.91 on epoch=94
05/18/2022 00:58:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.91 on epoch=97
05/18/2022 00:58:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.89 on epoch=99
05/18/2022 00:58:25 - INFO - __main__ - Global step 400 Train loss 0.93 Classification-F1 0.1 on epoch=99
05/18/2022 00:58:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.95 on epoch=102
05/18/2022 00:58:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.99 on epoch=104
05/18/2022 00:58:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.89 on epoch=107
05/18/2022 00:58:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.89 on epoch=109
05/18/2022 00:58:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.87 on epoch=112
05/18/2022 00:58:32 - INFO - __main__ - Global step 450 Train loss 0.92 Classification-F1 0.1313186813186813 on epoch=112
05/18/2022 00:58:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.96 on epoch=114
05/18/2022 00:58:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.94 on epoch=117
05/18/2022 00:58:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.88 on epoch=119
05/18/2022 00:58:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.95 on epoch=122
05/18/2022 00:58:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.96 on epoch=124
05/18/2022 00:58:39 - INFO - __main__ - Global step 500 Train loss 0.94 Classification-F1 0.10126582278481013 on epoch=124
05/18/2022 00:58:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.97 on epoch=127
05/18/2022 00:58:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.83 on epoch=129
05/18/2022 00:58:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.84 on epoch=132
05/18/2022 00:58:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.84 on epoch=134
05/18/2022 00:58:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.83 on epoch=137
05/18/2022 00:58:46 - INFO - __main__ - Global step 550 Train loss 0.86 Classification-F1 0.1 on epoch=137
05/18/2022 00:58:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.85 on epoch=139
05/18/2022 00:58:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.90 on epoch=142
05/18/2022 00:58:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.89 on epoch=144
05/18/2022 00:58:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.98 on epoch=147
05/18/2022 00:58:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.92 on epoch=149
05/18/2022 00:58:53 - INFO - __main__ - Global step 600 Train loss 0.91 Classification-F1 0.10126582278481013 on epoch=149
05/18/2022 00:58:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.88 on epoch=152
05/18/2022 00:58:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.86 on epoch=154
05/18/2022 00:58:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.91 on epoch=157
05/18/2022 00:58:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.86 on epoch=159
05/18/2022 00:58:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.81 on epoch=162
05/18/2022 00:59:00 - INFO - __main__ - Global step 650 Train loss 0.87 Classification-F1 0.10126582278481013 on epoch=162
05/18/2022 00:59:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.86 on epoch=164
05/18/2022 00:59:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.75 on epoch=167
05/18/2022 00:59:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.89 on epoch=169
05/18/2022 00:59:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.90 on epoch=172
05/18/2022 00:59:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.96 on epoch=174
05/18/2022 00:59:06 - INFO - __main__ - Global step 700 Train loss 0.87 Classification-F1 0.10126582278481013 on epoch=174
05/18/2022 00:59:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.83 on epoch=177
05/18/2022 00:59:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.89 on epoch=179
05/18/2022 00:59:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.82 on epoch=182
05/18/2022 00:59:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.89 on epoch=184
05/18/2022 00:59:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.90 on epoch=187
05/18/2022 00:59:13 - INFO - __main__ - Global step 750 Train loss 0.87 Classification-F1 0.10126582278481013 on epoch=187
05/18/2022 00:59:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.85 on epoch=189
05/18/2022 00:59:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.83 on epoch=192
05/18/2022 00:59:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.80 on epoch=194
05/18/2022 00:59:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.78 on epoch=197
05/18/2022 00:59:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.84 on epoch=199
05/18/2022 00:59:20 - INFO - __main__ - Global step 800 Train loss 0.82 Classification-F1 0.1 on epoch=199
05/18/2022 00:59:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.90 on epoch=202
05/18/2022 00:59:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.82 on epoch=204
05/18/2022 00:59:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.86 on epoch=207
05/18/2022 00:59:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.86 on epoch=209
05/18/2022 00:59:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.79 on epoch=212
05/18/2022 00:59:27 - INFO - __main__ - Global step 850 Train loss 0.85 Classification-F1 0.13381369016984046 on epoch=212
05/18/2022 00:59:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.88 on epoch=214
05/18/2022 00:59:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.83 on epoch=217
05/18/2022 00:59:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.90 on epoch=219
05/18/2022 00:59:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.81 on epoch=222
05/18/2022 00:59:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.91 on epoch=224
05/18/2022 00:59:34 - INFO - __main__ - Global step 900 Train loss 0.87 Classification-F1 0.10126582278481013 on epoch=224
05/18/2022 00:59:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.84 on epoch=227
05/18/2022 00:59:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.85 on epoch=229
05/18/2022 00:59:38 - INFO - __main__ - Step 930 Global step 930 Train loss 0.83 on epoch=232
05/18/2022 00:59:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.92 on epoch=234
05/18/2022 00:59:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.86 on epoch=237
05/18/2022 00:59:41 - INFO - __main__ - Global step 950 Train loss 0.86 Classification-F1 0.09615384615384615 on epoch=237
05/18/2022 00:59:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.88 on epoch=239
05/18/2022 00:59:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.88 on epoch=242
05/18/2022 00:59:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.83 on epoch=244
05/18/2022 00:59:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.84 on epoch=247
05/18/2022 00:59:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.92 on epoch=249
05/18/2022 00:59:48 - INFO - __main__ - Global step 1000 Train loss 0.87 Classification-F1 0.1 on epoch=249
05/18/2022 00:59:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.85 on epoch=252
05/18/2022 00:59:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.91 on epoch=254
05/18/2022 00:59:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.85 on epoch=257
05/18/2022 00:59:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.82 on epoch=259
05/18/2022 00:59:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.85 on epoch=262
05/18/2022 00:59:54 - INFO - __main__ - Global step 1050 Train loss 0.85 Classification-F1 0.16223908918406071 on epoch=262
05/18/2022 00:59:54 - INFO - __main__ - Saving model with best Classification-F1: 0.15423976608187134 -> 0.16223908918406071 on epoch=262, global_step=1050
05/18/2022 00:59:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.82 on epoch=264
05/18/2022 00:59:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.91 on epoch=267
05/18/2022 00:59:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.82 on epoch=269
05/18/2022 00:59:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.81 on epoch=272
05/18/2022 01:00:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.76 on epoch=274
05/18/2022 01:00:01 - INFO - __main__ - Global step 1100 Train loss 0.82 Classification-F1 0.1 on epoch=274
05/18/2022 01:00:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.88 on epoch=277
05/18/2022 01:00:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.84 on epoch=279
05/18/2022 01:00:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.83 on epoch=282
05/18/2022 01:00:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.81 on epoch=284
05/18/2022 01:00:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.85 on epoch=287
05/18/2022 01:00:08 - INFO - __main__ - Global step 1150 Train loss 0.84 Classification-F1 0.10256410256410256 on epoch=287
05/18/2022 01:00:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.81 on epoch=289
05/18/2022 01:00:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.85 on epoch=292
05/18/2022 01:00:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.78 on epoch=294
05/18/2022 01:00:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.74 on epoch=297
05/18/2022 01:00:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.81 on epoch=299
05/18/2022 01:00:15 - INFO - __main__ - Global step 1200 Train loss 0.80 Classification-F1 0.10126582278481013 on epoch=299
05/18/2022 01:00:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.80 on epoch=302
05/18/2022 01:00:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.80 on epoch=304
05/18/2022 01:00:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.82 on epoch=307
05/18/2022 01:00:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.76 on epoch=309
05/18/2022 01:00:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.85 on epoch=312
05/18/2022 01:00:22 - INFO - __main__ - Global step 1250 Train loss 0.81 Classification-F1 0.16378066378066378 on epoch=312
05/18/2022 01:00:22 - INFO - __main__ - Saving model with best Classification-F1: 0.16223908918406071 -> 0.16378066378066378 on epoch=312, global_step=1250
05/18/2022 01:00:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.84 on epoch=314
05/18/2022 01:00:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.85 on epoch=317
05/18/2022 01:00:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.77 on epoch=319
05/18/2022 01:00:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.89 on epoch=322
05/18/2022 01:00:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.78 on epoch=324
05/18/2022 01:00:29 - INFO - __main__ - Global step 1300 Train loss 0.82 Classification-F1 0.1 on epoch=324
05/18/2022 01:00:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.78 on epoch=327
05/18/2022 01:00:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.80 on epoch=329
05/18/2022 01:00:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.88 on epoch=332
05/18/2022 01:00:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.81 on epoch=334
05/18/2022 01:00:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.74 on epoch=337
05/18/2022 01:00:35 - INFO - __main__ - Global step 1350 Train loss 0.80 Classification-F1 0.13251935675997617 on epoch=337
05/18/2022 01:00:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.80 on epoch=339
05/18/2022 01:00:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.89 on epoch=342
05/18/2022 01:00:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.76 on epoch=344
05/18/2022 01:00:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.81 on epoch=347
05/18/2022 01:00:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.83 on epoch=349
05/18/2022 01:00:42 - INFO - __main__ - Global step 1400 Train loss 0.82 Classification-F1 0.13067758749069247 on epoch=349
05/18/2022 01:00:44 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.79 on epoch=352
05/18/2022 01:00:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.72 on epoch=354
05/18/2022 01:00:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.82 on epoch=357
05/18/2022 01:00:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.82 on epoch=359
05/18/2022 01:00:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.92 on epoch=362
05/18/2022 01:00:49 - INFO - __main__ - Global step 1450 Train loss 0.81 Classification-F1 0.1967741935483871 on epoch=362
05/18/2022 01:00:49 - INFO - __main__ - Saving model with best Classification-F1: 0.16378066378066378 -> 0.1967741935483871 on epoch=362, global_step=1450
05/18/2022 01:00:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.82 on epoch=364
05/18/2022 01:00:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.77 on epoch=367
05/18/2022 01:00:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.78 on epoch=369
05/18/2022 01:00:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.81 on epoch=372
05/18/2022 01:00:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.83 on epoch=374
05/18/2022 01:00:56 - INFO - __main__ - Global step 1500 Train loss 0.80 Classification-F1 0.15459213988625753 on epoch=374
05/18/2022 01:00:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.80 on epoch=377
05/18/2022 01:00:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.81 on epoch=379
05/18/2022 01:01:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.71 on epoch=382
05/18/2022 01:01:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.83 on epoch=384
05/18/2022 01:01:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.81 on epoch=387
05/18/2022 01:01:03 - INFO - __main__ - Global step 1550 Train loss 0.79 Classification-F1 0.12539184952978055 on epoch=387
05/18/2022 01:01:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.76 on epoch=389
05/18/2022 01:01:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.83 on epoch=392
05/18/2022 01:01:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.85 on epoch=394
05/18/2022 01:01:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.82 on epoch=397
05/18/2022 01:01:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.78 on epoch=399
05/18/2022 01:01:09 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.17045454545454547 on epoch=399
05/18/2022 01:01:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.80 on epoch=402
05/18/2022 01:01:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.74 on epoch=404
05/18/2022 01:01:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.80 on epoch=407
05/18/2022 01:01:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.83 on epoch=409
05/18/2022 01:01:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.81 on epoch=412
05/18/2022 01:01:16 - INFO - __main__ - Global step 1650 Train loss 0.80 Classification-F1 0.21388888888888885 on epoch=412
05/18/2022 01:01:16 - INFO - __main__ - Saving model with best Classification-F1: 0.1967741935483871 -> 0.21388888888888885 on epoch=412, global_step=1650
05/18/2022 01:01:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.82 on epoch=414
05/18/2022 01:01:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.80 on epoch=417
05/18/2022 01:01:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.77 on epoch=419
05/18/2022 01:01:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.79 on epoch=422
05/18/2022 01:01:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.77 on epoch=424
05/18/2022 01:01:22 - INFO - __main__ - Global step 1700 Train loss 0.79 Classification-F1 0.2127806346419165 on epoch=424
05/18/2022 01:01:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.83 on epoch=427
05/18/2022 01:01:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.83 on epoch=429
05/18/2022 01:01:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.79 on epoch=432
05/18/2022 01:01:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.78 on epoch=434
05/18/2022 01:01:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.72 on epoch=437
05/18/2022 01:01:29 - INFO - __main__ - Global step 1750 Train loss 0.79 Classification-F1 0.2492401215805471 on epoch=437
05/18/2022 01:01:29 - INFO - __main__ - Saving model with best Classification-F1: 0.21388888888888885 -> 0.2492401215805471 on epoch=437, global_step=1750
05/18/2022 01:01:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.77 on epoch=439
05/18/2022 01:01:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.79 on epoch=442
05/18/2022 01:01:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.73 on epoch=444
05/18/2022 01:01:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.78 on epoch=447
05/18/2022 01:01:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.72 on epoch=449
05/18/2022 01:01:35 - INFO - __main__ - Global step 1800 Train loss 0.76 Classification-F1 0.2712433339649712 on epoch=449
05/18/2022 01:01:35 - INFO - __main__ - Saving model with best Classification-F1: 0.2492401215805471 -> 0.2712433339649712 on epoch=449, global_step=1800
05/18/2022 01:01:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.69 on epoch=452
05/18/2022 01:01:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.77 on epoch=454
05/18/2022 01:01:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.78 on epoch=457
05/18/2022 01:01:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.74 on epoch=459
05/18/2022 01:01:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.72 on epoch=462
05/18/2022 01:01:42 - INFO - __main__ - Global step 1850 Train loss 0.74 Classification-F1 0.32355810616929703 on epoch=462
05/18/2022 01:01:42 - INFO - __main__ - Saving model with best Classification-F1: 0.2712433339649712 -> 0.32355810616929703 on epoch=462, global_step=1850
05/18/2022 01:01:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.75 on epoch=464
05/18/2022 01:01:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.81 on epoch=467
05/18/2022 01:01:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.70 on epoch=469
05/18/2022 01:01:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.75 on epoch=472
05/18/2022 01:01:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.78 on epoch=474
05/18/2022 01:01:48 - INFO - __main__ - Global step 1900 Train loss 0.76 Classification-F1 0.3785259838201015 on epoch=474
05/18/2022 01:01:48 - INFO - __main__ - Saving model with best Classification-F1: 0.32355810616929703 -> 0.3785259838201015 on epoch=474, global_step=1900
05/18/2022 01:01:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.78 on epoch=477
05/18/2022 01:01:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.68 on epoch=479
05/18/2022 01:01:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.85 on epoch=482
05/18/2022 01:01:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.69 on epoch=484
05/18/2022 01:01:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.74 on epoch=487
05/18/2022 01:01:55 - INFO - __main__ - Global step 1950 Train loss 0.75 Classification-F1 0.34444444444444444 on epoch=487
05/18/2022 01:01:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.67 on epoch=489
05/18/2022 01:01:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.81 on epoch=492
05/18/2022 01:01:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.72 on epoch=494
05/18/2022 01:01:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.71 on epoch=497
05/18/2022 01:02:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.70 on epoch=499
05/18/2022 01:02:01 - INFO - __main__ - Global step 2000 Train loss 0.72 Classification-F1 0.3620492472867312 on epoch=499
05/18/2022 01:02:02 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.72 on epoch=502
05/18/2022 01:02:03 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.65 on epoch=504
05/18/2022 01:02:04 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.75 on epoch=507
05/18/2022 01:02:06 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.71 on epoch=509
05/18/2022 01:02:07 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.68 on epoch=512
05/18/2022 01:02:07 - INFO - __main__ - Global step 2050 Train loss 0.70 Classification-F1 0.4572378840671524 on epoch=512
05/18/2022 01:02:07 - INFO - __main__ - Saving model with best Classification-F1: 0.3785259838201015 -> 0.4572378840671524 on epoch=512, global_step=2050
05/18/2022 01:02:08 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.73 on epoch=514
05/18/2022 01:02:10 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.75 on epoch=517
05/18/2022 01:02:11 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.71 on epoch=519
05/18/2022 01:02:12 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.70 on epoch=522
05/18/2022 01:02:13 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.71 on epoch=524
05/18/2022 01:02:14 - INFO - __main__ - Global step 2100 Train loss 0.72 Classification-F1 0.36066542153498676 on epoch=524
05/18/2022 01:02:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.69 on epoch=527
05/18/2022 01:02:16 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.62 on epoch=529
05/18/2022 01:02:17 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.75 on epoch=532
05/18/2022 01:02:18 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.66 on epoch=534
05/18/2022 01:02:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.69 on epoch=537
05/18/2022 01:02:20 - INFO - __main__ - Global step 2150 Train loss 0.68 Classification-F1 0.364985014985015 on epoch=537
05/18/2022 01:02:21 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.66 on epoch=539
05/18/2022 01:02:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.74 on epoch=542
05/18/2022 01:02:23 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.70 on epoch=544
05/18/2022 01:02:25 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.72 on epoch=547
05/18/2022 01:02:26 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.70 on epoch=549
05/18/2022 01:02:26 - INFO - __main__ - Global step 2200 Train loss 0.71 Classification-F1 0.4391406323294868 on epoch=549
05/18/2022 01:02:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.64 on epoch=552
05/18/2022 01:02:29 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.68 on epoch=554
05/18/2022 01:02:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.68 on epoch=557
05/18/2022 01:02:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.71 on epoch=559
05/18/2022 01:02:32 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.70 on epoch=562
05/18/2022 01:02:33 - INFO - __main__ - Global step 2250 Train loss 0.68 Classification-F1 0.42298818298818297 on epoch=562
05/18/2022 01:02:34 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.70 on epoch=564
05/18/2022 01:02:35 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.71 on epoch=567
05/18/2022 01:02:36 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.67 on epoch=569
05/18/2022 01:02:37 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.69 on epoch=572
05/18/2022 01:02:39 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.66 on epoch=574
05/18/2022 01:02:39 - INFO - __main__ - Global step 2300 Train loss 0.69 Classification-F1 0.3670556339514883 on epoch=574
05/18/2022 01:02:40 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.69 on epoch=577
05/18/2022 01:02:41 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.64 on epoch=579
05/18/2022 01:02:43 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.61 on epoch=582
05/18/2022 01:02:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.60 on epoch=584
05/18/2022 01:02:45 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.61 on epoch=587
05/18/2022 01:02:45 - INFO - __main__ - Global step 2350 Train loss 0.63 Classification-F1 0.4013717550302916 on epoch=587
05/18/2022 01:02:47 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.82 on epoch=589
05/18/2022 01:02:48 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.69 on epoch=592
05/18/2022 01:02:49 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.62 on epoch=594
05/18/2022 01:02:50 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.64 on epoch=597
05/18/2022 01:02:51 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.67 on epoch=599
05/18/2022 01:02:52 - INFO - __main__ - Global step 2400 Train loss 0.69 Classification-F1 0.42548717948717946 on epoch=599
05/18/2022 01:02:53 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.66 on epoch=602
05/18/2022 01:02:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.62 on epoch=604
05/18/2022 01:02:55 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.66 on epoch=607
05/18/2022 01:02:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.57 on epoch=609
05/18/2022 01:02:58 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.61 on epoch=612
05/18/2022 01:02:58 - INFO - __main__ - Global step 2450 Train loss 0.63 Classification-F1 0.5706420595533498 on epoch=612
05/18/2022 01:02:58 - INFO - __main__ - Saving model with best Classification-F1: 0.4572378840671524 -> 0.5706420595533498 on epoch=612, global_step=2450
05/18/2022 01:02:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.63 on epoch=614
05/18/2022 01:03:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.64 on epoch=617
05/18/2022 01:03:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.64 on epoch=619
05/18/2022 01:03:03 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.70 on epoch=622
05/18/2022 01:03:04 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.62 on epoch=624
05/18/2022 01:03:05 - INFO - __main__ - Global step 2500 Train loss 0.65 Classification-F1 0.4725392886683209 on epoch=624
05/18/2022 01:03:06 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.65 on epoch=627
05/18/2022 01:03:07 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.60 on epoch=629
05/18/2022 01:03:08 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.66 on epoch=632
05/18/2022 01:03:09 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.65 on epoch=634
05/18/2022 01:03:11 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.68 on epoch=637
05/18/2022 01:03:11 - INFO - __main__ - Global step 2550 Train loss 0.65 Classification-F1 0.4456339362618433 on epoch=637
05/18/2022 01:03:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.61 on epoch=639
05/18/2022 01:03:13 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.62 on epoch=642
05/18/2022 01:03:15 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.65 on epoch=644
05/18/2022 01:03:16 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.68 on epoch=647
05/18/2022 01:03:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.64 on epoch=649
05/18/2022 01:03:18 - INFO - __main__ - Global step 2600 Train loss 0.64 Classification-F1 0.4384325046778937 on epoch=649
05/18/2022 01:03:19 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.75 on epoch=652
05/18/2022 01:03:20 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.65 on epoch=654
05/18/2022 01:03:21 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.67 on epoch=657
05/18/2022 01:03:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.60 on epoch=659
05/18/2022 01:03:23 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.67 on epoch=662
05/18/2022 01:03:24 - INFO - __main__ - Global step 2650 Train loss 0.67 Classification-F1 0.4757252796886943 on epoch=662
05/18/2022 01:03:25 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.66 on epoch=664
05/18/2022 01:03:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.62 on epoch=667
05/18/2022 01:03:28 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.59 on epoch=669
05/18/2022 01:03:29 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.64 on epoch=672
05/18/2022 01:03:30 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.57 on epoch=674
05/18/2022 01:03:30 - INFO - __main__ - Global step 2700 Train loss 0.62 Classification-F1 0.4134423098528256 on epoch=674
05/18/2022 01:03:32 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.66 on epoch=677
05/18/2022 01:03:33 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.65 on epoch=679
05/18/2022 01:03:34 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.58 on epoch=682
05/18/2022 01:03:35 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.64 on epoch=684
05/18/2022 01:03:36 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.66 on epoch=687
05/18/2022 01:03:37 - INFO - __main__ - Global step 2750 Train loss 0.64 Classification-F1 0.42505368773103325 on epoch=687
05/18/2022 01:03:38 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.61 on epoch=689
05/18/2022 01:03:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.68 on epoch=692
05/18/2022 01:03:40 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.64 on epoch=694
05/18/2022 01:03:42 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.68 on epoch=697
05/18/2022 01:03:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.70 on epoch=699
05/18/2022 01:03:43 - INFO - __main__ - Global step 2800 Train loss 0.66 Classification-F1 0.47873243873243865 on epoch=699
05/18/2022 01:03:44 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.64 on epoch=702
05/18/2022 01:03:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.60 on epoch=704
05/18/2022 01:03:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.68 on epoch=707
05/18/2022 01:03:48 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.62 on epoch=709
05/18/2022 01:03:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.63 on epoch=712
05/18/2022 01:03:50 - INFO - __main__ - Global step 2850 Train loss 0.63 Classification-F1 0.6201587301587301 on epoch=712
05/18/2022 01:03:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5706420595533498 -> 0.6201587301587301 on epoch=712, global_step=2850
05/18/2022 01:03:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.59 on epoch=714
05/18/2022 01:03:52 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.62 on epoch=717
05/18/2022 01:03:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.59 on epoch=719
05/18/2022 01:03:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.61 on epoch=722
05/18/2022 01:03:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.55 on epoch=724
05/18/2022 01:03:56 - INFO - __main__ - Global step 2900 Train loss 0.59 Classification-F1 0.47178498985801215 on epoch=724
05/18/2022 01:03:58 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.70 on epoch=727
05/18/2022 01:03:59 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.56 on epoch=729
05/18/2022 01:04:00 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.57 on epoch=732
05/18/2022 01:04:01 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.63 on epoch=734
05/18/2022 01:04:03 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.64 on epoch=737
05/18/2022 01:04:03 - INFO - __main__ - Global step 2950 Train loss 0.62 Classification-F1 0.422063492063492 on epoch=737
05/18/2022 01:04:04 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.57 on epoch=739
05/18/2022 01:04:06 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.60 on epoch=742
05/18/2022 01:04:07 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.68 on epoch=744
05/18/2022 01:04:08 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.59 on epoch=747
05/18/2022 01:04:10 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.62 on epoch=749
05/18/2022 01:04:10 - INFO - __main__ - Global step 3000 Train loss 0.61 Classification-F1 0.4920213246588959 on epoch=749
05/18/2022 01:04:10 - INFO - __main__ - save last model!
05/18/2022 01:04:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 01:04:10 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 01:04:10 - INFO - __main__ - Printing 3 examples
05/18/2022 01:04:10 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 01:04:10 - INFO - __main__ - ['others']
05/18/2022 01:04:10 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 01:04:10 - INFO - __main__ - ['others']
05/18/2022 01:04:10 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 01:04:10 - INFO - __main__ - ['others']
05/18/2022 01:04:10 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:04:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:04:11 - INFO - __main__ - Printing 3 examples
05/18/2022 01:04:11 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/18/2022 01:04:11 - INFO - __main__ - ['sad']
05/18/2022 01:04:11 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/18/2022 01:04:11 - INFO - __main__ - ['sad']
05/18/2022 01:04:11 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/18/2022 01:04:11 - INFO - __main__ - ['sad']
05/18/2022 01:04:11 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:04:11 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:04:11 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 01:04:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:04:11 - INFO - __main__ - Printing 3 examples
05/18/2022 01:04:11 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/18/2022 01:04:11 - INFO - __main__ - ['sad']
05/18/2022 01:04:11 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/18/2022 01:04:11 - INFO - __main__ - ['sad']
05/18/2022 01:04:11 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/18/2022 01:04:11 - INFO - __main__ - ['sad']
05/18/2022 01:04:11 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:04:11 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:04:11 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 01:04:12 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:04:17 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 01:04:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 01:04:17 - INFO - __main__ - Starting training!
05/18/2022 01:04:18 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 01:05:01 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_21_0.3_8_predictions.txt
05/18/2022 01:05:01 - INFO - __main__ - Classification-F1 on test data: 0.0975
05/18/2022 01:05:01 - INFO - __main__ - prefix=emo_16_21, lr=0.3, bsz=8, dev_performance=0.6201587301587301, test_performance=0.09752211418613203
05/18/2022 01:05:01 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.2, bsz=8 ...
05/18/2022 01:05:02 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:05:02 - INFO - __main__ - Printing 3 examples
05/18/2022 01:05:02 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/18/2022 01:05:02 - INFO - __main__ - ['sad']
05/18/2022 01:05:02 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/18/2022 01:05:02 - INFO - __main__ - ['sad']
05/18/2022 01:05:02 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/18/2022 01:05:02 - INFO - __main__ - ['sad']
05/18/2022 01:05:02 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:05:02 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:05:02 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 01:05:02 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:05:02 - INFO - __main__ - Printing 3 examples
05/18/2022 01:05:02 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/18/2022 01:05:02 - INFO - __main__ - ['sad']
05/18/2022 01:05:02 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/18/2022 01:05:02 - INFO - __main__ - ['sad']
05/18/2022 01:05:02 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/18/2022 01:05:02 - INFO - __main__ - ['sad']
05/18/2022 01:05:02 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:05:03 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:05:03 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 01:05:09 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 01:05:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 01:05:09 - INFO - __main__ - Starting training!
05/18/2022 01:05:10 - INFO - __main__ - Step 10 Global step 10 Train loss 4.51 on epoch=2
05/18/2022 01:05:11 - INFO - __main__ - Step 20 Global step 20 Train loss 4.32 on epoch=4
05/18/2022 01:05:13 - INFO - __main__ - Step 30 Global step 30 Train loss 4.00 on epoch=7
05/18/2022 01:05:14 - INFO - __main__ - Step 40 Global step 40 Train loss 3.78 on epoch=9
05/18/2022 01:05:15 - INFO - __main__ - Step 50 Global step 50 Train loss 3.58 on epoch=12
05/18/2022 01:05:16 - INFO - __main__ - Global step 50 Train loss 4.04 Classification-F1 0.0 on epoch=12
05/18/2022 01:05:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
05/18/2022 01:05:18 - INFO - __main__ - Step 60 Global step 60 Train loss 3.35 on epoch=14
05/18/2022 01:05:19 - INFO - __main__ - Step 70 Global step 70 Train loss 3.00 on epoch=17
05/18/2022 01:05:20 - INFO - __main__ - Step 80 Global step 80 Train loss 2.79 on epoch=19
05/18/2022 01:05:21 - INFO - __main__ - Step 90 Global step 90 Train loss 2.71 on epoch=22
05/18/2022 01:05:23 - INFO - __main__ - Step 100 Global step 100 Train loss 2.36 on epoch=24
05/18/2022 01:05:23 - INFO - __main__ - Global step 100 Train loss 2.84 Classification-F1 0.1 on epoch=24
05/18/2022 01:05:23 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.1 on epoch=24, global_step=100
05/18/2022 01:05:24 - INFO - __main__ - Step 110 Global step 110 Train loss 2.40 on epoch=27
05/18/2022 01:05:25 - INFO - __main__ - Step 120 Global step 120 Train loss 2.13 on epoch=29
05/18/2022 01:05:27 - INFO - __main__ - Step 130 Global step 130 Train loss 2.12 on epoch=32
05/18/2022 01:05:28 - INFO - __main__ - Step 140 Global step 140 Train loss 1.80 on epoch=34
05/18/2022 01:05:29 - INFO - __main__ - Step 150 Global step 150 Train loss 1.66 on epoch=37
05/18/2022 01:05:30 - INFO - __main__ - Global step 150 Train loss 2.02 Classification-F1 0.1 on epoch=37
05/18/2022 01:05:31 - INFO - __main__ - Step 160 Global step 160 Train loss 1.52 on epoch=39
05/18/2022 01:05:32 - INFO - __main__ - Step 170 Global step 170 Train loss 1.50 on epoch=42
05/18/2022 01:05:33 - INFO - __main__ - Step 180 Global step 180 Train loss 1.47 on epoch=44
05/18/2022 01:05:34 - INFO - __main__ - Step 190 Global step 190 Train loss 1.39 on epoch=47
05/18/2022 01:05:36 - INFO - __main__ - Step 200 Global step 200 Train loss 1.25 on epoch=49
05/18/2022 01:05:36 - INFO - __main__ - Global step 200 Train loss 1.42 Classification-F1 0.09615384615384615 on epoch=49
05/18/2022 01:05:37 - INFO - __main__ - Step 210 Global step 210 Train loss 1.22 on epoch=52
05/18/2022 01:05:38 - INFO - __main__ - Step 220 Global step 220 Train loss 1.13 on epoch=54
05/18/2022 01:05:40 - INFO - __main__ - Step 230 Global step 230 Train loss 1.12 on epoch=57
05/18/2022 01:05:41 - INFO - __main__ - Step 240 Global step 240 Train loss 1.09 on epoch=59
05/18/2022 01:05:42 - INFO - __main__ - Step 250 Global step 250 Train loss 1.10 on epoch=62
05/18/2022 01:05:43 - INFO - __main__ - Global step 250 Train loss 1.13 Classification-F1 0.13067758749069247 on epoch=62
05/18/2022 01:05:43 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.13067758749069247 on epoch=62, global_step=250
05/18/2022 01:05:44 - INFO - __main__ - Step 260 Global step 260 Train loss 1.06 on epoch=64
05/18/2022 01:05:45 - INFO - __main__ - Step 270 Global step 270 Train loss 1.15 on epoch=67
05/18/2022 01:05:46 - INFO - __main__ - Step 280 Global step 280 Train loss 1.15 on epoch=69
05/18/2022 01:05:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.96 on epoch=72
05/18/2022 01:05:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.85 on epoch=74
05/18/2022 01:05:49 - INFO - __main__ - Global step 300 Train loss 1.04 Classification-F1 0.14221364221364222 on epoch=74
05/18/2022 01:05:49 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.14221364221364222 on epoch=74, global_step=300
05/18/2022 01:05:51 - INFO - __main__ - Step 310 Global step 310 Train loss 1.12 on epoch=77
05/18/2022 01:05:52 - INFO - __main__ - Step 320 Global step 320 Train loss 1.03 on epoch=79
05/18/2022 01:05:53 - INFO - __main__ - Step 330 Global step 330 Train loss 1.03 on epoch=82
05/18/2022 01:05:54 - INFO - __main__ - Step 340 Global step 340 Train loss 1.01 on epoch=84
05/18/2022 01:05:56 - INFO - __main__ - Step 350 Global step 350 Train loss 1.03 on epoch=87
05/18/2022 01:05:56 - INFO - __main__ - Global step 350 Train loss 1.04 Classification-F1 0.0974025974025974 on epoch=87
05/18/2022 01:05:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.95 on epoch=89
05/18/2022 01:05:58 - INFO - __main__ - Step 370 Global step 370 Train loss 1.02 on epoch=92
05/18/2022 01:06:00 - INFO - __main__ - Step 380 Global step 380 Train loss 1.01 on epoch=94
05/18/2022 01:06:01 - INFO - __main__ - Step 390 Global step 390 Train loss 1.02 on epoch=97
05/18/2022 01:06:02 - INFO - __main__ - Step 400 Global step 400 Train loss 1.01 on epoch=99
05/18/2022 01:06:03 - INFO - __main__ - Global step 400 Train loss 1.00 Classification-F1 0.1 on epoch=99
05/18/2022 01:06:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.93 on epoch=102
05/18/2022 01:06:05 - INFO - __main__ - Step 420 Global step 420 Train loss 1.13 on epoch=104
05/18/2022 01:06:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.95 on epoch=107
05/18/2022 01:06:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.86 on epoch=109
05/18/2022 01:06:09 - INFO - __main__ - Step 450 Global step 450 Train loss 1.01 on epoch=112
05/18/2022 01:06:09 - INFO - __main__ - Global step 450 Train loss 0.98 Classification-F1 0.12403499742665978 on epoch=112
05/18/2022 01:06:10 - INFO - __main__ - Step 460 Global step 460 Train loss 1.05 on epoch=114
05/18/2022 01:06:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.93 on epoch=117
05/18/2022 01:06:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.92 on epoch=119
05/18/2022 01:06:14 - INFO - __main__ - Step 490 Global step 490 Train loss 1.02 on epoch=122
05/18/2022 01:06:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.90 on epoch=124
05/18/2022 01:06:16 - INFO - __main__ - Global step 500 Train loss 0.96 Classification-F1 0.09210526315789473 on epoch=124
05/18/2022 01:06:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.91 on epoch=127
05/18/2022 01:06:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.95 on epoch=129
05/18/2022 01:06:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.80 on epoch=132
05/18/2022 01:06:21 - INFO - __main__ - Step 540 Global step 540 Train loss 1.10 on epoch=134
05/18/2022 01:06:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.96 on epoch=137
05/18/2022 01:06:22 - INFO - __main__ - Global step 550 Train loss 0.94 Classification-F1 0.09210526315789473 on epoch=137
05/18/2022 01:06:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.94 on epoch=139
05/18/2022 01:06:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.94 on epoch=142
05/18/2022 01:06:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.93 on epoch=144
05/18/2022 01:06:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.85 on epoch=147
05/18/2022 01:06:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.97 on epoch=149
05/18/2022 01:06:29 - INFO - __main__ - Global step 600 Train loss 0.93 Classification-F1 0.1 on epoch=149
05/18/2022 01:06:30 - INFO - __main__ - Step 610 Global step 610 Train loss 1.02 on epoch=152
05/18/2022 01:06:32 - INFO - __main__ - Step 620 Global step 620 Train loss 1.03 on epoch=154
05/18/2022 01:06:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.98 on epoch=157
05/18/2022 01:06:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.93 on epoch=159
05/18/2022 01:06:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.87 on epoch=162
05/18/2022 01:06:36 - INFO - __main__ - Global step 650 Train loss 0.97 Classification-F1 0.09615384615384615 on epoch=162
05/18/2022 01:06:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.89 on epoch=164
05/18/2022 01:06:38 - INFO - __main__ - Step 670 Global step 670 Train loss 1.03 on epoch=167
05/18/2022 01:06:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.94 on epoch=169
05/18/2022 01:06:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.89 on epoch=172
05/18/2022 01:06:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.90 on epoch=174
05/18/2022 01:06:43 - INFO - __main__ - Global step 700 Train loss 0.93 Classification-F1 0.1 on epoch=174
05/18/2022 01:06:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.83 on epoch=177
05/18/2022 01:06:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.88 on epoch=179
05/18/2022 01:06:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.90 on epoch=182
05/18/2022 01:06:47 - INFO - __main__ - Step 740 Global step 740 Train loss 0.91 on epoch=184
05/18/2022 01:06:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.86 on epoch=187
05/18/2022 01:06:49 - INFO - __main__ - Global step 750 Train loss 0.88 Classification-F1 0.0974025974025974 on epoch=187
05/18/2022 01:06:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.87 on epoch=189
05/18/2022 01:06:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.97 on epoch=192
05/18/2022 01:06:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.88 on epoch=194
05/18/2022 01:06:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.85 on epoch=197
05/18/2022 01:06:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.96 on epoch=199
05/18/2022 01:06:56 - INFO - __main__ - Global step 800 Train loss 0.91 Classification-F1 0.1 on epoch=199
05/18/2022 01:06:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.96 on epoch=202
05/18/2022 01:06:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.90 on epoch=204
05/18/2022 01:07:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.92 on epoch=207
05/18/2022 01:07:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.85 on epoch=209
05/18/2022 01:07:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.83 on epoch=212
05/18/2022 01:07:03 - INFO - __main__ - Global step 850 Train loss 0.89 Classification-F1 0.09210526315789473 on epoch=212
05/18/2022 01:07:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.95 on epoch=214
05/18/2022 01:07:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.91 on epoch=217
05/18/2022 01:07:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.87 on epoch=219
05/18/2022 01:07:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.94 on epoch=222
05/18/2022 01:07:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.94 on epoch=224
05/18/2022 01:07:10 - INFO - __main__ - Global step 900 Train loss 0.92 Classification-F1 0.10126582278481013 on epoch=224
05/18/2022 01:07:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.88 on epoch=227
05/18/2022 01:07:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.89 on epoch=229
05/18/2022 01:07:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.88 on epoch=232
05/18/2022 01:07:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.88 on epoch=234
05/18/2022 01:07:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.92 on epoch=237
05/18/2022 01:07:16 - INFO - __main__ - Global step 950 Train loss 0.89 Classification-F1 0.10126582278481013 on epoch=237
05/18/2022 01:07:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.96 on epoch=239
05/18/2022 01:07:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.94 on epoch=242
05/18/2022 01:07:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.92 on epoch=244
05/18/2022 01:07:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.83 on epoch=247
05/18/2022 01:07:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.87 on epoch=249
05/18/2022 01:07:23 - INFO - __main__ - Global step 1000 Train loss 0.90 Classification-F1 0.10126582278481013 on epoch=249
05/18/2022 01:07:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.76 on epoch=252
05/18/2022 01:07:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.88 on epoch=254
05/18/2022 01:07:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.88 on epoch=257
05/18/2022 01:07:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.89 on epoch=259
05/18/2022 01:07:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.92 on epoch=262
05/18/2022 01:07:30 - INFO - __main__ - Global step 1050 Train loss 0.87 Classification-F1 0.0974025974025974 on epoch=262
05/18/2022 01:07:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.78 on epoch=264
05/18/2022 01:07:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.76 on epoch=267
05/18/2022 01:07:33 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.93 on epoch=269
05/18/2022 01:07:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.87 on epoch=272
05/18/2022 01:07:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.90 on epoch=274
05/18/2022 01:07:36 - INFO - __main__ - Global step 1100 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=274
05/18/2022 01:07:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.83 on epoch=277
05/18/2022 01:07:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.85 on epoch=279
05/18/2022 01:07:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.89 on epoch=282
05/18/2022 01:07:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.91 on epoch=284
05/18/2022 01:07:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.79 on epoch=287
05/18/2022 01:07:43 - INFO - __main__ - Global step 1150 Train loss 0.85 Classification-F1 0.0974025974025974 on epoch=287
05/18/2022 01:07:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.90 on epoch=289
05/18/2022 01:07:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.84 on epoch=292
05/18/2022 01:07:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.87 on epoch=294
05/18/2022 01:07:48 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.83 on epoch=297
05/18/2022 01:07:49 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.90 on epoch=299
05/18/2022 01:07:50 - INFO - __main__ - Global step 1200 Train loss 0.87 Classification-F1 0.10126582278481013 on epoch=299
05/18/2022 01:07:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.93 on epoch=302
05/18/2022 01:07:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.84 on epoch=304
05/18/2022 01:07:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.84 on epoch=307
05/18/2022 01:07:55 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.81 on epoch=309
05/18/2022 01:07:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.81 on epoch=312
05/18/2022 01:07:57 - INFO - __main__ - Global step 1250 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=312
05/18/2022 01:07:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.89 on epoch=314
05/18/2022 01:07:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.86 on epoch=317
05/18/2022 01:08:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.89 on epoch=319
05/18/2022 01:08:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.80 on epoch=322
05/18/2022 01:08:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.80 on epoch=324
05/18/2022 01:08:03 - INFO - __main__ - Global step 1300 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=324
05/18/2022 01:08:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.83 on epoch=327
05/18/2022 01:08:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.74 on epoch=329
05/18/2022 01:08:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.83 on epoch=332
05/18/2022 01:08:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.75 on epoch=334
05/18/2022 01:08:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.83 on epoch=337
05/18/2022 01:08:10 - INFO - __main__ - Global step 1350 Train loss 0.80 Classification-F1 0.10126582278481013 on epoch=337
05/18/2022 01:08:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.85 on epoch=339
05/18/2022 01:08:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.88 on epoch=342
05/18/2022 01:08:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.89 on epoch=344
05/18/2022 01:08:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.79 on epoch=347
05/18/2022 01:08:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.87 on epoch=349
05/18/2022 01:08:17 - INFO - __main__ - Global step 1400 Train loss 0.86 Classification-F1 0.09615384615384615 on epoch=349
05/18/2022 01:08:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.80 on epoch=352
05/18/2022 01:08:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.89 on epoch=354
05/18/2022 01:08:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.90 on epoch=357
05/18/2022 01:08:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.88 on epoch=359
05/18/2022 01:08:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.80 on epoch=362
05/18/2022 01:08:24 - INFO - __main__ - Global step 1450 Train loss 0.86 Classification-F1 0.09615384615384615 on epoch=362
05/18/2022 01:08:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.82 on epoch=364
05/18/2022 01:08:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.86 on epoch=367
05/18/2022 01:08:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.77 on epoch=369
05/18/2022 01:08:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.97 on epoch=372
05/18/2022 01:08:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.83 on epoch=374
05/18/2022 01:08:31 - INFO - __main__ - Global step 1500 Train loss 0.85 Classification-F1 0.09615384615384615 on epoch=374
05/18/2022 01:08:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.82 on epoch=377
05/18/2022 01:08:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.91 on epoch=379
05/18/2022 01:08:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.85 on epoch=382
05/18/2022 01:08:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.79 on epoch=384
05/18/2022 01:08:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.85 on epoch=387
05/18/2022 01:08:38 - INFO - __main__ - Global step 1550 Train loss 0.85 Classification-F1 0.09210526315789473 on epoch=387
05/18/2022 01:08:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.87 on epoch=389
05/18/2022 01:08:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.78 on epoch=392
05/18/2022 01:08:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.78 on epoch=394
05/18/2022 01:08:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.80 on epoch=397
05/18/2022 01:08:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.84 on epoch=399
05/18/2022 01:08:44 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.09615384615384615 on epoch=399
05/18/2022 01:08:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.80 on epoch=402
05/18/2022 01:08:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.80 on epoch=404
05/18/2022 01:08:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.88 on epoch=407
05/18/2022 01:08:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.84 on epoch=409
05/18/2022 01:08:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.76 on epoch=412
05/18/2022 01:08:51 - INFO - __main__ - Global step 1650 Train loss 0.82 Classification-F1 0.0974025974025974 on epoch=412
05/18/2022 01:08:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.84 on epoch=414
05/18/2022 01:08:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.85 on epoch=417
05/18/2022 01:08:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.84 on epoch=419
05/18/2022 01:08:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.91 on epoch=422
05/18/2022 01:08:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.85 on epoch=424
05/18/2022 01:08:58 - INFO - __main__ - Global step 1700 Train loss 0.86 Classification-F1 0.09210526315789473 on epoch=424
05/18/2022 01:08:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.85 on epoch=427
05/18/2022 01:09:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.80 on epoch=429
05/18/2022 01:09:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.77 on epoch=432
05/18/2022 01:09:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.88 on epoch=434
05/18/2022 01:09:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.81 on epoch=437
05/18/2022 01:09:05 - INFO - __main__ - Global step 1750 Train loss 0.82 Classification-F1 0.08666666666666666 on epoch=437
05/18/2022 01:09:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.89 on epoch=439
05/18/2022 01:09:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.71 on epoch=442
05/18/2022 01:09:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.79 on epoch=444
05/18/2022 01:09:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.82 on epoch=447
05/18/2022 01:09:11 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.84 on epoch=449
05/18/2022 01:09:12 - INFO - __main__ - Global step 1800 Train loss 0.81 Classification-F1 0.08783783783783784 on epoch=449
05/18/2022 01:09:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.79 on epoch=452
05/18/2022 01:09:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.79 on epoch=454
05/18/2022 01:09:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.79 on epoch=457
05/18/2022 01:09:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.78 on epoch=459
05/18/2022 01:09:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.83 on epoch=462
05/18/2022 01:09:19 - INFO - __main__ - Global step 1850 Train loss 0.79 Classification-F1 0.1237183868762816 on epoch=462
05/18/2022 01:09:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.80 on epoch=464
05/18/2022 01:09:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.80 on epoch=467
05/18/2022 01:09:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.75 on epoch=469
05/18/2022 01:09:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.78 on epoch=472
05/18/2022 01:09:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.89 on epoch=474
05/18/2022 01:09:26 - INFO - __main__ - Global step 1900 Train loss 0.80 Classification-F1 0.13034188034188032 on epoch=474
05/18/2022 01:09:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.89 on epoch=477
05/18/2022 01:09:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.79 on epoch=479
05/18/2022 01:09:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.88 on epoch=482
05/18/2022 01:09:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.83 on epoch=484
05/18/2022 01:09:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.71 on epoch=487
05/18/2022 01:09:32 - INFO - __main__ - Global step 1950 Train loss 0.82 Classification-F1 0.11444444444444443 on epoch=487
05/18/2022 01:09:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.82 on epoch=489
05/18/2022 01:09:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.84 on epoch=492
05/18/2022 01:09:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.79 on epoch=494
05/18/2022 01:09:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.76 on epoch=497
05/18/2022 01:09:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.75 on epoch=499
05/18/2022 01:09:39 - INFO - __main__ - Global step 2000 Train loss 0.79 Classification-F1 0.1237183868762816 on epoch=499
05/18/2022 01:09:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.81 on epoch=502
05/18/2022 01:09:42 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.85 on epoch=504
05/18/2022 01:09:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.82 on epoch=507
05/18/2022 01:09:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.90 on epoch=509
05/18/2022 01:09:45 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.86 on epoch=512
05/18/2022 01:09:46 - INFO - __main__ - Global step 2050 Train loss 0.85 Classification-F1 0.10571428571428572 on epoch=512
05/18/2022 01:09:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.71 on epoch=514
05/18/2022 01:09:48 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.80 on epoch=517
05/18/2022 01:09:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.86 on epoch=519
05/18/2022 01:09:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.87 on epoch=522
05/18/2022 01:09:52 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.78 on epoch=524
05/18/2022 01:09:53 - INFO - __main__ - Global step 2100 Train loss 0.80 Classification-F1 0.12393162393162392 on epoch=524
05/18/2022 01:09:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.79 on epoch=527
05/18/2022 01:09:55 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.88 on epoch=529
05/18/2022 01:09:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.78 on epoch=532
05/18/2022 01:09:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.72 on epoch=534
05/18/2022 01:09:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.73 on epoch=537
05/18/2022 01:09:59 - INFO - __main__ - Global step 2150 Train loss 0.78 Classification-F1 0.11404109589041095 on epoch=537
05/18/2022 01:10:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.78 on epoch=539
05/18/2022 01:10:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.82 on epoch=542
05/18/2022 01:10:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.83 on epoch=544
05/18/2022 01:10:04 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.76 on epoch=547
05/18/2022 01:10:05 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.92 on epoch=549
05/18/2022 01:10:06 - INFO - __main__ - Global step 2200 Train loss 0.82 Classification-F1 0.11283783783783785 on epoch=549
05/18/2022 01:10:07 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.75 on epoch=552
05/18/2022 01:10:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.73 on epoch=554
05/18/2022 01:10:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.80 on epoch=557
05/18/2022 01:10:11 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.77 on epoch=559
05/18/2022 01:10:12 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.81 on epoch=562
05/18/2022 01:10:13 - INFO - __main__ - Global step 2250 Train loss 0.77 Classification-F1 0.11833333333333335 on epoch=562
05/18/2022 01:10:14 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.84 on epoch=564
05/18/2022 01:10:15 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.87 on epoch=567
05/18/2022 01:10:16 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.81 on epoch=569
05/18/2022 01:10:18 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.80 on epoch=572
05/18/2022 01:10:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.76 on epoch=574
05/18/2022 01:10:19 - INFO - __main__ - Global step 2300 Train loss 0.82 Classification-F1 0.1237183868762816 on epoch=574
05/18/2022 01:10:21 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.78 on epoch=577
05/18/2022 01:10:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.80 on epoch=579
05/18/2022 01:10:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.78 on epoch=582
05/18/2022 01:10:24 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.85 on epoch=584
05/18/2022 01:10:26 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.80 on epoch=587
05/18/2022 01:10:26 - INFO - __main__ - Global step 2350 Train loss 0.80 Classification-F1 0.09615384615384615 on epoch=587
05/18/2022 01:10:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.83 on epoch=589
05/18/2022 01:10:29 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.79 on epoch=592
05/18/2022 01:10:30 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.85 on epoch=594
05/18/2022 01:10:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.83 on epoch=597
05/18/2022 01:10:32 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.81 on epoch=599
05/18/2022 01:10:33 - INFO - __main__ - Global step 2400 Train loss 0.82 Classification-F1 0.11710526315789474 on epoch=599
05/18/2022 01:10:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.83 on epoch=602
05/18/2022 01:10:35 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.73 on epoch=604
05/18/2022 01:10:36 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.79 on epoch=607
05/18/2022 01:10:38 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.82 on epoch=609
05/18/2022 01:10:39 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.76 on epoch=612
05/18/2022 01:10:39 - INFO - __main__ - Global step 2450 Train loss 0.78 Classification-F1 0.1237183868762816 on epoch=612
05/18/2022 01:10:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.77 on epoch=614
05/18/2022 01:10:42 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.80 on epoch=617
05/18/2022 01:10:43 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.84 on epoch=619
05/18/2022 01:10:44 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.79 on epoch=622
05/18/2022 01:10:46 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.83 on epoch=624
05/18/2022 01:10:46 - INFO - __main__ - Global step 2500 Train loss 0.80 Classification-F1 0.09493670886075949 on epoch=624
05/18/2022 01:10:47 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.76 on epoch=627
05/18/2022 01:10:49 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.87 on epoch=629
05/18/2022 01:10:50 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.84 on epoch=632
05/18/2022 01:10:51 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.77 on epoch=634
05/18/2022 01:10:52 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.86 on epoch=637
05/18/2022 01:10:53 - INFO - __main__ - Global step 2550 Train loss 0.82 Classification-F1 0.11842105263157894 on epoch=637
05/18/2022 01:10:54 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.87 on epoch=639
05/18/2022 01:10:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.79 on epoch=642
05/18/2022 01:10:57 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.80 on epoch=644
05/18/2022 01:10:58 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.83 on epoch=647
05/18/2022 01:10:59 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.77 on epoch=649
05/18/2022 01:11:00 - INFO - __main__ - Global step 2600 Train loss 0.81 Classification-F1 0.11404109589041095 on epoch=649
05/18/2022 01:11:01 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.81 on epoch=652
05/18/2022 01:11:02 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.78 on epoch=654
05/18/2022 01:11:03 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.83 on epoch=657
05/18/2022 01:11:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.84 on epoch=659
05/18/2022 01:11:06 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.84 on epoch=662
05/18/2022 01:11:06 - INFO - __main__ - Global step 2650 Train loss 0.82 Classification-F1 0.11336176261549394 on epoch=662
05/18/2022 01:11:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.78 on epoch=664
05/18/2022 01:11:09 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.84 on epoch=667
05/18/2022 01:11:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.81 on epoch=669
05/18/2022 01:11:11 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.76 on epoch=672
05/18/2022 01:11:12 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.78 on epoch=674
05/18/2022 01:11:13 - INFO - __main__ - Global step 2700 Train loss 0.79 Classification-F1 0.11840411840411841 on epoch=674
05/18/2022 01:11:14 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.77 on epoch=677
05/18/2022 01:11:15 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.78 on epoch=679
05/18/2022 01:11:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.72 on epoch=682
05/18/2022 01:11:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.83 on epoch=684
05/18/2022 01:11:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.79 on epoch=687
05/18/2022 01:11:20 - INFO - __main__ - Global step 2750 Train loss 0.78 Classification-F1 0.12222222222222223 on epoch=687
05/18/2022 01:11:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.86 on epoch=689
05/18/2022 01:11:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.77 on epoch=692
05/18/2022 01:11:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.76 on epoch=694
05/18/2022 01:11:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.78 on epoch=697
05/18/2022 01:11:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.81 on epoch=699
05/18/2022 01:11:26 - INFO - __main__ - Global step 2800 Train loss 0.80 Classification-F1 0.14267676767676768 on epoch=699
05/18/2022 01:11:26 - INFO - __main__ - Saving model with best Classification-F1: 0.14221364221364222 -> 0.14267676767676768 on epoch=699, global_step=2800
05/18/2022 01:11:27 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.85 on epoch=702
05/18/2022 01:11:29 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.82 on epoch=704
05/18/2022 01:11:30 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.75 on epoch=707
05/18/2022 01:11:31 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.78 on epoch=709
05/18/2022 01:11:32 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.79 on epoch=712
05/18/2022 01:11:33 - INFO - __main__ - Global step 2850 Train loss 0.80 Classification-F1 0.14134495641344957 on epoch=712
05/18/2022 01:11:34 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.73 on epoch=714
05/18/2022 01:11:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.79 on epoch=717
05/18/2022 01:11:37 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.83 on epoch=719
05/18/2022 01:11:38 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.86 on epoch=722
05/18/2022 01:11:39 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.74 on epoch=724
05/18/2022 01:11:40 - INFO - __main__ - Global step 2900 Train loss 0.79 Classification-F1 0.11840411840411841 on epoch=724
05/18/2022 01:11:41 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.83 on epoch=727
05/18/2022 01:11:42 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.78 on epoch=729
05/18/2022 01:11:43 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.74 on epoch=732
05/18/2022 01:11:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.79 on epoch=734
05/18/2022 01:11:46 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.79 on epoch=737
05/18/2022 01:11:46 - INFO - __main__ - Global step 2950 Train loss 0.79 Classification-F1 0.13666014350945857 on epoch=737
05/18/2022 01:11:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.75 on epoch=739
05/18/2022 01:11:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.76 on epoch=742
05/18/2022 01:11:50 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.76 on epoch=744
05/18/2022 01:11:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.76 on epoch=747
05/18/2022 01:11:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.80 on epoch=749
05/18/2022 01:11:53 - INFO - __main__ - Global step 3000 Train loss 0.76 Classification-F1 0.14004914004914004 on epoch=749
05/18/2022 01:11:53 - INFO - __main__ - save last model!
05/18/2022 01:11:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 01:11:53 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 01:11:53 - INFO - __main__ - Printing 3 examples
05/18/2022 01:11:53 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 01:11:53 - INFO - __main__ - ['others']
05/18/2022 01:11:53 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 01:11:53 - INFO - __main__ - ['others']
05/18/2022 01:11:53 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 01:11:53 - INFO - __main__ - ['others']
05/18/2022 01:11:53 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:11:54 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:11:54 - INFO - __main__ - Printing 3 examples
05/18/2022 01:11:54 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/18/2022 01:11:54 - INFO - __main__ - ['happy']
05/18/2022 01:11:54 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/18/2022 01:11:54 - INFO - __main__ - ['happy']
05/18/2022 01:11:54 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/18/2022 01:11:54 - INFO - __main__ - ['happy']
05/18/2022 01:11:54 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:11:54 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:11:54 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 01:11:54 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:11:54 - INFO - __main__ - Printing 3 examples
05/18/2022 01:11:54 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/18/2022 01:11:54 - INFO - __main__ - ['happy']
05/18/2022 01:11:54 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/18/2022 01:11:54 - INFO - __main__ - ['happy']
05/18/2022 01:11:54 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/18/2022 01:11:54 - INFO - __main__ - ['happy']
05/18/2022 01:11:54 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:11:54 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:11:54 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 01:11:55 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:12:00 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 01:12:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 01:12:00 - INFO - __main__ - Starting training!
05/18/2022 01:12:00 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 01:12:44 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_21_0.2_8_predictions.txt
05/18/2022 01:12:44 - INFO - __main__ - Classification-F1 on test data: 0.0714
05/18/2022 01:12:44 - INFO - __main__ - prefix=emo_16_21, lr=0.2, bsz=8, dev_performance=0.14267676767676768, test_performance=0.07144068400259924
05/18/2022 01:12:44 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.5, bsz=8 ...
05/18/2022 01:12:45 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:12:45 - INFO - __main__ - Printing 3 examples
05/18/2022 01:12:45 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/18/2022 01:12:45 - INFO - __main__ - ['happy']
05/18/2022 01:12:45 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/18/2022 01:12:45 - INFO - __main__ - ['happy']
05/18/2022 01:12:45 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/18/2022 01:12:45 - INFO - __main__ - ['happy']
05/18/2022 01:12:45 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:12:45 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:12:45 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 01:12:45 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:12:45 - INFO - __main__ - Printing 3 examples
05/18/2022 01:12:45 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/18/2022 01:12:45 - INFO - __main__ - ['happy']
05/18/2022 01:12:45 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/18/2022 01:12:45 - INFO - __main__ - ['happy']
05/18/2022 01:12:45 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/18/2022 01:12:45 - INFO - __main__ - ['happy']
05/18/2022 01:12:45 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:12:45 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:12:45 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 01:12:51 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 01:12:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 01:12:52 - INFO - __main__ - Starting training!
05/18/2022 01:12:53 - INFO - __main__ - Step 10 Global step 10 Train loss 4.11 on epoch=2
05/18/2022 01:12:54 - INFO - __main__ - Step 20 Global step 20 Train loss 3.58 on epoch=4
05/18/2022 01:12:56 - INFO - __main__ - Step 30 Global step 30 Train loss 2.87 on epoch=7
05/18/2022 01:12:57 - INFO - __main__ - Step 40 Global step 40 Train loss 2.44 on epoch=9
05/18/2022 01:12:58 - INFO - __main__ - Step 50 Global step 50 Train loss 1.96 on epoch=12
05/18/2022 01:12:59 - INFO - __main__ - Global step 50 Train loss 2.99 Classification-F1 0.0810126582278481 on epoch=12
05/18/2022 01:12:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0810126582278481 on epoch=12, global_step=50
05/18/2022 01:13:00 - INFO - __main__ - Step 60 Global step 60 Train loss 1.77 on epoch=14
05/18/2022 01:13:01 - INFO - __main__ - Step 70 Global step 70 Train loss 1.53 on epoch=17
05/18/2022 01:13:02 - INFO - __main__ - Step 80 Global step 80 Train loss 1.39 on epoch=19
05/18/2022 01:13:04 - INFO - __main__ - Step 90 Global step 90 Train loss 1.13 on epoch=22
05/18/2022 01:13:05 - INFO - __main__ - Step 100 Global step 100 Train loss 1.04 on epoch=24
05/18/2022 01:13:06 - INFO - __main__ - Global step 100 Train loss 1.37 Classification-F1 0.08311688311688312 on epoch=24
05/18/2022 01:13:06 - INFO - __main__ - Saving model with best Classification-F1: 0.0810126582278481 -> 0.08311688311688312 on epoch=24, global_step=100
05/18/2022 01:13:07 - INFO - __main__ - Step 110 Global step 110 Train loss 1.08 on epoch=27
05/18/2022 01:13:08 - INFO - __main__ - Step 120 Global step 120 Train loss 1.01 on epoch=29
05/18/2022 01:13:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.96 on epoch=32
05/18/2022 01:13:11 - INFO - __main__ - Step 140 Global step 140 Train loss 1.00 on epoch=34
05/18/2022 01:13:12 - INFO - __main__ - Step 150 Global step 150 Train loss 1.02 on epoch=37
05/18/2022 01:13:12 - INFO - __main__ - Global step 150 Train loss 1.01 Classification-F1 0.13067758749069247 on epoch=37
05/18/2022 01:13:12 - INFO - __main__ - Saving model with best Classification-F1: 0.08311688311688312 -> 0.13067758749069247 on epoch=37, global_step=150
05/18/2022 01:13:14 - INFO - __main__ - Step 160 Global step 160 Train loss 1.00 on epoch=39
05/18/2022 01:13:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.91 on epoch=42
05/18/2022 01:13:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=44
05/18/2022 01:13:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.90 on epoch=47
05/18/2022 01:13:19 - INFO - __main__ - Step 200 Global step 200 Train loss 1.00 on epoch=49
05/18/2022 01:13:19 - INFO - __main__ - Global step 200 Train loss 0.94 Classification-F1 0.13067758749069247 on epoch=49
05/18/2022 01:13:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.83 on epoch=52
05/18/2022 01:13:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.97 on epoch=54
05/18/2022 01:13:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.93 on epoch=57
05/18/2022 01:13:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.94 on epoch=59
05/18/2022 01:13:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.93 on epoch=62
05/18/2022 01:13:26 - INFO - __main__ - Global step 250 Train loss 0.92 Classification-F1 0.10126582278481013 on epoch=62
05/18/2022 01:13:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.99 on epoch=64
05/18/2022 01:13:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.90 on epoch=67
05/18/2022 01:13:30 - INFO - __main__ - Step 280 Global step 280 Train loss 1.00 on epoch=69
05/18/2022 01:13:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.90 on epoch=72
05/18/2022 01:13:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.86 on epoch=74
05/18/2022 01:13:33 - INFO - __main__ - Global step 300 Train loss 0.93 Classification-F1 0.10126582278481013 on epoch=74
05/18/2022 01:13:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.90 on epoch=77
05/18/2022 01:13:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.85 on epoch=79
05/18/2022 01:13:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.93 on epoch=82
05/18/2022 01:13:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.91 on epoch=84
05/18/2022 01:13:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.94 on epoch=87
05/18/2022 01:13:39 - INFO - __main__ - Global step 350 Train loss 0.91 Classification-F1 0.10126582278481013 on epoch=87
05/18/2022 01:13:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.88 on epoch=89
05/18/2022 01:13:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.84 on epoch=92
05/18/2022 01:13:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.94 on epoch=94
05/18/2022 01:13:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=97
05/18/2022 01:13:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.95 on epoch=99
05/18/2022 01:13:46 - INFO - __main__ - Global step 400 Train loss 0.90 Classification-F1 0.10126582278481013 on epoch=99
05/18/2022 01:13:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.93 on epoch=102
05/18/2022 01:13:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.90 on epoch=104
05/18/2022 01:13:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.93 on epoch=107
05/18/2022 01:13:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.91 on epoch=109
05/18/2022 01:13:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.84 on epoch=112
05/18/2022 01:13:53 - INFO - __main__ - Global step 450 Train loss 0.90 Classification-F1 0.13197586726998492 on epoch=112
05/18/2022 01:13:53 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.13197586726998492 on epoch=112, global_step=450
05/18/2022 01:13:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.84 on epoch=114
05/18/2022 01:13:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.87 on epoch=117
05/18/2022 01:13:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.86 on epoch=119
05/18/2022 01:13:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.90 on epoch=122
05/18/2022 01:13:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.73 on epoch=124
05/18/2022 01:14:00 - INFO - __main__ - Global step 500 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=124
05/18/2022 01:14:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.84 on epoch=127
05/18/2022 01:14:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.85 on epoch=129
05/18/2022 01:14:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.89 on epoch=132
05/18/2022 01:14:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.84 on epoch=134
05/18/2022 01:14:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.92 on epoch=137
05/18/2022 01:14:07 - INFO - __main__ - Global step 550 Train loss 0.87 Classification-F1 0.13067758749069247 on epoch=137
05/18/2022 01:14:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.83 on epoch=139
05/18/2022 01:14:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.91 on epoch=142
05/18/2022 01:14:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.86 on epoch=144
05/18/2022 01:14:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.86 on epoch=147
05/18/2022 01:14:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.83 on epoch=149
05/18/2022 01:14:13 - INFO - __main__ - Global step 600 Train loss 0.86 Classification-F1 0.10126582278481013 on epoch=149
05/18/2022 01:14:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.87 on epoch=152
05/18/2022 01:14:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.87 on epoch=154
05/18/2022 01:14:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.74 on epoch=157
05/18/2022 01:14:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.77 on epoch=159
05/18/2022 01:14:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.82 on epoch=162
05/18/2022 01:14:20 - INFO - __main__ - Global step 650 Train loss 0.81 Classification-F1 0.13067758749069247 on epoch=162
05/18/2022 01:14:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.88 on epoch=164
05/18/2022 01:14:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.80 on epoch=167
05/18/2022 01:14:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.88 on epoch=169
05/18/2022 01:14:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.82 on epoch=172
05/18/2022 01:14:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.92 on epoch=174
05/18/2022 01:14:27 - INFO - __main__ - Global step 700 Train loss 0.86 Classification-F1 0.09615384615384615 on epoch=174
05/18/2022 01:14:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.81 on epoch=177
05/18/2022 01:14:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.86 on epoch=179
05/18/2022 01:14:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.88 on epoch=182
05/18/2022 01:14:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.92 on epoch=184
05/18/2022 01:14:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.73 on epoch=187
05/18/2022 01:14:34 - INFO - __main__ - Global step 750 Train loss 0.84 Classification-F1 0.13067758749069247 on epoch=187
05/18/2022 01:14:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.80 on epoch=189
05/18/2022 01:14:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.79 on epoch=192
05/18/2022 01:14:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.85 on epoch=194
05/18/2022 01:14:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.86 on epoch=197
05/18/2022 01:14:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.98 on epoch=199
05/18/2022 01:14:41 - INFO - __main__ - Global step 800 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=199
05/18/2022 01:14:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.81 on epoch=202
05/18/2022 01:14:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.81 on epoch=204
05/18/2022 01:14:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.78 on epoch=207
05/18/2022 01:14:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.89 on epoch=209
05/18/2022 01:14:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.82 on epoch=212
05/18/2022 01:14:47 - INFO - __main__ - Global step 850 Train loss 0.82 Classification-F1 0.17773972602739727 on epoch=212
05/18/2022 01:14:47 - INFO - __main__ - Saving model with best Classification-F1: 0.13197586726998492 -> 0.17773972602739727 on epoch=212, global_step=850
05/18/2022 01:14:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.83 on epoch=214
05/18/2022 01:14:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.87 on epoch=217
05/18/2022 01:14:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.82 on epoch=219
05/18/2022 01:14:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.85 on epoch=222
05/18/2022 01:14:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.82 on epoch=224
05/18/2022 01:14:54 - INFO - __main__ - Global step 900 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=224
05/18/2022 01:14:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.78 on epoch=227
05/18/2022 01:14:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.84 on epoch=229
05/18/2022 01:14:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.81 on epoch=232
05/18/2022 01:14:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.85 on epoch=234
05/18/2022 01:15:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.86 on epoch=237
05/18/2022 01:15:01 - INFO - __main__ - Global step 950 Train loss 0.83 Classification-F1 0.10256410256410256 on epoch=237
05/18/2022 01:15:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.82 on epoch=239
05/18/2022 01:15:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.76 on epoch=242
05/18/2022 01:15:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.88 on epoch=244
05/18/2022 01:15:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.83 on epoch=247
05/18/2022 01:15:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.77 on epoch=249
05/18/2022 01:15:08 - INFO - __main__ - Global step 1000 Train loss 0.81 Classification-F1 0.10126582278481013 on epoch=249
05/18/2022 01:15:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.84 on epoch=252
05/18/2022 01:15:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.81 on epoch=254
05/18/2022 01:15:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.82 on epoch=257
05/18/2022 01:15:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.87 on epoch=259
05/18/2022 01:15:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.76 on epoch=262
05/18/2022 01:15:15 - INFO - __main__ - Global step 1050 Train loss 0.82 Classification-F1 0.15789473684210525 on epoch=262
05/18/2022 01:15:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.79 on epoch=264
05/18/2022 01:15:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.81 on epoch=267
05/18/2022 01:15:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.77 on epoch=269
05/18/2022 01:15:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.81 on epoch=272
05/18/2022 01:15:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.82 on epoch=274
05/18/2022 01:15:22 - INFO - __main__ - Global step 1100 Train loss 0.80 Classification-F1 0.1831081081081081 on epoch=274
05/18/2022 01:15:22 - INFO - __main__ - Saving model with best Classification-F1: 0.17773972602739727 -> 0.1831081081081081 on epoch=274, global_step=1100
05/18/2022 01:15:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.83 on epoch=277
05/18/2022 01:15:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.87 on epoch=279
05/18/2022 01:15:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.76 on epoch=282
05/18/2022 01:15:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.80 on epoch=284
05/18/2022 01:15:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.84 on epoch=287
05/18/2022 01:15:30 - INFO - __main__ - Global step 1150 Train loss 0.82 Classification-F1 0.15416666666666667 on epoch=287
05/18/2022 01:15:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.87 on epoch=289
05/18/2022 01:15:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.82 on epoch=292
05/18/2022 01:15:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.71 on epoch=294
05/18/2022 01:15:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.89 on epoch=297
05/18/2022 01:15:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.87 on epoch=299
05/18/2022 01:15:37 - INFO - __main__ - Global step 1200 Train loss 0.83 Classification-F1 0.15789473684210525 on epoch=299
05/18/2022 01:15:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.89 on epoch=302
05/18/2022 01:15:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.82 on epoch=304
05/18/2022 01:15:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.76 on epoch=307
05/18/2022 01:15:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.75 on epoch=309
05/18/2022 01:15:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.79 on epoch=312
05/18/2022 01:15:45 - INFO - __main__ - Global step 1250 Train loss 0.80 Classification-F1 0.18166666666666667 on epoch=312
05/18/2022 01:15:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.85 on epoch=314
05/18/2022 01:15:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.86 on epoch=317
05/18/2022 01:15:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.81 on epoch=319
05/18/2022 01:15:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.88 on epoch=322
05/18/2022 01:15:52 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.83 on epoch=324
05/18/2022 01:15:52 - INFO - __main__ - Global step 1300 Train loss 0.85 Classification-F1 0.18166666666666667 on epoch=324
05/18/2022 01:15:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.78 on epoch=327
05/18/2022 01:15:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.83 on epoch=329
05/18/2022 01:15:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.80 on epoch=332
05/18/2022 01:15:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.75 on epoch=334
05/18/2022 01:16:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.83 on epoch=337
05/18/2022 01:16:00 - INFO - __main__ - Global step 1350 Train loss 0.80 Classification-F1 0.19364881693648817 on epoch=337
05/18/2022 01:16:00 - INFO - __main__ - Saving model with best Classification-F1: 0.1831081081081081 -> 0.19364881693648817 on epoch=337, global_step=1350
05/18/2022 01:16:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.74 on epoch=339
05/18/2022 01:16:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.83 on epoch=342
05/18/2022 01:16:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.75 on epoch=344
05/18/2022 01:16:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.83 on epoch=347
05/18/2022 01:16:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.80 on epoch=349
05/18/2022 01:16:08 - INFO - __main__ - Global step 1400 Train loss 0.79 Classification-F1 0.19364881693648817 on epoch=349
05/18/2022 01:16:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.89 on epoch=352
05/18/2022 01:16:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.83 on epoch=354
05/18/2022 01:16:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.83 on epoch=357
05/18/2022 01:16:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.81 on epoch=359
05/18/2022 01:16:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.79 on epoch=362
05/18/2022 01:16:15 - INFO - __main__ - Global step 1450 Train loss 0.83 Classification-F1 0.22916666666666666 on epoch=362
05/18/2022 01:16:15 - INFO - __main__ - Saving model with best Classification-F1: 0.19364881693648817 -> 0.22916666666666666 on epoch=362, global_step=1450
05/18/2022 01:16:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.74 on epoch=364
05/18/2022 01:16:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.80 on epoch=367
05/18/2022 01:16:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.83 on epoch=369
05/18/2022 01:16:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.82 on epoch=372
05/18/2022 01:16:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.80 on epoch=374
05/18/2022 01:16:23 - INFO - __main__ - Global step 1500 Train loss 0.80 Classification-F1 0.20869565217391303 on epoch=374
05/18/2022 01:16:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.76 on epoch=377
05/18/2022 01:16:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.88 on epoch=379
05/18/2022 01:16:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.78 on epoch=382
05/18/2022 01:16:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.82 on epoch=384
05/18/2022 01:16:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.81 on epoch=387
05/18/2022 01:16:31 - INFO - __main__ - Global step 1550 Train loss 0.81 Classification-F1 0.2027582159624413 on epoch=387
05/18/2022 01:16:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.78 on epoch=389
05/18/2022 01:16:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.82 on epoch=392
05/18/2022 01:16:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.80 on epoch=394
05/18/2022 01:16:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.83 on epoch=397
05/18/2022 01:16:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.81 on epoch=399
05/18/2022 01:16:38 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.18813131313131315 on epoch=399
05/18/2022 01:16:40 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.78 on epoch=402
05/18/2022 01:16:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.85 on epoch=404
05/18/2022 01:16:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.79 on epoch=407
05/18/2022 01:16:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.74 on epoch=409
05/18/2022 01:16:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.72 on epoch=412
05/18/2022 01:16:46 - INFO - __main__ - Global step 1650 Train loss 0.78 Classification-F1 0.25071895424836604 on epoch=412
05/18/2022 01:16:46 - INFO - __main__ - Saving model with best Classification-F1: 0.22916666666666666 -> 0.25071895424836604 on epoch=412, global_step=1650
05/18/2022 01:16:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.75 on epoch=414
05/18/2022 01:16:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.77 on epoch=417
05/18/2022 01:16:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.74 on epoch=419
05/18/2022 01:16:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.76 on epoch=422
05/18/2022 01:16:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.80 on epoch=424
05/18/2022 01:16:53 - INFO - __main__ - Global step 1700 Train loss 0.76 Classification-F1 0.19797782126549252 on epoch=424
05/18/2022 01:16:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.80 on epoch=427
05/18/2022 01:16:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.81 on epoch=429
05/18/2022 01:16:57 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.79 on epoch=432
05/18/2022 01:16:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.83 on epoch=434
05/18/2022 01:16:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.80 on epoch=437
05/18/2022 01:17:00 - INFO - __main__ - Global step 1750 Train loss 0.81 Classification-F1 0.2027582159624413 on epoch=437
05/18/2022 01:17:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.82 on epoch=439
05/18/2022 01:17:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.80 on epoch=442
05/18/2022 01:17:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.82 on epoch=444
05/18/2022 01:17:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.80 on epoch=447
05/18/2022 01:17:06 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.82 on epoch=449
05/18/2022 01:17:06 - INFO - __main__ - Global step 1800 Train loss 0.81 Classification-F1 0.23337856173677068 on epoch=449
05/18/2022 01:17:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.79 on epoch=452
05/18/2022 01:17:09 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.82 on epoch=454
05/18/2022 01:17:10 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.85 on epoch=457
05/18/2022 01:17:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.77 on epoch=459
05/18/2022 01:17:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.82 on epoch=462
05/18/2022 01:17:13 - INFO - __main__ - Global step 1850 Train loss 0.81 Classification-F1 0.2571091908876669 on epoch=462
05/18/2022 01:17:13 - INFO - __main__ - Saving model with best Classification-F1: 0.25071895424836604 -> 0.2571091908876669 on epoch=462, global_step=1850
05/18/2022 01:17:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.79 on epoch=464
05/18/2022 01:17:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.77 on epoch=467
05/18/2022 01:17:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.78 on epoch=469
05/18/2022 01:17:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.87 on epoch=472
05/18/2022 01:17:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.82 on epoch=474
05/18/2022 01:17:20 - INFO - __main__ - Global step 1900 Train loss 0.81 Classification-F1 0.25071895424836604 on epoch=474
05/18/2022 01:17:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.75 on epoch=477
05/18/2022 01:17:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.80 on epoch=479
05/18/2022 01:17:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.74 on epoch=482
05/18/2022 01:17:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.78 on epoch=484
05/18/2022 01:17:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.80 on epoch=487
05/18/2022 01:17:27 - INFO - __main__ - Global step 1950 Train loss 0.77 Classification-F1 0.25545343137254906 on epoch=487
05/18/2022 01:17:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.77 on epoch=489
05/18/2022 01:17:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.80 on epoch=492
05/18/2022 01:17:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.79 on epoch=494
05/18/2022 01:17:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.78 on epoch=497
05/18/2022 01:17:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.77 on epoch=499
05/18/2022 01:17:34 - INFO - __main__ - Global step 2000 Train loss 0.78 Classification-F1 0.24701492537313433 on epoch=499
05/18/2022 01:17:35 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.83 on epoch=502
05/18/2022 01:17:36 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.87 on epoch=504
05/18/2022 01:17:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.81 on epoch=507
05/18/2022 01:17:39 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.79 on epoch=509
05/18/2022 01:17:40 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.82 on epoch=512
05/18/2022 01:17:41 - INFO - __main__ - Global step 2050 Train loss 0.83 Classification-F1 0.2558699282837214 on epoch=512
05/18/2022 01:17:42 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.78 on epoch=514
05/18/2022 01:17:43 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.76 on epoch=517
05/18/2022 01:17:44 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.77 on epoch=519
05/18/2022 01:17:46 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.83 on epoch=522
05/18/2022 01:17:47 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.80 on epoch=524
05/18/2022 01:17:47 - INFO - __main__ - Global step 2100 Train loss 0.79 Classification-F1 0.24018525334314808 on epoch=524
05/18/2022 01:17:49 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.79 on epoch=527
05/18/2022 01:17:50 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.75 on epoch=529
05/18/2022 01:17:51 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.74 on epoch=532
05/18/2022 01:17:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.70 on epoch=534
05/18/2022 01:17:54 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.86 on epoch=537
05/18/2022 01:17:54 - INFO - __main__ - Global step 2150 Train loss 0.77 Classification-F1 0.20870535714285715 on epoch=537
05/18/2022 01:17:55 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.79 on epoch=539
05/18/2022 01:17:57 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.80 on epoch=542
05/18/2022 01:17:58 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.83 on epoch=544
05/18/2022 01:17:59 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.77 on epoch=547
05/18/2022 01:18:00 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.76 on epoch=549
05/18/2022 01:18:01 - INFO - __main__ - Global step 2200 Train loss 0.79 Classification-F1 0.19888987130366442 on epoch=549
05/18/2022 01:18:02 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.73 on epoch=552
05/18/2022 01:18:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.82 on epoch=554
05/18/2022 01:18:05 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.75 on epoch=557
05/18/2022 01:18:06 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.76 on epoch=559
05/18/2022 01:18:07 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.74 on epoch=562
05/18/2022 01:18:08 - INFO - __main__ - Global step 2250 Train loss 0.76 Classification-F1 0.22817460317460317 on epoch=562
05/18/2022 01:18:09 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.76 on epoch=564
05/18/2022 01:18:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.82 on epoch=567
05/18/2022 01:18:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.77 on epoch=569
05/18/2022 01:18:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.84 on epoch=572
05/18/2022 01:18:14 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.78 on epoch=574
05/18/2022 01:18:14 - INFO - __main__ - Global step 2300 Train loss 0.80 Classification-F1 0.2327848376235473 on epoch=574
05/18/2022 01:18:16 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.80 on epoch=577
05/18/2022 01:18:17 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.78 on epoch=579
05/18/2022 01:18:18 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.80 on epoch=582
05/18/2022 01:18:20 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.75 on epoch=584
05/18/2022 01:18:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.80 on epoch=587
05/18/2022 01:18:21 - INFO - __main__ - Global step 2350 Train loss 0.79 Classification-F1 0.20491373360938575 on epoch=587
05/18/2022 01:18:23 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.80 on epoch=589
05/18/2022 01:18:24 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.84 on epoch=592
05/18/2022 01:18:25 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.83 on epoch=594
05/18/2022 01:18:26 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.81 on epoch=597
05/18/2022 01:18:28 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.76 on epoch=599
05/18/2022 01:18:28 - INFO - __main__ - Global step 2400 Train loss 0.81 Classification-F1 0.21399215104601216 on epoch=599
05/18/2022 01:18:29 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.76 on epoch=602
05/18/2022 01:18:31 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.75 on epoch=604
05/18/2022 01:18:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.81 on epoch=607
05/18/2022 01:18:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.75 on epoch=609
05/18/2022 01:18:34 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.89 on epoch=612
05/18/2022 01:18:35 - INFO - __main__ - Global step 2450 Train loss 0.79 Classification-F1 0.2796652675483752 on epoch=612
05/18/2022 01:18:35 - INFO - __main__ - Saving model with best Classification-F1: 0.2571091908876669 -> 0.2796652675483752 on epoch=612, global_step=2450
05/18/2022 01:18:36 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.74 on epoch=614
05/18/2022 01:18:37 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.80 on epoch=617
05/18/2022 01:18:39 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.79 on epoch=619
05/18/2022 01:18:40 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.80 on epoch=622
05/18/2022 01:18:41 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.76 on epoch=624
05/18/2022 01:18:42 - INFO - __main__ - Global step 2500 Train loss 0.78 Classification-F1 0.2256720430107527 on epoch=624
05/18/2022 01:18:43 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.83 on epoch=627
05/18/2022 01:18:44 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.77 on epoch=629
05/18/2022 01:18:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.82 on epoch=632
05/18/2022 01:18:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.73 on epoch=634
05/18/2022 01:18:48 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.80 on epoch=637
05/18/2022 01:18:48 - INFO - __main__ - Global step 2550 Train loss 0.79 Classification-F1 0.26359320339830083 on epoch=637
05/18/2022 01:18:50 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.74 on epoch=639
05/18/2022 01:18:51 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.80 on epoch=642
05/18/2022 01:18:52 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.78 on epoch=644
05/18/2022 01:18:54 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.80 on epoch=647
05/18/2022 01:18:55 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.82 on epoch=649
05/18/2022 01:18:55 - INFO - __main__ - Global step 2600 Train loss 0.79 Classification-F1 0.2558699282837214 on epoch=649
05/18/2022 01:18:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.80 on epoch=652
05/18/2022 01:18:58 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.77 on epoch=654
05/18/2022 01:18:59 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.76 on epoch=657
05/18/2022 01:19:00 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.76 on epoch=659
05/18/2022 01:19:02 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.76 on epoch=662
05/18/2022 01:19:02 - INFO - __main__ - Global step 2650 Train loss 0.77 Classification-F1 0.24918558217738546 on epoch=662
05/18/2022 01:19:03 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.76 on epoch=664
05/18/2022 01:19:05 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.81 on epoch=667
05/18/2022 01:19:06 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.78 on epoch=669
05/18/2022 01:19:07 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.84 on epoch=672
05/18/2022 01:19:08 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.74 on epoch=674
05/18/2022 01:19:09 - INFO - __main__ - Global step 2700 Train loss 0.79 Classification-F1 0.23663682864450128 on epoch=674
05/18/2022 01:19:10 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.79 on epoch=677
05/18/2022 01:19:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.74 on epoch=679
05/18/2022 01:19:13 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.87 on epoch=682
05/18/2022 01:19:14 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.80 on epoch=684
05/18/2022 01:19:15 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.81 on epoch=687
05/18/2022 01:19:16 - INFO - __main__ - Global step 2750 Train loss 0.80 Classification-F1 0.26650537634408605 on epoch=687
05/18/2022 01:19:17 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.78 on epoch=689
05/18/2022 01:19:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.78 on epoch=692
05/18/2022 01:19:20 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.82 on epoch=694
05/18/2022 01:19:21 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.77 on epoch=697
05/18/2022 01:19:22 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.79 on epoch=699
05/18/2022 01:19:23 - INFO - __main__ - Global step 2800 Train loss 0.79 Classification-F1 0.19225302061122956 on epoch=699
05/18/2022 01:19:24 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.79 on epoch=702
05/18/2022 01:19:25 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.84 on epoch=704
05/18/2022 01:19:26 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.76 on epoch=707
05/18/2022 01:19:28 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.77 on epoch=709
05/18/2022 01:19:29 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.76 on epoch=712
05/18/2022 01:19:29 - INFO - __main__ - Global step 2850 Train loss 0.78 Classification-F1 0.3316520467836257 on epoch=712
05/18/2022 01:19:29 - INFO - __main__ - Saving model with best Classification-F1: 0.2796652675483752 -> 0.3316520467836257 on epoch=712, global_step=2850
05/18/2022 01:19:31 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.80 on epoch=714
05/18/2022 01:19:32 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.75 on epoch=717
05/18/2022 01:19:33 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.77 on epoch=719
05/18/2022 01:19:34 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.82 on epoch=722
05/18/2022 01:19:36 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.77 on epoch=724
05/18/2022 01:19:36 - INFO - __main__ - Global step 2900 Train loss 0.78 Classification-F1 0.35618659420289855 on epoch=724
05/18/2022 01:19:36 - INFO - __main__ - Saving model with best Classification-F1: 0.3316520467836257 -> 0.35618659420289855 on epoch=724, global_step=2900
05/18/2022 01:19:37 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.82 on epoch=727
05/18/2022 01:19:39 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.80 on epoch=729
05/18/2022 01:19:40 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.76 on epoch=732
05/18/2022 01:19:41 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.81 on epoch=734
05/18/2022 01:19:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.81 on epoch=737
05/18/2022 01:19:43 - INFO - __main__ - Global step 2950 Train loss 0.80 Classification-F1 0.24621212121212122 on epoch=737
05/18/2022 01:19:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.83 on epoch=739
05/18/2022 01:19:45 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.76 on epoch=742
05/18/2022 01:19:47 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.77 on epoch=744
05/18/2022 01:19:48 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.70 on epoch=747
05/18/2022 01:19:49 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.77 on epoch=749
05/18/2022 01:19:50 - INFO - __main__ - Global step 3000 Train loss 0.77 Classification-F1 0.2856534090909091 on epoch=749
05/18/2022 01:19:50 - INFO - __main__ - save last model!
05/18/2022 01:19:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 01:19:50 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 01:19:50 - INFO - __main__ - Printing 3 examples
05/18/2022 01:19:50 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 01:19:50 - INFO - __main__ - ['others']
05/18/2022 01:19:50 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 01:19:50 - INFO - __main__ - ['others']
05/18/2022 01:19:50 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 01:19:50 - INFO - __main__ - ['others']
05/18/2022 01:19:50 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:19:51 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:19:51 - INFO - __main__ - Printing 3 examples
05/18/2022 01:19:51 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/18/2022 01:19:51 - INFO - __main__ - ['happy']
05/18/2022 01:19:51 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/18/2022 01:19:51 - INFO - __main__ - ['happy']
05/18/2022 01:19:51 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/18/2022 01:19:51 - INFO - __main__ - ['happy']
05/18/2022 01:19:51 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:19:51 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:19:51 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 01:19:51 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:19:51 - INFO - __main__ - Printing 3 examples
05/18/2022 01:19:51 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/18/2022 01:19:51 - INFO - __main__ - ['happy']
05/18/2022 01:19:51 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/18/2022 01:19:51 - INFO - __main__ - ['happy']
05/18/2022 01:19:51 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/18/2022 01:19:51 - INFO - __main__ - ['happy']
05/18/2022 01:19:51 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:19:51 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:19:51 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 01:19:52 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:19:57 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 01:19:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 01:19:57 - INFO - __main__ - Starting training!
05/18/2022 01:19:57 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 01:20:41 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_42_0.5_8_predictions.txt
05/18/2022 01:20:41 - INFO - __main__ - Classification-F1 on test data: 0.1648
05/18/2022 01:20:41 - INFO - __main__ - prefix=emo_16_42, lr=0.5, bsz=8, dev_performance=0.35618659420289855, test_performance=0.16480105501429312
05/18/2022 01:20:41 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.4, bsz=8 ...
05/18/2022 01:20:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:20:42 - INFO - __main__ - Printing 3 examples
05/18/2022 01:20:42 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/18/2022 01:20:42 - INFO - __main__ - ['happy']
05/18/2022 01:20:42 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/18/2022 01:20:42 - INFO - __main__ - ['happy']
05/18/2022 01:20:42 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/18/2022 01:20:42 - INFO - __main__ - ['happy']
05/18/2022 01:20:42 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:20:42 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:20:42 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 01:20:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:20:42 - INFO - __main__ - Printing 3 examples
05/18/2022 01:20:42 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/18/2022 01:20:42 - INFO - __main__ - ['happy']
05/18/2022 01:20:42 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/18/2022 01:20:42 - INFO - __main__ - ['happy']
05/18/2022 01:20:42 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/18/2022 01:20:42 - INFO - __main__ - ['happy']
05/18/2022 01:20:42 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:20:42 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:20:42 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 01:20:48 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 01:20:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 01:20:49 - INFO - __main__ - Starting training!
05/18/2022 01:20:51 - INFO - __main__ - Step 10 Global step 10 Train loss 4.23 on epoch=2
05/18/2022 01:20:53 - INFO - __main__ - Step 20 Global step 20 Train loss 3.83 on epoch=4
05/18/2022 01:20:54 - INFO - __main__ - Step 30 Global step 30 Train loss 3.15 on epoch=7
05/18/2022 01:20:55 - INFO - __main__ - Step 40 Global step 40 Train loss 2.78 on epoch=9
05/18/2022 01:20:56 - INFO - __main__ - Step 50 Global step 50 Train loss 2.27 on epoch=12
05/18/2022 01:20:57 - INFO - __main__ - Global step 50 Train loss 3.25 Classification-F1 0.0810126582278481 on epoch=12
05/18/2022 01:20:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0810126582278481 on epoch=12, global_step=50
05/18/2022 01:20:58 - INFO - __main__ - Step 60 Global step 60 Train loss 1.91 on epoch=14
05/18/2022 01:20:59 - INFO - __main__ - Step 70 Global step 70 Train loss 1.60 on epoch=17
05/18/2022 01:21:01 - INFO - __main__ - Step 80 Global step 80 Train loss 1.62 on epoch=19
05/18/2022 01:21:02 - INFO - __main__ - Step 90 Global step 90 Train loss 1.35 on epoch=22
05/18/2022 01:21:03 - INFO - __main__ - Step 100 Global step 100 Train loss 1.26 on epoch=24
05/18/2022 01:21:04 - INFO - __main__ - Global step 100 Train loss 1.55 Classification-F1 0.0810126582278481 on epoch=24
05/18/2022 01:21:05 - INFO - __main__ - Step 110 Global step 110 Train loss 1.22 on epoch=27
05/18/2022 01:21:06 - INFO - __main__ - Step 120 Global step 120 Train loss 1.08 on epoch=29
05/18/2022 01:21:07 - INFO - __main__ - Step 130 Global step 130 Train loss 0.96 on epoch=32
05/18/2022 01:21:09 - INFO - __main__ - Step 140 Global step 140 Train loss 1.08 on epoch=34
05/18/2022 01:21:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.94 on epoch=37
05/18/2022 01:21:10 - INFO - __main__ - Global step 150 Train loss 1.05 Classification-F1 0.17368421052631577 on epoch=37
05/18/2022 01:21:10 - INFO - __main__ - Saving model with best Classification-F1: 0.0810126582278481 -> 0.17368421052631577 on epoch=37, global_step=150
05/18/2022 01:21:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.95 on epoch=39
05/18/2022 01:21:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.98 on epoch=42
05/18/2022 01:21:14 - INFO - __main__ - Step 180 Global step 180 Train loss 1.05 on epoch=44
05/18/2022 01:21:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.90 on epoch=47
05/18/2022 01:21:17 - INFO - __main__ - Step 200 Global step 200 Train loss 1.04 on epoch=49
05/18/2022 01:21:17 - INFO - __main__ - Global step 200 Train loss 0.98 Classification-F1 0.13067758749069247 on epoch=49
05/18/2022 01:21:18 - INFO - __main__ - Step 210 Global step 210 Train loss 1.03 on epoch=52
05/18/2022 01:21:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.94 on epoch=54
05/18/2022 01:21:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.86 on epoch=57
05/18/2022 01:21:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.90 on epoch=59
05/18/2022 01:21:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.98 on epoch=62
05/18/2022 01:21:24 - INFO - __main__ - Global step 250 Train loss 0.94 Classification-F1 0.20526315789473687 on epoch=62
05/18/2022 01:21:24 - INFO - __main__ - Saving model with best Classification-F1: 0.17368421052631577 -> 0.20526315789473687 on epoch=62, global_step=250
05/18/2022 01:21:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.91 on epoch=64
05/18/2022 01:21:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.87 on epoch=67
05/18/2022 01:21:28 - INFO - __main__ - Step 280 Global step 280 Train loss 1.04 on epoch=69
05/18/2022 01:21:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.87 on epoch=72
05/18/2022 01:21:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.87 on epoch=74
05/18/2022 01:21:31 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.10126582278481013 on epoch=74
05/18/2022 01:21:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.95 on epoch=77
05/18/2022 01:21:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.95 on epoch=79
05/18/2022 01:21:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.85 on epoch=82
05/18/2022 01:21:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.90 on epoch=84
05/18/2022 01:21:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.97 on epoch=87
05/18/2022 01:21:37 - INFO - __main__ - Global step 350 Train loss 0.93 Classification-F1 0.1770048309178744 on epoch=87
05/18/2022 01:21:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.90 on epoch=89
05/18/2022 01:21:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.91 on epoch=92
05/18/2022 01:21:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.90 on epoch=94
05/18/2022 01:21:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.87 on epoch=97
05/18/2022 01:21:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.93 on epoch=99
05/18/2022 01:21:44 - INFO - __main__ - Global step 400 Train loss 0.90 Classification-F1 0.10126582278481013 on epoch=99
05/18/2022 01:21:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.90 on epoch=102
05/18/2022 01:21:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.94 on epoch=104
05/18/2022 01:21:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.97 on epoch=107
05/18/2022 01:21:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.84 on epoch=109
05/18/2022 01:21:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.89 on epoch=112
05/18/2022 01:21:51 - INFO - __main__ - Global step 450 Train loss 0.91 Classification-F1 0.1609907120743034 on epoch=112
05/18/2022 01:21:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.90 on epoch=114
05/18/2022 01:21:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.90 on epoch=117
05/18/2022 01:21:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.84 on epoch=119
05/18/2022 01:21:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.86 on epoch=122
05/18/2022 01:21:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.90 on epoch=124
05/18/2022 01:21:57 - INFO - __main__ - Global step 500 Train loss 0.88 Classification-F1 0.10126582278481013 on epoch=124
05/18/2022 01:21:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.86 on epoch=127
05/18/2022 01:22:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.87 on epoch=129
05/18/2022 01:22:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.79 on epoch=132
05/18/2022 01:22:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.89 on epoch=134
05/18/2022 01:22:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.88 on epoch=137
05/18/2022 01:22:04 - INFO - __main__ - Global step 550 Train loss 0.86 Classification-F1 0.13067758749069247 on epoch=137
05/18/2022 01:22:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.89 on epoch=139
05/18/2022 01:22:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.77 on epoch=142
05/18/2022 01:22:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.92 on epoch=144
05/18/2022 01:22:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.82 on epoch=147
05/18/2022 01:22:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.77 on epoch=149
05/18/2022 01:22:10 - INFO - __main__ - Global step 600 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=149
05/18/2022 01:22:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.85 on epoch=152
05/18/2022 01:22:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.84 on epoch=154
05/18/2022 01:22:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.95 on epoch=157
05/18/2022 01:22:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.83 on epoch=159
05/18/2022 01:22:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.82 on epoch=162
05/18/2022 01:22:17 - INFO - __main__ - Global step 650 Train loss 0.86 Classification-F1 0.10256410256410256 on epoch=162
05/18/2022 01:22:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.83 on epoch=164
05/18/2022 01:22:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.92 on epoch=167
05/18/2022 01:22:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.83 on epoch=169
05/18/2022 01:22:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.86 on epoch=172
05/18/2022 01:22:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.84 on epoch=174
05/18/2022 01:22:24 - INFO - __main__ - Global step 700 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=174
05/18/2022 01:22:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.84 on epoch=177
05/18/2022 01:22:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.81 on epoch=179
05/18/2022 01:22:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.85 on epoch=182
05/18/2022 01:22:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.83 on epoch=184
05/18/2022 01:22:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.87 on epoch=187
05/18/2022 01:22:30 - INFO - __main__ - Global step 750 Train loss 0.84 Classification-F1 0.10389610389610389 on epoch=187
05/18/2022 01:22:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.84 on epoch=189
05/18/2022 01:22:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.86 on epoch=192
05/18/2022 01:22:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.78 on epoch=194
05/18/2022 01:22:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.73 on epoch=197
05/18/2022 01:22:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.81 on epoch=199
05/18/2022 01:22:37 - INFO - __main__ - Global step 800 Train loss 0.80 Classification-F1 0.1346749226006192 on epoch=199
05/18/2022 01:22:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.88 on epoch=202
05/18/2022 01:22:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.93 on epoch=204
05/18/2022 01:22:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=207
05/18/2022 01:22:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.77 on epoch=209
05/18/2022 01:22:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.85 on epoch=212
05/18/2022 01:22:43 - INFO - __main__ - Global step 850 Train loss 0.85 Classification-F1 0.15789473684210525 on epoch=212
05/18/2022 01:22:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.85 on epoch=214
05/18/2022 01:22:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.83 on epoch=217
05/18/2022 01:22:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.83 on epoch=219
05/18/2022 01:22:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.87 on epoch=222
05/18/2022 01:22:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.78 on epoch=224
05/18/2022 01:22:50 - INFO - __main__ - Global step 900 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=224
05/18/2022 01:22:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.83 on epoch=227
05/18/2022 01:22:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.84 on epoch=229
05/18/2022 01:22:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.88 on epoch=232
05/18/2022 01:22:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.86 on epoch=234
05/18/2022 01:22:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.82 on epoch=237
05/18/2022 01:22:57 - INFO - __main__ - Global step 950 Train loss 0.84 Classification-F1 0.10526315789473685 on epoch=237
05/18/2022 01:22:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.83 on epoch=239
05/18/2022 01:22:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.82 on epoch=242
05/18/2022 01:23:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.79 on epoch=244
05/18/2022 01:23:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.89 on epoch=247
05/18/2022 01:23:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.81 on epoch=249
05/18/2022 01:23:03 - INFO - __main__ - Global step 1000 Train loss 0.83 Classification-F1 0.10256410256410256 on epoch=249
05/18/2022 01:23:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.81 on epoch=252
05/18/2022 01:23:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.84 on epoch=254
05/18/2022 01:23:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.81 on epoch=257
05/18/2022 01:23:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.82 on epoch=259
05/18/2022 01:23:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.82 on epoch=262
05/18/2022 01:23:10 - INFO - __main__ - Global step 1050 Train loss 0.82 Classification-F1 0.15945165945165946 on epoch=262
05/18/2022 01:23:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.77 on epoch=264
05/18/2022 01:23:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.80 on epoch=267
05/18/2022 01:23:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.77 on epoch=269
05/18/2022 01:23:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.77 on epoch=272
05/18/2022 01:23:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.78 on epoch=274
05/18/2022 01:23:16 - INFO - __main__ - Global step 1100 Train loss 0.78 Classification-F1 0.15555555555555556 on epoch=274
05/18/2022 01:23:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.80 on epoch=277
05/18/2022 01:23:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.80 on epoch=279
05/18/2022 01:23:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.81 on epoch=282
05/18/2022 01:23:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.80 on epoch=284
05/18/2022 01:23:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.87 on epoch=287
05/18/2022 01:23:23 - INFO - __main__ - Global step 1150 Train loss 0.81 Classification-F1 0.13166666666666668 on epoch=287
05/18/2022 01:23:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.83 on epoch=289
05/18/2022 01:23:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.83 on epoch=292
05/18/2022 01:23:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.88 on epoch=294
05/18/2022 01:23:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.80 on epoch=297
05/18/2022 01:23:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.76 on epoch=299
05/18/2022 01:23:30 - INFO - __main__ - Global step 1200 Train loss 0.82 Classification-F1 0.13197586726998492 on epoch=299
05/18/2022 01:23:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.83 on epoch=302
05/18/2022 01:23:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.80 on epoch=304
05/18/2022 01:23:33 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.81 on epoch=307
05/18/2022 01:23:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.83 on epoch=309
05/18/2022 01:23:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.75 on epoch=312
05/18/2022 01:23:36 - INFO - __main__ - Global step 1250 Train loss 0.80 Classification-F1 0.13167388167388167 on epoch=312
05/18/2022 01:23:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.81 on epoch=314
05/18/2022 01:23:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.75 on epoch=317
05/18/2022 01:23:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.85 on epoch=319
05/18/2022 01:23:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.74 on epoch=322
05/18/2022 01:23:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.81 on epoch=324
05/18/2022 01:23:43 - INFO - __main__ - Global step 1300 Train loss 0.79 Classification-F1 0.13330786860198623 on epoch=324
05/18/2022 01:23:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.85 on epoch=327
05/18/2022 01:23:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.85 on epoch=329
05/18/2022 01:23:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.76 on epoch=332
05/18/2022 01:23:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.77 on epoch=334
05/18/2022 01:23:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.84 on epoch=337
05/18/2022 01:23:50 - INFO - __main__ - Global step 1350 Train loss 0.81 Classification-F1 0.20334620334620335 on epoch=337
05/18/2022 01:23:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.78 on epoch=339
05/18/2022 01:23:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.83 on epoch=342
05/18/2022 01:23:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.88 on epoch=344
05/18/2022 01:23:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.80 on epoch=347
05/18/2022 01:23:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.83 on epoch=349
05/18/2022 01:23:56 - INFO - __main__ - Global step 1400 Train loss 0.83 Classification-F1 0.15945165945165946 on epoch=349
05/18/2022 01:23:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.88 on epoch=352
05/18/2022 01:23:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.75 on epoch=354
05/18/2022 01:24:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.86 on epoch=357
05/18/2022 01:24:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.76 on epoch=359
05/18/2022 01:24:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.80 on epoch=362
05/18/2022 01:24:03 - INFO - __main__ - Global step 1450 Train loss 0.81 Classification-F1 0.1965894465894466 on epoch=362
05/18/2022 01:24:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.83 on epoch=364
05/18/2022 01:24:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.78 on epoch=367
05/18/2022 01:24:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.86 on epoch=369
05/18/2022 01:24:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.84 on epoch=372
05/18/2022 01:24:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.83 on epoch=374
05/18/2022 01:24:10 - INFO - __main__ - Global step 1500 Train loss 0.83 Classification-F1 0.15945165945165946 on epoch=374
05/18/2022 01:24:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.79 on epoch=377
05/18/2022 01:24:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.84 on epoch=379
05/18/2022 01:24:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.85 on epoch=382
05/18/2022 01:24:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.86 on epoch=384
05/18/2022 01:24:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.79 on epoch=387
05/18/2022 01:24:16 - INFO - __main__ - Global step 1550 Train loss 0.82 Classification-F1 0.13607843137254902 on epoch=387
05/18/2022 01:24:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.84 on epoch=389
05/18/2022 01:24:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.78 on epoch=392
05/18/2022 01:24:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.87 on epoch=394
05/18/2022 01:24:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.80 on epoch=397
05/18/2022 01:24:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.83 on epoch=399
05/18/2022 01:24:23 - INFO - __main__ - Global step 1600 Train loss 0.82 Classification-F1 0.16081871345029242 on epoch=399
05/18/2022 01:24:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.85 on epoch=402
05/18/2022 01:24:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.79 on epoch=404
05/18/2022 01:24:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.77 on epoch=407
05/18/2022 01:24:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.83 on epoch=409
05/18/2022 01:24:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.83 on epoch=412
05/18/2022 01:24:29 - INFO - __main__ - Global step 1650 Train loss 0.81 Classification-F1 0.17687437757860294 on epoch=412
05/18/2022 01:24:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.88 on epoch=414
05/18/2022 01:24:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.84 on epoch=417
05/18/2022 01:24:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.78 on epoch=419
05/18/2022 01:24:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.80 on epoch=422
05/18/2022 01:24:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.85 on epoch=424
05/18/2022 01:24:36 - INFO - __main__ - Global step 1700 Train loss 0.83 Classification-F1 0.2013484415263511 on epoch=424
05/18/2022 01:24:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.81 on epoch=427
05/18/2022 01:24:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.79 on epoch=429
05/18/2022 01:24:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.88 on epoch=432
05/18/2022 01:24:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.81 on epoch=434
05/18/2022 01:24:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.78 on epoch=437
05/18/2022 01:24:43 - INFO - __main__ - Global step 1750 Train loss 0.81 Classification-F1 0.2238749046529367 on epoch=437
05/18/2022 01:24:43 - INFO - __main__ - Saving model with best Classification-F1: 0.20526315789473687 -> 0.2238749046529367 on epoch=437, global_step=1750
05/18/2022 01:24:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.80 on epoch=439
05/18/2022 01:24:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.77 on epoch=442
05/18/2022 01:24:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.84 on epoch=444
05/18/2022 01:24:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.77 on epoch=447
05/18/2022 01:24:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.81 on epoch=449
05/18/2022 01:24:49 - INFO - __main__ - Global step 1800 Train loss 0.80 Classification-F1 0.20666666666666667 on epoch=449
05/18/2022 01:24:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.76 on epoch=452
05/18/2022 01:24:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.87 on epoch=454
05/18/2022 01:24:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.80 on epoch=457
05/18/2022 01:24:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.79 on epoch=459
05/18/2022 01:24:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.76 on epoch=462
05/18/2022 01:24:56 - INFO - __main__ - Global step 1850 Train loss 0.79 Classification-F1 0.1842105263157895 on epoch=462
05/18/2022 01:24:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.85 on epoch=464
05/18/2022 01:24:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.77 on epoch=467
05/18/2022 01:25:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.84 on epoch=469
05/18/2022 01:25:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.77 on epoch=472
05/18/2022 01:25:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.81 on epoch=474
05/18/2022 01:25:03 - INFO - __main__ - Global step 1900 Train loss 0.81 Classification-F1 0.1765873015873016 on epoch=474
05/18/2022 01:25:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.84 on epoch=477
05/18/2022 01:25:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.80 on epoch=479
05/18/2022 01:25:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.78 on epoch=482
05/18/2022 01:25:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.81 on epoch=484
05/18/2022 01:25:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.80 on epoch=487
05/18/2022 01:25:09 - INFO - __main__ - Global step 1950 Train loss 0.81 Classification-F1 0.1965894465894466 on epoch=487
05/18/2022 01:25:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.77 on epoch=489
05/18/2022 01:25:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.80 on epoch=492
05/18/2022 01:25:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.79 on epoch=494
05/18/2022 01:25:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.82 on epoch=497
05/18/2022 01:25:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.80 on epoch=499
05/18/2022 01:25:16 - INFO - __main__ - Global step 2000 Train loss 0.80 Classification-F1 0.22353801169590645 on epoch=499
05/18/2022 01:25:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.79 on epoch=502
05/18/2022 01:25:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.79 on epoch=504
05/18/2022 01:25:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.84 on epoch=507
05/18/2022 01:25:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.82 on epoch=509
05/18/2022 01:25:22 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.83 on epoch=512
05/18/2022 01:25:23 - INFO - __main__ - Global step 2050 Train loss 0.82 Classification-F1 0.21727841798264336 on epoch=512
05/18/2022 01:25:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.86 on epoch=514
05/18/2022 01:25:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.85 on epoch=517
05/18/2022 01:25:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.80 on epoch=519
05/18/2022 01:25:27 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.80 on epoch=522
05/18/2022 01:25:29 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.76 on epoch=524
05/18/2022 01:25:29 - INFO - __main__ - Global step 2100 Train loss 0.81 Classification-F1 0.18679950186799504 on epoch=524
05/18/2022 01:25:30 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.78 on epoch=527
05/18/2022 01:25:32 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.70 on epoch=529
05/18/2022 01:25:33 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.84 on epoch=532
05/18/2022 01:25:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.76 on epoch=534
05/18/2022 01:25:35 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.77 on epoch=537
05/18/2022 01:25:36 - INFO - __main__ - Global step 2150 Train loss 0.77 Classification-F1 0.24463109354413703 on epoch=537
05/18/2022 01:25:36 - INFO - __main__ - Saving model with best Classification-F1: 0.2238749046529367 -> 0.24463109354413703 on epoch=537, global_step=2150
05/18/2022 01:25:37 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.78 on epoch=539
05/18/2022 01:25:38 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.82 on epoch=542
05/18/2022 01:25:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.80 on epoch=544
05/18/2022 01:25:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.72 on epoch=547
05/18/2022 01:25:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.83 on epoch=549
05/18/2022 01:25:43 - INFO - __main__ - Global step 2200 Train loss 0.79 Classification-F1 0.1526315789473684 on epoch=549
05/18/2022 01:25:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.76 on epoch=552
05/18/2022 01:25:45 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.79 on epoch=554
05/18/2022 01:25:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.81 on epoch=557
05/18/2022 01:25:47 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.84 on epoch=559
05/18/2022 01:25:49 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.75 on epoch=562
05/18/2022 01:25:49 - INFO - __main__ - Global step 2250 Train loss 0.79 Classification-F1 0.26281094527363186 on epoch=562
05/18/2022 01:25:49 - INFO - __main__ - Saving model with best Classification-F1: 0.24463109354413703 -> 0.26281094527363186 on epoch=562, global_step=2250
05/18/2022 01:25:50 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.75 on epoch=564
05/18/2022 01:25:52 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.79 on epoch=567
05/18/2022 01:25:53 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.84 on epoch=569
05/18/2022 01:25:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.83 on epoch=572
05/18/2022 01:25:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.77 on epoch=574
05/18/2022 01:25:56 - INFO - __main__ - Global step 2300 Train loss 0.79 Classification-F1 0.236956244418931 on epoch=574
05/18/2022 01:25:57 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.82 on epoch=577
05/18/2022 01:25:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.84 on epoch=579
05/18/2022 01:25:59 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.76 on epoch=582
05/18/2022 01:26:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.79 on epoch=584
05/18/2022 01:26:02 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.82 on epoch=587
05/18/2022 01:26:02 - INFO - __main__ - Global step 2350 Train loss 0.80 Classification-F1 0.24112357467321777 on epoch=587
05/18/2022 01:26:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.75 on epoch=589
05/18/2022 01:26:05 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.80 on epoch=592
05/18/2022 01:26:06 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.75 on epoch=594
05/18/2022 01:26:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.81 on epoch=597
05/18/2022 01:26:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.76 on epoch=599
05/18/2022 01:26:09 - INFO - __main__ - Global step 2400 Train loss 0.78 Classification-F1 0.16865079365079366 on epoch=599
05/18/2022 01:26:10 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.77 on epoch=602
05/18/2022 01:26:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.78 on epoch=604
05/18/2022 01:26:13 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.73 on epoch=607
05/18/2022 01:26:14 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.81 on epoch=609
05/18/2022 01:26:15 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.86 on epoch=612
05/18/2022 01:26:16 - INFO - __main__ - Global step 2450 Train loss 0.79 Classification-F1 0.18813131313131315 on epoch=612
05/18/2022 01:26:17 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.80 on epoch=614
05/18/2022 01:26:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.76 on epoch=617
05/18/2022 01:26:20 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.86 on epoch=619
05/18/2022 01:26:21 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.75 on epoch=622
05/18/2022 01:26:22 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.78 on epoch=624
05/18/2022 01:26:23 - INFO - __main__ - Global step 2500 Train loss 0.79 Classification-F1 0.21722488038277513 on epoch=624
05/18/2022 01:26:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.77 on epoch=627
05/18/2022 01:26:25 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.87 on epoch=629
05/18/2022 01:26:26 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.81 on epoch=632
05/18/2022 01:26:28 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.77 on epoch=634
05/18/2022 01:26:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.77 on epoch=637
05/18/2022 01:26:29 - INFO - __main__ - Global step 2550 Train loss 0.80 Classification-F1 0.25533723522853957 on epoch=637
05/18/2022 01:26:31 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.77 on epoch=639
05/18/2022 01:26:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.79 on epoch=642
05/18/2022 01:26:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.81 on epoch=644
05/18/2022 01:26:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.76 on epoch=647
05/18/2022 01:26:35 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.80 on epoch=649
05/18/2022 01:26:36 - INFO - __main__ - Global step 2600 Train loss 0.78 Classification-F1 0.25422705314009664 on epoch=649
05/18/2022 01:26:37 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.84 on epoch=652
05/18/2022 01:26:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.82 on epoch=654
05/18/2022 01:26:40 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.76 on epoch=657
05/18/2022 01:26:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.87 on epoch=659
05/18/2022 01:26:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.77 on epoch=662
05/18/2022 01:26:43 - INFO - __main__ - Global step 2650 Train loss 0.81 Classification-F1 0.2995923913043478 on epoch=662
05/18/2022 01:26:43 - INFO - __main__ - Saving model with best Classification-F1: 0.26281094527363186 -> 0.2995923913043478 on epoch=662, global_step=2650
05/18/2022 01:26:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.79 on epoch=664
05/18/2022 01:26:45 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.81 on epoch=667
05/18/2022 01:26:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.73 on epoch=669
05/18/2022 01:26:47 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.74 on epoch=672
05/18/2022 01:26:49 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.77 on epoch=674
05/18/2022 01:26:49 - INFO - __main__ - Global step 2700 Train loss 0.77 Classification-F1 0.2816053511705685 on epoch=674
05/18/2022 01:26:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.79 on epoch=677
05/18/2022 01:26:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.73 on epoch=679
05/18/2022 01:26:53 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.78 on epoch=682
05/18/2022 01:26:54 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.82 on epoch=684
05/18/2022 01:26:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.78 on epoch=687
05/18/2022 01:26:56 - INFO - __main__ - Global step 2750 Train loss 0.78 Classification-F1 0.4520489188231124 on epoch=687
05/18/2022 01:26:56 - INFO - __main__ - Saving model with best Classification-F1: 0.2995923913043478 -> 0.4520489188231124 on epoch=687, global_step=2750
05/18/2022 01:26:57 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.87 on epoch=689
05/18/2022 01:26:58 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.80 on epoch=692
05/18/2022 01:27:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.84 on epoch=694
05/18/2022 01:27:01 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.82 on epoch=697
05/18/2022 01:27:02 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.73 on epoch=699
05/18/2022 01:27:03 - INFO - __main__ - Global step 2800 Train loss 0.81 Classification-F1 0.2621870882740448 on epoch=699
05/18/2022 01:27:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.78 on epoch=702
05/18/2022 01:27:05 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.85 on epoch=704
05/18/2022 01:27:06 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.76 on epoch=707
05/18/2022 01:27:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.79 on epoch=709
05/18/2022 01:27:09 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.78 on epoch=712
05/18/2022 01:27:09 - INFO - __main__ - Global step 2850 Train loss 0.79 Classification-F1 0.23912087912087915 on epoch=712
05/18/2022 01:27:10 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.82 on epoch=714
05/18/2022 01:27:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.80 on epoch=717
05/18/2022 01:27:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.81 on epoch=719
05/18/2022 01:27:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.86 on epoch=722
05/18/2022 01:27:15 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.80 on epoch=724
05/18/2022 01:27:16 - INFO - __main__ - Global step 2900 Train loss 0.82 Classification-F1 0.24662804515745693 on epoch=724
05/18/2022 01:27:17 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.81 on epoch=727
05/18/2022 01:27:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.73 on epoch=729
05/18/2022 01:27:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.81 on epoch=732
05/18/2022 01:27:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.81 on epoch=734
05/18/2022 01:27:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.72 on epoch=737
05/18/2022 01:27:23 - INFO - __main__ - Global step 2950 Train loss 0.78 Classification-F1 0.2611013986013986 on epoch=737
05/18/2022 01:27:24 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.82 on epoch=739
05/18/2022 01:27:25 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.82 on epoch=742
05/18/2022 01:27:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.79 on epoch=744
05/18/2022 01:27:27 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.87 on epoch=747
05/18/2022 01:27:29 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.83 on epoch=749
05/18/2022 01:27:29 - INFO - __main__ - Global step 3000 Train loss 0.83 Classification-F1 0.3476504172156346 on epoch=749
05/18/2022 01:27:29 - INFO - __main__ - save last model!
05/18/2022 01:27:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 01:27:29 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 01:27:29 - INFO - __main__ - Printing 3 examples
05/18/2022 01:27:29 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 01:27:29 - INFO - __main__ - ['others']
05/18/2022 01:27:29 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 01:27:29 - INFO - __main__ - ['others']
05/18/2022 01:27:29 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 01:27:29 - INFO - __main__ - ['others']
05/18/2022 01:27:29 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:27:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:27:30 - INFO - __main__ - Printing 3 examples
05/18/2022 01:27:30 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/18/2022 01:27:30 - INFO - __main__ - ['happy']
05/18/2022 01:27:30 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/18/2022 01:27:30 - INFO - __main__ - ['happy']
05/18/2022 01:27:30 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/18/2022 01:27:30 - INFO - __main__ - ['happy']
05/18/2022 01:27:30 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:27:30 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:27:30 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 01:27:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:27:30 - INFO - __main__ - Printing 3 examples
05/18/2022 01:27:30 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/18/2022 01:27:30 - INFO - __main__ - ['happy']
05/18/2022 01:27:30 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/18/2022 01:27:30 - INFO - __main__ - ['happy']
05/18/2022 01:27:30 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/18/2022 01:27:30 - INFO - __main__ - ['happy']
05/18/2022 01:27:30 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:27:30 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:27:30 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 01:27:31 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:27:36 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 01:27:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 01:27:36 - INFO - __main__ - Starting training!
05/18/2022 01:27:37 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 01:28:21 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_42_0.4_8_predictions.txt
05/18/2022 01:28:21 - INFO - __main__ - Classification-F1 on test data: 0.2410
05/18/2022 01:28:21 - INFO - __main__ - prefix=emo_16_42, lr=0.4, bsz=8, dev_performance=0.4520489188231124, test_performance=0.24099693960340024
05/18/2022 01:28:21 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.3, bsz=8 ...
05/18/2022 01:28:22 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:28:22 - INFO - __main__ - Printing 3 examples
05/18/2022 01:28:22 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/18/2022 01:28:22 - INFO - __main__ - ['happy']
05/18/2022 01:28:22 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/18/2022 01:28:22 - INFO - __main__ - ['happy']
05/18/2022 01:28:22 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/18/2022 01:28:22 - INFO - __main__ - ['happy']
05/18/2022 01:28:22 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:28:22 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:28:22 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 01:28:22 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:28:22 - INFO - __main__ - Printing 3 examples
05/18/2022 01:28:22 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/18/2022 01:28:22 - INFO - __main__ - ['happy']
05/18/2022 01:28:22 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/18/2022 01:28:22 - INFO - __main__ - ['happy']
05/18/2022 01:28:22 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/18/2022 01:28:22 - INFO - __main__ - ['happy']
05/18/2022 01:28:22 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:28:22 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:28:22 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 01:28:28 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 01:28:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 01:28:29 - INFO - __main__ - Starting training!
05/18/2022 01:28:30 - INFO - __main__ - Step 10 Global step 10 Train loss 4.24 on epoch=2
05/18/2022 01:28:31 - INFO - __main__ - Step 20 Global step 20 Train loss 3.90 on epoch=4
05/18/2022 01:28:33 - INFO - __main__ - Step 30 Global step 30 Train loss 3.35 on epoch=7
05/18/2022 01:28:34 - INFO - __main__ - Step 40 Global step 40 Train loss 3.04 on epoch=9
05/18/2022 01:28:35 - INFO - __main__ - Step 50 Global step 50 Train loss 2.67 on epoch=12
05/18/2022 01:28:36 - INFO - __main__ - Global step 50 Train loss 3.44 Classification-F1 0.0810126582278481 on epoch=12
05/18/2022 01:28:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0810126582278481 on epoch=12, global_step=50
05/18/2022 01:28:37 - INFO - __main__ - Step 60 Global step 60 Train loss 2.44 on epoch=14
05/18/2022 01:28:39 - INFO - __main__ - Step 70 Global step 70 Train loss 2.04 on epoch=17
05/18/2022 01:28:40 - INFO - __main__ - Step 80 Global step 80 Train loss 1.86 on epoch=19
05/18/2022 01:28:41 - INFO - __main__ - Step 90 Global step 90 Train loss 1.63 on epoch=22
05/18/2022 01:28:42 - INFO - __main__ - Step 100 Global step 100 Train loss 1.59 on epoch=24
05/18/2022 01:28:43 - INFO - __main__ - Global step 100 Train loss 1.91 Classification-F1 0.10664629488158898 on epoch=24
05/18/2022 01:28:43 - INFO - __main__ - Saving model with best Classification-F1: 0.0810126582278481 -> 0.10664629488158898 on epoch=24, global_step=100
05/18/2022 01:28:44 - INFO - __main__ - Step 110 Global step 110 Train loss 1.45 on epoch=27
05/18/2022 01:28:45 - INFO - __main__ - Step 120 Global step 120 Train loss 1.38 on epoch=29
05/18/2022 01:28:47 - INFO - __main__ - Step 130 Global step 130 Train loss 1.21 on epoch=32
05/18/2022 01:28:48 - INFO - __main__ - Step 140 Global step 140 Train loss 1.15 on epoch=34
05/18/2022 01:28:49 - INFO - __main__ - Step 150 Global step 150 Train loss 1.17 on epoch=37
05/18/2022 01:28:50 - INFO - __main__ - Global step 150 Train loss 1.27 Classification-F1 0.12631578947368421 on epoch=37
05/18/2022 01:28:50 - INFO - __main__ - Saving model with best Classification-F1: 0.10664629488158898 -> 0.12631578947368421 on epoch=37, global_step=150
05/18/2022 01:28:51 - INFO - __main__ - Step 160 Global step 160 Train loss 1.07 on epoch=39
05/18/2022 01:28:52 - INFO - __main__ - Step 170 Global step 170 Train loss 1.04 on epoch=42
05/18/2022 01:28:54 - INFO - __main__ - Step 180 Global step 180 Train loss 1.04 on epoch=44
05/18/2022 01:28:55 - INFO - __main__ - Step 190 Global step 190 Train loss 1.03 on epoch=47
05/18/2022 01:28:56 - INFO - __main__ - Step 200 Global step 200 Train loss 1.00 on epoch=49
05/18/2022 01:28:57 - INFO - __main__ - Global step 200 Train loss 1.04 Classification-F1 0.15022222222222223 on epoch=49
05/18/2022 01:28:57 - INFO - __main__ - Saving model with best Classification-F1: 0.12631578947368421 -> 0.15022222222222223 on epoch=49, global_step=200
05/18/2022 01:28:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.97 on epoch=52
05/18/2022 01:28:59 - INFO - __main__ - Step 220 Global step 220 Train loss 1.06 on epoch=54
05/18/2022 01:29:01 - INFO - __main__ - Step 230 Global step 230 Train loss 1.12 on epoch=57
05/18/2022 01:29:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.94 on epoch=59
05/18/2022 01:29:03 - INFO - __main__ - Step 250 Global step 250 Train loss 1.02 on epoch=62
05/18/2022 01:29:04 - INFO - __main__ - Global step 250 Train loss 1.02 Classification-F1 0.10664629488158898 on epoch=62
05/18/2022 01:29:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.93 on epoch=64
05/18/2022 01:29:06 - INFO - __main__ - Step 270 Global step 270 Train loss 1.00 on epoch=67
05/18/2022 01:29:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.98 on epoch=69
05/18/2022 01:29:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.94 on epoch=72
05/18/2022 01:29:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.96 on epoch=74
05/18/2022 01:29:11 - INFO - __main__ - Global step 300 Train loss 0.96 Classification-F1 0.10389610389610389 on epoch=74
05/18/2022 01:29:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.98 on epoch=77
05/18/2022 01:29:13 - INFO - __main__ - Step 320 Global step 320 Train loss 1.03 on epoch=79
05/18/2022 01:29:14 - INFO - __main__ - Step 330 Global step 330 Train loss 1.05 on epoch=82
05/18/2022 01:29:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.91 on epoch=84
05/18/2022 01:29:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.88 on epoch=87
05/18/2022 01:29:17 - INFO - __main__ - Global step 350 Train loss 0.97 Classification-F1 0.10126582278481013 on epoch=87
05/18/2022 01:29:19 - INFO - __main__ - Step 360 Global step 360 Train loss 1.05 on epoch=89
05/18/2022 01:29:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.95 on epoch=92
05/18/2022 01:29:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.90 on epoch=94
05/18/2022 01:29:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.91 on epoch=97
05/18/2022 01:29:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.90 on epoch=99
05/18/2022 01:29:24 - INFO - __main__ - Global step 400 Train loss 0.94 Classification-F1 0.10126582278481013 on epoch=99
05/18/2022 01:29:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.81 on epoch=102
05/18/2022 01:29:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.78 on epoch=104
05/18/2022 01:29:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.97 on epoch=107
05/18/2022 01:29:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.87 on epoch=109
05/18/2022 01:29:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.96 on epoch=112
05/18/2022 01:29:31 - INFO - __main__ - Global step 450 Train loss 0.88 Classification-F1 0.15945165945165946 on epoch=112
05/18/2022 01:29:31 - INFO - __main__ - Saving model with best Classification-F1: 0.15022222222222223 -> 0.15945165945165946 on epoch=112, global_step=450
05/18/2022 01:29:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.96 on epoch=114
05/18/2022 01:29:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.87 on epoch=117
05/18/2022 01:29:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.86 on epoch=119
05/18/2022 01:29:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.91 on epoch=122
05/18/2022 01:29:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.92 on epoch=124
05/18/2022 01:29:38 - INFO - __main__ - Global step 500 Train loss 0.90 Classification-F1 0.10126582278481013 on epoch=124
05/18/2022 01:29:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.94 on epoch=127
05/18/2022 01:29:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.95 on epoch=129
05/18/2022 01:29:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.81 on epoch=132
05/18/2022 01:29:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.87 on epoch=134
05/18/2022 01:29:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.83 on epoch=137
05/18/2022 01:29:44 - INFO - __main__ - Global step 550 Train loss 0.88 Classification-F1 0.10126582278481013 on epoch=137
05/18/2022 01:29:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.95 on epoch=139
05/18/2022 01:29:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.88 on epoch=142
05/18/2022 01:29:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.94 on epoch=144
05/18/2022 01:29:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.97 on epoch=147
05/18/2022 01:29:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.89 on epoch=149
05/18/2022 01:29:51 - INFO - __main__ - Global step 600 Train loss 0.92 Classification-F1 0.10126582278481013 on epoch=149
05/18/2022 01:29:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.85 on epoch=152
05/18/2022 01:29:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.97 on epoch=154
05/18/2022 01:29:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.86 on epoch=157
05/18/2022 01:29:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.90 on epoch=159
05/18/2022 01:29:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.84 on epoch=162
05/18/2022 01:29:57 - INFO - __main__ - Global step 650 Train loss 0.88 Classification-F1 0.10126582278481013 on epoch=162
05/18/2022 01:29:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.78 on epoch=164
05/18/2022 01:30:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.95 on epoch=167
05/18/2022 01:30:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.94 on epoch=169
05/18/2022 01:30:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.87 on epoch=172
05/18/2022 01:30:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.89 on epoch=174
05/18/2022 01:30:04 - INFO - __main__ - Global step 700 Train loss 0.89 Classification-F1 0.10126582278481013 on epoch=174
05/18/2022 01:30:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.84 on epoch=177
05/18/2022 01:30:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.87 on epoch=179
05/18/2022 01:30:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.88 on epoch=182
05/18/2022 01:30:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.94 on epoch=184
05/18/2022 01:30:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.86 on epoch=187
05/18/2022 01:30:10 - INFO - __main__ - Global step 750 Train loss 0.88 Classification-F1 0.10126582278481013 on epoch=187
05/18/2022 01:30:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.90 on epoch=189
05/18/2022 01:30:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.78 on epoch=192
05/18/2022 01:30:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.88 on epoch=194
05/18/2022 01:30:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.81 on epoch=197
05/18/2022 01:30:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.84 on epoch=199
05/18/2022 01:30:17 - INFO - __main__ - Global step 800 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=199
05/18/2022 01:30:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.91 on epoch=202
05/18/2022 01:30:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.84 on epoch=204
05/18/2022 01:30:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.98 on epoch=207
05/18/2022 01:30:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.88 on epoch=209
05/18/2022 01:30:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.86 on epoch=212
05/18/2022 01:30:23 - INFO - __main__ - Global step 850 Train loss 0.89 Classification-F1 0.13067758749069247 on epoch=212
05/18/2022 01:30:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.91 on epoch=214
05/18/2022 01:30:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.86 on epoch=217
05/18/2022 01:30:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.84 on epoch=219
05/18/2022 01:30:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.81 on epoch=222
05/18/2022 01:30:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.91 on epoch=224
05/18/2022 01:30:30 - INFO - __main__ - Global step 900 Train loss 0.86 Classification-F1 0.10126582278481013 on epoch=224
05/18/2022 01:30:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.82 on epoch=227
05/18/2022 01:30:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.92 on epoch=229
05/18/2022 01:30:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.82 on epoch=232
05/18/2022 01:30:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.86 on epoch=234
05/18/2022 01:30:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.82 on epoch=237
05/18/2022 01:30:36 - INFO - __main__ - Global step 950 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=237
05/18/2022 01:30:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.92 on epoch=239
05/18/2022 01:30:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.78 on epoch=242
05/18/2022 01:30:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.84 on epoch=244
05/18/2022 01:30:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.80 on epoch=247
05/18/2022 01:30:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.82 on epoch=249
05/18/2022 01:30:42 - INFO - __main__ - Global step 1000 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=249
05/18/2022 01:30:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.82 on epoch=252
05/18/2022 01:30:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.88 on epoch=254
05/18/2022 01:30:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.84 on epoch=257
05/18/2022 01:30:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.86 on epoch=259
05/18/2022 01:30:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.89 on epoch=262
05/18/2022 01:30:49 - INFO - __main__ - Global step 1050 Train loss 0.86 Classification-F1 0.10126582278481013 on epoch=262
05/18/2022 01:30:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.80 on epoch=264
05/18/2022 01:30:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.77 on epoch=267
05/18/2022 01:30:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.83 on epoch=269
05/18/2022 01:30:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.86 on epoch=272
05/18/2022 01:30:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.87 on epoch=274
05/18/2022 01:30:55 - INFO - __main__ - Global step 1100 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=274
05/18/2022 01:30:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.80 on epoch=277
05/18/2022 01:30:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.86 on epoch=279
05/18/2022 01:30:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.79 on epoch=282
05/18/2022 01:30:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.82 on epoch=284
05/18/2022 01:31:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.74 on epoch=287
05/18/2022 01:31:01 - INFO - __main__ - Global step 1150 Train loss 0.80 Classification-F1 0.13034188034188032 on epoch=287
05/18/2022 01:31:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.76 on epoch=289
05/18/2022 01:31:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.83 on epoch=292
05/18/2022 01:31:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.85 on epoch=294
05/18/2022 01:31:06 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.88 on epoch=297
05/18/2022 01:31:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.90 on epoch=299
05/18/2022 01:31:07 - INFO - __main__ - Global step 1200 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=299
05/18/2022 01:31:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.79 on epoch=302
05/18/2022 01:31:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.83 on epoch=304
05/18/2022 01:31:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.83 on epoch=307
05/18/2022 01:31:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.81 on epoch=309
05/18/2022 01:31:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.89 on epoch=312
05/18/2022 01:31:14 - INFO - __main__ - Global step 1250 Train loss 0.83 Classification-F1 0.13067758749069247 on epoch=312
05/18/2022 01:31:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.87 on epoch=314
05/18/2022 01:31:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.78 on epoch=317
05/18/2022 01:31:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.88 on epoch=319
05/18/2022 01:31:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.83 on epoch=322
05/18/2022 01:31:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.80 on epoch=324
05/18/2022 01:31:20 - INFO - __main__ - Global step 1300 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=324
05/18/2022 01:31:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.84 on epoch=327
05/18/2022 01:31:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.86 on epoch=329
05/18/2022 01:31:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.88 on epoch=332
05/18/2022 01:31:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.80 on epoch=334
05/18/2022 01:31:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.75 on epoch=337
05/18/2022 01:31:26 - INFO - __main__ - Global step 1350 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=337
05/18/2022 01:31:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.88 on epoch=339
05/18/2022 01:31:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.82 on epoch=342
05/18/2022 01:31:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.82 on epoch=344
05/18/2022 01:31:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.79 on epoch=347
05/18/2022 01:31:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.83 on epoch=349
05/18/2022 01:31:32 - INFO - __main__ - Global step 1400 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=349
05/18/2022 01:31:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.77 on epoch=352
05/18/2022 01:31:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.84 on epoch=354
05/18/2022 01:31:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.79 on epoch=357
05/18/2022 01:31:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.75 on epoch=359
05/18/2022 01:31:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.82 on epoch=362
05/18/2022 01:31:39 - INFO - __main__ - Global step 1450 Train loss 0.79 Classification-F1 0.10126582278481013 on epoch=362
05/18/2022 01:31:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.77 on epoch=364
05/18/2022 01:31:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.86 on epoch=367
05/18/2022 01:31:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.84 on epoch=369
05/18/2022 01:31:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.77 on epoch=372
05/18/2022 01:31:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.90 on epoch=374
05/18/2022 01:31:45 - INFO - __main__ - Global step 1500 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=374
05/18/2022 01:31:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.91 on epoch=377
05/18/2022 01:31:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.92 on epoch=379
05/18/2022 01:31:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.78 on epoch=382
05/18/2022 01:31:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.78 on epoch=384
05/18/2022 01:31:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.82 on epoch=387
05/18/2022 01:31:51 - INFO - __main__ - Global step 1550 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=387
05/18/2022 01:31:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.82 on epoch=389
05/18/2022 01:31:53 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.76 on epoch=392
05/18/2022 01:31:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.86 on epoch=394
05/18/2022 01:31:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.77 on epoch=397
05/18/2022 01:31:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.83 on epoch=399
05/18/2022 01:31:57 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.10126582278481013 on epoch=399
05/18/2022 01:31:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.88 on epoch=402
05/18/2022 01:32:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.87 on epoch=404
05/18/2022 01:32:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.80 on epoch=407
05/18/2022 01:32:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.76 on epoch=409
05/18/2022 01:32:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.85 on epoch=412
05/18/2022 01:32:04 - INFO - __main__ - Global step 1650 Train loss 0.83 Classification-F1 0.13067758749069247 on epoch=412
05/18/2022 01:32:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.82 on epoch=414
05/18/2022 01:32:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.91 on epoch=417
05/18/2022 01:32:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.81 on epoch=419
05/18/2022 01:32:08 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.82 on epoch=422
05/18/2022 01:32:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.84 on epoch=424
05/18/2022 01:32:10 - INFO - __main__ - Global step 1700 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=424
05/18/2022 01:32:11 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.86 on epoch=427
05/18/2022 01:32:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.86 on epoch=429
05/18/2022 01:32:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.75 on epoch=432
05/18/2022 01:32:15 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.80 on epoch=434
05/18/2022 01:32:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.82 on epoch=437
05/18/2022 01:32:16 - INFO - __main__ - Global step 1750 Train loss 0.82 Classification-F1 0.15 on epoch=437
05/18/2022 01:32:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.83 on epoch=439
05/18/2022 01:32:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.80 on epoch=442
05/18/2022 01:32:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.79 on epoch=444
05/18/2022 01:32:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.88 on epoch=447
05/18/2022 01:32:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.80 on epoch=449
05/18/2022 01:32:22 - INFO - __main__ - Global step 1800 Train loss 0.82 Classification-F1 0.10126582278481013 on epoch=449
05/18/2022 01:32:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.78 on epoch=452
05/18/2022 01:32:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.83 on epoch=454
05/18/2022 01:32:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.79 on epoch=457
05/18/2022 01:32:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.80 on epoch=459
05/18/2022 01:32:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.84 on epoch=462
05/18/2022 01:32:29 - INFO - __main__ - Global step 1850 Train loss 0.81 Classification-F1 0.13197586726998492 on epoch=462
05/18/2022 01:32:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.89 on epoch=464
05/18/2022 01:32:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.85 on epoch=467
05/18/2022 01:32:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.84 on epoch=469
05/18/2022 01:32:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.79 on epoch=472
05/18/2022 01:32:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.77 on epoch=474
05/18/2022 01:32:35 - INFO - __main__ - Global step 1900 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=474
05/18/2022 01:32:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.79 on epoch=477
05/18/2022 01:32:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.83 on epoch=479
05/18/2022 01:32:38 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.81 on epoch=482
05/18/2022 01:32:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.75 on epoch=484
05/18/2022 01:32:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.81 on epoch=487
05/18/2022 01:32:41 - INFO - __main__ - Global step 1950 Train loss 0.80 Classification-F1 0.1539829302987198 on epoch=487
05/18/2022 01:32:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.85 on epoch=489
05/18/2022 01:32:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.84 on epoch=492
05/18/2022 01:32:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.86 on epoch=494
05/18/2022 01:32:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.83 on epoch=497
05/18/2022 01:32:47 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.88 on epoch=499
05/18/2022 01:32:48 - INFO - __main__ - Global step 2000 Train loss 0.85 Classification-F1 0.10256410256410256 on epoch=499
05/18/2022 01:32:49 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.81 on epoch=502
05/18/2022 01:32:50 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.78 on epoch=504
05/18/2022 01:32:51 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.78 on epoch=507
05/18/2022 01:32:52 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.90 on epoch=509
05/18/2022 01:32:53 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.76 on epoch=512
05/18/2022 01:32:54 - INFO - __main__ - Global step 2050 Train loss 0.81 Classification-F1 0.13167388167388167 on epoch=512
05/18/2022 01:32:55 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.81 on epoch=514
05/18/2022 01:32:56 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.84 on epoch=517
05/18/2022 01:32:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.87 on epoch=519
05/18/2022 01:32:58 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.83 on epoch=522
05/18/2022 01:33:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.81 on epoch=524
05/18/2022 01:33:00 - INFO - __main__ - Global step 2100 Train loss 0.83 Classification-F1 0.13167388167388167 on epoch=524
05/18/2022 01:33:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.80 on epoch=527
05/18/2022 01:33:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.81 on epoch=529
05/18/2022 01:33:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.82 on epoch=532
05/18/2022 01:33:05 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.88 on epoch=534
05/18/2022 01:33:06 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.79 on epoch=537
05/18/2022 01:33:06 - INFO - __main__ - Global step 2150 Train loss 0.82 Classification-F1 0.13167388167388167 on epoch=537
05/18/2022 01:33:07 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.83 on epoch=539
05/18/2022 01:33:09 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.76 on epoch=542
05/18/2022 01:33:10 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.78 on epoch=544
05/18/2022 01:33:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.81 on epoch=547
05/18/2022 01:33:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.77 on epoch=549
05/18/2022 01:33:13 - INFO - __main__ - Global step 2200 Train loss 0.79 Classification-F1 0.10256410256410256 on epoch=549
05/18/2022 01:33:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.81 on epoch=552
05/18/2022 01:33:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.85 on epoch=554
05/18/2022 01:33:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.78 on epoch=557
05/18/2022 01:33:17 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.79 on epoch=559
05/18/2022 01:33:18 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.80 on epoch=562
05/18/2022 01:33:19 - INFO - __main__ - Global step 2250 Train loss 0.80 Classification-F1 0.18166666666666667 on epoch=562
05/18/2022 01:33:19 - INFO - __main__ - Saving model with best Classification-F1: 0.15945165945165946 -> 0.18166666666666667 on epoch=562, global_step=2250
05/18/2022 01:33:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.78 on epoch=564
05/18/2022 01:33:21 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.77 on epoch=567
05/18/2022 01:33:22 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.78 on epoch=569
05/18/2022 01:33:24 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.83 on epoch=572
05/18/2022 01:33:25 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.79 on epoch=574
05/18/2022 01:33:25 - INFO - __main__ - Global step 2300 Train loss 0.79 Classification-F1 0.10256410256410256 on epoch=574
05/18/2022 01:33:26 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.86 on epoch=577
05/18/2022 01:33:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.80 on epoch=579
05/18/2022 01:33:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.81 on epoch=582
05/18/2022 01:33:30 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.77 on epoch=584
05/18/2022 01:33:31 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.80 on epoch=587
05/18/2022 01:33:31 - INFO - __main__ - Global step 2350 Train loss 0.81 Classification-F1 0.20526315789473687 on epoch=587
05/18/2022 01:33:32 - INFO - __main__ - Saving model with best Classification-F1: 0.18166666666666667 -> 0.20526315789473687 on epoch=587, global_step=2350
05/18/2022 01:33:33 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.77 on epoch=589
05/18/2022 01:33:34 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.85 on epoch=592
05/18/2022 01:33:35 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.78 on epoch=594
05/18/2022 01:33:36 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.80 on epoch=597
05/18/2022 01:33:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.77 on epoch=599
05/18/2022 01:33:38 - INFO - __main__ - Global step 2400 Train loss 0.79 Classification-F1 0.18166666666666667 on epoch=599
05/18/2022 01:33:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.81 on epoch=602
05/18/2022 01:33:40 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.79 on epoch=604
05/18/2022 01:33:41 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.81 on epoch=607
05/18/2022 01:33:42 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.89 on epoch=609
05/18/2022 01:33:44 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.83 on epoch=612
05/18/2022 01:33:44 - INFO - __main__ - Global step 2450 Train loss 0.83 Classification-F1 0.22174447174447173 on epoch=612
05/18/2022 01:33:44 - INFO - __main__ - Saving model with best Classification-F1: 0.20526315789473687 -> 0.22174447174447173 on epoch=612, global_step=2450
05/18/2022 01:33:45 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.82 on epoch=614
05/18/2022 01:33:46 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.74 on epoch=617
05/18/2022 01:33:48 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.81 on epoch=619
05/18/2022 01:33:49 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.81 on epoch=622
05/18/2022 01:33:50 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.73 on epoch=624
05/18/2022 01:33:50 - INFO - __main__ - Global step 2500 Train loss 0.78 Classification-F1 0.18284347231715653 on epoch=624
05/18/2022 01:33:52 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.77 on epoch=627
05/18/2022 01:33:53 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.83 on epoch=629
05/18/2022 01:33:54 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.77 on epoch=632
05/18/2022 01:33:55 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.84 on epoch=634
05/18/2022 01:33:56 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.80 on epoch=637
05/18/2022 01:33:57 - INFO - __main__ - Global step 2550 Train loss 0.80 Classification-F1 0.17368421052631577 on epoch=637
05/18/2022 01:33:58 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.78 on epoch=639
05/18/2022 01:33:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.83 on epoch=642
05/18/2022 01:34:00 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.80 on epoch=644
05/18/2022 01:34:01 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.77 on epoch=647
05/18/2022 01:34:02 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.78 on epoch=649
05/18/2022 01:34:03 - INFO - __main__ - Global step 2600 Train loss 0.79 Classification-F1 0.13167388167388167 on epoch=649
05/18/2022 01:34:04 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.76 on epoch=652
05/18/2022 01:34:05 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.78 on epoch=654
05/18/2022 01:34:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.80 on epoch=657
05/18/2022 01:34:08 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.80 on epoch=659
05/18/2022 01:34:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.82 on epoch=662
05/18/2022 01:34:09 - INFO - __main__ - Global step 2650 Train loss 0.79 Classification-F1 0.19797782126549252 on epoch=662
05/18/2022 01:34:10 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.81 on epoch=664
05/18/2022 01:34:12 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.77 on epoch=667
05/18/2022 01:34:13 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.82 on epoch=669
05/18/2022 01:34:14 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.77 on epoch=672
05/18/2022 01:34:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.73 on epoch=674
05/18/2022 01:34:16 - INFO - __main__ - Global step 2700 Train loss 0.78 Classification-F1 0.1373668188736682 on epoch=674
05/18/2022 01:34:17 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.88 on epoch=677
05/18/2022 01:34:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.80 on epoch=679
05/18/2022 01:34:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.83 on epoch=682
05/18/2022 01:34:21 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.80 on epoch=684
05/18/2022 01:34:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.76 on epoch=687
05/18/2022 01:34:23 - INFO - __main__ - Global step 2750 Train loss 0.81 Classification-F1 0.125 on epoch=687
05/18/2022 01:34:24 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.82 on epoch=689
05/18/2022 01:34:25 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.76 on epoch=692
05/18/2022 01:34:26 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.77 on epoch=694
05/18/2022 01:34:28 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.82 on epoch=697
05/18/2022 01:34:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.92 on epoch=699
05/18/2022 01:34:29 - INFO - __main__ - Global step 2800 Train loss 0.82 Classification-F1 0.15789473684210525 on epoch=699
05/18/2022 01:34:31 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.81 on epoch=702
05/18/2022 01:34:32 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.85 on epoch=704
05/18/2022 01:34:33 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.80 on epoch=707
05/18/2022 01:34:34 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.81 on epoch=709
05/18/2022 01:34:36 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.79 on epoch=712
05/18/2022 01:34:36 - INFO - __main__ - Global step 2850 Train loss 0.81 Classification-F1 0.18166666666666667 on epoch=712
05/18/2022 01:34:37 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.80 on epoch=714
05/18/2022 01:34:39 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.80 on epoch=717
05/18/2022 01:34:40 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.80 on epoch=719
05/18/2022 01:34:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.70 on epoch=722
05/18/2022 01:34:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.82 on epoch=724
05/18/2022 01:34:43 - INFO - __main__ - Global step 2900 Train loss 0.79 Classification-F1 0.15789473684210525 on epoch=724
05/18/2022 01:34:44 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.75 on epoch=727
05/18/2022 01:34:46 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.71 on epoch=729
05/18/2022 01:34:47 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.77 on epoch=732
05/18/2022 01:34:48 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.79 on epoch=734
05/18/2022 01:34:49 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.79 on epoch=737
05/18/2022 01:34:50 - INFO - __main__ - Global step 2950 Train loss 0.76 Classification-F1 0.24154589371980673 on epoch=737
05/18/2022 01:34:50 - INFO - __main__ - Saving model with best Classification-F1: 0.22174447174447173 -> 0.24154589371980673 on epoch=737, global_step=2950
05/18/2022 01:34:51 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.82 on epoch=739
05/18/2022 01:34:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.83 on epoch=742
05/18/2022 01:34:54 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.70 on epoch=744
05/18/2022 01:34:55 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.78 on epoch=747
05/18/2022 01:34:56 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.76 on epoch=749
05/18/2022 01:34:57 - INFO - __main__ - Global step 3000 Train loss 0.78 Classification-F1 0.22393048128342247 on epoch=749
05/18/2022 01:34:57 - INFO - __main__ - save last model!
05/18/2022 01:34:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 01:34:57 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 01:34:57 - INFO - __main__ - Printing 3 examples
05/18/2022 01:34:57 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 01:34:57 - INFO - __main__ - ['others']
05/18/2022 01:34:57 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 01:34:57 - INFO - __main__ - ['others']
05/18/2022 01:34:57 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 01:34:57 - INFO - __main__ - ['others']
05/18/2022 01:34:57 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:34:57 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:34:57 - INFO - __main__ - Printing 3 examples
05/18/2022 01:34:57 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/18/2022 01:34:57 - INFO - __main__ - ['happy']
05/18/2022 01:34:57 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/18/2022 01:34:57 - INFO - __main__ - ['happy']
05/18/2022 01:34:57 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/18/2022 01:34:57 - INFO - __main__ - ['happy']
05/18/2022 01:34:57 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:34:57 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:34:57 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 01:34:57 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:34:57 - INFO - __main__ - Printing 3 examples
05/18/2022 01:34:57 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/18/2022 01:34:57 - INFO - __main__ - ['happy']
05/18/2022 01:34:57 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/18/2022 01:34:57 - INFO - __main__ - ['happy']
05/18/2022 01:34:57 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/18/2022 01:34:57 - INFO - __main__ - ['happy']
05/18/2022 01:34:57 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:34:57 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:34:57 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 01:34:59 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:35:03 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 01:35:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 01:35:03 - INFO - __main__ - Starting training!
05/18/2022 01:35:04 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 01:35:46 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_42_0.3_8_predictions.txt
05/18/2022 01:35:46 - INFO - __main__ - Classification-F1 on test data: 0.1137
05/18/2022 01:35:47 - INFO - __main__ - prefix=emo_16_42, lr=0.3, bsz=8, dev_performance=0.24154589371980673, test_performance=0.11372137083967966
05/18/2022 01:35:47 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.2, bsz=8 ...
05/18/2022 01:35:48 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:35:48 - INFO - __main__ - Printing 3 examples
05/18/2022 01:35:48 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/18/2022 01:35:48 - INFO - __main__ - ['happy']
05/18/2022 01:35:48 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/18/2022 01:35:48 - INFO - __main__ - ['happy']
05/18/2022 01:35:48 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/18/2022 01:35:48 - INFO - __main__ - ['happy']
05/18/2022 01:35:48 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:35:48 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:35:48 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 01:35:48 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:35:48 - INFO - __main__ - Printing 3 examples
05/18/2022 01:35:48 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/18/2022 01:35:48 - INFO - __main__ - ['happy']
05/18/2022 01:35:48 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/18/2022 01:35:48 - INFO - __main__ - ['happy']
05/18/2022 01:35:48 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/18/2022 01:35:48 - INFO - __main__ - ['happy']
05/18/2022 01:35:48 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:35:48 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:35:48 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 01:35:54 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 01:35:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 01:35:54 - INFO - __main__ - Starting training!
05/18/2022 01:35:56 - INFO - __main__ - Step 10 Global step 10 Train loss 4.22 on epoch=2
05/18/2022 01:35:57 - INFO - __main__ - Step 20 Global step 20 Train loss 3.98 on epoch=4
05/18/2022 01:35:58 - INFO - __main__ - Step 30 Global step 30 Train loss 3.64 on epoch=7
05/18/2022 01:35:59 - INFO - __main__ - Step 40 Global step 40 Train loss 3.49 on epoch=9
05/18/2022 01:36:01 - INFO - __main__ - Step 50 Global step 50 Train loss 3.19 on epoch=12
05/18/2022 01:36:01 - INFO - __main__ - Global step 50 Train loss 3.71 Classification-F1 0.0 on epoch=12
05/18/2022 01:36:01 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
05/18/2022 01:36:03 - INFO - __main__ - Step 60 Global step 60 Train loss 3.00 on epoch=14
05/18/2022 01:36:04 - INFO - __main__ - Step 70 Global step 70 Train loss 2.68 on epoch=17
05/18/2022 01:36:05 - INFO - __main__ - Step 80 Global step 80 Train loss 2.53 on epoch=19
05/18/2022 01:36:06 - INFO - __main__ - Step 90 Global step 90 Train loss 2.28 on epoch=22
05/18/2022 01:36:07 - INFO - __main__ - Step 100 Global step 100 Train loss 2.20 on epoch=24
05/18/2022 01:36:08 - INFO - __main__ - Global step 100 Train loss 2.54 Classification-F1 0.0810126582278481 on epoch=24
05/18/2022 01:36:08 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.0810126582278481 on epoch=24, global_step=100
05/18/2022 01:36:09 - INFO - __main__ - Step 110 Global step 110 Train loss 1.96 on epoch=27
05/18/2022 01:36:11 - INFO - __main__ - Step 120 Global step 120 Train loss 1.85 on epoch=29
05/18/2022 01:36:12 - INFO - __main__ - Step 130 Global step 130 Train loss 1.64 on epoch=32
05/18/2022 01:36:13 - INFO - __main__ - Step 140 Global step 140 Train loss 1.64 on epoch=34
05/18/2022 01:36:14 - INFO - __main__ - Step 150 Global step 150 Train loss 1.48 on epoch=37
05/18/2022 01:36:15 - INFO - __main__ - Global step 150 Train loss 1.72 Classification-F1 0.15975177304964538 on epoch=37
05/18/2022 01:36:15 - INFO - __main__ - Saving model with best Classification-F1: 0.0810126582278481 -> 0.15975177304964538 on epoch=37, global_step=150
05/18/2022 01:36:16 - INFO - __main__ - Step 160 Global step 160 Train loss 1.45 on epoch=39
05/18/2022 01:36:17 - INFO - __main__ - Step 170 Global step 170 Train loss 1.34 on epoch=42
05/18/2022 01:36:18 - INFO - __main__ - Step 180 Global step 180 Train loss 1.30 on epoch=44
05/18/2022 01:36:20 - INFO - __main__ - Step 190 Global step 190 Train loss 1.34 on epoch=47
05/18/2022 01:36:21 - INFO - __main__ - Step 200 Global step 200 Train loss 1.27 on epoch=49
05/18/2022 01:36:21 - INFO - __main__ - Global step 200 Train loss 1.34 Classification-F1 0.10558069381598793 on epoch=49
05/18/2022 01:36:23 - INFO - __main__ - Step 210 Global step 210 Train loss 1.15 on epoch=52
05/18/2022 01:36:24 - INFO - __main__ - Step 220 Global step 220 Train loss 1.24 on epoch=54
05/18/2022 01:36:25 - INFO - __main__ - Step 230 Global step 230 Train loss 1.03 on epoch=57
05/18/2022 01:36:26 - INFO - __main__ - Step 240 Global step 240 Train loss 1.02 on epoch=59
05/18/2022 01:36:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.94 on epoch=62
05/18/2022 01:36:28 - INFO - __main__ - Global step 250 Train loss 1.08 Classification-F1 0.13067758749069247 on epoch=62
05/18/2022 01:36:29 - INFO - __main__ - Step 260 Global step 260 Train loss 1.02 on epoch=64
05/18/2022 01:36:30 - INFO - __main__ - Step 270 Global step 270 Train loss 1.08 on epoch=67
05/18/2022 01:36:32 - INFO - __main__ - Step 280 Global step 280 Train loss 1.02 on epoch=69
05/18/2022 01:36:33 - INFO - __main__ - Step 290 Global step 290 Train loss 1.01 on epoch=72
05/18/2022 01:36:34 - INFO - __main__ - Step 300 Global step 300 Train loss 1.02 on epoch=74
05/18/2022 01:36:35 - INFO - __main__ - Global step 300 Train loss 1.03 Classification-F1 0.13067758749069247 on epoch=74
05/18/2022 01:36:36 - INFO - __main__ - Step 310 Global step 310 Train loss 1.06 on epoch=77
05/18/2022 01:36:37 - INFO - __main__ - Step 320 Global step 320 Train loss 1.07 on epoch=79
05/18/2022 01:36:38 - INFO - __main__ - Step 330 Global step 330 Train loss 1.08 on epoch=82
05/18/2022 01:36:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.98 on epoch=84
05/18/2022 01:36:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.92 on epoch=87
05/18/2022 01:36:41 - INFO - __main__ - Global step 350 Train loss 1.02 Classification-F1 0.13330786860198623 on epoch=87
05/18/2022 01:36:42 - INFO - __main__ - Step 360 Global step 360 Train loss 1.02 on epoch=89
05/18/2022 01:36:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.96 on epoch=92
05/18/2022 01:36:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.94 on epoch=94
05/18/2022 01:36:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.99 on epoch=97
05/18/2022 01:36:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.92 on epoch=99
05/18/2022 01:36:48 - INFO - __main__ - Global step 400 Train loss 0.97 Classification-F1 0.13067758749069247 on epoch=99
05/18/2022 01:36:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.97 on epoch=102
05/18/2022 01:36:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.95 on epoch=104
05/18/2022 01:36:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.94 on epoch=107
05/18/2022 01:36:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.97 on epoch=109
05/18/2022 01:36:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.99 on epoch=112
05/18/2022 01:36:54 - INFO - __main__ - Global step 450 Train loss 0.96 Classification-F1 0.13067758749069247 on epoch=112
05/18/2022 01:36:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.94 on epoch=114
05/18/2022 01:36:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.89 on epoch=117
05/18/2022 01:36:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.95 on epoch=119
05/18/2022 01:36:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.98 on epoch=122
05/18/2022 01:37:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.93 on epoch=124
05/18/2022 01:37:01 - INFO - __main__ - Global step 500 Train loss 0.94 Classification-F1 0.13067758749069247 on epoch=124
05/18/2022 01:37:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.95 on epoch=127
05/18/2022 01:37:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.98 on epoch=129
05/18/2022 01:37:04 - INFO - __main__ - Step 530 Global step 530 Train loss 1.02 on epoch=132
05/18/2022 01:37:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.90 on epoch=134
05/18/2022 01:37:07 - INFO - __main__ - Step 550 Global step 550 Train loss 1.02 on epoch=137
05/18/2022 01:37:07 - INFO - __main__ - Global step 550 Train loss 0.97 Classification-F1 0.13067758749069247 on epoch=137
05/18/2022 01:37:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.97 on epoch=139
05/18/2022 01:37:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.94 on epoch=142
05/18/2022 01:37:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.97 on epoch=144
05/18/2022 01:37:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.86 on epoch=147
05/18/2022 01:37:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.92 on epoch=149
05/18/2022 01:37:14 - INFO - __main__ - Global step 600 Train loss 0.93 Classification-F1 0.10126582278481013 on epoch=149
05/18/2022 01:37:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.93 on epoch=152
05/18/2022 01:37:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.88 on epoch=154
05/18/2022 01:37:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.94 on epoch=157
05/18/2022 01:37:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.99 on epoch=159
05/18/2022 01:37:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.87 on epoch=162
05/18/2022 01:37:20 - INFO - __main__ - Global step 650 Train loss 0.92 Classification-F1 0.15945165945165946 on epoch=162
05/18/2022 01:37:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.97 on epoch=164
05/18/2022 01:37:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.99 on epoch=167
05/18/2022 01:37:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.89 on epoch=169
05/18/2022 01:37:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.91 on epoch=172
05/18/2022 01:37:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.86 on epoch=174
05/18/2022 01:37:27 - INFO - __main__ - Global step 700 Train loss 0.92 Classification-F1 0.10126582278481013 on epoch=174
05/18/2022 01:37:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.92 on epoch=177
05/18/2022 01:37:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.96 on epoch=179
05/18/2022 01:37:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.92 on epoch=182
05/18/2022 01:37:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.96 on epoch=184
05/18/2022 01:37:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.87 on epoch=187
05/18/2022 01:37:33 - INFO - __main__ - Global step 750 Train loss 0.93 Classification-F1 0.10126582278481013 on epoch=187
05/18/2022 01:37:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.90 on epoch=189
05/18/2022 01:37:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.89 on epoch=192
05/18/2022 01:37:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.82 on epoch=194
05/18/2022 01:37:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.85 on epoch=197
05/18/2022 01:37:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.91 on epoch=199
05/18/2022 01:37:40 - INFO - __main__ - Global step 800 Train loss 0.87 Classification-F1 0.10126582278481013 on epoch=199
05/18/2022 01:37:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.89 on epoch=202
05/18/2022 01:37:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.89 on epoch=204
05/18/2022 01:37:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.96 on epoch=207
05/18/2022 01:37:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.92 on epoch=209
05/18/2022 01:37:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.83 on epoch=212
05/18/2022 01:37:46 - INFO - __main__ - Global step 850 Train loss 0.90 Classification-F1 0.12145969498910676 on epoch=212
05/18/2022 01:37:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.86 on epoch=214
05/18/2022 01:37:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.86 on epoch=217
05/18/2022 01:37:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.97 on epoch=219
05/18/2022 01:37:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.89 on epoch=222
05/18/2022 01:37:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.86 on epoch=224
05/18/2022 01:37:53 - INFO - __main__ - Global step 900 Train loss 0.88 Classification-F1 0.10126582278481013 on epoch=224
05/18/2022 01:37:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.83 on epoch=227
05/18/2022 01:37:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.89 on epoch=229
05/18/2022 01:37:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.91 on epoch=232
05/18/2022 01:37:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.94 on epoch=234
05/18/2022 01:37:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.87 on epoch=237
05/18/2022 01:37:59 - INFO - __main__ - Global step 950 Train loss 0.89 Classification-F1 0.10126582278481013 on epoch=237
05/18/2022 01:38:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.84 on epoch=239
05/18/2022 01:38:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.82 on epoch=242
05/18/2022 01:38:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.84 on epoch=244
05/18/2022 01:38:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.89 on epoch=247
05/18/2022 01:38:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.87 on epoch=249
05/18/2022 01:38:06 - INFO - __main__ - Global step 1000 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=249
05/18/2022 01:38:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.83 on epoch=252
05/18/2022 01:38:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.87 on epoch=254
05/18/2022 01:38:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.82 on epoch=257
05/18/2022 01:38:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.88 on epoch=259
05/18/2022 01:38:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.83 on epoch=262
05/18/2022 01:38:12 - INFO - __main__ - Global step 1050 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=262
05/18/2022 01:38:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.88 on epoch=264
05/18/2022 01:38:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.85 on epoch=267
05/18/2022 01:38:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.83 on epoch=269
05/18/2022 01:38:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.82 on epoch=272
05/18/2022 01:38:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.87 on epoch=274
05/18/2022 01:38:19 - INFO - __main__ - Global step 1100 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=274
05/18/2022 01:38:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.80 on epoch=277
05/18/2022 01:38:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.93 on epoch=279
05/18/2022 01:38:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.87 on epoch=282
05/18/2022 01:38:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.82 on epoch=284
05/18/2022 01:38:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.83 on epoch=287
05/18/2022 01:38:25 - INFO - __main__ - Global step 1150 Train loss 0.85 Classification-F1 0.13067758749069247 on epoch=287
05/18/2022 01:38:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.85 on epoch=289
05/18/2022 01:38:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.84 on epoch=292
05/18/2022 01:38:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.80 on epoch=294
05/18/2022 01:38:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.84 on epoch=297
05/18/2022 01:38:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.94 on epoch=299
05/18/2022 01:38:32 - INFO - __main__ - Global step 1200 Train loss 0.85 Classification-F1 0.11111111111111112 on epoch=299
05/18/2022 01:38:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.87 on epoch=302
05/18/2022 01:38:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.90 on epoch=304
05/18/2022 01:38:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.88 on epoch=307
05/18/2022 01:38:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.81 on epoch=309
05/18/2022 01:38:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.89 on epoch=312
05/18/2022 01:38:39 - INFO - __main__ - Global step 1250 Train loss 0.87 Classification-F1 0.13067758749069247 on epoch=312
05/18/2022 01:38:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.84 on epoch=314
05/18/2022 01:38:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.86 on epoch=317
05/18/2022 01:38:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.81 on epoch=319
05/18/2022 01:38:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.89 on epoch=322
05/18/2022 01:38:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.80 on epoch=324
05/18/2022 01:38:45 - INFO - __main__ - Global step 1300 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=324
05/18/2022 01:38:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.87 on epoch=327
05/18/2022 01:38:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.85 on epoch=329
05/18/2022 01:38:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.89 on epoch=332
05/18/2022 01:38:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.80 on epoch=334
05/18/2022 01:38:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.84 on epoch=337
05/18/2022 01:38:52 - INFO - __main__ - Global step 1350 Train loss 0.85 Classification-F1 0.1622222222222222 on epoch=337
05/18/2022 01:38:52 - INFO - __main__ - Saving model with best Classification-F1: 0.15975177304964538 -> 0.1622222222222222 on epoch=337, global_step=1350
05/18/2022 01:38:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.87 on epoch=339
05/18/2022 01:38:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.75 on epoch=342
05/18/2022 01:38:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.86 on epoch=344
05/18/2022 01:38:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.72 on epoch=347
05/18/2022 01:38:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.93 on epoch=349
05/18/2022 01:38:58 - INFO - __main__ - Global step 1400 Train loss 0.83 Classification-F1 0.13067758749069247 on epoch=349
05/18/2022 01:38:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.81 on epoch=352
05/18/2022 01:39:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.95 on epoch=354
05/18/2022 01:39:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.81 on epoch=357
05/18/2022 01:39:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.94 on epoch=359
05/18/2022 01:39:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.88 on epoch=362
05/18/2022 01:39:05 - INFO - __main__ - Global step 1450 Train loss 0.88 Classification-F1 0.13197586726998492 on epoch=362
05/18/2022 01:39:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.85 on epoch=364
05/18/2022 01:39:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.88 on epoch=367
05/18/2022 01:39:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.78 on epoch=369
05/18/2022 01:39:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.80 on epoch=372
05/18/2022 01:39:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.89 on epoch=374
05/18/2022 01:39:11 - INFO - __main__ - Global step 1500 Train loss 0.84 Classification-F1 0.1581196581196581 on epoch=374
05/18/2022 01:39:12 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.89 on epoch=377
05/18/2022 01:39:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.80 on epoch=379
05/18/2022 01:39:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.90 on epoch=382
05/18/2022 01:39:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.89 on epoch=384
05/18/2022 01:39:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.89 on epoch=387
05/18/2022 01:39:18 - INFO - __main__ - Global step 1550 Train loss 0.87 Classification-F1 0.13197586726998492 on epoch=387
05/18/2022 01:39:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.76 on epoch=389
05/18/2022 01:39:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.88 on epoch=392
05/18/2022 01:39:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.78 on epoch=394
05/18/2022 01:39:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.84 on epoch=397
05/18/2022 01:39:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.81 on epoch=399
05/18/2022 01:39:24 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.10126582278481013 on epoch=399
05/18/2022 01:39:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.84 on epoch=402
05/18/2022 01:39:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.80 on epoch=404
05/18/2022 01:39:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.76 on epoch=407
05/18/2022 01:39:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.84 on epoch=409
05/18/2022 01:39:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.83 on epoch=412
05/18/2022 01:39:31 - INFO - __main__ - Global step 1650 Train loss 0.81 Classification-F1 0.13067758749069247 on epoch=412
05/18/2022 01:39:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.79 on epoch=414
05/18/2022 01:39:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.85 on epoch=417
05/18/2022 01:39:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.78 on epoch=419
05/18/2022 01:39:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.84 on epoch=422
05/18/2022 01:39:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.84 on epoch=424
05/18/2022 01:39:38 - INFO - __main__ - Global step 1700 Train loss 0.82 Classification-F1 0.18284347231715653 on epoch=424
05/18/2022 01:39:38 - INFO - __main__ - Saving model with best Classification-F1: 0.1622222222222222 -> 0.18284347231715653 on epoch=424, global_step=1700
05/18/2022 01:39:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.75 on epoch=427
05/18/2022 01:39:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.82 on epoch=429
05/18/2022 01:39:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.84 on epoch=432
05/18/2022 01:39:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.88 on epoch=434
05/18/2022 01:39:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.81 on epoch=437
05/18/2022 01:39:44 - INFO - __main__ - Global step 1750 Train loss 0.82 Classification-F1 0.24447174447174447 on epoch=437
05/18/2022 01:39:44 - INFO - __main__ - Saving model with best Classification-F1: 0.18284347231715653 -> 0.24447174447174447 on epoch=437, global_step=1750
05/18/2022 01:39:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.72 on epoch=439
05/18/2022 01:39:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.81 on epoch=442
05/18/2022 01:39:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.85 on epoch=444
05/18/2022 01:39:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.86 on epoch=447
05/18/2022 01:39:50 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.83 on epoch=449
05/18/2022 01:39:51 - INFO - __main__ - Global step 1800 Train loss 0.81 Classification-F1 0.20666666666666667 on epoch=449
05/18/2022 01:39:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.80 on epoch=452
05/18/2022 01:39:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.89 on epoch=454
05/18/2022 01:39:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.85 on epoch=457
05/18/2022 01:39:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.80 on epoch=459
05/18/2022 01:39:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.86 on epoch=462
05/18/2022 01:39:57 - INFO - __main__ - Global step 1850 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=462
05/18/2022 01:39:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.80 on epoch=464
05/18/2022 01:40:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.82 on epoch=467
05/18/2022 01:40:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.80 on epoch=469
05/18/2022 01:40:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.84 on epoch=472
05/18/2022 01:40:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.83 on epoch=474
05/18/2022 01:40:04 - INFO - __main__ - Global step 1900 Train loss 0.82 Classification-F1 0.15945165945165946 on epoch=474
05/18/2022 01:40:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.79 on epoch=477
05/18/2022 01:40:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.85 on epoch=479
05/18/2022 01:40:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.85 on epoch=482
05/18/2022 01:40:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.74 on epoch=484
05/18/2022 01:40:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.88 on epoch=487
05/18/2022 01:40:11 - INFO - __main__ - Global step 1950 Train loss 0.82 Classification-F1 0.1842105263157895 on epoch=487
05/18/2022 01:40:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.84 on epoch=489
05/18/2022 01:40:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.86 on epoch=492
05/18/2022 01:40:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.82 on epoch=494
05/18/2022 01:40:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.75 on epoch=497
05/18/2022 01:40:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.82 on epoch=499
05/18/2022 01:40:17 - INFO - __main__ - Global step 2000 Train loss 0.82 Classification-F1 0.13330786860198623 on epoch=499
05/18/2022 01:40:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.84 on epoch=502
05/18/2022 01:40:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.78 on epoch=504
05/18/2022 01:40:21 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.81 on epoch=507
05/18/2022 01:40:22 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.76 on epoch=509
05/18/2022 01:40:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.83 on epoch=512
05/18/2022 01:40:24 - INFO - __main__ - Global step 2050 Train loss 0.80 Classification-F1 0.1842105263157895 on epoch=512
05/18/2022 01:40:25 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.78 on epoch=514
05/18/2022 01:40:26 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.75 on epoch=517
05/18/2022 01:40:27 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.80 on epoch=519
05/18/2022 01:40:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.81 on epoch=522
05/18/2022 01:40:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.84 on epoch=524
05/18/2022 01:40:30 - INFO - __main__ - Global step 2100 Train loss 0.80 Classification-F1 0.13197586726998492 on epoch=524
05/18/2022 01:40:32 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.93 on epoch=527
05/18/2022 01:40:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.82 on epoch=529
05/18/2022 01:40:34 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.79 on epoch=532
05/18/2022 01:40:35 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.92 on epoch=534
05/18/2022 01:40:37 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.77 on epoch=537
05/18/2022 01:40:37 - INFO - __main__ - Global step 2150 Train loss 0.85 Classification-F1 0.230952380952381 on epoch=537
05/18/2022 01:40:38 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.80 on epoch=539
05/18/2022 01:40:39 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.82 on epoch=542
05/18/2022 01:40:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.81 on epoch=544
05/18/2022 01:40:42 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.83 on epoch=547
05/18/2022 01:40:43 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.88 on epoch=549
05/18/2022 01:40:44 - INFO - __main__ - Global step 2200 Train loss 0.83 Classification-F1 0.15596101454280342 on epoch=549
05/18/2022 01:40:45 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.80 on epoch=552
05/18/2022 01:40:46 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.85 on epoch=554
05/18/2022 01:40:47 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.81 on epoch=557
05/18/2022 01:40:49 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.79 on epoch=559
05/18/2022 01:40:50 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.76 on epoch=562
05/18/2022 01:40:50 - INFO - __main__ - Global step 2250 Train loss 0.80 Classification-F1 0.15945165945165946 on epoch=562
05/18/2022 01:40:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.87 on epoch=564
05/18/2022 01:40:53 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.83 on epoch=567
05/18/2022 01:40:54 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.84 on epoch=569
05/18/2022 01:40:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.88 on epoch=572
05/18/2022 01:40:56 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.84 on epoch=574
05/18/2022 01:40:57 - INFO - __main__ - Global step 2300 Train loss 0.85 Classification-F1 0.1842105263157895 on epoch=574
05/18/2022 01:40:58 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.74 on epoch=577
05/18/2022 01:40:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.77 on epoch=579
05/18/2022 01:41:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.82 on epoch=582
05/18/2022 01:41:02 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.81 on epoch=584
05/18/2022 01:41:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.81 on epoch=587
05/18/2022 01:41:04 - INFO - __main__ - Global step 2350 Train loss 0.79 Classification-F1 0.23751987281399045 on epoch=587
05/18/2022 01:41:05 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.81 on epoch=589
05/18/2022 01:41:06 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.78 on epoch=592
05/18/2022 01:41:07 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.80 on epoch=594
05/18/2022 01:41:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.76 on epoch=597
05/18/2022 01:41:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.77 on epoch=599
05/18/2022 01:41:10 - INFO - __main__ - Global step 2400 Train loss 0.78 Classification-F1 0.2081081081081081 on epoch=599
05/18/2022 01:41:11 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.80 on epoch=602
05/18/2022 01:41:13 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.80 on epoch=604
05/18/2022 01:41:14 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.88 on epoch=607
05/18/2022 01:41:15 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.87 on epoch=609
05/18/2022 01:41:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.81 on epoch=612
05/18/2022 01:41:17 - INFO - __main__ - Global step 2450 Train loss 0.83 Classification-F1 0.20666666666666667 on epoch=612
05/18/2022 01:41:18 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.83 on epoch=614
05/18/2022 01:41:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.75 on epoch=617
05/18/2022 01:41:20 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.78 on epoch=619
05/18/2022 01:41:22 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.83 on epoch=622
05/18/2022 01:41:23 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.81 on epoch=624
05/18/2022 01:41:23 - INFO - __main__ - Global step 2500 Train loss 0.80 Classification-F1 0.20334620334620335 on epoch=624
05/18/2022 01:41:25 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.79 on epoch=627
05/18/2022 01:41:26 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.85 on epoch=629
05/18/2022 01:41:27 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.86 on epoch=632
05/18/2022 01:41:28 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.86 on epoch=634
05/18/2022 01:41:30 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.83 on epoch=637
05/18/2022 01:41:30 - INFO - __main__ - Global step 2550 Train loss 0.84 Classification-F1 0.19082633053221287 on epoch=637
05/18/2022 01:41:31 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.82 on epoch=639
05/18/2022 01:41:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.77 on epoch=642
05/18/2022 01:41:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.84 on epoch=644
05/18/2022 01:41:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.84 on epoch=647
05/18/2022 01:41:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.84 on epoch=649
05/18/2022 01:41:37 - INFO - __main__ - Global step 2600 Train loss 0.82 Classification-F1 0.22322540473225405 on epoch=649
05/18/2022 01:41:38 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.80 on epoch=652
05/18/2022 01:41:39 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.86 on epoch=654
05/18/2022 01:41:40 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.87 on epoch=657
05/18/2022 01:41:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.84 on epoch=659
05/18/2022 01:41:43 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.79 on epoch=662
05/18/2022 01:41:43 - INFO - __main__ - Global step 2650 Train loss 0.83 Classification-F1 0.21927016645326503 on epoch=662
05/18/2022 01:41:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.82 on epoch=664
05/18/2022 01:41:46 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.78 on epoch=667
05/18/2022 01:41:47 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.86 on epoch=669
05/18/2022 01:41:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.81 on epoch=672
05/18/2022 01:41:49 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.84 on epoch=674
05/18/2022 01:41:50 - INFO - __main__ - Global step 2700 Train loss 0.82 Classification-F1 0.17635135135135135 on epoch=674
05/18/2022 01:41:51 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.78 on epoch=677
05/18/2022 01:41:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.76 on epoch=679
05/18/2022 01:41:53 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.81 on epoch=682
05/18/2022 01:41:55 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.74 on epoch=684
05/18/2022 01:41:56 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.79 on epoch=687
05/18/2022 01:41:56 - INFO - __main__ - Global step 2750 Train loss 0.78 Classification-F1 0.175 on epoch=687
05/18/2022 01:41:58 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.88 on epoch=689
05/18/2022 01:41:59 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.79 on epoch=692
05/18/2022 01:42:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.78 on epoch=694
05/18/2022 01:42:01 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.75 on epoch=697
05/18/2022 01:42:03 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.76 on epoch=699
05/18/2022 01:42:03 - INFO - __main__ - Global step 2800 Train loss 0.79 Classification-F1 0.18997945313734785 on epoch=699
05/18/2022 01:42:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.78 on epoch=702
05/18/2022 01:42:05 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.86 on epoch=704
05/18/2022 01:42:07 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.78 on epoch=707
05/18/2022 01:42:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.75 on epoch=709
05/18/2022 01:42:09 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.84 on epoch=712
05/18/2022 01:42:10 - INFO - __main__ - Global step 2850 Train loss 0.80 Classification-F1 0.17644927536231886 on epoch=712
05/18/2022 01:42:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.84 on epoch=714
05/18/2022 01:42:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.85 on epoch=717
05/18/2022 01:42:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.77 on epoch=719
05/18/2022 01:42:15 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.87 on epoch=722
05/18/2022 01:42:16 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.79 on epoch=724
05/18/2022 01:42:16 - INFO - __main__ - Global step 2900 Train loss 0.83 Classification-F1 0.19797782126549252 on epoch=724
05/18/2022 01:42:18 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.84 on epoch=727
05/18/2022 01:42:19 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.79 on epoch=729
05/18/2022 01:42:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.81 on epoch=732
05/18/2022 01:42:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.77 on epoch=734
05/18/2022 01:42:23 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.77 on epoch=737
05/18/2022 01:42:23 - INFO - __main__ - Global step 2950 Train loss 0.80 Classification-F1 0.2306338028169014 on epoch=737
05/18/2022 01:42:24 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.80 on epoch=739
05/18/2022 01:42:26 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.77 on epoch=742
05/18/2022 01:42:27 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.79 on epoch=744
05/18/2022 01:42:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.77 on epoch=747
05/18/2022 01:42:29 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.86 on epoch=749
05/18/2022 01:42:30 - INFO - __main__ - Global step 3000 Train loss 0.80 Classification-F1 0.19797782126549252 on epoch=749
05/18/2022 01:42:30 - INFO - __main__ - save last model!
05/18/2022 01:42:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 01:42:30 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 01:42:30 - INFO - __main__ - Printing 3 examples
05/18/2022 01:42:30 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 01:42:30 - INFO - __main__ - ['others']
05/18/2022 01:42:30 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 01:42:30 - INFO - __main__ - ['others']
05/18/2022 01:42:30 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 01:42:30 - INFO - __main__ - ['others']
05/18/2022 01:42:30 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:42:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:42:30 - INFO - __main__ - Printing 3 examples
05/18/2022 01:42:30 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/18/2022 01:42:30 - INFO - __main__ - ['others']
05/18/2022 01:42:30 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/18/2022 01:42:30 - INFO - __main__ - ['others']
05/18/2022 01:42:30 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/18/2022 01:42:30 - INFO - __main__ - ['others']
05/18/2022 01:42:30 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:42:30 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:42:30 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 01:42:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:42:30 - INFO - __main__ - Printing 3 examples
05/18/2022 01:42:30 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/18/2022 01:42:30 - INFO - __main__ - ['others']
05/18/2022 01:42:30 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/18/2022 01:42:30 - INFO - __main__ - ['others']
05/18/2022 01:42:30 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/18/2022 01:42:30 - INFO - __main__ - ['others']
05/18/2022 01:42:30 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:42:30 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:42:31 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 01:42:32 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:42:37 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 01:42:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 01:42:37 - INFO - __main__ - Starting training!
05/18/2022 01:42:37 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 01:43:22 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_42_0.2_8_predictions.txt
05/18/2022 01:43:22 - INFO - __main__ - Classification-F1 on test data: 0.1017
05/18/2022 01:43:22 - INFO - __main__ - prefix=emo_16_42, lr=0.2, bsz=8, dev_performance=0.24447174447174447, test_performance=0.10169664156978525
05/18/2022 01:43:22 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.5, bsz=8 ...
05/18/2022 01:43:23 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:43:23 - INFO - __main__ - Printing 3 examples
05/18/2022 01:43:23 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/18/2022 01:43:23 - INFO - __main__ - ['others']
05/18/2022 01:43:23 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/18/2022 01:43:23 - INFO - __main__ - ['others']
05/18/2022 01:43:23 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/18/2022 01:43:23 - INFO - __main__ - ['others']
05/18/2022 01:43:23 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:43:23 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:43:23 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 01:43:23 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:43:23 - INFO - __main__ - Printing 3 examples
05/18/2022 01:43:23 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/18/2022 01:43:23 - INFO - __main__ - ['others']
05/18/2022 01:43:23 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/18/2022 01:43:23 - INFO - __main__ - ['others']
05/18/2022 01:43:23 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/18/2022 01:43:23 - INFO - __main__ - ['others']
05/18/2022 01:43:23 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:43:23 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:43:23 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 01:43:29 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 01:43:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 01:43:29 - INFO - __main__ - Starting training!
05/18/2022 01:43:31 - INFO - __main__ - Step 10 Global step 10 Train loss 4.17 on epoch=2
05/18/2022 01:43:32 - INFO - __main__ - Step 20 Global step 20 Train loss 3.62 on epoch=4
05/18/2022 01:43:33 - INFO - __main__ - Step 30 Global step 30 Train loss 3.00 on epoch=7
05/18/2022 01:43:35 - INFO - __main__ - Step 40 Global step 40 Train loss 2.39 on epoch=9
05/18/2022 01:43:36 - INFO - __main__ - Step 50 Global step 50 Train loss 1.87 on epoch=12
05/18/2022 01:43:36 - INFO - __main__ - Global step 50 Train loss 3.01 Classification-F1 0.1 on epoch=12
05/18/2022 01:43:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/18/2022 01:43:38 - INFO - __main__ - Step 60 Global step 60 Train loss 1.45 on epoch=14
05/18/2022 01:43:39 - INFO - __main__ - Step 70 Global step 70 Train loss 1.41 on epoch=17
05/18/2022 01:43:40 - INFO - __main__ - Step 80 Global step 80 Train loss 1.22 on epoch=19
05/18/2022 01:43:41 - INFO - __main__ - Step 90 Global step 90 Train loss 1.10 on epoch=22
05/18/2022 01:43:43 - INFO - __main__ - Step 100 Global step 100 Train loss 0.95 on epoch=24
05/18/2022 01:43:43 - INFO - __main__ - Global step 100 Train loss 1.23 Classification-F1 0.1 on epoch=24
05/18/2022 01:43:44 - INFO - __main__ - Step 110 Global step 110 Train loss 1.06 on epoch=27
05/18/2022 01:43:46 - INFO - __main__ - Step 120 Global step 120 Train loss 1.01 on epoch=29
05/18/2022 01:43:47 - INFO - __main__ - Step 130 Global step 130 Train loss 1.09 on epoch=32
05/18/2022 01:43:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.98 on epoch=34
05/18/2022 01:43:49 - INFO - __main__ - Step 150 Global step 150 Train loss 1.01 on epoch=37
05/18/2022 01:43:50 - INFO - __main__ - Global step 150 Train loss 1.03 Classification-F1 0.13034188034188032 on epoch=37
05/18/2022 01:43:50 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.13034188034188032 on epoch=37, global_step=150
05/18/2022 01:43:51 - INFO - __main__ - Step 160 Global step 160 Train loss 1.02 on epoch=39
05/18/2022 01:43:53 - INFO - __main__ - Step 170 Global step 170 Train loss 1.02 on epoch=42
05/18/2022 01:43:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.86 on epoch=44
05/18/2022 01:43:55 - INFO - __main__ - Step 190 Global step 190 Train loss 1.04 on epoch=47
05/18/2022 01:43:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.97 on epoch=49
05/18/2022 01:43:57 - INFO - __main__ - Global step 200 Train loss 0.98 Classification-F1 0.1 on epoch=49
05/18/2022 01:43:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.92 on epoch=52
05/18/2022 01:43:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.92 on epoch=54
05/18/2022 01:44:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.96 on epoch=57
05/18/2022 01:44:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.97 on epoch=59
05/18/2022 01:44:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.87 on epoch=62
05/18/2022 01:44:03 - INFO - __main__ - Global step 250 Train loss 0.93 Classification-F1 0.1 on epoch=62
05/18/2022 01:44:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=64
05/18/2022 01:44:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.93 on epoch=67
05/18/2022 01:44:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.90 on epoch=69
05/18/2022 01:44:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.91 on epoch=72
05/18/2022 01:44:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.95 on epoch=74
05/18/2022 01:44:10 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.1 on epoch=74
05/18/2022 01:44:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.95 on epoch=77
05/18/2022 01:44:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.86 on epoch=79
05/18/2022 01:44:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.88 on epoch=82
05/18/2022 01:44:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.84 on epoch=84
05/18/2022 01:44:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.84 on epoch=87
05/18/2022 01:44:17 - INFO - __main__ - Global step 350 Train loss 0.88 Classification-F1 0.1 on epoch=87
05/18/2022 01:44:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.92 on epoch=89
05/18/2022 01:44:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.81 on epoch=92
05/18/2022 01:44:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.86 on epoch=94
05/18/2022 01:44:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=97
05/18/2022 01:44:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.82 on epoch=99
05/18/2022 01:44:24 - INFO - __main__ - Global step 400 Train loss 0.86 Classification-F1 0.17712418300653593 on epoch=99
05/18/2022 01:44:24 - INFO - __main__ - Saving model with best Classification-F1: 0.13034188034188032 -> 0.17712418300653593 on epoch=99, global_step=400
05/18/2022 01:44:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.79 on epoch=102
05/18/2022 01:44:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.81 on epoch=104
05/18/2022 01:44:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.85 on epoch=107
05/18/2022 01:44:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.92 on epoch=109
05/18/2022 01:44:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.88 on epoch=112
05/18/2022 01:44:31 - INFO - __main__ - Global step 450 Train loss 0.85 Classification-F1 0.1 on epoch=112
05/18/2022 01:44:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.83 on epoch=114
05/18/2022 01:44:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.82 on epoch=117
05/18/2022 01:44:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.81 on epoch=119
05/18/2022 01:44:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.79 on epoch=122
05/18/2022 01:44:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.81 on epoch=124
05/18/2022 01:44:37 - INFO - __main__ - Global step 500 Train loss 0.81 Classification-F1 0.15505279034690797 on epoch=124
05/18/2022 01:44:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.88 on epoch=127
05/18/2022 01:44:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.81 on epoch=129
05/18/2022 01:44:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.92 on epoch=132
05/18/2022 01:44:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.84 on epoch=134
05/18/2022 01:44:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=137
05/18/2022 01:44:44 - INFO - __main__ - Global step 550 Train loss 0.86 Classification-F1 0.09493670886075949 on epoch=137
05/18/2022 01:44:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.88 on epoch=139
05/18/2022 01:44:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.90 on epoch=142
05/18/2022 01:44:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.83 on epoch=144
05/18/2022 01:44:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.82 on epoch=147
05/18/2022 01:44:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.82 on epoch=149
05/18/2022 01:44:51 - INFO - __main__ - Global step 600 Train loss 0.85 Classification-F1 0.1 on epoch=149
05/18/2022 01:44:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.89 on epoch=152
05/18/2022 01:44:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.78 on epoch=154
05/18/2022 01:44:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.88 on epoch=157
05/18/2022 01:44:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.81 on epoch=159
05/18/2022 01:44:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.87 on epoch=162
05/18/2022 01:44:58 - INFO - __main__ - Global step 650 Train loss 0.84 Classification-F1 0.1 on epoch=162
05/18/2022 01:44:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.87 on epoch=164
05/18/2022 01:45:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.87 on epoch=167
05/18/2022 01:45:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.81 on epoch=169
05/18/2022 01:45:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.76 on epoch=172
05/18/2022 01:45:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.85 on epoch=174
05/18/2022 01:45:04 - INFO - __main__ - Global step 700 Train loss 0.83 Classification-F1 0.1 on epoch=174
05/18/2022 01:45:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.83 on epoch=177
05/18/2022 01:45:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.82 on epoch=179
05/18/2022 01:45:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.86 on epoch=182
05/18/2022 01:45:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.86 on epoch=184
05/18/2022 01:45:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.77 on epoch=187
05/18/2022 01:45:11 - INFO - __main__ - Global step 750 Train loss 0.83 Classification-F1 0.1 on epoch=187
05/18/2022 01:45:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.88 on epoch=189
05/18/2022 01:45:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.81 on epoch=192
05/18/2022 01:45:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.80 on epoch=194
05/18/2022 01:45:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.81 on epoch=197
05/18/2022 01:45:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.81 on epoch=199
05/18/2022 01:45:18 - INFO - __main__ - Global step 800 Train loss 0.82 Classification-F1 0.1 on epoch=199
05/18/2022 01:45:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.85 on epoch=202
05/18/2022 01:45:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.85 on epoch=204
05/18/2022 01:45:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=207
05/18/2022 01:45:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.86 on epoch=209
05/18/2022 01:45:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.78 on epoch=212
05/18/2022 01:45:24 - INFO - __main__ - Global step 850 Train loss 0.83 Classification-F1 0.1 on epoch=212
05/18/2022 01:45:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.85 on epoch=214
05/18/2022 01:45:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.85 on epoch=217
05/18/2022 01:45:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.83 on epoch=219
05/18/2022 01:45:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.83 on epoch=222
05/18/2022 01:45:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.80 on epoch=224
05/18/2022 01:45:31 - INFO - __main__ - Global step 900 Train loss 0.83 Classification-F1 0.1 on epoch=224
05/18/2022 01:45:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.86 on epoch=227
05/18/2022 01:45:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.89 on epoch=229
05/18/2022 01:45:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.74 on epoch=232
05/18/2022 01:45:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.78 on epoch=234
05/18/2022 01:45:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.88 on epoch=237
05/18/2022 01:45:38 - INFO - __main__ - Global step 950 Train loss 0.83 Classification-F1 0.14460784313725492 on epoch=237
05/18/2022 01:45:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.79 on epoch=239
05/18/2022 01:45:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.88 on epoch=242
05/18/2022 01:45:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.77 on epoch=244
05/18/2022 01:45:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.83 on epoch=247
05/18/2022 01:45:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.79 on epoch=249
05/18/2022 01:45:45 - INFO - __main__ - Global step 1000 Train loss 0.81 Classification-F1 0.1 on epoch=249
05/18/2022 01:45:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.75 on epoch=252
05/18/2022 01:45:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.86 on epoch=254
05/18/2022 01:45:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.85 on epoch=257
05/18/2022 01:45:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.82 on epoch=259
05/18/2022 01:45:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.86 on epoch=262
05/18/2022 01:45:51 - INFO - __main__ - Global step 1050 Train loss 0.83 Classification-F1 0.1422354557947778 on epoch=262
05/18/2022 01:45:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.80 on epoch=264
05/18/2022 01:45:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.78 on epoch=267
05/18/2022 01:45:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.79 on epoch=269
05/18/2022 01:45:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.84 on epoch=272
05/18/2022 01:45:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.77 on epoch=274
05/18/2022 01:45:58 - INFO - __main__ - Global step 1100 Train loss 0.80 Classification-F1 0.1 on epoch=274
05/18/2022 01:45:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.73 on epoch=277
05/18/2022 01:46:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.81 on epoch=279
05/18/2022 01:46:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.77 on epoch=282
05/18/2022 01:46:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.82 on epoch=284
05/18/2022 01:46:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.81 on epoch=287
05/18/2022 01:46:05 - INFO - __main__ - Global step 1150 Train loss 0.79 Classification-F1 0.2085137085137085 on epoch=287
05/18/2022 01:46:05 - INFO - __main__ - Saving model with best Classification-F1: 0.17712418300653593 -> 0.2085137085137085 on epoch=287, global_step=1150
05/18/2022 01:46:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.79 on epoch=289
05/18/2022 01:46:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.85 on epoch=292
05/18/2022 01:46:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.85 on epoch=294
05/18/2022 01:46:10 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.83 on epoch=297
05/18/2022 01:46:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.77 on epoch=299
05/18/2022 01:46:12 - INFO - __main__ - Global step 1200 Train loss 0.82 Classification-F1 0.10126582278481013 on epoch=299
05/18/2022 01:46:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.80 on epoch=302
05/18/2022 01:46:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.82 on epoch=304
05/18/2022 01:46:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.80 on epoch=307
05/18/2022 01:46:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.81 on epoch=309
05/18/2022 01:46:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.83 on epoch=312
05/18/2022 01:46:18 - INFO - __main__ - Global step 1250 Train loss 0.82 Classification-F1 0.19444444444444445 on epoch=312
05/18/2022 01:46:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.78 on epoch=314
05/18/2022 01:46:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.83 on epoch=317
05/18/2022 01:46:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.78 on epoch=319
05/18/2022 01:46:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.81 on epoch=322
05/18/2022 01:46:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.86 on epoch=324
05/18/2022 01:46:25 - INFO - __main__ - Global step 1300 Train loss 0.81 Classification-F1 0.1 on epoch=324
05/18/2022 01:46:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.84 on epoch=327
05/18/2022 01:46:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.82 on epoch=329
05/18/2022 01:46:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.84 on epoch=332
05/18/2022 01:46:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.89 on epoch=334
05/18/2022 01:46:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.82 on epoch=337
05/18/2022 01:46:32 - INFO - __main__ - Global step 1350 Train loss 0.84 Classification-F1 0.1095890410958904 on epoch=337
05/18/2022 01:46:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.83 on epoch=339
05/18/2022 01:46:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.74 on epoch=342
05/18/2022 01:46:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.74 on epoch=344
05/18/2022 01:46:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.76 on epoch=347
05/18/2022 01:46:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.79 on epoch=349
05/18/2022 01:46:38 - INFO - __main__ - Global step 1400 Train loss 0.77 Classification-F1 0.10126582278481013 on epoch=349
05/18/2022 01:46:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.77 on epoch=352
05/18/2022 01:46:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.75 on epoch=354
05/18/2022 01:46:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.73 on epoch=357
05/18/2022 01:46:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.80 on epoch=359
05/18/2022 01:46:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.76 on epoch=362
05/18/2022 01:46:45 - INFO - __main__ - Global step 1450 Train loss 0.76 Classification-F1 0.22377622377622375 on epoch=362
05/18/2022 01:46:45 - INFO - __main__ - Saving model with best Classification-F1: 0.2085137085137085 -> 0.22377622377622375 on epoch=362, global_step=1450
05/18/2022 01:46:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.75 on epoch=364
05/18/2022 01:46:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.78 on epoch=367
05/18/2022 01:46:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.85 on epoch=369
05/18/2022 01:46:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.85 on epoch=372
05/18/2022 01:46:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.81 on epoch=374
05/18/2022 01:46:52 - INFO - __main__ - Global step 1500 Train loss 0.81 Classification-F1 0.2375598086124402 on epoch=374
05/18/2022 01:46:52 - INFO - __main__ - Saving model with best Classification-F1: 0.22377622377622375 -> 0.2375598086124402 on epoch=374, global_step=1500
05/18/2022 01:46:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.79 on epoch=377
05/18/2022 01:46:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.82 on epoch=379
05/18/2022 01:46:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.82 on epoch=382
05/18/2022 01:46:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.75 on epoch=384
05/18/2022 01:46:58 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.83 on epoch=387
05/18/2022 01:46:59 - INFO - __main__ - Global step 1550 Train loss 0.80 Classification-F1 0.26572864615294706 on epoch=387
05/18/2022 01:46:59 - INFO - __main__ - Saving model with best Classification-F1: 0.2375598086124402 -> 0.26572864615294706 on epoch=387, global_step=1550
05/18/2022 01:47:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.73 on epoch=389
05/18/2022 01:47:01 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.80 on epoch=392
05/18/2022 01:47:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.80 on epoch=394
05/18/2022 01:47:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.77 on epoch=397
05/18/2022 01:47:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.77 on epoch=399
05/18/2022 01:47:05 - INFO - __main__ - Global step 1600 Train loss 0.77 Classification-F1 0.2610344827586207 on epoch=399
05/18/2022 01:47:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.82 on epoch=402
05/18/2022 01:47:08 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.76 on epoch=404
05/18/2022 01:47:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.80 on epoch=407
05/18/2022 01:47:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.75 on epoch=409
05/18/2022 01:47:12 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.84 on epoch=412
05/18/2022 01:47:12 - INFO - __main__ - Global step 1650 Train loss 0.79 Classification-F1 0.45157237398616706 on epoch=412
05/18/2022 01:47:12 - INFO - __main__ - Saving model with best Classification-F1: 0.26572864615294706 -> 0.45157237398616706 on epoch=412, global_step=1650
05/18/2022 01:47:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.77 on epoch=414
05/18/2022 01:47:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.78 on epoch=417
05/18/2022 01:47:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.72 on epoch=419
05/18/2022 01:47:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.79 on epoch=422
05/18/2022 01:47:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.74 on epoch=424
05/18/2022 01:47:19 - INFO - __main__ - Global step 1700 Train loss 0.76 Classification-F1 0.2416666666666667 on epoch=424
05/18/2022 01:47:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.78 on epoch=427
05/18/2022 01:47:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.79 on epoch=429
05/18/2022 01:47:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.80 on epoch=432
05/18/2022 01:47:24 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.75 on epoch=434
05/18/2022 01:47:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.76 on epoch=437
05/18/2022 01:47:26 - INFO - __main__ - Global step 1750 Train loss 0.78 Classification-F1 0.4458562271062271 on epoch=437
05/18/2022 01:47:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.80 on epoch=439
05/18/2022 01:47:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.71 on epoch=442
05/18/2022 01:47:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.79 on epoch=444
05/18/2022 01:47:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.82 on epoch=447
05/18/2022 01:47:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.72 on epoch=449
05/18/2022 01:47:32 - INFO - __main__ - Global step 1800 Train loss 0.77 Classification-F1 0.46711018905596813 on epoch=449
05/18/2022 01:47:32 - INFO - __main__ - Saving model with best Classification-F1: 0.45157237398616706 -> 0.46711018905596813 on epoch=449, global_step=1800
05/18/2022 01:47:34 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.75 on epoch=452
05/18/2022 01:47:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.78 on epoch=454
05/18/2022 01:47:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.71 on epoch=457
05/18/2022 01:47:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.79 on epoch=459
05/18/2022 01:47:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.82 on epoch=462
05/18/2022 01:47:39 - INFO - __main__ - Global step 1850 Train loss 0.77 Classification-F1 0.4533938019652305 on epoch=462
05/18/2022 01:47:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.74 on epoch=464
05/18/2022 01:47:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.71 on epoch=467
05/18/2022 01:47:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.72 on epoch=469
05/18/2022 01:47:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.74 on epoch=472
05/18/2022 01:47:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.73 on epoch=474
05/18/2022 01:47:46 - INFO - __main__ - Global step 1900 Train loss 0.73 Classification-F1 0.48836618876941457 on epoch=474
05/18/2022 01:47:46 - INFO - __main__ - Saving model with best Classification-F1: 0.46711018905596813 -> 0.48836618876941457 on epoch=474, global_step=1900
05/18/2022 01:47:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.72 on epoch=477
05/18/2022 01:47:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.77 on epoch=479
05/18/2022 01:47:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.72 on epoch=482
05/18/2022 01:47:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.72 on epoch=484
05/18/2022 01:47:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.69 on epoch=487
05/18/2022 01:47:53 - INFO - __main__ - Global step 1950 Train loss 0.72 Classification-F1 0.4438177368144273 on epoch=487
05/18/2022 01:47:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.74 on epoch=489
05/18/2022 01:47:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.72 on epoch=492
05/18/2022 01:47:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.71 on epoch=494
05/18/2022 01:47:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.72 on epoch=497
05/18/2022 01:47:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.75 on epoch=499
05/18/2022 01:47:59 - INFO - __main__ - Global step 2000 Train loss 0.73 Classification-F1 0.5103174603174603 on epoch=499
05/18/2022 01:47:59 - INFO - __main__ - Saving model with best Classification-F1: 0.48836618876941457 -> 0.5103174603174603 on epoch=499, global_step=2000
05/18/2022 01:48:01 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.67 on epoch=502
05/18/2022 01:48:02 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.73 on epoch=504
05/18/2022 01:48:03 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.72 on epoch=507
05/18/2022 01:48:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.68 on epoch=509
05/18/2022 01:48:06 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.69 on epoch=512
05/18/2022 01:48:06 - INFO - __main__ - Global step 2050 Train loss 0.70 Classification-F1 0.5089686495936495 on epoch=512
05/18/2022 01:48:07 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.66 on epoch=514
05/18/2022 01:48:09 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.66 on epoch=517
05/18/2022 01:48:10 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.68 on epoch=519
05/18/2022 01:48:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.73 on epoch=522
05/18/2022 01:48:12 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.68 on epoch=524
05/18/2022 01:48:13 - INFO - __main__ - Global step 2100 Train loss 0.68 Classification-F1 0.4750646412411118 on epoch=524
05/18/2022 01:48:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.69 on epoch=527
05/18/2022 01:48:15 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.74 on epoch=529
05/18/2022 01:48:17 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.71 on epoch=532
05/18/2022 01:48:18 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.69 on epoch=534
05/18/2022 01:48:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.72 on epoch=537
05/18/2022 01:48:20 - INFO - __main__ - Global step 2150 Train loss 0.71 Classification-F1 0.47826953748006373 on epoch=537
05/18/2022 01:48:21 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.66 on epoch=539
05/18/2022 01:48:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.63 on epoch=542
05/18/2022 01:48:23 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.63 on epoch=544
05/18/2022 01:48:25 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.66 on epoch=547
05/18/2022 01:48:26 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.71 on epoch=549
05/18/2022 01:48:27 - INFO - __main__ - Global step 2200 Train loss 0.66 Classification-F1 0.47596196373286154 on epoch=549
05/18/2022 01:48:28 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.72 on epoch=552
05/18/2022 01:48:29 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.66 on epoch=554
05/18/2022 01:48:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.66 on epoch=557
05/18/2022 01:48:32 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.64 on epoch=559
05/18/2022 01:48:33 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.67 on epoch=562
05/18/2022 01:48:33 - INFO - __main__ - Global step 2250 Train loss 0.67 Classification-F1 0.5218000243575691 on epoch=562
05/18/2022 01:48:33 - INFO - __main__ - Saving model with best Classification-F1: 0.5103174603174603 -> 0.5218000243575691 on epoch=562, global_step=2250
05/18/2022 01:48:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.68 on epoch=564
05/18/2022 01:48:36 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.70 on epoch=567
05/18/2022 01:48:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.68 on epoch=569
05/18/2022 01:48:38 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.64 on epoch=572
05/18/2022 01:48:40 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.69 on epoch=574
05/18/2022 01:48:40 - INFO - __main__ - Global step 2300 Train loss 0.68 Classification-F1 0.45149667405764965 on epoch=574
05/18/2022 01:48:41 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.64 on epoch=577
05/18/2022 01:48:43 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.65 on epoch=579
05/18/2022 01:48:44 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.69 on epoch=582
05/18/2022 01:48:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.65 on epoch=584
05/18/2022 01:48:46 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.58 on epoch=587
05/18/2022 01:48:47 - INFO - __main__ - Global step 2350 Train loss 0.64 Classification-F1 0.48713450292397664 on epoch=587
05/18/2022 01:48:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.68 on epoch=589
05/18/2022 01:48:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.63 on epoch=592
05/18/2022 01:48:51 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.63 on epoch=594
05/18/2022 01:48:52 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.62 on epoch=597
05/18/2022 01:48:53 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.60 on epoch=599
05/18/2022 01:48:54 - INFO - __main__ - Global step 2400 Train loss 0.63 Classification-F1 0.45467658327698895 on epoch=599
05/18/2022 01:48:55 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.60 on epoch=602
05/18/2022 01:48:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.56 on epoch=604
05/18/2022 01:48:58 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.59 on epoch=607
05/18/2022 01:48:59 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.62 on epoch=609
05/18/2022 01:49:00 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.54 on epoch=612
05/18/2022 01:49:01 - INFO - __main__ - Global step 2450 Train loss 0.58 Classification-F1 0.5586528326745719 on epoch=612
05/18/2022 01:49:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5218000243575691 -> 0.5586528326745719 on epoch=612, global_step=2450
05/18/2022 01:49:02 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.62 on epoch=614
05/18/2022 01:49:03 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.55 on epoch=617
05/18/2022 01:49:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.58 on epoch=619
05/18/2022 01:49:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.63 on epoch=622
05/18/2022 01:49:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.63 on epoch=624
05/18/2022 01:49:07 - INFO - __main__ - Global step 2500 Train loss 0.60 Classification-F1 0.5548460144927536 on epoch=624
05/18/2022 01:49:09 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.57 on epoch=627
05/18/2022 01:49:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.63 on epoch=629
05/18/2022 01:49:11 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.57 on epoch=632
05/18/2022 01:49:12 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.70 on epoch=634
05/18/2022 01:49:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.62 on epoch=637
05/18/2022 01:49:14 - INFO - __main__ - Global step 2550 Train loss 0.62 Classification-F1 0.538949013949014 on epoch=637
05/18/2022 01:49:15 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.52 on epoch=639
05/18/2022 01:49:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.56 on epoch=642
05/18/2022 01:49:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.56 on epoch=644
05/18/2022 01:49:19 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.63 on epoch=647
05/18/2022 01:49:20 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.55 on epoch=649
05/18/2022 01:49:21 - INFO - __main__ - Global step 2600 Train loss 0.56 Classification-F1 0.4865216201423098 on epoch=649
05/18/2022 01:49:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.59 on epoch=652
05/18/2022 01:49:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.64 on epoch=654
05/18/2022 01:49:25 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.59 on epoch=657
05/18/2022 01:49:26 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.58 on epoch=659
05/18/2022 01:49:27 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.55 on epoch=662
05/18/2022 01:49:28 - INFO - __main__ - Global step 2650 Train loss 0.59 Classification-F1 0.5477063504446872 on epoch=662
05/18/2022 01:49:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.58 on epoch=664
05/18/2022 01:49:30 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.49 on epoch=667
05/18/2022 01:49:31 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.52 on epoch=669
05/18/2022 01:49:33 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.48 on epoch=672
05/18/2022 01:49:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.55 on epoch=674
05/18/2022 01:49:34 - INFO - __main__ - Global step 2700 Train loss 0.52 Classification-F1 0.5478571428571428 on epoch=674
05/18/2022 01:49:36 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.55 on epoch=677
05/18/2022 01:49:37 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.58 on epoch=679
05/18/2022 01:49:38 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.51 on epoch=682
05/18/2022 01:49:39 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.47 on epoch=684
05/18/2022 01:49:41 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.50 on epoch=687
05/18/2022 01:49:41 - INFO - __main__ - Global step 2750 Train loss 0.52 Classification-F1 0.5095424836601308 on epoch=687
05/18/2022 01:49:42 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.51 on epoch=689
05/18/2022 01:49:44 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.52 on epoch=692
05/18/2022 01:49:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.54 on epoch=694
05/18/2022 01:49:46 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.55 on epoch=697
05/18/2022 01:49:47 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.60 on epoch=699
05/18/2022 01:49:48 - INFO - __main__ - Global step 2800 Train loss 0.54 Classification-F1 0.524859943977591 on epoch=699
05/18/2022 01:49:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.41 on epoch=702
05/18/2022 01:49:50 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.49 on epoch=704
05/18/2022 01:49:52 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.49 on epoch=707
05/18/2022 01:49:53 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.55 on epoch=709
05/18/2022 01:49:54 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.51 on epoch=712
05/18/2022 01:49:55 - INFO - __main__ - Global step 2850 Train loss 0.49 Classification-F1 0.517153037357641 on epoch=712
05/18/2022 01:49:56 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.52 on epoch=714
05/18/2022 01:49:57 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.41 on epoch=717
05/18/2022 01:49:58 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.41 on epoch=719
05/18/2022 01:50:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.48 on epoch=722
05/18/2022 01:50:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.49 on epoch=724
05/18/2022 01:50:01 - INFO - __main__ - Global step 2900 Train loss 0.46 Classification-F1 0.5468494468494468 on epoch=724
05/18/2022 01:50:03 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.50 on epoch=727
05/18/2022 01:50:04 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.45 on epoch=729
05/18/2022 01:50:05 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.41 on epoch=732
05/18/2022 01:50:06 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.44 on epoch=734
05/18/2022 01:50:08 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.40 on epoch=737
05/18/2022 01:50:08 - INFO - __main__ - Global step 2950 Train loss 0.44 Classification-F1 0.5655735651084488 on epoch=737
05/18/2022 01:50:08 - INFO - __main__ - Saving model with best Classification-F1: 0.5586528326745719 -> 0.5655735651084488 on epoch=737, global_step=2950
05/18/2022 01:50:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.52 on epoch=739
05/18/2022 01:50:11 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.45 on epoch=742
05/18/2022 01:50:12 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.47 on epoch=744
05/18/2022 01:50:13 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.43 on epoch=747
05/18/2022 01:50:15 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.48 on epoch=749
05/18/2022 01:50:15 - INFO - __main__ - Global step 3000 Train loss 0.47 Classification-F1 0.5647865853658536 on epoch=749
05/18/2022 01:50:15 - INFO - __main__ - save last model!
05/18/2022 01:50:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 01:50:15 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 01:50:15 - INFO - __main__ - Printing 3 examples
05/18/2022 01:50:15 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 01:50:15 - INFO - __main__ - ['others']
05/18/2022 01:50:15 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 01:50:15 - INFO - __main__ - ['others']
05/18/2022 01:50:15 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 01:50:15 - INFO - __main__ - ['others']
05/18/2022 01:50:15 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:50:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:50:16 - INFO - __main__ - Printing 3 examples
05/18/2022 01:50:16 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/18/2022 01:50:16 - INFO - __main__ - ['others']
05/18/2022 01:50:16 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/18/2022 01:50:16 - INFO - __main__ - ['others']
05/18/2022 01:50:16 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/18/2022 01:50:16 - INFO - __main__ - ['others']
05/18/2022 01:50:16 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:50:16 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:50:16 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 01:50:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:50:16 - INFO - __main__ - Printing 3 examples
05/18/2022 01:50:16 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/18/2022 01:50:16 - INFO - __main__ - ['others']
05/18/2022 01:50:16 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/18/2022 01:50:16 - INFO - __main__ - ['others']
05/18/2022 01:50:16 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/18/2022 01:50:16 - INFO - __main__ - ['others']
05/18/2022 01:50:16 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:50:16 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:50:16 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 01:50:17 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:50:21 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 01:50:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 01:50:21 - INFO - __main__ - Starting training!
05/18/2022 01:50:23 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 01:51:06 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_87_0.5_8_predictions.txt
05/18/2022 01:51:06 - INFO - __main__ - Classification-F1 on test data: 0.1223
05/18/2022 01:51:06 - INFO - __main__ - prefix=emo_16_87, lr=0.5, bsz=8, dev_performance=0.5655735651084488, test_performance=0.12230081771829042
05/18/2022 01:51:06 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.4, bsz=8 ...
05/18/2022 01:51:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:51:07 - INFO - __main__ - Printing 3 examples
05/18/2022 01:51:07 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/18/2022 01:51:07 - INFO - __main__ - ['others']
05/18/2022 01:51:07 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/18/2022 01:51:07 - INFO - __main__ - ['others']
05/18/2022 01:51:07 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/18/2022 01:51:07 - INFO - __main__ - ['others']
05/18/2022 01:51:07 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:51:07 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:51:07 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 01:51:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:51:07 - INFO - __main__ - Printing 3 examples
05/18/2022 01:51:07 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/18/2022 01:51:07 - INFO - __main__ - ['others']
05/18/2022 01:51:07 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/18/2022 01:51:07 - INFO - __main__ - ['others']
05/18/2022 01:51:07 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/18/2022 01:51:07 - INFO - __main__ - ['others']
05/18/2022 01:51:07 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:51:07 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:51:07 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 01:51:13 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 01:51:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 01:51:13 - INFO - __main__ - Starting training!
05/18/2022 01:51:15 - INFO - __main__ - Step 10 Global step 10 Train loss 4.22 on epoch=2
05/18/2022 01:51:16 - INFO - __main__ - Step 20 Global step 20 Train loss 3.63 on epoch=4
05/18/2022 01:51:17 - INFO - __main__ - Step 30 Global step 30 Train loss 3.12 on epoch=7
05/18/2022 01:51:19 - INFO - __main__ - Step 40 Global step 40 Train loss 2.73 on epoch=9
05/18/2022 01:51:20 - INFO - __main__ - Step 50 Global step 50 Train loss 2.19 on epoch=12
05/18/2022 01:51:20 - INFO - __main__ - Global step 50 Train loss 3.17 Classification-F1 0.1 on epoch=12
05/18/2022 01:51:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/18/2022 01:51:22 - INFO - __main__ - Step 60 Global step 60 Train loss 1.86 on epoch=14
05/18/2022 01:51:23 - INFO - __main__ - Step 70 Global step 70 Train loss 1.68 on epoch=17
05/18/2022 01:51:24 - INFO - __main__ - Step 80 Global step 80 Train loss 1.33 on epoch=19
05/18/2022 01:51:26 - INFO - __main__ - Step 90 Global step 90 Train loss 1.31 on epoch=22
05/18/2022 01:51:27 - INFO - __main__ - Step 100 Global step 100 Train loss 1.03 on epoch=24
05/18/2022 01:51:27 - INFO - __main__ - Global step 100 Train loss 1.44 Classification-F1 0.1 on epoch=24
05/18/2022 01:51:29 - INFO - __main__ - Step 110 Global step 110 Train loss 1.16 on epoch=27
05/18/2022 01:51:30 - INFO - __main__ - Step 120 Global step 120 Train loss 1.11 on epoch=29
05/18/2022 01:51:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.99 on epoch=32
05/18/2022 01:51:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.98 on epoch=34
05/18/2022 01:51:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.98 on epoch=37
05/18/2022 01:51:34 - INFO - __main__ - Global step 150 Train loss 1.04 Classification-F1 0.1 on epoch=37
05/18/2022 01:51:35 - INFO - __main__ - Step 160 Global step 160 Train loss 1.01 on epoch=39
05/18/2022 01:51:37 - INFO - __main__ - Step 170 Global step 170 Train loss 1.04 on epoch=42
05/18/2022 01:51:38 - INFO - __main__ - Step 180 Global step 180 Train loss 1.02 on epoch=44
05/18/2022 01:51:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.93 on epoch=47
05/18/2022 01:51:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.98 on epoch=49
05/18/2022 01:51:41 - INFO - __main__ - Global step 200 Train loss 1.00 Classification-F1 0.1 on epoch=49
05/18/2022 01:51:42 - INFO - __main__ - Step 210 Global step 210 Train loss 1.05 on epoch=52
05/18/2022 01:51:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.96 on epoch=54
05/18/2022 01:51:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.94 on epoch=57
05/18/2022 01:51:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=59
05/18/2022 01:51:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.93 on epoch=62
05/18/2022 01:51:47 - INFO - __main__ - Global step 250 Train loss 0.95 Classification-F1 0.1476190476190476 on epoch=62
05/18/2022 01:51:47 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1476190476190476 on epoch=62, global_step=250
05/18/2022 01:51:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.98 on epoch=64
05/18/2022 01:51:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.95 on epoch=67
05/18/2022 01:51:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.94 on epoch=69
05/18/2022 01:51:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.83 on epoch=72
05/18/2022 01:51:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.87 on epoch=74
05/18/2022 01:51:54 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.1 on epoch=74
05/18/2022 01:51:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.93 on epoch=77
05/18/2022 01:51:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.97 on epoch=79
05/18/2022 01:51:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.94 on epoch=82
05/18/2022 01:51:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.93 on epoch=84
05/18/2022 01:52:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.93 on epoch=87
05/18/2022 01:52:01 - INFO - __main__ - Global step 350 Train loss 0.94 Classification-F1 0.13067758749069247 on epoch=87
05/18/2022 01:52:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.88 on epoch=89
05/18/2022 01:52:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.95 on epoch=92
05/18/2022 01:52:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.99 on epoch=94
05/18/2022 01:52:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.90 on epoch=97
05/18/2022 01:52:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.96 on epoch=99
05/18/2022 01:52:08 - INFO - __main__ - Global step 400 Train loss 0.93 Classification-F1 0.1 on epoch=99
05/18/2022 01:52:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.85 on epoch=102
05/18/2022 01:52:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.96 on epoch=104
05/18/2022 01:52:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.93 on epoch=107
05/18/2022 01:52:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.90 on epoch=109
05/18/2022 01:52:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.87 on epoch=112
05/18/2022 01:52:15 - INFO - __main__ - Global step 450 Train loss 0.90 Classification-F1 0.10126582278481013 on epoch=112
05/18/2022 01:52:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.91 on epoch=114
05/18/2022 01:52:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.93 on epoch=117
05/18/2022 01:52:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.87 on epoch=119
05/18/2022 01:52:19 - INFO - __main__ - Step 490 Global step 490 Train loss 1.04 on epoch=122
05/18/2022 01:52:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.92 on epoch=124
05/18/2022 01:52:21 - INFO - __main__ - Global step 500 Train loss 0.93 Classification-F1 0.1302118933697881 on epoch=124
05/18/2022 01:52:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.79 on epoch=127
05/18/2022 01:52:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.89 on epoch=129
05/18/2022 01:52:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.87 on epoch=132
05/18/2022 01:52:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.91 on epoch=134
05/18/2022 01:52:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.93 on epoch=137
05/18/2022 01:52:28 - INFO - __main__ - Global step 550 Train loss 0.88 Classification-F1 0.15620915032679739 on epoch=137
05/18/2022 01:52:28 - INFO - __main__ - Saving model with best Classification-F1: 0.1476190476190476 -> 0.15620915032679739 on epoch=137, global_step=550
05/18/2022 01:52:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.94 on epoch=139
05/18/2022 01:52:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.88 on epoch=142
05/18/2022 01:52:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.83 on epoch=144
05/18/2022 01:52:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.85 on epoch=147
05/18/2022 01:52:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.86 on epoch=149
05/18/2022 01:52:35 - INFO - __main__ - Global step 600 Train loss 0.87 Classification-F1 0.09493670886075949 on epoch=149
05/18/2022 01:52:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.84 on epoch=152
05/18/2022 01:52:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.86 on epoch=154
05/18/2022 01:52:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.84 on epoch=157
05/18/2022 01:52:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.86 on epoch=159
05/18/2022 01:52:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.89 on epoch=162
05/18/2022 01:52:41 - INFO - __main__ - Global step 650 Train loss 0.86 Classification-F1 0.1 on epoch=162
05/18/2022 01:52:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.80 on epoch=164
05/18/2022 01:52:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.86 on epoch=167
05/18/2022 01:52:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.85 on epoch=169
05/18/2022 01:52:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.89 on epoch=172
05/18/2022 01:52:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.81 on epoch=174
05/18/2022 01:52:48 - INFO - __main__ - Global step 700 Train loss 0.84 Classification-F1 0.1 on epoch=174
05/18/2022 01:52:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.80 on epoch=177
05/18/2022 01:52:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.78 on epoch=179
05/18/2022 01:52:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.82 on epoch=182
05/18/2022 01:52:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.82 on epoch=184
05/18/2022 01:52:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.88 on epoch=187
05/18/2022 01:52:55 - INFO - __main__ - Global step 750 Train loss 0.82 Classification-F1 0.13067758749069247 on epoch=187
05/18/2022 01:52:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.86 on epoch=189
05/18/2022 01:52:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.83 on epoch=192
05/18/2022 01:52:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.84 on epoch=194
05/18/2022 01:53:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.78 on epoch=197
05/18/2022 01:53:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.89 on epoch=199
05/18/2022 01:53:01 - INFO - __main__ - Global step 800 Train loss 0.84 Classification-F1 0.13067758749069247 on epoch=199
05/18/2022 01:53:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.86 on epoch=202
05/18/2022 01:53:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.88 on epoch=204
05/18/2022 01:53:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.81 on epoch=207
05/18/2022 01:53:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.83 on epoch=209
05/18/2022 01:53:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.83 on epoch=212
05/18/2022 01:53:08 - INFO - __main__ - Global step 850 Train loss 0.84 Classification-F1 0.1 on epoch=212
05/18/2022 01:53:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.84 on epoch=214
05/18/2022 01:53:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.78 on epoch=217
05/18/2022 01:53:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.86 on epoch=219
05/18/2022 01:53:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.88 on epoch=222
05/18/2022 01:53:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.78 on epoch=224
05/18/2022 01:53:15 - INFO - __main__ - Global step 900 Train loss 0.83 Classification-F1 0.1 on epoch=224
05/18/2022 01:53:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.82 on epoch=227
05/18/2022 01:53:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.82 on epoch=229
05/18/2022 01:53:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.90 on epoch=232
05/18/2022 01:53:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.88 on epoch=234
05/18/2022 01:53:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.84 on epoch=237
05/18/2022 01:53:22 - INFO - __main__ - Global step 950 Train loss 0.85 Classification-F1 0.1 on epoch=237
05/18/2022 01:53:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.82 on epoch=239
05/18/2022 01:53:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.81 on epoch=242
05/18/2022 01:53:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.82 on epoch=244
05/18/2022 01:53:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.90 on epoch=247
05/18/2022 01:53:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.88 on epoch=249
05/18/2022 01:53:28 - INFO - __main__ - Global step 1000 Train loss 0.85 Classification-F1 0.1 on epoch=249
05/18/2022 01:53:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.82 on epoch=252
05/18/2022 01:53:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.84 on epoch=254
05/18/2022 01:53:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.85 on epoch=257
05/18/2022 01:53:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.80 on epoch=259
05/18/2022 01:53:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.83 on epoch=262
05/18/2022 01:53:35 - INFO - __main__ - Global step 1050 Train loss 0.83 Classification-F1 0.1 on epoch=262
05/18/2022 01:53:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.84 on epoch=264
05/18/2022 01:53:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.77 on epoch=267
05/18/2022 01:53:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.80 on epoch=269
05/18/2022 01:53:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.87 on epoch=272
05/18/2022 01:53:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.75 on epoch=274
05/18/2022 01:53:42 - INFO - __main__ - Global step 1100 Train loss 0.80 Classification-F1 0.1500341763499658 on epoch=274
05/18/2022 01:53:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.83 on epoch=277
05/18/2022 01:53:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.81 on epoch=279
05/18/2022 01:53:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.77 on epoch=282
05/18/2022 01:53:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.76 on epoch=284
05/18/2022 01:53:48 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.81 on epoch=287
05/18/2022 01:53:48 - INFO - __main__ - Global step 1150 Train loss 0.80 Classification-F1 0.1 on epoch=287
05/18/2022 01:53:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.78 on epoch=289
05/18/2022 01:53:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.81 on epoch=292
05/18/2022 01:53:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.85 on epoch=294
05/18/2022 01:53:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.81 on epoch=297
05/18/2022 01:53:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.81 on epoch=299
05/18/2022 01:53:55 - INFO - __main__ - Global step 1200 Train loss 0.81 Classification-F1 0.10126582278481013 on epoch=299
05/18/2022 01:53:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.83 on epoch=302
05/18/2022 01:53:58 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.76 on epoch=304
05/18/2022 01:53:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.84 on epoch=307
05/18/2022 01:54:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.83 on epoch=309
05/18/2022 01:54:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.84 on epoch=312
05/18/2022 01:54:02 - INFO - __main__ - Global step 1250 Train loss 0.82 Classification-F1 0.1 on epoch=312
05/18/2022 01:54:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.85 on epoch=314
05/18/2022 01:54:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.76 on epoch=317
05/18/2022 01:54:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.80 on epoch=319
05/18/2022 01:54:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.85 on epoch=322
05/18/2022 01:54:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.79 on epoch=324
05/18/2022 01:54:08 - INFO - __main__ - Global step 1300 Train loss 0.81 Classification-F1 0.1 on epoch=324
05/18/2022 01:54:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.83 on epoch=327
05/18/2022 01:54:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.81 on epoch=329
05/18/2022 01:54:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.80 on epoch=332
05/18/2022 01:54:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.78 on epoch=334
05/18/2022 01:54:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.80 on epoch=337
05/18/2022 01:54:15 - INFO - __main__ - Global step 1350 Train loss 0.81 Classification-F1 0.1 on epoch=337
05/18/2022 01:54:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.75 on epoch=339
05/18/2022 01:54:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.75 on epoch=342
05/18/2022 01:54:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.80 on epoch=344
05/18/2022 01:54:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.81 on epoch=347
05/18/2022 01:54:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.83 on epoch=349
05/18/2022 01:54:22 - INFO - __main__ - Global step 1400 Train loss 0.79 Classification-F1 0.1 on epoch=349
05/18/2022 01:54:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.85 on epoch=352
05/18/2022 01:54:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.79 on epoch=354
05/18/2022 01:54:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.82 on epoch=357
05/18/2022 01:54:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.81 on epoch=359
05/18/2022 01:54:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.78 on epoch=362
05/18/2022 01:54:29 - INFO - __main__ - Global step 1450 Train loss 0.81 Classification-F1 0.1 on epoch=362
05/18/2022 01:54:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.81 on epoch=364
05/18/2022 01:54:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.82 on epoch=367
05/18/2022 01:54:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.78 on epoch=369
05/18/2022 01:54:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.85 on epoch=372
05/18/2022 01:54:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.78 on epoch=374
05/18/2022 01:54:35 - INFO - __main__ - Global step 1500 Train loss 0.81 Classification-F1 0.1 on epoch=374
05/18/2022 01:54:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.79 on epoch=377
05/18/2022 01:54:38 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.83 on epoch=379
05/18/2022 01:54:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.81 on epoch=382
05/18/2022 01:54:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.83 on epoch=384
05/18/2022 01:54:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.75 on epoch=387
05/18/2022 01:54:42 - INFO - __main__ - Global step 1550 Train loss 0.80 Classification-F1 0.14054336468129572 on epoch=387
05/18/2022 01:54:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.77 on epoch=389
05/18/2022 01:54:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.82 on epoch=392
05/18/2022 01:54:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.83 on epoch=394
05/18/2022 01:54:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.86 on epoch=397
05/18/2022 01:54:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.76 on epoch=399
05/18/2022 01:54:49 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.1 on epoch=399
05/18/2022 01:54:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.81 on epoch=402
05/18/2022 01:54:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.87 on epoch=404
05/18/2022 01:54:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.79 on epoch=407
05/18/2022 01:54:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.84 on epoch=409
05/18/2022 01:54:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.80 on epoch=412
05/18/2022 01:54:55 - INFO - __main__ - Global step 1650 Train loss 0.82 Classification-F1 0.1697802197802198 on epoch=412
05/18/2022 01:54:55 - INFO - __main__ - Saving model with best Classification-F1: 0.15620915032679739 -> 0.1697802197802198 on epoch=412, global_step=1650
05/18/2022 01:54:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.79 on epoch=414
05/18/2022 01:54:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.74 on epoch=417
05/18/2022 01:54:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.83 on epoch=419
05/18/2022 01:55:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.78 on epoch=422
05/18/2022 01:55:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.82 on epoch=424
05/18/2022 01:55:02 - INFO - __main__ - Global step 1700 Train loss 0.79 Classification-F1 0.1 on epoch=424
05/18/2022 01:55:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.79 on epoch=427
05/18/2022 01:55:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.78 on epoch=429
05/18/2022 01:55:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.77 on epoch=432
05/18/2022 01:55:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.78 on epoch=434
05/18/2022 01:55:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.75 on epoch=437
05/18/2022 01:55:09 - INFO - __main__ - Global step 1750 Train loss 0.77 Classification-F1 0.16666666666666669 on epoch=437
05/18/2022 01:55:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.84 on epoch=439
05/18/2022 01:55:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.82 on epoch=442
05/18/2022 01:55:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.76 on epoch=444
05/18/2022 01:55:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.87 on epoch=447
05/18/2022 01:55:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.77 on epoch=449
05/18/2022 01:55:16 - INFO - __main__ - Global step 1800 Train loss 0.81 Classification-F1 0.10666666666666667 on epoch=449
05/18/2022 01:55:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.79 on epoch=452
05/18/2022 01:55:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.80 on epoch=454
05/18/2022 01:55:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.79 on epoch=457
05/18/2022 01:55:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.80 on epoch=459
05/18/2022 01:55:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.83 on epoch=462
05/18/2022 01:55:22 - INFO - __main__ - Global step 1850 Train loss 0.80 Classification-F1 0.1 on epoch=462
05/18/2022 01:55:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.83 on epoch=464
05/18/2022 01:55:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.78 on epoch=467
05/18/2022 01:55:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.81 on epoch=469
05/18/2022 01:55:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.76 on epoch=472
05/18/2022 01:55:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.83 on epoch=474
05/18/2022 01:55:29 - INFO - __main__ - Global step 1900 Train loss 0.80 Classification-F1 0.1 on epoch=474
05/18/2022 01:55:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.76 on epoch=477
05/18/2022 01:55:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.81 on epoch=479
05/18/2022 01:55:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.85 on epoch=482
05/18/2022 01:55:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.75 on epoch=484
05/18/2022 01:55:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.80 on epoch=487
05/18/2022 01:55:36 - INFO - __main__ - Global step 1950 Train loss 0.80 Classification-F1 0.15490196078431373 on epoch=487
05/18/2022 01:55:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.73 on epoch=489
05/18/2022 01:55:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.82 on epoch=492
05/18/2022 01:55:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.73 on epoch=494
05/18/2022 01:55:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.75 on epoch=497
05/18/2022 01:55:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.79 on epoch=499
05/18/2022 01:55:42 - INFO - __main__ - Global step 2000 Train loss 0.77 Classification-F1 0.10256410256410256 on epoch=499
05/18/2022 01:55:44 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.83 on epoch=502
05/18/2022 01:55:45 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.71 on epoch=504
05/18/2022 01:55:46 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.74 on epoch=507
05/18/2022 01:55:47 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.81 on epoch=509
05/18/2022 01:55:49 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.79 on epoch=512
05/18/2022 01:55:49 - INFO - __main__ - Global step 2050 Train loss 0.78 Classification-F1 0.17480643240023822 on epoch=512
05/18/2022 01:55:49 - INFO - __main__ - Saving model with best Classification-F1: 0.1697802197802198 -> 0.17480643240023822 on epoch=512, global_step=2050
05/18/2022 01:55:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.79 on epoch=514
05/18/2022 01:55:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.73 on epoch=517
05/18/2022 01:55:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.79 on epoch=519
05/18/2022 01:55:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.78 on epoch=522
05/18/2022 01:55:55 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.80 on epoch=524
05/18/2022 01:55:56 - INFO - __main__ - Global step 2100 Train loss 0.78 Classification-F1 0.10126582278481013 on epoch=524
05/18/2022 01:55:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.73 on epoch=527
05/18/2022 01:55:58 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.75 on epoch=529
05/18/2022 01:56:00 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.80 on epoch=532
05/18/2022 01:56:01 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.80 on epoch=534
05/18/2022 01:56:02 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.75 on epoch=537
05/18/2022 01:56:03 - INFO - __main__ - Global step 2150 Train loss 0.77 Classification-F1 0.1 on epoch=537
05/18/2022 01:56:04 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.75 on epoch=539
05/18/2022 01:56:05 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.71 on epoch=542
05/18/2022 01:56:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.81 on epoch=544
05/18/2022 01:56:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.77 on epoch=547
05/18/2022 01:56:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.75 on epoch=549
05/18/2022 01:56:09 - INFO - __main__ - Global step 2200 Train loss 0.76 Classification-F1 0.13034188034188032 on epoch=549
05/18/2022 01:56:11 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.74 on epoch=552
05/18/2022 01:56:12 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.75 on epoch=554
05/18/2022 01:56:13 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.74 on epoch=557
05/18/2022 01:56:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.78 on epoch=559
05/18/2022 01:56:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.79 on epoch=562
05/18/2022 01:56:16 - INFO - __main__ - Global step 2250 Train loss 0.76 Classification-F1 0.21554834054834054 on epoch=562
05/18/2022 01:56:16 - INFO - __main__ - Saving model with best Classification-F1: 0.17480643240023822 -> 0.21554834054834054 on epoch=562, global_step=2250
05/18/2022 01:56:17 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.79 on epoch=564
05/18/2022 01:56:19 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.75 on epoch=567
05/18/2022 01:56:20 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.82 on epoch=569
05/18/2022 01:56:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.73 on epoch=572
05/18/2022 01:56:22 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.70 on epoch=574
05/18/2022 01:56:23 - INFO - __main__ - Global step 2300 Train loss 0.76 Classification-F1 0.18445535125631823 on epoch=574
05/18/2022 01:56:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.77 on epoch=577
05/18/2022 01:56:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.78 on epoch=579
05/18/2022 01:56:26 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.76 on epoch=582
05/18/2022 01:56:28 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.71 on epoch=584
05/18/2022 01:56:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.72 on epoch=587
05/18/2022 01:56:29 - INFO - __main__ - Global step 2350 Train loss 0.75 Classification-F1 0.31845238095238093 on epoch=587
05/18/2022 01:56:30 - INFO - __main__ - Saving model with best Classification-F1: 0.21554834054834054 -> 0.31845238095238093 on epoch=587, global_step=2350
05/18/2022 01:56:31 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.72 on epoch=589
05/18/2022 01:56:32 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.76 on epoch=592
05/18/2022 01:56:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.74 on epoch=594
05/18/2022 01:56:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.74 on epoch=597
05/18/2022 01:56:36 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.73 on epoch=599
05/18/2022 01:56:36 - INFO - __main__ - Global step 2400 Train loss 0.74 Classification-F1 0.20974025974025973 on epoch=599
05/18/2022 01:56:37 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.79 on epoch=602
05/18/2022 01:56:39 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.78 on epoch=604
05/18/2022 01:56:40 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.74 on epoch=607
05/18/2022 01:56:41 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.77 on epoch=609
05/18/2022 01:56:42 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.72 on epoch=612
05/18/2022 01:56:43 - INFO - __main__ - Global step 2450 Train loss 0.76 Classification-F1 0.29098583877995643 on epoch=612
05/18/2022 01:56:44 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.73 on epoch=614
05/18/2022 01:56:45 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.77 on epoch=617
05/18/2022 01:56:47 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.78 on epoch=619
05/18/2022 01:56:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.84 on epoch=622
05/18/2022 01:56:49 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.73 on epoch=624
05/18/2022 01:56:50 - INFO - __main__ - Global step 2500 Train loss 0.77 Classification-F1 0.2535156044813639 on epoch=624
05/18/2022 01:56:51 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.78 on epoch=627
05/18/2022 01:56:52 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.77 on epoch=629
05/18/2022 01:56:53 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.73 on epoch=632
05/18/2022 01:56:55 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.76 on epoch=634
05/18/2022 01:56:56 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.73 on epoch=637
05/18/2022 01:56:56 - INFO - __main__ - Global step 2550 Train loss 0.75 Classification-F1 0.40428276573787414 on epoch=637
05/18/2022 01:56:56 - INFO - __main__ - Saving model with best Classification-F1: 0.31845238095238093 -> 0.40428276573787414 on epoch=637, global_step=2550
05/18/2022 01:56:58 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.76 on epoch=639
05/18/2022 01:56:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.70 on epoch=642
05/18/2022 01:57:00 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.73 on epoch=644
05/18/2022 01:57:01 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.73 on epoch=647
05/18/2022 01:57:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.76 on epoch=649
05/18/2022 01:57:03 - INFO - __main__ - Global step 2600 Train loss 0.74 Classification-F1 0.38361230671130236 on epoch=649
05/18/2022 01:57:04 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.76 on epoch=652
05/18/2022 01:57:06 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.71 on epoch=654
05/18/2022 01:57:07 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.78 on epoch=657
05/18/2022 01:57:08 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.71 on epoch=659
05/18/2022 01:57:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.75 on epoch=662
05/18/2022 01:57:10 - INFO - __main__ - Global step 2650 Train loss 0.74 Classification-F1 0.46142686804451505 on epoch=662
05/18/2022 01:57:10 - INFO - __main__ - Saving model with best Classification-F1: 0.40428276573787414 -> 0.46142686804451505 on epoch=662, global_step=2650
05/18/2022 01:57:11 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.70 on epoch=664
05/18/2022 01:57:12 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.70 on epoch=667
05/18/2022 01:57:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.70 on epoch=669
05/18/2022 01:57:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.78 on epoch=672
05/18/2022 01:57:16 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.72 on epoch=674
05/18/2022 01:57:17 - INFO - __main__ - Global step 2700 Train loss 0.72 Classification-F1 0.4291600484587428 on epoch=674
05/18/2022 01:57:18 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.68 on epoch=677
05/18/2022 01:57:19 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.70 on epoch=679
05/18/2022 01:57:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.70 on epoch=682
05/18/2022 01:57:22 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.71 on epoch=684
05/18/2022 01:57:23 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.74 on epoch=687
05/18/2022 01:57:23 - INFO - __main__ - Global step 2750 Train loss 0.70 Classification-F1 0.42410572337042923 on epoch=687
05/18/2022 01:57:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.75 on epoch=689
05/18/2022 01:57:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.72 on epoch=692
05/18/2022 01:57:27 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.76 on epoch=694
05/18/2022 01:57:28 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.73 on epoch=697
05/18/2022 01:57:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.82 on epoch=699
05/18/2022 01:57:30 - INFO - __main__ - Global step 2800 Train loss 0.76 Classification-F1 0.40140096618357496 on epoch=699
05/18/2022 01:57:31 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.74 on epoch=702
05/18/2022 01:57:33 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.65 on epoch=704
05/18/2022 01:57:34 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.71 on epoch=707
05/18/2022 01:57:35 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.71 on epoch=709
05/18/2022 01:57:36 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.74 on epoch=712
05/18/2022 01:57:37 - INFO - __main__ - Global step 2850 Train loss 0.71 Classification-F1 0.4199393114834291 on epoch=712
05/18/2022 01:57:38 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.69 on epoch=714
05/18/2022 01:57:39 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.75 on epoch=717
05/18/2022 01:57:41 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.71 on epoch=719
05/18/2022 01:57:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.72 on epoch=722
05/18/2022 01:57:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.74 on epoch=724
05/18/2022 01:57:44 - INFO - __main__ - Global step 2900 Train loss 0.72 Classification-F1 0.43405506549051054 on epoch=724
05/18/2022 01:57:45 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.69 on epoch=727
05/18/2022 01:57:46 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.73 on epoch=729
05/18/2022 01:57:47 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.67 on epoch=732
05/18/2022 01:57:49 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.70 on epoch=734
05/18/2022 01:57:50 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.68 on epoch=737
05/18/2022 01:57:50 - INFO - __main__ - Global step 2950 Train loss 0.69 Classification-F1 0.40733442802408315 on epoch=737
05/18/2022 01:57:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.71 on epoch=739
05/18/2022 01:57:53 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.72 on epoch=742
05/18/2022 01:57:54 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.71 on epoch=744
05/18/2022 01:57:55 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.68 on epoch=747
05/18/2022 01:57:56 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.66 on epoch=749
05/18/2022 01:57:57 - INFO - __main__ - Global step 3000 Train loss 0.70 Classification-F1 0.44435286935286933 on epoch=749
05/18/2022 01:57:57 - INFO - __main__ - save last model!
05/18/2022 01:57:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 01:57:57 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 01:57:57 - INFO - __main__ - Printing 3 examples
05/18/2022 01:57:57 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 01:57:57 - INFO - __main__ - ['others']
05/18/2022 01:57:57 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 01:57:57 - INFO - __main__ - ['others']
05/18/2022 01:57:57 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 01:57:57 - INFO - __main__ - ['others']
05/18/2022 01:57:57 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:57:58 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:57:58 - INFO - __main__ - Printing 3 examples
05/18/2022 01:57:58 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/18/2022 01:57:58 - INFO - __main__ - ['others']
05/18/2022 01:57:58 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/18/2022 01:57:58 - INFO - __main__ - ['others']
05/18/2022 01:57:58 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/18/2022 01:57:58 - INFO - __main__ - ['others']
05/18/2022 01:57:58 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:57:58 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:57:58 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 01:57:58 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:57:58 - INFO - __main__ - Printing 3 examples
05/18/2022 01:57:58 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/18/2022 01:57:58 - INFO - __main__ - ['others']
05/18/2022 01:57:58 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/18/2022 01:57:58 - INFO - __main__ - ['others']
05/18/2022 01:57:58 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/18/2022 01:57:58 - INFO - __main__ - ['others']
05/18/2022 01:57:58 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:57:58 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:57:58 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 01:57:59 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:58:04 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 01:58:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 01:58:04 - INFO - __main__ - Starting training!
05/18/2022 01:58:04 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 01:58:48 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_87_0.4_8_predictions.txt
05/18/2022 01:58:48 - INFO - __main__ - Classification-F1 on test data: 0.2629
05/18/2022 01:58:48 - INFO - __main__ - prefix=emo_16_87, lr=0.4, bsz=8, dev_performance=0.46142686804451505, test_performance=0.26286077165415145
05/18/2022 01:58:48 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.3, bsz=8 ...
05/18/2022 01:58:49 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:58:49 - INFO - __main__ - Printing 3 examples
05/18/2022 01:58:49 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/18/2022 01:58:49 - INFO - __main__ - ['others']
05/18/2022 01:58:49 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/18/2022 01:58:49 - INFO - __main__ - ['others']
05/18/2022 01:58:49 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/18/2022 01:58:49 - INFO - __main__ - ['others']
05/18/2022 01:58:49 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:58:49 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:58:49 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 01:58:49 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 01:58:49 - INFO - __main__ - Printing 3 examples
05/18/2022 01:58:49 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/18/2022 01:58:49 - INFO - __main__ - ['others']
05/18/2022 01:58:49 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/18/2022 01:58:49 - INFO - __main__ - ['others']
05/18/2022 01:58:49 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/18/2022 01:58:49 - INFO - __main__ - ['others']
05/18/2022 01:58:49 - INFO - __main__ - Tokenizing Input ...
05/18/2022 01:58:49 - INFO - __main__ - Tokenizing Output ...
05/18/2022 01:58:49 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 01:58:55 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 01:58:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 01:58:56 - INFO - __main__ - Starting training!
05/18/2022 01:58:57 - INFO - __main__ - Step 10 Global step 10 Train loss 4.24 on epoch=2
05/18/2022 01:58:58 - INFO - __main__ - Step 20 Global step 20 Train loss 3.92 on epoch=4
05/18/2022 01:59:00 - INFO - __main__ - Step 30 Global step 30 Train loss 3.44 on epoch=7
05/18/2022 01:59:01 - INFO - __main__ - Step 40 Global step 40 Train loss 3.12 on epoch=9
05/18/2022 01:59:02 - INFO - __main__ - Step 50 Global step 50 Train loss 2.59 on epoch=12
05/18/2022 01:59:03 - INFO - __main__ - Global step 50 Train loss 3.46 Classification-F1 0.1 on epoch=12
05/18/2022 01:59:03 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/18/2022 01:59:04 - INFO - __main__ - Step 60 Global step 60 Train loss 2.28 on epoch=14
05/18/2022 01:59:05 - INFO - __main__ - Step 70 Global step 70 Train loss 2.08 on epoch=17
05/18/2022 01:59:07 - INFO - __main__ - Step 80 Global step 80 Train loss 1.71 on epoch=19
05/18/2022 01:59:08 - INFO - __main__ - Step 90 Global step 90 Train loss 1.60 on epoch=22
05/18/2022 01:59:09 - INFO - __main__ - Step 100 Global step 100 Train loss 1.39 on epoch=24
05/18/2022 01:59:10 - INFO - __main__ - Global step 100 Train loss 1.81 Classification-F1 0.13034188034188032 on epoch=24
05/18/2022 01:59:10 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.13034188034188032 on epoch=24, global_step=100
05/18/2022 01:59:11 - INFO - __main__ - Step 110 Global step 110 Train loss 1.35 on epoch=27
05/18/2022 01:59:12 - INFO - __main__ - Step 120 Global step 120 Train loss 1.28 on epoch=29
05/18/2022 01:59:13 - INFO - __main__ - Step 130 Global step 130 Train loss 1.25 on epoch=32
05/18/2022 01:59:15 - INFO - __main__ - Step 140 Global step 140 Train loss 1.09 on epoch=34
05/18/2022 01:59:16 - INFO - __main__ - Step 150 Global step 150 Train loss 1.06 on epoch=37
05/18/2022 01:59:16 - INFO - __main__ - Global step 150 Train loss 1.21 Classification-F1 0.2813816127375449 on epoch=37
05/18/2022 01:59:16 - INFO - __main__ - Saving model with best Classification-F1: 0.13034188034188032 -> 0.2813816127375449 on epoch=37, global_step=150
05/18/2022 01:59:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.97 on epoch=39
05/18/2022 01:59:19 - INFO - __main__ - Step 170 Global step 170 Train loss 1.04 on epoch=42
05/18/2022 01:59:20 - INFO - __main__ - Step 180 Global step 180 Train loss 1.03 on epoch=44
05/18/2022 01:59:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.92 on epoch=47
05/18/2022 01:59:22 - INFO - __main__ - Step 200 Global step 200 Train loss 1.02 on epoch=49
05/18/2022 01:59:23 - INFO - __main__ - Global step 200 Train loss 0.99 Classification-F1 0.1 on epoch=49
05/18/2022 01:59:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.93 on epoch=52
05/18/2022 01:59:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.98 on epoch=54
05/18/2022 01:59:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.90 on epoch=57
05/18/2022 01:59:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.93 on epoch=59
05/18/2022 01:59:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.91 on epoch=62
05/18/2022 01:59:30 - INFO - __main__ - Global step 250 Train loss 0.93 Classification-F1 0.1 on epoch=62
05/18/2022 01:59:31 - INFO - __main__ - Step 260 Global step 260 Train loss 1.02 on epoch=64
05/18/2022 01:59:32 - INFO - __main__ - Step 270 Global step 270 Train loss 1.02 on epoch=67
05/18/2022 01:59:33 - INFO - __main__ - Step 280 Global step 280 Train loss 1.07 on epoch=69
05/18/2022 01:59:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.98 on epoch=72
05/18/2022 01:59:36 - INFO - __main__ - Step 300 Global step 300 Train loss 1.07 on epoch=74
05/18/2022 01:59:36 - INFO - __main__ - Global step 300 Train loss 1.03 Classification-F1 0.1 on epoch=74
05/18/2022 01:59:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.99 on epoch=77
05/18/2022 01:59:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.86 on epoch=79
05/18/2022 01:59:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.89 on epoch=82
05/18/2022 01:59:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.92 on epoch=84
05/18/2022 01:59:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.92 on epoch=87
05/18/2022 01:59:43 - INFO - __main__ - Global step 350 Train loss 0.92 Classification-F1 0.13067758749069247 on epoch=87
05/18/2022 01:59:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.86 on epoch=89
05/18/2022 01:59:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.89 on epoch=92
05/18/2022 01:59:47 - INFO - __main__ - Step 380 Global step 380 Train loss 1.05 on epoch=94
05/18/2022 01:59:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.92 on epoch=97
05/18/2022 01:59:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.91 on epoch=99
05/18/2022 01:59:50 - INFO - __main__ - Global step 400 Train loss 0.93 Classification-F1 0.19973009446693657 on epoch=99
05/18/2022 01:59:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.87 on epoch=102
05/18/2022 01:59:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.98 on epoch=104
05/18/2022 01:59:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.90 on epoch=107
05/18/2022 01:59:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.94 on epoch=109
05/18/2022 01:59:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.82 on epoch=112
05/18/2022 01:59:56 - INFO - __main__ - Global step 450 Train loss 0.90 Classification-F1 0.1 on epoch=112
05/18/2022 01:59:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.87 on epoch=114
05/18/2022 01:59:59 - INFO - __main__ - Step 470 Global step 470 Train loss 1.01 on epoch=117
05/18/2022 02:00:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.97 on epoch=119
05/18/2022 02:00:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.97 on epoch=122
05/18/2022 02:00:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.93 on epoch=124
05/18/2022 02:00:03 - INFO - __main__ - Global step 500 Train loss 0.95 Classification-F1 0.1 on epoch=124
05/18/2022 02:00:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.85 on epoch=127
05/18/2022 02:00:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.84 on epoch=129
05/18/2022 02:00:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.86 on epoch=132
05/18/2022 02:00:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.92 on epoch=134
05/18/2022 02:00:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.86 on epoch=137
05/18/2022 02:00:10 - INFO - __main__ - Global step 550 Train loss 0.87 Classification-F1 0.1 on epoch=137
05/18/2022 02:00:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.97 on epoch=139
05/18/2022 02:00:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.91 on epoch=142
05/18/2022 02:00:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.94 on epoch=144
05/18/2022 02:00:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.83 on epoch=147
05/18/2022 02:00:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.89 on epoch=149
05/18/2022 02:00:17 - INFO - __main__ - Global step 600 Train loss 0.91 Classification-F1 0.1 on epoch=149
05/18/2022 02:00:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.93 on epoch=152
05/18/2022 02:00:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.86 on epoch=154
05/18/2022 02:00:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.91 on epoch=157
05/18/2022 02:00:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.84 on epoch=159
05/18/2022 02:00:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.83 on epoch=162
05/18/2022 02:00:23 - INFO - __main__ - Global step 650 Train loss 0.87 Classification-F1 0.19542483660130716 on epoch=162
05/18/2022 02:00:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.85 on epoch=164
05/18/2022 02:00:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.88 on epoch=167
05/18/2022 02:00:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.85 on epoch=169
05/18/2022 02:00:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.86 on epoch=172
05/18/2022 02:00:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.87 on epoch=174
05/18/2022 02:00:30 - INFO - __main__ - Global step 700 Train loss 0.86 Classification-F1 0.1 on epoch=174
05/18/2022 02:00:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.88 on epoch=177
05/18/2022 02:00:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.79 on epoch=179
05/18/2022 02:00:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.85 on epoch=182
05/18/2022 02:00:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.84 on epoch=184
05/18/2022 02:00:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.92 on epoch=187
05/18/2022 02:00:37 - INFO - __main__ - Global step 750 Train loss 0.85 Classification-F1 0.10389610389610389 on epoch=187
05/18/2022 02:00:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.86 on epoch=189
05/18/2022 02:00:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.84 on epoch=192
05/18/2022 02:00:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.85 on epoch=194
05/18/2022 02:00:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.85 on epoch=197
05/18/2022 02:00:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.85 on epoch=199
05/18/2022 02:00:44 - INFO - __main__ - Global step 800 Train loss 0.85 Classification-F1 0.1 on epoch=199
05/18/2022 02:00:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.88 on epoch=202
05/18/2022 02:00:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.78 on epoch=204
05/18/2022 02:00:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=207
05/18/2022 02:00:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.86 on epoch=209
05/18/2022 02:00:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.89 on epoch=212
05/18/2022 02:00:50 - INFO - __main__ - Global step 850 Train loss 0.85 Classification-F1 0.1 on epoch=212
05/18/2022 02:00:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.90 on epoch=214
05/18/2022 02:00:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.92 on epoch=217
05/18/2022 02:00:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.87 on epoch=219
05/18/2022 02:00:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.88 on epoch=222
05/18/2022 02:00:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.85 on epoch=224
05/18/2022 02:00:57 - INFO - __main__ - Global step 900 Train loss 0.88 Classification-F1 0.1 on epoch=224
05/18/2022 02:00:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.79 on epoch=227
05/18/2022 02:01:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.83 on epoch=229
05/18/2022 02:01:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.83 on epoch=232
05/18/2022 02:01:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.84 on epoch=234
05/18/2022 02:01:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.81 on epoch=237
05/18/2022 02:01:04 - INFO - __main__ - Global step 950 Train loss 0.82 Classification-F1 0.1 on epoch=237
05/18/2022 02:01:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.81 on epoch=239
05/18/2022 02:01:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.80 on epoch=242
05/18/2022 02:01:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.83 on epoch=244
05/18/2022 02:01:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.84 on epoch=247
05/18/2022 02:01:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.79 on epoch=249
05/18/2022 02:01:11 - INFO - __main__ - Global step 1000 Train loss 0.82 Classification-F1 0.1 on epoch=249
05/18/2022 02:01:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.84 on epoch=252
05/18/2022 02:01:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.86 on epoch=254
05/18/2022 02:01:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.84 on epoch=257
05/18/2022 02:01:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.83 on epoch=259
05/18/2022 02:01:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.82 on epoch=262
05/18/2022 02:01:18 - INFO - __main__ - Global step 1050 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=262
05/18/2022 02:01:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.81 on epoch=264
05/18/2022 02:01:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.83 on epoch=267
05/18/2022 02:01:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.83 on epoch=269
05/18/2022 02:01:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.90 on epoch=272
05/18/2022 02:01:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.89 on epoch=274
05/18/2022 02:01:24 - INFO - __main__ - Global step 1100 Train loss 0.85 Classification-F1 0.1 on epoch=274
05/18/2022 02:01:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.77 on epoch=277
05/18/2022 02:01:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.83 on epoch=279
05/18/2022 02:01:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.81 on epoch=282
05/18/2022 02:01:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.82 on epoch=284
05/18/2022 02:01:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.83 on epoch=287
05/18/2022 02:01:31 - INFO - __main__ - Global step 1150 Train loss 0.81 Classification-F1 0.1 on epoch=287
05/18/2022 02:01:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.83 on epoch=289
05/18/2022 02:01:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.79 on epoch=292
05/18/2022 02:01:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.88 on epoch=294
05/18/2022 02:01:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.83 on epoch=297
05/18/2022 02:01:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.79 on epoch=299
05/18/2022 02:01:37 - INFO - __main__ - Global step 1200 Train loss 0.82 Classification-F1 0.1 on epoch=299
05/18/2022 02:01:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.78 on epoch=302
05/18/2022 02:01:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.80 on epoch=304
05/18/2022 02:01:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.89 on epoch=307
05/18/2022 02:01:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.80 on epoch=309
05/18/2022 02:01:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.80 on epoch=312
05/18/2022 02:01:44 - INFO - __main__ - Global step 1250 Train loss 0.82 Classification-F1 0.11111111111111112 on epoch=312
05/18/2022 02:01:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.81 on epoch=314
05/18/2022 02:01:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.79 on epoch=317
05/18/2022 02:01:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.78 on epoch=319
05/18/2022 02:01:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.86 on epoch=322
05/18/2022 02:01:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.73 on epoch=324
05/18/2022 02:01:50 - INFO - __main__ - Global step 1300 Train loss 0.79 Classification-F1 0.1996370235934664 on epoch=324
05/18/2022 02:01:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.81 on epoch=327
05/18/2022 02:01:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.82 on epoch=329
05/18/2022 02:01:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.78 on epoch=332
05/18/2022 02:01:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.82 on epoch=334
05/18/2022 02:01:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.79 on epoch=337
05/18/2022 02:01:57 - INFO - __main__ - Global step 1350 Train loss 0.80 Classification-F1 0.1 on epoch=337
05/18/2022 02:01:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.85 on epoch=339
05/18/2022 02:01:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.85 on epoch=342
05/18/2022 02:02:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.83 on epoch=344
05/18/2022 02:02:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.83 on epoch=347
05/18/2022 02:02:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.81 on epoch=349
05/18/2022 02:02:04 - INFO - __main__ - Global step 1400 Train loss 0.84 Classification-F1 0.1 on epoch=349
05/18/2022 02:02:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.82 on epoch=352
05/18/2022 02:02:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.81 on epoch=354
05/18/2022 02:02:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.77 on epoch=357
05/18/2022 02:02:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.79 on epoch=359
05/18/2022 02:02:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.84 on epoch=362
05/18/2022 02:02:10 - INFO - __main__ - Global step 1450 Train loss 0.81 Classification-F1 0.10126582278481013 on epoch=362
05/18/2022 02:02:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.87 on epoch=364
05/18/2022 02:02:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.79 on epoch=367
05/18/2022 02:02:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.80 on epoch=369
05/18/2022 02:02:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.87 on epoch=372
05/18/2022 02:02:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.85 on epoch=374
05/18/2022 02:02:16 - INFO - __main__ - Global step 1500 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=374
05/18/2022 02:02:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.79 on epoch=377
05/18/2022 02:02:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.85 on epoch=379
05/18/2022 02:02:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.79 on epoch=382
05/18/2022 02:02:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.80 on epoch=384
05/18/2022 02:02:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.81 on epoch=387
05/18/2022 02:02:23 - INFO - __main__ - Global step 1550 Train loss 0.81 Classification-F1 0.20920303605313093 on epoch=387
05/18/2022 02:02:24 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.80 on epoch=389
05/18/2022 02:02:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.75 on epoch=392
05/18/2022 02:02:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.82 on epoch=394
05/18/2022 02:02:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.82 on epoch=397
05/18/2022 02:02:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.82 on epoch=399
05/18/2022 02:02:30 - INFO - __main__ - Global step 1600 Train loss 0.80 Classification-F1 0.17112712300566135 on epoch=399
05/18/2022 02:02:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.76 on epoch=402
05/18/2022 02:02:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.77 on epoch=404
05/18/2022 02:02:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.76 on epoch=407
05/18/2022 02:02:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.77 on epoch=409
05/18/2022 02:02:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.82 on epoch=412
05/18/2022 02:02:36 - INFO - __main__ - Global step 1650 Train loss 0.78 Classification-F1 0.131328171530673 on epoch=412
05/18/2022 02:02:37 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.80 on epoch=414
05/18/2022 02:02:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.81 on epoch=417
05/18/2022 02:02:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.79 on epoch=419
05/18/2022 02:02:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.85 on epoch=422
05/18/2022 02:02:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.77 on epoch=424
05/18/2022 02:02:43 - INFO - __main__ - Global step 1700 Train loss 0.81 Classification-F1 0.10126582278481013 on epoch=424
05/18/2022 02:02:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.81 on epoch=427
05/18/2022 02:02:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.83 on epoch=429
05/18/2022 02:02:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.71 on epoch=432
05/18/2022 02:02:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.82 on epoch=434
05/18/2022 02:02:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.80 on epoch=437
05/18/2022 02:02:49 - INFO - __main__ - Global step 1750 Train loss 0.80 Classification-F1 0.10389610389610389 on epoch=437
05/18/2022 02:02:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.84 on epoch=439
05/18/2022 02:02:52 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.77 on epoch=442
05/18/2022 02:02:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.81 on epoch=444
05/18/2022 02:02:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.79 on epoch=447
05/18/2022 02:02:55 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.78 on epoch=449
05/18/2022 02:02:56 - INFO - __main__ - Global step 1800 Train loss 0.80 Classification-F1 0.1 on epoch=449
05/18/2022 02:02:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.75 on epoch=452
05/18/2022 02:02:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.81 on epoch=454
05/18/2022 02:03:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.82 on epoch=457
05/18/2022 02:03:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.87 on epoch=459
05/18/2022 02:03:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.87 on epoch=462
05/18/2022 02:03:02 - INFO - __main__ - Global step 1850 Train loss 0.82 Classification-F1 0.1527777777777778 on epoch=462
05/18/2022 02:03:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.77 on epoch=464
05/18/2022 02:03:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.78 on epoch=467
05/18/2022 02:03:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.77 on epoch=469
05/18/2022 02:03:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.82 on epoch=472
05/18/2022 02:03:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.78 on epoch=474
05/18/2022 02:03:09 - INFO - __main__ - Global step 1900 Train loss 0.78 Classification-F1 0.1 on epoch=474
05/18/2022 02:03:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.82 on epoch=477
05/18/2022 02:03:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.84 on epoch=479
05/18/2022 02:03:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.76 on epoch=482
05/18/2022 02:03:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.87 on epoch=484
05/18/2022 02:03:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.84 on epoch=487
05/18/2022 02:03:16 - INFO - __main__ - Global step 1950 Train loss 0.83 Classification-F1 0.17480643240023822 on epoch=487
05/18/2022 02:03:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.78 on epoch=489
05/18/2022 02:03:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.80 on epoch=492
05/18/2022 02:03:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.84 on epoch=494
05/18/2022 02:03:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.75 on epoch=497
05/18/2022 02:03:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.83 on epoch=499
05/18/2022 02:03:22 - INFO - __main__ - Global step 2000 Train loss 0.80 Classification-F1 0.1996370235934664 on epoch=499
05/18/2022 02:03:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.82 on epoch=502
05/18/2022 02:03:25 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.86 on epoch=504
05/18/2022 02:03:26 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.77 on epoch=507
05/18/2022 02:03:27 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.81 on epoch=509
05/18/2022 02:03:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.81 on epoch=512
05/18/2022 02:03:29 - INFO - __main__ - Global step 2050 Train loss 0.81 Classification-F1 0.10126582278481013 on epoch=512
05/18/2022 02:03:30 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.76 on epoch=514
05/18/2022 02:03:31 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.80 on epoch=517
05/18/2022 02:03:32 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.76 on epoch=519
05/18/2022 02:03:34 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.73 on epoch=522
05/18/2022 02:03:35 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.81 on epoch=524
05/18/2022 02:03:35 - INFO - __main__ - Global step 2100 Train loss 0.77 Classification-F1 0.1 on epoch=524
05/18/2022 02:03:36 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.79 on epoch=527
05/18/2022 02:03:38 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.84 on epoch=529
05/18/2022 02:03:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.77 on epoch=532
05/18/2022 02:03:40 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.80 on epoch=534
05/18/2022 02:03:41 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.81 on epoch=537
05/18/2022 02:03:42 - INFO - __main__ - Global step 2150 Train loss 0.80 Classification-F1 0.10256410256410256 on epoch=537
05/18/2022 02:03:43 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.81 on epoch=539
05/18/2022 02:03:44 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.77 on epoch=542
05/18/2022 02:03:45 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.81 on epoch=544
05/18/2022 02:03:47 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.77 on epoch=547
05/18/2022 02:03:48 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.85 on epoch=549
05/18/2022 02:03:48 - INFO - __main__ - Global step 2200 Train loss 0.80 Classification-F1 0.10126582278481013 on epoch=549
05/18/2022 02:03:50 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.90 on epoch=552
05/18/2022 02:03:51 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.79 on epoch=554
05/18/2022 02:03:52 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.80 on epoch=557
05/18/2022 02:03:53 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.76 on epoch=559
05/18/2022 02:03:54 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.81 on epoch=562
05/18/2022 02:03:55 - INFO - __main__ - Global step 2250 Train loss 0.81 Classification-F1 0.10126582278481013 on epoch=562
05/18/2022 02:03:56 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.82 on epoch=564
05/18/2022 02:03:57 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.77 on epoch=567
05/18/2022 02:03:59 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.78 on epoch=569
05/18/2022 02:04:00 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.80 on epoch=572
05/18/2022 02:04:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.81 on epoch=574
05/18/2022 02:04:02 - INFO - __main__ - Global step 2300 Train loss 0.80 Classification-F1 0.1302118933697881 on epoch=574
05/18/2022 02:04:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.78 on epoch=577
05/18/2022 02:04:04 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.79 on epoch=579
05/18/2022 02:04:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.81 on epoch=582
05/18/2022 02:04:06 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.76 on epoch=584
05/18/2022 02:04:08 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.75 on epoch=587
05/18/2022 02:04:08 - INFO - __main__ - Global step 2350 Train loss 0.78 Classification-F1 0.10126582278481013 on epoch=587
05/18/2022 02:04:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.80 on epoch=589
05/18/2022 02:04:11 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.80 on epoch=592
05/18/2022 02:04:12 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.76 on epoch=594
05/18/2022 02:04:13 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.81 on epoch=597
05/18/2022 02:04:14 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.81 on epoch=599
05/18/2022 02:04:15 - INFO - __main__ - Global step 2400 Train loss 0.80 Classification-F1 0.10126582278481013 on epoch=599
05/18/2022 02:04:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.75 on epoch=602
05/18/2022 02:04:17 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.74 on epoch=604
05/18/2022 02:04:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.81 on epoch=607
05/18/2022 02:04:19 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.73 on epoch=609
05/18/2022 02:04:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.79 on epoch=612
05/18/2022 02:04:21 - INFO - __main__ - Global step 2450 Train loss 0.76 Classification-F1 0.1242521859180856 on epoch=612
05/18/2022 02:04:22 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.81 on epoch=614
05/18/2022 02:04:24 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.82 on epoch=617
05/18/2022 02:04:25 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.75 on epoch=619
05/18/2022 02:04:26 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.79 on epoch=622
05/18/2022 02:04:27 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.73 on epoch=624
05/18/2022 02:04:28 - INFO - __main__ - Global step 2500 Train loss 0.78 Classification-F1 0.10389610389610389 on epoch=624
05/18/2022 02:04:29 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.73 on epoch=627
05/18/2022 02:04:30 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.77 on epoch=629
05/18/2022 02:04:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.78 on epoch=632
05/18/2022 02:04:33 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.75 on epoch=634
05/18/2022 02:04:34 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.75 on epoch=637
05/18/2022 02:04:34 - INFO - __main__ - Global step 2550 Train loss 0.76 Classification-F1 0.10126582278481013 on epoch=637
05/18/2022 02:04:36 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.81 on epoch=639
05/18/2022 02:04:37 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.82 on epoch=642
05/18/2022 02:04:38 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.86 on epoch=644
05/18/2022 02:04:39 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.81 on epoch=647
05/18/2022 02:04:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.76 on epoch=649
05/18/2022 02:04:41 - INFO - __main__ - Global step 2600 Train loss 0.81 Classification-F1 0.1 on epoch=649
05/18/2022 02:04:42 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.81 on epoch=652
05/18/2022 02:04:43 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.78 on epoch=654
05/18/2022 02:04:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.78 on epoch=657
05/18/2022 02:04:46 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.76 on epoch=659
05/18/2022 02:04:47 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.89 on epoch=662
05/18/2022 02:04:47 - INFO - __main__ - Global step 2650 Train loss 0.80 Classification-F1 0.10389610389610389 on epoch=662
05/18/2022 02:04:49 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.82 on epoch=664
05/18/2022 02:04:50 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.76 on epoch=667
05/18/2022 02:04:51 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.83 on epoch=669
05/18/2022 02:04:52 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.77 on epoch=672
05/18/2022 02:04:54 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.77 on epoch=674
05/18/2022 02:04:54 - INFO - __main__ - Global step 2700 Train loss 0.79 Classification-F1 0.1 on epoch=674
05/18/2022 02:04:55 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.79 on epoch=677
05/18/2022 02:04:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.83 on epoch=679
05/18/2022 02:04:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.72 on epoch=682
05/18/2022 02:04:59 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.78 on epoch=684
05/18/2022 02:05:00 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.76 on epoch=687
05/18/2022 02:05:01 - INFO - __main__ - Global step 2750 Train loss 0.77 Classification-F1 0.13558823529411765 on epoch=687
05/18/2022 02:05:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.76 on epoch=689
05/18/2022 02:05:03 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.80 on epoch=692
05/18/2022 02:05:04 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.81 on epoch=694
05/18/2022 02:05:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.74 on epoch=697
05/18/2022 02:05:07 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.80 on epoch=699
05/18/2022 02:05:07 - INFO - __main__ - Global step 2800 Train loss 0.78 Classification-F1 0.10126582278481013 on epoch=699
05/18/2022 02:05:08 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.76 on epoch=702
05/18/2022 02:05:10 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.76 on epoch=704
05/18/2022 02:05:11 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.73 on epoch=707
05/18/2022 02:05:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.82 on epoch=709
05/18/2022 02:05:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.80 on epoch=712
05/18/2022 02:05:14 - INFO - __main__ - Global step 2850 Train loss 0.77 Classification-F1 0.1511156186612576 on epoch=712
05/18/2022 02:05:15 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.72 on epoch=714
05/18/2022 02:05:16 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.73 on epoch=717
05/18/2022 02:05:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.78 on epoch=719
05/18/2022 02:05:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.75 on epoch=722
05/18/2022 02:05:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.78 on epoch=724
05/18/2022 02:05:20 - INFO - __main__ - Global step 2900 Train loss 0.75 Classification-F1 0.10666666666666667 on epoch=724
05/18/2022 02:05:22 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.81 on epoch=727
05/18/2022 02:05:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.78 on epoch=729
05/18/2022 02:05:24 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.78 on epoch=732
05/18/2022 02:05:25 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.78 on epoch=734
05/18/2022 02:05:27 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.77 on epoch=737
05/18/2022 02:05:27 - INFO - __main__ - Global step 2950 Train loss 0.78 Classification-F1 0.10256410256410256 on epoch=737
05/18/2022 02:05:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.79 on epoch=739
05/18/2022 02:05:29 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.78 on epoch=742
05/18/2022 02:05:31 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.77 on epoch=744
05/18/2022 02:05:32 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.75 on epoch=747
05/18/2022 02:05:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.81 on epoch=749
05/18/2022 02:05:34 - INFO - __main__ - Global step 3000 Train loss 0.78 Classification-F1 0.0945945945945946 on epoch=749
05/18/2022 02:05:34 - INFO - __main__ - save last model!
05/18/2022 02:05:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 02:05:34 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 02:05:34 - INFO - __main__ - Printing 3 examples
05/18/2022 02:05:34 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 02:05:34 - INFO - __main__ - ['others']
05/18/2022 02:05:34 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 02:05:34 - INFO - __main__ - ['others']
05/18/2022 02:05:34 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 02:05:34 - INFO - __main__ - ['others']
05/18/2022 02:05:34 - INFO - __main__ - Tokenizing Input ...
05/18/2022 02:05:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 02:05:34 - INFO - __main__ - Printing 3 examples
05/18/2022 02:05:34 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/18/2022 02:05:34 - INFO - __main__ - ['others']
05/18/2022 02:05:34 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/18/2022 02:05:34 - INFO - __main__ - ['others']
05/18/2022 02:05:34 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/18/2022 02:05:34 - INFO - __main__ - ['others']
05/18/2022 02:05:34 - INFO - __main__ - Tokenizing Input ...
05/18/2022 02:05:34 - INFO - __main__ - Tokenizing Output ...
05/18/2022 02:05:34 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 02:05:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 02:05:34 - INFO - __main__ - Printing 3 examples
05/18/2022 02:05:34 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/18/2022 02:05:34 - INFO - __main__ - ['others']
05/18/2022 02:05:34 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/18/2022 02:05:34 - INFO - __main__ - ['others']
05/18/2022 02:05:34 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/18/2022 02:05:34 - INFO - __main__ - ['others']
05/18/2022 02:05:34 - INFO - __main__ - Tokenizing Input ...
05/18/2022 02:05:34 - INFO - __main__ - Tokenizing Output ...
05/18/2022 02:05:34 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 02:05:36 - INFO - __main__ - Tokenizing Output ...
05/18/2022 02:05:40 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 02:05:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 02:05:40 - INFO - __main__ - Starting training!
05/18/2022 02:05:41 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 02:06:25 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_87_0.3_8_predictions.txt
05/18/2022 02:06:25 - INFO - __main__ - Classification-F1 on test data: 0.0795
05/18/2022 02:06:25 - INFO - __main__ - prefix=emo_16_87, lr=0.3, bsz=8, dev_performance=0.2813816127375449, test_performance=0.07947565088904372
05/18/2022 02:06:25 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.2, bsz=8 ...
05/18/2022 02:06:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 02:06:26 - INFO - __main__ - Printing 3 examples
05/18/2022 02:06:26 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/18/2022 02:06:26 - INFO - __main__ - ['others']
05/18/2022 02:06:26 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/18/2022 02:06:26 - INFO - __main__ - ['others']
05/18/2022 02:06:26 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/18/2022 02:06:26 - INFO - __main__ - ['others']
05/18/2022 02:06:26 - INFO - __main__ - Tokenizing Input ...
05/18/2022 02:06:26 - INFO - __main__ - Tokenizing Output ...
05/18/2022 02:06:26 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 02:06:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 02:06:26 - INFO - __main__ - Printing 3 examples
05/18/2022 02:06:26 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/18/2022 02:06:26 - INFO - __main__ - ['others']
05/18/2022 02:06:26 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/18/2022 02:06:26 - INFO - __main__ - ['others']
05/18/2022 02:06:26 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/18/2022 02:06:26 - INFO - __main__ - ['others']
05/18/2022 02:06:26 - INFO - __main__ - Tokenizing Input ...
05/18/2022 02:06:26 - INFO - __main__ - Tokenizing Output ...
05/18/2022 02:06:26 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 02:06:32 - INFO - __main__ - load prompt embedding from ckpt
05/18/2022 02:06:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 02:06:33 - INFO - __main__ - Starting training!
05/18/2022 02:06:34 - INFO - __main__ - Step 10 Global step 10 Train loss 4.16 on epoch=2
05/18/2022 02:06:35 - INFO - __main__ - Step 20 Global step 20 Train loss 4.05 on epoch=4
05/18/2022 02:06:37 - INFO - __main__ - Step 30 Global step 30 Train loss 3.77 on epoch=7
05/18/2022 02:06:38 - INFO - __main__ - Step 40 Global step 40 Train loss 3.45 on epoch=9
05/18/2022 02:06:39 - INFO - __main__ - Step 50 Global step 50 Train loss 3.29 on epoch=12
05/18/2022 02:06:40 - INFO - __main__ - Global step 50 Train loss 3.74 Classification-F1 0.0 on epoch=12
05/18/2022 02:06:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
05/18/2022 02:06:41 - INFO - __main__ - Step 60 Global step 60 Train loss 2.94 on epoch=14
05/18/2022 02:06:43 - INFO - __main__ - Step 70 Global step 70 Train loss 2.80 on epoch=17
05/18/2022 02:06:44 - INFO - __main__ - Step 80 Global step 80 Train loss 2.56 on epoch=19
05/18/2022 02:06:45 - INFO - __main__ - Step 90 Global step 90 Train loss 2.34 on epoch=22
05/18/2022 02:06:46 - INFO - __main__ - Step 100 Global step 100 Train loss 2.20 on epoch=24
05/18/2022 02:06:47 - INFO - __main__ - Global step 100 Train loss 2.57 Classification-F1 0.1 on epoch=24
05/18/2022 02:06:47 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.1 on epoch=24, global_step=100
05/18/2022 02:06:48 - INFO - __main__ - Step 110 Global step 110 Train loss 2.02 on epoch=27
05/18/2022 02:06:49 - INFO - __main__ - Step 120 Global step 120 Train loss 1.80 on epoch=29
05/18/2022 02:06:51 - INFO - __main__ - Step 130 Global step 130 Train loss 1.77 on epoch=32
05/18/2022 02:06:52 - INFO - __main__ - Step 140 Global step 140 Train loss 1.55 on epoch=34
05/18/2022 02:06:53 - INFO - __main__ - Step 150 Global step 150 Train loss 1.63 on epoch=37
05/18/2022 02:06:54 - INFO - __main__ - Global step 150 Train loss 1.75 Classification-F1 0.1 on epoch=37
05/18/2022 02:06:55 - INFO - __main__ - Step 160 Global step 160 Train loss 1.46 on epoch=39
05/18/2022 02:06:56 - INFO - __main__ - Step 170 Global step 170 Train loss 1.43 on epoch=42
05/18/2022 02:06:57 - INFO - __main__ - Step 180 Global step 180 Train loss 1.30 on epoch=44
05/18/2022 02:06:59 - INFO - __main__ - Step 190 Global step 190 Train loss 1.37 on epoch=47
05/18/2022 02:07:00 - INFO - __main__ - Step 200 Global step 200 Train loss 1.11 on epoch=49
05/18/2022 02:07:00 - INFO - __main__ - Global step 200 Train loss 1.33 Classification-F1 0.1 on epoch=49
05/18/2022 02:07:02 - INFO - __main__ - Step 210 Global step 210 Train loss 1.23 on epoch=52
05/18/2022 02:07:03 - INFO - __main__ - Step 220 Global step 220 Train loss 1.12 on epoch=54
05/18/2022 02:07:04 - INFO - __main__ - Step 230 Global step 230 Train loss 1.06 on epoch=57
05/18/2022 02:07:05 - INFO - __main__ - Step 240 Global step 240 Train loss 1.11 on epoch=59
05/18/2022 02:07:07 - INFO - __main__ - Step 250 Global step 250 Train loss 1.14 on epoch=62
05/18/2022 02:07:07 - INFO - __main__ - Global step 250 Train loss 1.13 Classification-F1 0.1 on epoch=62
05/18/2022 02:07:08 - INFO - __main__ - Step 260 Global step 260 Train loss 1.00 on epoch=64
05/18/2022 02:07:10 - INFO - __main__ - Step 270 Global step 270 Train loss 1.06 on epoch=67
05/18/2022 02:07:11 - INFO - __main__ - Step 280 Global step 280 Train loss 1.07 on epoch=69
05/18/2022 02:07:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.91 on epoch=72
05/18/2022 02:07:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.98 on epoch=74
05/18/2022 02:07:14 - INFO - __main__ - Global step 300 Train loss 1.00 Classification-F1 0.1 on epoch=74
05/18/2022 02:07:15 - INFO - __main__ - Step 310 Global step 310 Train loss 1.02 on epoch=77
05/18/2022 02:07:16 - INFO - __main__ - Step 320 Global step 320 Train loss 1.03 on epoch=79
05/18/2022 02:07:18 - INFO - __main__ - Step 330 Global step 330 Train loss 1.00 on epoch=82
05/18/2022 02:07:19 - INFO - __main__ - Step 340 Global step 340 Train loss 1.04 on epoch=84
05/18/2022 02:07:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.92 on epoch=87
05/18/2022 02:07:21 - INFO - __main__ - Global step 350 Train loss 1.00 Classification-F1 0.15526315789473685 on epoch=87
05/18/2022 02:07:21 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.15526315789473685 on epoch=87, global_step=350
05/18/2022 02:07:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.97 on epoch=89
05/18/2022 02:07:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.92 on epoch=92
05/18/2022 02:07:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.93 on epoch=94
05/18/2022 02:07:26 - INFO - __main__ - Step 390 Global step 390 Train loss 1.05 on epoch=97
05/18/2022 02:07:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.95 on epoch=99
05/18/2022 02:07:27 - INFO - __main__ - Global step 400 Train loss 0.96 Classification-F1 0.1 on epoch=99
05/18/2022 02:07:29 - INFO - __main__ - Step 410 Global step 410 Train loss 1.05 on epoch=102
05/18/2022 02:07:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.92 on epoch=104
05/18/2022 02:07:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.93 on epoch=107
05/18/2022 02:07:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.93 on epoch=109
05/18/2022 02:07:34 - INFO - __main__ - Step 450 Global step 450 Train loss 1.04 on epoch=112
05/18/2022 02:07:34 - INFO - __main__ - Global step 450 Train loss 0.97 Classification-F1 0.1 on epoch=112
05/18/2022 02:07:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.96 on epoch=114
05/18/2022 02:07:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.94 on epoch=117
05/18/2022 02:07:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.89 on epoch=119
05/18/2022 02:07:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.93 on epoch=122
05/18/2022 02:07:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.92 on epoch=124
05/18/2022 02:07:41 - INFO - __main__ - Global step 500 Train loss 0.93 Classification-F1 0.1 on epoch=124
05/18/2022 02:07:42 - INFO - __main__ - Step 510 Global step 510 Train loss 1.04 on epoch=127
05/18/2022 02:07:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.83 on epoch=129
05/18/2022 02:07:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.90 on epoch=132
05/18/2022 02:07:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.98 on epoch=134
05/18/2022 02:07:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.93 on epoch=137
05/18/2022 02:07:48 - INFO - __main__ - Global step 550 Train loss 0.93 Classification-F1 0.1 on epoch=137
05/18/2022 02:07:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.95 on epoch=139
05/18/2022 02:07:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.97 on epoch=142
05/18/2022 02:07:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.83 on epoch=144
05/18/2022 02:07:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.98 on epoch=147
05/18/2022 02:07:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.87 on epoch=149
05/18/2022 02:07:54 - INFO - __main__ - Global step 600 Train loss 0.92 Classification-F1 0.1 on epoch=149
05/18/2022 02:07:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.99 on epoch=152
05/18/2022 02:07:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.86 on epoch=154
05/18/2022 02:07:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.97 on epoch=157
05/18/2022 02:07:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.91 on epoch=159
05/18/2022 02:08:01 - INFO - __main__ - Step 650 Global step 650 Train loss 1.01 on epoch=162
05/18/2022 02:08:01 - INFO - __main__ - Global step 650 Train loss 0.95 Classification-F1 0.13034188034188032 on epoch=162
05/18/2022 02:08:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.91 on epoch=164
05/18/2022 02:08:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.87 on epoch=167
05/18/2022 02:08:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.87 on epoch=169
05/18/2022 02:08:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.89 on epoch=172
05/18/2022 02:08:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.96 on epoch=174
05/18/2022 02:08:08 - INFO - __main__ - Global step 700 Train loss 0.90 Classification-F1 0.1 on epoch=174
05/18/2022 02:08:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.93 on epoch=177
05/18/2022 02:08:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.85 on epoch=179
05/18/2022 02:08:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.91 on epoch=182
05/18/2022 02:08:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.93 on epoch=184
05/18/2022 02:08:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.89 on epoch=187
05/18/2022 02:08:14 - INFO - __main__ - Global step 750 Train loss 0.90 Classification-F1 0.1 on epoch=187
05/18/2022 02:08:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.88 on epoch=189
05/18/2022 02:08:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.89 on epoch=192
05/18/2022 02:08:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.87 on epoch=194
05/18/2022 02:08:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.91 on epoch=197
05/18/2022 02:08:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.87 on epoch=199
05/18/2022 02:08:21 - INFO - __main__ - Global step 800 Train loss 0.88 Classification-F1 0.1 on epoch=199
05/18/2022 02:08:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.78 on epoch=202
05/18/2022 02:08:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.88 on epoch=204
05/18/2022 02:08:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.86 on epoch=207
05/18/2022 02:08:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.96 on epoch=209
05/18/2022 02:08:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.81 on epoch=212
05/18/2022 02:08:28 - INFO - __main__ - Global step 850 Train loss 0.86 Classification-F1 0.1 on epoch=212
05/18/2022 02:08:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.83 on epoch=214
05/18/2022 02:08:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.89 on epoch=217
05/18/2022 02:08:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.89 on epoch=219
05/18/2022 02:08:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.76 on epoch=222
05/18/2022 02:08:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.86 on epoch=224
05/18/2022 02:08:35 - INFO - __main__ - Global step 900 Train loss 0.85 Classification-F1 0.1 on epoch=224
05/18/2022 02:08:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.83 on epoch=227
05/18/2022 02:08:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.86 on epoch=229
05/18/2022 02:08:38 - INFO - __main__ - Step 930 Global step 930 Train loss 0.93 on epoch=232
05/18/2022 02:08:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.84 on epoch=234
05/18/2022 02:08:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.82 on epoch=237
05/18/2022 02:08:42 - INFO - __main__ - Global step 950 Train loss 0.86 Classification-F1 0.1 on epoch=237
05/18/2022 02:08:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.78 on epoch=239
05/18/2022 02:08:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.77 on epoch=242
05/18/2022 02:08:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.85 on epoch=244
05/18/2022 02:08:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.85 on epoch=247
05/18/2022 02:08:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.87 on epoch=249
05/18/2022 02:08:48 - INFO - __main__ - Global step 1000 Train loss 0.82 Classification-F1 0.1 on epoch=249
05/18/2022 02:08:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.92 on epoch=252
05/18/2022 02:08:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.78 on epoch=254
05/18/2022 02:08:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.85 on epoch=257
05/18/2022 02:08:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.85 on epoch=259
05/18/2022 02:08:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.80 on epoch=262
05/18/2022 02:08:55 - INFO - __main__ - Global step 1050 Train loss 0.84 Classification-F1 0.1 on epoch=262
05/18/2022 02:08:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.81 on epoch=264
05/18/2022 02:08:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.78 on epoch=267
05/18/2022 02:08:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.75 on epoch=269
05/18/2022 02:09:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.88 on epoch=272
05/18/2022 02:09:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.85 on epoch=274
05/18/2022 02:09:02 - INFO - __main__ - Global step 1100 Train loss 0.81 Classification-F1 0.1 on epoch=274
05/18/2022 02:09:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.82 on epoch=277
05/18/2022 02:09:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.86 on epoch=279
05/18/2022 02:09:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.85 on epoch=282
05/18/2022 02:09:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.89 on epoch=284
05/18/2022 02:09:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.91 on epoch=287
05/18/2022 02:09:08 - INFO - __main__ - Global step 1150 Train loss 0.86 Classification-F1 0.1 on epoch=287
05/18/2022 02:09:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.89 on epoch=289
05/18/2022 02:09:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.90 on epoch=292
05/18/2022 02:09:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.83 on epoch=294
05/18/2022 02:09:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.84 on epoch=297
05/18/2022 02:09:15 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.90 on epoch=299
05/18/2022 02:09:15 - INFO - __main__ - Global step 1200 Train loss 0.87 Classification-F1 0.1 on epoch=299
05/18/2022 02:09:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.89 on epoch=302
05/18/2022 02:09:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.84 on epoch=304
05/18/2022 02:09:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.86 on epoch=307
05/18/2022 02:09:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.92 on epoch=309
05/18/2022 02:09:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.76 on epoch=312
05/18/2022 02:09:22 - INFO - __main__ - Global step 1250 Train loss 0.85 Classification-F1 0.1 on epoch=312
05/18/2022 02:09:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.84 on epoch=314
05/18/2022 02:09:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.80 on epoch=317
05/18/2022 02:09:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.91 on epoch=319
05/18/2022 02:09:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.87 on epoch=322
05/18/2022 02:09:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.76 on epoch=324
05/18/2022 02:09:29 - INFO - __main__ - Global step 1300 Train loss 0.84 Classification-F1 0.1 on epoch=324
05/18/2022 02:09:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.83 on epoch=327
05/18/2022 02:09:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.84 on epoch=329
05/18/2022 02:09:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.79 on epoch=332
05/18/2022 02:09:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.80 on epoch=334
05/18/2022 02:09:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.82 on epoch=337
05/18/2022 02:09:35 - INFO - __main__ - Global step 1350 Train loss 0.82 Classification-F1 0.13034188034188032 on epoch=337
05/18/2022 02:09:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.83 on epoch=339
05/18/2022 02:09:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.87 on epoch=342
05/18/2022 02:09:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.82 on epoch=344
05/18/2022 02:09:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.85 on epoch=347
05/18/2022 02:09:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.84 on epoch=349
05/18/2022 02:09:42 - INFO - __main__ - Global step 1400 Train loss 0.84 Classification-F1 0.1 on epoch=349
05/18/2022 02:09:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.82 on epoch=352
05/18/2022 02:09:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.82 on epoch=354
05/18/2022 02:09:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.84 on epoch=357
05/18/2022 02:09:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.87 on epoch=359
05/18/2022 02:09:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.85 on epoch=362
05/18/2022 02:09:49 - INFO - __main__ - Global step 1450 Train loss 0.84 Classification-F1 0.1 on epoch=362
05/18/2022 02:09:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.82 on epoch=364
05/18/2022 02:09:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.86 on epoch=367
05/18/2022 02:09:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.82 on epoch=369
05/18/2022 02:09:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.83 on epoch=372
05/18/2022 02:09:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.78 on epoch=374
05/18/2022 02:09:56 - INFO - __main__ - Global step 1500 Train loss 0.82 Classification-F1 0.1 on epoch=374
05/18/2022 02:09:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.82 on epoch=377
05/18/2022 02:09:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.86 on epoch=379
05/18/2022 02:09:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.84 on epoch=382
05/18/2022 02:10:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.83 on epoch=384
05/18/2022 02:10:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.76 on epoch=387
05/18/2022 02:10:02 - INFO - __main__ - Global step 1550 Train loss 0.82 Classification-F1 0.1 on epoch=387
05/18/2022 02:10:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.80 on epoch=389
05/18/2022 02:10:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.80 on epoch=392
05/18/2022 02:10:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.79 on epoch=394
05/18/2022 02:10:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.79 on epoch=397
05/18/2022 02:10:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.83 on epoch=399
05/18/2022 02:10:09 - INFO - __main__ - Global step 1600 Train loss 0.80 Classification-F1 0.1 on epoch=399
05/18/2022 02:10:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.78 on epoch=402
05/18/2022 02:10:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.85 on epoch=404
05/18/2022 02:10:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.84 on epoch=407
05/18/2022 02:10:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.88 on epoch=409
05/18/2022 02:10:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.74 on epoch=412
05/18/2022 02:10:16 - INFO - __main__ - Global step 1650 Train loss 0.82 Classification-F1 0.10126582278481013 on epoch=412
05/18/2022 02:10:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.79 on epoch=414
05/18/2022 02:10:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.88 on epoch=417
05/18/2022 02:10:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.84 on epoch=419
05/18/2022 02:10:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.90 on epoch=422
05/18/2022 02:10:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.88 on epoch=424
05/18/2022 02:10:22 - INFO - __main__ - Global step 1700 Train loss 0.86 Classification-F1 0.1 on epoch=424
05/18/2022 02:10:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.85 on epoch=427
05/18/2022 02:10:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.90 on epoch=429
05/18/2022 02:10:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.83 on epoch=432
05/18/2022 02:10:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.80 on epoch=434
05/18/2022 02:10:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.81 on epoch=437
05/18/2022 02:10:29 - INFO - __main__ - Global step 1750 Train loss 0.84 Classification-F1 0.1 on epoch=437
05/18/2022 02:10:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.80 on epoch=439
05/18/2022 02:10:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.85 on epoch=442
05/18/2022 02:10:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.77 on epoch=444
05/18/2022 02:10:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.86 on epoch=447
05/18/2022 02:10:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.87 on epoch=449
05/18/2022 02:10:36 - INFO - __main__ - Global step 1800 Train loss 0.83 Classification-F1 0.1 on epoch=449
05/18/2022 02:10:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.81 on epoch=452
05/18/2022 02:10:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.74 on epoch=454
05/18/2022 02:10:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.80 on epoch=457
05/18/2022 02:10:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.78 on epoch=459
05/18/2022 02:10:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.86 on epoch=462
05/18/2022 02:10:43 - INFO - __main__ - Global step 1850 Train loss 0.80 Classification-F1 0.1 on epoch=462
05/18/2022 02:10:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.85 on epoch=464
05/18/2022 02:10:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.85 on epoch=467
05/18/2022 02:10:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.80 on epoch=469
05/18/2022 02:10:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.85 on epoch=472
05/18/2022 02:10:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.86 on epoch=474
05/18/2022 02:10:49 - INFO - __main__ - Global step 1900 Train loss 0.84 Classification-F1 0.1 on epoch=474
05/18/2022 02:10:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.77 on epoch=477
05/18/2022 02:10:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.86 on epoch=479
05/18/2022 02:10:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.79 on epoch=482
05/18/2022 02:10:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.83 on epoch=484
05/18/2022 02:10:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.83 on epoch=487
05/18/2022 02:10:56 - INFO - __main__ - Global step 1950 Train loss 0.81 Classification-F1 0.1 on epoch=487
05/18/2022 02:10:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.85 on epoch=489
05/18/2022 02:10:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.84 on epoch=492
05/18/2022 02:11:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.86 on epoch=494
05/18/2022 02:11:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.76 on epoch=497
05/18/2022 02:11:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.85 on epoch=499
05/18/2022 02:11:03 - INFO - __main__ - Global step 2000 Train loss 0.83 Classification-F1 0.1 on epoch=499
05/18/2022 02:11:04 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.79 on epoch=502
05/18/2022 02:11:05 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.79 on epoch=504
05/18/2022 02:11:07 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.86 on epoch=507
05/18/2022 02:11:08 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.84 on epoch=509
05/18/2022 02:11:09 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.82 on epoch=512
05/18/2022 02:11:10 - INFO - __main__ - Global step 2050 Train loss 0.82 Classification-F1 0.1 on epoch=512
05/18/2022 02:11:11 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.77 on epoch=514
05/18/2022 02:11:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.81 on epoch=517
05/18/2022 02:11:13 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.83 on epoch=519
05/18/2022 02:11:15 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.84 on epoch=522
05/18/2022 02:11:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.80 on epoch=524
05/18/2022 02:11:16 - INFO - __main__ - Global step 2100 Train loss 0.81 Classification-F1 0.1 on epoch=524
05/18/2022 02:11:18 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.86 on epoch=527
05/18/2022 02:11:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.79 on epoch=529
05/18/2022 02:11:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.81 on epoch=532
05/18/2022 02:11:21 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.80 on epoch=534
05/18/2022 02:11:23 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.84 on epoch=537
05/18/2022 02:11:23 - INFO - __main__ - Global step 2150 Train loss 0.82 Classification-F1 0.1 on epoch=537
05/18/2022 02:11:24 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.85 on epoch=539
05/18/2022 02:11:26 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.76 on epoch=542
05/18/2022 02:11:27 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.83 on epoch=544
05/18/2022 02:11:28 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.81 on epoch=547
05/18/2022 02:11:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.86 on epoch=549
05/18/2022 02:11:30 - INFO - __main__ - Global step 2200 Train loss 0.82 Classification-F1 0.1 on epoch=549
05/18/2022 02:11:31 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.80 on epoch=552
05/18/2022 02:11:32 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.80 on epoch=554
05/18/2022 02:11:34 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.81 on epoch=557
05/18/2022 02:11:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.78 on epoch=559
05/18/2022 02:11:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.89 on epoch=562
05/18/2022 02:11:37 - INFO - __main__ - Global step 2250 Train loss 0.82 Classification-F1 0.1 on epoch=562
05/18/2022 02:11:38 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.85 on epoch=564
05/18/2022 02:11:39 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.80 on epoch=567
05/18/2022 02:11:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.81 on epoch=569
05/18/2022 02:11:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.87 on epoch=572
05/18/2022 02:11:43 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.77 on epoch=574
05/18/2022 02:11:43 - INFO - __main__ - Global step 2300 Train loss 0.82 Classification-F1 0.1 on epoch=574
05/18/2022 02:11:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.88 on epoch=577
05/18/2022 02:11:46 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.81 on epoch=579
05/18/2022 02:11:47 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.87 on epoch=582
05/18/2022 02:11:49 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.81 on epoch=584
05/18/2022 02:11:50 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.79 on epoch=587
05/18/2022 02:11:50 - INFO - __main__ - Global step 2350 Train loss 0.83 Classification-F1 0.1 on epoch=587
05/18/2022 02:11:52 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.82 on epoch=589
05/18/2022 02:11:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.81 on epoch=592
05/18/2022 02:11:54 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.83 on epoch=594
05/18/2022 02:11:55 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.84 on epoch=597
05/18/2022 02:11:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.82 on epoch=599
05/18/2022 02:11:57 - INFO - __main__ - Global step 2400 Train loss 0.83 Classification-F1 0.1 on epoch=599
05/18/2022 02:11:58 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.83 on epoch=602
05/18/2022 02:12:00 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.85 on epoch=604
05/18/2022 02:12:01 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.78 on epoch=607
05/18/2022 02:12:02 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.82 on epoch=609
05/18/2022 02:12:03 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.82 on epoch=612
05/18/2022 02:12:04 - INFO - __main__ - Global step 2450 Train loss 0.82 Classification-F1 0.1 on epoch=612
05/18/2022 02:12:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.79 on epoch=614
05/18/2022 02:12:06 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.89 on epoch=617
05/18/2022 02:12:08 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.81 on epoch=619
05/18/2022 02:12:09 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.80 on epoch=622
05/18/2022 02:12:10 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.80 on epoch=624
05/18/2022 02:12:11 - INFO - __main__ - Global step 2500 Train loss 0.82 Classification-F1 0.1 on epoch=624
05/18/2022 02:12:12 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.86 on epoch=627
05/18/2022 02:12:13 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.83 on epoch=629
05/18/2022 02:12:14 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.80 on epoch=632
05/18/2022 02:12:15 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.81 on epoch=634
05/18/2022 02:12:17 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.74 on epoch=637
05/18/2022 02:12:17 - INFO - __main__ - Global step 2550 Train loss 0.81 Classification-F1 0.1 on epoch=637
05/18/2022 02:12:18 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.83 on epoch=639
05/18/2022 02:12:20 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.79 on epoch=642
05/18/2022 02:12:21 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.77 on epoch=644
05/18/2022 02:12:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.83 on epoch=647
05/18/2022 02:12:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.80 on epoch=649
05/18/2022 02:12:24 - INFO - __main__ - Global step 2600 Train loss 0.80 Classification-F1 0.1 on epoch=649
05/18/2022 02:12:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.86 on epoch=652
05/18/2022 02:12:26 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.74 on epoch=654
05/18/2022 02:12:28 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.83 on epoch=657
05/18/2022 02:12:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.81 on epoch=659
05/18/2022 02:12:30 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.79 on epoch=662
05/18/2022 02:12:31 - INFO - __main__ - Global step 2650 Train loss 0.81 Classification-F1 0.1 on epoch=662
05/18/2022 02:12:32 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.76 on epoch=664
05/18/2022 02:12:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.82 on epoch=667
05/18/2022 02:12:34 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.82 on epoch=669
05/18/2022 02:12:36 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.78 on epoch=672
05/18/2022 02:12:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.81 on epoch=674
05/18/2022 02:12:37 - INFO - __main__ - Global step 2700 Train loss 0.80 Classification-F1 0.1 on epoch=674
05/18/2022 02:12:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.77 on epoch=677
05/18/2022 02:12:40 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.76 on epoch=679
05/18/2022 02:12:41 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.84 on epoch=682
05/18/2022 02:12:42 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.73 on epoch=684
05/18/2022 02:12:44 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.86 on epoch=687
05/18/2022 02:12:44 - INFO - __main__ - Global step 2750 Train loss 0.79 Classification-F1 0.1 on epoch=687
05/18/2022 02:12:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.82 on epoch=689
05/18/2022 02:12:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.81 on epoch=692
05/18/2022 02:12:48 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.84 on epoch=694
05/18/2022 02:12:49 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.79 on epoch=697
05/18/2022 02:12:50 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.80 on epoch=699
05/18/2022 02:12:51 - INFO - __main__ - Global step 2800 Train loss 0.81 Classification-F1 0.1 on epoch=699
05/18/2022 02:12:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.76 on epoch=702
05/18/2022 02:12:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.84 on epoch=704
05/18/2022 02:12:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.81 on epoch=707
05/18/2022 02:12:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.82 on epoch=709
05/18/2022 02:12:57 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.75 on epoch=712
05/18/2022 02:12:58 - INFO - __main__ - Global step 2850 Train loss 0.80 Classification-F1 0.1 on epoch=712
05/18/2022 02:12:59 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.81 on epoch=714
05/18/2022 02:13:00 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.80 on epoch=717
05/18/2022 02:13:01 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.75 on epoch=719
05/18/2022 02:13:03 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.79 on epoch=722
05/18/2022 02:13:04 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.77 on epoch=724
05/18/2022 02:13:04 - INFO - __main__ - Global step 2900 Train loss 0.78 Classification-F1 0.1 on epoch=724
05/18/2022 02:13:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.78 on epoch=727
05/18/2022 02:13:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.84 on epoch=729
05/18/2022 02:13:08 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.82 on epoch=732
05/18/2022 02:13:09 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.86 on epoch=734
05/18/2022 02:13:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.78 on epoch=737
05/18/2022 02:13:11 - INFO - __main__ - Global step 2950 Train loss 0.82 Classification-F1 0.1 on epoch=737
05/18/2022 02:13:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.81 on epoch=739
05/18/2022 02:13:14 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.85 on epoch=742
05/18/2022 02:13:15 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.83 on epoch=744
05/18/2022 02:13:16 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.80 on epoch=747
05/18/2022 02:13:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.82 on epoch=749
05/18/2022 02:13:18 - INFO - __main__ - Global step 3000 Train loss 0.82 Classification-F1 0.1 on epoch=749
05/18/2022 02:13:18 - INFO - __main__ - save last model!
05/18/2022 02:13:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 02:13:18 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 02:13:18 - INFO - __main__ - Printing 3 examples
05/18/2022 02:13:18 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 02:13:18 - INFO - __main__ - ['others']
05/18/2022 02:13:18 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 02:13:18 - INFO - __main__ - ['others']
05/18/2022 02:13:18 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 02:13:18 - INFO - __main__ - ['others']
05/18/2022 02:13:18 - INFO - __main__ - Tokenizing Input ...
05/18/2022 02:13:20 - INFO - __main__ - Tokenizing Output ...
05/18/2022 02:13:25 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 02:14:09 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_87_0.2_8_predictions.txt
05/18/2022 02:14:09 - INFO - __main__ - Classification-F1 on test data: 0.0275
05/18/2022 02:14:09 - INFO - __main__ - prefix=emo_16_87, lr=0.2, bsz=8, dev_performance=0.15526315789473685, test_performance=0.02746894914165758
05/21/2022 09:17:14 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-base-multitask-cls2cls-5e-1-4-20', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-base-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='4,5')
05/21/2022 09:17:14 - INFO - __main__ - models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo
05/21/2022 09:17:14 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-base-multitask-cls2cls-5e-1-4-20', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-base-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='4,5')
05/21/2022 09:17:14 - INFO - __main__ - models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo
05/21/2022 09:17:15 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 09:17:15 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 09:17:15 - INFO - __main__ - args.device: cuda:0
05/21/2022 09:17:15 - INFO - __main__ - Using 2 gpus
05/21/2022 09:17:15 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/21/2022 09:17:15 - INFO - __main__ - args.device: cuda:1
05/21/2022 09:17:15 - INFO - __main__ - Using 2 gpus
05/21/2022 09:17:15 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/21/2022 09:17:24 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
05/21/2022 09:17:25 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:17:25 - INFO - __main__ - Printing 3 examples
05/21/2022 09:17:25 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 09:17:25 - INFO - __main__ - ['others']
05/21/2022 09:17:25 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 09:17:25 - INFO - __main__ - ['others']
05/21/2022 09:17:25 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 09:17:25 - INFO - __main__ - ['others']
05/21/2022 09:17:25 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:17:25 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:17:25 - INFO - __main__ - Printing 3 examples
05/21/2022 09:17:25 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 09:17:25 - INFO - __main__ - ['others']
05/21/2022 09:17:25 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 09:17:25 - INFO - __main__ - ['others']
05/21/2022 09:17:25 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 09:17:25 - INFO - __main__ - ['others']
05/21/2022 09:17:25 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:17:25 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:17:25 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:17:25 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 09:17:25 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:17:25 - INFO - __main__ - Printing 3 examples
05/21/2022 09:17:25 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 09:17:25 - INFO - __main__ - ['others']
05/21/2022 09:17:25 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 09:17:25 - INFO - __main__ - ['others']
05/21/2022 09:17:25 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 09:17:25 - INFO - __main__ - ['others']
05/21/2022 09:17:25 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:17:25 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 09:17:25 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:17:25 - INFO - __main__ - Printing 3 examples
05/21/2022 09:17:25 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 09:17:25 - INFO - __main__ - ['others']
05/21/2022 09:17:25 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 09:17:25 - INFO - __main__ - ['others']
05/21/2022 09:17:25 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 09:17:25 - INFO - __main__ - ['others']
05/21/2022 09:17:25 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:17:25 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:17:25 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:17:25 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 09:17:25 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 09:17:32 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 09:17:32 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 09:17:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 09:17:32 - INFO - __main__ - Starting training!
05/21/2022 09:17:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 09:17:38 - INFO - __main__ - Starting training!
05/21/2022 09:17:41 - INFO - __main__ - Step 10 Global step 10 Train loss 3.97 on epoch=2
05/21/2022 09:17:42 - INFO - __main__ - Step 20 Global step 20 Train loss 3.30 on epoch=4
05/21/2022 09:17:43 - INFO - __main__ - Step 30 Global step 30 Train loss 2.67 on epoch=7
05/21/2022 09:17:44 - INFO - __main__ - Step 40 Global step 40 Train loss 2.06 on epoch=9
05/21/2022 09:17:46 - INFO - __main__ - Step 50 Global step 50 Train loss 1.78 on epoch=12
05/21/2022 09:17:46 - INFO - __main__ - Global step 50 Train loss 2.76 Classification-F1 0.1 on epoch=12
05/21/2022 09:17:46 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/21/2022 09:17:48 - INFO - __main__ - Step 60 Global step 60 Train loss 1.35 on epoch=14
05/21/2022 09:17:49 - INFO - __main__ - Step 70 Global step 70 Train loss 1.34 on epoch=17
05/21/2022 09:17:50 - INFO - __main__ - Step 80 Global step 80 Train loss 1.06 on epoch=19
05/21/2022 09:17:52 - INFO - __main__ - Step 90 Global step 90 Train loss 1.09 on epoch=22
05/21/2022 09:17:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.93 on epoch=24
05/21/2022 09:17:54 - INFO - __main__ - Global step 100 Train loss 1.15 Classification-F1 0.16138763197586728 on epoch=24
05/21/2022 09:17:54 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.16138763197586728 on epoch=24, global_step=100
05/21/2022 09:17:55 - INFO - __main__ - Step 110 Global step 110 Train loss 1.01 on epoch=27
05/21/2022 09:17:56 - INFO - __main__ - Step 120 Global step 120 Train loss 0.99 on epoch=29
05/21/2022 09:17:58 - INFO - __main__ - Step 130 Global step 130 Train loss 1.01 on epoch=32
05/21/2022 09:17:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.94 on epoch=34
05/21/2022 09:18:01 - INFO - __main__ - Step 150 Global step 150 Train loss 1.12 on epoch=37
05/21/2022 09:18:01 - INFO - __main__ - Global step 150 Train loss 1.01 Classification-F1 0.1 on epoch=37
05/21/2022 09:18:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.93 on epoch=39
05/21/2022 09:18:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.93 on epoch=42
05/21/2022 09:18:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.90 on epoch=44
05/21/2022 09:18:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.99 on epoch=47
05/21/2022 09:18:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.90 on epoch=49
05/21/2022 09:18:08 - INFO - __main__ - Global step 200 Train loss 0.93 Classification-F1 0.1758862352861128 on epoch=49
05/21/2022 09:18:08 - INFO - __main__ - Saving model with best Classification-F1: 0.16138763197586728 -> 0.1758862352861128 on epoch=49, global_step=200
05/21/2022 09:18:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.96 on epoch=52
05/21/2022 09:18:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.90 on epoch=54
05/21/2022 09:18:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.90 on epoch=57
05/21/2022 09:18:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.92 on epoch=59
05/21/2022 09:18:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.94 on epoch=62
05/21/2022 09:18:15 - INFO - __main__ - Global step 250 Train loss 0.92 Classification-F1 0.1546731022378236 on epoch=62
05/21/2022 09:18:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=64
05/21/2022 09:18:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.95 on epoch=67
05/21/2022 09:18:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.85 on epoch=69
05/21/2022 09:18:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.90 on epoch=72
05/21/2022 09:18:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.81 on epoch=74
05/21/2022 09:18:22 - INFO - __main__ - Global step 300 Train loss 0.87 Classification-F1 0.16944444444444445 on epoch=74
05/21/2022 09:18:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.92 on epoch=77
05/21/2022 09:18:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.79 on epoch=79
05/21/2022 09:18:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.89 on epoch=82
05/21/2022 09:18:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.91 on epoch=84
05/21/2022 09:18:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.85 on epoch=87
05/21/2022 09:18:30 - INFO - __main__ - Global step 350 Train loss 0.87 Classification-F1 0.1974891774891775 on epoch=87
05/21/2022 09:18:30 - INFO - __main__ - Saving model with best Classification-F1: 0.1758862352861128 -> 0.1974891774891775 on epoch=87, global_step=350
05/21/2022 09:18:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.88 on epoch=89
05/21/2022 09:18:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.82 on epoch=92
05/21/2022 09:18:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.83 on epoch=94
05/21/2022 09:18:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=97
05/21/2022 09:18:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.88 on epoch=99
05/21/2022 09:18:37 - INFO - __main__ - Global step 400 Train loss 0.86 Classification-F1 0.12499999999999999 on epoch=99
05/21/2022 09:18:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.81 on epoch=102
05/21/2022 09:18:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.95 on epoch=104
05/21/2022 09:18:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.92 on epoch=107
05/21/2022 09:18:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.92 on epoch=109
05/21/2022 09:18:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.87 on epoch=112
05/21/2022 09:18:45 - INFO - __main__ - Global step 450 Train loss 0.89 Classification-F1 0.1743014200641319 on epoch=112
05/21/2022 09:18:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.81 on epoch=114
05/21/2022 09:18:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.91 on epoch=117
05/21/2022 09:18:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.83 on epoch=119
05/21/2022 09:18:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.86 on epoch=122
05/21/2022 09:18:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.90 on epoch=124
05/21/2022 09:18:52 - INFO - __main__ - Global step 500 Train loss 0.86 Classification-F1 0.156201749871333 on epoch=124
05/21/2022 09:18:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.89 on epoch=127
05/21/2022 09:18:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.85 on epoch=129
05/21/2022 09:18:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.82 on epoch=132
05/21/2022 09:18:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.80 on epoch=134
05/21/2022 09:18:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.86 on epoch=137
05/21/2022 09:19:00 - INFO - __main__ - Global step 550 Train loss 0.84 Classification-F1 0.11285714285714285 on epoch=137
05/21/2022 09:19:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.81 on epoch=139
05/21/2022 09:19:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.78 on epoch=142
05/21/2022 09:19:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.84 on epoch=144
05/21/2022 09:19:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.79 on epoch=147
05/21/2022 09:19:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.83 on epoch=149
05/21/2022 09:19:07 - INFO - __main__ - Global step 600 Train loss 0.81 Classification-F1 0.1412763767370046 on epoch=149
05/21/2022 09:19:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.83 on epoch=152
05/21/2022 09:19:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.80 on epoch=154
05/21/2022 09:19:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.82 on epoch=157
05/21/2022 09:19:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.80 on epoch=159
05/21/2022 09:19:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.77 on epoch=162
05/21/2022 09:19:14 - INFO - __main__ - Global step 650 Train loss 0.80 Classification-F1 0.1912442396313364 on epoch=162
05/21/2022 09:19:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.96 on epoch=164
05/21/2022 09:19:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.79 on epoch=167
05/21/2022 09:19:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.85 on epoch=169
05/21/2022 09:19:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.86 on epoch=172
05/21/2022 09:19:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.84 on epoch=174
05/21/2022 09:19:21 - INFO - __main__ - Global step 700 Train loss 0.86 Classification-F1 0.15015015015015015 on epoch=174
05/21/2022 09:19:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.80 on epoch=177
05/21/2022 09:19:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.81 on epoch=179
05/21/2022 09:19:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.86 on epoch=182
05/21/2022 09:19:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.83 on epoch=184
05/21/2022 09:19:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.85 on epoch=187
05/21/2022 09:19:29 - INFO - __main__ - Global step 750 Train loss 0.83 Classification-F1 0.16059379217273953 on epoch=187
05/21/2022 09:19:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.77 on epoch=189
05/21/2022 09:19:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.82 on epoch=192
05/21/2022 09:19:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.81 on epoch=194
05/21/2022 09:19:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.81 on epoch=197
05/21/2022 09:19:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.79 on epoch=199
05/21/2022 09:19:36 - INFO - __main__ - Global step 800 Train loss 0.80 Classification-F1 0.10757575757575757 on epoch=199
05/21/2022 09:19:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.84 on epoch=202
05/21/2022 09:19:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.83 on epoch=204
05/21/2022 09:19:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.84 on epoch=207
05/21/2022 09:19:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.79 on epoch=209
05/21/2022 09:19:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.86 on epoch=212
05/21/2022 09:19:43 - INFO - __main__ - Global step 850 Train loss 0.83 Classification-F1 0.08904109589041095 on epoch=212
05/21/2022 09:19:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.79 on epoch=214
05/21/2022 09:19:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.82 on epoch=217
05/21/2022 09:19:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.81 on epoch=219
05/21/2022 09:19:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.81 on epoch=222
05/21/2022 09:19:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.81 on epoch=224
05/21/2022 09:19:50 - INFO - __main__ - Global step 900 Train loss 0.81 Classification-F1 0.1390013495276653 on epoch=224
05/21/2022 09:19:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.84 on epoch=227
05/21/2022 09:19:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.85 on epoch=229
05/21/2022 09:19:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.81 on epoch=232
05/21/2022 09:19:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.86 on epoch=234
05/21/2022 09:19:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.80 on epoch=237
05/21/2022 09:19:57 - INFO - __main__ - Global step 950 Train loss 0.83 Classification-F1 0.1853146853146853 on epoch=237
05/21/2022 09:19:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.78 on epoch=239
05/21/2022 09:20:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.85 on epoch=242
05/21/2022 09:20:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.83 on epoch=244
05/21/2022 09:20:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.77 on epoch=247
05/21/2022 09:20:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.76 on epoch=249
05/21/2022 09:20:05 - INFO - __main__ - Global step 1000 Train loss 0.80 Classification-F1 0.13149768399382397 on epoch=249
05/21/2022 09:20:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.80 on epoch=252
05/21/2022 09:20:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.72 on epoch=254
05/21/2022 09:20:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.79 on epoch=257
05/21/2022 09:20:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.78 on epoch=259
05/21/2022 09:20:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.84 on epoch=262
05/21/2022 09:20:12 - INFO - __main__ - Global step 1050 Train loss 0.79 Classification-F1 0.13233835200746966 on epoch=262
05/21/2022 09:20:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.76 on epoch=264
05/21/2022 09:20:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.78 on epoch=267
05/21/2022 09:20:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.75 on epoch=269
05/21/2022 09:20:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.82 on epoch=272
05/21/2022 09:20:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.76 on epoch=274
05/21/2022 09:20:20 - INFO - __main__ - Global step 1100 Train loss 0.77 Classification-F1 0.15192359429647564 on epoch=274
05/21/2022 09:20:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.80 on epoch=277
05/21/2022 09:20:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.78 on epoch=279
05/21/2022 09:20:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.81 on epoch=282
05/21/2022 09:20:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.81 on epoch=284
05/21/2022 09:20:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.80 on epoch=287
05/21/2022 09:20:27 - INFO - __main__ - Global step 1150 Train loss 0.80 Classification-F1 0.16105095884507648 on epoch=287
05/21/2022 09:20:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.77 on epoch=289
05/21/2022 09:20:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.83 on epoch=292
05/21/2022 09:20:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.75 on epoch=294
05/21/2022 09:20:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.76 on epoch=297
05/21/2022 09:20:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.83 on epoch=299
05/21/2022 09:20:34 - INFO - __main__ - Global step 1200 Train loss 0.79 Classification-F1 0.13035714285714284 on epoch=299
05/21/2022 09:20:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.74 on epoch=302
05/21/2022 09:20:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.78 on epoch=304
05/21/2022 09:20:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.71 on epoch=307
05/21/2022 09:20:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.81 on epoch=309
05/21/2022 09:20:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.73 on epoch=312
05/21/2022 09:20:41 - INFO - __main__ - Global step 1250 Train loss 0.76 Classification-F1 0.10892857142857143 on epoch=312
05/21/2022 09:20:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.70 on epoch=314
05/21/2022 09:20:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.75 on epoch=317
05/21/2022 09:20:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.77 on epoch=319
05/21/2022 09:20:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.81 on epoch=322
05/21/2022 09:20:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.84 on epoch=324
05/21/2022 09:20:48 - INFO - __main__ - Global step 1300 Train loss 0.77 Classification-F1 0.1247628083491461 on epoch=324
05/21/2022 09:20:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.78 on epoch=327
05/21/2022 09:20:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.72 on epoch=329
05/21/2022 09:20:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.83 on epoch=332
05/21/2022 09:20:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.82 on epoch=334
05/21/2022 09:20:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.79 on epoch=337
05/21/2022 09:20:55 - INFO - __main__ - Global step 1350 Train loss 0.79 Classification-F1 0.13275613275613274 on epoch=337
05/21/2022 09:20:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.77 on epoch=339
05/21/2022 09:20:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.83 on epoch=342
05/21/2022 09:20:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.74 on epoch=344
05/21/2022 09:21:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.73 on epoch=347
05/21/2022 09:21:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.70 on epoch=349
05/21/2022 09:21:02 - INFO - __main__ - Global step 1400 Train loss 0.75 Classification-F1 0.11657231085949563 on epoch=349
05/21/2022 09:21:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.75 on epoch=352
05/21/2022 09:21:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.75 on epoch=354
05/21/2022 09:21:06 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.83 on epoch=357
05/21/2022 09:21:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.81 on epoch=359
05/21/2022 09:21:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.77 on epoch=362
05/21/2022 09:21:09 - INFO - __main__ - Global step 1450 Train loss 0.78 Classification-F1 0.125 on epoch=362
05/21/2022 09:21:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.83 on epoch=364
05/21/2022 09:21:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.73 on epoch=367
05/21/2022 09:21:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.79 on epoch=369
05/21/2022 09:21:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.75 on epoch=372
05/21/2022 09:21:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.74 on epoch=374
05/21/2022 09:21:17 - INFO - __main__ - Global step 1500 Train loss 0.77 Classification-F1 0.14284543248737622 on epoch=374
05/21/2022 09:21:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.77 on epoch=377
05/21/2022 09:21:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.76 on epoch=379
05/21/2022 09:21:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.75 on epoch=382
05/21/2022 09:21:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.76 on epoch=384
05/21/2022 09:21:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.80 on epoch=387
05/21/2022 09:21:25 - INFO - __main__ - Global step 1550 Train loss 0.77 Classification-F1 0.12008547008547008 on epoch=387
05/21/2022 09:21:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.75 on epoch=389
05/21/2022 09:21:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.79 on epoch=392
05/21/2022 09:21:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.80 on epoch=394
05/21/2022 09:21:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.77 on epoch=397
05/21/2022 09:21:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.77 on epoch=399
05/21/2022 09:21:32 - INFO - __main__ - Global step 1600 Train loss 0.78 Classification-F1 0.1176046176046176 on epoch=399
05/21/2022 09:21:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.80 on epoch=402
05/21/2022 09:21:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.74 on epoch=404
05/21/2022 09:21:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.74 on epoch=407
05/21/2022 09:21:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.76 on epoch=409
05/21/2022 09:21:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.82 on epoch=412
05/21/2022 09:21:40 - INFO - __main__ - Global step 1650 Train loss 0.77 Classification-F1 0.09999999999999999 on epoch=412
05/21/2022 09:21:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.79 on epoch=414
05/21/2022 09:21:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.77 on epoch=417
05/21/2022 09:21:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.78 on epoch=419
05/21/2022 09:21:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.77 on epoch=422
05/21/2022 09:21:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.75 on epoch=424
05/21/2022 09:21:47 - INFO - __main__ - Global step 1700 Train loss 0.77 Classification-F1 0.14777327935222673 on epoch=424
05/21/2022 09:21:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.79 on epoch=427
05/21/2022 09:21:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.79 on epoch=429
05/21/2022 09:21:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.81 on epoch=432
05/21/2022 09:21:52 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.80 on epoch=434
05/21/2022 09:21:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.72 on epoch=437
05/21/2022 09:21:54 - INFO - __main__ - Global step 1750 Train loss 0.78 Classification-F1 0.13883847549909256 on epoch=437
05/21/2022 09:21:56 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.71 on epoch=439
05/21/2022 09:21:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.80 on epoch=442
05/21/2022 09:21:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.73 on epoch=444
05/21/2022 09:22:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.74 on epoch=447
05/21/2022 09:22:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.77 on epoch=449
05/21/2022 09:22:01 - INFO - __main__ - Global step 1800 Train loss 0.75 Classification-F1 0.10074441687344912 on epoch=449
05/21/2022 09:22:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.76 on epoch=452
05/21/2022 09:22:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.72 on epoch=454
05/21/2022 09:22:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.73 on epoch=457
05/21/2022 09:22:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.84 on epoch=459
05/21/2022 09:22:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.72 on epoch=462
05/21/2022 09:22:09 - INFO - __main__ - Global step 1850 Train loss 0.75 Classification-F1 0.13300248138957815 on epoch=462
05/21/2022 09:22:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.78 on epoch=464
05/21/2022 09:22:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.73 on epoch=467
05/21/2022 09:22:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.80 on epoch=469
05/21/2022 09:22:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.76 on epoch=472
05/21/2022 09:22:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.73 on epoch=474
05/21/2022 09:22:15 - INFO - __main__ - Global step 1900 Train loss 0.76 Classification-F1 0.09285714285714285 on epoch=474
05/21/2022 09:22:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.76 on epoch=477
05/21/2022 09:22:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.75 on epoch=479
05/21/2022 09:22:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.72 on epoch=482
05/21/2022 09:22:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.71 on epoch=484
05/21/2022 09:22:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.78 on epoch=487
05/21/2022 09:22:23 - INFO - __main__ - Global step 1950 Train loss 0.75 Classification-F1 0.1181214421252372 on epoch=487
05/21/2022 09:22:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.71 on epoch=489
05/21/2022 09:22:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.77 on epoch=492
05/21/2022 09:22:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.74 on epoch=494
05/21/2022 09:22:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.72 on epoch=497
05/21/2022 09:22:30 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.73 on epoch=499
05/21/2022 09:22:30 - INFO - __main__ - Global step 2000 Train loss 0.74 Classification-F1 0.11662763466042156 on epoch=499
05/21/2022 09:22:32 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.78 on epoch=502
05/21/2022 09:22:33 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.69 on epoch=504
05/21/2022 09:22:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.75 on epoch=507
05/21/2022 09:22:36 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.74 on epoch=509
05/21/2022 09:22:37 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.79 on epoch=512
05/21/2022 09:22:38 - INFO - __main__ - Global step 2050 Train loss 0.75 Classification-F1 0.121895773234667 on epoch=512
05/21/2022 09:22:39 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.80 on epoch=514
05/21/2022 09:22:40 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.76 on epoch=517
05/21/2022 09:22:42 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.72 on epoch=519
05/21/2022 09:22:43 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.71 on epoch=522
05/21/2022 09:22:45 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.76 on epoch=524
05/21/2022 09:22:45 - INFO - __main__ - Global step 2100 Train loss 0.75 Classification-F1 0.10277777777777777 on epoch=524
05/21/2022 09:22:47 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.73 on epoch=527
05/21/2022 09:22:48 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.70 on epoch=529
05/21/2022 09:22:49 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.80 on epoch=532
05/21/2022 09:22:51 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.71 on epoch=534
05/21/2022 09:22:52 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.75 on epoch=537
05/21/2022 09:22:53 - INFO - __main__ - Global step 2150 Train loss 0.74 Classification-F1 0.12590299277605782 on epoch=537
05/21/2022 09:22:54 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.73 on epoch=539
05/21/2022 09:22:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.77 on epoch=542
05/21/2022 09:22:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.73 on epoch=544
05/21/2022 09:22:58 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.80 on epoch=547
05/21/2022 09:23:00 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.75 on epoch=549
05/21/2022 09:23:00 - INFO - __main__ - Global step 2200 Train loss 0.76 Classification-F1 0.11177278973889145 on epoch=549
05/21/2022 09:23:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.80 on epoch=552
05/21/2022 09:23:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.74 on epoch=554
05/21/2022 09:23:04 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.70 on epoch=557
05/21/2022 09:23:05 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.77 on epoch=559
05/21/2022 09:23:07 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.75 on epoch=562
05/21/2022 09:23:07 - INFO - __main__ - Global step 2250 Train loss 0.75 Classification-F1 0.10547504025764896 on epoch=562
05/21/2022 09:23:09 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.74 on epoch=564
05/21/2022 09:23:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.73 on epoch=567
05/21/2022 09:23:12 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.78 on epoch=569
05/21/2022 09:23:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.80 on epoch=572
05/21/2022 09:23:14 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.76 on epoch=574
05/21/2022 09:23:14 - INFO - __main__ - Global step 2300 Train loss 0.76 Classification-F1 0.18679201886033003 on epoch=574
05/21/2022 09:23:16 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.81 on epoch=577
05/21/2022 09:23:17 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.69 on epoch=579
05/21/2022 09:23:19 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.72 on epoch=582
05/21/2022 09:23:20 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.77 on epoch=584
05/21/2022 09:23:22 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.78 on epoch=587
05/21/2022 09:23:23 - INFO - __main__ - Global step 2350 Train loss 0.76 Classification-F1 0.13022941970310392 on epoch=587
05/21/2022 09:23:24 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.72 on epoch=589
05/21/2022 09:23:25 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.73 on epoch=592
05/21/2022 09:23:26 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.74 on epoch=594
05/21/2022 09:23:28 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.76 on epoch=597
05/21/2022 09:23:29 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.71 on epoch=599
05/21/2022 09:23:30 - INFO - __main__ - Global step 2400 Train loss 0.73 Classification-F1 0.18869731800766282 on epoch=599
05/21/2022 09:23:31 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.76 on epoch=602
05/21/2022 09:23:32 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.75 on epoch=604
05/21/2022 09:23:34 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.72 on epoch=607
05/21/2022 09:23:35 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.74 on epoch=609
05/21/2022 09:23:36 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.69 on epoch=612
05/21/2022 09:23:37 - INFO - __main__ - Global step 2450 Train loss 0.73 Classification-F1 0.17978504820610086 on epoch=612
05/21/2022 09:23:38 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.75 on epoch=614
05/21/2022 09:23:40 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.69 on epoch=617
05/21/2022 09:23:41 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.78 on epoch=619
05/21/2022 09:23:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.78 on epoch=622
05/21/2022 09:23:43 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.73 on epoch=624
05/21/2022 09:23:44 - INFO - __main__ - Global step 2500 Train loss 0.74 Classification-F1 0.1911402329749104 on epoch=624
05/21/2022 09:23:45 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.70 on epoch=627
05/21/2022 09:23:46 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.75 on epoch=629
05/21/2022 09:23:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.72 on epoch=632
05/21/2022 09:23:49 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.67 on epoch=634
05/21/2022 09:23:51 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.75 on epoch=637
05/21/2022 09:23:51 - INFO - __main__ - Global step 2550 Train loss 0.72 Classification-F1 0.1911402329749104 on epoch=637
05/21/2022 09:23:53 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.70 on epoch=639
05/21/2022 09:23:54 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.73 on epoch=642
05/21/2022 09:23:55 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.73 on epoch=644
05/21/2022 09:23:56 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.73 on epoch=647
05/21/2022 09:23:57 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.70 on epoch=649
05/21/2022 09:23:58 - INFO - __main__ - Global step 2600 Train loss 0.72 Classification-F1 0.15620915032679739 on epoch=649
05/21/2022 09:24:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.74 on epoch=652
05/21/2022 09:24:01 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.72 on epoch=654
05/21/2022 09:24:03 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.68 on epoch=657
05/21/2022 09:24:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.68 on epoch=659
05/21/2022 09:24:05 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.74 on epoch=662
05/21/2022 09:24:06 - INFO - __main__ - Global step 2650 Train loss 0.71 Classification-F1 0.24071207430340558 on epoch=662
05/21/2022 09:24:06 - INFO - __main__ - Saving model with best Classification-F1: 0.1974891774891775 -> 0.24071207430340558 on epoch=662, global_step=2650
05/21/2022 09:24:08 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.77 on epoch=664
05/21/2022 09:24:09 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.81 on epoch=667
05/21/2022 09:24:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.76 on epoch=669
05/21/2022 09:24:11 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.74 on epoch=672
05/21/2022 09:24:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.73 on epoch=674
05/21/2022 09:24:13 - INFO - __main__ - Global step 2700 Train loss 0.76 Classification-F1 0.14734961781322697 on epoch=674
05/21/2022 09:24:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.82 on epoch=677
05/21/2022 09:24:16 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.70 on epoch=679
05/21/2022 09:24:18 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.75 on epoch=682
05/21/2022 09:24:19 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.71 on epoch=684
05/21/2022 09:24:21 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.79 on epoch=687
05/21/2022 09:24:21 - INFO - __main__ - Global step 2750 Train loss 0.75 Classification-F1 0.15582419408441345 on epoch=687
05/21/2022 09:24:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.72 on epoch=689
05/21/2022 09:24:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.69 on epoch=692
05/21/2022 09:24:25 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.70 on epoch=694
05/21/2022 09:24:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.73 on epoch=697
05/21/2022 09:24:28 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.78 on epoch=699
05/21/2022 09:24:29 - INFO - __main__ - Global step 2800 Train loss 0.73 Classification-F1 0.21250000000000002 on epoch=699
05/21/2022 09:24:30 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.75 on epoch=702
05/21/2022 09:24:32 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.77 on epoch=704
05/21/2022 09:24:33 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.72 on epoch=707
05/21/2022 09:24:35 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.75 on epoch=709
05/21/2022 09:24:36 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.75 on epoch=712
05/21/2022 09:24:37 - INFO - __main__ - Global step 2850 Train loss 0.75 Classification-F1 0.20093703148425784 on epoch=712
05/21/2022 09:24:38 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.71 on epoch=714
05/21/2022 09:24:39 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.74 on epoch=717
05/21/2022 09:24:41 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.73 on epoch=719
05/21/2022 09:24:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.71 on epoch=722
05/21/2022 09:24:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.73 on epoch=724
05/21/2022 09:24:44 - INFO - __main__ - Global step 2900 Train loss 0.72 Classification-F1 0.24104774535809018 on epoch=724
05/21/2022 09:24:44 - INFO - __main__ - Saving model with best Classification-F1: 0.24071207430340558 -> 0.24104774535809018 on epoch=724, global_step=2900
05/21/2022 09:24:45 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.70 on epoch=727
05/21/2022 09:24:47 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.73 on epoch=729
05/21/2022 09:24:48 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.76 on epoch=732
05/21/2022 09:24:49 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.70 on epoch=734
05/21/2022 09:24:50 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.69 on epoch=737
05/21/2022 09:24:51 - INFO - __main__ - Global step 2950 Train loss 0.72 Classification-F1 0.19004879301489472 on epoch=737
05/21/2022 09:24:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.74 on epoch=739
05/21/2022 09:24:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.73 on epoch=742
05/21/2022 09:24:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.69 on epoch=744
05/21/2022 09:24:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.77 on epoch=747
05/21/2022 09:24:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.69 on epoch=749
05/21/2022 09:25:00 - INFO - __main__ - Global step 3000 Train loss 0.73 Classification-F1 0.1935185185185185 on epoch=749
05/21/2022 09:25:00 - INFO - __main__ - save last model!
05/21/2022 09:25:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 09:25:00 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 09:25:00 - INFO - __main__ - Printing 3 examples
05/21/2022 09:25:00 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 09:25:00 - INFO - __main__ - ['others']
05/21/2022 09:25:00 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 09:25:00 - INFO - __main__ - ['others']
05/21/2022 09:25:00 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 09:25:00 - INFO - __main__ - ['others']
05/21/2022 09:25:00 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:25:00 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:25:00 - INFO - __main__ - Printing 3 examples
05/21/2022 09:25:00 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 09:25:00 - INFO - __main__ - ['others']
05/21/2022 09:25:00 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 09:25:00 - INFO - __main__ - ['others']
05/21/2022 09:25:00 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 09:25:00 - INFO - __main__ - ['others']
05/21/2022 09:25:00 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:25:00 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:25:00 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 09:25:00 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:25:00 - INFO - __main__ - Printing 3 examples
05/21/2022 09:25:00 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 09:25:00 - INFO - __main__ - ['others']
05/21/2022 09:25:00 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 09:25:00 - INFO - __main__ - ['others']
05/21/2022 09:25:00 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 09:25:00 - INFO - __main__ - ['others']
05/21/2022 09:25:00 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:25:00 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:25:00 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 09:25:03 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:25:08 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 09:25:08 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 09:25:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 09:25:08 - INFO - __main__ - Starting training!
05/21/2022 09:25:51 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_100_0.5_8_predictions.txt
05/21/2022 09:25:51 - INFO - __main__ - Classification-F1 on test data: 0.1098
05/21/2022 09:25:51 - INFO - __main__ - prefix=emo_16_100, lr=0.5, bsz=8, dev_performance=0.24104774535809018, test_performance=0.10980573185676215
05/21/2022 09:25:51 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.4, bsz=8 ...
05/21/2022 09:25:52 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:25:52 - INFO - __main__ - Printing 3 examples
05/21/2022 09:25:52 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 09:25:52 - INFO - __main__ - ['others']
05/21/2022 09:25:52 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 09:25:52 - INFO - __main__ - ['others']
05/21/2022 09:25:52 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 09:25:52 - INFO - __main__ - ['others']
05/21/2022 09:25:52 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:25:52 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:25:52 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 09:25:52 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:25:52 - INFO - __main__ - Printing 3 examples
05/21/2022 09:25:52 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 09:25:52 - INFO - __main__ - ['others']
05/21/2022 09:25:52 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 09:25:52 - INFO - __main__ - ['others']
05/21/2022 09:25:52 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 09:25:52 - INFO - __main__ - ['others']
05/21/2022 09:25:52 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:25:52 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:25:53 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 09:25:59 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 09:26:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 09:26:00 - INFO - __main__ - Starting training!
05/21/2022 09:26:01 - INFO - __main__ - Step 10 Global step 10 Train loss 4.05 on epoch=2
05/21/2022 09:26:03 - INFO - __main__ - Step 20 Global step 20 Train loss 3.59 on epoch=4
05/21/2022 09:26:04 - INFO - __main__ - Step 30 Global step 30 Train loss 2.90 on epoch=7
05/21/2022 09:26:05 - INFO - __main__ - Step 40 Global step 40 Train loss 2.42 on epoch=9
05/21/2022 09:26:07 - INFO - __main__ - Step 50 Global step 50 Train loss 2.05 on epoch=12
05/21/2022 09:26:07 - INFO - __main__ - Global step 50 Train loss 3.00 Classification-F1 0.1 on epoch=12
05/21/2022 09:26:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/21/2022 09:26:08 - INFO - __main__ - Step 60 Global step 60 Train loss 1.66 on epoch=14
05/21/2022 09:26:10 - INFO - __main__ - Step 70 Global step 70 Train loss 1.55 on epoch=17
05/21/2022 09:26:12 - INFO - __main__ - Step 80 Global step 80 Train loss 1.33 on epoch=19
05/21/2022 09:26:13 - INFO - __main__ - Step 90 Global step 90 Train loss 1.28 on epoch=22
05/21/2022 09:26:14 - INFO - __main__ - Step 100 Global step 100 Train loss 1.02 on epoch=24
05/21/2022 09:26:15 - INFO - __main__ - Global step 100 Train loss 1.37 Classification-F1 0.1609907120743034 on epoch=24
05/21/2022 09:26:15 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1609907120743034 on epoch=24, global_step=100
05/21/2022 09:26:16 - INFO - __main__ - Step 110 Global step 110 Train loss 1.11 on epoch=27
05/21/2022 09:26:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.92 on epoch=29
05/21/2022 09:26:19 - INFO - __main__ - Step 130 Global step 130 Train loss 0.97 on epoch=32
05/21/2022 09:26:21 - INFO - __main__ - Step 140 Global step 140 Train loss 0.97 on epoch=34
05/21/2022 09:26:22 - INFO - __main__ - Step 150 Global step 150 Train loss 1.06 on epoch=37
05/21/2022 09:26:23 - INFO - __main__ - Global step 150 Train loss 1.01 Classification-F1 0.1 on epoch=37
05/21/2022 09:26:24 - INFO - __main__ - Step 160 Global step 160 Train loss 0.96 on epoch=39
05/21/2022 09:26:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.98 on epoch=42
05/21/2022 09:26:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.96 on epoch=44
05/21/2022 09:26:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.88 on epoch=47
05/21/2022 09:26:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.84 on epoch=49
05/21/2022 09:26:31 - INFO - __main__ - Global step 200 Train loss 0.92 Classification-F1 0.17890810170221935 on epoch=49
05/21/2022 09:26:31 - INFO - __main__ - Saving model with best Classification-F1: 0.1609907120743034 -> 0.17890810170221935 on epoch=49, global_step=200
05/21/2022 09:26:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.97 on epoch=52
05/21/2022 09:26:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.94 on epoch=54
05/21/2022 09:26:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.90 on epoch=57
05/21/2022 09:26:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.86 on epoch=59
05/21/2022 09:26:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.85 on epoch=62
05/21/2022 09:26:39 - INFO - __main__ - Global step 250 Train loss 0.90 Classification-F1 0.12407862407862408 on epoch=62
05/21/2022 09:26:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.97 on epoch=64
05/21/2022 09:26:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.98 on epoch=67
05/21/2022 09:26:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.89 on epoch=69
05/21/2022 09:26:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.83 on epoch=72
05/21/2022 09:26:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.92 on epoch=74
05/21/2022 09:26:47 - INFO - __main__ - Global step 300 Train loss 0.92 Classification-F1 0.11762954139368673 on epoch=74
05/21/2022 09:26:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.93 on epoch=77
05/21/2022 09:26:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.93 on epoch=79
05/21/2022 09:26:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.90 on epoch=82
05/21/2022 09:26:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.89 on epoch=84
05/21/2022 09:26:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.91 on epoch=87
05/21/2022 09:26:55 - INFO - __main__ - Global step 350 Train loss 0.91 Classification-F1 0.13067758749069247 on epoch=87
05/21/2022 09:26:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.86 on epoch=89
05/21/2022 09:26:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.82 on epoch=92
05/21/2022 09:26:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.95 on epoch=94
05/21/2022 09:27:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.91 on epoch=97
05/21/2022 09:27:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.90 on epoch=99
05/21/2022 09:27:03 - INFO - __main__ - Global step 400 Train loss 0.89 Classification-F1 0.19791666666666669 on epoch=99
05/21/2022 09:27:03 - INFO - __main__ - Saving model with best Classification-F1: 0.17890810170221935 -> 0.19791666666666669 on epoch=99, global_step=400
05/21/2022 09:27:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.94 on epoch=102
05/21/2022 09:27:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.89 on epoch=104
05/21/2022 09:27:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.89 on epoch=107
05/21/2022 09:27:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.89 on epoch=109
05/21/2022 09:27:10 - INFO - __main__ - Step 450 Global step 450 Train loss 1.03 on epoch=112
05/21/2022 09:27:10 - INFO - __main__ - Global step 450 Train loss 0.93 Classification-F1 0.10126582278481013 on epoch=112
05/21/2022 09:27:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.90 on epoch=114
05/21/2022 09:27:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.82 on epoch=117
05/21/2022 09:27:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.98 on epoch=119
05/21/2022 09:27:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.89 on epoch=122
05/21/2022 09:27:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.82 on epoch=124
05/21/2022 09:27:18 - INFO - __main__ - Global step 500 Train loss 0.88 Classification-F1 0.19957983193277312 on epoch=124
05/21/2022 09:27:18 - INFO - __main__ - Saving model with best Classification-F1: 0.19791666666666669 -> 0.19957983193277312 on epoch=124, global_step=500
05/21/2022 09:27:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.91 on epoch=127
05/21/2022 09:27:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.84 on epoch=129
05/21/2022 09:27:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.94 on epoch=132
05/21/2022 09:27:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.79 on epoch=134
05/21/2022 09:27:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.86 on epoch=137
05/21/2022 09:27:25 - INFO - __main__ - Global step 550 Train loss 0.87 Classification-F1 0.12499999999999999 on epoch=137
05/21/2022 09:27:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.80 on epoch=139
05/21/2022 09:27:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.83 on epoch=142
05/21/2022 09:27:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.83 on epoch=144
05/21/2022 09:27:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.81 on epoch=147
05/21/2022 09:27:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.89 on epoch=149
05/21/2022 09:27:33 - INFO - __main__ - Global step 600 Train loss 0.83 Classification-F1 0.10389610389610389 on epoch=149
05/21/2022 09:27:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.81 on epoch=152
05/21/2022 09:27:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.75 on epoch=154
05/21/2022 09:27:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.83 on epoch=157
05/21/2022 09:27:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.82 on epoch=159
05/21/2022 09:27:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.83 on epoch=162
05/21/2022 09:27:40 - INFO - __main__ - Global step 650 Train loss 0.81 Classification-F1 0.09615384615384615 on epoch=162
05/21/2022 09:27:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.85 on epoch=164
05/21/2022 09:27:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.83 on epoch=167
05/21/2022 09:27:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.77 on epoch=169
05/21/2022 09:27:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.81 on epoch=172
05/21/2022 09:27:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.86 on epoch=174
05/21/2022 09:27:47 - INFO - __main__ - Global step 700 Train loss 0.83 Classification-F1 0.17573497147871872 on epoch=174
05/21/2022 09:27:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.88 on epoch=177
05/21/2022 09:27:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.78 on epoch=179
05/21/2022 09:27:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.86 on epoch=182
05/21/2022 09:27:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.88 on epoch=184
05/21/2022 09:27:55 - INFO - __main__ - Step 750 Global step 750 Train loss 0.87 on epoch=187
05/21/2022 09:27:56 - INFO - __main__ - Global step 750 Train loss 0.86 Classification-F1 0.1784090909090909 on epoch=187
05/21/2022 09:27:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.89 on epoch=189
05/21/2022 09:27:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.90 on epoch=192
05/21/2022 09:28:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.85 on epoch=194
05/21/2022 09:28:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.88 on epoch=197
05/21/2022 09:28:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.87 on epoch=199
05/21/2022 09:28:04 - INFO - __main__ - Global step 800 Train loss 0.88 Classification-F1 0.1412763767370046 on epoch=199
05/21/2022 09:28:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.84 on epoch=202
05/21/2022 09:28:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.86 on epoch=204
05/21/2022 09:28:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.80 on epoch=207
05/21/2022 09:28:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.81 on epoch=209
05/21/2022 09:28:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.79 on epoch=212
05/21/2022 09:28:11 - INFO - __main__ - Global step 850 Train loss 0.82 Classification-F1 0.2048611111111111 on epoch=212
05/21/2022 09:28:11 - INFO - __main__ - Saving model with best Classification-F1: 0.19957983193277312 -> 0.2048611111111111 on epoch=212, global_step=850
05/21/2022 09:28:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.85 on epoch=214
05/21/2022 09:28:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.79 on epoch=217
05/21/2022 09:28:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.78 on epoch=219
05/21/2022 09:28:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.79 on epoch=222
05/21/2022 09:28:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.81 on epoch=224
05/21/2022 09:28:19 - INFO - __main__ - Global step 900 Train loss 0.80 Classification-F1 0.19068450849202268 on epoch=224
05/21/2022 09:28:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.80 on epoch=227
05/21/2022 09:28:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.80 on epoch=229
05/21/2022 09:28:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.73 on epoch=232
05/21/2022 09:28:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.79 on epoch=234
05/21/2022 09:28:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.87 on epoch=237
05/21/2022 09:28:27 - INFO - __main__ - Global step 950 Train loss 0.80 Classification-F1 0.18218623481781374 on epoch=237
05/21/2022 09:28:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.82 on epoch=239
05/21/2022 09:28:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.78 on epoch=242
05/21/2022 09:28:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.76 on epoch=244
05/21/2022 09:28:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.85 on epoch=247
05/21/2022 09:28:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.79 on epoch=249
05/21/2022 09:28:34 - INFO - __main__ - Global step 1000 Train loss 0.80 Classification-F1 0.1570048309178744 on epoch=249
05/21/2022 09:28:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.77 on epoch=252
05/21/2022 09:28:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.79 on epoch=254
05/21/2022 09:28:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.78 on epoch=257
05/21/2022 09:28:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.83 on epoch=259
05/21/2022 09:28:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.81 on epoch=262
05/21/2022 09:28:42 - INFO - __main__ - Global step 1050 Train loss 0.79 Classification-F1 0.15 on epoch=262
05/21/2022 09:28:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.84 on epoch=264
05/21/2022 09:28:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.80 on epoch=267
05/21/2022 09:28:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.86 on epoch=269
05/21/2022 09:28:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.82 on epoch=272
05/21/2022 09:28:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.79 on epoch=274
05/21/2022 09:28:50 - INFO - __main__ - Global step 1100 Train loss 0.82 Classification-F1 0.2065573770491803 on epoch=274
05/21/2022 09:28:50 - INFO - __main__ - Saving model with best Classification-F1: 0.2048611111111111 -> 0.2065573770491803 on epoch=274, global_step=1100
05/21/2022 09:28:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.81 on epoch=277
05/21/2022 09:28:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.77 on epoch=279
05/21/2022 09:28:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.77 on epoch=282
05/21/2022 09:28:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.80 on epoch=284
05/21/2022 09:28:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.79 on epoch=287
05/21/2022 09:28:59 - INFO - __main__ - Global step 1150 Train loss 0.79 Classification-F1 0.09722222222222222 on epoch=287
05/21/2022 09:29:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.82 on epoch=289
05/21/2022 09:29:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.72 on epoch=292
05/21/2022 09:29:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.82 on epoch=294
05/21/2022 09:29:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.80 on epoch=297
05/21/2022 09:29:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.77 on epoch=299
05/21/2022 09:29:08 - INFO - __main__ - Global step 1200 Train loss 0.78 Classification-F1 0.16666666666666666 on epoch=299
05/21/2022 09:29:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.79 on epoch=302
05/21/2022 09:29:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.84 on epoch=304
05/21/2022 09:29:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.82 on epoch=307
05/21/2022 09:29:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.86 on epoch=309
05/21/2022 09:29:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.80 on epoch=312
05/21/2022 09:29:15 - INFO - __main__ - Global step 1250 Train loss 0.82 Classification-F1 0.10206653225806452 on epoch=312
05/21/2022 09:29:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.75 on epoch=314
05/21/2022 09:29:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.75 on epoch=317
05/21/2022 09:29:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.81 on epoch=319
05/21/2022 09:29:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.74 on epoch=322
05/21/2022 09:29:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.86 on epoch=324
05/21/2022 09:29:23 - INFO - __main__ - Global step 1300 Train loss 0.78 Classification-F1 0.13300248138957815 on epoch=324
05/21/2022 09:29:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.81 on epoch=327
05/21/2022 09:29:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.80 on epoch=329
05/21/2022 09:29:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.80 on epoch=332
05/21/2022 09:29:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.78 on epoch=334
05/21/2022 09:29:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.83 on epoch=337
05/21/2022 09:29:30 - INFO - __main__ - Global step 1350 Train loss 0.80 Classification-F1 0.11078022632519356 on epoch=337
05/21/2022 09:29:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.76 on epoch=339
05/21/2022 09:29:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.88 on epoch=342
05/21/2022 09:29:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.76 on epoch=344
05/21/2022 09:29:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.80 on epoch=347
05/21/2022 09:29:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.73 on epoch=349
05/21/2022 09:29:38 - INFO - __main__ - Global step 1400 Train loss 0.79 Classification-F1 0.09615384615384615 on epoch=349
05/21/2022 09:29:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.80 on epoch=352
05/21/2022 09:29:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.73 on epoch=354
05/21/2022 09:29:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.76 on epoch=357
05/21/2022 09:29:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.83 on epoch=359
05/21/2022 09:29:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.84 on epoch=362
05/21/2022 09:29:45 - INFO - __main__ - Global step 1450 Train loss 0.79 Classification-F1 0.1542857142857143 on epoch=362
05/21/2022 09:29:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.82 on epoch=364
05/21/2022 09:29:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.83 on epoch=367
05/21/2022 09:29:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.77 on epoch=369
05/21/2022 09:29:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.81 on epoch=372
05/21/2022 09:29:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.82 on epoch=374
05/21/2022 09:29:52 - INFO - __main__ - Global step 1500 Train loss 0.81 Classification-F1 0.10609243697478991 on epoch=374
05/21/2022 09:29:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.78 on epoch=377
05/21/2022 09:29:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.79 on epoch=379
05/21/2022 09:29:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.79 on epoch=382
05/21/2022 09:29:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.77 on epoch=384
05/21/2022 09:29:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.81 on epoch=387
05/21/2022 09:30:00 - INFO - __main__ - Global step 1550 Train loss 0.79 Classification-F1 0.1111111111111111 on epoch=387
05/21/2022 09:30:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.78 on epoch=389
05/21/2022 09:30:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.82 on epoch=392
05/21/2022 09:30:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.78 on epoch=394
05/21/2022 09:30:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.78 on epoch=397
05/21/2022 09:30:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.73 on epoch=399
05/21/2022 09:30:07 - INFO - __main__ - Global step 1600 Train loss 0.78 Classification-F1 0.12368421052631579 on epoch=399
05/21/2022 09:30:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.77 on epoch=402
05/21/2022 09:30:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.75 on epoch=404
05/21/2022 09:30:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.75 on epoch=407
05/21/2022 09:30:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.86 on epoch=409
05/21/2022 09:30:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.81 on epoch=412
05/21/2022 09:30:14 - INFO - __main__ - Global step 1650 Train loss 0.79 Classification-F1 0.11666666666666667 on epoch=412
05/21/2022 09:30:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.75 on epoch=414
05/21/2022 09:30:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.74 on epoch=417
05/21/2022 09:30:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.84 on epoch=419
05/21/2022 09:30:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.76 on epoch=422
05/21/2022 09:30:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.79 on epoch=424
05/21/2022 09:30:22 - INFO - __main__ - Global step 1700 Train loss 0.78 Classification-F1 0.12368421052631579 on epoch=424
05/21/2022 09:30:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.81 on epoch=427
05/21/2022 09:30:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.74 on epoch=429
05/21/2022 09:30:27 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.77 on epoch=432
05/21/2022 09:30:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.76 on epoch=434
05/21/2022 09:30:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.75 on epoch=437
05/21/2022 09:30:30 - INFO - __main__ - Global step 1750 Train loss 0.76 Classification-F1 0.11208791208791208 on epoch=437
05/21/2022 09:30:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.79 on epoch=439
05/21/2022 09:30:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.79 on epoch=442
05/21/2022 09:30:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.79 on epoch=444
05/21/2022 09:30:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.81 on epoch=447
05/21/2022 09:30:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.85 on epoch=449
05/21/2022 09:30:38 - INFO - __main__ - Global step 1800 Train loss 0.80 Classification-F1 0.1111111111111111 on epoch=449
05/21/2022 09:30:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.82 on epoch=452
05/21/2022 09:30:41 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.77 on epoch=454
05/21/2022 09:30:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.82 on epoch=457
05/21/2022 09:30:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.80 on epoch=459
05/21/2022 09:30:45 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.82 on epoch=462
05/21/2022 09:30:46 - INFO - __main__ - Global step 1850 Train loss 0.80 Classification-F1 0.12499999999999999 on epoch=462
05/21/2022 09:30:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.84 on epoch=464
05/21/2022 09:30:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.77 on epoch=467
05/21/2022 09:30:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.80 on epoch=469
05/21/2022 09:30:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.84 on epoch=472
05/21/2022 09:30:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.81 on epoch=474
05/21/2022 09:30:53 - INFO - __main__ - Global step 1900 Train loss 0.81 Classification-F1 0.15859154929577468 on epoch=474
05/21/2022 09:30:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.75 on epoch=477
05/21/2022 09:30:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.80 on epoch=479
05/21/2022 09:30:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.75 on epoch=482
05/21/2022 09:31:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.77 on epoch=484
05/21/2022 09:31:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.81 on epoch=487
05/21/2022 09:31:02 - INFO - __main__ - Global step 1950 Train loss 0.78 Classification-F1 0.177928916191312 on epoch=487
05/21/2022 09:31:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.72 on epoch=489
05/21/2022 09:31:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.73 on epoch=492
05/21/2022 09:31:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.77 on epoch=494
05/21/2022 09:31:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.72 on epoch=497
05/21/2022 09:31:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.80 on epoch=499
05/21/2022 09:31:09 - INFO - __main__ - Global step 2000 Train loss 0.75 Classification-F1 0.13846153846153847 on epoch=499
05/21/2022 09:31:11 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.74 on epoch=502
05/21/2022 09:31:12 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.80 on epoch=504
05/21/2022 09:31:14 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.80 on epoch=507
05/21/2022 09:31:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.73 on epoch=509
05/21/2022 09:31:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.77 on epoch=512
05/21/2022 09:31:17 - INFO - __main__ - Global step 2050 Train loss 0.77 Classification-F1 0.12198332602018429 on epoch=512
05/21/2022 09:31:19 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.81 on epoch=514
05/21/2022 09:31:20 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.76 on epoch=517
05/21/2022 09:31:21 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.77 on epoch=519
05/21/2022 09:31:23 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.80 on epoch=522
05/21/2022 09:31:24 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.77 on epoch=524
05/21/2022 09:31:25 - INFO - __main__ - Global step 2100 Train loss 0.78 Classification-F1 0.10843920145190562 on epoch=524
05/21/2022 09:31:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.75 on epoch=527
05/21/2022 09:31:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.70 on epoch=529
05/21/2022 09:31:29 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.77 on epoch=532
05/21/2022 09:31:31 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.84 on epoch=534
05/21/2022 09:31:32 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.80 on epoch=537
05/21/2022 09:31:33 - INFO - __main__ - Global step 2150 Train loss 0.77 Classification-F1 0.1111111111111111 on epoch=537
05/21/2022 09:31:34 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.74 on epoch=539
05/21/2022 09:31:35 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.82 on epoch=542
05/21/2022 09:31:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.74 on epoch=544
05/21/2022 09:31:38 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.79 on epoch=547
05/21/2022 09:31:40 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.80 on epoch=549
05/21/2022 09:31:40 - INFO - __main__ - Global step 2200 Train loss 0.78 Classification-F1 0.17171945701357466 on epoch=549
05/21/2022 09:31:42 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.78 on epoch=552
05/21/2022 09:31:43 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.71 on epoch=554
05/21/2022 09:31:44 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.78 on epoch=557
05/21/2022 09:31:46 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.77 on epoch=559
05/21/2022 09:31:47 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.78 on epoch=562
05/21/2022 09:31:48 - INFO - __main__ - Global step 2250 Train loss 0.76 Classification-F1 0.11657231085949563 on epoch=562
05/21/2022 09:31:49 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.78 on epoch=564
05/21/2022 09:31:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.78 on epoch=567
05/21/2022 09:31:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.76 on epoch=569
05/21/2022 09:31:53 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.76 on epoch=572
05/21/2022 09:31:54 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.81 on epoch=574
05/21/2022 09:31:55 - INFO - __main__ - Global step 2300 Train loss 0.78 Classification-F1 0.11053864168618267 on epoch=574
05/21/2022 09:31:56 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.80 on epoch=577
05/21/2022 09:31:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.75 on epoch=579
05/21/2022 09:31:59 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.74 on epoch=582
05/21/2022 09:32:00 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.78 on epoch=584
05/21/2022 09:32:02 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.83 on epoch=587
05/21/2022 09:32:02 - INFO - __main__ - Global step 2350 Train loss 0.78 Classification-F1 0.12482435597189696 on epoch=587
05/21/2022 09:32:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.76 on epoch=589
05/21/2022 09:32:05 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.83 on epoch=592
05/21/2022 09:32:06 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.77 on epoch=594
05/21/2022 09:32:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.75 on epoch=597
05/21/2022 09:32:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.79 on epoch=599
05/21/2022 09:32:10 - INFO - __main__ - Global step 2400 Train loss 0.78 Classification-F1 0.12169312169312169 on epoch=599
05/21/2022 09:32:11 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.77 on epoch=602
05/21/2022 09:32:13 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.77 on epoch=604
05/21/2022 09:32:14 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.75 on epoch=607
05/21/2022 09:32:16 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.75 on epoch=609
05/21/2022 09:32:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.74 on epoch=612
05/21/2022 09:32:18 - INFO - __main__ - Global step 2450 Train loss 0.75 Classification-F1 0.11714285714285715 on epoch=612
05/21/2022 09:32:19 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.76 on epoch=614
05/21/2022 09:32:21 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.74 on epoch=617
05/21/2022 09:32:22 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.77 on epoch=619
05/21/2022 09:32:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.78 on epoch=622
05/21/2022 09:32:25 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.72 on epoch=624
05/21/2022 09:32:26 - INFO - __main__ - Global step 2500 Train loss 0.75 Classification-F1 0.09493670886075949 on epoch=624
05/21/2022 09:32:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.75 on epoch=627
05/21/2022 09:32:29 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.79 on epoch=629
05/21/2022 09:32:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.83 on epoch=632
05/21/2022 09:32:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.79 on epoch=634
05/21/2022 09:32:33 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.75 on epoch=637
05/21/2022 09:32:34 - INFO - __main__ - Global step 2550 Train loss 0.78 Classification-F1 0.1313186813186813 on epoch=637
05/21/2022 09:32:35 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.78 on epoch=639
05/21/2022 09:32:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.74 on epoch=642
05/21/2022 09:32:38 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.77 on epoch=644
05/21/2022 09:32:39 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.73 on epoch=647
05/21/2022 09:32:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.69 on epoch=649
05/21/2022 09:32:41 - INFO - __main__ - Global step 2600 Train loss 0.74 Classification-F1 0.10256410256410256 on epoch=649
05/21/2022 09:32:42 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.77 on epoch=652
05/21/2022 09:32:44 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.70 on epoch=654
05/21/2022 09:32:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.75 on epoch=657
05/21/2022 09:32:47 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.69 on epoch=659
05/21/2022 09:32:48 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.82 on epoch=662
05/21/2022 09:32:49 - INFO - __main__ - Global step 2650 Train loss 0.75 Classification-F1 0.09090909090909091 on epoch=662
05/21/2022 09:32:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.77 on epoch=664
05/21/2022 09:32:52 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.76 on epoch=667
05/21/2022 09:32:53 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.74 on epoch=669
05/21/2022 09:32:55 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.76 on epoch=672
05/21/2022 09:32:56 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.69 on epoch=674
05/21/2022 09:32:57 - INFO - __main__ - Global step 2700 Train loss 0.75 Classification-F1 0.1238095238095238 on epoch=674
05/21/2022 09:32:58 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.73 on epoch=677
05/21/2022 09:32:59 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.76 on epoch=679
05/21/2022 09:33:01 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.74 on epoch=682
05/21/2022 09:33:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.78 on epoch=684
05/21/2022 09:33:03 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.74 on epoch=687
05/21/2022 09:33:04 - INFO - __main__ - Global step 2750 Train loss 0.75 Classification-F1 0.109375 on epoch=687
05/21/2022 09:33:05 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.68 on epoch=689
05/21/2022 09:33:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.74 on epoch=692
05/21/2022 09:33:09 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.74 on epoch=694
05/21/2022 09:33:10 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.77 on epoch=697
05/21/2022 09:33:11 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.75 on epoch=699
05/21/2022 09:33:12 - INFO - __main__ - Global step 2800 Train loss 0.74 Classification-F1 0.09589041095890412 on epoch=699
05/21/2022 09:33:13 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.78 on epoch=702
05/21/2022 09:33:14 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.69 on epoch=704
05/21/2022 09:33:16 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.72 on epoch=707
05/21/2022 09:33:17 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.75 on epoch=709
05/21/2022 09:33:18 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.73 on epoch=712
05/21/2022 09:33:19 - INFO - __main__ - Global step 2850 Train loss 0.73 Classification-F1 0.14583333333333331 on epoch=712
05/21/2022 09:33:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.73 on epoch=714
05/21/2022 09:33:22 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.83 on epoch=717
05/21/2022 09:33:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.75 on epoch=719
05/21/2022 09:33:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.72 on epoch=722
05/21/2022 09:33:26 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.73 on epoch=724
05/21/2022 09:33:27 - INFO - __main__ - Global step 2900 Train loss 0.75 Classification-F1 0.10494505494505495 on epoch=724
05/21/2022 09:33:28 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.75 on epoch=727
05/21/2022 09:33:30 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.73 on epoch=729
05/21/2022 09:33:31 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.76 on epoch=732
05/21/2022 09:33:33 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.75 on epoch=734
05/21/2022 09:33:34 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.71 on epoch=737
05/21/2022 09:33:35 - INFO - __main__ - Global step 2950 Train loss 0.74 Classification-F1 0.09305210918114144 on epoch=737
05/21/2022 09:33:36 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.76 on epoch=739
05/21/2022 09:33:38 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.71 on epoch=742
05/21/2022 09:33:39 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.70 on epoch=744
05/21/2022 09:33:41 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.76 on epoch=747
05/21/2022 09:33:42 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.71 on epoch=749
05/21/2022 09:33:43 - INFO - __main__ - Global step 3000 Train loss 0.73 Classification-F1 0.13021778584392013 on epoch=749
05/21/2022 09:33:43 - INFO - __main__ - save last model!
05/21/2022 09:33:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 09:33:43 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 09:33:43 - INFO - __main__ - Printing 3 examples
05/21/2022 09:33:43 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 09:33:43 - INFO - __main__ - ['others']
05/21/2022 09:33:43 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 09:33:43 - INFO - __main__ - ['others']
05/21/2022 09:33:43 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 09:33:43 - INFO - __main__ - ['others']
05/21/2022 09:33:43 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:33:43 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:33:43 - INFO - __main__ - Printing 3 examples
05/21/2022 09:33:43 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 09:33:43 - INFO - __main__ - ['others']
05/21/2022 09:33:43 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 09:33:43 - INFO - __main__ - ['others']
05/21/2022 09:33:43 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 09:33:43 - INFO - __main__ - ['others']
05/21/2022 09:33:43 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:33:43 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:33:43 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 09:33:43 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:33:43 - INFO - __main__ - Printing 3 examples
05/21/2022 09:33:43 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 09:33:43 - INFO - __main__ - ['others']
05/21/2022 09:33:43 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 09:33:43 - INFO - __main__ - ['others']
05/21/2022 09:33:43 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 09:33:43 - INFO - __main__ - ['others']
05/21/2022 09:33:43 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:33:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:33:44 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 09:33:46 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:33:50 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 09:33:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 09:33:50 - INFO - __main__ - Starting training!
05/21/2022 09:33:53 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 09:34:41 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_100_0.4_8_predictions.txt
05/21/2022 09:34:41 - INFO - __main__ - Classification-F1 on test data: 0.0513
05/21/2022 09:34:41 - INFO - __main__ - prefix=emo_16_100, lr=0.4, bsz=8, dev_performance=0.2065573770491803, test_performance=0.05126280658379339
05/21/2022 09:34:41 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.3, bsz=8 ...
05/21/2022 09:34:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:34:42 - INFO - __main__ - Printing 3 examples
05/21/2022 09:34:42 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 09:34:42 - INFO - __main__ - ['others']
05/21/2022 09:34:42 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 09:34:42 - INFO - __main__ - ['others']
05/21/2022 09:34:42 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 09:34:42 - INFO - __main__ - ['others']
05/21/2022 09:34:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:34:42 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:34:42 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 09:34:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:34:42 - INFO - __main__ - Printing 3 examples
05/21/2022 09:34:42 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 09:34:42 - INFO - __main__ - ['others']
05/21/2022 09:34:42 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 09:34:42 - INFO - __main__ - ['others']
05/21/2022 09:34:42 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 09:34:42 - INFO - __main__ - ['others']
05/21/2022 09:34:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:34:42 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:34:42 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 09:34:49 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 09:34:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 09:34:50 - INFO - __main__ - Starting training!
05/21/2022 09:34:52 - INFO - __main__ - Step 10 Global step 10 Train loss 4.12 on epoch=2
05/21/2022 09:34:54 - INFO - __main__ - Step 20 Global step 20 Train loss 3.65 on epoch=4
05/21/2022 09:34:55 - INFO - __main__ - Step 30 Global step 30 Train loss 3.46 on epoch=7
05/21/2022 09:34:57 - INFO - __main__ - Step 40 Global step 40 Train loss 2.98 on epoch=9
05/21/2022 09:34:58 - INFO - __main__ - Step 50 Global step 50 Train loss 2.64 on epoch=12
05/21/2022 09:34:59 - INFO - __main__ - Global step 50 Train loss 3.37 Classification-F1 0.1 on epoch=12
05/21/2022 09:34:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/21/2022 09:35:01 - INFO - __main__ - Step 60 Global step 60 Train loss 2.10 on epoch=14
05/21/2022 09:35:02 - INFO - __main__ - Step 70 Global step 70 Train loss 1.86 on epoch=17
05/21/2022 09:35:03 - INFO - __main__ - Step 80 Global step 80 Train loss 1.66 on epoch=19
05/21/2022 09:35:05 - INFO - __main__ - Step 90 Global step 90 Train loss 1.51 on epoch=22
05/21/2022 09:35:06 - INFO - __main__ - Step 100 Global step 100 Train loss 1.30 on epoch=24
05/21/2022 09:35:06 - INFO - __main__ - Global step 100 Train loss 1.68 Classification-F1 0.1 on epoch=24
05/21/2022 09:35:08 - INFO - __main__ - Step 110 Global step 110 Train loss 1.20 on epoch=27
05/21/2022 09:35:09 - INFO - __main__ - Step 120 Global step 120 Train loss 1.17 on epoch=29
05/21/2022 09:35:10 - INFO - __main__ - Step 130 Global step 130 Train loss 1.23 on epoch=32
05/21/2022 09:35:12 - INFO - __main__ - Step 140 Global step 140 Train loss 1.03 on epoch=34
05/21/2022 09:35:13 - INFO - __main__ - Step 150 Global step 150 Train loss 1.09 on epoch=37
05/21/2022 09:35:14 - INFO - __main__ - Global step 150 Train loss 1.14 Classification-F1 0.1982859531772575 on epoch=37
05/21/2022 09:35:14 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1982859531772575 on epoch=37, global_step=150
05/21/2022 09:35:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.97 on epoch=39
05/21/2022 09:35:17 - INFO - __main__ - Step 170 Global step 170 Train loss 1.01 on epoch=42
05/21/2022 09:35:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.92 on epoch=44
05/21/2022 09:35:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.95 on epoch=47
05/21/2022 09:35:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.93 on epoch=49
05/21/2022 09:35:21 - INFO - __main__ - Global step 200 Train loss 0.96 Classification-F1 0.10126582278481013 on epoch=49
05/21/2022 09:35:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.90 on epoch=52
05/21/2022 09:35:24 - INFO - __main__ - Step 220 Global step 220 Train loss 1.00 on epoch=54
05/21/2022 09:35:25 - INFO - __main__ - Step 230 Global step 230 Train loss 1.04 on epoch=57
05/21/2022 09:35:27 - INFO - __main__ - Step 240 Global step 240 Train loss 1.00 on epoch=59
05/21/2022 09:35:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.93 on epoch=62
05/21/2022 09:35:29 - INFO - __main__ - Global step 250 Train loss 0.98 Classification-F1 0.10833715071003205 on epoch=62
05/21/2022 09:35:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.92 on epoch=64
05/21/2022 09:35:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.96 on epoch=67
05/21/2022 09:35:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.93 on epoch=69
05/21/2022 09:35:34 - INFO - __main__ - Step 290 Global step 290 Train loss 1.04 on epoch=72
05/21/2022 09:35:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.98 on epoch=74
05/21/2022 09:35:36 - INFO - __main__ - Global step 300 Train loss 0.97 Classification-F1 0.16865079365079366 on epoch=74
05/21/2022 09:35:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.91 on epoch=77
05/21/2022 09:35:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.94 on epoch=79
05/21/2022 09:35:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.97 on epoch=82
05/21/2022 09:35:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.93 on epoch=84
05/21/2022 09:35:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.92 on epoch=87
05/21/2022 09:35:43 - INFO - __main__ - Global step 350 Train loss 0.93 Classification-F1 0.16223908918406071 on epoch=87
05/21/2022 09:35:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.89 on epoch=89
05/21/2022 09:35:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.87 on epoch=92
05/21/2022 09:35:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.92 on epoch=94
05/21/2022 09:35:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.96 on epoch=97
05/21/2022 09:35:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.85 on epoch=99
05/21/2022 09:35:51 - INFO - __main__ - Global step 400 Train loss 0.90 Classification-F1 0.10126582278481013 on epoch=99
05/21/2022 09:35:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.87 on epoch=102
05/21/2022 09:35:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.92 on epoch=104
05/21/2022 09:35:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.84 on epoch=107
05/21/2022 09:35:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.91 on epoch=109
05/21/2022 09:35:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.90 on epoch=112
05/21/2022 09:35:58 - INFO - __main__ - Global step 450 Train loss 0.89 Classification-F1 0.14383875400824553 on epoch=112
05/21/2022 09:36:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.87 on epoch=114
05/21/2022 09:36:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.79 on epoch=117
05/21/2022 09:36:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.82 on epoch=119
05/21/2022 09:36:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.80 on epoch=122
05/21/2022 09:36:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.96 on epoch=124
05/21/2022 09:36:07 - INFO - __main__ - Global step 500 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=124
05/21/2022 09:36:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.95 on epoch=127
05/21/2022 09:36:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.87 on epoch=129
05/21/2022 09:36:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.86 on epoch=132
05/21/2022 09:36:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.81 on epoch=134
05/21/2022 09:36:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.85 on epoch=137
05/21/2022 09:36:15 - INFO - __main__ - Global step 550 Train loss 0.87 Classification-F1 0.12403499742665978 on epoch=137
05/21/2022 09:36:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.87 on epoch=139
05/21/2022 09:36:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.78 on epoch=142
05/21/2022 09:36:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.87 on epoch=144
05/21/2022 09:36:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.83 on epoch=147
05/21/2022 09:36:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.93 on epoch=149
05/21/2022 09:36:23 - INFO - __main__ - Global step 600 Train loss 0.86 Classification-F1 0.13194444444444445 on epoch=149
05/21/2022 09:36:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.87 on epoch=152
05/21/2022 09:36:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.79 on epoch=154
05/21/2022 09:36:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.80 on epoch=157
05/21/2022 09:36:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.82 on epoch=159
05/21/2022 09:36:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.80 on epoch=162
05/21/2022 09:36:31 - INFO - __main__ - Global step 650 Train loss 0.81 Classification-F1 0.17744755244755245 on epoch=162
05/21/2022 09:36:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.87 on epoch=164
05/21/2022 09:36:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.91 on epoch=167
05/21/2022 09:36:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.83 on epoch=169
05/21/2022 09:36:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.85 on epoch=172
05/21/2022 09:36:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.78 on epoch=174
05/21/2022 09:36:38 - INFO - __main__ - Global step 700 Train loss 0.85 Classification-F1 0.11666666666666667 on epoch=174
05/21/2022 09:36:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.91 on epoch=177
05/21/2022 09:36:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.91 on epoch=179
05/21/2022 09:36:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.80 on epoch=182
05/21/2022 09:36:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.78 on epoch=184
05/21/2022 09:36:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.85 on epoch=187
05/21/2022 09:36:46 - INFO - __main__ - Global step 750 Train loss 0.85 Classification-F1 0.11687344913151365 on epoch=187
05/21/2022 09:36:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.87 on epoch=189
05/21/2022 09:36:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.89 on epoch=192
05/21/2022 09:36:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.83 on epoch=194
05/21/2022 09:36:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.81 on epoch=197
05/21/2022 09:36:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.78 on epoch=199
05/21/2022 09:36:53 - INFO - __main__ - Global step 800 Train loss 0.84 Classification-F1 0.09210526315789473 on epoch=199
05/21/2022 09:36:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.86 on epoch=202
05/21/2022 09:36:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.86 on epoch=204
05/21/2022 09:36:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.81 on epoch=207
05/21/2022 09:36:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.79 on epoch=209
05/21/2022 09:37:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.81 on epoch=212
05/21/2022 09:37:01 - INFO - __main__ - Global step 850 Train loss 0.83 Classification-F1 0.13333333333333333 on epoch=212
05/21/2022 09:37:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.84 on epoch=214
05/21/2022 09:37:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.84 on epoch=217
05/21/2022 09:37:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.93 on epoch=219
05/21/2022 09:37:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.78 on epoch=222
05/21/2022 09:37:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.85 on epoch=224
05/21/2022 09:37:08 - INFO - __main__ - Global step 900 Train loss 0.85 Classification-F1 0.13035714285714284 on epoch=224
05/21/2022 09:37:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.84 on epoch=227
05/21/2022 09:37:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.87 on epoch=229
05/21/2022 09:37:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.84 on epoch=232
05/21/2022 09:37:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.83 on epoch=234
05/21/2022 09:37:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.90 on epoch=237
05/21/2022 09:37:15 - INFO - __main__ - Global step 950 Train loss 0.85 Classification-F1 0.10476190476190475 on epoch=237
05/21/2022 09:37:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.89 on epoch=239
05/21/2022 09:37:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.82 on epoch=242
05/21/2022 09:37:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.83 on epoch=244
05/21/2022 09:37:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.85 on epoch=247
05/21/2022 09:37:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.79 on epoch=249
05/21/2022 09:37:22 - INFO - __main__ - Global step 1000 Train loss 0.84 Classification-F1 0.12394957983193278 on epoch=249
05/21/2022 09:37:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.97 on epoch=252
05/21/2022 09:37:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.95 on epoch=254
05/21/2022 09:37:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.79 on epoch=257
05/21/2022 09:37:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.86 on epoch=259
05/21/2022 09:37:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.86 on epoch=262
05/21/2022 09:37:30 - INFO - __main__ - Global step 1050 Train loss 0.89 Classification-F1 0.10234192037470727 on epoch=262
05/21/2022 09:37:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.85 on epoch=264
05/21/2022 09:37:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.87 on epoch=267
05/21/2022 09:37:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.80 on epoch=269
05/21/2022 09:37:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.81 on epoch=272
05/21/2022 09:37:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.85 on epoch=274
05/21/2022 09:37:37 - INFO - __main__ - Global step 1100 Train loss 0.84 Classification-F1 0.10675381263616557 on epoch=274
05/21/2022 09:37:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.89 on epoch=277
05/21/2022 09:37:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.84 on epoch=279
05/21/2022 09:37:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.83 on epoch=282
05/21/2022 09:37:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.83 on epoch=284
05/21/2022 09:37:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.78 on epoch=287
05/21/2022 09:37:45 - INFO - __main__ - Global step 1150 Train loss 0.83 Classification-F1 0.1253101736972705 on epoch=287
05/21/2022 09:37:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.87 on epoch=289
05/21/2022 09:37:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.89 on epoch=292
05/21/2022 09:37:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.80 on epoch=294
05/21/2022 09:37:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.78 on epoch=297
05/21/2022 09:37:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.81 on epoch=299
05/21/2022 09:37:52 - INFO - __main__ - Global step 1200 Train loss 0.83 Classification-F1 0.12394957983193278 on epoch=299
05/21/2022 09:37:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.78 on epoch=302
05/21/2022 09:37:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.85 on epoch=304
05/21/2022 09:37:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.81 on epoch=307
05/21/2022 09:37:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.83 on epoch=309
05/21/2022 09:37:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.73 on epoch=312
05/21/2022 09:38:00 - INFO - __main__ - Global step 1250 Train loss 0.80 Classification-F1 0.08 on epoch=312
05/21/2022 09:38:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.87 on epoch=314
05/21/2022 09:38:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.85 on epoch=317
05/21/2022 09:38:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.82 on epoch=319
05/21/2022 09:38:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.79 on epoch=322
05/21/2022 09:38:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.75 on epoch=324
05/21/2022 09:38:07 - INFO - __main__ - Global step 1300 Train loss 0.82 Classification-F1 0.08333333333333333 on epoch=324
05/21/2022 09:38:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.84 on epoch=327
05/21/2022 09:38:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.76 on epoch=329
05/21/2022 09:38:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.84 on epoch=332
05/21/2022 09:38:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.82 on epoch=334
05/21/2022 09:38:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.86 on epoch=337
05/21/2022 09:38:15 - INFO - __main__ - Global step 1350 Train loss 0.82 Classification-F1 0.1347521402927368 on epoch=337
05/21/2022 09:38:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.80 on epoch=339
05/21/2022 09:38:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.80 on epoch=342
05/21/2022 09:38:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.85 on epoch=344
05/21/2022 09:38:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.82 on epoch=347
05/21/2022 09:38:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.80 on epoch=349
05/21/2022 09:38:22 - INFO - __main__ - Global step 1400 Train loss 0.82 Classification-F1 0.09090909090909091 on epoch=349
05/21/2022 09:38:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.77 on epoch=352
05/21/2022 09:38:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.81 on epoch=354
05/21/2022 09:38:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.83 on epoch=357
05/21/2022 09:38:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.78 on epoch=359
05/21/2022 09:38:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.84 on epoch=362
05/21/2022 09:38:29 - INFO - __main__ - Global step 1450 Train loss 0.81 Classification-F1 0.1328125 on epoch=362
05/21/2022 09:38:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.84 on epoch=364
05/21/2022 09:38:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.73 on epoch=367
05/21/2022 09:38:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.74 on epoch=369
05/21/2022 09:38:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.79 on epoch=372
05/21/2022 09:38:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.80 on epoch=374
05/21/2022 09:38:36 - INFO - __main__ - Global step 1500 Train loss 0.78 Classification-F1 0.0974025974025974 on epoch=374
05/21/2022 09:38:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.84 on epoch=377
05/21/2022 09:38:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.80 on epoch=379
05/21/2022 09:38:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.79 on epoch=382
05/21/2022 09:38:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.72 on epoch=384
05/21/2022 09:38:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.77 on epoch=387
05/21/2022 09:38:43 - INFO - __main__ - Global step 1550 Train loss 0.79 Classification-F1 0.1313186813186813 on epoch=387
05/21/2022 09:38:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.84 on epoch=389
05/21/2022 09:38:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.81 on epoch=392
05/21/2022 09:38:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.85 on epoch=394
05/21/2022 09:38:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.80 on epoch=397
05/21/2022 09:38:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.74 on epoch=399
05/21/2022 09:38:50 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.11732186732186733 on epoch=399
05/21/2022 09:38:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.77 on epoch=402
05/21/2022 09:38:53 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.75 on epoch=404
05/21/2022 09:38:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.77 on epoch=407
05/21/2022 09:38:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.85 on epoch=409
05/21/2022 09:38:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.74 on epoch=412
05/21/2022 09:38:57 - INFO - __main__ - Global step 1650 Train loss 0.78 Classification-F1 0.11208791208791208 on epoch=412
05/21/2022 09:38:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.80 on epoch=414
05/21/2022 09:39:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.84 on epoch=417
05/21/2022 09:39:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.73 on epoch=419
05/21/2022 09:39:03 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.84 on epoch=422
05/21/2022 09:39:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.81 on epoch=424
05/21/2022 09:39:04 - INFO - __main__ - Global step 1700 Train loss 0.80 Classification-F1 0.11078022632519356 on epoch=424
05/21/2022 09:39:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.79 on epoch=427
05/21/2022 09:39:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.84 on epoch=429
05/21/2022 09:39:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.82 on epoch=432
05/21/2022 09:39:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.80 on epoch=434
05/21/2022 09:39:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.83 on epoch=437
05/21/2022 09:39:12 - INFO - __main__ - Global step 1750 Train loss 0.81 Classification-F1 0.11666666666666667 on epoch=437
05/21/2022 09:39:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.76 on epoch=439
05/21/2022 09:39:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.77 on epoch=442
05/21/2022 09:39:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.80 on epoch=444
05/21/2022 09:39:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.78 on epoch=447
05/21/2022 09:39:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.79 on epoch=449
05/21/2022 09:39:19 - INFO - __main__ - Global step 1800 Train loss 0.78 Classification-F1 0.0945945945945946 on epoch=449
05/21/2022 09:39:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.80 on epoch=452
05/21/2022 09:39:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.76 on epoch=454
05/21/2022 09:39:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.83 on epoch=457
05/21/2022 09:39:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.77 on epoch=459
05/21/2022 09:39:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.77 on epoch=462
05/21/2022 09:39:26 - INFO - __main__ - Global step 1850 Train loss 0.79 Classification-F1 0.11056511056511056 on epoch=462
05/21/2022 09:39:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.77 on epoch=464
05/21/2022 09:39:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.75 on epoch=467
05/21/2022 09:39:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.78 on epoch=469
05/21/2022 09:39:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.79 on epoch=472
05/21/2022 09:39:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.86 on epoch=474
05/21/2022 09:39:34 - INFO - __main__ - Global step 1900 Train loss 0.79 Classification-F1 0.0945945945945946 on epoch=474
05/21/2022 09:39:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.81 on epoch=477
05/21/2022 09:39:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.72 on epoch=479
05/21/2022 09:39:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.73 on epoch=482
05/21/2022 09:39:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.80 on epoch=484
05/21/2022 09:39:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.76 on epoch=487
05/21/2022 09:39:41 - INFO - __main__ - Global step 1950 Train loss 0.76 Classification-F1 0.13275613275613274 on epoch=487
05/21/2022 09:39:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.82 on epoch=489
05/21/2022 09:39:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.80 on epoch=492
05/21/2022 09:39:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.72 on epoch=494
05/21/2022 09:39:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.76 on epoch=497
05/21/2022 09:39:47 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.82 on epoch=499
05/21/2022 09:39:47 - INFO - __main__ - Global step 2000 Train loss 0.78 Classification-F1 0.09333333333333334 on epoch=499
05/21/2022 09:39:49 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.81 on epoch=502
05/21/2022 09:39:50 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.76 on epoch=504
05/21/2022 09:39:51 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.77 on epoch=507
05/21/2022 09:39:53 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.85 on epoch=509
05/21/2022 09:39:54 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.75 on epoch=512
05/21/2022 09:39:55 - INFO - __main__ - Global step 2050 Train loss 0.79 Classification-F1 0.09305210918114144 on epoch=512
05/21/2022 09:39:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.75 on epoch=514
05/21/2022 09:39:57 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.76 on epoch=517
05/21/2022 09:39:59 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.74 on epoch=519
05/21/2022 09:40:00 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.74 on epoch=522
05/21/2022 09:40:01 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.78 on epoch=524
05/21/2022 09:40:02 - INFO - __main__ - Global step 2100 Train loss 0.75 Classification-F1 0.11944444444444444 on epoch=524
05/21/2022 09:40:04 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.75 on epoch=527
05/21/2022 09:40:05 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.76 on epoch=529
05/21/2022 09:40:06 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.77 on epoch=532
05/21/2022 09:40:07 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.80 on epoch=534
05/21/2022 09:40:09 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.72 on epoch=537
05/21/2022 09:40:09 - INFO - __main__ - Global step 2150 Train loss 0.76 Classification-F1 0.11208791208791208 on epoch=537
05/21/2022 09:40:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.73 on epoch=539
05/21/2022 09:40:12 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.84 on epoch=542
05/21/2022 09:40:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.80 on epoch=544
05/21/2022 09:40:15 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.73 on epoch=547
05/21/2022 09:40:16 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.79 on epoch=549
05/21/2022 09:40:17 - INFO - __main__ - Global step 2200 Train loss 0.78 Classification-F1 0.09999999999999999 on epoch=549
05/21/2022 09:40:18 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.77 on epoch=552
05/21/2022 09:40:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.77 on epoch=554
05/21/2022 09:40:20 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.73 on epoch=557
05/21/2022 09:40:21 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.71 on epoch=559
05/21/2022 09:40:23 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.71 on epoch=562
05/21/2022 09:40:23 - INFO - __main__ - Global step 2250 Train loss 0.74 Classification-F1 0.14279379157427938 on epoch=562
05/21/2022 09:40:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.83 on epoch=564
05/21/2022 09:40:25 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.74 on epoch=567
05/21/2022 09:40:27 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.79 on epoch=569
05/21/2022 09:40:28 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.74 on epoch=572
05/21/2022 09:40:30 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.73 on epoch=574
05/21/2022 09:40:30 - INFO - __main__ - Global step 2300 Train loss 0.77 Classification-F1 0.11272141706924316 on epoch=574
05/21/2022 09:40:31 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.80 on epoch=577
05/21/2022 09:40:33 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.79 on epoch=579
05/21/2022 09:40:34 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.79 on epoch=582
05/21/2022 09:40:36 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.78 on epoch=584
05/21/2022 09:40:37 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.79 on epoch=587
05/21/2022 09:40:37 - INFO - __main__ - Global step 2350 Train loss 0.79 Classification-F1 0.1176046176046176 on epoch=587
05/21/2022 09:40:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.76 on epoch=589
05/21/2022 09:40:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.77 on epoch=592
05/21/2022 09:40:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.78 on epoch=594
05/21/2022 09:40:42 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.78 on epoch=597
05/21/2022 09:40:44 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.78 on epoch=599
05/21/2022 09:40:44 - INFO - __main__ - Global step 2400 Train loss 0.77 Classification-F1 0.10757575757575757 on epoch=599
05/21/2022 09:40:45 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.77 on epoch=602
05/21/2022 09:40:46 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.78 on epoch=604
05/21/2022 09:40:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.80 on epoch=607
05/21/2022 09:40:49 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.78 on epoch=609
05/21/2022 09:40:50 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.75 on epoch=612
05/21/2022 09:40:51 - INFO - __main__ - Global step 2450 Train loss 0.77 Classification-F1 0.1451048951048951 on epoch=612
05/21/2022 09:40:52 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.82 on epoch=614
05/21/2022 09:40:54 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.78 on epoch=617
05/21/2022 09:40:55 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.75 on epoch=619
05/21/2022 09:40:57 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.79 on epoch=622
05/21/2022 09:40:59 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.78 on epoch=624
05/21/2022 09:40:59 - INFO - __main__ - Global step 2500 Train loss 0.78 Classification-F1 0.13022941970310392 on epoch=624
05/21/2022 09:41:01 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.83 on epoch=627
05/21/2022 09:41:02 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.72 on epoch=629
05/21/2022 09:41:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.79 on epoch=632
05/21/2022 09:41:05 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.70 on epoch=634
05/21/2022 09:41:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.77 on epoch=637
05/21/2022 09:41:07 - INFO - __main__ - Global step 2550 Train loss 0.76 Classification-F1 0.1437908496732026 on epoch=637
05/21/2022 09:41:09 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.75 on epoch=639
05/21/2022 09:41:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.75 on epoch=642
05/21/2022 09:41:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.71 on epoch=644
05/21/2022 09:41:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.78 on epoch=647
05/21/2022 09:41:14 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.74 on epoch=649
05/21/2022 09:41:15 - INFO - __main__ - Global step 2600 Train loss 0.75 Classification-F1 0.13032524049473201 on epoch=649
05/21/2022 09:41:16 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.78 on epoch=652
05/21/2022 09:41:18 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.80 on epoch=654
05/21/2022 09:41:19 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.74 on epoch=657
05/21/2022 09:41:21 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.81 on epoch=659
05/21/2022 09:41:22 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.76 on epoch=662
05/21/2022 09:41:23 - INFO - __main__ - Global step 2650 Train loss 0.78 Classification-F1 0.1337028824833703 on epoch=662
05/21/2022 09:41:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.68 on epoch=664
05/21/2022 09:41:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.74 on epoch=667
05/21/2022 09:41:28 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.79 on epoch=669
05/21/2022 09:41:29 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.76 on epoch=672
05/21/2022 09:41:30 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.77 on epoch=674
05/21/2022 09:41:31 - INFO - __main__ - Global step 2700 Train loss 0.75 Classification-F1 0.14285714285714285 on epoch=674
05/21/2022 09:41:32 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.78 on epoch=677
05/21/2022 09:41:34 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.74 on epoch=679
05/21/2022 09:41:35 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.82 on epoch=682
05/21/2022 09:41:36 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.72 on epoch=684
05/21/2022 09:41:38 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.76 on epoch=687
05/21/2022 09:41:38 - INFO - __main__ - Global step 2750 Train loss 0.76 Classification-F1 0.14285714285714285 on epoch=687
05/21/2022 09:41:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.70 on epoch=689
05/21/2022 09:41:41 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.76 on epoch=692
05/21/2022 09:41:42 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.78 on epoch=694
05/21/2022 09:41:44 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.77 on epoch=697
05/21/2022 09:41:45 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.73 on epoch=699
05/21/2022 09:41:46 - INFO - __main__ - Global step 2800 Train loss 0.75 Classification-F1 0.13022941970310392 on epoch=699
05/21/2022 09:41:47 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.81 on epoch=702
05/21/2022 09:41:49 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.79 on epoch=704
05/21/2022 09:41:51 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.77 on epoch=707
05/21/2022 09:41:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.73 on epoch=709
05/21/2022 09:41:53 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.83 on epoch=712
05/21/2022 09:41:54 - INFO - __main__ - Global step 2850 Train loss 0.79 Classification-F1 0.14337568058076222 on epoch=712
05/21/2022 09:41:56 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.77 on epoch=714
05/21/2022 09:41:57 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.72 on epoch=717
05/21/2022 09:41:59 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.72 on epoch=719
05/21/2022 09:42:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.80 on epoch=722
05/21/2022 09:42:02 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.73 on epoch=724
05/21/2022 09:42:03 - INFO - __main__ - Global step 2900 Train loss 0.75 Classification-F1 0.16988795518207284 on epoch=724
05/21/2022 09:42:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.75 on epoch=727
05/21/2022 09:42:06 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.75 on epoch=729
05/21/2022 09:42:07 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.78 on epoch=732
05/21/2022 09:42:09 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.74 on epoch=734
05/21/2022 09:42:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.74 on epoch=737
05/21/2022 09:42:11 - INFO - __main__ - Global step 2950 Train loss 0.75 Classification-F1 0.12403499742665978 on epoch=737
05/21/2022 09:42:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.68 on epoch=739
05/21/2022 09:42:14 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.70 on epoch=742
05/21/2022 09:42:16 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.76 on epoch=744
05/21/2022 09:42:17 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.76 on epoch=747
05/21/2022 09:42:18 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.67 on epoch=749
05/21/2022 09:42:19 - INFO - __main__ - Global step 3000 Train loss 0.71 Classification-F1 0.11208791208791208 on epoch=749
05/21/2022 09:42:19 - INFO - __main__ - save last model!
05/21/2022 09:42:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 09:42:19 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 09:42:19 - INFO - __main__ - Printing 3 examples
05/21/2022 09:42:19 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 09:42:19 - INFO - __main__ - ['others']
05/21/2022 09:42:19 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 09:42:19 - INFO - __main__ - ['others']
05/21/2022 09:42:19 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 09:42:19 - INFO - __main__ - ['others']
05/21/2022 09:42:19 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:42:20 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:42:20 - INFO - __main__ - Printing 3 examples
05/21/2022 09:42:20 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 09:42:20 - INFO - __main__ - ['others']
05/21/2022 09:42:20 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 09:42:20 - INFO - __main__ - ['others']
05/21/2022 09:42:20 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 09:42:20 - INFO - __main__ - ['others']
05/21/2022 09:42:20 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:42:20 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:42:20 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 09:42:20 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:42:20 - INFO - __main__ - Printing 3 examples
05/21/2022 09:42:20 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 09:42:20 - INFO - __main__ - ['others']
05/21/2022 09:42:20 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 09:42:20 - INFO - __main__ - ['others']
05/21/2022 09:42:20 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 09:42:20 - INFO - __main__ - ['others']
05/21/2022 09:42:20 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:42:20 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:42:20 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 09:42:22 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:42:26 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 09:42:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 09:42:27 - INFO - __main__ - Starting training!
05/21/2022 09:42:29 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 09:43:15 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_100_0.3_8_predictions.txt
05/21/2022 09:43:15 - INFO - __main__ - Classification-F1 on test data: 0.0575
05/21/2022 09:43:15 - INFO - __main__ - prefix=emo_16_100, lr=0.3, bsz=8, dev_performance=0.1982859531772575, test_performance=0.057494644158672614
05/21/2022 09:43:15 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.2, bsz=8 ...
05/21/2022 09:43:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:43:16 - INFO - __main__ - Printing 3 examples
05/21/2022 09:43:16 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 09:43:16 - INFO - __main__ - ['others']
05/21/2022 09:43:16 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 09:43:16 - INFO - __main__ - ['others']
05/21/2022 09:43:16 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 09:43:16 - INFO - __main__ - ['others']
05/21/2022 09:43:16 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:43:16 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:43:16 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 09:43:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:43:16 - INFO - __main__ - Printing 3 examples
05/21/2022 09:43:16 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 09:43:16 - INFO - __main__ - ['others']
05/21/2022 09:43:16 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 09:43:16 - INFO - __main__ - ['others']
05/21/2022 09:43:16 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 09:43:16 - INFO - __main__ - ['others']
05/21/2022 09:43:16 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:43:17 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:43:17 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 09:43:22 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 09:43:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 09:43:23 - INFO - __main__ - Starting training!
05/21/2022 09:43:24 - INFO - __main__ - Step 10 Global step 10 Train loss 4.06 on epoch=2
05/21/2022 09:43:26 - INFO - __main__ - Step 20 Global step 20 Train loss 3.82 on epoch=4
05/21/2022 09:43:27 - INFO - __main__ - Step 30 Global step 30 Train loss 3.60 on epoch=7
05/21/2022 09:43:28 - INFO - __main__ - Step 40 Global step 40 Train loss 3.21 on epoch=9
05/21/2022 09:43:29 - INFO - __main__ - Step 50 Global step 50 Train loss 2.99 on epoch=12
05/21/2022 09:43:30 - INFO - __main__ - Global step 50 Train loss 3.54 Classification-F1 0.0 on epoch=12
05/21/2022 09:43:30 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
05/21/2022 09:43:32 - INFO - __main__ - Step 60 Global step 60 Train loss 2.74 on epoch=14
05/21/2022 09:43:33 - INFO - __main__ - Step 70 Global step 70 Train loss 2.63 on epoch=17
05/21/2022 09:43:35 - INFO - __main__ - Step 80 Global step 80 Train loss 2.25 on epoch=19
05/21/2022 09:43:36 - INFO - __main__ - Step 90 Global step 90 Train loss 2.20 on epoch=22
05/21/2022 09:43:38 - INFO - __main__ - Step 100 Global step 100 Train loss 1.87 on epoch=24
05/21/2022 09:43:38 - INFO - __main__ - Global step 100 Train loss 2.34 Classification-F1 0.1 on epoch=24
05/21/2022 09:43:38 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.1 on epoch=24, global_step=100
05/21/2022 09:43:40 - INFO - __main__ - Step 110 Global step 110 Train loss 1.85 on epoch=27
05/21/2022 09:43:41 - INFO - __main__ - Step 120 Global step 120 Train loss 1.66 on epoch=29
05/21/2022 09:43:43 - INFO - __main__ - Step 130 Global step 130 Train loss 1.65 on epoch=32
05/21/2022 09:43:44 - INFO - __main__ - Step 140 Global step 140 Train loss 1.33 on epoch=34
05/21/2022 09:43:45 - INFO - __main__ - Step 150 Global step 150 Train loss 1.26 on epoch=37
05/21/2022 09:43:46 - INFO - __main__ - Global step 150 Train loss 1.55 Classification-F1 0.1778584392014519 on epoch=37
05/21/2022 09:43:46 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1778584392014519 on epoch=37, global_step=150
05/21/2022 09:43:47 - INFO - __main__ - Step 160 Global step 160 Train loss 1.27 on epoch=39
05/21/2022 09:43:49 - INFO - __main__ - Step 170 Global step 170 Train loss 1.23 on epoch=42
05/21/2022 09:43:50 - INFO - __main__ - Step 180 Global step 180 Train loss 1.05 on epoch=44
05/21/2022 09:43:52 - INFO - __main__ - Step 190 Global step 190 Train loss 1.12 on epoch=47
05/21/2022 09:43:53 - INFO - __main__ - Step 200 Global step 200 Train loss 1.11 on epoch=49
05/21/2022 09:43:54 - INFO - __main__ - Global step 200 Train loss 1.15 Classification-F1 0.1 on epoch=49
05/21/2022 09:43:55 - INFO - __main__ - Step 210 Global step 210 Train loss 1.08 on epoch=52
05/21/2022 09:43:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.97 on epoch=54
05/21/2022 09:43:58 - INFO - __main__ - Step 230 Global step 230 Train loss 1.03 on epoch=57
05/21/2022 09:43:59 - INFO - __main__ - Step 240 Global step 240 Train loss 1.09 on epoch=59
05/21/2022 09:44:01 - INFO - __main__ - Step 250 Global step 250 Train loss 1.05 on epoch=62
05/21/2022 09:44:02 - INFO - __main__ - Global step 250 Train loss 1.04 Classification-F1 0.09493670886075949 on epoch=62
05/21/2022 09:44:03 - INFO - __main__ - Step 260 Global step 260 Train loss 1.01 on epoch=64
05/21/2022 09:44:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.96 on epoch=67
05/21/2022 09:44:06 - INFO - __main__ - Step 280 Global step 280 Train loss 1.00 on epoch=69
05/21/2022 09:44:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.92 on epoch=72
05/21/2022 09:44:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.96 on epoch=74
05/21/2022 09:44:09 - INFO - __main__ - Global step 300 Train loss 0.97 Classification-F1 0.1 on epoch=74
05/21/2022 09:44:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.98 on epoch=77
05/21/2022 09:44:12 - INFO - __main__ - Step 320 Global step 320 Train loss 1.06 on epoch=79
05/21/2022 09:44:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.91 on epoch=82
05/21/2022 09:44:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.89 on epoch=84
05/21/2022 09:44:16 - INFO - __main__ - Step 350 Global step 350 Train loss 1.05 on epoch=87
05/21/2022 09:44:17 - INFO - __main__ - Global step 350 Train loss 0.98 Classification-F1 0.16138763197586728 on epoch=87
05/21/2022 09:44:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.96 on epoch=89
05/21/2022 09:44:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.99 on epoch=92
05/21/2022 09:44:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.93 on epoch=94
05/21/2022 09:44:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.93 on epoch=97
05/21/2022 09:44:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.97 on epoch=99
05/21/2022 09:44:25 - INFO - __main__ - Global step 400 Train loss 0.96 Classification-F1 0.14069478908188585 on epoch=99
05/21/2022 09:44:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.87 on epoch=102
05/21/2022 09:44:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.95 on epoch=104
05/21/2022 09:44:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.95 on epoch=107
05/21/2022 09:44:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.97 on epoch=109
05/21/2022 09:44:32 - INFO - __main__ - Step 450 Global step 450 Train loss 1.01 on epoch=112
05/21/2022 09:44:33 - INFO - __main__ - Global step 450 Train loss 0.95 Classification-F1 0.16451612903225807 on epoch=112
05/21/2022 09:44:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.91 on epoch=114
05/21/2022 09:44:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.95 on epoch=117
05/21/2022 09:44:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.82 on epoch=119
05/21/2022 09:44:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.89 on epoch=122
05/21/2022 09:44:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.94 on epoch=124
05/21/2022 09:44:40 - INFO - __main__ - Global step 500 Train loss 0.90 Classification-F1 0.1 on epoch=124
05/21/2022 09:44:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.87 on epoch=127
05/21/2022 09:44:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.95 on epoch=129
05/21/2022 09:44:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.97 on epoch=132
05/21/2022 09:44:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.95 on epoch=134
05/21/2022 09:44:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=137
05/21/2022 09:44:47 - INFO - __main__ - Global step 550 Train loss 0.92 Classification-F1 0.13997113997113997 on epoch=137
05/21/2022 09:44:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.94 on epoch=139
05/21/2022 09:44:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.85 on epoch=142
05/21/2022 09:44:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.91 on epoch=144
05/21/2022 09:44:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.82 on epoch=147
05/21/2022 09:44:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.91 on epoch=149
05/21/2022 09:44:55 - INFO - __main__ - Global step 600 Train loss 0.89 Classification-F1 0.11666666666666667 on epoch=149
05/21/2022 09:44:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.90 on epoch=152
05/21/2022 09:44:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.91 on epoch=154
05/21/2022 09:44:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.87 on epoch=157
05/21/2022 09:45:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.90 on epoch=159
05/21/2022 09:45:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.95 on epoch=162
05/21/2022 09:45:02 - INFO - __main__ - Global step 650 Train loss 0.90 Classification-F1 0.1 on epoch=162
05/21/2022 09:45:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.79 on epoch=164
05/21/2022 09:45:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.91 on epoch=167
05/21/2022 09:45:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.85 on epoch=169
05/21/2022 09:45:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.91 on epoch=172
05/21/2022 09:45:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.85 on epoch=174
05/21/2022 09:45:10 - INFO - __main__ - Global step 700 Train loss 0.86 Classification-F1 0.13381369016984046 on epoch=174
05/21/2022 09:45:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.85 on epoch=177
05/21/2022 09:45:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.85 on epoch=179
05/21/2022 09:45:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.90 on epoch=182
05/21/2022 09:45:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.88 on epoch=184
05/21/2022 09:45:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.80 on epoch=187
05/21/2022 09:45:17 - INFO - __main__ - Global step 750 Train loss 0.86 Classification-F1 0.13879981676591846 on epoch=187
05/21/2022 09:45:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.90 on epoch=189
05/21/2022 09:45:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.90 on epoch=192
05/21/2022 09:45:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.88 on epoch=194
05/21/2022 09:45:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.91 on epoch=197
05/21/2022 09:45:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.88 on epoch=199
05/21/2022 09:45:24 - INFO - __main__ - Global step 800 Train loss 0.89 Classification-F1 0.14069478908188585 on epoch=199
05/21/2022 09:45:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.90 on epoch=202
05/21/2022 09:45:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.91 on epoch=204
05/21/2022 09:45:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.88 on epoch=207
05/21/2022 09:45:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.82 on epoch=209
05/21/2022 09:45:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.88 on epoch=212
05/21/2022 09:45:32 - INFO - __main__ - Global step 850 Train loss 0.88 Classification-F1 0.12368421052631579 on epoch=212
05/21/2022 09:45:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.86 on epoch=214
05/21/2022 09:45:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.93 on epoch=217
05/21/2022 09:45:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.83 on epoch=219
05/21/2022 09:45:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.86 on epoch=222
05/21/2022 09:45:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.92 on epoch=224
05/21/2022 09:45:39 - INFO - __main__ - Global step 900 Train loss 0.88 Classification-F1 0.16005291005291006 on epoch=224
05/21/2022 09:45:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.85 on epoch=227
05/21/2022 09:45:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.90 on epoch=229
05/21/2022 09:45:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.79 on epoch=232
05/21/2022 09:45:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.84 on epoch=234
05/21/2022 09:45:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.91 on epoch=237
05/21/2022 09:45:47 - INFO - __main__ - Global step 950 Train loss 0.86 Classification-F1 0.1 on epoch=237
05/21/2022 09:45:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.87 on epoch=239
05/21/2022 09:45:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.81 on epoch=242
05/21/2022 09:45:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.86 on epoch=244
05/21/2022 09:45:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.85 on epoch=247
05/21/2022 09:45:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.82 on epoch=249
05/21/2022 09:45:55 - INFO - __main__ - Global step 1000 Train loss 0.84 Classification-F1 0.08974358974358974 on epoch=249
05/21/2022 09:45:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.80 on epoch=252
05/21/2022 09:45:58 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.84 on epoch=254
05/21/2022 09:45:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.81 on epoch=257
05/21/2022 09:46:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.81 on epoch=259
05/21/2022 09:46:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.84 on epoch=262
05/21/2022 09:46:03 - INFO - __main__ - Global step 1050 Train loss 0.82 Classification-F1 0.13475499092558985 on epoch=262
05/21/2022 09:46:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.80 on epoch=264
05/21/2022 09:46:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.81 on epoch=267
05/21/2022 09:46:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.71 on epoch=269
05/21/2022 09:46:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.88 on epoch=272
05/21/2022 09:46:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.80 on epoch=274
05/21/2022 09:46:10 - INFO - __main__ - Global step 1100 Train loss 0.80 Classification-F1 0.10135135135135136 on epoch=274
05/21/2022 09:46:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.83 on epoch=277
05/21/2022 09:46:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.79 on epoch=279
05/21/2022 09:46:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.83 on epoch=282
05/21/2022 09:46:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.82 on epoch=284
05/21/2022 09:46:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.77 on epoch=287
05/21/2022 09:46:17 - INFO - __main__ - Global step 1150 Train loss 0.81 Classification-F1 0.10679361811631496 on epoch=287
05/21/2022 09:46:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.93 on epoch=289
05/21/2022 09:46:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.76 on epoch=292
05/21/2022 09:46:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.79 on epoch=294
05/21/2022 09:46:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.84 on epoch=297
05/21/2022 09:46:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.81 on epoch=299
05/21/2022 09:46:25 - INFO - __main__ - Global step 1200 Train loss 0.83 Classification-F1 0.1402116402116402 on epoch=299
05/21/2022 09:46:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.80 on epoch=302
05/21/2022 09:46:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.93 on epoch=304
05/21/2022 09:46:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.89 on epoch=307
05/21/2022 09:46:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.76 on epoch=309
05/21/2022 09:46:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.87 on epoch=312
05/21/2022 09:46:32 - INFO - __main__ - Global step 1250 Train loss 0.85 Classification-F1 0.13251935675997617 on epoch=312
05/21/2022 09:46:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.92 on epoch=314
05/21/2022 09:46:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.81 on epoch=317
05/21/2022 09:46:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.76 on epoch=319
05/21/2022 09:46:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.82 on epoch=322
05/21/2022 09:46:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.84 on epoch=324
05/21/2022 09:46:39 - INFO - __main__ - Global step 1300 Train loss 0.83 Classification-F1 0.1409090909090909 on epoch=324
05/21/2022 09:46:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.85 on epoch=327
05/21/2022 09:46:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.81 on epoch=329
05/21/2022 09:46:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.86 on epoch=332
05/21/2022 09:46:44 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.85 on epoch=334
05/21/2022 09:46:46 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.84 on epoch=337
05/21/2022 09:46:46 - INFO - __main__ - Global step 1350 Train loss 0.84 Classification-F1 0.12287581699346406 on epoch=337
05/21/2022 09:46:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.93 on epoch=339
05/21/2022 09:46:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.74 on epoch=342
05/21/2022 09:46:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.72 on epoch=344
05/21/2022 09:46:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.86 on epoch=347
05/21/2022 09:46:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.78 on epoch=349
05/21/2022 09:46:54 - INFO - __main__ - Global step 1400 Train loss 0.81 Classification-F1 0.13398692810457516 on epoch=349
05/21/2022 09:46:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.83 on epoch=352
05/21/2022 09:46:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.86 on epoch=354
05/21/2022 09:46:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.84 on epoch=357
05/21/2022 09:47:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.74 on epoch=359
05/21/2022 09:47:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.84 on epoch=362
05/21/2022 09:47:02 - INFO - __main__ - Global step 1450 Train loss 0.82 Classification-F1 0.14521739130434783 on epoch=362
05/21/2022 09:47:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.85 on epoch=364
05/21/2022 09:47:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.86 on epoch=367
05/21/2022 09:47:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.86 on epoch=369
05/21/2022 09:47:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.86 on epoch=372
05/21/2022 09:47:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.86 on epoch=374
05/21/2022 09:47:11 - INFO - __main__ - Global step 1500 Train loss 0.86 Classification-F1 0.10843672456575681 on epoch=374
05/21/2022 09:47:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.85 on epoch=377
05/21/2022 09:47:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.82 on epoch=379
05/21/2022 09:47:16 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.82 on epoch=382
05/21/2022 09:47:18 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.89 on epoch=384
05/21/2022 09:47:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.78 on epoch=387
05/21/2022 09:47:20 - INFO - __main__ - Global step 1550 Train loss 0.83 Classification-F1 0.09090909090909091 on epoch=387
05/21/2022 09:47:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.85 on epoch=389
05/21/2022 09:47:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.76 on epoch=392
05/21/2022 09:47:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.77 on epoch=394
05/21/2022 09:47:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.70 on epoch=397
05/21/2022 09:47:28 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.86 on epoch=399
05/21/2022 09:47:28 - INFO - __main__ - Global step 1600 Train loss 0.79 Classification-F1 0.14285714285714285 on epoch=399
05/21/2022 09:47:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.80 on epoch=402
05/21/2022 09:47:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.78 on epoch=404
05/21/2022 09:47:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.85 on epoch=407
05/21/2022 09:47:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.80 on epoch=409
05/21/2022 09:47:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.85 on epoch=412
05/21/2022 09:47:36 - INFO - __main__ - Global step 1650 Train loss 0.82 Classification-F1 0.1392857142857143 on epoch=412
05/21/2022 09:47:37 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.75 on epoch=414
05/21/2022 09:47:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.84 on epoch=417
05/21/2022 09:47:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.92 on epoch=419
05/21/2022 09:47:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.81 on epoch=422
05/21/2022 09:47:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.76 on epoch=424
05/21/2022 09:47:43 - INFO - __main__ - Global step 1700 Train loss 0.82 Classification-F1 0.14304519526107942 on epoch=424
05/21/2022 09:47:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.77 on epoch=427
05/21/2022 09:47:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.81 on epoch=429
05/21/2022 09:47:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.81 on epoch=432
05/21/2022 09:47:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.83 on epoch=434
05/21/2022 09:47:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.84 on epoch=437
05/21/2022 09:47:50 - INFO - __main__ - Global step 1750 Train loss 0.81 Classification-F1 0.13482414242292662 on epoch=437
05/21/2022 09:47:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.78 on epoch=439
05/21/2022 09:47:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.82 on epoch=442
05/21/2022 09:47:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.82 on epoch=444
05/21/2022 09:47:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.76 on epoch=447
05/21/2022 09:47:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.83 on epoch=449
05/21/2022 09:47:57 - INFO - __main__ - Global step 1800 Train loss 0.80 Classification-F1 0.09333333333333334 on epoch=449
05/21/2022 09:47:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.80 on epoch=452
05/21/2022 09:48:01 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.82 on epoch=454
05/21/2022 09:48:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.88 on epoch=457
05/21/2022 09:48:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.82 on epoch=459
05/21/2022 09:48:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.85 on epoch=462
05/21/2022 09:48:06 - INFO - __main__ - Global step 1850 Train loss 0.83 Classification-F1 0.11005692599620492 on epoch=462
05/21/2022 09:48:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.79 on epoch=464
05/21/2022 09:48:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.77 on epoch=467
05/21/2022 09:48:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.85 on epoch=469
05/21/2022 09:48:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.77 on epoch=472
05/21/2022 09:48:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.84 on epoch=474
05/21/2022 09:48:14 - INFO - __main__ - Global step 1900 Train loss 0.81 Classification-F1 0.11705989110707803 on epoch=474
05/21/2022 09:48:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.78 on epoch=477
05/21/2022 09:48:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.84 on epoch=479
05/21/2022 09:48:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.84 on epoch=482
05/21/2022 09:48:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.83 on epoch=484
05/21/2022 09:48:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.77 on epoch=487
05/21/2022 09:48:21 - INFO - __main__ - Global step 1950 Train loss 0.81 Classification-F1 0.13361123299692845 on epoch=487
05/21/2022 09:48:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.81 on epoch=489
05/21/2022 09:48:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.80 on epoch=492
05/21/2022 09:48:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.84 on epoch=494
05/21/2022 09:48:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.84 on epoch=497
05/21/2022 09:48:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.80 on epoch=499
05/21/2022 09:48:29 - INFO - __main__ - Global step 2000 Train loss 0.82 Classification-F1 0.13398692810457516 on epoch=499
05/21/2022 09:48:30 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.80 on epoch=502
05/21/2022 09:48:31 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.76 on epoch=504
05/21/2022 09:48:33 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.82 on epoch=507
05/21/2022 09:48:34 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.80 on epoch=509
05/21/2022 09:48:35 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.80 on epoch=512
05/21/2022 09:48:36 - INFO - __main__ - Global step 2050 Train loss 0.80 Classification-F1 0.12456575682382134 on epoch=512
05/21/2022 09:48:37 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.82 on epoch=514
05/21/2022 09:48:38 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.77 on epoch=517
05/21/2022 09:48:40 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.80 on epoch=519
05/21/2022 09:48:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.79 on epoch=522
05/21/2022 09:48:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.74 on epoch=524
05/21/2022 09:48:43 - INFO - __main__ - Global step 2100 Train loss 0.78 Classification-F1 0.1171875 on epoch=524
05/21/2022 09:48:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.87 on epoch=527
05/21/2022 09:48:45 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.91 on epoch=529
05/21/2022 09:48:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.81 on epoch=532
05/21/2022 09:48:48 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.80 on epoch=534
05/21/2022 09:48:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.76 on epoch=537
05/21/2022 09:48:50 - INFO - __main__ - Global step 2150 Train loss 0.83 Classification-F1 0.12424242424242424 on epoch=537
05/21/2022 09:48:52 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.83 on epoch=539
05/21/2022 09:48:54 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.82 on epoch=542
05/21/2022 09:48:55 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.78 on epoch=544
05/21/2022 09:48:56 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.78 on epoch=547
05/21/2022 09:48:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.78 on epoch=549
05/21/2022 09:48:58 - INFO - __main__ - Global step 2200 Train loss 0.80 Classification-F1 0.1392857142857143 on epoch=549
05/21/2022 09:49:00 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.80 on epoch=552
05/21/2022 09:49:01 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.79 on epoch=554
05/21/2022 09:49:02 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.81 on epoch=557
05/21/2022 09:49:04 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.84 on epoch=559
05/21/2022 09:49:05 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.77 on epoch=562
05/21/2022 09:49:06 - INFO - __main__ - Global step 2250 Train loss 0.80 Classification-F1 0.1392857142857143 on epoch=562
05/21/2022 09:49:07 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.82 on epoch=564
05/21/2022 09:49:09 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.83 on epoch=567
05/21/2022 09:49:10 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.75 on epoch=569
05/21/2022 09:49:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.81 on epoch=572
05/21/2022 09:49:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.80 on epoch=574
05/21/2022 09:49:13 - INFO - __main__ - Global step 2300 Train loss 0.80 Classification-F1 0.09589041095890412 on epoch=574
05/21/2022 09:49:15 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.80 on epoch=577
05/21/2022 09:49:16 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.76 on epoch=579
05/21/2022 09:49:17 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.78 on epoch=582
05/21/2022 09:49:19 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.79 on epoch=584
05/21/2022 09:49:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.75 on epoch=587
05/21/2022 09:49:21 - INFO - __main__ - Global step 2350 Train loss 0.78 Classification-F1 0.1390013495276653 on epoch=587
05/21/2022 09:49:22 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.77 on epoch=589
05/21/2022 09:49:24 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.82 on epoch=592
05/21/2022 09:49:25 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.75 on epoch=594
05/21/2022 09:49:26 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.80 on epoch=597
05/21/2022 09:49:28 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.78 on epoch=599
05/21/2022 09:49:28 - INFO - __main__ - Global step 2400 Train loss 0.79 Classification-F1 0.1333333333333333 on epoch=599
05/21/2022 09:49:30 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.81 on epoch=602
05/21/2022 09:49:31 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.84 on epoch=604
05/21/2022 09:49:33 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.75 on epoch=607
05/21/2022 09:49:34 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.77 on epoch=609
05/21/2022 09:49:36 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.79 on epoch=612
05/21/2022 09:49:36 - INFO - __main__ - Global step 2450 Train loss 0.79 Classification-F1 0.1333333333333333 on epoch=612
05/21/2022 09:49:37 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.79 on epoch=614
05/21/2022 09:49:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.75 on epoch=617
05/21/2022 09:49:40 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.74 on epoch=619
05/21/2022 09:49:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.79 on epoch=622
05/21/2022 09:49:43 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.85 on epoch=624
05/21/2022 09:49:44 - INFO - __main__ - Global step 2500 Train loss 0.79 Classification-F1 0.09493670886075949 on epoch=624
05/21/2022 09:49:45 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.80 on epoch=627
05/21/2022 09:49:47 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.80 on epoch=629
05/21/2022 09:49:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.81 on epoch=632
05/21/2022 09:49:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.80 on epoch=634
05/21/2022 09:49:51 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.80 on epoch=637
05/21/2022 09:49:51 - INFO - __main__ - Global step 2550 Train loss 0.80 Classification-F1 0.1134453781512605 on epoch=637
05/21/2022 09:49:53 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.77 on epoch=639
05/21/2022 09:49:54 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.86 on epoch=642
05/21/2022 09:49:55 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.83 on epoch=644
05/21/2022 09:49:56 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.85 on epoch=647
05/21/2022 09:49:58 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.76 on epoch=649
05/21/2022 09:49:58 - INFO - __main__ - Global step 2600 Train loss 0.81 Classification-F1 0.12618595825426943 on epoch=649
05/21/2022 09:50:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.77 on epoch=652
05/21/2022 09:50:01 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.80 on epoch=654
05/21/2022 09:50:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.89 on epoch=657
05/21/2022 09:50:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.83 on epoch=659
05/21/2022 09:50:05 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.79 on epoch=662
05/21/2022 09:50:06 - INFO - __main__ - Global step 2650 Train loss 0.82 Classification-F1 0.11176470588235293 on epoch=662
05/21/2022 09:50:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.78 on epoch=664
05/21/2022 09:50:08 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.77 on epoch=667
05/21/2022 09:50:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.74 on epoch=669
05/21/2022 09:50:11 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.80 on epoch=672
05/21/2022 09:50:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.79 on epoch=674
05/21/2022 09:50:13 - INFO - __main__ - Global step 2700 Train loss 0.77 Classification-F1 0.13968957871396895 on epoch=674
05/21/2022 09:50:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.78 on epoch=677
05/21/2022 09:50:16 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.76 on epoch=679
05/21/2022 09:50:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.82 on epoch=682
05/21/2022 09:50:19 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.81 on epoch=684
05/21/2022 09:50:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.82 on epoch=687
05/21/2022 09:50:20 - INFO - __main__ - Global step 2750 Train loss 0.80 Classification-F1 0.12198332602018429 on epoch=687
05/21/2022 09:50:22 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.82 on epoch=689
05/21/2022 09:50:23 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.80 on epoch=692
05/21/2022 09:50:25 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.81 on epoch=694
05/21/2022 09:50:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.79 on epoch=697
05/21/2022 09:50:27 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.78 on epoch=699
05/21/2022 09:50:28 - INFO - __main__ - Global step 2800 Train loss 0.80 Classification-F1 0.12424242424242424 on epoch=699
05/21/2022 09:50:29 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.80 on epoch=702
05/21/2022 09:50:31 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.75 on epoch=704
05/21/2022 09:50:32 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.73 on epoch=707
05/21/2022 09:50:34 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.77 on epoch=709
05/21/2022 09:50:35 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.82 on epoch=712
05/21/2022 09:50:36 - INFO - __main__ - Global step 2850 Train loss 0.78 Classification-F1 0.1472743930371049 on epoch=712
05/21/2022 09:50:37 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.77 on epoch=714
05/21/2022 09:50:39 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.75 on epoch=717
05/21/2022 09:50:40 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.79 on epoch=719
05/21/2022 09:50:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.86 on epoch=722
05/21/2022 09:50:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.83 on epoch=724
05/21/2022 09:50:44 - INFO - __main__ - Global step 2900 Train loss 0.80 Classification-F1 0.12499999999999999 on epoch=724
05/21/2022 09:50:46 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.77 on epoch=727
05/21/2022 09:50:48 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.76 on epoch=729
05/21/2022 09:50:49 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.82 on epoch=732
05/21/2022 09:50:51 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.79 on epoch=734
05/21/2022 09:50:52 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.80 on epoch=737
05/21/2022 09:50:53 - INFO - __main__ - Global step 2950 Train loss 0.79 Classification-F1 0.11740890688259109 on epoch=737
05/21/2022 09:50:55 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.81 on epoch=739
05/21/2022 09:50:56 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.75 on epoch=742
05/21/2022 09:50:58 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.74 on epoch=744
05/21/2022 09:51:00 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.74 on epoch=747
05/21/2022 09:51:01 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.74 on epoch=749
05/21/2022 09:51:02 - INFO - __main__ - Global step 3000 Train loss 0.76 Classification-F1 0.10277777777777777 on epoch=749
05/21/2022 09:51:02 - INFO - __main__ - save last model!
05/21/2022 09:51:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 09:51:02 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 09:51:02 - INFO - __main__ - Printing 3 examples
05/21/2022 09:51:02 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 09:51:02 - INFO - __main__ - ['others']
05/21/2022 09:51:02 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 09:51:02 - INFO - __main__ - ['others']
05/21/2022 09:51:02 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 09:51:02 - INFO - __main__ - ['others']
05/21/2022 09:51:02 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:51:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:51:03 - INFO - __main__ - Printing 3 examples
05/21/2022 09:51:03 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 09:51:03 - INFO - __main__ - ['others']
05/21/2022 09:51:03 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 09:51:03 - INFO - __main__ - ['others']
05/21/2022 09:51:03 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 09:51:03 - INFO - __main__ - ['others']
05/21/2022 09:51:03 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:51:03 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:51:03 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 09:51:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:51:03 - INFO - __main__ - Printing 3 examples
05/21/2022 09:51:03 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/21/2022 09:51:03 - INFO - __main__ - ['others']
05/21/2022 09:51:03 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/21/2022 09:51:03 - INFO - __main__ - ['others']
05/21/2022 09:51:03 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/21/2022 09:51:03 - INFO - __main__ - ['others']
05/21/2022 09:51:03 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:51:03 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:51:03 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 09:51:06 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:51:10 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 09:51:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 09:51:10 - INFO - __main__ - Starting training!
05/21/2022 09:51:14 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 09:52:03 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_100_0.2_8_predictions.txt
05/21/2022 09:52:03 - INFO - __main__ - Classification-F1 on test data: 0.0446
05/21/2022 09:52:03 - INFO - __main__ - prefix=emo_16_100, lr=0.2, bsz=8, dev_performance=0.1778584392014519, test_performance=0.044621152681889875
05/21/2022 09:52:03 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.5, bsz=8 ...
05/21/2022 09:52:04 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:52:04 - INFO - __main__ - Printing 3 examples
05/21/2022 09:52:04 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 09:52:04 - INFO - __main__ - ['others']
05/21/2022 09:52:04 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 09:52:04 - INFO - __main__ - ['others']
05/21/2022 09:52:04 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 09:52:04 - INFO - __main__ - ['others']
05/21/2022 09:52:04 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:52:04 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:52:04 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 09:52:04 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:52:04 - INFO - __main__ - Printing 3 examples
05/21/2022 09:52:04 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/21/2022 09:52:04 - INFO - __main__ - ['others']
05/21/2022 09:52:04 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/21/2022 09:52:04 - INFO - __main__ - ['others']
05/21/2022 09:52:04 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/21/2022 09:52:04 - INFO - __main__ - ['others']
05/21/2022 09:52:04 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:52:04 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:52:04 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 09:52:11 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 09:52:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 09:52:11 - INFO - __main__ - Starting training!
05/21/2022 09:52:12 - INFO - __main__ - Step 10 Global step 10 Train loss 4.19 on epoch=2
05/21/2022 09:52:14 - INFO - __main__ - Step 20 Global step 20 Train loss 3.51 on epoch=4
05/21/2022 09:52:15 - INFO - __main__ - Step 30 Global step 30 Train loss 3.04 on epoch=7
05/21/2022 09:52:17 - INFO - __main__ - Step 40 Global step 40 Train loss 2.34 on epoch=9
05/21/2022 09:52:18 - INFO - __main__ - Step 50 Global step 50 Train loss 2.03 on epoch=12
05/21/2022 09:52:19 - INFO - __main__ - Global step 50 Train loss 3.02 Classification-F1 0.1 on epoch=12
05/21/2022 09:52:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/21/2022 09:52:21 - INFO - __main__ - Step 60 Global step 60 Train loss 1.62 on epoch=14
05/21/2022 09:52:23 - INFO - __main__ - Step 70 Global step 70 Train loss 1.43 on epoch=17
05/21/2022 09:52:24 - INFO - __main__ - Step 80 Global step 80 Train loss 1.24 on epoch=19
05/21/2022 09:52:25 - INFO - __main__ - Step 90 Global step 90 Train loss 1.17 on epoch=22
05/21/2022 09:52:27 - INFO - __main__ - Step 100 Global step 100 Train loss 1.05 on epoch=24
05/21/2022 09:52:27 - INFO - __main__ - Global step 100 Train loss 1.30 Classification-F1 0.2188339455073735 on epoch=24
05/21/2022 09:52:27 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.2188339455073735 on epoch=24, global_step=100
05/21/2022 09:52:29 - INFO - __main__ - Step 110 Global step 110 Train loss 1.03 on epoch=27
05/21/2022 09:52:30 - INFO - __main__ - Step 120 Global step 120 Train loss 1.06 on epoch=29
05/21/2022 09:52:32 - INFO - __main__ - Step 130 Global step 130 Train loss 1.04 on epoch=32
05/21/2022 09:52:33 - INFO - __main__ - Step 140 Global step 140 Train loss 1.03 on epoch=34
05/21/2022 09:52:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.96 on epoch=37
05/21/2022 09:52:35 - INFO - __main__ - Global step 150 Train loss 1.03 Classification-F1 0.18854427736006682 on epoch=37
05/21/2022 09:52:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.99 on epoch=39
05/21/2022 09:52:38 - INFO - __main__ - Step 170 Global step 170 Train loss 1.03 on epoch=42
05/21/2022 09:52:40 - INFO - __main__ - Step 180 Global step 180 Train loss 1.04 on epoch=44
05/21/2022 09:52:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.93 on epoch=47
05/21/2022 09:52:42 - INFO - __main__ - Step 200 Global step 200 Train loss 0.98 on epoch=49
05/21/2022 09:52:43 - INFO - __main__ - Global step 200 Train loss 1.00 Classification-F1 0.1 on epoch=49
05/21/2022 09:52:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.96 on epoch=52
05/21/2022 09:52:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.89 on epoch=54
05/21/2022 09:52:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.89 on epoch=57
05/21/2022 09:52:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.98 on epoch=59
05/21/2022 09:52:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.93 on epoch=62
05/21/2022 09:52:51 - INFO - __main__ - Global step 250 Train loss 0.93 Classification-F1 0.1 on epoch=62
05/21/2022 09:52:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.96 on epoch=64
05/21/2022 09:52:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.77 on epoch=67
05/21/2022 09:52:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.82 on epoch=69
05/21/2022 09:52:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.98 on epoch=72
05/21/2022 09:52:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.92 on epoch=74
05/21/2022 09:52:59 - INFO - __main__ - Global step 300 Train loss 0.89 Classification-F1 0.1 on epoch=74
05/21/2022 09:53:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.97 on epoch=77
05/21/2022 09:53:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.83 on epoch=79
05/21/2022 09:53:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.82 on epoch=82
05/21/2022 09:53:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.98 on epoch=84
05/21/2022 09:53:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.87 on epoch=87
05/21/2022 09:53:06 - INFO - __main__ - Global step 350 Train loss 0.89 Classification-F1 0.1 on epoch=87
05/21/2022 09:53:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.85 on epoch=89
05/21/2022 09:53:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.93 on epoch=92
05/21/2022 09:53:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.89 on epoch=94
05/21/2022 09:53:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.83 on epoch=97
05/21/2022 09:53:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.99 on epoch=99
05/21/2022 09:53:14 - INFO - __main__ - Global step 400 Train loss 0.90 Classification-F1 0.09615384615384615 on epoch=99
05/21/2022 09:53:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.96 on epoch=102
05/21/2022 09:53:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.84 on epoch=104
05/21/2022 09:53:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.90 on epoch=107
05/21/2022 09:53:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.87 on epoch=109
05/21/2022 09:53:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.88 on epoch=112
05/21/2022 09:53:22 - INFO - __main__ - Global step 450 Train loss 0.89 Classification-F1 0.1 on epoch=112
05/21/2022 09:53:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.88 on epoch=114
05/21/2022 09:53:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.87 on epoch=117
05/21/2022 09:53:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.90 on epoch=119
05/21/2022 09:53:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.88 on epoch=122
05/21/2022 09:53:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.83 on epoch=124
05/21/2022 09:53:30 - INFO - __main__ - Global step 500 Train loss 0.87 Classification-F1 0.09868421052631579 on epoch=124
05/21/2022 09:53:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.83 on epoch=127
05/21/2022 09:53:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.91 on epoch=129
05/21/2022 09:53:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.80 on epoch=132
05/21/2022 09:53:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.81 on epoch=134
05/21/2022 09:53:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=137
05/21/2022 09:53:36 - INFO - __main__ - Global step 550 Train loss 0.83 Classification-F1 0.1 on epoch=137
05/21/2022 09:53:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.91 on epoch=139
05/21/2022 09:53:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.87 on epoch=142
05/21/2022 09:53:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.81 on epoch=144
05/21/2022 09:53:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.87 on epoch=147
05/21/2022 09:53:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.90 on epoch=149
05/21/2022 09:53:44 - INFO - __main__ - Global step 600 Train loss 0.87 Classification-F1 0.14210526315789473 on epoch=149
05/21/2022 09:53:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.87 on epoch=152
05/21/2022 09:53:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.91 on epoch=154
05/21/2022 09:53:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.89 on epoch=157
05/21/2022 09:53:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.82 on epoch=159
05/21/2022 09:53:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.80 on epoch=162
05/21/2022 09:53:53 - INFO - __main__ - Global step 650 Train loss 0.86 Classification-F1 0.1 on epoch=162
05/21/2022 09:53:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.89 on epoch=164
05/21/2022 09:53:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.86 on epoch=167
05/21/2022 09:53:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.82 on epoch=169
05/21/2022 09:53:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.90 on epoch=172
05/21/2022 09:54:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.77 on epoch=174
05/21/2022 09:54:01 - INFO - __main__ - Global step 700 Train loss 0.85 Classification-F1 0.1476190476190476 on epoch=174
05/21/2022 09:54:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.83 on epoch=177
05/21/2022 09:54:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.87 on epoch=179
05/21/2022 09:54:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.79 on epoch=182
05/21/2022 09:54:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.79 on epoch=184
05/21/2022 09:54:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.83 on epoch=187
05/21/2022 09:54:08 - INFO - __main__ - Global step 750 Train loss 0.82 Classification-F1 0.1255656108597285 on epoch=187
05/21/2022 09:54:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.86 on epoch=189
05/21/2022 09:54:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.89 on epoch=192
05/21/2022 09:54:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.90 on epoch=194
05/21/2022 09:54:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.83 on epoch=197
05/21/2022 09:54:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.78 on epoch=199
05/21/2022 09:54:16 - INFO - __main__ - Global step 800 Train loss 0.85 Classification-F1 0.13154929577464788 on epoch=199
05/21/2022 09:54:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.82 on epoch=202
05/21/2022 09:54:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.89 on epoch=204
05/21/2022 09:54:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.79 on epoch=207
05/21/2022 09:54:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.79 on epoch=209
05/21/2022 09:54:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.89 on epoch=212
05/21/2022 09:54:24 - INFO - __main__ - Global step 850 Train loss 0.84 Classification-F1 0.1875 on epoch=212
05/21/2022 09:54:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.80 on epoch=214
05/21/2022 09:54:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.74 on epoch=217
05/21/2022 09:54:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.77 on epoch=219
05/21/2022 09:54:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.77 on epoch=222
05/21/2022 09:54:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.83 on epoch=224
05/21/2022 09:54:31 - INFO - __main__ - Global step 900 Train loss 0.78 Classification-F1 0.09868421052631579 on epoch=224
05/21/2022 09:54:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.77 on epoch=227
05/21/2022 09:54:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.78 on epoch=229
05/21/2022 09:54:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.83 on epoch=232
05/21/2022 09:54:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.83 on epoch=234
05/21/2022 09:54:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.85 on epoch=237
05/21/2022 09:54:39 - INFO - __main__ - Global step 950 Train loss 0.81 Classification-F1 0.1 on epoch=237
05/21/2022 09:54:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.77 on epoch=239
05/21/2022 09:54:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.84 on epoch=242
05/21/2022 09:54:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.87 on epoch=244
05/21/2022 09:54:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.78 on epoch=247
05/21/2022 09:54:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.85 on epoch=249
05/21/2022 09:54:46 - INFO - __main__ - Global step 1000 Train loss 0.82 Classification-F1 0.1515492957746479 on epoch=249
05/21/2022 09:54:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.79 on epoch=252
05/21/2022 09:54:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.85 on epoch=254
05/21/2022 09:54:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.84 on epoch=257
05/21/2022 09:54:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.78 on epoch=259
05/21/2022 09:54:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.82 on epoch=262
05/21/2022 09:54:53 - INFO - __main__ - Global step 1050 Train loss 0.82 Classification-F1 0.09493670886075949 on epoch=262
05/21/2022 09:54:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.81 on epoch=264
05/21/2022 09:54:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.84 on epoch=267
05/21/2022 09:54:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.76 on epoch=269
05/21/2022 09:54:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.78 on epoch=272
05/21/2022 09:54:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.81 on epoch=274
05/21/2022 09:55:00 - INFO - __main__ - Global step 1100 Train loss 0.80 Classification-F1 0.11722488038277512 on epoch=274
05/21/2022 09:55:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.81 on epoch=277
05/21/2022 09:55:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.86 on epoch=279
05/21/2022 09:55:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.86 on epoch=282
05/21/2022 09:55:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.83 on epoch=284
05/21/2022 09:55:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.81 on epoch=287
05/21/2022 09:55:07 - INFO - __main__ - Global step 1150 Train loss 0.83 Classification-F1 0.11714285714285715 on epoch=287
05/21/2022 09:55:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.78 on epoch=289
05/21/2022 09:55:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.81 on epoch=292
05/21/2022 09:55:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.85 on epoch=294
05/21/2022 09:55:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.84 on epoch=297
05/21/2022 09:55:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.75 on epoch=299
05/21/2022 09:55:15 - INFO - __main__ - Global step 1200 Train loss 0.81 Classification-F1 0.1337028824833703 on epoch=299
05/21/2022 09:55:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.78 on epoch=302
05/21/2022 09:55:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.77 on epoch=304
05/21/2022 09:55:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.82 on epoch=307
05/21/2022 09:55:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.77 on epoch=309
05/21/2022 09:55:22 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.82 on epoch=312
05/21/2022 09:55:23 - INFO - __main__ - Global step 1250 Train loss 0.79 Classification-F1 0.1332923832923833 on epoch=312
05/21/2022 09:55:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.82 on epoch=314
05/21/2022 09:55:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.81 on epoch=317
05/21/2022 09:55:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.76 on epoch=319
05/21/2022 09:55:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.81 on epoch=322
05/21/2022 09:55:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.81 on epoch=324
05/21/2022 09:55:30 - INFO - __main__ - Global step 1300 Train loss 0.80 Classification-F1 0.09615384615384615 on epoch=324
05/21/2022 09:55:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.85 on epoch=327
05/21/2022 09:55:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.84 on epoch=329
05/21/2022 09:55:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.79 on epoch=332
05/21/2022 09:55:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.76 on epoch=334
05/21/2022 09:55:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.74 on epoch=337
05/21/2022 09:55:38 - INFO - __main__ - Global step 1350 Train loss 0.80 Classification-F1 0.2491341991341991 on epoch=337
05/21/2022 09:55:38 - INFO - __main__ - Saving model with best Classification-F1: 0.2188339455073735 -> 0.2491341991341991 on epoch=337, global_step=1350
05/21/2022 09:55:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.79 on epoch=339
05/21/2022 09:55:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.82 on epoch=342
05/21/2022 09:55:42 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.81 on epoch=344
05/21/2022 09:55:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.82 on epoch=347
05/21/2022 09:55:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.76 on epoch=349
05/21/2022 09:55:45 - INFO - __main__ - Global step 1400 Train loss 0.80 Classification-F1 0.12450704225352113 on epoch=349
05/21/2022 09:55:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.77 on epoch=352
05/21/2022 09:55:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.80 on epoch=354
05/21/2022 09:55:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.87 on epoch=357
05/21/2022 09:55:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.78 on epoch=359
05/21/2022 09:55:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.78 on epoch=362
05/21/2022 09:55:52 - INFO - __main__ - Global step 1450 Train loss 0.80 Classification-F1 0.18999453253143794 on epoch=362
05/21/2022 09:55:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.74 on epoch=364
05/21/2022 09:55:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.76 on epoch=367
05/21/2022 09:55:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.77 on epoch=369
05/21/2022 09:55:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.85 on epoch=372
05/21/2022 09:55:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.84 on epoch=374
05/21/2022 09:56:00 - INFO - __main__ - Global step 1500 Train loss 0.79 Classification-F1 0.16373636148007592 on epoch=374
05/21/2022 09:56:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.82 on epoch=377
05/21/2022 09:56:03 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.83 on epoch=379
05/21/2022 09:56:04 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.85 on epoch=382
05/21/2022 09:56:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.75 on epoch=384
05/21/2022 09:56:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.84 on epoch=387
05/21/2022 09:56:07 - INFO - __main__ - Global step 1550 Train loss 0.82 Classification-F1 0.11969993476842793 on epoch=387
05/21/2022 09:56:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.77 on epoch=389
05/21/2022 09:56:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.82 on epoch=392
05/21/2022 09:56:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.72 on epoch=394
05/21/2022 09:56:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.74 on epoch=397
05/21/2022 09:56:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.86 on epoch=399
05/21/2022 09:56:15 - INFO - __main__ - Global step 1600 Train loss 0.78 Classification-F1 0.11674718196457327 on epoch=399
05/21/2022 09:56:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.79 on epoch=402
05/21/2022 09:56:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.85 on epoch=404
05/21/2022 09:56:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.85 on epoch=407
05/21/2022 09:56:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.80 on epoch=409
05/21/2022 09:56:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.83 on epoch=412
05/21/2022 09:56:23 - INFO - __main__ - Global step 1650 Train loss 0.82 Classification-F1 0.20907738095238096 on epoch=412
05/21/2022 09:56:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.84 on epoch=414
05/21/2022 09:56:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.81 on epoch=417
05/21/2022 09:56:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.81 on epoch=419
05/21/2022 09:56:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.78 on epoch=422
05/21/2022 09:56:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.80 on epoch=424
05/21/2022 09:56:30 - INFO - __main__ - Global step 1700 Train loss 0.81 Classification-F1 0.17011224779766979 on epoch=424
05/21/2022 09:56:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.78 on epoch=427
05/21/2022 09:56:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.74 on epoch=429
05/21/2022 09:56:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.79 on epoch=432
05/21/2022 09:56:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.79 on epoch=434
05/21/2022 09:56:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.82 on epoch=437
05/21/2022 09:56:37 - INFO - __main__ - Global step 1750 Train loss 0.78 Classification-F1 0.2294764358071388 on epoch=437
05/21/2022 09:56:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.82 on epoch=439
05/21/2022 09:56:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.75 on epoch=442
05/21/2022 09:56:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.78 on epoch=444
05/21/2022 09:56:42 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.77 on epoch=447
05/21/2022 09:56:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.79 on epoch=449
05/21/2022 09:56:45 - INFO - __main__ - Global step 1800 Train loss 0.78 Classification-F1 0.1478328173374613 on epoch=449
05/21/2022 09:56:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.80 on epoch=452
05/21/2022 09:56:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.75 on epoch=454
05/21/2022 09:56:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.81 on epoch=457
05/21/2022 09:56:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.79 on epoch=459
05/21/2022 09:56:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.77 on epoch=462
05/21/2022 09:56:52 - INFO - __main__ - Global step 1850 Train loss 0.78 Classification-F1 0.2267949767949768 on epoch=462
05/21/2022 09:56:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.74 on epoch=464
05/21/2022 09:56:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.77 on epoch=467
05/21/2022 09:56:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.80 on epoch=469
05/21/2022 09:56:58 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.81 on epoch=472
05/21/2022 09:57:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.81 on epoch=474
05/21/2022 09:57:00 - INFO - __main__ - Global step 1900 Train loss 0.78 Classification-F1 0.11274509803921567 on epoch=474
05/21/2022 09:57:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.82 on epoch=477
05/21/2022 09:57:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.81 on epoch=479
05/21/2022 09:57:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.84 on epoch=482
05/21/2022 09:57:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.74 on epoch=484
05/21/2022 09:57:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.76 on epoch=487
05/21/2022 09:57:09 - INFO - __main__ - Global step 1950 Train loss 0.79 Classification-F1 0.11415362731152205 on epoch=487
05/21/2022 09:57:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.75 on epoch=489
05/21/2022 09:57:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.76 on epoch=492
05/21/2022 09:57:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.70 on epoch=494
05/21/2022 09:57:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.77 on epoch=497
05/21/2022 09:57:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.78 on epoch=499
05/21/2022 09:57:16 - INFO - __main__ - Global step 2000 Train loss 0.75 Classification-F1 0.1480294483920592 on epoch=499
05/21/2022 09:57:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.78 on epoch=502
05/21/2022 09:57:19 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.78 on epoch=504
05/21/2022 09:57:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.75 on epoch=507
05/21/2022 09:57:22 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.78 on epoch=509
05/21/2022 09:57:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.81 on epoch=512
05/21/2022 09:57:23 - INFO - __main__ - Global step 2050 Train loss 0.78 Classification-F1 0.10126582278481013 on epoch=512
05/21/2022 09:57:25 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.83 on epoch=514
05/21/2022 09:57:26 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.82 on epoch=517
05/21/2022 09:57:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.81 on epoch=519
05/21/2022 09:57:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.76 on epoch=522
05/21/2022 09:57:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.78 on epoch=524
05/21/2022 09:57:31 - INFO - __main__ - Global step 2100 Train loss 0.80 Classification-F1 0.154093567251462 on epoch=524
05/21/2022 09:57:32 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.78 on epoch=527
05/21/2022 09:57:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.73 on epoch=529
05/21/2022 09:57:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.82 on epoch=532
05/21/2022 09:57:36 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.72 on epoch=534
05/21/2022 09:57:37 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.80 on epoch=537
05/21/2022 09:57:38 - INFO - __main__ - Global step 2150 Train loss 0.77 Classification-F1 0.19667767369768852 on epoch=537
05/21/2022 09:57:39 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.80 on epoch=539
05/21/2022 09:57:41 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.79 on epoch=542
05/21/2022 09:57:42 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.78 on epoch=544
05/21/2022 09:57:44 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.76 on epoch=547
05/21/2022 09:57:45 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.90 on epoch=549
05/21/2022 09:57:45 - INFO - __main__ - Global step 2200 Train loss 0.81 Classification-F1 0.12518037518037517 on epoch=549
05/21/2022 09:57:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.77 on epoch=552
05/21/2022 09:57:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.75 on epoch=554
05/21/2022 09:57:49 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.81 on epoch=557
05/21/2022 09:57:51 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.76 on epoch=559
05/21/2022 09:57:52 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.81 on epoch=562
05/21/2022 09:57:53 - INFO - __main__ - Global step 2250 Train loss 0.78 Classification-F1 0.2268728004022122 on epoch=562
05/21/2022 09:57:55 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.75 on epoch=564
05/21/2022 09:57:56 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.81 on epoch=567
05/21/2022 09:57:58 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.78 on epoch=569
05/21/2022 09:57:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.79 on epoch=572
05/21/2022 09:58:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.78 on epoch=574
05/21/2022 09:58:01 - INFO - __main__ - Global step 2300 Train loss 0.78 Classification-F1 0.18581399735898887 on epoch=574
05/21/2022 09:58:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.72 on epoch=577
05/21/2022 09:58:04 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.73 on epoch=579
05/21/2022 09:58:06 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.71 on epoch=582
05/21/2022 09:58:07 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.78 on epoch=584
05/21/2022 09:58:08 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.76 on epoch=587
05/21/2022 09:58:09 - INFO - __main__ - Global step 2350 Train loss 0.74 Classification-F1 0.18301435406698563 on epoch=587
05/21/2022 09:58:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.86 on epoch=589
05/21/2022 09:58:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.68 on epoch=592
05/21/2022 09:58:13 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.77 on epoch=594
05/21/2022 09:58:15 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.77 on epoch=597
05/21/2022 09:58:17 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.74 on epoch=599
05/21/2022 09:58:17 - INFO - __main__ - Global step 2400 Train loss 0.76 Classification-F1 0.15441176470588236 on epoch=599
05/21/2022 09:58:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.82 on epoch=602
05/21/2022 09:58:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.73 on epoch=604
05/21/2022 09:58:21 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.77 on epoch=607
05/21/2022 09:58:23 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.75 on epoch=609
05/21/2022 09:58:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.70 on epoch=612
05/21/2022 09:58:25 - INFO - __main__ - Global step 2450 Train loss 0.75 Classification-F1 0.17484393757503003 on epoch=612
05/21/2022 09:58:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.76 on epoch=614
05/21/2022 09:58:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.83 on epoch=617
05/21/2022 09:58:29 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.78 on epoch=619
05/21/2022 09:58:31 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.86 on epoch=622
05/21/2022 09:58:32 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.75 on epoch=624
05/21/2022 09:58:32 - INFO - __main__ - Global step 2500 Train loss 0.80 Classification-F1 0.12368421052631579 on epoch=624
05/21/2022 09:58:34 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.72 on epoch=627
05/21/2022 09:58:35 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.77 on epoch=629
05/21/2022 09:58:36 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.77 on epoch=632
05/21/2022 09:58:38 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.75 on epoch=634
05/21/2022 09:58:39 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.74 on epoch=637
05/21/2022 09:58:40 - INFO - __main__ - Global step 2550 Train loss 0.75 Classification-F1 0.12275641025641025 on epoch=637
05/21/2022 09:58:41 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.83 on epoch=639
05/21/2022 09:58:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.78 on epoch=642
05/21/2022 09:58:44 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.74 on epoch=644
05/21/2022 09:58:46 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.73 on epoch=647
05/21/2022 09:58:47 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.74 on epoch=649
05/21/2022 09:58:48 - INFO - __main__ - Global step 2600 Train loss 0.77 Classification-F1 0.13497442455242964 on epoch=649
05/21/2022 09:58:49 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.85 on epoch=652
05/21/2022 09:58:51 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.82 on epoch=654
05/21/2022 09:58:52 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.72 on epoch=657
05/21/2022 09:58:54 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.78 on epoch=659
05/21/2022 09:58:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.76 on epoch=662
05/21/2022 09:58:56 - INFO - __main__ - Global step 2650 Train loss 0.78 Classification-F1 0.2961301912069176 on epoch=662
05/21/2022 09:58:56 - INFO - __main__ - Saving model with best Classification-F1: 0.2491341991341991 -> 0.2961301912069176 on epoch=662, global_step=2650
05/21/2022 09:58:57 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.77 on epoch=664
05/21/2022 09:58:58 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.76 on epoch=667
05/21/2022 09:59:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.76 on epoch=669
05/21/2022 09:59:01 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.78 on epoch=672
05/21/2022 09:59:03 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.76 on epoch=674
05/21/2022 09:59:03 - INFO - __main__ - Global step 2700 Train loss 0.77 Classification-F1 0.16630681818181817 on epoch=674
05/21/2022 09:59:05 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.71 on epoch=677
05/21/2022 09:59:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.71 on epoch=679
05/21/2022 09:59:08 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.75 on epoch=682
05/21/2022 09:59:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.74 on epoch=684
05/21/2022 09:59:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.78 on epoch=687
05/21/2022 09:59:11 - INFO - __main__ - Global step 2750 Train loss 0.74 Classification-F1 0.22233201581027665 on epoch=687
05/21/2022 09:59:13 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.74 on epoch=689
05/21/2022 09:59:14 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.73 on epoch=692
05/21/2022 09:59:15 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.72 on epoch=694
05/21/2022 09:59:16 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.80 on epoch=697
05/21/2022 09:59:18 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.73 on epoch=699
05/21/2022 09:59:18 - INFO - __main__ - Global step 2800 Train loss 0.75 Classification-F1 0.19318181818181815 on epoch=699
05/21/2022 09:59:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.68 on epoch=702
05/21/2022 09:59:21 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.82 on epoch=704
05/21/2022 09:59:22 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.72 on epoch=707
05/21/2022 09:59:23 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.81 on epoch=709
05/21/2022 09:59:24 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.76 on epoch=712
05/21/2022 09:59:25 - INFO - __main__ - Global step 2850 Train loss 0.76 Classification-F1 0.2578099838969404 on epoch=712
05/21/2022 09:59:26 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.76 on epoch=714
05/21/2022 09:59:28 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.72 on epoch=717
05/21/2022 09:59:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.73 on epoch=719
05/21/2022 09:59:30 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.72 on epoch=722
05/21/2022 09:59:32 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.76 on epoch=724
05/21/2022 09:59:33 - INFO - __main__ - Global step 2900 Train loss 0.74 Classification-F1 0.21735578564114233 on epoch=724
05/21/2022 09:59:34 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.71 on epoch=727
05/21/2022 09:59:35 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.65 on epoch=729
05/21/2022 09:59:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.83 on epoch=732
05/21/2022 09:59:38 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.79 on epoch=734
05/21/2022 09:59:40 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.70 on epoch=737
05/21/2022 09:59:40 - INFO - __main__ - Global step 2950 Train loss 0.74 Classification-F1 0.25049261083743846 on epoch=737
05/21/2022 09:59:41 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.79 on epoch=739
05/21/2022 09:59:43 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.75 on epoch=742
05/21/2022 09:59:44 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.81 on epoch=744
05/21/2022 09:59:46 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.75 on epoch=747
05/21/2022 09:59:47 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.71 on epoch=749
05/21/2022 09:59:48 - INFO - __main__ - Global step 3000 Train loss 0.76 Classification-F1 0.2299218897044984 on epoch=749
05/21/2022 09:59:48 - INFO - __main__ - save last model!
05/21/2022 09:59:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 09:59:48 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 09:59:48 - INFO - __main__ - Printing 3 examples
05/21/2022 09:59:48 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 09:59:48 - INFO - __main__ - ['others']
05/21/2022 09:59:48 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 09:59:48 - INFO - __main__ - ['others']
05/21/2022 09:59:48 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 09:59:48 - INFO - __main__ - ['others']
05/21/2022 09:59:48 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:59:49 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:59:49 - INFO - __main__ - Printing 3 examples
05/21/2022 09:59:49 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 09:59:49 - INFO - __main__ - ['others']
05/21/2022 09:59:49 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 09:59:49 - INFO - __main__ - ['others']
05/21/2022 09:59:49 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 09:59:49 - INFO - __main__ - ['others']
05/21/2022 09:59:49 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:59:49 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:59:49 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 09:59:49 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 09:59:49 - INFO - __main__ - Printing 3 examples
05/21/2022 09:59:49 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/21/2022 09:59:49 - INFO - __main__ - ['others']
05/21/2022 09:59:49 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/21/2022 09:59:49 - INFO - __main__ - ['others']
05/21/2022 09:59:49 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/21/2022 09:59:49 - INFO - __main__ - ['others']
05/21/2022 09:59:49 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:59:49 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:59:49 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 09:59:50 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:59:56 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 09:59:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 09:59:56 - INFO - __main__ - Starting training!
05/21/2022 09:59:58 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 10:00:49 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_13_0.5_8_predictions.txt
05/21/2022 10:00:49 - INFO - __main__ - Classification-F1 on test data: 0.1207
05/21/2022 10:00:50 - INFO - __main__ - prefix=emo_16_13, lr=0.5, bsz=8, dev_performance=0.2961301912069176, test_performance=0.12066386332039226
05/21/2022 10:00:50 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.4, bsz=8 ...
05/21/2022 10:00:51 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:00:51 - INFO - __main__ - Printing 3 examples
05/21/2022 10:00:51 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 10:00:51 - INFO - __main__ - ['others']
05/21/2022 10:00:51 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 10:00:51 - INFO - __main__ - ['others']
05/21/2022 10:00:51 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 10:00:51 - INFO - __main__ - ['others']
05/21/2022 10:00:51 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:00:51 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:00:51 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 10:00:51 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:00:51 - INFO - __main__ - Printing 3 examples
05/21/2022 10:00:51 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/21/2022 10:00:51 - INFO - __main__ - ['others']
05/21/2022 10:00:51 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/21/2022 10:00:51 - INFO - __main__ - ['others']
05/21/2022 10:00:51 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/21/2022 10:00:51 - INFO - __main__ - ['others']
05/21/2022 10:00:51 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:00:51 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:00:51 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 10:00:57 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 10:00:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 10:00:57 - INFO - __main__ - Starting training!
05/21/2022 10:00:59 - INFO - __main__ - Step 10 Global step 10 Train loss 4.33 on epoch=2
05/21/2022 10:01:00 - INFO - __main__ - Step 20 Global step 20 Train loss 3.67 on epoch=4
05/21/2022 10:01:01 - INFO - __main__ - Step 30 Global step 30 Train loss 3.34 on epoch=7
05/21/2022 10:01:03 - INFO - __main__ - Step 40 Global step 40 Train loss 2.56 on epoch=9
05/21/2022 10:01:04 - INFO - __main__ - Step 50 Global step 50 Train loss 2.22 on epoch=12
05/21/2022 10:01:05 - INFO - __main__ - Global step 50 Train loss 3.23 Classification-F1 0.1 on epoch=12
05/21/2022 10:01:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/21/2022 10:01:06 - INFO - __main__ - Step 60 Global step 60 Train loss 1.75 on epoch=14
05/21/2022 10:01:07 - INFO - __main__ - Step 70 Global step 70 Train loss 1.64 on epoch=17
05/21/2022 10:01:08 - INFO - __main__ - Step 80 Global step 80 Train loss 1.34 on epoch=19
05/21/2022 10:01:10 - INFO - __main__ - Step 90 Global step 90 Train loss 1.38 on epoch=22
05/21/2022 10:01:11 - INFO - __main__ - Step 100 Global step 100 Train loss 1.19 on epoch=24
05/21/2022 10:01:12 - INFO - __main__ - Global step 100 Train loss 1.46 Classification-F1 0.11805555555555555 on epoch=24
05/21/2022 10:01:12 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.11805555555555555 on epoch=24, global_step=100
05/21/2022 10:01:13 - INFO - __main__ - Step 110 Global step 110 Train loss 1.09 on epoch=27
05/21/2022 10:01:15 - INFO - __main__ - Step 120 Global step 120 Train loss 1.08 on epoch=29
05/21/2022 10:01:16 - INFO - __main__ - Step 130 Global step 130 Train loss 1.15 on epoch=32
05/21/2022 10:01:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.98 on epoch=34
05/21/2022 10:01:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.89 on epoch=37
05/21/2022 10:01:19 - INFO - __main__ - Global step 150 Train loss 1.04 Classification-F1 0.09615384615384615 on epoch=37
05/21/2022 10:01:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.96 on epoch=39
05/21/2022 10:01:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.95 on epoch=42
05/21/2022 10:01:23 - INFO - __main__ - Step 180 Global step 180 Train loss 1.08 on epoch=44
05/21/2022 10:01:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.94 on epoch=47
05/21/2022 10:01:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.93 on epoch=49
05/21/2022 10:01:26 - INFO - __main__ - Global step 200 Train loss 0.97 Classification-F1 0.1659804426145136 on epoch=49
05/21/2022 10:01:27 - INFO - __main__ - Saving model with best Classification-F1: 0.11805555555555555 -> 0.1659804426145136 on epoch=49, global_step=200
05/21/2022 10:01:28 - INFO - __main__ - Step 210 Global step 210 Train loss 1.05 on epoch=52
05/21/2022 10:01:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.90 on epoch=54
05/21/2022 10:01:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.95 on epoch=57
05/21/2022 10:01:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.95 on epoch=59
05/21/2022 10:01:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.93 on epoch=62
05/21/2022 10:01:34 - INFO - __main__ - Global step 250 Train loss 0.96 Classification-F1 0.1 on epoch=62
05/21/2022 10:01:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.90 on epoch=64
05/21/2022 10:01:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.94 on epoch=67
05/21/2022 10:01:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.91 on epoch=69
05/21/2022 10:01:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.94 on epoch=72
05/21/2022 10:01:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.97 on epoch=74
05/21/2022 10:01:42 - INFO - __main__ - Global step 300 Train loss 0.93 Classification-F1 0.11507936507936507 on epoch=74
05/21/2022 10:01:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.89 on epoch=77
05/21/2022 10:01:44 - INFO - __main__ - Step 320 Global step 320 Train loss 1.00 on epoch=79
05/21/2022 10:01:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.95 on epoch=82
05/21/2022 10:01:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.96 on epoch=84
05/21/2022 10:01:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.85 on epoch=87
05/21/2022 10:01:48 - INFO - __main__ - Global step 350 Train loss 0.93 Classification-F1 0.1 on epoch=87
05/21/2022 10:01:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.90 on epoch=89
05/21/2022 10:01:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.91 on epoch=92
05/21/2022 10:01:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.83 on epoch=94
05/21/2022 10:01:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.79 on epoch=97
05/21/2022 10:01:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.91 on epoch=99
05/21/2022 10:01:56 - INFO - __main__ - Global step 400 Train loss 0.87 Classification-F1 0.1 on epoch=99
05/21/2022 10:01:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.91 on epoch=102
05/21/2022 10:01:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.94 on epoch=104
05/21/2022 10:02:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.93 on epoch=107
05/21/2022 10:02:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.85 on epoch=109
05/21/2022 10:02:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.83 on epoch=112
05/21/2022 10:02:03 - INFO - __main__ - Global step 450 Train loss 0.89 Classification-F1 0.1 on epoch=112
05/21/2022 10:02:05 - INFO - __main__ - Step 460 Global step 460 Train loss 1.01 on epoch=114
05/21/2022 10:02:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.92 on epoch=117
05/21/2022 10:02:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.92 on epoch=119
05/21/2022 10:02:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.90 on epoch=122
05/21/2022 10:02:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.91 on epoch=124
05/21/2022 10:02:11 - INFO - __main__ - Global step 500 Train loss 0.93 Classification-F1 0.10256410256410256 on epoch=124
05/21/2022 10:02:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.98 on epoch=127
05/21/2022 10:02:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.91 on epoch=129
05/21/2022 10:02:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.88 on epoch=132
05/21/2022 10:02:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.82 on epoch=134
05/21/2022 10:02:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=137
05/21/2022 10:02:18 - INFO - __main__ - Global step 550 Train loss 0.88 Classification-F1 0.1 on epoch=137
05/21/2022 10:02:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.84 on epoch=139
05/21/2022 10:02:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.92 on epoch=142
05/21/2022 10:02:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.84 on epoch=144
05/21/2022 10:02:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.84 on epoch=147
05/21/2022 10:02:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.85 on epoch=149
05/21/2022 10:02:25 - INFO - __main__ - Global step 600 Train loss 0.86 Classification-F1 0.1 on epoch=149
05/21/2022 10:02:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.89 on epoch=152
05/21/2022 10:02:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.84 on epoch=154
05/21/2022 10:02:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.81 on epoch=157
05/21/2022 10:02:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.82 on epoch=159
05/21/2022 10:02:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.79 on epoch=162
05/21/2022 10:02:32 - INFO - __main__ - Global step 650 Train loss 0.83 Classification-F1 0.1 on epoch=162
05/21/2022 10:02:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.85 on epoch=164
05/21/2022 10:02:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.89 on epoch=167
05/21/2022 10:02:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.79 on epoch=169
05/21/2022 10:02:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.86 on epoch=172
05/21/2022 10:02:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.79 on epoch=174
05/21/2022 10:02:40 - INFO - __main__ - Global step 700 Train loss 0.84 Classification-F1 0.1 on epoch=174
05/21/2022 10:02:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.85 on epoch=177
05/21/2022 10:02:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.89 on epoch=179
05/21/2022 10:02:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.83 on epoch=182
05/21/2022 10:02:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.78 on epoch=184
05/21/2022 10:02:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.93 on epoch=187
05/21/2022 10:02:46 - INFO - __main__ - Global step 750 Train loss 0.86 Classification-F1 0.12590579710144928 on epoch=187
05/21/2022 10:02:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.85 on epoch=189
05/21/2022 10:02:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.83 on epoch=192
05/21/2022 10:02:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.83 on epoch=194
05/21/2022 10:02:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.91 on epoch=197
05/21/2022 10:02:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.82 on epoch=199
05/21/2022 10:02:54 - INFO - __main__ - Global step 800 Train loss 0.85 Classification-F1 0.1 on epoch=199
05/21/2022 10:02:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.82 on epoch=202
05/21/2022 10:02:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.76 on epoch=204
05/21/2022 10:02:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.83 on epoch=207
05/21/2022 10:02:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.82 on epoch=209
05/21/2022 10:03:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.89 on epoch=212
05/21/2022 10:03:00 - INFO - __main__ - Global step 850 Train loss 0.82 Classification-F1 0.1 on epoch=212
05/21/2022 10:03:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.81 on epoch=214
05/21/2022 10:03:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.75 on epoch=217
05/21/2022 10:03:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.84 on epoch=219
05/21/2022 10:03:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.78 on epoch=222
05/21/2022 10:03:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.82 on epoch=224
05/21/2022 10:03:07 - INFO - __main__ - Global step 900 Train loss 0.80 Classification-F1 0.1 on epoch=224
05/21/2022 10:03:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.83 on epoch=227
05/21/2022 10:03:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.81 on epoch=229
05/21/2022 10:03:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.82 on epoch=232
05/21/2022 10:03:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.81 on epoch=234
05/21/2022 10:03:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.85 on epoch=237
05/21/2022 10:03:14 - INFO - __main__ - Global step 950 Train loss 0.82 Classification-F1 0.1 on epoch=237
05/21/2022 10:03:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.89 on epoch=239
05/21/2022 10:03:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.85 on epoch=242
05/21/2022 10:03:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.81 on epoch=244
05/21/2022 10:03:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.75 on epoch=247
05/21/2022 10:03:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.84 on epoch=249
05/21/2022 10:03:21 - INFO - __main__ - Global step 1000 Train loss 0.83 Classification-F1 0.08974358974358974 on epoch=249
05/21/2022 10:03:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.77 on epoch=252
05/21/2022 10:03:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.87 on epoch=254
05/21/2022 10:03:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.80 on epoch=257
05/21/2022 10:03:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.78 on epoch=259
05/21/2022 10:03:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.81 on epoch=262
05/21/2022 10:03:28 - INFO - __main__ - Global step 1050 Train loss 0.81 Classification-F1 0.1 on epoch=262
05/21/2022 10:03:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.83 on epoch=264
05/21/2022 10:03:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.79 on epoch=267
05/21/2022 10:03:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.77 on epoch=269
05/21/2022 10:03:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.83 on epoch=272
05/21/2022 10:03:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.84 on epoch=274
05/21/2022 10:03:35 - INFO - __main__ - Global step 1100 Train loss 0.81 Classification-F1 0.1 on epoch=274
05/21/2022 10:03:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.76 on epoch=277
05/21/2022 10:03:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.82 on epoch=279
05/21/2022 10:03:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.83 on epoch=282
05/21/2022 10:03:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.82 on epoch=284
05/21/2022 10:03:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.82 on epoch=287
05/21/2022 10:03:43 - INFO - __main__ - Global step 1150 Train loss 0.81 Classification-F1 0.1238095238095238 on epoch=287
05/21/2022 10:03:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.75 on epoch=289
05/21/2022 10:03:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.73 on epoch=292
05/21/2022 10:03:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.82 on epoch=294
05/21/2022 10:03:48 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.80 on epoch=297
05/21/2022 10:03:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.85 on epoch=299
05/21/2022 10:03:50 - INFO - __main__ - Global step 1200 Train loss 0.79 Classification-F1 0.1 on epoch=299
05/21/2022 10:03:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.83 on epoch=302
05/21/2022 10:03:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.81 on epoch=304
05/21/2022 10:03:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.88 on epoch=307
05/21/2022 10:03:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.83 on epoch=309
05/21/2022 10:03:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.84 on epoch=312
05/21/2022 10:03:58 - INFO - __main__ - Global step 1250 Train loss 0.84 Classification-F1 0.17979490646833446 on epoch=312
05/21/2022 10:03:58 - INFO - __main__ - Saving model with best Classification-F1: 0.1659804426145136 -> 0.17979490646833446 on epoch=312, global_step=1250
05/21/2022 10:03:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.82 on epoch=314
05/21/2022 10:04:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.81 on epoch=317
05/21/2022 10:04:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.79 on epoch=319
05/21/2022 10:04:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.86 on epoch=322
05/21/2022 10:04:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.79 on epoch=324
05/21/2022 10:04:05 - INFO - __main__ - Global step 1300 Train loss 0.81 Classification-F1 0.16695652173913045 on epoch=324
05/21/2022 10:04:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.89 on epoch=327
05/21/2022 10:04:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.82 on epoch=329
05/21/2022 10:04:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.77 on epoch=332
05/21/2022 10:04:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.88 on epoch=334
05/21/2022 10:04:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.78 on epoch=337
05/21/2022 10:04:12 - INFO - __main__ - Global step 1350 Train loss 0.83 Classification-F1 0.19732679541161074 on epoch=337
05/21/2022 10:04:12 - INFO - __main__ - Saving model with best Classification-F1: 0.17979490646833446 -> 0.19732679541161074 on epoch=337, global_step=1350
05/21/2022 10:04:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.78 on epoch=339
05/21/2022 10:04:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.81 on epoch=342
05/21/2022 10:04:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.78 on epoch=344
05/21/2022 10:04:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.80 on epoch=347
05/21/2022 10:04:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.87 on epoch=349
05/21/2022 10:04:18 - INFO - __main__ - Global step 1400 Train loss 0.81 Classification-F1 0.10833333333333331 on epoch=349
05/21/2022 10:04:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.75 on epoch=352
05/21/2022 10:04:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.76 on epoch=354
05/21/2022 10:04:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.85 on epoch=357
05/21/2022 10:04:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.77 on epoch=359
05/21/2022 10:04:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.76 on epoch=362
05/21/2022 10:04:26 - INFO - __main__ - Global step 1450 Train loss 0.78 Classification-F1 0.15585364580614855 on epoch=362
05/21/2022 10:04:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.80 on epoch=364
05/21/2022 10:04:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.79 on epoch=367
05/21/2022 10:04:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.81 on epoch=369
05/21/2022 10:04:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.81 on epoch=372
05/21/2022 10:04:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.80 on epoch=374
05/21/2022 10:04:34 - INFO - __main__ - Global step 1500 Train loss 0.80 Classification-F1 0.16637984981226533 on epoch=374
05/21/2022 10:04:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.87 on epoch=377
05/21/2022 10:04:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.78 on epoch=379
05/21/2022 10:04:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.82 on epoch=382
05/21/2022 10:04:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.83 on epoch=384
05/21/2022 10:04:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.85 on epoch=387
05/21/2022 10:04:42 - INFO - __main__ - Global step 1550 Train loss 0.83 Classification-F1 0.15572755417956655 on epoch=387
05/21/2022 10:04:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.85 on epoch=389
05/21/2022 10:04:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.80 on epoch=392
05/21/2022 10:04:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.82 on epoch=394
05/21/2022 10:04:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.84 on epoch=397
05/21/2022 10:04:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.72 on epoch=399
05/21/2022 10:04:50 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.15582419408441342 on epoch=399
05/21/2022 10:04:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.89 on epoch=402
05/21/2022 10:04:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.87 on epoch=404
05/21/2022 10:04:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.84 on epoch=407
05/21/2022 10:04:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.75 on epoch=409
05/21/2022 10:04:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.83 on epoch=412
05/21/2022 10:04:57 - INFO - __main__ - Global step 1650 Train loss 0.83 Classification-F1 0.11868686868686869 on epoch=412
05/21/2022 10:04:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.82 on epoch=414
05/21/2022 10:05:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.73 on epoch=417
05/21/2022 10:05:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.84 on epoch=419
05/21/2022 10:05:03 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.81 on epoch=422
05/21/2022 10:05:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.84 on epoch=424
05/21/2022 10:05:05 - INFO - __main__ - Global step 1700 Train loss 0.81 Classification-F1 0.13654401154401155 on epoch=424
05/21/2022 10:05:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.83 on epoch=427
05/21/2022 10:05:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.86 on epoch=429
05/21/2022 10:05:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.78 on epoch=432
05/21/2022 10:05:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.81 on epoch=434
05/21/2022 10:05:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.87 on epoch=437
05/21/2022 10:05:12 - INFO - __main__ - Global step 1750 Train loss 0.83 Classification-F1 0.20383986928104575 on epoch=437
05/21/2022 10:05:12 - INFO - __main__ - Saving model with best Classification-F1: 0.19732679541161074 -> 0.20383986928104575 on epoch=437, global_step=1750
05/21/2022 10:05:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.84 on epoch=439
05/21/2022 10:05:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.75 on epoch=442
05/21/2022 10:05:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.80 on epoch=444
05/21/2022 10:05:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.86 on epoch=447
05/21/2022 10:05:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.83 on epoch=449
05/21/2022 10:05:20 - INFO - __main__ - Global step 1800 Train loss 0.82 Classification-F1 0.13067758749069247 on epoch=449
05/21/2022 10:05:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.76 on epoch=452
05/21/2022 10:05:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.83 on epoch=454
05/21/2022 10:05:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.85 on epoch=457
05/21/2022 10:05:26 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.75 on epoch=459
05/21/2022 10:05:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.78 on epoch=462
05/21/2022 10:05:28 - INFO - __main__ - Global step 1850 Train loss 0.79 Classification-F1 0.2332516339869281 on epoch=462
05/21/2022 10:05:28 - INFO - __main__ - Saving model with best Classification-F1: 0.20383986928104575 -> 0.2332516339869281 on epoch=462, global_step=1850
05/21/2022 10:05:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.77 on epoch=464
05/21/2022 10:05:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.80 on epoch=467
05/21/2022 10:05:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.73 on epoch=469
05/21/2022 10:05:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.74 on epoch=472
05/21/2022 10:05:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.79 on epoch=474
05/21/2022 10:05:35 - INFO - __main__ - Global step 1900 Train loss 0.76 Classification-F1 0.14367690058479532 on epoch=474
05/21/2022 10:05:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.81 on epoch=477
05/21/2022 10:05:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.80 on epoch=479
05/21/2022 10:05:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.77 on epoch=482
05/21/2022 10:05:41 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.80 on epoch=484
05/21/2022 10:05:43 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.80 on epoch=487
05/21/2022 10:05:43 - INFO - __main__ - Global step 1950 Train loss 0.79 Classification-F1 0.19916582540778918 on epoch=487
05/21/2022 10:05:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.84 on epoch=489
05/21/2022 10:05:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.83 on epoch=492
05/21/2022 10:05:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.78 on epoch=494
05/21/2022 10:05:49 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.82 on epoch=497
05/21/2022 10:05:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.78 on epoch=499
05/21/2022 10:05:51 - INFO - __main__ - Global step 2000 Train loss 0.81 Classification-F1 0.18721318314559865 on epoch=499
05/21/2022 10:05:52 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.78 on epoch=502
05/21/2022 10:05:53 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.80 on epoch=504
05/21/2022 10:05:55 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.78 on epoch=507
05/21/2022 10:05:56 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.80 on epoch=509
05/21/2022 10:05:57 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.80 on epoch=512
05/21/2022 10:05:58 - INFO - __main__ - Global step 2050 Train loss 0.79 Classification-F1 0.2227947491105386 on epoch=512
05/21/2022 10:05:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.77 on epoch=514
05/21/2022 10:06:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.85 on epoch=517
05/21/2022 10:06:02 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.75 on epoch=519
05/21/2022 10:06:03 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.87 on epoch=522
05/21/2022 10:06:05 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.85 on epoch=524
05/21/2022 10:06:06 - INFO - __main__ - Global step 2100 Train loss 0.82 Classification-F1 0.16677723721067372 on epoch=524
05/21/2022 10:06:07 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.83 on epoch=527
05/21/2022 10:06:09 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.80 on epoch=529
05/21/2022 10:06:10 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.76 on epoch=532
05/21/2022 10:06:11 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.76 on epoch=534
05/21/2022 10:06:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.75 on epoch=537
05/21/2022 10:06:13 - INFO - __main__ - Global step 2150 Train loss 0.78 Classification-F1 0.2216672534178255 on epoch=537
05/21/2022 10:06:14 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.79 on epoch=539
05/21/2022 10:06:16 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.82 on epoch=542
05/21/2022 10:06:17 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.87 on epoch=544
05/21/2022 10:06:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.76 on epoch=547
05/21/2022 10:06:19 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.79 on epoch=549
05/21/2022 10:06:20 - INFO - __main__ - Global step 2200 Train loss 0.81 Classification-F1 0.21457426221577164 on epoch=549
05/21/2022 10:06:21 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.77 on epoch=552
05/21/2022 10:06:22 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.74 on epoch=554
05/21/2022 10:06:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.83 on epoch=557
05/21/2022 10:06:25 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.80 on epoch=559
05/21/2022 10:06:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.74 on epoch=562
05/21/2022 10:06:27 - INFO - __main__ - Global step 2250 Train loss 0.77 Classification-F1 0.20022629600125832 on epoch=562
05/21/2022 10:06:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.84 on epoch=564
05/21/2022 10:06:29 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.81 on epoch=567
05/21/2022 10:06:31 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.87 on epoch=569
05/21/2022 10:06:32 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.80 on epoch=572
05/21/2022 10:06:34 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.81 on epoch=574
05/21/2022 10:06:34 - INFO - __main__ - Global step 2300 Train loss 0.83 Classification-F1 0.20502364066193854 on epoch=574
05/21/2022 10:06:36 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.76 on epoch=577
05/21/2022 10:06:37 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.82 on epoch=579
05/21/2022 10:06:39 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.79 on epoch=582
05/21/2022 10:06:40 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.73 on epoch=584
05/21/2022 10:06:42 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.81 on epoch=587
05/21/2022 10:06:42 - INFO - __main__ - Global step 2350 Train loss 0.78 Classification-F1 0.265245995423341 on epoch=587
05/21/2022 10:06:42 - INFO - __main__ - Saving model with best Classification-F1: 0.2332516339869281 -> 0.265245995423341 on epoch=587, global_step=2350
05/21/2022 10:06:44 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.75 on epoch=589
05/21/2022 10:06:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.71 on epoch=592
05/21/2022 10:06:47 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.76 on epoch=594
05/21/2022 10:06:49 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.81 on epoch=597
05/21/2022 10:06:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.78 on epoch=599
05/21/2022 10:06:51 - INFO - __main__ - Global step 2400 Train loss 0.76 Classification-F1 0.18087833219412167 on epoch=599
05/21/2022 10:06:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.77 on epoch=602
05/21/2022 10:06:53 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.72 on epoch=604
05/21/2022 10:06:55 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.72 on epoch=607
05/21/2022 10:06:56 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.82 on epoch=609
05/21/2022 10:06:58 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.77 on epoch=612
05/21/2022 10:06:58 - INFO - __main__ - Global step 2450 Train loss 0.76 Classification-F1 0.2575890175890176 on epoch=612
05/21/2022 10:07:00 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.78 on epoch=614
05/21/2022 10:07:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.76 on epoch=617
05/21/2022 10:07:03 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.78 on epoch=619
05/21/2022 10:07:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.76 on epoch=622
05/21/2022 10:07:06 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.74 on epoch=624
05/21/2022 10:07:07 - INFO - __main__ - Global step 2500 Train loss 0.76 Classification-F1 0.17883964582077788 on epoch=624
05/21/2022 10:07:08 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.77 on epoch=627
05/21/2022 10:07:09 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.82 on epoch=629
05/21/2022 10:07:10 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.80 on epoch=632
05/21/2022 10:07:12 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.79 on epoch=634
05/21/2022 10:07:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.75 on epoch=637
05/21/2022 10:07:14 - INFO - __main__ - Global step 2550 Train loss 0.79 Classification-F1 0.17718715393133996 on epoch=637
05/21/2022 10:07:15 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.72 on epoch=639
05/21/2022 10:07:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.73 on epoch=642
05/21/2022 10:07:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.79 on epoch=644
05/21/2022 10:07:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.82 on epoch=647
05/21/2022 10:07:21 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.76 on epoch=649
05/21/2022 10:07:21 - INFO - __main__ - Global step 2600 Train loss 0.76 Classification-F1 0.1982115923292394 on epoch=649
05/21/2022 10:07:23 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.73 on epoch=652
05/21/2022 10:07:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.77 on epoch=654
05/21/2022 10:07:26 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.82 on epoch=657
05/21/2022 10:07:27 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.78 on epoch=659
05/21/2022 10:07:29 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.76 on epoch=662
05/21/2022 10:07:29 - INFO - __main__ - Global step 2650 Train loss 0.77 Classification-F1 0.20945532435740513 on epoch=662
05/21/2022 10:07:31 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.77 on epoch=664
05/21/2022 10:07:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.78 on epoch=667
05/21/2022 10:07:34 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.73 on epoch=669
05/21/2022 10:07:36 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.83 on epoch=672
05/21/2022 10:07:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.70 on epoch=674
05/21/2022 10:07:38 - INFO - __main__ - Global step 2700 Train loss 0.76 Classification-F1 0.20604979979979976 on epoch=674
05/21/2022 10:07:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.78 on epoch=677
05/21/2022 10:07:40 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.74 on epoch=679
05/21/2022 10:07:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.78 on epoch=682
05/21/2022 10:07:43 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.80 on epoch=684
05/21/2022 10:07:44 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.79 on epoch=687
05/21/2022 10:07:45 - INFO - __main__ - Global step 2750 Train loss 0.78 Classification-F1 0.22188384397686722 on epoch=687
05/21/2022 10:07:46 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.73 on epoch=689
05/21/2022 10:07:48 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.78 on epoch=692
05/21/2022 10:07:49 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.83 on epoch=694
05/21/2022 10:07:50 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.74 on epoch=697
05/21/2022 10:07:52 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.78 on epoch=699
05/21/2022 10:07:52 - INFO - __main__ - Global step 2800 Train loss 0.77 Classification-F1 0.1821196196196196 on epoch=699
05/21/2022 10:07:54 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.80 on epoch=702
05/21/2022 10:07:55 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.77 on epoch=704
05/21/2022 10:07:57 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.78 on epoch=707
05/21/2022 10:07:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.72 on epoch=709
05/21/2022 10:08:00 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.75 on epoch=712
05/21/2022 10:08:00 - INFO - __main__ - Global step 2850 Train loss 0.76 Classification-F1 0.2619230769230769 on epoch=712
05/21/2022 10:08:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.80 on epoch=714
05/21/2022 10:08:03 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.79 on epoch=717
05/21/2022 10:08:05 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.81 on epoch=719
05/21/2022 10:08:06 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.79 on epoch=722
05/21/2022 10:08:07 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.79 on epoch=724
05/21/2022 10:08:08 - INFO - __main__ - Global step 2900 Train loss 0.80 Classification-F1 0.215625 on epoch=724
05/21/2022 10:08:09 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.74 on epoch=727
05/21/2022 10:08:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.75 on epoch=729
05/21/2022 10:08:12 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.81 on epoch=732
05/21/2022 10:08:14 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.76 on epoch=734
05/21/2022 10:08:15 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.73 on epoch=737
05/21/2022 10:08:16 - INFO - __main__ - Global step 2950 Train loss 0.76 Classification-F1 0.2617998796570225 on epoch=737
05/21/2022 10:08:17 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.75 on epoch=739
05/21/2022 10:08:18 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.72 on epoch=742
05/21/2022 10:08:20 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.75 on epoch=744
05/21/2022 10:08:21 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.74 on epoch=747
05/21/2022 10:08:23 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.81 on epoch=749
05/21/2022 10:08:23 - INFO - __main__ - Global step 3000 Train loss 0.75 Classification-F1 0.2167172450191318 on epoch=749
05/21/2022 10:08:23 - INFO - __main__ - save last model!
05/21/2022 10:08:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 10:08:23 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 10:08:23 - INFO - __main__ - Printing 3 examples
05/21/2022 10:08:23 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 10:08:23 - INFO - __main__ - ['others']
05/21/2022 10:08:23 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 10:08:23 - INFO - __main__ - ['others']
05/21/2022 10:08:23 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 10:08:23 - INFO - __main__ - ['others']
05/21/2022 10:08:23 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:08:24 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:08:24 - INFO - __main__ - Printing 3 examples
05/21/2022 10:08:24 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 10:08:24 - INFO - __main__ - ['others']
05/21/2022 10:08:24 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 10:08:24 - INFO - __main__ - ['others']
05/21/2022 10:08:24 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 10:08:24 - INFO - __main__ - ['others']
05/21/2022 10:08:24 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:08:24 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:08:24 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 10:08:24 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:08:24 - INFO - __main__ - Printing 3 examples
05/21/2022 10:08:24 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/21/2022 10:08:24 - INFO - __main__ - ['others']
05/21/2022 10:08:24 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/21/2022 10:08:24 - INFO - __main__ - ['others']
05/21/2022 10:08:24 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/21/2022 10:08:24 - INFO - __main__ - ['others']
05/21/2022 10:08:24 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:08:24 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:08:24 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 10:08:25 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:08:30 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 10:08:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 10:08:30 - INFO - __main__ - Starting training!
05/21/2022 10:08:31 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 10:09:15 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_13_0.4_8_predictions.txt
05/21/2022 10:09:15 - INFO - __main__ - Classification-F1 on test data: 0.1027
05/21/2022 10:09:15 - INFO - __main__ - prefix=emo_16_13, lr=0.4, bsz=8, dev_performance=0.265245995423341, test_performance=0.10273527639977276
05/21/2022 10:09:15 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.3, bsz=8 ...
05/21/2022 10:09:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:09:16 - INFO - __main__ - Printing 3 examples
05/21/2022 10:09:16 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 10:09:16 - INFO - __main__ - ['others']
05/21/2022 10:09:16 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 10:09:16 - INFO - __main__ - ['others']
05/21/2022 10:09:16 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 10:09:16 - INFO - __main__ - ['others']
05/21/2022 10:09:16 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:09:16 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:09:16 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 10:09:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:09:16 - INFO - __main__ - Printing 3 examples
05/21/2022 10:09:16 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/21/2022 10:09:16 - INFO - __main__ - ['others']
05/21/2022 10:09:16 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/21/2022 10:09:16 - INFO - __main__ - ['others']
05/21/2022 10:09:16 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/21/2022 10:09:16 - INFO - __main__ - ['others']
05/21/2022 10:09:16 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:09:16 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:09:16 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 10:09:23 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 10:09:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 10:09:23 - INFO - __main__ - Starting training!
05/21/2022 10:09:25 - INFO - __main__ - Step 10 Global step 10 Train loss 4.34 on epoch=2
05/21/2022 10:09:26 - INFO - __main__ - Step 20 Global step 20 Train loss 3.89 on epoch=4
05/21/2022 10:09:27 - INFO - __main__ - Step 30 Global step 30 Train loss 3.60 on epoch=7
05/21/2022 10:09:29 - INFO - __main__ - Step 40 Global step 40 Train loss 3.09 on epoch=9
05/21/2022 10:09:30 - INFO - __main__ - Step 50 Global step 50 Train loss 2.76 on epoch=12
05/21/2022 10:09:31 - INFO - __main__ - Global step 50 Train loss 3.54 Classification-F1 0.1 on epoch=12
05/21/2022 10:09:31 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/21/2022 10:09:32 - INFO - __main__ - Step 60 Global step 60 Train loss 2.28 on epoch=14
05/21/2022 10:09:33 - INFO - __main__ - Step 70 Global step 70 Train loss 2.16 on epoch=17
05/21/2022 10:09:34 - INFO - __main__ - Step 80 Global step 80 Train loss 1.75 on epoch=19
05/21/2022 10:09:36 - INFO - __main__ - Step 90 Global step 90 Train loss 1.57 on epoch=22
05/21/2022 10:09:37 - INFO - __main__ - Step 100 Global step 100 Train loss 1.30 on epoch=24
05/21/2022 10:09:37 - INFO - __main__ - Global step 100 Train loss 1.81 Classification-F1 0.10126582278481013 on epoch=24
05/21/2022 10:09:37 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.10126582278481013 on epoch=24, global_step=100
05/21/2022 10:09:39 - INFO - __main__ - Step 110 Global step 110 Train loss 1.39 on epoch=27
05/21/2022 10:09:40 - INFO - __main__ - Step 120 Global step 120 Train loss 1.16 on epoch=29
05/21/2022 10:09:41 - INFO - __main__ - Step 130 Global step 130 Train loss 1.13 on epoch=32
05/21/2022 10:09:42 - INFO - __main__ - Step 140 Global step 140 Train loss 1.06 on epoch=34
05/21/2022 10:09:44 - INFO - __main__ - Step 150 Global step 150 Train loss 1.06 on epoch=37
05/21/2022 10:09:44 - INFO - __main__ - Global step 150 Train loss 1.16 Classification-F1 0.1 on epoch=37
05/21/2022 10:09:46 - INFO - __main__ - Step 160 Global step 160 Train loss 1.12 on epoch=39
05/21/2022 10:09:48 - INFO - __main__ - Step 170 Global step 170 Train loss 1.07 on epoch=42
05/21/2022 10:09:49 - INFO - __main__ - Step 180 Global step 180 Train loss 1.06 on epoch=44
05/21/2022 10:09:51 - INFO - __main__ - Step 190 Global step 190 Train loss 1.07 on epoch=47
05/21/2022 10:09:52 - INFO - __main__ - Step 200 Global step 200 Train loss 1.14 on epoch=49
05/21/2022 10:09:53 - INFO - __main__ - Global step 200 Train loss 1.09 Classification-F1 0.1 on epoch=49
05/21/2022 10:09:54 - INFO - __main__ - Step 210 Global step 210 Train loss 1.04 on epoch=52
05/21/2022 10:09:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.93 on epoch=54
05/21/2022 10:09:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.98 on epoch=57
05/21/2022 10:09:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.98 on epoch=59
05/21/2022 10:10:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.94 on epoch=62
05/21/2022 10:10:00 - INFO - __main__ - Global step 250 Train loss 0.97 Classification-F1 0.1 on epoch=62
05/21/2022 10:10:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.92 on epoch=64
05/21/2022 10:10:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.92 on epoch=67
05/21/2022 10:10:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.93 on epoch=69
05/21/2022 10:10:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.93 on epoch=72
05/21/2022 10:10:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.88 on epoch=74
05/21/2022 10:10:08 - INFO - __main__ - Global step 300 Train loss 0.92 Classification-F1 0.1 on epoch=74
05/21/2022 10:10:09 - INFO - __main__ - Step 310 Global step 310 Train loss 1.01 on epoch=77
05/21/2022 10:10:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.99 on epoch=79
05/21/2022 10:10:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.94 on epoch=82
05/21/2022 10:10:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.91 on epoch=84
05/21/2022 10:10:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.93 on epoch=87
05/21/2022 10:10:15 - INFO - __main__ - Global step 350 Train loss 0.96 Classification-F1 0.1 on epoch=87
05/21/2022 10:10:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.88 on epoch=89
05/21/2022 10:10:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.86 on epoch=92
05/21/2022 10:10:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.78 on epoch=94
05/21/2022 10:10:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.87 on epoch=97
05/21/2022 10:10:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.91 on epoch=99
05/21/2022 10:10:23 - INFO - __main__ - Global step 400 Train loss 0.86 Classification-F1 0.09615384615384615 on epoch=99
05/21/2022 10:10:24 - INFO - __main__ - Step 410 Global step 410 Train loss 1.00 on epoch=102
05/21/2022 10:10:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.88 on epoch=104
05/21/2022 10:10:27 - INFO - __main__ - Step 430 Global step 430 Train loss 1.02 on epoch=107
05/21/2022 10:10:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.96 on epoch=109
05/21/2022 10:10:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.98 on epoch=112
05/21/2022 10:10:30 - INFO - __main__ - Global step 450 Train loss 0.97 Classification-F1 0.10256410256410256 on epoch=112
05/21/2022 10:10:30 - INFO - __main__ - Saving model with best Classification-F1: 0.10126582278481013 -> 0.10256410256410256 on epoch=112, global_step=450
05/21/2022 10:10:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.90 on epoch=114
05/21/2022 10:10:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.80 on epoch=117
05/21/2022 10:10:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.87 on epoch=119
05/21/2022 10:10:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.92 on epoch=122
05/21/2022 10:10:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.82 on epoch=124
05/21/2022 10:10:37 - INFO - __main__ - Global step 500 Train loss 0.86 Classification-F1 0.0974025974025974 on epoch=124
05/21/2022 10:10:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.88 on epoch=127
05/21/2022 10:10:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.86 on epoch=129
05/21/2022 10:10:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.88 on epoch=132
05/21/2022 10:10:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.89 on epoch=134
05/21/2022 10:10:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.84 on epoch=137
05/21/2022 10:10:45 - INFO - __main__ - Global step 550 Train loss 0.87 Classification-F1 0.13067758749069247 on epoch=137
05/21/2022 10:10:45 - INFO - __main__ - Saving model with best Classification-F1: 0.10256410256410256 -> 0.13067758749069247 on epoch=137, global_step=550
05/21/2022 10:10:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.91 on epoch=139
05/21/2022 10:10:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.94 on epoch=142
05/21/2022 10:10:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.91 on epoch=144
05/21/2022 10:10:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.90 on epoch=147
05/21/2022 10:10:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.95 on epoch=149
05/21/2022 10:10:52 - INFO - __main__ - Global step 600 Train loss 0.92 Classification-F1 0.10126582278481013 on epoch=149
05/21/2022 10:10:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.84 on epoch=152
05/21/2022 10:10:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.79 on epoch=154
05/21/2022 10:10:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.95 on epoch=157
05/21/2022 10:10:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.86 on epoch=159
05/21/2022 10:10:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.84 on epoch=162
05/21/2022 10:10:59 - INFO - __main__ - Global step 650 Train loss 0.86 Classification-F1 0.1 on epoch=162
05/21/2022 10:11:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.85 on epoch=164
05/21/2022 10:11:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.84 on epoch=167
05/21/2022 10:11:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.87 on epoch=169
05/21/2022 10:11:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.86 on epoch=172
05/21/2022 10:11:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.86 on epoch=174
05/21/2022 10:11:06 - INFO - __main__ - Global step 700 Train loss 0.86 Classification-F1 0.1237183868762816 on epoch=174
05/21/2022 10:11:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.83 on epoch=177
05/21/2022 10:11:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.89 on epoch=179
05/21/2022 10:11:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.85 on epoch=182
05/21/2022 10:11:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.87 on epoch=184
05/21/2022 10:11:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.90 on epoch=187
05/21/2022 10:11:14 - INFO - __main__ - Global step 750 Train loss 0.87 Classification-F1 0.1 on epoch=187
05/21/2022 10:11:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.89 on epoch=189
05/21/2022 10:11:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.80 on epoch=192
05/21/2022 10:11:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.83 on epoch=194
05/21/2022 10:11:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.84 on epoch=197
05/21/2022 10:11:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.83 on epoch=199
05/21/2022 10:11:22 - INFO - __main__ - Global step 800 Train loss 0.84 Classification-F1 0.1302118933697881 on epoch=199
05/21/2022 10:11:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.83 on epoch=202
05/21/2022 10:11:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.91 on epoch=204
05/21/2022 10:11:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.86 on epoch=207
05/21/2022 10:11:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.87 on epoch=209
05/21/2022 10:11:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.81 on epoch=212
05/21/2022 10:11:29 - INFO - __main__ - Global step 850 Train loss 0.86 Classification-F1 0.10126582278481013 on epoch=212
05/21/2022 10:11:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.84 on epoch=214
05/21/2022 10:11:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.87 on epoch=217
05/21/2022 10:11:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.76 on epoch=219
05/21/2022 10:11:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.90 on epoch=222
05/21/2022 10:11:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.89 on epoch=224
05/21/2022 10:11:36 - INFO - __main__ - Global step 900 Train loss 0.85 Classification-F1 0.1 on epoch=224
05/21/2022 10:11:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.84 on epoch=227
05/21/2022 10:11:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.75 on epoch=229
05/21/2022 10:11:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.86 on epoch=232
05/21/2022 10:11:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.92 on epoch=234
05/21/2022 10:11:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.91 on epoch=237
05/21/2022 10:11:43 - INFO - __main__ - Global step 950 Train loss 0.86 Classification-F1 0.1 on epoch=237
05/21/2022 10:11:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.81 on epoch=239
05/21/2022 10:11:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.81 on epoch=242
05/21/2022 10:11:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.81 on epoch=244
05/21/2022 10:11:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.79 on epoch=247
05/21/2022 10:11:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.81 on epoch=249
05/21/2022 10:11:50 - INFO - __main__ - Global step 1000 Train loss 0.81 Classification-F1 0.1 on epoch=249
05/21/2022 10:11:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.80 on epoch=252
05/21/2022 10:11:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.92 on epoch=254
05/21/2022 10:11:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.80 on epoch=257
05/21/2022 10:11:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.84 on epoch=259
05/21/2022 10:11:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.84 on epoch=262
05/21/2022 10:11:58 - INFO - __main__ - Global step 1050 Train loss 0.84 Classification-F1 0.13067758749069247 on epoch=262
05/21/2022 10:11:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.85 on epoch=264
05/21/2022 10:12:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.80 on epoch=267
05/21/2022 10:12:02 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.82 on epoch=269
05/21/2022 10:12:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.79 on epoch=272
05/21/2022 10:12:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.83 on epoch=274
05/21/2022 10:12:05 - INFO - __main__ - Global step 1100 Train loss 0.82 Classification-F1 0.13067758749069247 on epoch=274
05/21/2022 10:12:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.89 on epoch=277
05/21/2022 10:12:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.83 on epoch=279
05/21/2022 10:12:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.90 on epoch=282
05/21/2022 10:12:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.82 on epoch=284
05/21/2022 10:12:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.82 on epoch=287
05/21/2022 10:12:12 - INFO - __main__ - Global step 1150 Train loss 0.85 Classification-F1 0.1238095238095238 on epoch=287
05/21/2022 10:12:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.79 on epoch=289
05/21/2022 10:12:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.80 on epoch=292
05/21/2022 10:12:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.77 on epoch=294
05/21/2022 10:12:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.86 on epoch=297
05/21/2022 10:12:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.87 on epoch=299
05/21/2022 10:12:19 - INFO - __main__ - Global step 1200 Train loss 0.82 Classification-F1 0.1486842105263158 on epoch=299
05/21/2022 10:12:19 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.1486842105263158 on epoch=299, global_step=1200
05/21/2022 10:12:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.81 on epoch=302
05/21/2022 10:12:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.80 on epoch=304
05/21/2022 10:12:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.87 on epoch=307
05/21/2022 10:12:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.87 on epoch=309
05/21/2022 10:12:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.80 on epoch=312
05/21/2022 10:12:27 - INFO - __main__ - Global step 1250 Train loss 0.83 Classification-F1 0.1 on epoch=312
05/21/2022 10:12:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.75 on epoch=314
05/21/2022 10:12:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.86 on epoch=317
05/21/2022 10:12:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.85 on epoch=319
05/21/2022 10:12:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.86 on epoch=322
05/21/2022 10:12:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.88 on epoch=324
05/21/2022 10:12:34 - INFO - __main__ - Global step 1300 Train loss 0.84 Classification-F1 0.16277641277641278 on epoch=324
05/21/2022 10:12:34 - INFO - __main__ - Saving model with best Classification-F1: 0.1486842105263158 -> 0.16277641277641278 on epoch=324, global_step=1300
05/21/2022 10:12:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.85 on epoch=327
05/21/2022 10:12:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.83 on epoch=329
05/21/2022 10:12:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.81 on epoch=332
05/21/2022 10:12:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.80 on epoch=334
05/21/2022 10:12:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.88 on epoch=337
05/21/2022 10:12:41 - INFO - __main__ - Global step 1350 Train loss 0.83 Classification-F1 0.1 on epoch=337
05/21/2022 10:12:42 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.78 on epoch=339
05/21/2022 10:12:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.87 on epoch=342
05/21/2022 10:12:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.93 on epoch=344
05/21/2022 10:12:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.87 on epoch=347
05/21/2022 10:12:47 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.82 on epoch=349
05/21/2022 10:12:48 - INFO - __main__ - Global step 1400 Train loss 0.85 Classification-F1 0.15809312638580933 on epoch=349
05/21/2022 10:12:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.83 on epoch=352
05/21/2022 10:12:50 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.84 on epoch=354
05/21/2022 10:12:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.83 on epoch=357
05/21/2022 10:12:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.89 on epoch=359
05/21/2022 10:12:54 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.81 on epoch=362
05/21/2022 10:12:55 - INFO - __main__ - Global step 1450 Train loss 0.84 Classification-F1 0.1 on epoch=362
05/21/2022 10:12:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.88 on epoch=364
05/21/2022 10:12:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.85 on epoch=367
05/21/2022 10:12:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.74 on epoch=369
05/21/2022 10:13:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.86 on epoch=372
05/21/2022 10:13:01 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.76 on epoch=374
05/21/2022 10:13:02 - INFO - __main__ - Global step 1500 Train loss 0.82 Classification-F1 0.12681436210847974 on epoch=374
05/21/2022 10:13:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.83 on epoch=377
05/21/2022 10:13:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.83 on epoch=379
05/21/2022 10:13:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.85 on epoch=382
05/21/2022 10:13:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.76 on epoch=384
05/21/2022 10:13:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.82 on epoch=387
05/21/2022 10:13:09 - INFO - __main__ - Global step 1550 Train loss 0.82 Classification-F1 0.12393162393162392 on epoch=387
05/21/2022 10:13:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.79 on epoch=389
05/21/2022 10:13:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.84 on epoch=392
05/21/2022 10:13:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.89 on epoch=394
05/21/2022 10:13:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.87 on epoch=397
05/21/2022 10:13:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.81 on epoch=399
05/21/2022 10:13:16 - INFO - __main__ - Global step 1600 Train loss 0.84 Classification-F1 0.09493670886075949 on epoch=399
05/21/2022 10:13:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.80 on epoch=402
05/21/2022 10:13:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.80 on epoch=404
05/21/2022 10:13:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.82 on epoch=407
05/21/2022 10:13:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.84 on epoch=409
05/21/2022 10:13:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.79 on epoch=412
05/21/2022 10:13:24 - INFO - __main__ - Global step 1650 Train loss 0.81 Classification-F1 0.11176836861768369 on epoch=412
05/21/2022 10:13:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.81 on epoch=414
05/21/2022 10:13:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.74 on epoch=417
05/21/2022 10:13:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.85 on epoch=419
05/21/2022 10:13:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.78 on epoch=422
05/21/2022 10:13:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.78 on epoch=424
05/21/2022 10:13:31 - INFO - __main__ - Global step 1700 Train loss 0.80 Classification-F1 0.13067758749069247 on epoch=424
05/21/2022 10:13:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.85 on epoch=427
05/21/2022 10:13:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.79 on epoch=429
05/21/2022 10:13:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.77 on epoch=432
05/21/2022 10:13:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.79 on epoch=434
05/21/2022 10:13:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.81 on epoch=437
05/21/2022 10:13:38 - INFO - __main__ - Global step 1750 Train loss 0.80 Classification-F1 0.1 on epoch=437
05/21/2022 10:13:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.77 on epoch=439
05/21/2022 10:13:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.83 on epoch=442
05/21/2022 10:13:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.84 on epoch=444
05/21/2022 10:13:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.81 on epoch=447
05/21/2022 10:13:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.75 on epoch=449
05/21/2022 10:13:45 - INFO - __main__ - Global step 1800 Train loss 0.80 Classification-F1 0.15459213988625753 on epoch=449
05/21/2022 10:13:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.81 on epoch=452
05/21/2022 10:13:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.77 on epoch=454
05/21/2022 10:13:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.83 on epoch=457
05/21/2022 10:13:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.75 on epoch=459
05/21/2022 10:13:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.89 on epoch=462
05/21/2022 10:13:53 - INFO - __main__ - Global step 1850 Train loss 0.81 Classification-F1 0.22972385301152423 on epoch=462
05/21/2022 10:13:53 - INFO - __main__ - Saving model with best Classification-F1: 0.16277641277641278 -> 0.22972385301152423 on epoch=462, global_step=1850
05/21/2022 10:13:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.76 on epoch=464
05/21/2022 10:13:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.85 on epoch=467
05/21/2022 10:13:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.83 on epoch=469
05/21/2022 10:13:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.87 on epoch=472
05/21/2022 10:14:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.83 on epoch=474
05/21/2022 10:14:01 - INFO - __main__ - Global step 1900 Train loss 0.83 Classification-F1 0.13067758749069247 on epoch=474
05/21/2022 10:14:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.79 on epoch=477
05/21/2022 10:14:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.88 on epoch=479
05/21/2022 10:14:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.86 on epoch=482
05/21/2022 10:14:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.83 on epoch=484
05/21/2022 10:14:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.85 on epoch=487
05/21/2022 10:14:09 - INFO - __main__ - Global step 1950 Train loss 0.84 Classification-F1 0.13067758749069247 on epoch=487
05/21/2022 10:14:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.77 on epoch=489
05/21/2022 10:14:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.85 on epoch=492
05/21/2022 10:14:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.83 on epoch=494
05/21/2022 10:14:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.74 on epoch=497
05/21/2022 10:14:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.80 on epoch=499
05/21/2022 10:14:16 - INFO - __main__ - Global step 2000 Train loss 0.80 Classification-F1 0.1715492957746479 on epoch=499
05/21/2022 10:14:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.88 on epoch=502
05/21/2022 10:14:19 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.80 on epoch=504
05/21/2022 10:14:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.75 on epoch=507
05/21/2022 10:14:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.86 on epoch=509
05/21/2022 10:14:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.77 on epoch=512
05/21/2022 10:14:23 - INFO - __main__ - Global step 2050 Train loss 0.81 Classification-F1 0.17320261437908496 on epoch=512
05/21/2022 10:14:25 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.80 on epoch=514
05/21/2022 10:14:26 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.84 on epoch=517
05/21/2022 10:14:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.80 on epoch=519
05/21/2022 10:14:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.75 on epoch=522
05/21/2022 10:14:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.82 on epoch=524
05/21/2022 10:14:31 - INFO - __main__ - Global step 2100 Train loss 0.80 Classification-F1 0.18130697094891468 on epoch=524
05/21/2022 10:14:32 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.84 on epoch=527
05/21/2022 10:14:34 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.81 on epoch=529
05/21/2022 10:14:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.79 on epoch=532
05/21/2022 10:14:36 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.77 on epoch=534
05/21/2022 10:14:38 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.84 on epoch=537
05/21/2022 10:14:38 - INFO - __main__ - Global step 2150 Train loss 0.81 Classification-F1 0.19923240614075088 on epoch=537
05/21/2022 10:14:40 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.80 on epoch=539
05/21/2022 10:14:41 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.85 on epoch=542
05/21/2022 10:14:43 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.82 on epoch=544
05/21/2022 10:14:44 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.83 on epoch=547
05/21/2022 10:14:45 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.76 on epoch=549
05/21/2022 10:14:46 - INFO - __main__ - Global step 2200 Train loss 0.81 Classification-F1 0.1437908496732026 on epoch=549
05/21/2022 10:14:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.75 on epoch=552
05/21/2022 10:14:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.78 on epoch=554
05/21/2022 10:14:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.76 on epoch=557
05/21/2022 10:14:51 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.80 on epoch=559
05/21/2022 10:14:52 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.80 on epoch=562
05/21/2022 10:14:53 - INFO - __main__ - Global step 2250 Train loss 0.78 Classification-F1 0.15560224089635855 on epoch=562
05/21/2022 10:14:55 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.80 on epoch=564
05/21/2022 10:14:56 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.79 on epoch=567
05/21/2022 10:14:58 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.87 on epoch=569
05/21/2022 10:14:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.81 on epoch=572
05/21/2022 10:15:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.78 on epoch=574
05/21/2022 10:15:01 - INFO - __main__ - Global step 2300 Train loss 0.81 Classification-F1 0.18721318314559865 on epoch=574
05/21/2022 10:15:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.84 on epoch=577
05/21/2022 10:15:04 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.81 on epoch=579
05/21/2022 10:15:06 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.82 on epoch=582
05/21/2022 10:15:07 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.82 on epoch=584
05/21/2022 10:15:08 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.78 on epoch=587
05/21/2022 10:15:09 - INFO - __main__ - Global step 2350 Train loss 0.82 Classification-F1 0.18888888888888888 on epoch=587
05/21/2022 10:15:10 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.77 on epoch=589
05/21/2022 10:15:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.82 on epoch=592
05/21/2022 10:15:13 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.80 on epoch=594
05/21/2022 10:15:14 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.75 on epoch=597
05/21/2022 10:15:16 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.79 on epoch=599
05/21/2022 10:15:17 - INFO - __main__ - Global step 2400 Train loss 0.79 Classification-F1 0.14553934781459754 on epoch=599
05/21/2022 10:15:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.77 on epoch=602
05/21/2022 10:15:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.78 on epoch=604
05/21/2022 10:15:21 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.79 on epoch=607
05/21/2022 10:15:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.73 on epoch=609
05/21/2022 10:15:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.83 on epoch=612
05/21/2022 10:15:24 - INFO - __main__ - Global step 2450 Train loss 0.78 Classification-F1 0.17547753564702717 on epoch=612
05/21/2022 10:15:26 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.77 on epoch=614
05/21/2022 10:15:27 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.76 on epoch=617
05/21/2022 10:15:28 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.81 on epoch=619
05/21/2022 10:15:29 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.79 on epoch=622
05/21/2022 10:15:31 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.80 on epoch=624
05/21/2022 10:15:31 - INFO - __main__ - Global step 2500 Train loss 0.78 Classification-F1 0.1090909090909091 on epoch=624
05/21/2022 10:15:33 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.73 on epoch=627
05/21/2022 10:15:34 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.81 on epoch=629
05/21/2022 10:15:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.72 on epoch=632
05/21/2022 10:15:37 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.83 on epoch=634
05/21/2022 10:15:38 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.75 on epoch=637
05/21/2022 10:15:39 - INFO - __main__ - Global step 2550 Train loss 0.77 Classification-F1 0.08675464320625612 on epoch=637
05/21/2022 10:15:40 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.80 on epoch=639
05/21/2022 10:15:41 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.78 on epoch=642
05/21/2022 10:15:43 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.84 on epoch=644
05/21/2022 10:15:44 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.83 on epoch=647
05/21/2022 10:15:45 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.76 on epoch=649
05/21/2022 10:15:46 - INFO - __main__ - Global step 2600 Train loss 0.80 Classification-F1 0.17369852369852368 on epoch=649
05/21/2022 10:15:47 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.84 on epoch=652
05/21/2022 10:15:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.78 on epoch=654
05/21/2022 10:15:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.84 on epoch=657
05/21/2022 10:15:51 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.74 on epoch=659
05/21/2022 10:15:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.76 on epoch=662
05/21/2022 10:15:53 - INFO - __main__ - Global step 2650 Train loss 0.79 Classification-F1 0.1313362812769629 on epoch=662
05/21/2022 10:15:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.77 on epoch=664
05/21/2022 10:15:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.79 on epoch=667
05/21/2022 10:15:57 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.81 on epoch=669
05/21/2022 10:15:58 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.83 on epoch=672
05/21/2022 10:16:00 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.79 on epoch=674
05/21/2022 10:16:00 - INFO - __main__ - Global step 2700 Train loss 0.80 Classification-F1 0.16123850670211587 on epoch=674
05/21/2022 10:16:02 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.78 on epoch=677
05/21/2022 10:16:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.72 on epoch=679
05/21/2022 10:16:04 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.73 on epoch=682
05/21/2022 10:16:05 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.79 on epoch=684
05/21/2022 10:16:06 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.83 on epoch=687
05/21/2022 10:16:07 - INFO - __main__ - Global step 2750 Train loss 0.77 Classification-F1 0.0831043956043956 on epoch=687
05/21/2022 10:16:08 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.79 on epoch=689
05/21/2022 10:16:09 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.80 on epoch=692
05/21/2022 10:16:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.80 on epoch=694
05/21/2022 10:16:12 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.75 on epoch=697
05/21/2022 10:16:13 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.83 on epoch=699
05/21/2022 10:16:14 - INFO - __main__ - Global step 2800 Train loss 0.79 Classification-F1 0.10199240986717267 on epoch=699
05/21/2022 10:16:15 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.80 on epoch=702
05/21/2022 10:16:17 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.74 on epoch=704
05/21/2022 10:16:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.73 on epoch=707
05/21/2022 10:16:21 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.78 on epoch=709
05/21/2022 10:16:22 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.76 on epoch=712
05/21/2022 10:16:23 - INFO - __main__ - Global step 2850 Train loss 0.76 Classification-F1 0.07950191570881227 on epoch=712
05/21/2022 10:16:24 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.81 on epoch=714
05/21/2022 10:16:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.82 on epoch=717
05/21/2022 10:16:27 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.80 on epoch=719
05/21/2022 10:16:29 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.77 on epoch=722
05/21/2022 10:16:30 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.75 on epoch=724
05/21/2022 10:16:31 - INFO - __main__ - Global step 2900 Train loss 0.79 Classification-F1 0.08232118758434548 on epoch=724
05/21/2022 10:16:32 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.76 on epoch=727
05/21/2022 10:16:33 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.78 on epoch=729
05/21/2022 10:16:35 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.79 on epoch=732
05/21/2022 10:16:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.77 on epoch=734
05/21/2022 10:16:38 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.74 on epoch=737
05/21/2022 10:16:38 - INFO - __main__ - Global step 2950 Train loss 0.77 Classification-F1 0.10667903525046382 on epoch=737
05/21/2022 10:16:40 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.76 on epoch=739
05/21/2022 10:16:41 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.79 on epoch=742
05/21/2022 10:16:42 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.83 on epoch=744
05/21/2022 10:16:43 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.74 on epoch=747
05/21/2022 10:16:45 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.76 on epoch=749
05/21/2022 10:16:45 - INFO - __main__ - Global step 3000 Train loss 0.77 Classification-F1 0.15606107984156767 on epoch=749
05/21/2022 10:16:45 - INFO - __main__ - save last model!
05/21/2022 10:16:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 10:16:45 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 10:16:45 - INFO - __main__ - Printing 3 examples
05/21/2022 10:16:45 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 10:16:45 - INFO - __main__ - ['others']
05/21/2022 10:16:45 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 10:16:45 - INFO - __main__ - ['others']
05/21/2022 10:16:45 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 10:16:45 - INFO - __main__ - ['others']
05/21/2022 10:16:45 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:16:46 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:16:46 - INFO - __main__ - Printing 3 examples
05/21/2022 10:16:46 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 10:16:46 - INFO - __main__ - ['others']
05/21/2022 10:16:46 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 10:16:46 - INFO - __main__ - ['others']
05/21/2022 10:16:46 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 10:16:46 - INFO - __main__ - ['others']
05/21/2022 10:16:46 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:16:46 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:16:46 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 10:16:46 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:16:46 - INFO - __main__ - Printing 3 examples
05/21/2022 10:16:46 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/21/2022 10:16:46 - INFO - __main__ - ['others']
05/21/2022 10:16:46 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/21/2022 10:16:46 - INFO - __main__ - ['others']
05/21/2022 10:16:46 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/21/2022 10:16:46 - INFO - __main__ - ['others']
05/21/2022 10:16:46 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:16:46 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:16:46 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 10:16:48 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:16:53 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 10:16:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 10:16:53 - INFO - __main__ - Starting training!
05/21/2022 10:16:53 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 10:17:37 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_13_0.3_8_predictions.txt
05/21/2022 10:17:37 - INFO - __main__ - Classification-F1 on test data: 0.0631
05/21/2022 10:17:37 - INFO - __main__ - prefix=emo_16_13, lr=0.3, bsz=8, dev_performance=0.22972385301152423, test_performance=0.06308911141687151
05/21/2022 10:17:37 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.2, bsz=8 ...
05/21/2022 10:17:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:17:38 - INFO - __main__ - Printing 3 examples
05/21/2022 10:17:38 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 10:17:38 - INFO - __main__ - ['others']
05/21/2022 10:17:38 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 10:17:38 - INFO - __main__ - ['others']
05/21/2022 10:17:38 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 10:17:38 - INFO - __main__ - ['others']
05/21/2022 10:17:38 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:17:38 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:17:38 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 10:17:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:17:38 - INFO - __main__ - Printing 3 examples
05/21/2022 10:17:38 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/21/2022 10:17:38 - INFO - __main__ - ['others']
05/21/2022 10:17:38 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/21/2022 10:17:38 - INFO - __main__ - ['others']
05/21/2022 10:17:38 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/21/2022 10:17:38 - INFO - __main__ - ['others']
05/21/2022 10:17:38 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:17:38 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:17:39 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 10:17:45 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 10:17:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 10:17:45 - INFO - __main__ - Starting training!
05/21/2022 10:17:48 - INFO - __main__ - Step 10 Global step 10 Train loss 4.46 on epoch=2
05/21/2022 10:17:49 - INFO - __main__ - Step 20 Global step 20 Train loss 4.11 on epoch=4
05/21/2022 10:17:50 - INFO - __main__ - Step 30 Global step 30 Train loss 3.94 on epoch=7
05/21/2022 10:17:52 - INFO - __main__ - Step 40 Global step 40 Train loss 3.52 on epoch=9
05/21/2022 10:17:53 - INFO - __main__ - Step 50 Global step 50 Train loss 3.44 on epoch=12
05/21/2022 10:17:54 - INFO - __main__ - Global step 50 Train loss 3.89 Classification-F1 0.0 on epoch=12
05/21/2022 10:17:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
05/21/2022 10:17:56 - INFO - __main__ - Step 60 Global step 60 Train loss 2.98 on epoch=14
05/21/2022 10:17:57 - INFO - __main__ - Step 70 Global step 70 Train loss 2.84 on epoch=17
05/21/2022 10:17:58 - INFO - __main__ - Step 80 Global step 80 Train loss 2.53 on epoch=19
05/21/2022 10:18:00 - INFO - __main__ - Step 90 Global step 90 Train loss 2.49 on epoch=22
05/21/2022 10:18:01 - INFO - __main__ - Step 100 Global step 100 Train loss 2.11 on epoch=24
05/21/2022 10:18:02 - INFO - __main__ - Global step 100 Train loss 2.59 Classification-F1 0.1 on epoch=24
05/21/2022 10:18:02 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.1 on epoch=24, global_step=100
05/21/2022 10:18:03 - INFO - __main__ - Step 110 Global step 110 Train loss 2.18 on epoch=27
05/21/2022 10:18:04 - INFO - __main__ - Step 120 Global step 120 Train loss 1.75 on epoch=29
05/21/2022 10:18:06 - INFO - __main__ - Step 130 Global step 130 Train loss 1.75 on epoch=32
05/21/2022 10:18:07 - INFO - __main__ - Step 140 Global step 140 Train loss 1.58 on epoch=34
05/21/2022 10:18:09 - INFO - __main__ - Step 150 Global step 150 Train loss 1.51 on epoch=37
05/21/2022 10:18:09 - INFO - __main__ - Global step 150 Train loss 1.75 Classification-F1 0.13034188034188032 on epoch=37
05/21/2022 10:18:09 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.13034188034188032 on epoch=37, global_step=150
05/21/2022 10:18:11 - INFO - __main__ - Step 160 Global step 160 Train loss 1.41 on epoch=39
05/21/2022 10:18:12 - INFO - __main__ - Step 170 Global step 170 Train loss 1.46 on epoch=42
05/21/2022 10:18:13 - INFO - __main__ - Step 180 Global step 180 Train loss 1.18 on epoch=44
05/21/2022 10:18:15 - INFO - __main__ - Step 190 Global step 190 Train loss 1.33 on epoch=47
05/21/2022 10:18:16 - INFO - __main__ - Step 200 Global step 200 Train loss 1.18 on epoch=49
05/21/2022 10:18:17 - INFO - __main__ - Global step 200 Train loss 1.31 Classification-F1 0.1237183868762816 on epoch=49
05/21/2022 10:18:18 - INFO - __main__ - Step 210 Global step 210 Train loss 1.24 on epoch=52
05/21/2022 10:18:19 - INFO - __main__ - Step 220 Global step 220 Train loss 1.15 on epoch=54
05/21/2022 10:18:21 - INFO - __main__ - Step 230 Global step 230 Train loss 1.11 on epoch=57
05/21/2022 10:18:22 - INFO - __main__ - Step 240 Global step 240 Train loss 1.13 on epoch=59
05/21/2022 10:18:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.97 on epoch=62
05/21/2022 10:18:24 - INFO - __main__ - Global step 250 Train loss 1.12 Classification-F1 0.1 on epoch=62
05/21/2022 10:18:25 - INFO - __main__ - Step 260 Global step 260 Train loss 1.21 on epoch=64
05/21/2022 10:18:26 - INFO - __main__ - Step 270 Global step 270 Train loss 1.20 on epoch=67
05/21/2022 10:18:28 - INFO - __main__ - Step 280 Global step 280 Train loss 1.06 on epoch=69
05/21/2022 10:18:30 - INFO - __main__ - Step 290 Global step 290 Train loss 1.13 on epoch=72
05/21/2022 10:18:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.94 on epoch=74
05/21/2022 10:18:32 - INFO - __main__ - Global step 300 Train loss 1.11 Classification-F1 0.1 on epoch=74
05/21/2022 10:18:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.99 on epoch=77
05/21/2022 10:18:34 - INFO - __main__ - Step 320 Global step 320 Train loss 1.01 on epoch=79
05/21/2022 10:18:36 - INFO - __main__ - Step 330 Global step 330 Train loss 1.04 on epoch=82
05/21/2022 10:18:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.95 on epoch=84
05/21/2022 10:18:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.98 on epoch=87
05/21/2022 10:18:40 - INFO - __main__ - Global step 350 Train loss 0.99 Classification-F1 0.1 on epoch=87
05/21/2022 10:18:41 - INFO - __main__ - Step 360 Global step 360 Train loss 1.06 on epoch=89
05/21/2022 10:18:42 - INFO - __main__ - Step 370 Global step 370 Train loss 1.03 on epoch=92
05/21/2022 10:18:44 - INFO - __main__ - Step 380 Global step 380 Train loss 1.01 on epoch=94
05/21/2022 10:18:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.96 on epoch=97
05/21/2022 10:18:47 - INFO - __main__ - Step 400 Global step 400 Train loss 1.07 on epoch=99
05/21/2022 10:18:47 - INFO - __main__ - Global step 400 Train loss 1.02 Classification-F1 0.1 on epoch=99
05/21/2022 10:18:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.95 on epoch=102
05/21/2022 10:18:50 - INFO - __main__ - Step 420 Global step 420 Train loss 1.03 on epoch=104
05/21/2022 10:18:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.96 on epoch=107
05/21/2022 10:18:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.99 on epoch=109
05/21/2022 10:18:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.86 on epoch=112
05/21/2022 10:18:54 - INFO - __main__ - Global step 450 Train loss 0.96 Classification-F1 0.1 on epoch=112
05/21/2022 10:18:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.92 on epoch=114
05/21/2022 10:18:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.85 on epoch=117
05/21/2022 10:18:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.89 on epoch=119
05/21/2022 10:19:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.96 on epoch=122
05/21/2022 10:19:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.94 on epoch=124
05/21/2022 10:19:02 - INFO - __main__ - Global step 500 Train loss 0.91 Classification-F1 0.1 on epoch=124
05/21/2022 10:19:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.98 on epoch=127
05/21/2022 10:19:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.94 on epoch=129
05/21/2022 10:19:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.89 on epoch=132
05/21/2022 10:19:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.93 on epoch=134
05/21/2022 10:19:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.95 on epoch=137
05/21/2022 10:19:10 - INFO - __main__ - Global step 550 Train loss 0.94 Classification-F1 0.09493670886075949 on epoch=137
05/21/2022 10:19:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.97 on epoch=139
05/21/2022 10:19:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.96 on epoch=142
05/21/2022 10:19:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.88 on epoch=144
05/21/2022 10:19:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.93 on epoch=147
05/21/2022 10:19:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.91 on epoch=149
05/21/2022 10:19:17 - INFO - __main__ - Global step 600 Train loss 0.93 Classification-F1 0.09493670886075949 on epoch=149
05/21/2022 10:19:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.92 on epoch=152
05/21/2022 10:19:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.89 on epoch=154
05/21/2022 10:19:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.91 on epoch=157
05/21/2022 10:19:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.89 on epoch=159
05/21/2022 10:19:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.84 on epoch=162
05/21/2022 10:19:25 - INFO - __main__ - Global step 650 Train loss 0.89 Classification-F1 0.09615384615384615 on epoch=162
05/21/2022 10:19:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.91 on epoch=164
05/21/2022 10:19:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.96 on epoch=167
05/21/2022 10:19:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.86 on epoch=169
05/21/2022 10:19:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.93 on epoch=172
05/21/2022 10:19:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.84 on epoch=174
05/21/2022 10:19:34 - INFO - __main__ - Global step 700 Train loss 0.90 Classification-F1 0.1 on epoch=174
05/21/2022 10:19:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.93 on epoch=177
05/21/2022 10:19:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.85 on epoch=179
05/21/2022 10:19:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.83 on epoch=182
05/21/2022 10:19:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.98 on epoch=184
05/21/2022 10:19:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.83 on epoch=187
05/21/2022 10:19:42 - INFO - __main__ - Global step 750 Train loss 0.88 Classification-F1 0.1 on epoch=187
05/21/2022 10:19:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.86 on epoch=189
05/21/2022 10:19:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.93 on epoch=192
05/21/2022 10:19:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.90 on epoch=194
05/21/2022 10:19:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.91 on epoch=197
05/21/2022 10:19:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.90 on epoch=199
05/21/2022 10:19:50 - INFO - __main__ - Global step 800 Train loss 0.90 Classification-F1 0.1 on epoch=199
05/21/2022 10:19:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.83 on epoch=202
05/21/2022 10:19:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.95 on epoch=204
05/21/2022 10:19:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.93 on epoch=207
05/21/2022 10:19:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.89 on epoch=209
05/21/2022 10:19:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.94 on epoch=212
05/21/2022 10:19:58 - INFO - __main__ - Global step 850 Train loss 0.91 Classification-F1 0.1 on epoch=212
05/21/2022 10:19:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.88 on epoch=214
05/21/2022 10:20:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.85 on epoch=217
05/21/2022 10:20:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.83 on epoch=219
05/21/2022 10:20:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.92 on epoch=222
05/21/2022 10:20:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.86 on epoch=224
05/21/2022 10:20:05 - INFO - __main__ - Global step 900 Train loss 0.87 Classification-F1 0.09615384615384615 on epoch=224
05/21/2022 10:20:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.95 on epoch=227
05/21/2022 10:20:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.93 on epoch=229
05/21/2022 10:20:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.89 on epoch=232
05/21/2022 10:20:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.84 on epoch=234
05/21/2022 10:20:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.90 on epoch=237
05/21/2022 10:20:13 - INFO - __main__ - Global step 950 Train loss 0.90 Classification-F1 0.1 on epoch=237
05/21/2022 10:20:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.86 on epoch=239
05/21/2022 10:20:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.86 on epoch=242
05/21/2022 10:20:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.86 on epoch=244
05/21/2022 10:20:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.94 on epoch=247
05/21/2022 10:20:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.77 on epoch=249
05/21/2022 10:20:21 - INFO - __main__ - Global step 1000 Train loss 0.86 Classification-F1 0.0974025974025974 on epoch=249
05/21/2022 10:20:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.90 on epoch=252
05/21/2022 10:20:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.81 on epoch=254
05/21/2022 10:20:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.81 on epoch=257
05/21/2022 10:20:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.88 on epoch=259
05/21/2022 10:20:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.80 on epoch=262
05/21/2022 10:20:27 - INFO - __main__ - Global step 1050 Train loss 0.84 Classification-F1 0.09210526315789473 on epoch=262
05/21/2022 10:20:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.91 on epoch=264
05/21/2022 10:20:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.90 on epoch=267
05/21/2022 10:20:31 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.90 on epoch=269
05/21/2022 10:20:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.87 on epoch=272
05/21/2022 10:20:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.82 on epoch=274
05/21/2022 10:20:35 - INFO - __main__ - Global step 1100 Train loss 0.88 Classification-F1 0.1 on epoch=274
05/21/2022 10:20:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.90 on epoch=277
05/21/2022 10:20:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.88 on epoch=279
05/21/2022 10:20:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.93 on epoch=282
05/21/2022 10:20:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.88 on epoch=284
05/21/2022 10:20:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.93 on epoch=287
05/21/2022 10:20:42 - INFO - __main__ - Global step 1150 Train loss 0.90 Classification-F1 0.1 on epoch=287
05/21/2022 10:20:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.85 on epoch=289
05/21/2022 10:20:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.81 on epoch=292
05/21/2022 10:20:46 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.88 on epoch=294
05/21/2022 10:20:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.83 on epoch=297
05/21/2022 10:20:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.87 on epoch=299
05/21/2022 10:20:49 - INFO - __main__ - Global step 1200 Train loss 0.85 Classification-F1 0.1 on epoch=299
05/21/2022 10:20:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.84 on epoch=302
05/21/2022 10:20:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.88 on epoch=304
05/21/2022 10:20:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.80 on epoch=307
05/21/2022 10:20:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.83 on epoch=309
05/21/2022 10:20:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.85 on epoch=312
05/21/2022 10:20:56 - INFO - __main__ - Global step 1250 Train loss 0.84 Classification-F1 0.1 on epoch=312
05/21/2022 10:20:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.86 on epoch=314
05/21/2022 10:20:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.84 on epoch=317
05/21/2022 10:21:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.88 on epoch=319
05/21/2022 10:21:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.93 on epoch=322
05/21/2022 10:21:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.86 on epoch=324
05/21/2022 10:21:04 - INFO - __main__ - Global step 1300 Train loss 0.88 Classification-F1 0.10126582278481013 on epoch=324
05/21/2022 10:21:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.83 on epoch=327
05/21/2022 10:21:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.89 on epoch=329
05/21/2022 10:21:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.86 on epoch=332
05/21/2022 10:21:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.80 on epoch=334
05/21/2022 10:21:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.83 on epoch=337
05/21/2022 10:21:11 - INFO - __main__ - Global step 1350 Train loss 0.84 Classification-F1 0.1 on epoch=337
05/21/2022 10:21:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.89 on epoch=339
05/21/2022 10:21:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.88 on epoch=342
05/21/2022 10:21:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.78 on epoch=344
05/21/2022 10:21:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.85 on epoch=347
05/21/2022 10:21:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.77 on epoch=349
05/21/2022 10:21:19 - INFO - __main__ - Global step 1400 Train loss 0.83 Classification-F1 0.1 on epoch=349
05/21/2022 10:21:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.90 on epoch=352
05/21/2022 10:21:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.78 on epoch=354
05/21/2022 10:21:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.87 on epoch=357
05/21/2022 10:21:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.91 on epoch=359
05/21/2022 10:21:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.82 on epoch=362
05/21/2022 10:21:27 - INFO - __main__ - Global step 1450 Train loss 0.86 Classification-F1 0.1 on epoch=362
05/21/2022 10:21:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.81 on epoch=364
05/21/2022 10:21:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.89 on epoch=367
05/21/2022 10:21:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.85 on epoch=369
05/21/2022 10:21:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.85 on epoch=372
05/21/2022 10:21:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.79 on epoch=374
05/21/2022 10:21:34 - INFO - __main__ - Global step 1500 Train loss 0.84 Classification-F1 0.09493670886075949 on epoch=374
05/21/2022 10:21:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.84 on epoch=377
05/21/2022 10:21:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.90 on epoch=379
05/21/2022 10:21:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.87 on epoch=382
05/21/2022 10:21:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.82 on epoch=384
05/21/2022 10:21:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.84 on epoch=387
05/21/2022 10:21:42 - INFO - __main__ - Global step 1550 Train loss 0.85 Classification-F1 0.1 on epoch=387
05/21/2022 10:21:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.74 on epoch=389
05/21/2022 10:21:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.82 on epoch=392
05/21/2022 10:21:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.80 on epoch=394
05/21/2022 10:21:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.85 on epoch=397
05/21/2022 10:21:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.85 on epoch=399
05/21/2022 10:21:49 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.1 on epoch=399
05/21/2022 10:21:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.86 on epoch=402
05/21/2022 10:21:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.81 on epoch=404
05/21/2022 10:21:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.85 on epoch=407
05/21/2022 10:21:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.76 on epoch=409
05/21/2022 10:21:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.86 on epoch=412
05/21/2022 10:21:56 - INFO - __main__ - Global step 1650 Train loss 0.83 Classification-F1 0.1 on epoch=412
05/21/2022 10:21:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.84 on epoch=414
05/21/2022 10:21:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.86 on epoch=417
05/21/2022 10:22:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.85 on epoch=419
05/21/2022 10:22:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.79 on epoch=422
05/21/2022 10:22:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.87 on epoch=424
05/21/2022 10:22:03 - INFO - __main__ - Global step 1700 Train loss 0.84 Classification-F1 0.1 on epoch=424
05/21/2022 10:22:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.83 on epoch=427
05/21/2022 10:22:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.85 on epoch=429
05/21/2022 10:22:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.71 on epoch=432
05/21/2022 10:22:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.82 on epoch=434
05/21/2022 10:22:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.84 on epoch=437
05/21/2022 10:22:10 - INFO - __main__ - Global step 1750 Train loss 0.81 Classification-F1 0.1 on epoch=437
05/21/2022 10:22:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.72 on epoch=439
05/21/2022 10:22:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.81 on epoch=442
05/21/2022 10:22:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.82 on epoch=444
05/21/2022 10:22:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.83 on epoch=447
05/21/2022 10:22:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.76 on epoch=449
05/21/2022 10:22:17 - INFO - __main__ - Global step 1800 Train loss 0.79 Classification-F1 0.14915966386554622 on epoch=449
05/21/2022 10:22:17 - INFO - __main__ - Saving model with best Classification-F1: 0.13034188034188032 -> 0.14915966386554622 on epoch=449, global_step=1800
05/21/2022 10:22:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.87 on epoch=452
05/21/2022 10:22:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.78 on epoch=454
05/21/2022 10:22:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.88 on epoch=457
05/21/2022 10:22:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.80 on epoch=459
05/21/2022 10:22:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.84 on epoch=462
05/21/2022 10:22:24 - INFO - __main__ - Global step 1850 Train loss 0.84 Classification-F1 0.1 on epoch=462
05/21/2022 10:22:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.81 on epoch=464
05/21/2022 10:22:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.82 on epoch=467
05/21/2022 10:22:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.84 on epoch=469
05/21/2022 10:22:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.89 on epoch=472
05/21/2022 10:22:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.93 on epoch=474
05/21/2022 10:22:31 - INFO - __main__ - Global step 1900 Train loss 0.86 Classification-F1 0.09333333333333334 on epoch=474
05/21/2022 10:22:33 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.79 on epoch=477
05/21/2022 10:22:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.83 on epoch=479
05/21/2022 10:22:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.80 on epoch=482
05/21/2022 10:22:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.82 on epoch=484
05/21/2022 10:22:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.78 on epoch=487
05/21/2022 10:22:38 - INFO - __main__ - Global step 1950 Train loss 0.80 Classification-F1 0.12518037518037517 on epoch=487
05/21/2022 10:22:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.81 on epoch=489
05/21/2022 10:22:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.85 on epoch=492
05/21/2022 10:22:42 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.85 on epoch=494
05/21/2022 10:22:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.76 on epoch=497
05/21/2022 10:22:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.81 on epoch=499
05/21/2022 10:22:45 - INFO - __main__ - Global step 2000 Train loss 0.82 Classification-F1 0.1 on epoch=499
05/21/2022 10:22:46 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.84 on epoch=502
05/21/2022 10:22:48 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.82 on epoch=504
05/21/2022 10:22:49 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.82 on epoch=507
05/21/2022 10:22:50 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.82 on epoch=509
05/21/2022 10:22:51 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.84 on epoch=512
05/21/2022 10:22:52 - INFO - __main__ - Global step 2050 Train loss 0.83 Classification-F1 0.12393162393162392 on epoch=512
05/21/2022 10:22:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.84 on epoch=514
05/21/2022 10:22:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.80 on epoch=517
05/21/2022 10:22:56 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.88 on epoch=519
05/21/2022 10:22:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.81 on epoch=522
05/21/2022 10:22:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.79 on epoch=524
05/21/2022 10:22:59 - INFO - __main__ - Global step 2100 Train loss 0.83 Classification-F1 0.1 on epoch=524
05/21/2022 10:23:00 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.89 on epoch=527
05/21/2022 10:23:01 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.78 on epoch=529
05/21/2022 10:23:03 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.85 on epoch=532
05/21/2022 10:23:04 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.82 on epoch=534
05/21/2022 10:23:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.83 on epoch=537
05/21/2022 10:23:06 - INFO - __main__ - Global step 2150 Train loss 0.83 Classification-F1 0.09493670886075949 on epoch=537
05/21/2022 10:23:07 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.79 on epoch=539
05/21/2022 10:23:09 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.78 on epoch=542
05/21/2022 10:23:10 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.82 on epoch=544
05/21/2022 10:23:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.81 on epoch=547
05/21/2022 10:23:13 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.85 on epoch=549
05/21/2022 10:23:14 - INFO - __main__ - Global step 2200 Train loss 0.81 Classification-F1 0.1 on epoch=549
05/21/2022 10:23:15 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.87 on epoch=552
05/21/2022 10:23:17 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.85 on epoch=554
05/21/2022 10:23:18 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.83 on epoch=557
05/21/2022 10:23:19 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.79 on epoch=559
05/21/2022 10:23:20 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.82 on epoch=562
05/21/2022 10:23:21 - INFO - __main__ - Global step 2250 Train loss 0.83 Classification-F1 0.1 on epoch=562
05/21/2022 10:23:22 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.85 on epoch=564
05/21/2022 10:23:24 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.78 on epoch=567
05/21/2022 10:23:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.82 on epoch=569
05/21/2022 10:23:27 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.75 on epoch=572
05/21/2022 10:23:29 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.79 on epoch=574
05/21/2022 10:23:29 - INFO - __main__ - Global step 2300 Train loss 0.80 Classification-F1 0.10126582278481013 on epoch=574
05/21/2022 10:23:30 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.78 on epoch=577
05/21/2022 10:23:32 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.77 on epoch=579
05/21/2022 10:23:33 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.83 on epoch=582
05/21/2022 10:23:35 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.89 on epoch=584
05/21/2022 10:23:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.83 on epoch=587
05/21/2022 10:23:37 - INFO - __main__ - Global step 2350 Train loss 0.82 Classification-F1 0.10126582278481013 on epoch=587
05/21/2022 10:23:39 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.72 on epoch=589
05/21/2022 10:23:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.89 on epoch=592
05/21/2022 10:23:42 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.81 on epoch=594
05/21/2022 10:23:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.80 on epoch=597
05/21/2022 10:23:44 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.83 on epoch=599
05/21/2022 10:23:45 - INFO - __main__ - Global step 2400 Train loss 0.81 Classification-F1 0.09493670886075949 on epoch=599
05/21/2022 10:23:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.79 on epoch=602
05/21/2022 10:23:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.79 on epoch=604
05/21/2022 10:23:49 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.81 on epoch=607
05/21/2022 10:23:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.81 on epoch=609
05/21/2022 10:23:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.90 on epoch=612
05/21/2022 10:23:53 - INFO - __main__ - Global step 2450 Train loss 0.82 Classification-F1 0.09210526315789473 on epoch=612
05/21/2022 10:23:54 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.80 on epoch=614
05/21/2022 10:23:55 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.78 on epoch=617
05/21/2022 10:23:57 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.72 on epoch=619
05/21/2022 10:23:58 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.83 on epoch=622
05/21/2022 10:24:00 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.79 on epoch=624
05/21/2022 10:24:00 - INFO - __main__ - Global step 2500 Train loss 0.78 Classification-F1 0.1586672879776328 on epoch=624
05/21/2022 10:24:00 - INFO - __main__ - Saving model with best Classification-F1: 0.14915966386554622 -> 0.1586672879776328 on epoch=624, global_step=2500
05/21/2022 10:24:02 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.81 on epoch=627
05/21/2022 10:24:03 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.80 on epoch=629
05/21/2022 10:24:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.85 on epoch=632
05/21/2022 10:24:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.81 on epoch=634
05/21/2022 10:24:07 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.81 on epoch=637
05/21/2022 10:24:08 - INFO - __main__ - Global step 2550 Train loss 0.82 Classification-F1 0.09090909090909091 on epoch=637
05/21/2022 10:24:09 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.79 on epoch=639
05/21/2022 10:24:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.76 on epoch=642
05/21/2022 10:24:12 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.79 on epoch=644
05/21/2022 10:24:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.74 on epoch=647
05/21/2022 10:24:14 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.78 on epoch=649
05/21/2022 10:24:15 - INFO - __main__ - Global step 2600 Train loss 0.77 Classification-F1 0.14766081871345027 on epoch=649
05/21/2022 10:24:16 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.82 on epoch=652
05/21/2022 10:24:18 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.79 on epoch=654
05/21/2022 10:24:19 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.80 on epoch=657
05/21/2022 10:24:20 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.83 on epoch=659
05/21/2022 10:24:22 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.87 on epoch=662
05/21/2022 10:24:22 - INFO - __main__ - Global step 2650 Train loss 0.82 Classification-F1 0.12151702786377708 on epoch=662
05/21/2022 10:24:23 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.86 on epoch=664
05/21/2022 10:24:25 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.85 on epoch=667
05/21/2022 10:24:26 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.78 on epoch=669
05/21/2022 10:24:27 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.82 on epoch=672
05/21/2022 10:24:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.76 on epoch=674
05/21/2022 10:24:29 - INFO - __main__ - Global step 2700 Train loss 0.81 Classification-F1 0.11166666666666666 on epoch=674
05/21/2022 10:24:30 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.77 on epoch=677
05/21/2022 10:24:32 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.78 on epoch=679
05/21/2022 10:24:33 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.90 on epoch=682
05/21/2022 10:24:35 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.83 on epoch=684
05/21/2022 10:24:36 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.83 on epoch=687
05/21/2022 10:24:37 - INFO - __main__ - Global step 2750 Train loss 0.82 Classification-F1 0.12518037518037517 on epoch=687
05/21/2022 10:24:38 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.81 on epoch=689
05/21/2022 10:24:40 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.89 on epoch=692
05/21/2022 10:24:41 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.76 on epoch=694
05/21/2022 10:24:42 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.81 on epoch=697
05/21/2022 10:24:44 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.75 on epoch=699
05/21/2022 10:24:44 - INFO - __main__ - Global step 2800 Train loss 0.80 Classification-F1 0.16666666666666666 on epoch=699
05/21/2022 10:24:44 - INFO - __main__ - Saving model with best Classification-F1: 0.1586672879776328 -> 0.16666666666666666 on epoch=699, global_step=2800
05/21/2022 10:24:46 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.82 on epoch=702
05/21/2022 10:24:47 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.79 on epoch=704
05/21/2022 10:24:49 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.82 on epoch=707
05/21/2022 10:24:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.84 on epoch=709
05/21/2022 10:24:51 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.78 on epoch=712
05/21/2022 10:24:52 - INFO - __main__ - Global step 2850 Train loss 0.81 Classification-F1 0.10996955859969558 on epoch=712
05/21/2022 10:24:53 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.85 on epoch=714
05/21/2022 10:24:54 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.80 on epoch=717
05/21/2022 10:24:56 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.81 on epoch=719
05/21/2022 10:24:57 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.76 on epoch=722
05/21/2022 10:24:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.75 on epoch=724
05/21/2022 10:24:59 - INFO - __main__ - Global step 2900 Train loss 0.79 Classification-F1 0.11913145539906103 on epoch=724
05/21/2022 10:25:01 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.82 on epoch=727
05/21/2022 10:25:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.86 on epoch=729
05/21/2022 10:25:03 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.77 on epoch=732
05/21/2022 10:25:05 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.76 on epoch=734
05/21/2022 10:25:06 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.80 on epoch=737
05/21/2022 10:25:06 - INFO - __main__ - Global step 2950 Train loss 0.80 Classification-F1 0.10126582278481013 on epoch=737
05/21/2022 10:25:08 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.73 on epoch=739
05/21/2022 10:25:09 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.80 on epoch=742
05/21/2022 10:25:10 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.79 on epoch=744
05/21/2022 10:25:12 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.75 on epoch=747
05/21/2022 10:25:13 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.80 on epoch=749
05/21/2022 10:25:14 - INFO - __main__ - Global step 3000 Train loss 0.77 Classification-F1 0.15409356725146198 on epoch=749
05/21/2022 10:25:14 - INFO - __main__ - save last model!
05/21/2022 10:25:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 10:25:14 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 10:25:14 - INFO - __main__ - Printing 3 examples
05/21/2022 10:25:14 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 10:25:14 - INFO - __main__ - ['others']
05/21/2022 10:25:14 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 10:25:14 - INFO - __main__ - ['others']
05/21/2022 10:25:14 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 10:25:14 - INFO - __main__ - ['others']
05/21/2022 10:25:14 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:25:14 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:25:14 - INFO - __main__ - Printing 3 examples
05/21/2022 10:25:14 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 10:25:14 - INFO - __main__ - ['sad']
05/21/2022 10:25:14 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 10:25:14 - INFO - __main__ - ['sad']
05/21/2022 10:25:14 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 10:25:14 - INFO - __main__ - ['sad']
05/21/2022 10:25:14 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:25:14 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:25:15 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 10:25:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:25:15 - INFO - __main__ - Printing 3 examples
05/21/2022 10:25:15 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/21/2022 10:25:15 - INFO - __main__ - ['sad']
05/21/2022 10:25:15 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/21/2022 10:25:15 - INFO - __main__ - ['sad']
05/21/2022 10:25:15 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/21/2022 10:25:15 - INFO - __main__ - ['sad']
05/21/2022 10:25:15 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:25:15 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:25:15 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 10:25:16 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:25:20 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 10:25:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 10:25:21 - INFO - __main__ - Starting training!
05/21/2022 10:25:21 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 10:26:05 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_13_0.2_8_predictions.txt
05/21/2022 10:26:05 - INFO - __main__ - Classification-F1 on test data: 0.0646
05/21/2022 10:26:05 - INFO - __main__ - prefix=emo_16_13, lr=0.2, bsz=8, dev_performance=0.16666666666666666, test_performance=0.0645849793808082
05/21/2022 10:26:05 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.5, bsz=8 ...
05/21/2022 10:26:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:26:06 - INFO - __main__ - Printing 3 examples
05/21/2022 10:26:06 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 10:26:06 - INFO - __main__ - ['sad']
05/21/2022 10:26:06 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 10:26:06 - INFO - __main__ - ['sad']
05/21/2022 10:26:06 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 10:26:06 - INFO - __main__ - ['sad']
05/21/2022 10:26:06 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:26:06 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:26:06 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 10:26:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:26:06 - INFO - __main__ - Printing 3 examples
05/21/2022 10:26:06 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/21/2022 10:26:06 - INFO - __main__ - ['sad']
05/21/2022 10:26:06 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/21/2022 10:26:06 - INFO - __main__ - ['sad']
05/21/2022 10:26:06 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/21/2022 10:26:06 - INFO - __main__ - ['sad']
05/21/2022 10:26:06 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:26:06 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:26:06 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 10:26:12 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 10:26:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 10:26:12 - INFO - __main__ - Starting training!
05/21/2022 10:26:14 - INFO - __main__ - Step 10 Global step 10 Train loss 4.43 on epoch=2
05/21/2022 10:26:15 - INFO - __main__ - Step 20 Global step 20 Train loss 3.79 on epoch=4
05/21/2022 10:26:16 - INFO - __main__ - Step 30 Global step 30 Train loss 3.22 on epoch=7
05/21/2022 10:26:18 - INFO - __main__ - Step 40 Global step 40 Train loss 2.74 on epoch=9
05/21/2022 10:26:19 - INFO - __main__ - Step 50 Global step 50 Train loss 2.22 on epoch=12
05/21/2022 10:26:20 - INFO - __main__ - Global step 50 Train loss 3.28 Classification-F1 0.1 on epoch=12
05/21/2022 10:26:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/21/2022 10:26:21 - INFO - __main__ - Step 60 Global step 60 Train loss 1.77 on epoch=14
05/21/2022 10:26:22 - INFO - __main__ - Step 70 Global step 70 Train loss 1.66 on epoch=17
05/21/2022 10:26:24 - INFO - __main__ - Step 80 Global step 80 Train loss 1.30 on epoch=19
05/21/2022 10:26:25 - INFO - __main__ - Step 90 Global step 90 Train loss 1.07 on epoch=22
05/21/2022 10:26:26 - INFO - __main__ - Step 100 Global step 100 Train loss 1.10 on epoch=24
05/21/2022 10:26:27 - INFO - __main__ - Global step 100 Train loss 1.38 Classification-F1 0.1 on epoch=24
05/21/2022 10:26:28 - INFO - __main__ - Step 110 Global step 110 Train loss 1.03 on epoch=27
05/21/2022 10:26:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.97 on epoch=29
05/21/2022 10:26:31 - INFO - __main__ - Step 130 Global step 130 Train loss 1.00 on epoch=32
05/21/2022 10:26:32 - INFO - __main__ - Step 140 Global step 140 Train loss 1.02 on epoch=34
05/21/2022 10:26:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.91 on epoch=37
05/21/2022 10:26:34 - INFO - __main__ - Global step 150 Train loss 0.99 Classification-F1 0.1 on epoch=37
05/21/2022 10:26:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.90 on epoch=39
05/21/2022 10:26:37 - INFO - __main__ - Step 170 Global step 170 Train loss 1.01 on epoch=42
05/21/2022 10:26:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.98 on epoch=44
05/21/2022 10:26:40 - INFO - __main__ - Step 190 Global step 190 Train loss 1.01 on epoch=47
05/21/2022 10:26:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.91 on epoch=49
05/21/2022 10:26:42 - INFO - __main__ - Global step 200 Train loss 0.96 Classification-F1 0.09493670886075949 on epoch=49
05/21/2022 10:26:43 - INFO - __main__ - Step 210 Global step 210 Train loss 1.00 on epoch=52
05/21/2022 10:26:45 - INFO - __main__ - Step 220 Global step 220 Train loss 1.02 on epoch=54
05/21/2022 10:26:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.93 on epoch=57
05/21/2022 10:26:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.93 on epoch=59
05/21/2022 10:26:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.85 on epoch=62
05/21/2022 10:26:49 - INFO - __main__ - Global step 250 Train loss 0.95 Classification-F1 0.0974025974025974 on epoch=62
05/21/2022 10:26:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.89 on epoch=64
05/21/2022 10:26:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.94 on epoch=67
05/21/2022 10:26:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.83 on epoch=69
05/21/2022 10:26:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.85 on epoch=72
05/21/2022 10:26:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.92 on epoch=74
05/21/2022 10:26:57 - INFO - __main__ - Global step 300 Train loss 0.89 Classification-F1 0.1 on epoch=74
05/21/2022 10:26:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.93 on epoch=77
05/21/2022 10:27:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.86 on epoch=79
05/21/2022 10:27:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.87 on epoch=82
05/21/2022 10:27:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.90 on epoch=84
05/21/2022 10:27:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.95 on epoch=87
05/21/2022 10:27:05 - INFO - __main__ - Global step 350 Train loss 0.90 Classification-F1 0.11762954139368673 on epoch=87
05/21/2022 10:27:05 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.11762954139368673 on epoch=87, global_step=350
05/21/2022 10:27:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.85 on epoch=89
05/21/2022 10:27:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.90 on epoch=92
05/21/2022 10:27:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.92 on epoch=94
05/21/2022 10:27:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=97
05/21/2022 10:27:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.89 on epoch=99
05/21/2022 10:27:12 - INFO - __main__ - Global step 400 Train loss 0.89 Classification-F1 0.1 on epoch=99
05/21/2022 10:27:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.83 on epoch=102
05/21/2022 10:27:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.85 on epoch=104
05/21/2022 10:27:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.84 on epoch=107
05/21/2022 10:27:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.79 on epoch=109
05/21/2022 10:27:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.79 on epoch=112
05/21/2022 10:27:19 - INFO - __main__ - Global step 450 Train loss 0.82 Classification-F1 0.08783783783783784 on epoch=112
05/21/2022 10:27:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.80 on epoch=114
05/21/2022 10:27:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.89 on epoch=117
05/21/2022 10:27:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.83 on epoch=119
05/21/2022 10:27:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.85 on epoch=122
05/21/2022 10:27:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.86 on epoch=124
05/21/2022 10:27:27 - INFO - __main__ - Global step 500 Train loss 0.85 Classification-F1 0.1 on epoch=124
05/21/2022 10:27:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.78 on epoch=127
05/21/2022 10:27:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.81 on epoch=129
05/21/2022 10:27:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.85 on epoch=132
05/21/2022 10:27:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.82 on epoch=134
05/21/2022 10:27:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.89 on epoch=137
05/21/2022 10:27:35 - INFO - __main__ - Global step 550 Train loss 0.83 Classification-F1 0.09090909090909091 on epoch=137
05/21/2022 10:27:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.89 on epoch=139
05/21/2022 10:27:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.84 on epoch=142
05/21/2022 10:27:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.87 on epoch=144
05/21/2022 10:27:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.85 on epoch=147
05/21/2022 10:27:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.82 on epoch=149
05/21/2022 10:27:43 - INFO - __main__ - Global step 600 Train loss 0.85 Classification-F1 0.1 on epoch=149
05/21/2022 10:27:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.86 on epoch=152
05/21/2022 10:27:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.79 on epoch=154
05/21/2022 10:27:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.82 on epoch=157
05/21/2022 10:27:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.78 on epoch=159
05/21/2022 10:27:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.85 on epoch=162
05/21/2022 10:27:51 - INFO - __main__ - Global step 650 Train loss 0.82 Classification-F1 0.09615384615384615 on epoch=162
05/21/2022 10:27:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.79 on epoch=164
05/21/2022 10:27:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.86 on epoch=167
05/21/2022 10:27:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.82 on epoch=169
05/21/2022 10:27:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.89 on epoch=172
05/21/2022 10:27:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.88 on epoch=174
05/21/2022 10:27:58 - INFO - __main__ - Global step 700 Train loss 0.85 Classification-F1 0.09493670886075949 on epoch=174
05/21/2022 10:28:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.88 on epoch=177
05/21/2022 10:28:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.80 on epoch=179
05/21/2022 10:28:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.88 on epoch=182
05/21/2022 10:28:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.75 on epoch=184
05/21/2022 10:28:04 - INFO - __main__ - Step 750 Global step 750 Train loss 0.80 on epoch=187
05/21/2022 10:28:05 - INFO - __main__ - Global step 750 Train loss 0.82 Classification-F1 0.1659804426145136 on epoch=187
05/21/2022 10:28:05 - INFO - __main__ - Saving model with best Classification-F1: 0.11762954139368673 -> 0.1659804426145136 on epoch=187, global_step=750
05/21/2022 10:28:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.83 on epoch=189
05/21/2022 10:28:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.75 on epoch=192
05/21/2022 10:28:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.76 on epoch=194
05/21/2022 10:28:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.83 on epoch=197
05/21/2022 10:28:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.77 on epoch=199
05/21/2022 10:28:13 - INFO - __main__ - Global step 800 Train loss 0.79 Classification-F1 0.2201454340075541 on epoch=199
05/21/2022 10:28:13 - INFO - __main__ - Saving model with best Classification-F1: 0.1659804426145136 -> 0.2201454340075541 on epoch=199, global_step=800
05/21/2022 10:28:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.76 on epoch=202
05/21/2022 10:28:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.77 on epoch=204
05/21/2022 10:28:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.83 on epoch=207
05/21/2022 10:28:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.80 on epoch=209
05/21/2022 10:28:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.78 on epoch=212
05/21/2022 10:28:20 - INFO - __main__ - Global step 850 Train loss 0.79 Classification-F1 0.23790322580645162 on epoch=212
05/21/2022 10:28:21 - INFO - __main__ - Saving model with best Classification-F1: 0.2201454340075541 -> 0.23790322580645162 on epoch=212, global_step=850
05/21/2022 10:28:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.82 on epoch=214
05/21/2022 10:28:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.84 on epoch=217
05/21/2022 10:28:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.75 on epoch=219
05/21/2022 10:28:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.76 on epoch=222
05/21/2022 10:28:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.82 on epoch=224
05/21/2022 10:28:29 - INFO - __main__ - Global step 900 Train loss 0.80 Classification-F1 0.4600563457706315 on epoch=224
05/21/2022 10:28:29 - INFO - __main__ - Saving model with best Classification-F1: 0.23790322580645162 -> 0.4600563457706315 on epoch=224, global_step=900
05/21/2022 10:28:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.72 on epoch=227
05/21/2022 10:28:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.72 on epoch=229
05/21/2022 10:28:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.74 on epoch=232
05/21/2022 10:28:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.79 on epoch=234
05/21/2022 10:28:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.75 on epoch=237
05/21/2022 10:28:37 - INFO - __main__ - Global step 950 Train loss 0.74 Classification-F1 0.4319101950680898 on epoch=237
05/21/2022 10:28:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.70 on epoch=239
05/21/2022 10:28:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.72 on epoch=242
05/21/2022 10:28:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.83 on epoch=244
05/21/2022 10:28:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.73 on epoch=247
05/21/2022 10:28:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.76 on epoch=249
05/21/2022 10:28:45 - INFO - __main__ - Global step 1000 Train loss 0.75 Classification-F1 0.5321621621621622 on epoch=249
05/21/2022 10:28:45 - INFO - __main__ - Saving model with best Classification-F1: 0.4600563457706315 -> 0.5321621621621622 on epoch=249, global_step=1000
05/21/2022 10:28:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.77 on epoch=252
05/21/2022 10:28:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.74 on epoch=254
05/21/2022 10:28:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.80 on epoch=257
05/21/2022 10:28:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.70 on epoch=259
05/21/2022 10:28:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.69 on epoch=262
05/21/2022 10:28:52 - INFO - __main__ - Global step 1050 Train loss 0.74 Classification-F1 0.517786561264822 on epoch=262
05/21/2022 10:28:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.68 on epoch=264
05/21/2022 10:28:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.73 on epoch=267
05/21/2022 10:28:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.70 on epoch=269
05/21/2022 10:28:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.70 on epoch=272
05/21/2022 10:28:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.68 on epoch=274
05/21/2022 10:29:00 - INFO - __main__ - Global step 1100 Train loss 0.70 Classification-F1 0.46862073883896727 on epoch=274
05/21/2022 10:29:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.69 on epoch=277
05/21/2022 10:29:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.70 on epoch=279
05/21/2022 10:29:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.64 on epoch=282
05/21/2022 10:29:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.71 on epoch=284
05/21/2022 10:29:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.72 on epoch=287
05/21/2022 10:29:07 - INFO - __main__ - Global step 1150 Train loss 0.69 Classification-F1 0.5271083505866114 on epoch=287
05/21/2022 10:29:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.64 on epoch=289
05/21/2022 10:29:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.72 on epoch=292
05/21/2022 10:29:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.71 on epoch=294
05/21/2022 10:29:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.71 on epoch=297
05/21/2022 10:29:15 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.64 on epoch=299
05/21/2022 10:29:15 - INFO - __main__ - Global step 1200 Train loss 0.68 Classification-F1 0.547620494988916 on epoch=299
05/21/2022 10:29:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5321621621621622 -> 0.547620494988916 on epoch=299, global_step=1200
05/21/2022 10:29:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.62 on epoch=302
05/21/2022 10:29:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.67 on epoch=304
05/21/2022 10:29:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.75 on epoch=307
05/21/2022 10:29:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.64 on epoch=309
05/21/2022 10:29:22 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.65 on epoch=312
05/21/2022 10:29:23 - INFO - __main__ - Global step 1250 Train loss 0.67 Classification-F1 0.5177569254701742 on epoch=312
05/21/2022 10:29:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.62 on epoch=314
05/21/2022 10:29:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.67 on epoch=317
05/21/2022 10:29:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.70 on epoch=319
05/21/2022 10:29:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.58 on epoch=322
05/21/2022 10:29:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.66 on epoch=324
05/21/2022 10:29:31 - INFO - __main__ - Global step 1300 Train loss 0.65 Classification-F1 0.5617438867438866 on epoch=324
05/21/2022 10:29:31 - INFO - __main__ - Saving model with best Classification-F1: 0.547620494988916 -> 0.5617438867438866 on epoch=324, global_step=1300
05/21/2022 10:29:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.78 on epoch=327
05/21/2022 10:29:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.70 on epoch=329
05/21/2022 10:29:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.70 on epoch=332
05/21/2022 10:29:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.56 on epoch=334
05/21/2022 10:29:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.72 on epoch=337
05/21/2022 10:29:39 - INFO - __main__ - Global step 1350 Train loss 0.69 Classification-F1 0.5311258916522075 on epoch=337
05/21/2022 10:29:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.61 on epoch=339
05/21/2022 10:29:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.66 on epoch=342
05/21/2022 10:29:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.62 on epoch=344
05/21/2022 10:29:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.65 on epoch=347
05/21/2022 10:29:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.58 on epoch=349
05/21/2022 10:29:47 - INFO - __main__ - Global step 1400 Train loss 0.63 Classification-F1 0.5001569593354583 on epoch=349
05/21/2022 10:29:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.65 on epoch=352
05/21/2022 10:29:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.61 on epoch=354
05/21/2022 10:29:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.66 on epoch=357
05/21/2022 10:29:52 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.59 on epoch=359
05/21/2022 10:29:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.67 on epoch=362
05/21/2022 10:29:54 - INFO - __main__ - Global step 1450 Train loss 0.63 Classification-F1 0.6007635175282234 on epoch=362
05/21/2022 10:29:54 - INFO - __main__ - Saving model with best Classification-F1: 0.5617438867438866 -> 0.6007635175282234 on epoch=362, global_step=1450
05/21/2022 10:29:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.54 on epoch=364
05/21/2022 10:29:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.64 on epoch=367
05/21/2022 10:29:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.56 on epoch=369
05/21/2022 10:30:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.63 on epoch=372
05/21/2022 10:30:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.60 on epoch=374
05/21/2022 10:30:02 - INFO - __main__ - Global step 1500 Train loss 0.59 Classification-F1 0.4748803827751196 on epoch=374
05/21/2022 10:30:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.63 on epoch=377
05/21/2022 10:30:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.66 on epoch=379
05/21/2022 10:30:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.54 on epoch=382
05/21/2022 10:30:08 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.57 on epoch=384
05/21/2022 10:30:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.55 on epoch=387
05/21/2022 10:30:10 - INFO - __main__ - Global step 1550 Train loss 0.59 Classification-F1 0.5712074303405573 on epoch=387
05/21/2022 10:30:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.53 on epoch=389
05/21/2022 10:30:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.60 on epoch=392
05/21/2022 10:30:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.59 on epoch=394
05/21/2022 10:30:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.57 on epoch=397
05/21/2022 10:30:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.55 on epoch=399
05/21/2022 10:30:17 - INFO - __main__ - Global step 1600 Train loss 0.57 Classification-F1 0.46859349800526273 on epoch=399
05/21/2022 10:30:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.60 on epoch=402
05/21/2022 10:30:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.55 on epoch=404
05/21/2022 10:30:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.60 on epoch=407
05/21/2022 10:30:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.55 on epoch=409
05/21/2022 10:30:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.58 on epoch=412
05/21/2022 10:30:25 - INFO - __main__ - Global step 1650 Train loss 0.58 Classification-F1 0.5299242424242424 on epoch=412
05/21/2022 10:30:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.55 on epoch=414
05/21/2022 10:30:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.60 on epoch=417
05/21/2022 10:30:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.52 on epoch=419
05/21/2022 10:30:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.54 on epoch=422
05/21/2022 10:30:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.65 on epoch=424
05/21/2022 10:30:34 - INFO - __main__ - Global step 1700 Train loss 0.57 Classification-F1 0.472436974789916 on epoch=424
05/21/2022 10:30:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.47 on epoch=427
05/21/2022 10:30:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.58 on epoch=429
05/21/2022 10:30:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.62 on epoch=432
05/21/2022 10:30:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.52 on epoch=434
05/21/2022 10:30:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.60 on epoch=437
05/21/2022 10:30:41 - INFO - __main__ - Global step 1750 Train loss 0.56 Classification-F1 0.6154761904761905 on epoch=437
05/21/2022 10:30:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6007635175282234 -> 0.6154761904761905 on epoch=437, global_step=1750
05/21/2022 10:30:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.53 on epoch=439
05/21/2022 10:30:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.57 on epoch=442
05/21/2022 10:30:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.54 on epoch=444
05/21/2022 10:30:47 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.53 on epoch=447
05/21/2022 10:30:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.53 on epoch=449
05/21/2022 10:30:49 - INFO - __main__ - Global step 1800 Train loss 0.54 Classification-F1 0.4994926641985466 on epoch=449
05/21/2022 10:30:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.58 on epoch=452
05/21/2022 10:30:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.56 on epoch=454
05/21/2022 10:30:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.54 on epoch=457
05/21/2022 10:30:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.53 on epoch=459
05/21/2022 10:30:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.56 on epoch=462
05/21/2022 10:30:56 - INFO - __main__ - Global step 1850 Train loss 0.55 Classification-F1 0.7003708133971291 on epoch=462
05/21/2022 10:30:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6154761904761905 -> 0.7003708133971291 on epoch=462, global_step=1850
05/21/2022 10:30:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.54 on epoch=464
05/21/2022 10:30:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.58 on epoch=467
05/21/2022 10:31:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.51 on epoch=469
05/21/2022 10:31:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.52 on epoch=472
05/21/2022 10:31:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.49 on epoch=474
05/21/2022 10:31:04 - INFO - __main__ - Global step 1900 Train loss 0.53 Classification-F1 0.5125272331154684 on epoch=474
05/21/2022 10:31:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.53 on epoch=477
05/21/2022 10:31:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.53 on epoch=479
05/21/2022 10:31:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.54 on epoch=482
05/21/2022 10:31:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=484
05/21/2022 10:31:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.53 on epoch=487
05/21/2022 10:31:11 - INFO - __main__ - Global step 1950 Train loss 0.51 Classification-F1 0.686032800216861 on epoch=487
05/21/2022 10:31:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=489
05/21/2022 10:31:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.59 on epoch=492
05/21/2022 10:31:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=494
05/21/2022 10:31:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.44 on epoch=497
05/21/2022 10:31:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.52 on epoch=499
05/21/2022 10:31:19 - INFO - __main__ - Global step 2000 Train loss 0.49 Classification-F1 0.5123725348725349 on epoch=499
05/21/2022 10:31:20 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.54 on epoch=502
05/21/2022 10:31:22 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.44 on epoch=504
05/21/2022 10:31:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.49 on epoch=507
05/21/2022 10:31:24 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.42 on epoch=509
05/21/2022 10:31:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.49 on epoch=512
05/21/2022 10:31:26 - INFO - __main__ - Global step 2050 Train loss 0.48 Classification-F1 0.6726190476190477 on epoch=512
05/21/2022 10:31:27 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.47 on epoch=514
05/21/2022 10:31:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.55 on epoch=517
05/21/2022 10:31:30 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.47 on epoch=519
05/21/2022 10:31:32 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.37 on epoch=522
05/21/2022 10:31:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.47 on epoch=524
05/21/2022 10:31:33 - INFO - __main__ - Global step 2100 Train loss 0.47 Classification-F1 0.6713269178639198 on epoch=524
05/21/2022 10:31:35 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.53 on epoch=527
05/21/2022 10:31:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.45 on epoch=529
05/21/2022 10:31:37 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.57 on epoch=532
05/21/2022 10:31:39 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.36 on epoch=534
05/21/2022 10:31:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.48 on epoch=537
05/21/2022 10:31:41 - INFO - __main__ - Global step 2150 Train loss 0.48 Classification-F1 0.6880684420598051 on epoch=537
05/21/2022 10:31:42 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.43 on epoch=539
05/21/2022 10:31:43 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.41 on epoch=542
05/21/2022 10:31:45 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.47 on epoch=544
05/21/2022 10:31:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.52 on epoch=547
05/21/2022 10:31:48 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.42 on epoch=549
05/21/2022 10:31:49 - INFO - __main__ - Global step 2200 Train loss 0.45 Classification-F1 0.7352954913438785 on epoch=549
05/21/2022 10:31:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7003708133971291 -> 0.7352954913438785 on epoch=549, global_step=2200
05/21/2022 10:31:50 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.46 on epoch=552
05/21/2022 10:31:52 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.48 on epoch=554
05/21/2022 10:31:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.40 on epoch=557
05/21/2022 10:31:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.45 on epoch=559
05/21/2022 10:31:56 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.50 on epoch=562
05/21/2022 10:31:56 - INFO - __main__ - Global step 2250 Train loss 0.46 Classification-F1 0.7336124896608768 on epoch=562
05/21/2022 10:31:58 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.50 on epoch=564
05/21/2022 10:31:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.46 on epoch=567
05/21/2022 10:32:01 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.41 on epoch=569
05/21/2022 10:32:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.41 on epoch=572
05/21/2022 10:32:03 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.40 on epoch=574
05/21/2022 10:32:04 - INFO - __main__ - Global step 2300 Train loss 0.44 Classification-F1 0.7191295546558705 on epoch=574
05/21/2022 10:32:05 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.39 on epoch=577
05/21/2022 10:32:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.42 on epoch=579
05/21/2022 10:32:08 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.41 on epoch=582
05/21/2022 10:32:10 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.44 on epoch=584
05/21/2022 10:32:11 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.36 on epoch=587
05/21/2022 10:32:12 - INFO - __main__ - Global step 2350 Train loss 0.41 Classification-F1 0.6554388422035482 on epoch=587
05/21/2022 10:32:14 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.43 on epoch=589
05/21/2022 10:32:15 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.45 on epoch=592
05/21/2022 10:32:17 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.42 on epoch=594
05/21/2022 10:32:18 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.36 on epoch=597
05/21/2022 10:32:20 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.40 on epoch=599
05/21/2022 10:32:21 - INFO - __main__ - Global step 2400 Train loss 0.41 Classification-F1 0.7180707805707804 on epoch=599
05/21/2022 10:32:22 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.42 on epoch=602
05/21/2022 10:32:24 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.36 on epoch=604
05/21/2022 10:32:25 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.39 on epoch=607
05/21/2022 10:32:27 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.44 on epoch=609
05/21/2022 10:32:28 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.42 on epoch=612
05/21/2022 10:32:28 - INFO - __main__ - Global step 2450 Train loss 0.41 Classification-F1 0.7180707805707804 on epoch=612
05/21/2022 10:32:30 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.37 on epoch=614
05/21/2022 10:32:31 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.44 on epoch=617
05/21/2022 10:32:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.35 on epoch=619
05/21/2022 10:32:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.44 on epoch=622
05/21/2022 10:32:35 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.45 on epoch=624
05/21/2022 10:32:36 - INFO - __main__ - Global step 2500 Train loss 0.41 Classification-F1 0.6668859649122807 on epoch=624
05/21/2022 10:32:37 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.35 on epoch=627
05/21/2022 10:32:39 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.35 on epoch=629
05/21/2022 10:32:41 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.42 on epoch=632
05/21/2022 10:32:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.38 on epoch=634
05/21/2022 10:32:44 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.35 on epoch=637
05/21/2022 10:32:44 - INFO - __main__ - Global step 2550 Train loss 0.37 Classification-F1 0.6733343875184482 on epoch=637
05/21/2022 10:32:46 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.34 on epoch=639
05/21/2022 10:32:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.35 on epoch=642
05/21/2022 10:32:49 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.40 on epoch=644
05/21/2022 10:32:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.44 on epoch=647
05/21/2022 10:32:51 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.34 on epoch=649
05/21/2022 10:32:52 - INFO - __main__ - Global step 2600 Train loss 0.37 Classification-F1 0.6510100884294433 on epoch=649
05/21/2022 10:32:53 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.41 on epoch=652
05/21/2022 10:32:54 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.45 on epoch=654
05/21/2022 10:32:56 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.39 on epoch=657
05/21/2022 10:32:57 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.38 on epoch=659
05/21/2022 10:32:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.44 on epoch=662
05/21/2022 10:33:00 - INFO - __main__ - Global step 2650 Train loss 0.42 Classification-F1 0.7502253775073248 on epoch=662
05/21/2022 10:33:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7352954913438785 -> 0.7502253775073248 on epoch=662, global_step=2650
05/21/2022 10:33:01 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.37 on epoch=664
05/21/2022 10:33:02 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.40 on epoch=667
05/21/2022 10:33:04 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.33 on epoch=669
05/21/2022 10:33:05 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.37 on epoch=672
05/21/2022 10:33:06 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.35 on epoch=674
05/21/2022 10:33:07 - INFO - __main__ - Global step 2700 Train loss 0.36 Classification-F1 0.645691975285243 on epoch=674
05/21/2022 10:33:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.43 on epoch=677
05/21/2022 10:33:10 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.26 on epoch=679
05/21/2022 10:33:11 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.34 on epoch=682
05/21/2022 10:33:13 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.28 on epoch=684
05/21/2022 10:33:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.31 on epoch=687
05/21/2022 10:33:15 - INFO - __main__ - Global step 2750 Train loss 0.32 Classification-F1 0.751259115066123 on epoch=687
05/21/2022 10:33:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7502253775073248 -> 0.751259115066123 on epoch=687, global_step=2750
05/21/2022 10:33:16 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.30 on epoch=689
05/21/2022 10:33:17 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.31 on epoch=692
05/21/2022 10:33:19 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.35 on epoch=694
05/21/2022 10:33:20 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.35 on epoch=697
05/21/2022 10:33:22 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.32 on epoch=699
05/21/2022 10:33:22 - INFO - __main__ - Global step 2800 Train loss 0.32 Classification-F1 0.698724716144071 on epoch=699
05/21/2022 10:33:23 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.29 on epoch=702
05/21/2022 10:33:25 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.31 on epoch=704
05/21/2022 10:33:26 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.37 on epoch=707
05/21/2022 10:33:28 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.30 on epoch=709
05/21/2022 10:33:29 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.33 on epoch=712
05/21/2022 10:33:30 - INFO - __main__ - Global step 2850 Train loss 0.32 Classification-F1 0.7172916666666667 on epoch=712
05/21/2022 10:33:32 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.31 on epoch=714
05/21/2022 10:33:33 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.43 on epoch=717
05/21/2022 10:33:35 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.34 on epoch=719
05/21/2022 10:33:36 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.29 on epoch=722
05/21/2022 10:33:38 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.27 on epoch=724
05/21/2022 10:33:39 - INFO - __main__ - Global step 2900 Train loss 0.33 Classification-F1 0.7168181818181818 on epoch=724
05/21/2022 10:33:41 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.36 on epoch=727
05/21/2022 10:33:42 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.35 on epoch=729
05/21/2022 10:33:44 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.33 on epoch=732
05/21/2022 10:33:45 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.29 on epoch=734
05/21/2022 10:33:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.30 on epoch=737
05/21/2022 10:33:47 - INFO - __main__ - Global step 2950 Train loss 0.33 Classification-F1 0.6860272913403844 on epoch=737
05/21/2022 10:33:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.42 on epoch=739
05/21/2022 10:33:50 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.27 on epoch=742
05/21/2022 10:33:52 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.27 on epoch=744
05/21/2022 10:33:53 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.37 on epoch=747
05/21/2022 10:33:55 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.29 on epoch=749
05/21/2022 10:33:55 - INFO - __main__ - Global step 3000 Train loss 0.32 Classification-F1 0.6882114789235533 on epoch=749
05/21/2022 10:33:55 - INFO - __main__ - save last model!
05/21/2022 10:33:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 10:33:55 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 10:33:55 - INFO - __main__ - Printing 3 examples
05/21/2022 10:33:55 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 10:33:55 - INFO - __main__ - ['others']
05/21/2022 10:33:55 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 10:33:55 - INFO - __main__ - ['others']
05/21/2022 10:33:55 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 10:33:55 - INFO - __main__ - ['others']
05/21/2022 10:33:55 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:33:56 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:33:56 - INFO - __main__ - Printing 3 examples
05/21/2022 10:33:56 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 10:33:56 - INFO - __main__ - ['sad']
05/21/2022 10:33:56 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 10:33:56 - INFO - __main__ - ['sad']
05/21/2022 10:33:56 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 10:33:56 - INFO - __main__ - ['sad']
05/21/2022 10:33:56 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:33:56 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:33:56 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 10:33:56 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:33:56 - INFO - __main__ - Printing 3 examples
05/21/2022 10:33:56 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/21/2022 10:33:56 - INFO - __main__ - ['sad']
05/21/2022 10:33:56 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/21/2022 10:33:56 - INFO - __main__ - ['sad']
05/21/2022 10:33:56 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/21/2022 10:33:56 - INFO - __main__ - ['sad']
05/21/2022 10:33:56 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:33:56 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:33:56 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 10:33:57 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:34:02 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 10:34:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 10:34:02 - INFO - __main__ - Starting training!
05/21/2022 10:34:03 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 10:34:51 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_21_0.5_8_predictions.txt
05/21/2022 10:34:51 - INFO - __main__ - Classification-F1 on test data: 0.1296
05/21/2022 10:34:51 - INFO - __main__ - prefix=emo_16_21, lr=0.5, bsz=8, dev_performance=0.751259115066123, test_performance=0.1295880342062252
05/21/2022 10:34:51 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.4, bsz=8 ...
05/21/2022 10:34:52 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:34:52 - INFO - __main__ - Printing 3 examples
05/21/2022 10:34:52 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 10:34:52 - INFO - __main__ - ['sad']
05/21/2022 10:34:52 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 10:34:52 - INFO - __main__ - ['sad']
05/21/2022 10:34:52 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 10:34:52 - INFO - __main__ - ['sad']
05/21/2022 10:34:52 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:34:52 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:34:52 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 10:34:52 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:34:52 - INFO - __main__ - Printing 3 examples
05/21/2022 10:34:52 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/21/2022 10:34:52 - INFO - __main__ - ['sad']
05/21/2022 10:34:52 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/21/2022 10:34:52 - INFO - __main__ - ['sad']
05/21/2022 10:34:52 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/21/2022 10:34:52 - INFO - __main__ - ['sad']
05/21/2022 10:34:52 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:34:52 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:34:52 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 10:34:59 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 10:34:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 10:34:59 - INFO - __main__ - Starting training!
05/21/2022 10:35:01 - INFO - __main__ - Step 10 Global step 10 Train loss 4.49 on epoch=2
05/21/2022 10:35:02 - INFO - __main__ - Step 20 Global step 20 Train loss 3.88 on epoch=4
05/21/2022 10:35:04 - INFO - __main__ - Step 30 Global step 30 Train loss 3.47 on epoch=7
05/21/2022 10:35:06 - INFO - __main__ - Step 40 Global step 40 Train loss 2.98 on epoch=9
05/21/2022 10:35:07 - INFO - __main__ - Step 50 Global step 50 Train loss 2.57 on epoch=12
05/21/2022 10:35:08 - INFO - __main__ - Global step 50 Train loss 3.48 Classification-F1 0.1 on epoch=12
05/21/2022 10:35:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/21/2022 10:35:09 - INFO - __main__ - Step 60 Global step 60 Train loss 2.19 on epoch=14
05/21/2022 10:35:11 - INFO - __main__ - Step 70 Global step 70 Train loss 1.94 on epoch=17
05/21/2022 10:35:12 - INFO - __main__ - Step 80 Global step 80 Train loss 1.69 on epoch=19
05/21/2022 10:35:14 - INFO - __main__ - Step 90 Global step 90 Train loss 1.53 on epoch=22
05/21/2022 10:35:15 - INFO - __main__ - Step 100 Global step 100 Train loss 1.30 on epoch=24
05/21/2022 10:35:16 - INFO - __main__ - Global step 100 Train loss 1.73 Classification-F1 0.14211309523809523 on epoch=24
05/21/2022 10:35:16 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.14211309523809523 on epoch=24, global_step=100
05/21/2022 10:35:17 - INFO - __main__ - Step 110 Global step 110 Train loss 1.43 on epoch=27
05/21/2022 10:35:19 - INFO - __main__ - Step 120 Global step 120 Train loss 1.19 on epoch=29
05/21/2022 10:35:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.99 on epoch=32
05/21/2022 10:35:22 - INFO - __main__ - Step 140 Global step 140 Train loss 1.09 on epoch=34
05/21/2022 10:35:23 - INFO - __main__ - Step 150 Global step 150 Train loss 1.15 on epoch=37
05/21/2022 10:35:24 - INFO - __main__ - Global step 150 Train loss 1.17 Classification-F1 0.14180672268907563 on epoch=37
05/21/2022 10:35:25 - INFO - __main__ - Step 160 Global step 160 Train loss 1.00 on epoch=39
05/21/2022 10:35:27 - INFO - __main__ - Step 170 Global step 170 Train loss 1.03 on epoch=42
05/21/2022 10:35:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.91 on epoch=44
05/21/2022 10:35:30 - INFO - __main__ - Step 190 Global step 190 Train loss 1.03 on epoch=47
05/21/2022 10:35:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.94 on epoch=49
05/21/2022 10:35:32 - INFO - __main__ - Global step 200 Train loss 0.98 Classification-F1 0.10126582278481013 on epoch=49
05/21/2022 10:35:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.96 on epoch=52
05/21/2022 10:35:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.87 on epoch=54
05/21/2022 10:35:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.94 on epoch=57
05/21/2022 10:35:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.96 on epoch=59
05/21/2022 10:35:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.80 on epoch=62
05/21/2022 10:35:40 - INFO - __main__ - Global step 250 Train loss 0.91 Classification-F1 0.19836065573770492 on epoch=62
05/21/2022 10:35:40 - INFO - __main__ - Saving model with best Classification-F1: 0.14211309523809523 -> 0.19836065573770492 on epoch=62, global_step=250
05/21/2022 10:35:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=64
05/21/2022 10:35:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.95 on epoch=67
05/21/2022 10:35:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.93 on epoch=69
05/21/2022 10:35:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.89 on epoch=72
05/21/2022 10:35:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.91 on epoch=74
05/21/2022 10:35:48 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.10126582278481013 on epoch=74
05/21/2022 10:35:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.92 on epoch=77
05/21/2022 10:35:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.93 on epoch=79
05/21/2022 10:35:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.94 on epoch=82
05/21/2022 10:35:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.96 on epoch=84
05/21/2022 10:35:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.96 on epoch=87
05/21/2022 10:35:56 - INFO - __main__ - Global step 350 Train loss 0.94 Classification-F1 0.14782608695652172 on epoch=87
05/21/2022 10:35:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.91 on epoch=89
05/21/2022 10:35:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.90 on epoch=92
05/21/2022 10:36:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.94 on epoch=94
05/21/2022 10:36:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=97
05/21/2022 10:36:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.91 on epoch=99
05/21/2022 10:36:03 - INFO - __main__ - Global step 400 Train loss 0.91 Classification-F1 0.10126582278481013 on epoch=99
05/21/2022 10:36:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.96 on epoch=102
05/21/2022 10:36:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.88 on epoch=104
05/21/2022 10:36:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.89 on epoch=107
05/21/2022 10:36:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.79 on epoch=109
05/21/2022 10:36:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.81 on epoch=112
05/21/2022 10:36:10 - INFO - __main__ - Global step 450 Train loss 0.87 Classification-F1 0.14095238095238094 on epoch=112
05/21/2022 10:36:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.89 on epoch=114
05/21/2022 10:36:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.96 on epoch=117
05/21/2022 10:36:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.87 on epoch=119
05/21/2022 10:36:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.89 on epoch=122
05/21/2022 10:36:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.90 on epoch=124
05/21/2022 10:36:18 - INFO - __main__ - Global step 500 Train loss 0.90 Classification-F1 0.10126582278481013 on epoch=124
05/21/2022 10:36:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.80 on epoch=127
05/21/2022 10:36:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.79 on epoch=129
05/21/2022 10:36:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.93 on epoch=132
05/21/2022 10:36:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.85 on epoch=134
05/21/2022 10:36:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.89 on epoch=137
05/21/2022 10:36:25 - INFO - __main__ - Global step 550 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=137
05/21/2022 10:36:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.74 on epoch=139
05/21/2022 10:36:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.84 on epoch=142
05/21/2022 10:36:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.87 on epoch=144
05/21/2022 10:36:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.92 on epoch=147
05/21/2022 10:36:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.84 on epoch=149
05/21/2022 10:36:33 - INFO - __main__ - Global step 600 Train loss 0.84 Classification-F1 0.13067758749069247 on epoch=149
05/21/2022 10:36:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.83 on epoch=152
05/21/2022 10:36:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.82 on epoch=154
05/21/2022 10:36:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.93 on epoch=157
05/21/2022 10:36:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.83 on epoch=159
05/21/2022 10:36:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.82 on epoch=162
05/21/2022 10:36:41 - INFO - __main__ - Global step 650 Train loss 0.85 Classification-F1 0.1332923832923833 on epoch=162
05/21/2022 10:36:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.84 on epoch=164
05/21/2022 10:36:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.76 on epoch=167
05/21/2022 10:36:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.82 on epoch=169
05/21/2022 10:36:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.86 on epoch=172
05/21/2022 10:36:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.84 on epoch=174
05/21/2022 10:36:49 - INFO - __main__ - Global step 700 Train loss 0.82 Classification-F1 0.1 on epoch=174
05/21/2022 10:36:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.86 on epoch=177
05/21/2022 10:36:52 - INFO - __main__ - Step 720 Global step 720 Train loss 0.92 on epoch=179
05/21/2022 10:36:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.82 on epoch=182
05/21/2022 10:36:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.86 on epoch=184
05/21/2022 10:36:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.85 on epoch=187
05/21/2022 10:36:56 - INFO - __main__ - Global step 750 Train loss 0.86 Classification-F1 0.1 on epoch=187
05/21/2022 10:36:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.82 on epoch=189
05/21/2022 10:36:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.89 on epoch=192
05/21/2022 10:37:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.79 on epoch=194
05/21/2022 10:37:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.79 on epoch=197
05/21/2022 10:37:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.73 on epoch=199
05/21/2022 10:37:04 - INFO - __main__ - Global step 800 Train loss 0.80 Classification-F1 0.13067758749069247 on epoch=199
05/21/2022 10:37:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.77 on epoch=202
05/21/2022 10:37:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.81 on epoch=204
05/21/2022 10:37:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=207
05/21/2022 10:37:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.81 on epoch=209
05/21/2022 10:37:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.84 on epoch=212
05/21/2022 10:37:12 - INFO - __main__ - Global step 850 Train loss 0.81 Classification-F1 0.14046941678520625 on epoch=212
05/21/2022 10:37:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.77 on epoch=214
05/21/2022 10:37:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.82 on epoch=217
05/21/2022 10:37:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.85 on epoch=219
05/21/2022 10:37:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.80 on epoch=222
05/21/2022 10:37:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.92 on epoch=224
05/21/2022 10:37:20 - INFO - __main__ - Global step 900 Train loss 0.83 Classification-F1 0.13067758749069247 on epoch=224
05/21/2022 10:37:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.85 on epoch=227
05/21/2022 10:37:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.84 on epoch=229
05/21/2022 10:37:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.89 on epoch=232
05/21/2022 10:37:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.89 on epoch=234
05/21/2022 10:37:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.81 on epoch=237
05/21/2022 10:37:29 - INFO - __main__ - Global step 950 Train loss 0.86 Classification-F1 0.13067758749069247 on epoch=237
05/21/2022 10:37:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.81 on epoch=239
05/21/2022 10:37:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.87 on epoch=242
05/21/2022 10:37:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.83 on epoch=244
05/21/2022 10:37:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.80 on epoch=247
05/21/2022 10:37:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.84 on epoch=249
05/21/2022 10:37:37 - INFO - __main__ - Global step 1000 Train loss 0.83 Classification-F1 0.1 on epoch=249
05/21/2022 10:37:38 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.79 on epoch=252
05/21/2022 10:37:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.78 on epoch=254
05/21/2022 10:37:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.80 on epoch=257
05/21/2022 10:37:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.79 on epoch=259
05/21/2022 10:37:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.86 on epoch=262
05/21/2022 10:37:45 - INFO - __main__ - Global step 1050 Train loss 0.80 Classification-F1 0.13067758749069247 on epoch=262
05/21/2022 10:37:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.79 on epoch=264
05/21/2022 10:37:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.87 on epoch=267
05/21/2022 10:37:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.77 on epoch=269
05/21/2022 10:37:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.85 on epoch=272
05/21/2022 10:37:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.80 on epoch=274
05/21/2022 10:37:52 - INFO - __main__ - Global step 1100 Train loss 0.82 Classification-F1 0.12393162393162392 on epoch=274
05/21/2022 10:37:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.84 on epoch=277
05/21/2022 10:37:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.85 on epoch=279
05/21/2022 10:37:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.79 on epoch=282
05/21/2022 10:37:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.81 on epoch=284
05/21/2022 10:37:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.87 on epoch=287
05/21/2022 10:37:59 - INFO - __main__ - Global step 1150 Train loss 0.83 Classification-F1 0.13067758749069247 on epoch=287
05/21/2022 10:38:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.81 on epoch=289
05/21/2022 10:38:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.84 on epoch=292
05/21/2022 10:38:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.78 on epoch=294
05/21/2022 10:38:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.78 on epoch=297
05/21/2022 10:38:06 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.78 on epoch=299
05/21/2022 10:38:07 - INFO - __main__ - Global step 1200 Train loss 0.80 Classification-F1 0.13067758749069247 on epoch=299
05/21/2022 10:38:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.74 on epoch=302
05/21/2022 10:38:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.81 on epoch=304
05/21/2022 10:38:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.77 on epoch=307
05/21/2022 10:38:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.81 on epoch=309
05/21/2022 10:38:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.88 on epoch=312
05/21/2022 10:38:14 - INFO - __main__ - Global step 1250 Train loss 0.80 Classification-F1 0.12393162393162392 on epoch=312
05/21/2022 10:38:16 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.82 on epoch=314
05/21/2022 10:38:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.84 on epoch=317
05/21/2022 10:38:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.86 on epoch=319
05/21/2022 10:38:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.84 on epoch=322
05/21/2022 10:38:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.80 on epoch=324
05/21/2022 10:38:22 - INFO - __main__ - Global step 1300 Train loss 0.83 Classification-F1 0.12393162393162392 on epoch=324
05/21/2022 10:38:23 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.83 on epoch=327
05/21/2022 10:38:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.75 on epoch=329
05/21/2022 10:38:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.81 on epoch=332
05/21/2022 10:38:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.78 on epoch=334
05/21/2022 10:38:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.83 on epoch=337
05/21/2022 10:38:29 - INFO - __main__ - Global step 1350 Train loss 0.80 Classification-F1 0.22588872947553001 on epoch=337
05/21/2022 10:38:29 - INFO - __main__ - Saving model with best Classification-F1: 0.19836065573770492 -> 0.22588872947553001 on epoch=337, global_step=1350
05/21/2022 10:38:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.74 on epoch=339
05/21/2022 10:38:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.78 on epoch=342
05/21/2022 10:38:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.72 on epoch=344
05/21/2022 10:38:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.77 on epoch=347
05/21/2022 10:38:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.79 on epoch=349
05/21/2022 10:38:37 - INFO - __main__ - Global step 1400 Train loss 0.76 Classification-F1 0.2604150413481042 on epoch=349
05/21/2022 10:38:37 - INFO - __main__ - Saving model with best Classification-F1: 0.22588872947553001 -> 0.2604150413481042 on epoch=349, global_step=1400
05/21/2022 10:38:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.83 on epoch=352
05/21/2022 10:38:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.78 on epoch=354
05/21/2022 10:38:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.79 on epoch=357
05/21/2022 10:38:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.77 on epoch=359
05/21/2022 10:38:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.76 on epoch=362
05/21/2022 10:38:44 - INFO - __main__ - Global step 1450 Train loss 0.79 Classification-F1 0.2668669618384989 on epoch=362
05/21/2022 10:38:45 - INFO - __main__ - Saving model with best Classification-F1: 0.2604150413481042 -> 0.2668669618384989 on epoch=362, global_step=1450
05/21/2022 10:38:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.80 on epoch=364
05/21/2022 10:38:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.87 on epoch=367
05/21/2022 10:38:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.75 on epoch=369
05/21/2022 10:38:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.78 on epoch=372
05/21/2022 10:38:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.78 on epoch=374
05/21/2022 10:38:52 - INFO - __main__ - Global step 1500 Train loss 0.80 Classification-F1 0.2879319510898458 on epoch=374
05/21/2022 10:38:53 - INFO - __main__ - Saving model with best Classification-F1: 0.2668669618384989 -> 0.2879319510898458 on epoch=374, global_step=1500
05/21/2022 10:38:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.77 on epoch=377
05/21/2022 10:38:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.76 on epoch=379
05/21/2022 10:38:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.76 on epoch=382
05/21/2022 10:38:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.74 on epoch=384
05/21/2022 10:38:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.80 on epoch=387
05/21/2022 10:39:00 - INFO - __main__ - Global step 1550 Train loss 0.76 Classification-F1 0.3248538011695906 on epoch=387
05/21/2022 10:39:00 - INFO - __main__ - Saving model with best Classification-F1: 0.2879319510898458 -> 0.3248538011695906 on epoch=387, global_step=1550
05/21/2022 10:39:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.80 on epoch=389
05/21/2022 10:39:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.79 on epoch=392
05/21/2022 10:39:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.74 on epoch=394
05/21/2022 10:39:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.75 on epoch=397
05/21/2022 10:39:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.76 on epoch=399
05/21/2022 10:39:07 - INFO - __main__ - Global step 1600 Train loss 0.77 Classification-F1 0.3490767490009148 on epoch=399
05/21/2022 10:39:07 - INFO - __main__ - Saving model with best Classification-F1: 0.3248538011695906 -> 0.3490767490009148 on epoch=399, global_step=1600
05/21/2022 10:39:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.82 on epoch=402
05/21/2022 10:39:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.71 on epoch=404
05/21/2022 10:39:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.74 on epoch=407
05/21/2022 10:39:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.73 on epoch=409
05/21/2022 10:39:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.80 on epoch=412
05/21/2022 10:39:14 - INFO - __main__ - Global step 1650 Train loss 0.76 Classification-F1 0.33942414174972313 on epoch=412
05/21/2022 10:39:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.76 on epoch=414
05/21/2022 10:39:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.68 on epoch=417
05/21/2022 10:39:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.69 on epoch=419
05/21/2022 10:39:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.74 on epoch=422
05/21/2022 10:39:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.77 on epoch=424
05/21/2022 10:39:22 - INFO - __main__ - Global step 1700 Train loss 0.73 Classification-F1 0.46497584541062803 on epoch=424
05/21/2022 10:39:22 - INFO - __main__ - Saving model with best Classification-F1: 0.3490767490009148 -> 0.46497584541062803 on epoch=424, global_step=1700
05/21/2022 10:39:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.77 on epoch=427
05/21/2022 10:39:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.69 on epoch=429
05/21/2022 10:39:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.78 on epoch=432
05/21/2022 10:39:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.74 on epoch=434
05/21/2022 10:39:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.70 on epoch=437
05/21/2022 10:39:29 - INFO - __main__ - Global step 1750 Train loss 0.74 Classification-F1 0.4398809523809524 on epoch=437
05/21/2022 10:39:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.75 on epoch=439
05/21/2022 10:39:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.66 on epoch=442
05/21/2022 10:39:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.66 on epoch=444
05/21/2022 10:39:35 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.65 on epoch=447
05/21/2022 10:39:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.72 on epoch=449
05/21/2022 10:39:37 - INFO - __main__ - Global step 1800 Train loss 0.69 Classification-F1 0.4546267011692543 on epoch=449
05/21/2022 10:39:38 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.70 on epoch=452
05/21/2022 10:39:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.63 on epoch=454
05/21/2022 10:39:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.77 on epoch=457
05/21/2022 10:39:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.73 on epoch=459
05/21/2022 10:39:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.74 on epoch=462
05/21/2022 10:39:44 - INFO - __main__ - Global step 1850 Train loss 0.71 Classification-F1 0.4301932367149759 on epoch=462
05/21/2022 10:39:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.62 on epoch=464
05/21/2022 10:39:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.76 on epoch=467
05/21/2022 10:39:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.70 on epoch=469
05/21/2022 10:39:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.66 on epoch=472
05/21/2022 10:39:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.67 on epoch=474
05/21/2022 10:39:50 - INFO - __main__ - Global step 1900 Train loss 0.68 Classification-F1 0.4301725162709144 on epoch=474
05/21/2022 10:39:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.68 on epoch=477
05/21/2022 10:39:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.70 on epoch=479
05/21/2022 10:39:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.60 on epoch=482
05/21/2022 10:39:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.72 on epoch=484
05/21/2022 10:39:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.70 on epoch=487
05/21/2022 10:39:58 - INFO - __main__ - Global step 1950 Train loss 0.68 Classification-F1 0.4181016644174539 on epoch=487
05/21/2022 10:39:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.72 on epoch=489
05/21/2022 10:40:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.67 on epoch=492
05/21/2022 10:40:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.68 on epoch=494
05/21/2022 10:40:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.68 on epoch=497
05/21/2022 10:40:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.68 on epoch=499
05/21/2022 10:40:05 - INFO - __main__ - Global step 2000 Train loss 0.69 Classification-F1 0.4770188375078366 on epoch=499
05/21/2022 10:40:05 - INFO - __main__ - Saving model with best Classification-F1: 0.46497584541062803 -> 0.4770188375078366 on epoch=499, global_step=2000
05/21/2022 10:40:06 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.66 on epoch=502
05/21/2022 10:40:08 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.66 on epoch=504
05/21/2022 10:40:09 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.72 on epoch=507
05/21/2022 10:40:11 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.66 on epoch=509
05/21/2022 10:40:12 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.67 on epoch=512
05/21/2022 10:40:13 - INFO - __main__ - Global step 2050 Train loss 0.67 Classification-F1 0.5249716949716949 on epoch=512
05/21/2022 10:40:13 - INFO - __main__ - Saving model with best Classification-F1: 0.4770188375078366 -> 0.5249716949716949 on epoch=512, global_step=2050
05/21/2022 10:40:14 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.65 on epoch=514
05/21/2022 10:40:16 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.61 on epoch=517
05/21/2022 10:40:17 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.64 on epoch=519
05/21/2022 10:40:18 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.65 on epoch=522
05/21/2022 10:40:20 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.63 on epoch=524
05/21/2022 10:40:20 - INFO - __main__ - Global step 2100 Train loss 0.64 Classification-F1 0.39282051282051283 on epoch=524
05/21/2022 10:40:22 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.75 on epoch=527
05/21/2022 10:40:23 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.64 on epoch=529
05/21/2022 10:40:24 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.66 on epoch=532
05/21/2022 10:40:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.59 on epoch=534
05/21/2022 10:40:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.53 on epoch=537
05/21/2022 10:40:28 - INFO - __main__ - Global step 2150 Train loss 0.63 Classification-F1 0.39959950338419636 on epoch=537
05/21/2022 10:40:29 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.58 on epoch=539
05/21/2022 10:40:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.67 on epoch=542
05/21/2022 10:40:32 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.56 on epoch=544
05/21/2022 10:40:34 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.64 on epoch=547
05/21/2022 10:40:35 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.56 on epoch=549
05/21/2022 10:40:36 - INFO - __main__ - Global step 2200 Train loss 0.60 Classification-F1 0.5238738738738739 on epoch=549
05/21/2022 10:40:37 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.54 on epoch=552
05/21/2022 10:40:38 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.66 on epoch=554
05/21/2022 10:40:40 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.56 on epoch=557
05/21/2022 10:40:41 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.61 on epoch=559
05/21/2022 10:40:42 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.60 on epoch=562
05/21/2022 10:40:43 - INFO - __main__ - Global step 2250 Train loss 0.60 Classification-F1 0.5003417634996582 on epoch=562
05/21/2022 10:40:44 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.57 on epoch=564
05/21/2022 10:40:45 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.59 on epoch=567
05/21/2022 10:40:46 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.58 on epoch=569
05/21/2022 10:40:47 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.60 on epoch=572
05/21/2022 10:40:49 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.55 on epoch=574
05/21/2022 10:40:49 - INFO - __main__ - Global step 2300 Train loss 0.58 Classification-F1 0.43747644846043005 on epoch=574
05/21/2022 10:40:50 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.63 on epoch=577
05/21/2022 10:40:52 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.55 on epoch=579
05/21/2022 10:40:53 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.68 on epoch=582
05/21/2022 10:40:55 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.52 on epoch=584
05/21/2022 10:40:56 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.68 on epoch=587
05/21/2022 10:40:57 - INFO - __main__ - Global step 2350 Train loss 0.61 Classification-F1 0.5706043956043956 on epoch=587
05/21/2022 10:40:57 - INFO - __main__ - Saving model with best Classification-F1: 0.5249716949716949 -> 0.5706043956043956 on epoch=587, global_step=2350
05/21/2022 10:40:58 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.53 on epoch=589
05/21/2022 10:41:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.74 on epoch=592
05/21/2022 10:41:01 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.62 on epoch=594
05/21/2022 10:41:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.63 on epoch=597
05/21/2022 10:41:03 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.61 on epoch=599
05/21/2022 10:41:04 - INFO - __main__ - Global step 2400 Train loss 0.63 Classification-F1 0.5357051282051282 on epoch=599
05/21/2022 10:41:05 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.66 on epoch=602
05/21/2022 10:41:06 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.59 on epoch=604
05/21/2022 10:41:08 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.64 on epoch=607
05/21/2022 10:41:09 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.56 on epoch=609
05/21/2022 10:41:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.56 on epoch=612
05/21/2022 10:41:11 - INFO - __main__ - Global step 2450 Train loss 0.60 Classification-F1 0.45407407407407413 on epoch=612
05/21/2022 10:41:12 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.61 on epoch=614
05/21/2022 10:41:13 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.54 on epoch=617
05/21/2022 10:41:14 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.55 on epoch=619
05/21/2022 10:41:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.63 on epoch=622
05/21/2022 10:41:17 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.58 on epoch=624
05/21/2022 10:41:18 - INFO - __main__ - Global step 2500 Train loss 0.58 Classification-F1 0.4551766513056835 on epoch=624
05/21/2022 10:41:19 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.62 on epoch=627
05/21/2022 10:41:21 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.58 on epoch=629
05/21/2022 10:41:22 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.62 on epoch=632
05/21/2022 10:41:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.55 on epoch=634
05/21/2022 10:41:25 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.59 on epoch=637
05/21/2022 10:41:25 - INFO - __main__ - Global step 2550 Train loss 0.59 Classification-F1 0.4304080923028291 on epoch=637
05/21/2022 10:41:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.59 on epoch=639
05/21/2022 10:41:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.63 on epoch=642
05/21/2022 10:41:29 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.65 on epoch=644
05/21/2022 10:41:31 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.53 on epoch=647
05/21/2022 10:41:32 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.54 on epoch=649
05/21/2022 10:41:32 - INFO - __main__ - Global step 2600 Train loss 0.59 Classification-F1 0.4365934065934066 on epoch=649
05/21/2022 10:41:34 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.52 on epoch=652
05/21/2022 10:41:35 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.61 on epoch=654
05/21/2022 10:41:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.65 on epoch=657
05/21/2022 10:41:38 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.61 on epoch=659
05/21/2022 10:41:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.57 on epoch=662
05/21/2022 10:41:39 - INFO - __main__ - Global step 2650 Train loss 0.59 Classification-F1 0.4716227916227916 on epoch=662
05/21/2022 10:41:41 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.63 on epoch=664
05/21/2022 10:41:42 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.54 on epoch=667
05/21/2022 10:41:44 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.63 on epoch=669
05/21/2022 10:41:45 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.57 on epoch=672
05/21/2022 10:41:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.54 on epoch=674
05/21/2022 10:41:47 - INFO - __main__ - Global step 2700 Train loss 0.58 Classification-F1 0.4287063385908059 on epoch=674
05/21/2022 10:41:48 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.58 on epoch=677
05/21/2022 10:41:49 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.52 on epoch=679
05/21/2022 10:41:51 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.58 on epoch=682
05/21/2022 10:41:53 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.49 on epoch=684
05/21/2022 10:41:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.56 on epoch=687
05/21/2022 10:41:55 - INFO - __main__ - Global step 2750 Train loss 0.55 Classification-F1 0.5202055442866802 on epoch=687
05/21/2022 10:41:56 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.55 on epoch=689
05/21/2022 10:41:58 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.56 on epoch=692
05/21/2022 10:42:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.51 on epoch=694
05/21/2022 10:42:01 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.51 on epoch=697
05/21/2022 10:42:03 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.58 on epoch=699
05/21/2022 10:42:03 - INFO - __main__ - Global step 2800 Train loss 0.54 Classification-F1 0.43096653214300273 on epoch=699
05/21/2022 10:42:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.57 on epoch=702
05/21/2022 10:42:06 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.60 on epoch=704
05/21/2022 10:42:08 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.53 on epoch=707
05/21/2022 10:42:09 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.54 on epoch=709
05/21/2022 10:42:11 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.57 on epoch=712
05/21/2022 10:42:12 - INFO - __main__ - Global step 2850 Train loss 0.56 Classification-F1 0.44761904761904764 on epoch=712
05/21/2022 10:42:13 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.47 on epoch=714
05/21/2022 10:42:15 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.52 on epoch=717
05/21/2022 10:42:16 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.52 on epoch=719
05/21/2022 10:42:18 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.64 on epoch=722
05/21/2022 10:42:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.49 on epoch=724
05/21/2022 10:42:20 - INFO - __main__ - Global step 2900 Train loss 0.53 Classification-F1 0.4617733990147784 on epoch=724
05/21/2022 10:42:22 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.51 on epoch=727
05/21/2022 10:42:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.64 on epoch=729
05/21/2022 10:42:25 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.55 on epoch=732
05/21/2022 10:42:27 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.52 on epoch=734
05/21/2022 10:42:28 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.53 on epoch=737
05/21/2022 10:42:29 - INFO - __main__ - Global step 2950 Train loss 0.55 Classification-F1 0.48672349226428924 on epoch=737
05/21/2022 10:42:30 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.51 on epoch=739
05/21/2022 10:42:31 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.59 on epoch=742
05/21/2022 10:42:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.43 on epoch=744
05/21/2022 10:42:35 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.56 on epoch=747
05/21/2022 10:42:36 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.43 on epoch=749
05/21/2022 10:42:37 - INFO - __main__ - Global step 3000 Train loss 0.50 Classification-F1 0.4708660540239487 on epoch=749
05/21/2022 10:42:37 - INFO - __main__ - save last model!
05/21/2022 10:42:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 10:42:37 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 10:42:37 - INFO - __main__ - Printing 3 examples
05/21/2022 10:42:37 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 10:42:37 - INFO - __main__ - ['others']
05/21/2022 10:42:37 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 10:42:37 - INFO - __main__ - ['others']
05/21/2022 10:42:37 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 10:42:37 - INFO - __main__ - ['others']
05/21/2022 10:42:37 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:42:37 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:42:37 - INFO - __main__ - Printing 3 examples
05/21/2022 10:42:37 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 10:42:37 - INFO - __main__ - ['sad']
05/21/2022 10:42:37 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 10:42:37 - INFO - __main__ - ['sad']
05/21/2022 10:42:37 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 10:42:37 - INFO - __main__ - ['sad']
05/21/2022 10:42:37 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:42:37 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:42:38 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 10:42:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:42:38 - INFO - __main__ - Printing 3 examples
05/21/2022 10:42:38 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/21/2022 10:42:38 - INFO - __main__ - ['sad']
05/21/2022 10:42:38 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/21/2022 10:42:38 - INFO - __main__ - ['sad']
05/21/2022 10:42:38 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/21/2022 10:42:38 - INFO - __main__ - ['sad']
05/21/2022 10:42:38 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:42:38 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:42:38 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 10:42:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:42:44 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 10:42:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 10:42:44 - INFO - __main__ - Starting training!
05/21/2022 10:42:45 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 10:43:33 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_21_0.4_8_predictions.txt
05/21/2022 10:43:33 - INFO - __main__ - Classification-F1 on test data: 0.0850
05/21/2022 10:43:33 - INFO - __main__ - prefix=emo_16_21, lr=0.4, bsz=8, dev_performance=0.5706043956043956, test_performance=0.08497876232863517
05/21/2022 10:43:33 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.3, bsz=8 ...
05/21/2022 10:43:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:43:34 - INFO - __main__ - Printing 3 examples
05/21/2022 10:43:34 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 10:43:34 - INFO - __main__ - ['sad']
05/21/2022 10:43:34 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 10:43:34 - INFO - __main__ - ['sad']
05/21/2022 10:43:34 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 10:43:34 - INFO - __main__ - ['sad']
05/21/2022 10:43:34 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:43:34 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:43:35 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 10:43:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:43:35 - INFO - __main__ - Printing 3 examples
05/21/2022 10:43:35 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/21/2022 10:43:35 - INFO - __main__ - ['sad']
05/21/2022 10:43:35 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/21/2022 10:43:35 - INFO - __main__ - ['sad']
05/21/2022 10:43:35 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/21/2022 10:43:35 - INFO - __main__ - ['sad']
05/21/2022 10:43:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:43:35 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:43:35 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 10:43:40 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 10:43:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 10:43:41 - INFO - __main__ - Starting training!
05/21/2022 10:43:42 - INFO - __main__ - Step 10 Global step 10 Train loss 4.41 on epoch=2
05/21/2022 10:43:44 - INFO - __main__ - Step 20 Global step 20 Train loss 3.98 on epoch=4
05/21/2022 10:43:45 - INFO - __main__ - Step 30 Global step 30 Train loss 3.73 on epoch=7
05/21/2022 10:43:46 - INFO - __main__ - Step 40 Global step 40 Train loss 3.40 on epoch=9
05/21/2022 10:43:48 - INFO - __main__ - Step 50 Global step 50 Train loss 2.94 on epoch=12
05/21/2022 10:43:49 - INFO - __main__ - Global step 50 Train loss 3.69 Classification-F1 0.1 on epoch=12
05/21/2022 10:43:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/21/2022 10:43:51 - INFO - __main__ - Step 60 Global step 60 Train loss 2.65 on epoch=14
05/21/2022 10:43:52 - INFO - __main__ - Step 70 Global step 70 Train loss 2.38 on epoch=17
05/21/2022 10:43:54 - INFO - __main__ - Step 80 Global step 80 Train loss 2.06 on epoch=19
05/21/2022 10:43:55 - INFO - __main__ - Step 90 Global step 90 Train loss 2.05 on epoch=22
05/21/2022 10:43:57 - INFO - __main__ - Step 100 Global step 100 Train loss 1.61 on epoch=24
05/21/2022 10:43:57 - INFO - __main__ - Global step 100 Train loss 2.15 Classification-F1 0.1302118933697881 on epoch=24
05/21/2022 10:43:57 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1302118933697881 on epoch=24, global_step=100
05/21/2022 10:43:59 - INFO - __main__ - Step 110 Global step 110 Train loss 1.60 on epoch=27
05/21/2022 10:44:00 - INFO - __main__ - Step 120 Global step 120 Train loss 1.33 on epoch=29
05/21/2022 10:44:02 - INFO - __main__ - Step 130 Global step 130 Train loss 1.27 on epoch=32
05/21/2022 10:44:03 - INFO - __main__ - Step 140 Global step 140 Train loss 1.12 on epoch=34
05/21/2022 10:44:04 - INFO - __main__ - Step 150 Global step 150 Train loss 1.20 on epoch=37
05/21/2022 10:44:05 - INFO - __main__ - Global step 150 Train loss 1.31 Classification-F1 0.13653308480894688 on epoch=37
05/21/2022 10:44:05 - INFO - __main__ - Saving model with best Classification-F1: 0.1302118933697881 -> 0.13653308480894688 on epoch=37, global_step=150
05/21/2022 10:44:06 - INFO - __main__ - Step 160 Global step 160 Train loss 1.06 on epoch=39
05/21/2022 10:44:08 - INFO - __main__ - Step 170 Global step 170 Train loss 1.08 on epoch=42
05/21/2022 10:44:09 - INFO - __main__ - Step 180 Global step 180 Train loss 1.04 on epoch=44
05/21/2022 10:44:11 - INFO - __main__ - Step 190 Global step 190 Train loss 1.02 on epoch=47
05/21/2022 10:44:12 - INFO - __main__ - Step 200 Global step 200 Train loss 1.07 on epoch=49
05/21/2022 10:44:13 - INFO - __main__ - Global step 200 Train loss 1.05 Classification-F1 0.15423976608187134 on epoch=49
05/21/2022 10:44:13 - INFO - __main__ - Saving model with best Classification-F1: 0.13653308480894688 -> 0.15423976608187134 on epoch=49, global_step=200
05/21/2022 10:44:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.96 on epoch=52
05/21/2022 10:44:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.97 on epoch=54
05/21/2022 10:44:17 - INFO - __main__ - Step 230 Global step 230 Train loss 1.06 on epoch=57
05/21/2022 10:44:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.99 on epoch=59
05/21/2022 10:44:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.86 on epoch=62
05/21/2022 10:44:21 - INFO - __main__ - Global step 250 Train loss 0.97 Classification-F1 0.09210526315789473 on epoch=62
05/21/2022 10:44:22 - INFO - __main__ - Step 260 Global step 260 Train loss 1.01 on epoch=64
05/21/2022 10:44:23 - INFO - __main__ - Step 270 Global step 270 Train loss 1.00 on epoch=67
05/21/2022 10:44:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.94 on epoch=69
05/21/2022 10:44:26 - INFO - __main__ - Step 290 Global step 290 Train loss 1.04 on epoch=72
05/21/2022 10:44:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.93 on epoch=74
05/21/2022 10:44:28 - INFO - __main__ - Global step 300 Train loss 0.98 Classification-F1 0.10126582278481013 on epoch=74
05/21/2022 10:44:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.91 on epoch=77
05/21/2022 10:44:31 - INFO - __main__ - Step 320 Global step 320 Train loss 1.03 on epoch=79
05/21/2022 10:44:32 - INFO - __main__ - Step 330 Global step 330 Train loss 1.01 on epoch=82
05/21/2022 10:44:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.98 on epoch=84
05/21/2022 10:44:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.94 on epoch=87
05/21/2022 10:44:36 - INFO - __main__ - Global step 350 Train loss 0.97 Classification-F1 0.10126582278481013 on epoch=87
05/21/2022 10:44:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.95 on epoch=89
05/21/2022 10:44:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.98 on epoch=92
05/21/2022 10:44:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.91 on epoch=94
05/21/2022 10:44:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.91 on epoch=97
05/21/2022 10:44:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.89 on epoch=99
05/21/2022 10:44:43 - INFO - __main__ - Global step 400 Train loss 0.93 Classification-F1 0.1 on epoch=99
05/21/2022 10:44:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.95 on epoch=102
05/21/2022 10:44:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.99 on epoch=104
05/21/2022 10:44:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.89 on epoch=107
05/21/2022 10:44:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.89 on epoch=109
05/21/2022 10:44:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.87 on epoch=112
05/21/2022 10:44:50 - INFO - __main__ - Global step 450 Train loss 0.92 Classification-F1 0.1313186813186813 on epoch=112
05/21/2022 10:44:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.96 on epoch=114
05/21/2022 10:44:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.94 on epoch=117
05/21/2022 10:44:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.88 on epoch=119
05/21/2022 10:44:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.95 on epoch=122
05/21/2022 10:44:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.96 on epoch=124
05/21/2022 10:44:58 - INFO - __main__ - Global step 500 Train loss 0.94 Classification-F1 0.10126582278481013 on epoch=124
05/21/2022 10:44:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.97 on epoch=127
05/21/2022 10:45:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.83 on epoch=129
05/21/2022 10:45:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.84 on epoch=132
05/21/2022 10:45:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.84 on epoch=134
05/21/2022 10:45:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.83 on epoch=137
05/21/2022 10:45:06 - INFO - __main__ - Global step 550 Train loss 0.86 Classification-F1 0.1 on epoch=137
05/21/2022 10:45:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.85 on epoch=139
05/21/2022 10:45:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.90 on epoch=142
05/21/2022 10:45:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.89 on epoch=144
05/21/2022 10:45:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.98 on epoch=147
05/21/2022 10:45:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.92 on epoch=149
05/21/2022 10:45:13 - INFO - __main__ - Global step 600 Train loss 0.91 Classification-F1 0.10126582278481013 on epoch=149
05/21/2022 10:45:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.88 on epoch=152
05/21/2022 10:45:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.86 on epoch=154
05/21/2022 10:45:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.91 on epoch=157
05/21/2022 10:45:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.86 on epoch=159
05/21/2022 10:45:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.81 on epoch=162
05/21/2022 10:45:20 - INFO - __main__ - Global step 650 Train loss 0.87 Classification-F1 0.10126582278481013 on epoch=162
05/21/2022 10:45:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.86 on epoch=164
05/21/2022 10:45:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.75 on epoch=167
05/21/2022 10:45:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.89 on epoch=169
05/21/2022 10:45:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.90 on epoch=172
05/21/2022 10:45:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.96 on epoch=174
05/21/2022 10:45:27 - INFO - __main__ - Global step 700 Train loss 0.87 Classification-F1 0.10126582278481013 on epoch=174
05/21/2022 10:45:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.83 on epoch=177
05/21/2022 10:45:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.89 on epoch=179
05/21/2022 10:45:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.82 on epoch=182
05/21/2022 10:45:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.89 on epoch=184
05/21/2022 10:45:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.90 on epoch=187
05/21/2022 10:45:35 - INFO - __main__ - Global step 750 Train loss 0.87 Classification-F1 0.10126582278481013 on epoch=187
05/21/2022 10:45:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.85 on epoch=189
05/21/2022 10:45:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.83 on epoch=192
05/21/2022 10:45:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.80 on epoch=194
05/21/2022 10:45:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.78 on epoch=197
05/21/2022 10:45:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.84 on epoch=199
05/21/2022 10:45:43 - INFO - __main__ - Global step 800 Train loss 0.82 Classification-F1 0.1 on epoch=199
05/21/2022 10:45:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.90 on epoch=202
05/21/2022 10:45:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.82 on epoch=204
05/21/2022 10:45:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.86 on epoch=207
05/21/2022 10:45:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.86 on epoch=209
05/21/2022 10:45:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.79 on epoch=212
05/21/2022 10:45:51 - INFO - __main__ - Global step 850 Train loss 0.85 Classification-F1 0.13381369016984046 on epoch=212
05/21/2022 10:45:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.88 on epoch=214
05/21/2022 10:45:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.83 on epoch=217
05/21/2022 10:45:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.90 on epoch=219
05/21/2022 10:45:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.81 on epoch=222
05/21/2022 10:45:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.91 on epoch=224
05/21/2022 10:46:00 - INFO - __main__ - Global step 900 Train loss 0.87 Classification-F1 0.10126582278481013 on epoch=224
05/21/2022 10:46:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.84 on epoch=227
05/21/2022 10:46:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.85 on epoch=229
05/21/2022 10:46:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.83 on epoch=232
05/21/2022 10:46:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.92 on epoch=234
05/21/2022 10:46:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.86 on epoch=237
05/21/2022 10:46:08 - INFO - __main__ - Global step 950 Train loss 0.86 Classification-F1 0.09615384615384615 on epoch=237
05/21/2022 10:46:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.88 on epoch=239
05/21/2022 10:46:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.88 on epoch=242
05/21/2022 10:46:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.83 on epoch=244
05/21/2022 10:46:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.84 on epoch=247
05/21/2022 10:46:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.92 on epoch=249
05/21/2022 10:46:16 - INFO - __main__ - Global step 1000 Train loss 0.87 Classification-F1 0.1 on epoch=249
05/21/2022 10:46:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.85 on epoch=252
05/21/2022 10:46:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.91 on epoch=254
05/21/2022 10:46:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.85 on epoch=257
05/21/2022 10:46:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.82 on epoch=259
05/21/2022 10:46:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.85 on epoch=262
05/21/2022 10:46:24 - INFO - __main__ - Global step 1050 Train loss 0.85 Classification-F1 0.16223908918406071 on epoch=262
05/21/2022 10:46:24 - INFO - __main__ - Saving model with best Classification-F1: 0.15423976608187134 -> 0.16223908918406071 on epoch=262, global_step=1050
05/21/2022 10:46:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.82 on epoch=264
05/21/2022 10:46:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.91 on epoch=267
05/21/2022 10:46:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.82 on epoch=269
05/21/2022 10:46:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.81 on epoch=272
05/21/2022 10:46:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.76 on epoch=274
05/21/2022 10:46:32 - INFO - __main__ - Global step 1100 Train loss 0.82 Classification-F1 0.1 on epoch=274
05/21/2022 10:46:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.88 on epoch=277
05/21/2022 10:46:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.84 on epoch=279
05/21/2022 10:46:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.83 on epoch=282
05/21/2022 10:46:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.81 on epoch=284
05/21/2022 10:46:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.85 on epoch=287
05/21/2022 10:46:39 - INFO - __main__ - Global step 1150 Train loss 0.84 Classification-F1 0.10256410256410256 on epoch=287
05/21/2022 10:46:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.81 on epoch=289
05/21/2022 10:46:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.85 on epoch=292
05/21/2022 10:46:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.78 on epoch=294
05/21/2022 10:46:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.74 on epoch=297
05/21/2022 10:46:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.81 on epoch=299
05/21/2022 10:46:47 - INFO - __main__ - Global step 1200 Train loss 0.80 Classification-F1 0.10126582278481013 on epoch=299
05/21/2022 10:46:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.80 on epoch=302
05/21/2022 10:46:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.80 on epoch=304
05/21/2022 10:46:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.82 on epoch=307
05/21/2022 10:46:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.76 on epoch=309
05/21/2022 10:46:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.85 on epoch=312
05/21/2022 10:46:55 - INFO - __main__ - Global step 1250 Train loss 0.81 Classification-F1 0.16378066378066378 on epoch=312
05/21/2022 10:46:55 - INFO - __main__ - Saving model with best Classification-F1: 0.16223908918406071 -> 0.16378066378066378 on epoch=312, global_step=1250
05/21/2022 10:46:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.84 on epoch=314
05/21/2022 10:46:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.85 on epoch=317
05/21/2022 10:46:59 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.77 on epoch=319
05/21/2022 10:47:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.89 on epoch=322
05/21/2022 10:47:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.78 on epoch=324
05/21/2022 10:47:02 - INFO - __main__ - Global step 1300 Train loss 0.82 Classification-F1 0.1 on epoch=324
05/21/2022 10:47:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.78 on epoch=327
05/21/2022 10:47:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.80 on epoch=329
05/21/2022 10:47:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.88 on epoch=332
05/21/2022 10:47:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.81 on epoch=334
05/21/2022 10:47:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.74 on epoch=337
05/21/2022 10:47:10 - INFO - __main__ - Global step 1350 Train loss 0.80 Classification-F1 0.13251935675997617 on epoch=337
05/21/2022 10:47:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.80 on epoch=339
05/21/2022 10:47:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.89 on epoch=342
05/21/2022 10:47:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.76 on epoch=344
05/21/2022 10:47:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.81 on epoch=347
05/21/2022 10:47:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.83 on epoch=349
05/21/2022 10:47:18 - INFO - __main__ - Global step 1400 Train loss 0.82 Classification-F1 0.13067758749069247 on epoch=349
05/21/2022 10:47:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.79 on epoch=352
05/21/2022 10:47:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.72 on epoch=354
05/21/2022 10:47:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.82 on epoch=357
05/21/2022 10:47:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.82 on epoch=359
05/21/2022 10:47:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.92 on epoch=362
05/21/2022 10:47:26 - INFO - __main__ - Global step 1450 Train loss 0.81 Classification-F1 0.1967741935483871 on epoch=362
05/21/2022 10:47:26 - INFO - __main__ - Saving model with best Classification-F1: 0.16378066378066378 -> 0.1967741935483871 on epoch=362, global_step=1450
05/21/2022 10:47:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.82 on epoch=364
05/21/2022 10:47:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.77 on epoch=367
05/21/2022 10:47:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.78 on epoch=369
05/21/2022 10:47:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.81 on epoch=372
05/21/2022 10:47:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.83 on epoch=374
05/21/2022 10:47:35 - INFO - __main__ - Global step 1500 Train loss 0.80 Classification-F1 0.15459213988625753 on epoch=374
05/21/2022 10:47:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.80 on epoch=377
05/21/2022 10:47:38 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.81 on epoch=379
05/21/2022 10:47:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.71 on epoch=382
05/21/2022 10:47:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.83 on epoch=384
05/21/2022 10:47:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.81 on epoch=387
05/21/2022 10:47:42 - INFO - __main__ - Global step 1550 Train loss 0.79 Classification-F1 0.12539184952978055 on epoch=387
05/21/2022 10:47:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.76 on epoch=389
05/21/2022 10:47:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.83 on epoch=392
05/21/2022 10:47:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.85 on epoch=394
05/21/2022 10:47:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.82 on epoch=397
05/21/2022 10:47:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.78 on epoch=399
05/21/2022 10:47:50 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.17045454545454547 on epoch=399
05/21/2022 10:47:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.80 on epoch=402
05/21/2022 10:47:53 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.74 on epoch=404
05/21/2022 10:47:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.80 on epoch=407
05/21/2022 10:47:56 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.83 on epoch=409
05/21/2022 10:47:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.81 on epoch=412
05/21/2022 10:47:58 - INFO - __main__ - Global step 1650 Train loss 0.80 Classification-F1 0.21388888888888885 on epoch=412
05/21/2022 10:47:58 - INFO - __main__ - Saving model with best Classification-F1: 0.1967741935483871 -> 0.21388888888888885 on epoch=412, global_step=1650
05/21/2022 10:47:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.82 on epoch=414
05/21/2022 10:48:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.80 on epoch=417
05/21/2022 10:48:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.77 on epoch=419
05/21/2022 10:48:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.79 on epoch=422
05/21/2022 10:48:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.77 on epoch=424
05/21/2022 10:48:06 - INFO - __main__ - Global step 1700 Train loss 0.79 Classification-F1 0.2127806346419165 on epoch=424
05/21/2022 10:48:07 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.83 on epoch=427
05/21/2022 10:48:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.83 on epoch=429
05/21/2022 10:48:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.79 on epoch=432
05/21/2022 10:48:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.78 on epoch=434
05/21/2022 10:48:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.72 on epoch=437
05/21/2022 10:48:13 - INFO - __main__ - Global step 1750 Train loss 0.79 Classification-F1 0.2492401215805471 on epoch=437
05/21/2022 10:48:13 - INFO - __main__ - Saving model with best Classification-F1: 0.21388888888888885 -> 0.2492401215805471 on epoch=437, global_step=1750
05/21/2022 10:48:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.77 on epoch=439
05/21/2022 10:48:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.79 on epoch=442
05/21/2022 10:48:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.73 on epoch=444
05/21/2022 10:48:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.78 on epoch=447
05/21/2022 10:48:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.72 on epoch=449
05/21/2022 10:48:21 - INFO - __main__ - Global step 1800 Train loss 0.76 Classification-F1 0.2712433339649712 on epoch=449
05/21/2022 10:48:21 - INFO - __main__ - Saving model with best Classification-F1: 0.2492401215805471 -> 0.2712433339649712 on epoch=449, global_step=1800
05/21/2022 10:48:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.69 on epoch=452
05/21/2022 10:48:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.77 on epoch=454
05/21/2022 10:48:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.78 on epoch=457
05/21/2022 10:48:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.74 on epoch=459
05/21/2022 10:48:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.72 on epoch=462
05/21/2022 10:48:30 - INFO - __main__ - Global step 1850 Train loss 0.74 Classification-F1 0.32355810616929703 on epoch=462
05/21/2022 10:48:30 - INFO - __main__ - Saving model with best Classification-F1: 0.2712433339649712 -> 0.32355810616929703 on epoch=462, global_step=1850
05/21/2022 10:48:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.75 on epoch=464
05/21/2022 10:48:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.81 on epoch=467
05/21/2022 10:48:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.70 on epoch=469
05/21/2022 10:48:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.75 on epoch=472
05/21/2022 10:48:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.78 on epoch=474
05/21/2022 10:48:37 - INFO - __main__ - Global step 1900 Train loss 0.76 Classification-F1 0.3785259838201015 on epoch=474
05/21/2022 10:48:37 - INFO - __main__ - Saving model with best Classification-F1: 0.32355810616929703 -> 0.3785259838201015 on epoch=474, global_step=1900
05/21/2022 10:48:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.78 on epoch=477
05/21/2022 10:48:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.68 on epoch=479
05/21/2022 10:48:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.85 on epoch=482
05/21/2022 10:48:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.69 on epoch=484
05/21/2022 10:48:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.74 on epoch=487
05/21/2022 10:48:46 - INFO - __main__ - Global step 1950 Train loss 0.75 Classification-F1 0.34444444444444444 on epoch=487
05/21/2022 10:48:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.67 on epoch=489
05/21/2022 10:48:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.81 on epoch=492
05/21/2022 10:48:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.72 on epoch=494
05/21/2022 10:48:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.71 on epoch=497
05/21/2022 10:48:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.70 on epoch=499
05/21/2022 10:48:53 - INFO - __main__ - Global step 2000 Train loss 0.72 Classification-F1 0.3620492472867312 on epoch=499
05/21/2022 10:48:55 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.72 on epoch=502
05/21/2022 10:48:56 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.65 on epoch=504
05/21/2022 10:48:57 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.75 on epoch=507
05/21/2022 10:48:59 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.71 on epoch=509
05/21/2022 10:49:00 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.68 on epoch=512
05/21/2022 10:49:01 - INFO - __main__ - Global step 2050 Train loss 0.70 Classification-F1 0.4572378840671524 on epoch=512
05/21/2022 10:49:01 - INFO - __main__ - Saving model with best Classification-F1: 0.3785259838201015 -> 0.4572378840671524 on epoch=512, global_step=2050
05/21/2022 10:49:02 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.73 on epoch=514
05/21/2022 10:49:04 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.75 on epoch=517
05/21/2022 10:49:05 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.71 on epoch=519
05/21/2022 10:49:06 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.70 on epoch=522
05/21/2022 10:49:08 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.71 on epoch=524
05/21/2022 10:49:09 - INFO - __main__ - Global step 2100 Train loss 0.72 Classification-F1 0.36066542153498676 on epoch=524
05/21/2022 10:49:10 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.69 on epoch=527
05/21/2022 10:49:12 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.62 on epoch=529
05/21/2022 10:49:13 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.75 on epoch=532
05/21/2022 10:49:14 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.66 on epoch=534
05/21/2022 10:49:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.69 on epoch=537
05/21/2022 10:49:16 - INFO - __main__ - Global step 2150 Train loss 0.68 Classification-F1 0.364985014985015 on epoch=537
05/21/2022 10:49:18 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.66 on epoch=539
05/21/2022 10:49:19 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.74 on epoch=542
05/21/2022 10:49:20 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.70 on epoch=544
05/21/2022 10:49:22 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.72 on epoch=547
05/21/2022 10:49:23 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.70 on epoch=549
05/21/2022 10:49:24 - INFO - __main__ - Global step 2200 Train loss 0.71 Classification-F1 0.4391406323294868 on epoch=549
05/21/2022 10:49:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.64 on epoch=552
05/21/2022 10:49:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.68 on epoch=554
05/21/2022 10:49:28 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.68 on epoch=557
05/21/2022 10:49:30 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.71 on epoch=559
05/21/2022 10:49:31 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.70 on epoch=562
05/21/2022 10:49:32 - INFO - __main__ - Global step 2250 Train loss 0.68 Classification-F1 0.42298818298818297 on epoch=562
05/21/2022 10:49:33 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.70 on epoch=564
05/21/2022 10:49:35 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.71 on epoch=567
05/21/2022 10:49:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.67 on epoch=569
05/21/2022 10:49:38 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.69 on epoch=572
05/21/2022 10:49:39 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.66 on epoch=574
05/21/2022 10:49:40 - INFO - __main__ - Global step 2300 Train loss 0.69 Classification-F1 0.3670556339514883 on epoch=574
05/21/2022 10:49:41 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.69 on epoch=577
05/21/2022 10:49:43 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.64 on epoch=579
05/21/2022 10:49:44 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.61 on epoch=582
05/21/2022 10:49:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.60 on epoch=584
05/21/2022 10:49:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.61 on epoch=587
05/21/2022 10:49:48 - INFO - __main__ - Global step 2350 Train loss 0.63 Classification-F1 0.4013717550302916 on epoch=587
05/21/2022 10:49:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.82 on epoch=589
05/21/2022 10:49:51 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.69 on epoch=592
05/21/2022 10:49:52 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.62 on epoch=594
05/21/2022 10:49:54 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.64 on epoch=597
05/21/2022 10:49:55 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.67 on epoch=599
05/21/2022 10:49:56 - INFO - __main__ - Global step 2400 Train loss 0.69 Classification-F1 0.42548717948717946 on epoch=599
05/21/2022 10:49:57 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.66 on epoch=602
05/21/2022 10:49:58 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.62 on epoch=604
05/21/2022 10:50:00 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.66 on epoch=607
05/21/2022 10:50:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.57 on epoch=609
05/21/2022 10:50:03 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.61 on epoch=612
05/21/2022 10:50:03 - INFO - __main__ - Global step 2450 Train loss 0.63 Classification-F1 0.5706420595533498 on epoch=612
05/21/2022 10:50:03 - INFO - __main__ - Saving model with best Classification-F1: 0.4572378840671524 -> 0.5706420595533498 on epoch=612, global_step=2450
05/21/2022 10:50:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.63 on epoch=614
05/21/2022 10:50:06 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.64 on epoch=617
05/21/2022 10:50:08 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.64 on epoch=619
05/21/2022 10:50:09 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.70 on epoch=622
05/21/2022 10:50:11 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.62 on epoch=624
05/21/2022 10:50:11 - INFO - __main__ - Global step 2500 Train loss 0.65 Classification-F1 0.4725392886683209 on epoch=624
05/21/2022 10:50:12 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.65 on epoch=627
05/21/2022 10:50:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.60 on epoch=629
05/21/2022 10:50:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.66 on epoch=632
05/21/2022 10:50:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.65 on epoch=634
05/21/2022 10:50:18 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.68 on epoch=637
05/21/2022 10:50:19 - INFO - __main__ - Global step 2550 Train loss 0.65 Classification-F1 0.4456339362618433 on epoch=637
05/21/2022 10:50:21 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.61 on epoch=639
05/21/2022 10:50:22 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.62 on epoch=642
05/21/2022 10:50:24 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.65 on epoch=644
05/21/2022 10:50:25 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.68 on epoch=647
05/21/2022 10:50:27 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.64 on epoch=649
05/21/2022 10:50:27 - INFO - __main__ - Global step 2600 Train loss 0.64 Classification-F1 0.4384325046778937 on epoch=649
05/21/2022 10:50:28 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.75 on epoch=652
05/21/2022 10:50:30 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.65 on epoch=654
05/21/2022 10:50:32 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.67 on epoch=657
05/21/2022 10:50:33 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.60 on epoch=659
05/21/2022 10:50:34 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.67 on epoch=662
05/21/2022 10:50:35 - INFO - __main__ - Global step 2650 Train loss 0.67 Classification-F1 0.4757252796886943 on epoch=662
05/21/2022 10:50:36 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.66 on epoch=664
05/21/2022 10:50:37 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.62 on epoch=667
05/21/2022 10:50:39 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.59 on epoch=669
05/21/2022 10:50:40 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.64 on epoch=672
05/21/2022 10:50:41 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.57 on epoch=674
05/21/2022 10:50:42 - INFO - __main__ - Global step 2700 Train loss 0.62 Classification-F1 0.4134423098528256 on epoch=674
05/21/2022 10:50:43 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.66 on epoch=677
05/21/2022 10:50:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.65 on epoch=679
05/21/2022 10:50:46 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.58 on epoch=682
05/21/2022 10:50:47 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.64 on epoch=684
05/21/2022 10:50:49 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.66 on epoch=687
05/21/2022 10:50:49 - INFO - __main__ - Global step 2750 Train loss 0.64 Classification-F1 0.42505368773103325 on epoch=687
05/21/2022 10:50:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.61 on epoch=689
05/21/2022 10:50:52 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.68 on epoch=692
05/21/2022 10:50:53 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.64 on epoch=694
05/21/2022 10:50:55 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.68 on epoch=697
05/21/2022 10:50:56 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.70 on epoch=699
05/21/2022 10:50:57 - INFO - __main__ - Global step 2800 Train loss 0.66 Classification-F1 0.47873243873243865 on epoch=699
05/21/2022 10:50:58 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.64 on epoch=702
05/21/2022 10:50:59 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.60 on epoch=704
05/21/2022 10:51:01 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.68 on epoch=707
05/21/2022 10:51:02 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.62 on epoch=709
05/21/2022 10:51:04 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.63 on epoch=712
05/21/2022 10:51:04 - INFO - __main__ - Global step 2850 Train loss 0.63 Classification-F1 0.6201587301587301 on epoch=712
05/21/2022 10:51:04 - INFO - __main__ - Saving model with best Classification-F1: 0.5706420595533498 -> 0.6201587301587301 on epoch=712, global_step=2850
05/21/2022 10:51:06 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.59 on epoch=714
05/21/2022 10:51:07 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.62 on epoch=717
05/21/2022 10:51:09 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.59 on epoch=719
05/21/2022 10:51:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.61 on epoch=722
05/21/2022 10:51:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.55 on epoch=724
05/21/2022 10:51:12 - INFO - __main__ - Global step 2900 Train loss 0.59 Classification-F1 0.47178498985801215 on epoch=724
05/21/2022 10:51:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.70 on epoch=727
05/21/2022 10:51:15 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.56 on epoch=729
05/21/2022 10:51:16 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.57 on epoch=732
05/21/2022 10:51:18 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.63 on epoch=734
05/21/2022 10:51:19 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.64 on epoch=737
05/21/2022 10:51:20 - INFO - __main__ - Global step 2950 Train loss 0.62 Classification-F1 0.422063492063492 on epoch=737
05/21/2022 10:51:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.57 on epoch=739
05/21/2022 10:51:22 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.60 on epoch=742
05/21/2022 10:51:24 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.68 on epoch=744
05/21/2022 10:51:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.59 on epoch=747
05/21/2022 10:51:27 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.62 on epoch=749
05/21/2022 10:51:27 - INFO - __main__ - Global step 3000 Train loss 0.61 Classification-F1 0.4920213246588959 on epoch=749
05/21/2022 10:51:27 - INFO - __main__ - save last model!
05/21/2022 10:51:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 10:51:27 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 10:51:27 - INFO - __main__ - Printing 3 examples
05/21/2022 10:51:27 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 10:51:27 - INFO - __main__ - ['others']
05/21/2022 10:51:27 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 10:51:27 - INFO - __main__ - ['others']
05/21/2022 10:51:27 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 10:51:27 - INFO - __main__ - ['others']
05/21/2022 10:51:27 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:51:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:51:28 - INFO - __main__ - Printing 3 examples
05/21/2022 10:51:28 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 10:51:28 - INFO - __main__ - ['sad']
05/21/2022 10:51:28 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 10:51:28 - INFO - __main__ - ['sad']
05/21/2022 10:51:28 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 10:51:28 - INFO - __main__ - ['sad']
05/21/2022 10:51:28 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:51:28 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:51:28 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 10:51:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:51:28 - INFO - __main__ - Printing 3 examples
05/21/2022 10:51:28 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/21/2022 10:51:28 - INFO - __main__ - ['sad']
05/21/2022 10:51:28 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/21/2022 10:51:28 - INFO - __main__ - ['sad']
05/21/2022 10:51:28 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/21/2022 10:51:28 - INFO - __main__ - ['sad']
05/21/2022 10:51:28 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:51:28 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:51:28 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 10:51:30 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:51:34 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 10:51:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 10:51:34 - INFO - __main__ - Starting training!
05/21/2022 10:51:35 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 10:52:21 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_21_0.3_8_predictions.txt
05/21/2022 10:52:21 - INFO - __main__ - Classification-F1 on test data: 0.0975
05/21/2022 10:52:21 - INFO - __main__ - prefix=emo_16_21, lr=0.3, bsz=8, dev_performance=0.6201587301587301, test_performance=0.09752211418613203
05/21/2022 10:52:21 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.2, bsz=8 ...
05/21/2022 10:52:22 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:52:22 - INFO - __main__ - Printing 3 examples
05/21/2022 10:52:22 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 10:52:22 - INFO - __main__ - ['sad']
05/21/2022 10:52:22 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 10:52:22 - INFO - __main__ - ['sad']
05/21/2022 10:52:22 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 10:52:22 - INFO - __main__ - ['sad']
05/21/2022 10:52:22 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:52:22 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:52:22 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 10:52:22 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 10:52:22 - INFO - __main__ - Printing 3 examples
05/21/2022 10:52:22 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/21/2022 10:52:22 - INFO - __main__ - ['sad']
05/21/2022 10:52:22 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/21/2022 10:52:22 - INFO - __main__ - ['sad']
05/21/2022 10:52:22 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/21/2022 10:52:22 - INFO - __main__ - ['sad']
05/21/2022 10:52:22 - INFO - __main__ - Tokenizing Input ...
05/21/2022 10:52:22 - INFO - __main__ - Tokenizing Output ...
05/21/2022 10:52:22 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 10:52:29 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 10:52:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 10:52:29 - INFO - __main__ - Starting training!
05/21/2022 10:52:31 - INFO - __main__ - Step 10 Global step 10 Train loss 4.51 on epoch=2
05/21/2022 10:52:33 - INFO - __main__ - Step 20 Global step 20 Train loss 4.32 on epoch=4
05/21/2022 10:52:34 - INFO - __main__ - Step 30 Global step 30 Train loss 4.00 on epoch=7
05/21/2022 10:52:36 - INFO - __main__ - Step 40 Global step 40 Train loss 3.78 on epoch=9
05/21/2022 10:52:37 - INFO - __main__ - Step 50 Global step 50 Train loss 3.58 on epoch=12
05/21/2022 10:52:38 - INFO - __main__ - Global step 50 Train loss 4.04 Classification-F1 0.0 on epoch=12
05/21/2022 10:52:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
05/21/2022 10:52:39 - INFO - __main__ - Step 60 Global step 60 Train loss 3.35 on epoch=14
05/21/2022 10:52:41 - INFO - __main__ - Step 70 Global step 70 Train loss 3.00 on epoch=17
05/21/2022 10:52:42 - INFO - __main__ - Step 80 Global step 80 Train loss 2.79 on epoch=19
05/21/2022 10:52:44 - INFO - __main__ - Step 90 Global step 90 Train loss 2.71 on epoch=22
05/21/2022 10:52:46 - INFO - __main__ - Step 100 Global step 100 Train loss 2.36 on epoch=24
05/21/2022 10:52:46 - INFO - __main__ - Global step 100 Train loss 2.84 Classification-F1 0.1 on epoch=24
05/21/2022 10:52:46 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.1 on epoch=24, global_step=100
05/21/2022 10:52:48 - INFO - __main__ - Step 110 Global step 110 Train loss 2.40 on epoch=27
05/21/2022 10:52:49 - INFO - __main__ - Step 120 Global step 120 Train loss 2.13 on epoch=29
05/21/2022 10:52:50 - INFO - __main__ - Step 130 Global step 130 Train loss 2.12 on epoch=32
05/21/2022 10:52:52 - INFO - __main__ - Step 140 Global step 140 Train loss 1.80 on epoch=34
05/21/2022 10:52:53 - INFO - __main__ - Step 150 Global step 150 Train loss 1.66 on epoch=37
05/21/2022 10:52:54 - INFO - __main__ - Global step 150 Train loss 2.02 Classification-F1 0.1 on epoch=37
05/21/2022 10:52:55 - INFO - __main__ - Step 160 Global step 160 Train loss 1.52 on epoch=39
05/21/2022 10:52:56 - INFO - __main__ - Step 170 Global step 170 Train loss 1.50 on epoch=42
05/21/2022 10:52:58 - INFO - __main__ - Step 180 Global step 180 Train loss 1.47 on epoch=44
05/21/2022 10:53:00 - INFO - __main__ - Step 190 Global step 190 Train loss 1.39 on epoch=47
05/21/2022 10:53:01 - INFO - __main__ - Step 200 Global step 200 Train loss 1.25 on epoch=49
05/21/2022 10:53:02 - INFO - __main__ - Global step 200 Train loss 1.42 Classification-F1 0.09615384615384615 on epoch=49
05/21/2022 10:53:03 - INFO - __main__ - Step 210 Global step 210 Train loss 1.22 on epoch=52
05/21/2022 10:53:05 - INFO - __main__ - Step 220 Global step 220 Train loss 1.13 on epoch=54
05/21/2022 10:53:07 - INFO - __main__ - Step 230 Global step 230 Train loss 1.12 on epoch=57
05/21/2022 10:53:08 - INFO - __main__ - Step 240 Global step 240 Train loss 1.09 on epoch=59
05/21/2022 10:53:09 - INFO - __main__ - Step 250 Global step 250 Train loss 1.10 on epoch=62
05/21/2022 10:53:10 - INFO - __main__ - Global step 250 Train loss 1.13 Classification-F1 0.13067758749069247 on epoch=62
05/21/2022 10:53:10 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.13067758749069247 on epoch=62, global_step=250
05/21/2022 10:53:11 - INFO - __main__ - Step 260 Global step 260 Train loss 1.06 on epoch=64
05/21/2022 10:53:12 - INFO - __main__ - Step 270 Global step 270 Train loss 1.15 on epoch=67
05/21/2022 10:53:14 - INFO - __main__ - Step 280 Global step 280 Train loss 1.15 on epoch=69
05/21/2022 10:53:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.96 on epoch=72
05/21/2022 10:53:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.85 on epoch=74
05/21/2022 10:53:18 - INFO - __main__ - Global step 300 Train loss 1.04 Classification-F1 0.14221364221364222 on epoch=74
05/21/2022 10:53:18 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.14221364221364222 on epoch=74, global_step=300
05/21/2022 10:53:19 - INFO - __main__ - Step 310 Global step 310 Train loss 1.12 on epoch=77
05/21/2022 10:53:20 - INFO - __main__ - Step 320 Global step 320 Train loss 1.03 on epoch=79
05/21/2022 10:53:22 - INFO - __main__ - Step 330 Global step 330 Train loss 1.03 on epoch=82
05/21/2022 10:53:23 - INFO - __main__ - Step 340 Global step 340 Train loss 1.01 on epoch=84
05/21/2022 10:53:24 - INFO - __main__ - Step 350 Global step 350 Train loss 1.03 on epoch=87
05/21/2022 10:53:25 - INFO - __main__ - Global step 350 Train loss 1.04 Classification-F1 0.0974025974025974 on epoch=87
05/21/2022 10:53:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.95 on epoch=89
05/21/2022 10:53:28 - INFO - __main__ - Step 370 Global step 370 Train loss 1.02 on epoch=92
05/21/2022 10:53:29 - INFO - __main__ - Step 380 Global step 380 Train loss 1.01 on epoch=94
05/21/2022 10:53:30 - INFO - __main__ - Step 390 Global step 390 Train loss 1.02 on epoch=97
05/21/2022 10:53:32 - INFO - __main__ - Step 400 Global step 400 Train loss 1.01 on epoch=99
05/21/2022 10:53:32 - INFO - __main__ - Global step 400 Train loss 1.00 Classification-F1 0.1 on epoch=99
05/21/2022 10:53:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.93 on epoch=102
05/21/2022 10:53:35 - INFO - __main__ - Step 420 Global step 420 Train loss 1.13 on epoch=104
05/21/2022 10:53:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.95 on epoch=107
05/21/2022 10:53:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.86 on epoch=109
05/21/2022 10:53:39 - INFO - __main__ - Step 450 Global step 450 Train loss 1.01 on epoch=112
05/21/2022 10:53:40 - INFO - __main__ - Global step 450 Train loss 0.98 Classification-F1 0.12403499742665978 on epoch=112
05/21/2022 10:53:42 - INFO - __main__ - Step 460 Global step 460 Train loss 1.05 on epoch=114
05/21/2022 10:53:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.93 on epoch=117
05/21/2022 10:53:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.92 on epoch=119
05/21/2022 10:53:46 - INFO - __main__ - Step 490 Global step 490 Train loss 1.02 on epoch=122
05/21/2022 10:53:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.90 on epoch=124
05/21/2022 10:53:48 - INFO - __main__ - Global step 500 Train loss 0.96 Classification-F1 0.09210526315789473 on epoch=124
05/21/2022 10:53:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.91 on epoch=127
05/21/2022 10:53:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.95 on epoch=129
05/21/2022 10:53:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.80 on epoch=132
05/21/2022 10:53:55 - INFO - __main__ - Step 540 Global step 540 Train loss 1.10 on epoch=134
05/21/2022 10:53:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.96 on epoch=137
05/21/2022 10:53:58 - INFO - __main__ - Global step 550 Train loss 0.94 Classification-F1 0.09210526315789473 on epoch=137
05/21/2022 10:53:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.94 on epoch=139
05/21/2022 10:54:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.94 on epoch=142
05/21/2022 10:54:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.93 on epoch=144
05/21/2022 10:54:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.85 on epoch=147
05/21/2022 10:54:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.97 on epoch=149
05/21/2022 10:54:06 - INFO - __main__ - Global step 600 Train loss 0.93 Classification-F1 0.1 on epoch=149
05/21/2022 10:54:07 - INFO - __main__ - Step 610 Global step 610 Train loss 1.02 on epoch=152
05/21/2022 10:54:09 - INFO - __main__ - Step 620 Global step 620 Train loss 1.03 on epoch=154
05/21/2022 10:54:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.98 on epoch=157
05/21/2022 10:54:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.93 on epoch=159
05/21/2022 10:54:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.87 on epoch=162
05/21/2022 10:54:13 - INFO - __main__ - Global step 650 Train loss 0.97 Classification-F1 0.09615384615384615 on epoch=162
05/21/2022 10:54:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.89 on epoch=164
05/21/2022 10:54:16 - INFO - __main__ - Step 670 Global step 670 Train loss 1.03 on epoch=167
05/21/2022 10:54:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.94 on epoch=169
05/21/2022 10:54:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.89 on epoch=172
05/21/2022 10:54:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.90 on epoch=174
05/21/2022 10:54:22 - INFO - __main__ - Global step 700 Train loss 0.93 Classification-F1 0.1 on epoch=174
05/21/2022 10:54:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.83 on epoch=177
05/21/2022 10:54:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.88 on epoch=179
05/21/2022 10:54:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.90 on epoch=182
05/21/2022 10:54:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.91 on epoch=184
05/21/2022 10:54:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.86 on epoch=187
05/21/2022 10:54:30 - INFO - __main__ - Global step 750 Train loss 0.88 Classification-F1 0.0974025974025974 on epoch=187
05/21/2022 10:54:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.87 on epoch=189
05/21/2022 10:54:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.97 on epoch=192
05/21/2022 10:54:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.88 on epoch=194
05/21/2022 10:54:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.85 on epoch=197
05/21/2022 10:54:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.96 on epoch=199
05/21/2022 10:54:38 - INFO - __main__ - Global step 800 Train loss 0.91 Classification-F1 0.1 on epoch=199
05/21/2022 10:54:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.96 on epoch=202
05/21/2022 10:54:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.90 on epoch=204
05/21/2022 10:54:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.92 on epoch=207
05/21/2022 10:54:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.85 on epoch=209
05/21/2022 10:54:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.83 on epoch=212
05/21/2022 10:54:45 - INFO - __main__ - Global step 850 Train loss 0.89 Classification-F1 0.09210526315789473 on epoch=212
05/21/2022 10:54:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.95 on epoch=214
05/21/2022 10:54:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.91 on epoch=217
05/21/2022 10:54:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.87 on epoch=219
05/21/2022 10:54:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.94 on epoch=222
05/21/2022 10:54:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.94 on epoch=224
05/21/2022 10:54:52 - INFO - __main__ - Global step 900 Train loss 0.92 Classification-F1 0.10126582278481013 on epoch=224
05/21/2022 10:54:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.88 on epoch=227
05/21/2022 10:54:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.89 on epoch=229
05/21/2022 10:54:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.88 on epoch=232
05/21/2022 10:54:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.88 on epoch=234
05/21/2022 10:54:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.92 on epoch=237
05/21/2022 10:55:00 - INFO - __main__ - Global step 950 Train loss 0.89 Classification-F1 0.10126582278481013 on epoch=237
05/21/2022 10:55:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.96 on epoch=239
05/21/2022 10:55:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.94 on epoch=242
05/21/2022 10:55:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.92 on epoch=244
05/21/2022 10:55:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.83 on epoch=247
05/21/2022 10:55:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.87 on epoch=249
05/21/2022 10:55:07 - INFO - __main__ - Global step 1000 Train loss 0.90 Classification-F1 0.10126582278481013 on epoch=249
05/21/2022 10:55:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.76 on epoch=252
05/21/2022 10:55:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.88 on epoch=254
05/21/2022 10:55:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.88 on epoch=257
05/21/2022 10:55:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.89 on epoch=259
05/21/2022 10:55:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.92 on epoch=262
05/21/2022 10:55:14 - INFO - __main__ - Global step 1050 Train loss 0.87 Classification-F1 0.0974025974025974 on epoch=262
05/21/2022 10:55:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.78 on epoch=264
05/21/2022 10:55:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.76 on epoch=267
05/21/2022 10:55:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.93 on epoch=269
05/21/2022 10:55:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.87 on epoch=272
05/21/2022 10:55:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.90 on epoch=274
05/21/2022 10:55:21 - INFO - __main__ - Global step 1100 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=274
05/21/2022 10:55:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.83 on epoch=277
05/21/2022 10:55:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.85 on epoch=279
05/21/2022 10:55:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.89 on epoch=282
05/21/2022 10:55:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.91 on epoch=284
05/21/2022 10:55:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.79 on epoch=287
05/21/2022 10:55:29 - INFO - __main__ - Global step 1150 Train loss 0.85 Classification-F1 0.0974025974025974 on epoch=287
05/21/2022 10:55:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.90 on epoch=289
05/21/2022 10:55:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.84 on epoch=292
05/21/2022 10:55:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.87 on epoch=294
05/21/2022 10:55:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.83 on epoch=297
05/21/2022 10:55:35 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.90 on epoch=299
05/21/2022 10:55:36 - INFO - __main__ - Global step 1200 Train loss 0.87 Classification-F1 0.10126582278481013 on epoch=299
05/21/2022 10:55:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.93 on epoch=302
05/21/2022 10:55:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.84 on epoch=304
05/21/2022 10:55:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.84 on epoch=307
05/21/2022 10:55:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.81 on epoch=309
05/21/2022 10:55:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.81 on epoch=312
05/21/2022 10:55:45 - INFO - __main__ - Global step 1250 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=312
05/21/2022 10:55:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.89 on epoch=314
05/21/2022 10:55:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.86 on epoch=317
05/21/2022 10:55:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.89 on epoch=319
05/21/2022 10:55:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.80 on epoch=322
05/21/2022 10:55:52 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.80 on epoch=324
05/21/2022 10:55:52 - INFO - __main__ - Global step 1300 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=324
05/21/2022 10:55:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.83 on epoch=327
05/21/2022 10:55:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.74 on epoch=329
05/21/2022 10:55:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.83 on epoch=332
05/21/2022 10:55:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.75 on epoch=334
05/21/2022 10:56:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.83 on epoch=337
05/21/2022 10:56:01 - INFO - __main__ - Global step 1350 Train loss 0.80 Classification-F1 0.10126582278481013 on epoch=337
05/21/2022 10:56:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.85 on epoch=339
05/21/2022 10:56:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.88 on epoch=342
05/21/2022 10:56:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.89 on epoch=344
05/21/2022 10:56:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.79 on epoch=347
05/21/2022 10:56:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.87 on epoch=349
05/21/2022 10:56:09 - INFO - __main__ - Global step 1400 Train loss 0.86 Classification-F1 0.09615384615384615 on epoch=349
05/21/2022 10:56:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.80 on epoch=352
05/21/2022 10:56:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.89 on epoch=354
05/21/2022 10:56:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.90 on epoch=357
05/21/2022 10:56:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.88 on epoch=359
05/21/2022 10:56:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.80 on epoch=362
05/21/2022 10:56:16 - INFO - __main__ - Global step 1450 Train loss 0.86 Classification-F1 0.09615384615384615 on epoch=362
05/21/2022 10:56:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.82 on epoch=364
05/21/2022 10:56:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.86 on epoch=367
05/21/2022 10:56:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.77 on epoch=369
05/21/2022 10:56:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.97 on epoch=372
05/21/2022 10:56:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.83 on epoch=374
05/21/2022 10:56:24 - INFO - __main__ - Global step 1500 Train loss 0.85 Classification-F1 0.09615384615384615 on epoch=374
05/21/2022 10:56:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.82 on epoch=377
05/21/2022 10:56:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.91 on epoch=379
05/21/2022 10:56:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.85 on epoch=382
05/21/2022 10:56:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.79 on epoch=384
05/21/2022 10:56:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.85 on epoch=387
05/21/2022 10:56:33 - INFO - __main__ - Global step 1550 Train loss 0.85 Classification-F1 0.09210526315789473 on epoch=387
05/21/2022 10:56:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.87 on epoch=389
05/21/2022 10:56:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.78 on epoch=392
05/21/2022 10:56:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.78 on epoch=394
05/21/2022 10:56:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.80 on epoch=397
05/21/2022 10:56:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.84 on epoch=399
05/21/2022 10:56:40 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.09615384615384615 on epoch=399
05/21/2022 10:56:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.80 on epoch=402
05/21/2022 10:56:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.80 on epoch=404
05/21/2022 10:56:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.88 on epoch=407
05/21/2022 10:56:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.84 on epoch=409
05/21/2022 10:56:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.76 on epoch=412
05/21/2022 10:56:47 - INFO - __main__ - Global step 1650 Train loss 0.82 Classification-F1 0.0974025974025974 on epoch=412
05/21/2022 10:56:49 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.84 on epoch=414
05/21/2022 10:56:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.85 on epoch=417
05/21/2022 10:56:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.84 on epoch=419
05/21/2022 10:56:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.91 on epoch=422
05/21/2022 10:56:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.85 on epoch=424
05/21/2022 10:56:55 - INFO - __main__ - Global step 1700 Train loss 0.86 Classification-F1 0.09210526315789473 on epoch=424
05/21/2022 10:56:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.85 on epoch=427
05/21/2022 10:56:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.80 on epoch=429
05/21/2022 10:56:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.77 on epoch=432
05/21/2022 10:56:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.88 on epoch=434
05/21/2022 10:57:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.81 on epoch=437
05/21/2022 10:57:01 - INFO - __main__ - Global step 1750 Train loss 0.82 Classification-F1 0.08666666666666666 on epoch=437
05/21/2022 10:57:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.89 on epoch=439
05/21/2022 10:57:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.71 on epoch=442
05/21/2022 10:57:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.79 on epoch=444
05/21/2022 10:57:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.82 on epoch=447
05/21/2022 10:57:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.84 on epoch=449
05/21/2022 10:57:08 - INFO - __main__ - Global step 1800 Train loss 0.81 Classification-F1 0.08783783783783784 on epoch=449
05/21/2022 10:57:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.79 on epoch=452
05/21/2022 10:57:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.79 on epoch=454
05/21/2022 10:57:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.79 on epoch=457
05/21/2022 10:57:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.78 on epoch=459
05/21/2022 10:57:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.83 on epoch=462
05/21/2022 10:57:16 - INFO - __main__ - Global step 1850 Train loss 0.79 Classification-F1 0.1237183868762816 on epoch=462
05/21/2022 10:57:17 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.80 on epoch=464
05/21/2022 10:57:19 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.80 on epoch=467
05/21/2022 10:57:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.75 on epoch=469
05/21/2022 10:57:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.78 on epoch=472
05/21/2022 10:57:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.89 on epoch=474
05/21/2022 10:57:24 - INFO - __main__ - Global step 1900 Train loss 0.80 Classification-F1 0.13034188034188032 on epoch=474
05/21/2022 10:57:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.89 on epoch=477
05/21/2022 10:57:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.79 on epoch=479
05/21/2022 10:57:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.88 on epoch=482
05/21/2022 10:57:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.83 on epoch=484
05/21/2022 10:57:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.71 on epoch=487
05/21/2022 10:57:32 - INFO - __main__ - Global step 1950 Train loss 0.82 Classification-F1 0.11444444444444443 on epoch=487
05/21/2022 10:57:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.82 on epoch=489
05/21/2022 10:57:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.84 on epoch=492
05/21/2022 10:57:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.79 on epoch=494
05/21/2022 10:57:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.76 on epoch=497
05/21/2022 10:57:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.75 on epoch=499
05/21/2022 10:57:39 - INFO - __main__ - Global step 2000 Train loss 0.79 Classification-F1 0.1237183868762816 on epoch=499
05/21/2022 10:57:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.81 on epoch=502
05/21/2022 10:57:42 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.85 on epoch=504
05/21/2022 10:57:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.82 on epoch=507
05/21/2022 10:57:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.90 on epoch=509
05/21/2022 10:57:46 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.86 on epoch=512
05/21/2022 10:57:46 - INFO - __main__ - Global step 2050 Train loss 0.85 Classification-F1 0.10571428571428572 on epoch=512
05/21/2022 10:57:48 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.71 on epoch=514
05/21/2022 10:57:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.80 on epoch=517
05/21/2022 10:57:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.86 on epoch=519
05/21/2022 10:57:52 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.87 on epoch=522
05/21/2022 10:57:54 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.78 on epoch=524
05/21/2022 10:57:55 - INFO - __main__ - Global step 2100 Train loss 0.80 Classification-F1 0.12393162393162392 on epoch=524
05/21/2022 10:57:56 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.79 on epoch=527
05/21/2022 10:57:58 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.88 on epoch=529
05/21/2022 10:57:59 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.78 on epoch=532
05/21/2022 10:58:01 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.72 on epoch=534
05/21/2022 10:58:03 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.73 on epoch=537
05/21/2022 10:58:03 - INFO - __main__ - Global step 2150 Train loss 0.78 Classification-F1 0.11404109589041095 on epoch=537
05/21/2022 10:58:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.78 on epoch=539
05/21/2022 10:58:06 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.82 on epoch=542
05/21/2022 10:58:08 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.83 on epoch=544
05/21/2022 10:58:09 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.76 on epoch=547
05/21/2022 10:58:11 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.92 on epoch=549
05/21/2022 10:58:11 - INFO - __main__ - Global step 2200 Train loss 0.82 Classification-F1 0.11283783783783785 on epoch=549
05/21/2022 10:58:13 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.75 on epoch=552
05/21/2022 10:58:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.73 on epoch=554
05/21/2022 10:58:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.80 on epoch=557
05/21/2022 10:58:18 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.77 on epoch=559
05/21/2022 10:58:19 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.81 on epoch=562
05/21/2022 10:58:20 - INFO - __main__ - Global step 2250 Train loss 0.77 Classification-F1 0.11833333333333335 on epoch=562
05/21/2022 10:58:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.84 on epoch=564
05/21/2022 10:58:23 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.87 on epoch=567
05/21/2022 10:58:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.81 on epoch=569
05/21/2022 10:58:26 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.80 on epoch=572
05/21/2022 10:58:28 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.76 on epoch=574
05/21/2022 10:58:28 - INFO - __main__ - Global step 2300 Train loss 0.82 Classification-F1 0.1237183868762816 on epoch=574
05/21/2022 10:58:30 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.78 on epoch=577
05/21/2022 10:58:31 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.80 on epoch=579
05/21/2022 10:58:33 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.78 on epoch=582
05/21/2022 10:58:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.85 on epoch=584
05/21/2022 10:58:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.80 on epoch=587
05/21/2022 10:58:37 - INFO - __main__ - Global step 2350 Train loss 0.80 Classification-F1 0.09615384615384615 on epoch=587
05/21/2022 10:58:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.83 on epoch=589
05/21/2022 10:58:39 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.79 on epoch=592
05/21/2022 10:58:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.85 on epoch=594
05/21/2022 10:58:42 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.83 on epoch=597
05/21/2022 10:58:43 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.81 on epoch=599
05/21/2022 10:58:44 - INFO - __main__ - Global step 2400 Train loss 0.82 Classification-F1 0.11710526315789474 on epoch=599
05/21/2022 10:58:45 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.83 on epoch=602
05/21/2022 10:58:47 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.73 on epoch=604
05/21/2022 10:58:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.79 on epoch=607
05/21/2022 10:58:49 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.82 on epoch=609
05/21/2022 10:58:50 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.76 on epoch=612
05/21/2022 10:58:51 - INFO - __main__ - Global step 2450 Train loss 0.78 Classification-F1 0.1237183868762816 on epoch=612
05/21/2022 10:58:52 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.77 on epoch=614
05/21/2022 10:58:53 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.80 on epoch=617
05/21/2022 10:58:55 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.84 on epoch=619
05/21/2022 10:58:56 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.79 on epoch=622
05/21/2022 10:58:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.83 on epoch=624
05/21/2022 10:58:58 - INFO - __main__ - Global step 2500 Train loss 0.80 Classification-F1 0.09493670886075949 on epoch=624
05/21/2022 10:58:59 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.76 on epoch=627
05/21/2022 10:59:01 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.87 on epoch=629
05/21/2022 10:59:03 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.84 on epoch=632
05/21/2022 10:59:04 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.77 on epoch=634
05/21/2022 10:59:05 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.86 on epoch=637
05/21/2022 10:59:06 - INFO - __main__ - Global step 2550 Train loss 0.82 Classification-F1 0.11842105263157894 on epoch=637
05/21/2022 10:59:07 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.87 on epoch=639
05/21/2022 10:59:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.79 on epoch=642
05/21/2022 10:59:10 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.80 on epoch=644
05/21/2022 10:59:12 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.83 on epoch=647
05/21/2022 10:59:13 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.77 on epoch=649
05/21/2022 10:59:14 - INFO - __main__ - Global step 2600 Train loss 0.81 Classification-F1 0.11404109589041095 on epoch=649
05/21/2022 10:59:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.81 on epoch=652
05/21/2022 10:59:17 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.78 on epoch=654
05/21/2022 10:59:18 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.83 on epoch=657
05/21/2022 10:59:20 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.84 on epoch=659
05/21/2022 10:59:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.84 on epoch=662
05/21/2022 10:59:22 - INFO - __main__ - Global step 2650 Train loss 0.82 Classification-F1 0.11336176261549394 on epoch=662
05/21/2022 10:59:23 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.78 on epoch=664
05/21/2022 10:59:24 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.84 on epoch=667
05/21/2022 10:59:26 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.81 on epoch=669
05/21/2022 10:59:28 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.76 on epoch=672
05/21/2022 10:59:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.78 on epoch=674
05/21/2022 10:59:30 - INFO - __main__ - Global step 2700 Train loss 0.79 Classification-F1 0.11840411840411841 on epoch=674
05/21/2022 10:59:31 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.77 on epoch=677
05/21/2022 10:59:33 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.78 on epoch=679
05/21/2022 10:59:34 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.72 on epoch=682
05/21/2022 10:59:36 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.83 on epoch=684
05/21/2022 10:59:38 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.79 on epoch=687
05/21/2022 10:59:38 - INFO - __main__ - Global step 2750 Train loss 0.78 Classification-F1 0.12222222222222223 on epoch=687
05/21/2022 10:59:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.86 on epoch=689
05/21/2022 10:59:41 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.77 on epoch=692
05/21/2022 10:59:43 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.76 on epoch=694
05/21/2022 10:59:44 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.78 on epoch=697
05/21/2022 10:59:46 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.81 on epoch=699
05/21/2022 10:59:46 - INFO - __main__ - Global step 2800 Train loss 0.80 Classification-F1 0.14267676767676768 on epoch=699
05/21/2022 10:59:46 - INFO - __main__ - Saving model with best Classification-F1: 0.14221364221364222 -> 0.14267676767676768 on epoch=699, global_step=2800
05/21/2022 10:59:47 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.85 on epoch=702
05/21/2022 10:59:49 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.82 on epoch=704
05/21/2022 10:59:50 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.75 on epoch=707
05/21/2022 10:59:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.78 on epoch=709
05/21/2022 10:59:54 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.79 on epoch=712
05/21/2022 10:59:54 - INFO - __main__ - Global step 2850 Train loss 0.80 Classification-F1 0.14134495641344957 on epoch=712
05/21/2022 10:59:56 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.73 on epoch=714
05/21/2022 10:59:58 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.79 on epoch=717
05/21/2022 10:59:59 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.83 on epoch=719
05/21/2022 11:00:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.86 on epoch=722
05/21/2022 11:00:02 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.74 on epoch=724
05/21/2022 11:00:02 - INFO - __main__ - Global step 2900 Train loss 0.79 Classification-F1 0.11840411840411841 on epoch=724
05/21/2022 11:00:03 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.83 on epoch=727
05/21/2022 11:00:04 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.78 on epoch=729
05/21/2022 11:00:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.74 on epoch=732
05/21/2022 11:00:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.79 on epoch=734
05/21/2022 11:00:08 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.79 on epoch=737
05/21/2022 11:00:09 - INFO - __main__ - Global step 2950 Train loss 0.79 Classification-F1 0.13666014350945857 on epoch=737
05/21/2022 11:00:10 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.75 on epoch=739
05/21/2022 11:00:12 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.76 on epoch=742
05/21/2022 11:00:13 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.76 on epoch=744
05/21/2022 11:00:15 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.76 on epoch=747
05/21/2022 11:00:16 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.80 on epoch=749
05/21/2022 11:00:17 - INFO - __main__ - Global step 3000 Train loss 0.76 Classification-F1 0.14004914004914004 on epoch=749
05/21/2022 11:00:17 - INFO - __main__ - save last model!
05/21/2022 11:00:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 11:00:17 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 11:00:17 - INFO - __main__ - Printing 3 examples
05/21/2022 11:00:17 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 11:00:17 - INFO - __main__ - ['others']
05/21/2022 11:00:17 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 11:00:17 - INFO - __main__ - ['others']
05/21/2022 11:00:17 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 11:00:17 - INFO - __main__ - ['others']
05/21/2022 11:00:17 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:00:17 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:00:17 - INFO - __main__ - Printing 3 examples
05/21/2022 11:00:17 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/21/2022 11:00:17 - INFO - __main__ - ['happy']
05/21/2022 11:00:17 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/21/2022 11:00:17 - INFO - __main__ - ['happy']
05/21/2022 11:00:17 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/21/2022 11:00:17 - INFO - __main__ - ['happy']
05/21/2022 11:00:17 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:00:17 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:00:17 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 11:00:17 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:00:17 - INFO - __main__ - Printing 3 examples
05/21/2022 11:00:17 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/21/2022 11:00:17 - INFO - __main__ - ['happy']
05/21/2022 11:00:17 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/21/2022 11:00:17 - INFO - __main__ - ['happy']
05/21/2022 11:00:17 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/21/2022 11:00:17 - INFO - __main__ - ['happy']
05/21/2022 11:00:17 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:00:17 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:00:18 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 11:00:19 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:00:23 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 11:00:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 11:00:23 - INFO - __main__ - Starting training!
05/21/2022 11:00:24 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 11:01:09 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_21_0.2_8_predictions.txt
05/21/2022 11:01:09 - INFO - __main__ - Classification-F1 on test data: 0.0714
05/21/2022 11:01:09 - INFO - __main__ - prefix=emo_16_21, lr=0.2, bsz=8, dev_performance=0.14267676767676768, test_performance=0.07144068400259924
05/21/2022 11:01:09 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.5, bsz=8 ...
05/21/2022 11:01:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:01:10 - INFO - __main__ - Printing 3 examples
05/21/2022 11:01:10 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/21/2022 11:01:10 - INFO - __main__ - ['happy']
05/21/2022 11:01:10 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/21/2022 11:01:10 - INFO - __main__ - ['happy']
05/21/2022 11:01:10 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/21/2022 11:01:10 - INFO - __main__ - ['happy']
05/21/2022 11:01:10 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:01:10 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:01:10 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 11:01:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:01:10 - INFO - __main__ - Printing 3 examples
05/21/2022 11:01:10 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/21/2022 11:01:10 - INFO - __main__ - ['happy']
05/21/2022 11:01:10 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/21/2022 11:01:10 - INFO - __main__ - ['happy']
05/21/2022 11:01:10 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/21/2022 11:01:10 - INFO - __main__ - ['happy']
05/21/2022 11:01:10 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:01:10 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:01:10 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 11:01:16 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 11:01:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 11:01:16 - INFO - __main__ - Starting training!
05/21/2022 11:01:19 - INFO - __main__ - Step 10 Global step 10 Train loss 4.11 on epoch=2
05/21/2022 11:01:21 - INFO - __main__ - Step 20 Global step 20 Train loss 3.58 on epoch=4
05/21/2022 11:01:22 - INFO - __main__ - Step 30 Global step 30 Train loss 2.87 on epoch=7
05/21/2022 11:01:24 - INFO - __main__ - Step 40 Global step 40 Train loss 2.44 on epoch=9
05/21/2022 11:01:25 - INFO - __main__ - Step 50 Global step 50 Train loss 1.96 on epoch=12
05/21/2022 11:01:26 - INFO - __main__ - Global step 50 Train loss 2.99 Classification-F1 0.0810126582278481 on epoch=12
05/21/2022 11:01:26 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0810126582278481 on epoch=12, global_step=50
05/21/2022 11:01:28 - INFO - __main__ - Step 60 Global step 60 Train loss 1.77 on epoch=14
05/21/2022 11:01:29 - INFO - __main__ - Step 70 Global step 70 Train loss 1.53 on epoch=17
05/21/2022 11:01:31 - INFO - __main__ - Step 80 Global step 80 Train loss 1.39 on epoch=19
05/21/2022 11:01:32 - INFO - __main__ - Step 90 Global step 90 Train loss 1.13 on epoch=22
05/21/2022 11:01:34 - INFO - __main__ - Step 100 Global step 100 Train loss 1.04 on epoch=24
05/21/2022 11:01:35 - INFO - __main__ - Global step 100 Train loss 1.37 Classification-F1 0.08311688311688312 on epoch=24
05/21/2022 11:01:35 - INFO - __main__ - Saving model with best Classification-F1: 0.0810126582278481 -> 0.08311688311688312 on epoch=24, global_step=100
05/21/2022 11:01:36 - INFO - __main__ - Step 110 Global step 110 Train loss 1.08 on epoch=27
05/21/2022 11:01:38 - INFO - __main__ - Step 120 Global step 120 Train loss 1.01 on epoch=29
05/21/2022 11:01:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.96 on epoch=32
05/21/2022 11:01:41 - INFO - __main__ - Step 140 Global step 140 Train loss 1.00 on epoch=34
05/21/2022 11:01:43 - INFO - __main__ - Step 150 Global step 150 Train loss 1.02 on epoch=37
05/21/2022 11:01:44 - INFO - __main__ - Global step 150 Train loss 1.01 Classification-F1 0.13067758749069247 on epoch=37
05/21/2022 11:01:44 - INFO - __main__ - Saving model with best Classification-F1: 0.08311688311688312 -> 0.13067758749069247 on epoch=37, global_step=150
05/21/2022 11:01:45 - INFO - __main__ - Step 160 Global step 160 Train loss 1.00 on epoch=39
05/21/2022 11:01:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.91 on epoch=42
05/21/2022 11:01:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=44
05/21/2022 11:01:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.90 on epoch=47
05/21/2022 11:01:50 - INFO - __main__ - Step 200 Global step 200 Train loss 1.00 on epoch=49
05/21/2022 11:01:51 - INFO - __main__ - Global step 200 Train loss 0.94 Classification-F1 0.13067758749069247 on epoch=49
05/21/2022 11:01:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.83 on epoch=52
05/21/2022 11:01:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.97 on epoch=54
05/21/2022 11:01:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.93 on epoch=57
05/21/2022 11:01:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.94 on epoch=59
05/21/2022 11:01:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.93 on epoch=62
05/21/2022 11:01:58 - INFO - __main__ - Global step 250 Train loss 0.92 Classification-F1 0.10126582278481013 on epoch=62
05/21/2022 11:02:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.99 on epoch=64
05/21/2022 11:02:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.90 on epoch=67
05/21/2022 11:02:03 - INFO - __main__ - Step 280 Global step 280 Train loss 1.00 on epoch=69
05/21/2022 11:02:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.90 on epoch=72
05/21/2022 11:02:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.86 on epoch=74
05/21/2022 11:02:06 - INFO - __main__ - Global step 300 Train loss 0.93 Classification-F1 0.10126582278481013 on epoch=74
05/21/2022 11:02:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.90 on epoch=77
05/21/2022 11:02:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.85 on epoch=79
05/21/2022 11:02:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.93 on epoch=82
05/21/2022 11:02:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.91 on epoch=84
05/21/2022 11:02:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.94 on epoch=87
05/21/2022 11:02:14 - INFO - __main__ - Global step 350 Train loss 0.91 Classification-F1 0.10126582278481013 on epoch=87
05/21/2022 11:02:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.88 on epoch=89
05/21/2022 11:02:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.84 on epoch=92
05/21/2022 11:02:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.94 on epoch=94
05/21/2022 11:02:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=97
05/21/2022 11:02:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.95 on epoch=99
05/21/2022 11:02:21 - INFO - __main__ - Global step 400 Train loss 0.90 Classification-F1 0.10126582278481013 on epoch=99
05/21/2022 11:02:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.93 on epoch=102
05/21/2022 11:02:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.90 on epoch=104
05/21/2022 11:02:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.93 on epoch=107
05/21/2022 11:02:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.91 on epoch=109
05/21/2022 11:02:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.84 on epoch=112
05/21/2022 11:02:29 - INFO - __main__ - Global step 450 Train loss 0.90 Classification-F1 0.13197586726998492 on epoch=112
05/21/2022 11:02:29 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.13197586726998492 on epoch=112, global_step=450
05/21/2022 11:02:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.84 on epoch=114
05/21/2022 11:02:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.87 on epoch=117
05/21/2022 11:02:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.86 on epoch=119
05/21/2022 11:02:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.90 on epoch=122
05/21/2022 11:02:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.73 on epoch=124
05/21/2022 11:02:36 - INFO - __main__ - Global step 500 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=124
05/21/2022 11:02:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.84 on epoch=127
05/21/2022 11:02:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.85 on epoch=129
05/21/2022 11:02:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.89 on epoch=132
05/21/2022 11:02:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.84 on epoch=134
05/21/2022 11:02:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.92 on epoch=137
05/21/2022 11:02:43 - INFO - __main__ - Global step 550 Train loss 0.87 Classification-F1 0.13067758749069247 on epoch=137
05/21/2022 11:02:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.83 on epoch=139
05/21/2022 11:02:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.91 on epoch=142
05/21/2022 11:02:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.86 on epoch=144
05/21/2022 11:02:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.86 on epoch=147
05/21/2022 11:02:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.83 on epoch=149
05/21/2022 11:02:50 - INFO - __main__ - Global step 600 Train loss 0.86 Classification-F1 0.10126582278481013 on epoch=149
05/21/2022 11:02:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.87 on epoch=152
05/21/2022 11:02:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.87 on epoch=154
05/21/2022 11:02:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.74 on epoch=157
05/21/2022 11:02:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.77 on epoch=159
05/21/2022 11:02:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.82 on epoch=162
05/21/2022 11:02:58 - INFO - __main__ - Global step 650 Train loss 0.81 Classification-F1 0.13067758749069247 on epoch=162
05/21/2022 11:02:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.88 on epoch=164
05/21/2022 11:03:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.80 on epoch=167
05/21/2022 11:03:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.88 on epoch=169
05/21/2022 11:03:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.82 on epoch=172
05/21/2022 11:03:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.92 on epoch=174
05/21/2022 11:03:06 - INFO - __main__ - Global step 700 Train loss 0.86 Classification-F1 0.09615384615384615 on epoch=174
05/21/2022 11:03:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.81 on epoch=177
05/21/2022 11:03:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.86 on epoch=179
05/21/2022 11:03:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.88 on epoch=182
05/21/2022 11:03:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.92 on epoch=184
05/21/2022 11:03:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.73 on epoch=187
05/21/2022 11:03:13 - INFO - __main__ - Global step 750 Train loss 0.84 Classification-F1 0.13067758749069247 on epoch=187
05/21/2022 11:03:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.80 on epoch=189
05/21/2022 11:03:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.79 on epoch=192
05/21/2022 11:03:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.85 on epoch=194
05/21/2022 11:03:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.86 on epoch=197
05/21/2022 11:03:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.98 on epoch=199
05/21/2022 11:03:21 - INFO - __main__ - Global step 800 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=199
05/21/2022 11:03:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.81 on epoch=202
05/21/2022 11:03:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.81 on epoch=204
05/21/2022 11:03:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.78 on epoch=207
05/21/2022 11:03:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.89 on epoch=209
05/21/2022 11:03:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.82 on epoch=212
05/21/2022 11:03:29 - INFO - __main__ - Global step 850 Train loss 0.82 Classification-F1 0.17773972602739727 on epoch=212
05/21/2022 11:03:29 - INFO - __main__ - Saving model with best Classification-F1: 0.13197586726998492 -> 0.17773972602739727 on epoch=212, global_step=850
05/21/2022 11:03:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.83 on epoch=214
05/21/2022 11:03:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.87 on epoch=217
05/21/2022 11:03:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.82 on epoch=219
05/21/2022 11:03:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.85 on epoch=222
05/21/2022 11:03:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.82 on epoch=224
05/21/2022 11:03:36 - INFO - __main__ - Global step 900 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=224
05/21/2022 11:03:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.78 on epoch=227
05/21/2022 11:03:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.84 on epoch=229
05/21/2022 11:03:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.81 on epoch=232
05/21/2022 11:03:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.85 on epoch=234
05/21/2022 11:03:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.86 on epoch=237
05/21/2022 11:03:44 - INFO - __main__ - Global step 950 Train loss 0.83 Classification-F1 0.10256410256410256 on epoch=237
05/21/2022 11:03:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.82 on epoch=239
05/21/2022 11:03:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.76 on epoch=242
05/21/2022 11:03:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.88 on epoch=244
05/21/2022 11:03:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.83 on epoch=247
05/21/2022 11:03:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.77 on epoch=249
05/21/2022 11:03:51 - INFO - __main__ - Global step 1000 Train loss 0.81 Classification-F1 0.10126582278481013 on epoch=249
05/21/2022 11:03:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.84 on epoch=252
05/21/2022 11:03:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.81 on epoch=254
05/21/2022 11:03:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.82 on epoch=257
05/21/2022 11:03:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.87 on epoch=259
05/21/2022 11:03:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.76 on epoch=262
05/21/2022 11:03:59 - INFO - __main__ - Global step 1050 Train loss 0.82 Classification-F1 0.15789473684210525 on epoch=262
05/21/2022 11:04:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.79 on epoch=264
05/21/2022 11:04:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.81 on epoch=267
05/21/2022 11:04:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.77 on epoch=269
05/21/2022 11:04:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.81 on epoch=272
05/21/2022 11:04:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.82 on epoch=274
05/21/2022 11:04:07 - INFO - __main__ - Global step 1100 Train loss 0.80 Classification-F1 0.1831081081081081 on epoch=274
05/21/2022 11:04:07 - INFO - __main__ - Saving model with best Classification-F1: 0.17773972602739727 -> 0.1831081081081081 on epoch=274, global_step=1100
05/21/2022 11:04:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.83 on epoch=277
05/21/2022 11:04:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.87 on epoch=279
05/21/2022 11:04:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.76 on epoch=282
05/21/2022 11:04:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.80 on epoch=284
05/21/2022 11:04:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.84 on epoch=287
05/21/2022 11:04:14 - INFO - __main__ - Global step 1150 Train loss 0.82 Classification-F1 0.15416666666666667 on epoch=287
05/21/2022 11:04:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.87 on epoch=289
05/21/2022 11:04:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.82 on epoch=292
05/21/2022 11:04:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.71 on epoch=294
05/21/2022 11:04:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.89 on epoch=297
05/21/2022 11:04:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.87 on epoch=299
05/21/2022 11:04:22 - INFO - __main__ - Global step 1200 Train loss 0.83 Classification-F1 0.15789473684210525 on epoch=299
05/21/2022 11:04:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.89 on epoch=302
05/21/2022 11:04:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.82 on epoch=304
05/21/2022 11:04:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.76 on epoch=307
05/21/2022 11:04:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.75 on epoch=309
05/21/2022 11:04:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.79 on epoch=312
05/21/2022 11:04:30 - INFO - __main__ - Global step 1250 Train loss 0.80 Classification-F1 0.18166666666666667 on epoch=312
05/21/2022 11:04:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.85 on epoch=314
05/21/2022 11:04:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.86 on epoch=317
05/21/2022 11:04:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.81 on epoch=319
05/21/2022 11:04:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.88 on epoch=322
05/21/2022 11:04:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.83 on epoch=324
05/21/2022 11:04:38 - INFO - __main__ - Global step 1300 Train loss 0.85 Classification-F1 0.18166666666666667 on epoch=324
05/21/2022 11:04:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.78 on epoch=327
05/21/2022 11:04:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.83 on epoch=329
05/21/2022 11:04:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.80 on epoch=332
05/21/2022 11:04:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.75 on epoch=334
05/21/2022 11:04:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.83 on epoch=337
05/21/2022 11:04:45 - INFO - __main__ - Global step 1350 Train loss 0.80 Classification-F1 0.19364881693648817 on epoch=337
05/21/2022 11:04:45 - INFO - __main__ - Saving model with best Classification-F1: 0.1831081081081081 -> 0.19364881693648817 on epoch=337, global_step=1350
05/21/2022 11:04:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.74 on epoch=339
05/21/2022 11:04:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.83 on epoch=342
05/21/2022 11:04:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.75 on epoch=344
05/21/2022 11:04:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.83 on epoch=347
05/21/2022 11:04:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.80 on epoch=349
05/21/2022 11:04:53 - INFO - __main__ - Global step 1400 Train loss 0.79 Classification-F1 0.19364881693648817 on epoch=349
05/21/2022 11:04:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.89 on epoch=352
05/21/2022 11:04:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.83 on epoch=354
05/21/2022 11:04:58 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.83 on epoch=357
05/21/2022 11:04:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.81 on epoch=359
05/21/2022 11:05:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.79 on epoch=362
05/21/2022 11:05:01 - INFO - __main__ - Global step 1450 Train loss 0.83 Classification-F1 0.22916666666666666 on epoch=362
05/21/2022 11:05:01 - INFO - __main__ - Saving model with best Classification-F1: 0.19364881693648817 -> 0.22916666666666666 on epoch=362, global_step=1450
05/21/2022 11:05:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.74 on epoch=364
05/21/2022 11:05:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.80 on epoch=367
05/21/2022 11:05:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.83 on epoch=369
05/21/2022 11:05:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.82 on epoch=372
05/21/2022 11:05:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.80 on epoch=374
05/21/2022 11:05:10 - INFO - __main__ - Global step 1500 Train loss 0.80 Classification-F1 0.20869565217391303 on epoch=374
05/21/2022 11:05:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.76 on epoch=377
05/21/2022 11:05:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.88 on epoch=379
05/21/2022 11:05:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.78 on epoch=382
05/21/2022 11:05:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.82 on epoch=384
05/21/2022 11:05:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.81 on epoch=387
05/21/2022 11:05:17 - INFO - __main__ - Global step 1550 Train loss 0.81 Classification-F1 0.2027582159624413 on epoch=387
05/21/2022 11:05:18 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.78 on epoch=389
05/21/2022 11:05:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.82 on epoch=392
05/21/2022 11:05:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.80 on epoch=394
05/21/2022 11:05:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.83 on epoch=397
05/21/2022 11:05:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.81 on epoch=399
05/21/2022 11:05:25 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.18813131313131315 on epoch=399
05/21/2022 11:05:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.78 on epoch=402
05/21/2022 11:05:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.85 on epoch=404
05/21/2022 11:05:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.79 on epoch=407
05/21/2022 11:05:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.74 on epoch=409
05/21/2022 11:05:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.72 on epoch=412
05/21/2022 11:05:32 - INFO - __main__ - Global step 1650 Train loss 0.78 Classification-F1 0.25071895424836604 on epoch=412
05/21/2022 11:05:32 - INFO - __main__ - Saving model with best Classification-F1: 0.22916666666666666 -> 0.25071895424836604 on epoch=412, global_step=1650
05/21/2022 11:05:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.75 on epoch=414
05/21/2022 11:05:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.77 on epoch=417
05/21/2022 11:05:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.74 on epoch=419
05/21/2022 11:05:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.76 on epoch=422
05/21/2022 11:05:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.80 on epoch=424
05/21/2022 11:05:40 - INFO - __main__ - Global step 1700 Train loss 0.76 Classification-F1 0.19797782126549252 on epoch=424
05/21/2022 11:05:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.80 on epoch=427
05/21/2022 11:05:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.81 on epoch=429
05/21/2022 11:05:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.79 on epoch=432
05/21/2022 11:05:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.83 on epoch=434
05/21/2022 11:05:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.80 on epoch=437
05/21/2022 11:05:47 - INFO - __main__ - Global step 1750 Train loss 0.81 Classification-F1 0.2027582159624413 on epoch=437
05/21/2022 11:05:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.82 on epoch=439
05/21/2022 11:05:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.80 on epoch=442
05/21/2022 11:05:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.82 on epoch=444
05/21/2022 11:05:53 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.80 on epoch=447
05/21/2022 11:05:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.82 on epoch=449
05/21/2022 11:05:55 - INFO - __main__ - Global step 1800 Train loss 0.81 Classification-F1 0.23337856173677068 on epoch=449
05/21/2022 11:05:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.79 on epoch=452
05/21/2022 11:05:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.82 on epoch=454
05/21/2022 11:05:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.85 on epoch=457
05/21/2022 11:06:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.77 on epoch=459
05/21/2022 11:06:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.82 on epoch=462
05/21/2022 11:06:03 - INFO - __main__ - Global step 1850 Train loss 0.81 Classification-F1 0.2571091908876669 on epoch=462
05/21/2022 11:06:03 - INFO - __main__ - Saving model with best Classification-F1: 0.25071895424836604 -> 0.2571091908876669 on epoch=462, global_step=1850
05/21/2022 11:06:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.79 on epoch=464
05/21/2022 11:06:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.77 on epoch=467
05/21/2022 11:06:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.78 on epoch=469
05/21/2022 11:06:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.87 on epoch=472
05/21/2022 11:06:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.82 on epoch=474
05/21/2022 11:06:10 - INFO - __main__ - Global step 1900 Train loss 0.81 Classification-F1 0.25071895424836604 on epoch=474
05/21/2022 11:06:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.75 on epoch=477
05/21/2022 11:06:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.80 on epoch=479
05/21/2022 11:06:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.74 on epoch=482
05/21/2022 11:06:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.78 on epoch=484
05/21/2022 11:06:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.80 on epoch=487
05/21/2022 11:06:18 - INFO - __main__ - Global step 1950 Train loss 0.77 Classification-F1 0.25545343137254906 on epoch=487
05/21/2022 11:06:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.77 on epoch=489
05/21/2022 11:06:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.80 on epoch=492
05/21/2022 11:06:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.79 on epoch=494
05/21/2022 11:06:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.78 on epoch=497
05/21/2022 11:06:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.77 on epoch=499
05/21/2022 11:06:26 - INFO - __main__ - Global step 2000 Train loss 0.78 Classification-F1 0.24701492537313433 on epoch=499
05/21/2022 11:06:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.83 on epoch=502
05/21/2022 11:06:29 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.87 on epoch=504
05/21/2022 11:06:30 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.81 on epoch=507
05/21/2022 11:06:32 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.79 on epoch=509
05/21/2022 11:06:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.82 on epoch=512
05/21/2022 11:06:33 - INFO - __main__ - Global step 2050 Train loss 0.83 Classification-F1 0.2558699282837214 on epoch=512
05/21/2022 11:06:35 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.78 on epoch=514
05/21/2022 11:06:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.76 on epoch=517
05/21/2022 11:06:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.77 on epoch=519
05/21/2022 11:06:40 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.83 on epoch=522
05/21/2022 11:06:41 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.80 on epoch=524
05/21/2022 11:06:41 - INFO - __main__ - Global step 2100 Train loss 0.79 Classification-F1 0.24018525334314808 on epoch=524
05/21/2022 11:06:43 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.79 on epoch=527
05/21/2022 11:06:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.75 on epoch=529
05/21/2022 11:06:45 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.74 on epoch=532
05/21/2022 11:06:46 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.70 on epoch=534
05/21/2022 11:06:48 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.86 on epoch=537
05/21/2022 11:06:48 - INFO - __main__ - Global step 2150 Train loss 0.77 Classification-F1 0.20870535714285715 on epoch=537
05/21/2022 11:06:50 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.79 on epoch=539
05/21/2022 11:06:51 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.80 on epoch=542
05/21/2022 11:06:53 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.83 on epoch=544
05/21/2022 11:06:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.77 on epoch=547
05/21/2022 11:06:55 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.76 on epoch=549
05/21/2022 11:06:56 - INFO - __main__ - Global step 2200 Train loss 0.79 Classification-F1 0.19888987130366442 on epoch=549
05/21/2022 11:06:57 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.73 on epoch=552
05/21/2022 11:06:59 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.82 on epoch=554
05/21/2022 11:07:00 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.75 on epoch=557
05/21/2022 11:07:01 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.76 on epoch=559
05/21/2022 11:07:03 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.74 on epoch=562
05/21/2022 11:07:03 - INFO - __main__ - Global step 2250 Train loss 0.76 Classification-F1 0.22817460317460317 on epoch=562
05/21/2022 11:07:04 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.76 on epoch=564
05/21/2022 11:07:06 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.82 on epoch=567
05/21/2022 11:07:07 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.77 on epoch=569
05/21/2022 11:07:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.84 on epoch=572
05/21/2022 11:07:11 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.78 on epoch=574
05/21/2022 11:07:11 - INFO - __main__ - Global step 2300 Train loss 0.80 Classification-F1 0.2327848376235473 on epoch=574
05/21/2022 11:07:13 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.80 on epoch=577
05/21/2022 11:07:14 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.78 on epoch=579
05/21/2022 11:07:16 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.80 on epoch=582
05/21/2022 11:07:17 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.75 on epoch=584
05/21/2022 11:07:18 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.80 on epoch=587
05/21/2022 11:07:19 - INFO - __main__ - Global step 2350 Train loss 0.79 Classification-F1 0.20491373360938575 on epoch=587
05/21/2022 11:07:20 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.80 on epoch=589
05/21/2022 11:07:21 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.84 on epoch=592
05/21/2022 11:07:23 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.83 on epoch=594
05/21/2022 11:07:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.81 on epoch=597
05/21/2022 11:07:26 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.76 on epoch=599
05/21/2022 11:07:27 - INFO - __main__ - Global step 2400 Train loss 0.81 Classification-F1 0.21399215104601216 on epoch=599
05/21/2022 11:07:28 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.76 on epoch=602
05/21/2022 11:07:30 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.75 on epoch=604
05/21/2022 11:07:31 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.81 on epoch=607
05/21/2022 11:07:32 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.75 on epoch=609
05/21/2022 11:07:33 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.89 on epoch=612
05/21/2022 11:07:34 - INFO - __main__ - Global step 2450 Train loss 0.79 Classification-F1 0.2796652675483752 on epoch=612
05/21/2022 11:07:34 - INFO - __main__ - Saving model with best Classification-F1: 0.2571091908876669 -> 0.2796652675483752 on epoch=612, global_step=2450
05/21/2022 11:07:35 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.74 on epoch=614
05/21/2022 11:07:37 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.80 on epoch=617
05/21/2022 11:07:38 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.79 on epoch=619
05/21/2022 11:07:40 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.80 on epoch=622
05/21/2022 11:07:41 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.76 on epoch=624
05/21/2022 11:07:42 - INFO - __main__ - Global step 2500 Train loss 0.78 Classification-F1 0.2256720430107527 on epoch=624
05/21/2022 11:07:43 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.83 on epoch=627
05/21/2022 11:07:44 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.77 on epoch=629
05/21/2022 11:07:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.82 on epoch=632
05/21/2022 11:07:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.73 on epoch=634
05/21/2022 11:07:48 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.80 on epoch=637
05/21/2022 11:07:49 - INFO - __main__ - Global step 2550 Train loss 0.79 Classification-F1 0.26359320339830083 on epoch=637
05/21/2022 11:07:50 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.74 on epoch=639
05/21/2022 11:07:52 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.80 on epoch=642
05/21/2022 11:07:53 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.78 on epoch=644
05/21/2022 11:07:55 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.80 on epoch=647
05/21/2022 11:07:56 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.82 on epoch=649
05/21/2022 11:07:57 - INFO - __main__ - Global step 2600 Train loss 0.79 Classification-F1 0.2558699282837214 on epoch=649
05/21/2022 11:07:58 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.80 on epoch=652
05/21/2022 11:08:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.77 on epoch=654
05/21/2022 11:08:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.76 on epoch=657
05/21/2022 11:08:03 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.76 on epoch=659
05/21/2022 11:08:05 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.76 on epoch=662
05/21/2022 11:08:05 - INFO - __main__ - Global step 2650 Train loss 0.77 Classification-F1 0.24918558217738546 on epoch=662
05/21/2022 11:08:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.76 on epoch=664
05/21/2022 11:08:08 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.81 on epoch=667
05/21/2022 11:08:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.78 on epoch=669
05/21/2022 11:08:12 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.84 on epoch=672
05/21/2022 11:08:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.74 on epoch=674
05/21/2022 11:08:13 - INFO - __main__ - Global step 2700 Train loss 0.79 Classification-F1 0.23663682864450128 on epoch=674
05/21/2022 11:08:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.79 on epoch=677
05/21/2022 11:08:16 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.74 on epoch=679
05/21/2022 11:08:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.87 on epoch=682
05/21/2022 11:08:19 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.80 on epoch=684
05/21/2022 11:08:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.81 on epoch=687
05/21/2022 11:08:21 - INFO - __main__ - Global step 2750 Train loss 0.80 Classification-F1 0.26650537634408605 on epoch=687
05/21/2022 11:08:22 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.78 on epoch=689
05/21/2022 11:08:23 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.78 on epoch=692
05/21/2022 11:08:25 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.82 on epoch=694
05/21/2022 11:08:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.77 on epoch=697
05/21/2022 11:08:27 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.79 on epoch=699
05/21/2022 11:08:28 - INFO - __main__ - Global step 2800 Train loss 0.79 Classification-F1 0.19225302061122956 on epoch=699
05/21/2022 11:08:30 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.79 on epoch=702
05/21/2022 11:08:31 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.84 on epoch=704
05/21/2022 11:08:32 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.76 on epoch=707
05/21/2022 11:08:34 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.77 on epoch=709
05/21/2022 11:08:35 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.76 on epoch=712
05/21/2022 11:08:36 - INFO - __main__ - Global step 2850 Train loss 0.78 Classification-F1 0.3316520467836257 on epoch=712
05/21/2022 11:08:36 - INFO - __main__ - Saving model with best Classification-F1: 0.2796652675483752 -> 0.3316520467836257 on epoch=712, global_step=2850
05/21/2022 11:08:37 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.80 on epoch=714
05/21/2022 11:08:38 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.75 on epoch=717
05/21/2022 11:08:40 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.77 on epoch=719
05/21/2022 11:08:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.82 on epoch=722
05/21/2022 11:08:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.77 on epoch=724
05/21/2022 11:08:43 - INFO - __main__ - Global step 2900 Train loss 0.78 Classification-F1 0.35618659420289855 on epoch=724
05/21/2022 11:08:43 - INFO - __main__ - Saving model with best Classification-F1: 0.3316520467836257 -> 0.35618659420289855 on epoch=724, global_step=2900
05/21/2022 11:08:45 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.82 on epoch=727
05/21/2022 11:08:46 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.80 on epoch=729
05/21/2022 11:08:47 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.76 on epoch=732
05/21/2022 11:08:49 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.81 on epoch=734
05/21/2022 11:08:51 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.81 on epoch=737
05/21/2022 11:08:51 - INFO - __main__ - Global step 2950 Train loss 0.80 Classification-F1 0.24621212121212122 on epoch=737
05/21/2022 11:08:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.83 on epoch=739
05/21/2022 11:08:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.76 on epoch=742
05/21/2022 11:08:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.77 on epoch=744
05/21/2022 11:08:56 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.70 on epoch=747
05/21/2022 11:08:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.77 on epoch=749
05/21/2022 11:08:58 - INFO - __main__ - Global step 3000 Train loss 0.77 Classification-F1 0.2856534090909091 on epoch=749
05/21/2022 11:08:58 - INFO - __main__ - save last model!
05/21/2022 11:08:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 11:08:58 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 11:08:58 - INFO - __main__ - Printing 3 examples
05/21/2022 11:08:58 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 11:08:58 - INFO - __main__ - ['others']
05/21/2022 11:08:58 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 11:08:58 - INFO - __main__ - ['others']
05/21/2022 11:08:58 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 11:08:58 - INFO - __main__ - ['others']
05/21/2022 11:08:58 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:08:59 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:08:59 - INFO - __main__ - Printing 3 examples
05/21/2022 11:08:59 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/21/2022 11:08:59 - INFO - __main__ - ['happy']
05/21/2022 11:08:59 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/21/2022 11:08:59 - INFO - __main__ - ['happy']
05/21/2022 11:08:59 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/21/2022 11:08:59 - INFO - __main__ - ['happy']
05/21/2022 11:08:59 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:08:59 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:08:59 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 11:08:59 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:08:59 - INFO - __main__ - Printing 3 examples
05/21/2022 11:08:59 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/21/2022 11:08:59 - INFO - __main__ - ['happy']
05/21/2022 11:08:59 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/21/2022 11:08:59 - INFO - __main__ - ['happy']
05/21/2022 11:08:59 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/21/2022 11:08:59 - INFO - __main__ - ['happy']
05/21/2022 11:08:59 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:08:59 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:08:59 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 11:09:00 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:09:04 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 11:09:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 11:09:05 - INFO - __main__ - Starting training!
05/21/2022 11:09:07 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 11:09:54 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_42_0.5_8_predictions.txt
05/21/2022 11:09:55 - INFO - __main__ - Classification-F1 on test data: 0.1648
05/21/2022 11:09:55 - INFO - __main__ - prefix=emo_16_42, lr=0.5, bsz=8, dev_performance=0.35618659420289855, test_performance=0.16480105501429312
05/21/2022 11:09:55 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.4, bsz=8 ...
05/21/2022 11:09:56 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:09:56 - INFO - __main__ - Printing 3 examples
05/21/2022 11:09:56 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/21/2022 11:09:56 - INFO - __main__ - ['happy']
05/21/2022 11:09:56 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/21/2022 11:09:56 - INFO - __main__ - ['happy']
05/21/2022 11:09:56 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/21/2022 11:09:56 - INFO - __main__ - ['happy']
05/21/2022 11:09:56 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:09:56 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:09:56 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 11:09:56 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:09:56 - INFO - __main__ - Printing 3 examples
05/21/2022 11:09:56 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/21/2022 11:09:56 - INFO - __main__ - ['happy']
05/21/2022 11:09:56 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/21/2022 11:09:56 - INFO - __main__ - ['happy']
05/21/2022 11:09:56 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/21/2022 11:09:56 - INFO - __main__ - ['happy']
05/21/2022 11:09:56 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:09:56 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:09:56 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 11:10:01 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 11:10:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 11:10:01 - INFO - __main__ - Starting training!
05/21/2022 11:10:03 - INFO - __main__ - Step 10 Global step 10 Train loss 4.23 on epoch=2
05/21/2022 11:10:04 - INFO - __main__ - Step 20 Global step 20 Train loss 3.83 on epoch=4
05/21/2022 11:10:06 - INFO - __main__ - Step 30 Global step 30 Train loss 3.15 on epoch=7
05/21/2022 11:10:07 - INFO - __main__ - Step 40 Global step 40 Train loss 2.78 on epoch=9
05/21/2022 11:10:08 - INFO - __main__ - Step 50 Global step 50 Train loss 2.27 on epoch=12
05/21/2022 11:10:09 - INFO - __main__ - Global step 50 Train loss 3.25 Classification-F1 0.0810126582278481 on epoch=12
05/21/2022 11:10:09 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0810126582278481 on epoch=12, global_step=50
05/21/2022 11:10:10 - INFO - __main__ - Step 60 Global step 60 Train loss 1.91 on epoch=14
05/21/2022 11:10:12 - INFO - __main__ - Step 70 Global step 70 Train loss 1.60 on epoch=17
05/21/2022 11:10:13 - INFO - __main__ - Step 80 Global step 80 Train loss 1.62 on epoch=19
05/21/2022 11:10:15 - INFO - __main__ - Step 90 Global step 90 Train loss 1.35 on epoch=22
05/21/2022 11:10:16 - INFO - __main__ - Step 100 Global step 100 Train loss 1.26 on epoch=24
05/21/2022 11:10:16 - INFO - __main__ - Global step 100 Train loss 1.55 Classification-F1 0.0810126582278481 on epoch=24
05/21/2022 11:10:18 - INFO - __main__ - Step 110 Global step 110 Train loss 1.22 on epoch=27
05/21/2022 11:10:19 - INFO - __main__ - Step 120 Global step 120 Train loss 1.08 on epoch=29
05/21/2022 11:10:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.96 on epoch=32
05/21/2022 11:10:22 - INFO - __main__ - Step 140 Global step 140 Train loss 1.08 on epoch=34
05/21/2022 11:10:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.94 on epoch=37
05/21/2022 11:10:23 - INFO - __main__ - Global step 150 Train loss 1.05 Classification-F1 0.17368421052631577 on epoch=37
05/21/2022 11:10:23 - INFO - __main__ - Saving model with best Classification-F1: 0.0810126582278481 -> 0.17368421052631577 on epoch=37, global_step=150
05/21/2022 11:10:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.95 on epoch=39
05/21/2022 11:10:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.98 on epoch=42
05/21/2022 11:10:28 - INFO - __main__ - Step 180 Global step 180 Train loss 1.05 on epoch=44
05/21/2022 11:10:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.90 on epoch=47
05/21/2022 11:10:31 - INFO - __main__ - Step 200 Global step 200 Train loss 1.04 on epoch=49
05/21/2022 11:10:31 - INFO - __main__ - Global step 200 Train loss 0.98 Classification-F1 0.13067758749069247 on epoch=49
05/21/2022 11:10:33 - INFO - __main__ - Step 210 Global step 210 Train loss 1.03 on epoch=52
05/21/2022 11:10:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.94 on epoch=54
05/21/2022 11:10:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.86 on epoch=57
05/21/2022 11:10:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.90 on epoch=59
05/21/2022 11:10:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.98 on epoch=62
05/21/2022 11:10:40 - INFO - __main__ - Global step 250 Train loss 0.94 Classification-F1 0.20526315789473687 on epoch=62
05/21/2022 11:10:40 - INFO - __main__ - Saving model with best Classification-F1: 0.17368421052631577 -> 0.20526315789473687 on epoch=62, global_step=250
05/21/2022 11:10:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.91 on epoch=64
05/21/2022 11:10:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.87 on epoch=67
05/21/2022 11:10:44 - INFO - __main__ - Step 280 Global step 280 Train loss 1.04 on epoch=69
05/21/2022 11:10:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.87 on epoch=72
05/21/2022 11:10:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.87 on epoch=74
05/21/2022 11:10:47 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.10126582278481013 on epoch=74
05/21/2022 11:10:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.95 on epoch=77
05/21/2022 11:10:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.95 on epoch=79
05/21/2022 11:10:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.85 on epoch=82
05/21/2022 11:10:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.90 on epoch=84
05/21/2022 11:10:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.97 on epoch=87
05/21/2022 11:10:54 - INFO - __main__ - Global step 350 Train loss 0.93 Classification-F1 0.1770048309178744 on epoch=87
05/21/2022 11:10:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.90 on epoch=89
05/21/2022 11:10:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.91 on epoch=92
05/21/2022 11:10:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.90 on epoch=94
05/21/2022 11:11:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.87 on epoch=97
05/21/2022 11:11:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.93 on epoch=99
05/21/2022 11:11:01 - INFO - __main__ - Global step 400 Train loss 0.90 Classification-F1 0.10126582278481013 on epoch=99
05/21/2022 11:11:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.90 on epoch=102
05/21/2022 11:11:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.94 on epoch=104
05/21/2022 11:11:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.97 on epoch=107
05/21/2022 11:11:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.84 on epoch=109
05/21/2022 11:11:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.89 on epoch=112
05/21/2022 11:11:09 - INFO - __main__ - Global step 450 Train loss 0.91 Classification-F1 0.1609907120743034 on epoch=112
05/21/2022 11:11:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.90 on epoch=114
05/21/2022 11:11:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.90 on epoch=117
05/21/2022 11:11:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.84 on epoch=119
05/21/2022 11:11:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.86 on epoch=122
05/21/2022 11:11:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.90 on epoch=124
05/21/2022 11:11:16 - INFO - __main__ - Global step 500 Train loss 0.88 Classification-F1 0.10126582278481013 on epoch=124
05/21/2022 11:11:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.86 on epoch=127
05/21/2022 11:11:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.87 on epoch=129
05/21/2022 11:11:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.79 on epoch=132
05/21/2022 11:11:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.89 on epoch=134
05/21/2022 11:11:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.88 on epoch=137
05/21/2022 11:11:24 - INFO - __main__ - Global step 550 Train loss 0.86 Classification-F1 0.13067758749069247 on epoch=137
05/21/2022 11:11:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.89 on epoch=139
05/21/2022 11:11:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.77 on epoch=142
05/21/2022 11:11:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.92 on epoch=144
05/21/2022 11:11:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.82 on epoch=147
05/21/2022 11:11:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.77 on epoch=149
05/21/2022 11:11:31 - INFO - __main__ - Global step 600 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=149
05/21/2022 11:11:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.85 on epoch=152
05/21/2022 11:11:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.84 on epoch=154
05/21/2022 11:11:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.95 on epoch=157
05/21/2022 11:11:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.83 on epoch=159
05/21/2022 11:11:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.82 on epoch=162
05/21/2022 11:11:39 - INFO - __main__ - Global step 650 Train loss 0.86 Classification-F1 0.10256410256410256 on epoch=162
05/21/2022 11:11:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.83 on epoch=164
05/21/2022 11:11:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.92 on epoch=167
05/21/2022 11:11:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.83 on epoch=169
05/21/2022 11:11:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.86 on epoch=172
05/21/2022 11:11:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.84 on epoch=174
05/21/2022 11:11:47 - INFO - __main__ - Global step 700 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=174
05/21/2022 11:11:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.84 on epoch=177
05/21/2022 11:11:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.81 on epoch=179
05/21/2022 11:11:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.85 on epoch=182
05/21/2022 11:11:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.83 on epoch=184
05/21/2022 11:11:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.87 on epoch=187
05/21/2022 11:11:55 - INFO - __main__ - Global step 750 Train loss 0.84 Classification-F1 0.10389610389610389 on epoch=187
05/21/2022 11:11:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.84 on epoch=189
05/21/2022 11:11:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.86 on epoch=192
05/21/2022 11:11:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.78 on epoch=194
05/21/2022 11:12:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.73 on epoch=197
05/21/2022 11:12:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.81 on epoch=199
05/21/2022 11:12:02 - INFO - __main__ - Global step 800 Train loss 0.80 Classification-F1 0.1346749226006192 on epoch=199
05/21/2022 11:12:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.88 on epoch=202
05/21/2022 11:12:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.93 on epoch=204
05/21/2022 11:12:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=207
05/21/2022 11:12:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.77 on epoch=209
05/21/2022 11:12:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.85 on epoch=212
05/21/2022 11:12:09 - INFO - __main__ - Global step 850 Train loss 0.85 Classification-F1 0.15789473684210525 on epoch=212
05/21/2022 11:12:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.85 on epoch=214
05/21/2022 11:12:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.83 on epoch=217
05/21/2022 11:12:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.83 on epoch=219
05/21/2022 11:12:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.87 on epoch=222
05/21/2022 11:12:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.78 on epoch=224
05/21/2022 11:12:16 - INFO - __main__ - Global step 900 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=224
05/21/2022 11:12:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.83 on epoch=227
05/21/2022 11:12:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.84 on epoch=229
05/21/2022 11:12:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.88 on epoch=232
05/21/2022 11:12:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.86 on epoch=234
05/21/2022 11:12:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.82 on epoch=237
05/21/2022 11:12:23 - INFO - __main__ - Global step 950 Train loss 0.84 Classification-F1 0.10526315789473685 on epoch=237
05/21/2022 11:12:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.83 on epoch=239
05/21/2022 11:12:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.82 on epoch=242
05/21/2022 11:12:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.79 on epoch=244
05/21/2022 11:12:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.89 on epoch=247
05/21/2022 11:12:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.81 on epoch=249
05/21/2022 11:12:31 - INFO - __main__ - Global step 1000 Train loss 0.83 Classification-F1 0.10256410256410256 on epoch=249
05/21/2022 11:12:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.81 on epoch=252
05/21/2022 11:12:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.84 on epoch=254
05/21/2022 11:12:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.81 on epoch=257
05/21/2022 11:12:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.82 on epoch=259
05/21/2022 11:12:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.82 on epoch=262
05/21/2022 11:12:38 - INFO - __main__ - Global step 1050 Train loss 0.82 Classification-F1 0.15945165945165946 on epoch=262
05/21/2022 11:12:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.77 on epoch=264
05/21/2022 11:12:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.80 on epoch=267
05/21/2022 11:12:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.77 on epoch=269
05/21/2022 11:12:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.77 on epoch=272
05/21/2022 11:12:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.78 on epoch=274
05/21/2022 11:12:47 - INFO - __main__ - Global step 1100 Train loss 0.78 Classification-F1 0.15555555555555556 on epoch=274
05/21/2022 11:12:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.80 on epoch=277
05/21/2022 11:12:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.80 on epoch=279
05/21/2022 11:12:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.81 on epoch=282
05/21/2022 11:12:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.80 on epoch=284
05/21/2022 11:12:53 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.87 on epoch=287
05/21/2022 11:12:54 - INFO - __main__ - Global step 1150 Train loss 0.81 Classification-F1 0.13166666666666668 on epoch=287
05/21/2022 11:12:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.83 on epoch=289
05/21/2022 11:12:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.83 on epoch=292
05/21/2022 11:12:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.88 on epoch=294
05/21/2022 11:12:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.80 on epoch=297
05/21/2022 11:13:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.76 on epoch=299
05/21/2022 11:13:01 - INFO - __main__ - Global step 1200 Train loss 0.82 Classification-F1 0.13197586726998492 on epoch=299
05/21/2022 11:13:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.83 on epoch=302
05/21/2022 11:13:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.80 on epoch=304
05/21/2022 11:13:05 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.81 on epoch=307
05/21/2022 11:13:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.83 on epoch=309
05/21/2022 11:13:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.75 on epoch=312
05/21/2022 11:13:08 - INFO - __main__ - Global step 1250 Train loss 0.80 Classification-F1 0.13167388167388167 on epoch=312
05/21/2022 11:13:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.81 on epoch=314
05/21/2022 11:13:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.75 on epoch=317
05/21/2022 11:13:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.85 on epoch=319
05/21/2022 11:13:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.74 on epoch=322
05/21/2022 11:13:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.81 on epoch=324
05/21/2022 11:13:16 - INFO - __main__ - Global step 1300 Train loss 0.79 Classification-F1 0.13330786860198623 on epoch=324
05/21/2022 11:13:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.85 on epoch=327
05/21/2022 11:13:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.85 on epoch=329
05/21/2022 11:13:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.76 on epoch=332
05/21/2022 11:13:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.77 on epoch=334
05/21/2022 11:13:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.84 on epoch=337
05/21/2022 11:13:22 - INFO - __main__ - Global step 1350 Train loss 0.81 Classification-F1 0.20334620334620335 on epoch=337
05/21/2022 11:13:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.78 on epoch=339
05/21/2022 11:13:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.83 on epoch=342
05/21/2022 11:13:26 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.88 on epoch=344
05/21/2022 11:13:28 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.80 on epoch=347
05/21/2022 11:13:29 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.83 on epoch=349
05/21/2022 11:13:30 - INFO - __main__ - Global step 1400 Train loss 0.83 Classification-F1 0.15945165945165946 on epoch=349
05/21/2022 11:13:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.88 on epoch=352
05/21/2022 11:13:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.75 on epoch=354
05/21/2022 11:13:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.86 on epoch=357
05/21/2022 11:13:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.76 on epoch=359
05/21/2022 11:13:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.80 on epoch=362
05/21/2022 11:13:38 - INFO - __main__ - Global step 1450 Train loss 0.81 Classification-F1 0.1965894465894466 on epoch=362
05/21/2022 11:13:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.83 on epoch=364
05/21/2022 11:13:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.78 on epoch=367
05/21/2022 11:13:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.86 on epoch=369
05/21/2022 11:13:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.84 on epoch=372
05/21/2022 11:13:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.83 on epoch=374
05/21/2022 11:13:45 - INFO - __main__ - Global step 1500 Train loss 0.83 Classification-F1 0.15945165945165946 on epoch=374
05/21/2022 11:13:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.79 on epoch=377
05/21/2022 11:13:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.84 on epoch=379
05/21/2022 11:13:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.85 on epoch=382
05/21/2022 11:13:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.86 on epoch=384
05/21/2022 11:13:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.79 on epoch=387
05/21/2022 11:13:53 - INFO - __main__ - Global step 1550 Train loss 0.82 Classification-F1 0.13607843137254902 on epoch=387
05/21/2022 11:13:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.84 on epoch=389
05/21/2022 11:13:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.78 on epoch=392
05/21/2022 11:13:58 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.87 on epoch=394
05/21/2022 11:13:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.80 on epoch=397
05/21/2022 11:14:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.83 on epoch=399
05/21/2022 11:14:01 - INFO - __main__ - Global step 1600 Train loss 0.82 Classification-F1 0.16081871345029242 on epoch=399
05/21/2022 11:14:03 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.85 on epoch=402
05/21/2022 11:14:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.79 on epoch=404
05/21/2022 11:14:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.77 on epoch=407
05/21/2022 11:14:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.83 on epoch=409
05/21/2022 11:14:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.83 on epoch=412
05/21/2022 11:14:09 - INFO - __main__ - Global step 1650 Train loss 0.81 Classification-F1 0.17687437757860294 on epoch=412
05/21/2022 11:14:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.88 on epoch=414
05/21/2022 11:14:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.84 on epoch=417
05/21/2022 11:14:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.78 on epoch=419
05/21/2022 11:14:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.80 on epoch=422
05/21/2022 11:14:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.85 on epoch=424
05/21/2022 11:14:17 - INFO - __main__ - Global step 1700 Train loss 0.83 Classification-F1 0.2013484415263511 on epoch=424
05/21/2022 11:14:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.81 on epoch=427
05/21/2022 11:14:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.79 on epoch=429
05/21/2022 11:14:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.88 on epoch=432
05/21/2022 11:14:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.81 on epoch=434
05/21/2022 11:14:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.78 on epoch=437
05/21/2022 11:14:25 - INFO - __main__ - Global step 1750 Train loss 0.81 Classification-F1 0.2238749046529367 on epoch=437
05/21/2022 11:14:25 - INFO - __main__ - Saving model with best Classification-F1: 0.20526315789473687 -> 0.2238749046529367 on epoch=437, global_step=1750
05/21/2022 11:14:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.80 on epoch=439
05/21/2022 11:14:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.77 on epoch=442
05/21/2022 11:14:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.84 on epoch=444
05/21/2022 11:14:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.77 on epoch=447
05/21/2022 11:14:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.81 on epoch=449
05/21/2022 11:14:33 - INFO - __main__ - Global step 1800 Train loss 0.80 Classification-F1 0.20666666666666667 on epoch=449
05/21/2022 11:14:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.76 on epoch=452
05/21/2022 11:14:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.87 on epoch=454
05/21/2022 11:14:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.80 on epoch=457
05/21/2022 11:14:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.79 on epoch=459
05/21/2022 11:14:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.76 on epoch=462
05/21/2022 11:14:41 - INFO - __main__ - Global step 1850 Train loss 0.79 Classification-F1 0.1842105263157895 on epoch=462
05/21/2022 11:14:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.85 on epoch=464
05/21/2022 11:14:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.77 on epoch=467
05/21/2022 11:14:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.84 on epoch=469
05/21/2022 11:14:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.77 on epoch=472
05/21/2022 11:14:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.81 on epoch=474
05/21/2022 11:14:48 - INFO - __main__ - Global step 1900 Train loss 0.81 Classification-F1 0.1765873015873016 on epoch=474
05/21/2022 11:14:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.84 on epoch=477
05/21/2022 11:14:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.80 on epoch=479
05/21/2022 11:14:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.78 on epoch=482
05/21/2022 11:14:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.81 on epoch=484
05/21/2022 11:14:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.80 on epoch=487
05/21/2022 11:14:56 - INFO - __main__ - Global step 1950 Train loss 0.81 Classification-F1 0.1965894465894466 on epoch=487
05/21/2022 11:14:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.77 on epoch=489
05/21/2022 11:14:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.80 on epoch=492
05/21/2022 11:15:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.79 on epoch=494
05/21/2022 11:15:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.82 on epoch=497
05/21/2022 11:15:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.80 on epoch=499
05/21/2022 11:15:04 - INFO - __main__ - Global step 2000 Train loss 0.80 Classification-F1 0.22353801169590645 on epoch=499
05/21/2022 11:15:06 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.79 on epoch=502
05/21/2022 11:15:07 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.79 on epoch=504
05/21/2022 11:15:08 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.84 on epoch=507
05/21/2022 11:15:10 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.82 on epoch=509
05/21/2022 11:15:11 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.83 on epoch=512
05/21/2022 11:15:11 - INFO - __main__ - Global step 2050 Train loss 0.82 Classification-F1 0.21727841798264336 on epoch=512
05/21/2022 11:15:13 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.86 on epoch=514
05/21/2022 11:15:14 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.85 on epoch=517
05/21/2022 11:15:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.80 on epoch=519
05/21/2022 11:15:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.80 on epoch=522
05/21/2022 11:15:18 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.76 on epoch=524
05/21/2022 11:15:19 - INFO - __main__ - Global step 2100 Train loss 0.81 Classification-F1 0.18679950186799504 on epoch=524
05/21/2022 11:15:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.78 on epoch=527
05/21/2022 11:15:22 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.70 on epoch=529
05/21/2022 11:15:23 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.84 on epoch=532
05/21/2022 11:15:25 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.76 on epoch=534
05/21/2022 11:15:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.77 on epoch=537
05/21/2022 11:15:27 - INFO - __main__ - Global step 2150 Train loss 0.77 Classification-F1 0.24463109354413703 on epoch=537
05/21/2022 11:15:27 - INFO - __main__ - Saving model with best Classification-F1: 0.2238749046529367 -> 0.24463109354413703 on epoch=537, global_step=2150
05/21/2022 11:15:29 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.78 on epoch=539
05/21/2022 11:15:30 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.82 on epoch=542
05/21/2022 11:15:32 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.80 on epoch=544
05/21/2022 11:15:33 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.72 on epoch=547
05/21/2022 11:15:35 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.83 on epoch=549
05/21/2022 11:15:36 - INFO - __main__ - Global step 2200 Train loss 0.79 Classification-F1 0.1526315789473684 on epoch=549
05/21/2022 11:15:37 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.76 on epoch=552
05/21/2022 11:15:39 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.79 on epoch=554
05/21/2022 11:15:40 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.81 on epoch=557
05/21/2022 11:15:41 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.84 on epoch=559
05/21/2022 11:15:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.75 on epoch=562
05/21/2022 11:15:43 - INFO - __main__ - Global step 2250 Train loss 0.79 Classification-F1 0.26281094527363186 on epoch=562
05/21/2022 11:15:43 - INFO - __main__ - Saving model with best Classification-F1: 0.24463109354413703 -> 0.26281094527363186 on epoch=562, global_step=2250
05/21/2022 11:15:45 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.75 on epoch=564
05/21/2022 11:15:46 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.79 on epoch=567
05/21/2022 11:15:48 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.84 on epoch=569
05/21/2022 11:15:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.83 on epoch=572
05/21/2022 11:15:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.77 on epoch=574
05/21/2022 11:15:51 - INFO - __main__ - Global step 2300 Train loss 0.79 Classification-F1 0.236956244418931 on epoch=574
05/21/2022 11:15:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.82 on epoch=577
05/21/2022 11:15:54 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.84 on epoch=579
05/21/2022 11:15:56 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.76 on epoch=582
05/21/2022 11:15:57 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.79 on epoch=584
05/21/2022 11:15:58 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.82 on epoch=587
05/21/2022 11:15:59 - INFO - __main__ - Global step 2350 Train loss 0.80 Classification-F1 0.24112357467321777 on epoch=587
05/21/2022 11:16:00 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.75 on epoch=589
05/21/2022 11:16:01 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.80 on epoch=592
05/21/2022 11:16:02 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.75 on epoch=594
05/21/2022 11:16:04 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.81 on epoch=597
05/21/2022 11:16:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.76 on epoch=599
05/21/2022 11:16:06 - INFO - __main__ - Global step 2400 Train loss 0.78 Classification-F1 0.16865079365079366 on epoch=599
05/21/2022 11:16:07 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.77 on epoch=602
05/21/2022 11:16:09 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.78 on epoch=604
05/21/2022 11:16:10 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.73 on epoch=607
05/21/2022 11:16:12 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.81 on epoch=609
05/21/2022 11:16:13 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.86 on epoch=612
05/21/2022 11:16:13 - INFO - __main__ - Global step 2450 Train loss 0.79 Classification-F1 0.18813131313131315 on epoch=612
05/21/2022 11:16:15 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.80 on epoch=614
05/21/2022 11:16:16 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.76 on epoch=617
05/21/2022 11:16:17 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.86 on epoch=619
05/21/2022 11:16:19 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.75 on epoch=622
05/21/2022 11:16:20 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.78 on epoch=624
05/21/2022 11:16:21 - INFO - __main__ - Global step 2500 Train loss 0.79 Classification-F1 0.21722488038277513 on epoch=624
05/21/2022 11:16:22 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.77 on epoch=627
05/21/2022 11:16:23 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.87 on epoch=629
05/21/2022 11:16:24 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.81 on epoch=632
05/21/2022 11:16:26 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.77 on epoch=634
05/21/2022 11:16:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.77 on epoch=637
05/21/2022 11:16:27 - INFO - __main__ - Global step 2550 Train loss 0.80 Classification-F1 0.25533723522853957 on epoch=637
05/21/2022 11:16:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.77 on epoch=639
05/21/2022 11:16:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.79 on epoch=642
05/21/2022 11:16:32 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.81 on epoch=644
05/21/2022 11:16:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.76 on epoch=647
05/21/2022 11:16:35 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.80 on epoch=649
05/21/2022 11:16:35 - INFO - __main__ - Global step 2600 Train loss 0.78 Classification-F1 0.25422705314009664 on epoch=649
05/21/2022 11:16:37 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.84 on epoch=652
05/21/2022 11:16:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.82 on epoch=654
05/21/2022 11:16:40 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.76 on epoch=657
05/21/2022 11:16:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.87 on epoch=659
05/21/2022 11:16:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.77 on epoch=662
05/21/2022 11:16:42 - INFO - __main__ - Global step 2650 Train loss 0.81 Classification-F1 0.2995923913043478 on epoch=662
05/21/2022 11:16:42 - INFO - __main__ - Saving model with best Classification-F1: 0.26281094527363186 -> 0.2995923913043478 on epoch=662, global_step=2650
05/21/2022 11:16:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.79 on epoch=664
05/21/2022 11:16:45 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.81 on epoch=667
05/21/2022 11:16:47 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.73 on epoch=669
05/21/2022 11:16:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.74 on epoch=672
05/21/2022 11:16:49 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.77 on epoch=674
05/21/2022 11:16:50 - INFO - __main__ - Global step 2700 Train loss 0.77 Classification-F1 0.2816053511705685 on epoch=674
05/21/2022 11:16:51 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.79 on epoch=677
05/21/2022 11:16:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.73 on epoch=679
05/21/2022 11:16:54 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.78 on epoch=682
05/21/2022 11:16:55 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.82 on epoch=684
05/21/2022 11:16:56 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.78 on epoch=687
05/21/2022 11:16:57 - INFO - __main__ - Global step 2750 Train loss 0.78 Classification-F1 0.4520489188231124 on epoch=687
05/21/2022 11:16:57 - INFO - __main__ - Saving model with best Classification-F1: 0.2995923913043478 -> 0.4520489188231124 on epoch=687, global_step=2750
05/21/2022 11:16:58 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.87 on epoch=689
05/21/2022 11:17:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.80 on epoch=692
05/21/2022 11:17:01 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.84 on epoch=694
05/21/2022 11:17:03 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.82 on epoch=697
05/21/2022 11:17:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.73 on epoch=699
05/21/2022 11:17:05 - INFO - __main__ - Global step 2800 Train loss 0.81 Classification-F1 0.2621870882740448 on epoch=699
05/21/2022 11:17:06 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.78 on epoch=702
05/21/2022 11:17:08 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.85 on epoch=704
05/21/2022 11:17:09 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.76 on epoch=707
05/21/2022 11:17:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.79 on epoch=709
05/21/2022 11:17:12 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.78 on epoch=712
05/21/2022 11:17:13 - INFO - __main__ - Global step 2850 Train loss 0.79 Classification-F1 0.23912087912087915 on epoch=712
05/21/2022 11:17:14 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.82 on epoch=714
05/21/2022 11:17:15 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.80 on epoch=717
05/21/2022 11:17:16 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.81 on epoch=719
05/21/2022 11:17:18 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.86 on epoch=722
05/21/2022 11:17:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.80 on epoch=724
05/21/2022 11:17:20 - INFO - __main__ - Global step 2900 Train loss 0.82 Classification-F1 0.24662804515745693 on epoch=724
05/21/2022 11:17:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.81 on epoch=727
05/21/2022 11:17:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.73 on epoch=729
05/21/2022 11:17:24 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.81 on epoch=732
05/21/2022 11:17:26 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.81 on epoch=734
05/21/2022 11:17:27 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.72 on epoch=737
05/21/2022 11:17:28 - INFO - __main__ - Global step 2950 Train loss 0.78 Classification-F1 0.2611013986013986 on epoch=737
05/21/2022 11:17:29 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.82 on epoch=739
05/21/2022 11:17:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.82 on epoch=742
05/21/2022 11:17:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.79 on epoch=744
05/21/2022 11:17:33 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.87 on epoch=747
05/21/2022 11:17:34 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.83 on epoch=749
05/21/2022 11:17:35 - INFO - __main__ - Global step 3000 Train loss 0.83 Classification-F1 0.3476504172156346 on epoch=749
05/21/2022 11:17:35 - INFO - __main__ - save last model!
05/21/2022 11:17:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 11:17:35 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 11:17:35 - INFO - __main__ - Printing 3 examples
05/21/2022 11:17:35 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 11:17:35 - INFO - __main__ - ['others']
05/21/2022 11:17:35 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 11:17:35 - INFO - __main__ - ['others']
05/21/2022 11:17:35 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 11:17:35 - INFO - __main__ - ['others']
05/21/2022 11:17:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:17:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:17:35 - INFO - __main__ - Printing 3 examples
05/21/2022 11:17:35 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/21/2022 11:17:35 - INFO - __main__ - ['happy']
05/21/2022 11:17:35 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/21/2022 11:17:35 - INFO - __main__ - ['happy']
05/21/2022 11:17:35 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/21/2022 11:17:35 - INFO - __main__ - ['happy']
05/21/2022 11:17:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:17:35 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:17:35 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 11:17:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:17:35 - INFO - __main__ - Printing 3 examples
05/21/2022 11:17:35 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/21/2022 11:17:35 - INFO - __main__ - ['happy']
05/21/2022 11:17:35 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/21/2022 11:17:35 - INFO - __main__ - ['happy']
05/21/2022 11:17:35 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/21/2022 11:17:35 - INFO - __main__ - ['happy']
05/21/2022 11:17:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:17:35 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:17:35 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 11:17:37 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:17:42 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 11:17:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 11:17:42 - INFO - __main__ - Starting training!
05/21/2022 11:17:43 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 11:18:26 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_42_0.4_8_predictions.txt
05/21/2022 11:18:26 - INFO - __main__ - Classification-F1 on test data: 0.2410
05/21/2022 11:18:26 - INFO - __main__ - prefix=emo_16_42, lr=0.4, bsz=8, dev_performance=0.4520489188231124, test_performance=0.24099693960340024
05/21/2022 11:18:26 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.3, bsz=8 ...
05/21/2022 11:18:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:18:27 - INFO - __main__ - Printing 3 examples
05/21/2022 11:18:27 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/21/2022 11:18:27 - INFO - __main__ - ['happy']
05/21/2022 11:18:27 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/21/2022 11:18:27 - INFO - __main__ - ['happy']
05/21/2022 11:18:27 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/21/2022 11:18:27 - INFO - __main__ - ['happy']
05/21/2022 11:18:27 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:18:27 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:18:27 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 11:18:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:18:27 - INFO - __main__ - Printing 3 examples
05/21/2022 11:18:27 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/21/2022 11:18:27 - INFO - __main__ - ['happy']
05/21/2022 11:18:27 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/21/2022 11:18:27 - INFO - __main__ - ['happy']
05/21/2022 11:18:27 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/21/2022 11:18:27 - INFO - __main__ - ['happy']
05/21/2022 11:18:27 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:18:27 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:18:27 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 11:18:33 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 11:18:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 11:18:33 - INFO - __main__ - Starting training!
05/21/2022 11:18:34 - INFO - __main__ - Step 10 Global step 10 Train loss 4.24 on epoch=2
05/21/2022 11:18:35 - INFO - __main__ - Step 20 Global step 20 Train loss 3.90 on epoch=4
05/21/2022 11:18:37 - INFO - __main__ - Step 30 Global step 30 Train loss 3.35 on epoch=7
05/21/2022 11:18:39 - INFO - __main__ - Step 40 Global step 40 Train loss 3.04 on epoch=9
05/21/2022 11:18:40 - INFO - __main__ - Step 50 Global step 50 Train loss 2.67 on epoch=12
05/21/2022 11:18:41 - INFO - __main__ - Global step 50 Train loss 3.44 Classification-F1 0.0810126582278481 on epoch=12
05/21/2022 11:18:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0810126582278481 on epoch=12, global_step=50
05/21/2022 11:18:42 - INFO - __main__ - Step 60 Global step 60 Train loss 2.44 on epoch=14
05/21/2022 11:18:44 - INFO - __main__ - Step 70 Global step 70 Train loss 2.04 on epoch=17
05/21/2022 11:18:45 - INFO - __main__ - Step 80 Global step 80 Train loss 1.86 on epoch=19
05/21/2022 11:18:47 - INFO - __main__ - Step 90 Global step 90 Train loss 1.63 on epoch=22
05/21/2022 11:18:48 - INFO - __main__ - Step 100 Global step 100 Train loss 1.59 on epoch=24
05/21/2022 11:18:49 - INFO - __main__ - Global step 100 Train loss 1.91 Classification-F1 0.10664629488158898 on epoch=24
05/21/2022 11:18:49 - INFO - __main__ - Saving model with best Classification-F1: 0.0810126582278481 -> 0.10664629488158898 on epoch=24, global_step=100
05/21/2022 11:18:50 - INFO - __main__ - Step 110 Global step 110 Train loss 1.45 on epoch=27
05/21/2022 11:18:51 - INFO - __main__ - Step 120 Global step 120 Train loss 1.38 on epoch=29
05/21/2022 11:18:53 - INFO - __main__ - Step 130 Global step 130 Train loss 1.21 on epoch=32
05/21/2022 11:18:54 - INFO - __main__ - Step 140 Global step 140 Train loss 1.15 on epoch=34
05/21/2022 11:18:55 - INFO - __main__ - Step 150 Global step 150 Train loss 1.17 on epoch=37
05/21/2022 11:18:56 - INFO - __main__ - Global step 150 Train loss 1.27 Classification-F1 0.12631578947368421 on epoch=37
05/21/2022 11:18:56 - INFO - __main__ - Saving model with best Classification-F1: 0.10664629488158898 -> 0.12631578947368421 on epoch=37, global_step=150
05/21/2022 11:18:57 - INFO - __main__ - Step 160 Global step 160 Train loss 1.07 on epoch=39
05/21/2022 11:18:59 - INFO - __main__ - Step 170 Global step 170 Train loss 1.04 on epoch=42
05/21/2022 11:19:00 - INFO - __main__ - Step 180 Global step 180 Train loss 1.04 on epoch=44
05/21/2022 11:19:02 - INFO - __main__ - Step 190 Global step 190 Train loss 1.03 on epoch=47
05/21/2022 11:19:03 - INFO - __main__ - Step 200 Global step 200 Train loss 1.00 on epoch=49
05/21/2022 11:19:04 - INFO - __main__ - Global step 200 Train loss 1.04 Classification-F1 0.15022222222222223 on epoch=49
05/21/2022 11:19:04 - INFO - __main__ - Saving model with best Classification-F1: 0.12631578947368421 -> 0.15022222222222223 on epoch=49, global_step=200
05/21/2022 11:19:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.97 on epoch=52
05/21/2022 11:19:07 - INFO - __main__ - Step 220 Global step 220 Train loss 1.06 on epoch=54
05/21/2022 11:19:08 - INFO - __main__ - Step 230 Global step 230 Train loss 1.12 on epoch=57
05/21/2022 11:19:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.94 on epoch=59
05/21/2022 11:19:11 - INFO - __main__ - Step 250 Global step 250 Train loss 1.02 on epoch=62
05/21/2022 11:19:12 - INFO - __main__ - Global step 250 Train loss 1.02 Classification-F1 0.10664629488158898 on epoch=62
05/21/2022 11:19:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.93 on epoch=64
05/21/2022 11:19:15 - INFO - __main__ - Step 270 Global step 270 Train loss 1.00 on epoch=67
05/21/2022 11:19:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.98 on epoch=69
05/21/2022 11:19:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.94 on epoch=72
05/21/2022 11:19:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.96 on epoch=74
05/21/2022 11:19:19 - INFO - __main__ - Global step 300 Train loss 0.96 Classification-F1 0.10389610389610389 on epoch=74
05/21/2022 11:19:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.98 on epoch=77
05/21/2022 11:19:22 - INFO - __main__ - Step 320 Global step 320 Train loss 1.03 on epoch=79
05/21/2022 11:19:23 - INFO - __main__ - Step 330 Global step 330 Train loss 1.05 on epoch=82
05/21/2022 11:19:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.91 on epoch=84
05/21/2022 11:19:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.88 on epoch=87
05/21/2022 11:19:27 - INFO - __main__ - Global step 350 Train loss 0.97 Classification-F1 0.10126582278481013 on epoch=87
05/21/2022 11:19:28 - INFO - __main__ - Step 360 Global step 360 Train loss 1.05 on epoch=89
05/21/2022 11:19:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.95 on epoch=92
05/21/2022 11:19:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.90 on epoch=94
05/21/2022 11:19:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.91 on epoch=97
05/21/2022 11:19:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.90 on epoch=99
05/21/2022 11:19:34 - INFO - __main__ - Global step 400 Train loss 0.94 Classification-F1 0.10126582278481013 on epoch=99
05/21/2022 11:19:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.81 on epoch=102
05/21/2022 11:19:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.78 on epoch=104
05/21/2022 11:19:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.97 on epoch=107
05/21/2022 11:19:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.87 on epoch=109
05/21/2022 11:19:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.96 on epoch=112
05/21/2022 11:19:42 - INFO - __main__ - Global step 450 Train loss 0.88 Classification-F1 0.15945165945165946 on epoch=112
05/21/2022 11:19:42 - INFO - __main__ - Saving model with best Classification-F1: 0.15022222222222223 -> 0.15945165945165946 on epoch=112, global_step=450
05/21/2022 11:19:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.96 on epoch=114
05/21/2022 11:19:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.87 on epoch=117
05/21/2022 11:19:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.86 on epoch=119
05/21/2022 11:19:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.91 on epoch=122
05/21/2022 11:19:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.92 on epoch=124
05/21/2022 11:19:49 - INFO - __main__ - Global step 500 Train loss 0.90 Classification-F1 0.10126582278481013 on epoch=124
05/21/2022 11:19:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.94 on epoch=127
05/21/2022 11:19:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.95 on epoch=129
05/21/2022 11:19:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.81 on epoch=132
05/21/2022 11:19:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.87 on epoch=134
05/21/2022 11:19:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.83 on epoch=137
05/21/2022 11:19:57 - INFO - __main__ - Global step 550 Train loss 0.88 Classification-F1 0.10126582278481013 on epoch=137
05/21/2022 11:19:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.95 on epoch=139
05/21/2022 11:20:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.88 on epoch=142
05/21/2022 11:20:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.94 on epoch=144
05/21/2022 11:20:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.97 on epoch=147
05/21/2022 11:20:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.89 on epoch=149
05/21/2022 11:20:05 - INFO - __main__ - Global step 600 Train loss 0.92 Classification-F1 0.10126582278481013 on epoch=149
05/21/2022 11:20:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.85 on epoch=152
05/21/2022 11:20:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.97 on epoch=154
05/21/2022 11:20:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.86 on epoch=157
05/21/2022 11:20:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.90 on epoch=159
05/21/2022 11:20:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.84 on epoch=162
05/21/2022 11:20:13 - INFO - __main__ - Global step 650 Train loss 0.88 Classification-F1 0.10126582278481013 on epoch=162
05/21/2022 11:20:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.78 on epoch=164
05/21/2022 11:20:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.95 on epoch=167
05/21/2022 11:20:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.94 on epoch=169
05/21/2022 11:20:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.87 on epoch=172
05/21/2022 11:20:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.89 on epoch=174
05/21/2022 11:20:20 - INFO - __main__ - Global step 700 Train loss 0.89 Classification-F1 0.10126582278481013 on epoch=174
05/21/2022 11:20:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.84 on epoch=177
05/21/2022 11:20:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.87 on epoch=179
05/21/2022 11:20:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.88 on epoch=182
05/21/2022 11:20:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.94 on epoch=184
05/21/2022 11:20:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.86 on epoch=187
05/21/2022 11:20:27 - INFO - __main__ - Global step 750 Train loss 0.88 Classification-F1 0.10126582278481013 on epoch=187
05/21/2022 11:20:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.90 on epoch=189
05/21/2022 11:20:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.78 on epoch=192
05/21/2022 11:20:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.88 on epoch=194
05/21/2022 11:20:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.81 on epoch=197
05/21/2022 11:20:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.84 on epoch=199
05/21/2022 11:20:34 - INFO - __main__ - Global step 800 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=199
05/21/2022 11:20:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.91 on epoch=202
05/21/2022 11:20:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.84 on epoch=204
05/21/2022 11:20:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.98 on epoch=207
05/21/2022 11:20:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.88 on epoch=209
05/21/2022 11:20:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.86 on epoch=212
05/21/2022 11:20:42 - INFO - __main__ - Global step 850 Train loss 0.89 Classification-F1 0.13067758749069247 on epoch=212
05/21/2022 11:20:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.91 on epoch=214
05/21/2022 11:20:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.86 on epoch=217
05/21/2022 11:20:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.84 on epoch=219
05/21/2022 11:20:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.81 on epoch=222
05/21/2022 11:20:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.91 on epoch=224
05/21/2022 11:20:50 - INFO - __main__ - Global step 900 Train loss 0.86 Classification-F1 0.10126582278481013 on epoch=224
05/21/2022 11:20:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.82 on epoch=227
05/21/2022 11:20:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.92 on epoch=229
05/21/2022 11:20:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.82 on epoch=232
05/21/2022 11:20:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.86 on epoch=234
05/21/2022 11:20:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.82 on epoch=237
05/21/2022 11:20:57 - INFO - __main__ - Global step 950 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=237
05/21/2022 11:20:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.92 on epoch=239
05/21/2022 11:21:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.78 on epoch=242
05/21/2022 11:21:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.84 on epoch=244
05/21/2022 11:21:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.80 on epoch=247
05/21/2022 11:21:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.82 on epoch=249
05/21/2022 11:21:04 - INFO - __main__ - Global step 1000 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=249
05/21/2022 11:21:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.82 on epoch=252
05/21/2022 11:21:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.88 on epoch=254
05/21/2022 11:21:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.84 on epoch=257
05/21/2022 11:21:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.86 on epoch=259
05/21/2022 11:21:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.89 on epoch=262
05/21/2022 11:21:12 - INFO - __main__ - Global step 1050 Train loss 0.86 Classification-F1 0.10126582278481013 on epoch=262
05/21/2022 11:21:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.80 on epoch=264
05/21/2022 11:21:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.77 on epoch=267
05/21/2022 11:21:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.83 on epoch=269
05/21/2022 11:21:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.86 on epoch=272
05/21/2022 11:21:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.87 on epoch=274
05/21/2022 11:21:20 - INFO - __main__ - Global step 1100 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=274
05/21/2022 11:21:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.80 on epoch=277
05/21/2022 11:21:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.86 on epoch=279
05/21/2022 11:21:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.79 on epoch=282
05/21/2022 11:21:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.82 on epoch=284
05/21/2022 11:21:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.74 on epoch=287
05/21/2022 11:21:28 - INFO - __main__ - Global step 1150 Train loss 0.80 Classification-F1 0.13034188034188032 on epoch=287
05/21/2022 11:21:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.76 on epoch=289
05/21/2022 11:21:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.83 on epoch=292
05/21/2022 11:21:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.85 on epoch=294
05/21/2022 11:21:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.88 on epoch=297
05/21/2022 11:21:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.90 on epoch=299
05/21/2022 11:21:35 - INFO - __main__ - Global step 1200 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=299
05/21/2022 11:21:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.79 on epoch=302
05/21/2022 11:21:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.83 on epoch=304
05/21/2022 11:21:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.83 on epoch=307
05/21/2022 11:21:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.81 on epoch=309
05/21/2022 11:21:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.89 on epoch=312
05/21/2022 11:21:42 - INFO - __main__ - Global step 1250 Train loss 0.83 Classification-F1 0.13067758749069247 on epoch=312
05/21/2022 11:21:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.87 on epoch=314
05/21/2022 11:21:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.78 on epoch=317
05/21/2022 11:21:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.88 on epoch=319
05/21/2022 11:21:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.83 on epoch=322
05/21/2022 11:21:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.80 on epoch=324
05/21/2022 11:21:50 - INFO - __main__ - Global step 1300 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=324
05/21/2022 11:21:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.84 on epoch=327
05/21/2022 11:21:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.86 on epoch=329
05/21/2022 11:21:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.88 on epoch=332
05/21/2022 11:21:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.80 on epoch=334
05/21/2022 11:21:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.75 on epoch=337
05/21/2022 11:21:58 - INFO - __main__ - Global step 1350 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=337
05/21/2022 11:21:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.88 on epoch=339
05/21/2022 11:22:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.82 on epoch=342
05/21/2022 11:22:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.82 on epoch=344
05/21/2022 11:22:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.79 on epoch=347
05/21/2022 11:22:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.83 on epoch=349
05/21/2022 11:22:05 - INFO - __main__ - Global step 1400 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=349
05/21/2022 11:22:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.77 on epoch=352
05/21/2022 11:22:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.84 on epoch=354
05/21/2022 11:22:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.79 on epoch=357
05/21/2022 11:22:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.75 on epoch=359
05/21/2022 11:22:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.82 on epoch=362
05/21/2022 11:22:12 - INFO - __main__ - Global step 1450 Train loss 0.79 Classification-F1 0.10126582278481013 on epoch=362
05/21/2022 11:22:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.77 on epoch=364
05/21/2022 11:22:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.86 on epoch=367
05/21/2022 11:22:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.84 on epoch=369
05/21/2022 11:22:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.77 on epoch=372
05/21/2022 11:22:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.90 on epoch=374
05/21/2022 11:22:20 - INFO - __main__ - Global step 1500 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=374
05/21/2022 11:22:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.91 on epoch=377
05/21/2022 11:22:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.92 on epoch=379
05/21/2022 11:22:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.78 on epoch=382
05/21/2022 11:22:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.78 on epoch=384
05/21/2022 11:22:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.82 on epoch=387
05/21/2022 11:22:27 - INFO - __main__ - Global step 1550 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=387
05/21/2022 11:22:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.82 on epoch=389
05/21/2022 11:22:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.76 on epoch=392
05/21/2022 11:22:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.86 on epoch=394
05/21/2022 11:22:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.77 on epoch=397
05/21/2022 11:22:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.83 on epoch=399
05/21/2022 11:22:35 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.10126582278481013 on epoch=399
05/21/2022 11:22:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.88 on epoch=402
05/21/2022 11:22:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.87 on epoch=404
05/21/2022 11:22:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.80 on epoch=407
05/21/2022 11:22:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.76 on epoch=409
05/21/2022 11:22:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.85 on epoch=412
05/21/2022 11:22:42 - INFO - __main__ - Global step 1650 Train loss 0.83 Classification-F1 0.13067758749069247 on epoch=412
05/21/2022 11:22:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.82 on epoch=414
05/21/2022 11:22:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.91 on epoch=417
05/21/2022 11:22:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.81 on epoch=419
05/21/2022 11:22:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.82 on epoch=422
05/21/2022 11:22:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.84 on epoch=424
05/21/2022 11:22:50 - INFO - __main__ - Global step 1700 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=424
05/21/2022 11:22:52 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.86 on epoch=427
05/21/2022 11:22:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.86 on epoch=429
05/21/2022 11:22:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.75 on epoch=432
05/21/2022 11:22:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.80 on epoch=434
05/21/2022 11:22:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.82 on epoch=437
05/21/2022 11:22:58 - INFO - __main__ - Global step 1750 Train loss 0.82 Classification-F1 0.15 on epoch=437
05/21/2022 11:23:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.83 on epoch=439
05/21/2022 11:23:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.80 on epoch=442
05/21/2022 11:23:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.79 on epoch=444
05/21/2022 11:23:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.88 on epoch=447
05/21/2022 11:23:06 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.80 on epoch=449
05/21/2022 11:23:06 - INFO - __main__ - Global step 1800 Train loss 0.82 Classification-F1 0.10126582278481013 on epoch=449
05/21/2022 11:23:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.78 on epoch=452
05/21/2022 11:23:09 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.83 on epoch=454
05/21/2022 11:23:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.79 on epoch=457
05/21/2022 11:23:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.80 on epoch=459
05/21/2022 11:23:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.84 on epoch=462
05/21/2022 11:23:14 - INFO - __main__ - Global step 1850 Train loss 0.81 Classification-F1 0.13197586726998492 on epoch=462
05/21/2022 11:23:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.89 on epoch=464
05/21/2022 11:23:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.85 on epoch=467
05/21/2022 11:23:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.84 on epoch=469
05/21/2022 11:23:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.79 on epoch=472
05/21/2022 11:23:22 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.77 on epoch=474
05/21/2022 11:23:22 - INFO - __main__ - Global step 1900 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=474
05/21/2022 11:23:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.79 on epoch=477
05/21/2022 11:23:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.83 on epoch=479
05/21/2022 11:23:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.81 on epoch=482
05/21/2022 11:23:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.75 on epoch=484
05/21/2022 11:23:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.81 on epoch=487
05/21/2022 11:23:31 - INFO - __main__ - Global step 1950 Train loss 0.80 Classification-F1 0.1539829302987198 on epoch=487
05/21/2022 11:23:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.85 on epoch=489
05/21/2022 11:23:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.84 on epoch=492
05/21/2022 11:23:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.86 on epoch=494
05/21/2022 11:23:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.83 on epoch=497
05/21/2022 11:23:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.88 on epoch=499
05/21/2022 11:23:38 - INFO - __main__ - Global step 2000 Train loss 0.85 Classification-F1 0.10256410256410256 on epoch=499
05/21/2022 11:23:39 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.81 on epoch=502
05/21/2022 11:23:41 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.78 on epoch=504
05/21/2022 11:23:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.78 on epoch=507
05/21/2022 11:23:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.90 on epoch=509
05/21/2022 11:23:45 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.76 on epoch=512
05/21/2022 11:23:45 - INFO - __main__ - Global step 2050 Train loss 0.81 Classification-F1 0.13167388167388167 on epoch=512
05/21/2022 11:23:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.81 on epoch=514
05/21/2022 11:23:48 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.84 on epoch=517
05/21/2022 11:23:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.87 on epoch=519
05/21/2022 11:23:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.83 on epoch=522
05/21/2022 11:23:52 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.81 on epoch=524
05/21/2022 11:23:53 - INFO - __main__ - Global step 2100 Train loss 0.83 Classification-F1 0.13167388167388167 on epoch=524
05/21/2022 11:23:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.80 on epoch=527
05/21/2022 11:23:56 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.81 on epoch=529
05/21/2022 11:23:57 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.82 on epoch=532
05/21/2022 11:23:59 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.88 on epoch=534
05/21/2022 11:24:00 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.79 on epoch=537
05/21/2022 11:24:01 - INFO - __main__ - Global step 2150 Train loss 0.82 Classification-F1 0.13167388167388167 on epoch=537
05/21/2022 11:24:02 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.83 on epoch=539
05/21/2022 11:24:03 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.76 on epoch=542
05/21/2022 11:24:05 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.78 on epoch=544
05/21/2022 11:24:06 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.81 on epoch=547
05/21/2022 11:24:08 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.77 on epoch=549
05/21/2022 11:24:08 - INFO - __main__ - Global step 2200 Train loss 0.79 Classification-F1 0.10256410256410256 on epoch=549
05/21/2022 11:24:10 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.81 on epoch=552
05/21/2022 11:24:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.85 on epoch=554
05/21/2022 11:24:13 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.78 on epoch=557
05/21/2022 11:24:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.79 on epoch=559
05/21/2022 11:24:16 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.80 on epoch=562
05/21/2022 11:24:17 - INFO - __main__ - Global step 2250 Train loss 0.80 Classification-F1 0.18166666666666667 on epoch=562
05/21/2022 11:24:17 - INFO - __main__ - Saving model with best Classification-F1: 0.15945165945165946 -> 0.18166666666666667 on epoch=562, global_step=2250
05/21/2022 11:24:18 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.78 on epoch=564
05/21/2022 11:24:19 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.77 on epoch=567
05/21/2022 11:24:21 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.78 on epoch=569
05/21/2022 11:24:22 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.83 on epoch=572
05/21/2022 11:24:23 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.79 on epoch=574
05/21/2022 11:24:24 - INFO - __main__ - Global step 2300 Train loss 0.79 Classification-F1 0.10256410256410256 on epoch=574
05/21/2022 11:24:25 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.86 on epoch=577
05/21/2022 11:24:27 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.80 on epoch=579
05/21/2022 11:24:28 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.81 on epoch=582
05/21/2022 11:24:30 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.77 on epoch=584
05/21/2022 11:24:31 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.80 on epoch=587
05/21/2022 11:24:32 - INFO - __main__ - Global step 2350 Train loss 0.81 Classification-F1 0.20526315789473687 on epoch=587
05/21/2022 11:24:32 - INFO - __main__ - Saving model with best Classification-F1: 0.18166666666666667 -> 0.20526315789473687 on epoch=587, global_step=2350
05/21/2022 11:24:33 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.77 on epoch=589
05/21/2022 11:24:34 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.85 on epoch=592
05/21/2022 11:24:36 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.78 on epoch=594
05/21/2022 11:24:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.80 on epoch=597
05/21/2022 11:24:39 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.77 on epoch=599
05/21/2022 11:24:39 - INFO - __main__ - Global step 2400 Train loss 0.79 Classification-F1 0.18166666666666667 on epoch=599
05/21/2022 11:24:41 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.81 on epoch=602
05/21/2022 11:24:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.79 on epoch=604
05/21/2022 11:24:43 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.81 on epoch=607
05/21/2022 11:24:45 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.89 on epoch=609
05/21/2022 11:24:46 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.83 on epoch=612
05/21/2022 11:24:47 - INFO - __main__ - Global step 2450 Train loss 0.83 Classification-F1 0.22174447174447173 on epoch=612
05/21/2022 11:24:47 - INFO - __main__ - Saving model with best Classification-F1: 0.20526315789473687 -> 0.22174447174447173 on epoch=612, global_step=2450
05/21/2022 11:24:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.82 on epoch=614
05/21/2022 11:24:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.74 on epoch=617
05/21/2022 11:24:51 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.81 on epoch=619
05/21/2022 11:24:52 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.81 on epoch=622
05/21/2022 11:24:54 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.73 on epoch=624
05/21/2022 11:24:54 - INFO - __main__ - Global step 2500 Train loss 0.78 Classification-F1 0.18284347231715653 on epoch=624
05/21/2022 11:24:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.77 on epoch=627
05/21/2022 11:24:57 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.83 on epoch=629
05/21/2022 11:24:59 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.77 on epoch=632
05/21/2022 11:25:00 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.84 on epoch=634
05/21/2022 11:25:02 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.80 on epoch=637
05/21/2022 11:25:03 - INFO - __main__ - Global step 2550 Train loss 0.80 Classification-F1 0.17368421052631577 on epoch=637
05/21/2022 11:25:04 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.78 on epoch=639
05/21/2022 11:25:06 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.83 on epoch=642
05/21/2022 11:25:07 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.80 on epoch=644
05/21/2022 11:25:08 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.77 on epoch=647
05/21/2022 11:25:10 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.78 on epoch=649
05/21/2022 11:25:10 - INFO - __main__ - Global step 2600 Train loss 0.79 Classification-F1 0.13167388167388167 on epoch=649
05/21/2022 11:25:12 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.76 on epoch=652
05/21/2022 11:25:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.78 on epoch=654
05/21/2022 11:25:15 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.80 on epoch=657
05/21/2022 11:25:16 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.80 on epoch=659
05/21/2022 11:25:18 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.82 on epoch=662
05/21/2022 11:25:18 - INFO - __main__ - Global step 2650 Train loss 0.79 Classification-F1 0.19797782126549252 on epoch=662
05/21/2022 11:25:20 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.81 on epoch=664
05/21/2022 11:25:21 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.77 on epoch=667
05/21/2022 11:25:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.82 on epoch=669
05/21/2022 11:25:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.77 on epoch=672
05/21/2022 11:25:25 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.73 on epoch=674
05/21/2022 11:25:26 - INFO - __main__ - Global step 2700 Train loss 0.78 Classification-F1 0.1373668188736682 on epoch=674
05/21/2022 11:25:28 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.88 on epoch=677
05/21/2022 11:25:29 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.80 on epoch=679
05/21/2022 11:25:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.83 on epoch=682
05/21/2022 11:25:32 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.80 on epoch=684
05/21/2022 11:25:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.76 on epoch=687
05/21/2022 11:25:35 - INFO - __main__ - Global step 2750 Train loss 0.81 Classification-F1 0.125 on epoch=687
05/21/2022 11:25:36 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.82 on epoch=689
05/21/2022 11:25:38 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.76 on epoch=692
05/21/2022 11:25:39 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.77 on epoch=694
05/21/2022 11:25:41 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.82 on epoch=697
05/21/2022 11:25:42 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.92 on epoch=699
05/21/2022 11:25:42 - INFO - __main__ - Global step 2800 Train loss 0.82 Classification-F1 0.15789473684210525 on epoch=699
05/21/2022 11:25:44 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.81 on epoch=702
05/21/2022 11:25:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.85 on epoch=704
05/21/2022 11:25:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.80 on epoch=707
05/21/2022 11:25:49 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.81 on epoch=709
05/21/2022 11:25:50 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.79 on epoch=712
05/21/2022 11:25:51 - INFO - __main__ - Global step 2850 Train loss 0.81 Classification-F1 0.18166666666666667 on epoch=712
05/21/2022 11:25:52 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.80 on epoch=714
05/21/2022 11:25:54 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.80 on epoch=717
05/21/2022 11:25:55 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.80 on epoch=719
05/21/2022 11:25:57 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.70 on epoch=722
05/21/2022 11:25:59 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.82 on epoch=724
05/21/2022 11:25:59 - INFO - __main__ - Global step 2900 Train loss 0.79 Classification-F1 0.15789473684210525 on epoch=724
05/21/2022 11:26:01 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.75 on epoch=727
05/21/2022 11:26:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.71 on epoch=729
05/21/2022 11:26:03 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.77 on epoch=732
05/21/2022 11:26:04 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.79 on epoch=734
05/21/2022 11:26:05 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.79 on epoch=737
05/21/2022 11:26:06 - INFO - __main__ - Global step 2950 Train loss 0.76 Classification-F1 0.24154589371980673 on epoch=737
05/21/2022 11:26:06 - INFO - __main__ - Saving model with best Classification-F1: 0.22174447174447173 -> 0.24154589371980673 on epoch=737, global_step=2950
05/21/2022 11:26:07 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.82 on epoch=739
05/21/2022 11:26:08 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.83 on epoch=742
05/21/2022 11:26:10 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.70 on epoch=744
05/21/2022 11:26:11 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.78 on epoch=747
05/21/2022 11:26:12 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.76 on epoch=749
05/21/2022 11:26:13 - INFO - __main__ - Global step 3000 Train loss 0.78 Classification-F1 0.22393048128342247 on epoch=749
05/21/2022 11:26:13 - INFO - __main__ - save last model!
05/21/2022 11:26:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 11:26:13 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 11:26:13 - INFO - __main__ - Printing 3 examples
05/21/2022 11:26:13 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 11:26:13 - INFO - __main__ - ['others']
05/21/2022 11:26:13 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 11:26:13 - INFO - __main__ - ['others']
05/21/2022 11:26:13 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 11:26:13 - INFO - __main__ - ['others']
05/21/2022 11:26:13 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:26:13 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:26:13 - INFO - __main__ - Printing 3 examples
05/21/2022 11:26:13 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/21/2022 11:26:13 - INFO - __main__ - ['happy']
05/21/2022 11:26:13 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/21/2022 11:26:13 - INFO - __main__ - ['happy']
05/21/2022 11:26:13 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/21/2022 11:26:13 - INFO - __main__ - ['happy']
05/21/2022 11:26:13 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:26:13 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:26:13 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 11:26:13 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:26:13 - INFO - __main__ - Printing 3 examples
05/21/2022 11:26:13 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/21/2022 11:26:13 - INFO - __main__ - ['happy']
05/21/2022 11:26:13 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/21/2022 11:26:13 - INFO - __main__ - ['happy']
05/21/2022 11:26:13 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/21/2022 11:26:13 - INFO - __main__ - ['happy']
05/21/2022 11:26:13 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:26:13 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:26:13 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 11:26:15 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:26:19 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 11:26:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 11:26:19 - INFO - __main__ - Starting training!
05/21/2022 11:26:20 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 11:27:04 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_42_0.3_8_predictions.txt
05/21/2022 11:27:04 - INFO - __main__ - Classification-F1 on test data: 0.1137
05/21/2022 11:27:04 - INFO - __main__ - prefix=emo_16_42, lr=0.3, bsz=8, dev_performance=0.24154589371980673, test_performance=0.11372137083967966
05/21/2022 11:27:04 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.2, bsz=8 ...
05/21/2022 11:27:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:27:05 - INFO - __main__ - Printing 3 examples
05/21/2022 11:27:05 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/21/2022 11:27:05 - INFO - __main__ - ['happy']
05/21/2022 11:27:05 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/21/2022 11:27:05 - INFO - __main__ - ['happy']
05/21/2022 11:27:05 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/21/2022 11:27:05 - INFO - __main__ - ['happy']
05/21/2022 11:27:05 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:27:05 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:27:05 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 11:27:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:27:05 - INFO - __main__ - Printing 3 examples
05/21/2022 11:27:05 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/21/2022 11:27:05 - INFO - __main__ - ['happy']
05/21/2022 11:27:05 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/21/2022 11:27:05 - INFO - __main__ - ['happy']
05/21/2022 11:27:05 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/21/2022 11:27:05 - INFO - __main__ - ['happy']
05/21/2022 11:27:05 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:27:05 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:27:05 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 11:27:11 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 11:27:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 11:27:11 - INFO - __main__ - Starting training!
05/21/2022 11:27:12 - INFO - __main__ - Step 10 Global step 10 Train loss 4.22 on epoch=2
05/21/2022 11:27:14 - INFO - __main__ - Step 20 Global step 20 Train loss 3.98 on epoch=4
05/21/2022 11:27:15 - INFO - __main__ - Step 30 Global step 30 Train loss 3.64 on epoch=7
05/21/2022 11:27:17 - INFO - __main__ - Step 40 Global step 40 Train loss 3.49 on epoch=9
05/21/2022 11:27:18 - INFO - __main__ - Step 50 Global step 50 Train loss 3.19 on epoch=12
05/21/2022 11:27:19 - INFO - __main__ - Global step 50 Train loss 3.71 Classification-F1 0.0 on epoch=12
05/21/2022 11:27:19 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
05/21/2022 11:27:20 - INFO - __main__ - Step 60 Global step 60 Train loss 3.00 on epoch=14
05/21/2022 11:27:22 - INFO - __main__ - Step 70 Global step 70 Train loss 2.68 on epoch=17
05/21/2022 11:27:23 - INFO - __main__ - Step 80 Global step 80 Train loss 2.53 on epoch=19
05/21/2022 11:27:24 - INFO - __main__ - Step 90 Global step 90 Train loss 2.28 on epoch=22
05/21/2022 11:27:26 - INFO - __main__ - Step 100 Global step 100 Train loss 2.20 on epoch=24
05/21/2022 11:27:27 - INFO - __main__ - Global step 100 Train loss 2.54 Classification-F1 0.0810126582278481 on epoch=24
05/21/2022 11:27:27 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.0810126582278481 on epoch=24, global_step=100
05/21/2022 11:27:28 - INFO - __main__ - Step 110 Global step 110 Train loss 1.96 on epoch=27
05/21/2022 11:27:29 - INFO - __main__ - Step 120 Global step 120 Train loss 1.85 on epoch=29
05/21/2022 11:27:31 - INFO - __main__ - Step 130 Global step 130 Train loss 1.64 on epoch=32
05/21/2022 11:27:32 - INFO - __main__ - Step 140 Global step 140 Train loss 1.64 on epoch=34
05/21/2022 11:27:33 - INFO - __main__ - Step 150 Global step 150 Train loss 1.48 on epoch=37
05/21/2022 11:27:34 - INFO - __main__ - Global step 150 Train loss 1.72 Classification-F1 0.15975177304964538 on epoch=37
05/21/2022 11:27:34 - INFO - __main__ - Saving model with best Classification-F1: 0.0810126582278481 -> 0.15975177304964538 on epoch=37, global_step=150
05/21/2022 11:27:35 - INFO - __main__ - Step 160 Global step 160 Train loss 1.45 on epoch=39
05/21/2022 11:27:37 - INFO - __main__ - Step 170 Global step 170 Train loss 1.34 on epoch=42
05/21/2022 11:27:38 - INFO - __main__ - Step 180 Global step 180 Train loss 1.30 on epoch=44
05/21/2022 11:27:40 - INFO - __main__ - Step 190 Global step 190 Train loss 1.34 on epoch=47
05/21/2022 11:27:41 - INFO - __main__ - Step 200 Global step 200 Train loss 1.27 on epoch=49
05/21/2022 11:27:42 - INFO - __main__ - Global step 200 Train loss 1.34 Classification-F1 0.10558069381598793 on epoch=49
05/21/2022 11:27:44 - INFO - __main__ - Step 210 Global step 210 Train loss 1.15 on epoch=52
05/21/2022 11:27:45 - INFO - __main__ - Step 220 Global step 220 Train loss 1.24 on epoch=54
05/21/2022 11:27:47 - INFO - __main__ - Step 230 Global step 230 Train loss 1.03 on epoch=57
05/21/2022 11:27:48 - INFO - __main__ - Step 240 Global step 240 Train loss 1.02 on epoch=59
05/21/2022 11:27:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.94 on epoch=62
05/21/2022 11:27:50 - INFO - __main__ - Global step 250 Train loss 1.08 Classification-F1 0.13067758749069247 on epoch=62
05/21/2022 11:27:51 - INFO - __main__ - Step 260 Global step 260 Train loss 1.02 on epoch=64
05/21/2022 11:27:53 - INFO - __main__ - Step 270 Global step 270 Train loss 1.08 on epoch=67
05/21/2022 11:27:54 - INFO - __main__ - Step 280 Global step 280 Train loss 1.02 on epoch=69
05/21/2022 11:27:56 - INFO - __main__ - Step 290 Global step 290 Train loss 1.01 on epoch=72
05/21/2022 11:27:57 - INFO - __main__ - Step 300 Global step 300 Train loss 1.02 on epoch=74
05/21/2022 11:27:58 - INFO - __main__ - Global step 300 Train loss 1.03 Classification-F1 0.13067758749069247 on epoch=74
05/21/2022 11:27:59 - INFO - __main__ - Step 310 Global step 310 Train loss 1.06 on epoch=77
05/21/2022 11:28:01 - INFO - __main__ - Step 320 Global step 320 Train loss 1.07 on epoch=79
05/21/2022 11:28:02 - INFO - __main__ - Step 330 Global step 330 Train loss 1.08 on epoch=82
05/21/2022 11:28:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.98 on epoch=84
05/21/2022 11:28:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.92 on epoch=87
05/21/2022 11:28:06 - INFO - __main__ - Global step 350 Train loss 1.02 Classification-F1 0.13330786860198623 on epoch=87
05/21/2022 11:28:08 - INFO - __main__ - Step 360 Global step 360 Train loss 1.02 on epoch=89
05/21/2022 11:28:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.96 on epoch=92
05/21/2022 11:28:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.94 on epoch=94
05/21/2022 11:28:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.99 on epoch=97
05/21/2022 11:28:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.92 on epoch=99
05/21/2022 11:28:14 - INFO - __main__ - Global step 400 Train loss 0.97 Classification-F1 0.13067758749069247 on epoch=99
05/21/2022 11:28:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.97 on epoch=102
05/21/2022 11:28:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.95 on epoch=104
05/21/2022 11:28:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.94 on epoch=107
05/21/2022 11:28:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.97 on epoch=109
05/21/2022 11:28:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.99 on epoch=112
05/21/2022 11:28:22 - INFO - __main__ - Global step 450 Train loss 0.96 Classification-F1 0.13067758749069247 on epoch=112
05/21/2022 11:28:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.94 on epoch=114
05/21/2022 11:28:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.89 on epoch=117
05/21/2022 11:28:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.95 on epoch=119
05/21/2022 11:28:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.98 on epoch=122
05/21/2022 11:28:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.93 on epoch=124
05/21/2022 11:28:30 - INFO - __main__ - Global step 500 Train loss 0.94 Classification-F1 0.13067758749069247 on epoch=124
05/21/2022 11:28:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.95 on epoch=127
05/21/2022 11:28:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.98 on epoch=129
05/21/2022 11:28:34 - INFO - __main__ - Step 530 Global step 530 Train loss 1.02 on epoch=132
05/21/2022 11:28:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.90 on epoch=134
05/21/2022 11:28:36 - INFO - __main__ - Step 550 Global step 550 Train loss 1.02 on epoch=137
05/21/2022 11:28:37 - INFO - __main__ - Global step 550 Train loss 0.97 Classification-F1 0.13067758749069247 on epoch=137
05/21/2022 11:28:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.97 on epoch=139
05/21/2022 11:28:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.94 on epoch=142
05/21/2022 11:28:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.97 on epoch=144
05/21/2022 11:28:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.86 on epoch=147
05/21/2022 11:28:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.92 on epoch=149
05/21/2022 11:28:44 - INFO - __main__ - Global step 600 Train loss 0.93 Classification-F1 0.10126582278481013 on epoch=149
05/21/2022 11:28:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.93 on epoch=152
05/21/2022 11:28:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.88 on epoch=154
05/21/2022 11:28:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.94 on epoch=157
05/21/2022 11:28:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.99 on epoch=159
05/21/2022 11:28:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.87 on epoch=162
05/21/2022 11:28:52 - INFO - __main__ - Global step 650 Train loss 0.92 Classification-F1 0.15945165945165946 on epoch=162
05/21/2022 11:28:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.97 on epoch=164
05/21/2022 11:28:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.99 on epoch=167
05/21/2022 11:28:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.89 on epoch=169
05/21/2022 11:28:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.91 on epoch=172
05/21/2022 11:28:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.86 on epoch=174
05/21/2022 11:29:00 - INFO - __main__ - Global step 700 Train loss 0.92 Classification-F1 0.10126582278481013 on epoch=174
05/21/2022 11:29:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.92 on epoch=177
05/21/2022 11:29:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.96 on epoch=179
05/21/2022 11:29:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.92 on epoch=182
05/21/2022 11:29:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.96 on epoch=184
05/21/2022 11:29:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.87 on epoch=187
05/21/2022 11:29:08 - INFO - __main__ - Global step 750 Train loss 0.93 Classification-F1 0.10126582278481013 on epoch=187
05/21/2022 11:29:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.90 on epoch=189
05/21/2022 11:29:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.89 on epoch=192
05/21/2022 11:29:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.82 on epoch=194
05/21/2022 11:29:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.85 on epoch=197
05/21/2022 11:29:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.91 on epoch=199
05/21/2022 11:29:15 - INFO - __main__ - Global step 800 Train loss 0.87 Classification-F1 0.10126582278481013 on epoch=199
05/21/2022 11:29:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.89 on epoch=202
05/21/2022 11:29:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.89 on epoch=204
05/21/2022 11:29:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.96 on epoch=207
05/21/2022 11:29:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.92 on epoch=209
05/21/2022 11:29:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.83 on epoch=212
05/21/2022 11:29:22 - INFO - __main__ - Global step 850 Train loss 0.90 Classification-F1 0.12145969498910676 on epoch=212
05/21/2022 11:29:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.86 on epoch=214
05/21/2022 11:29:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.86 on epoch=217
05/21/2022 11:29:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.97 on epoch=219
05/21/2022 11:29:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.89 on epoch=222
05/21/2022 11:29:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.86 on epoch=224
05/21/2022 11:29:30 - INFO - __main__ - Global step 900 Train loss 0.88 Classification-F1 0.10126582278481013 on epoch=224
05/21/2022 11:29:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.83 on epoch=227
05/21/2022 11:29:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.89 on epoch=229
05/21/2022 11:29:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.91 on epoch=232
05/21/2022 11:29:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.94 on epoch=234
05/21/2022 11:29:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.87 on epoch=237
05/21/2022 11:29:38 - INFO - __main__ - Global step 950 Train loss 0.89 Classification-F1 0.10126582278481013 on epoch=237
05/21/2022 11:29:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.84 on epoch=239
05/21/2022 11:29:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.82 on epoch=242
05/21/2022 11:29:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.84 on epoch=244
05/21/2022 11:29:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.89 on epoch=247
05/21/2022 11:29:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.87 on epoch=249
05/21/2022 11:29:46 - INFO - __main__ - Global step 1000 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=249
05/21/2022 11:29:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.83 on epoch=252
05/21/2022 11:29:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.87 on epoch=254
05/21/2022 11:29:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.82 on epoch=257
05/21/2022 11:29:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.88 on epoch=259
05/21/2022 11:29:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.83 on epoch=262
05/21/2022 11:29:54 - INFO - __main__ - Global step 1050 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=262
05/21/2022 11:29:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.88 on epoch=264
05/21/2022 11:29:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.85 on epoch=267
05/21/2022 11:29:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.83 on epoch=269
05/21/2022 11:29:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.82 on epoch=272
05/21/2022 11:30:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.87 on epoch=274
05/21/2022 11:30:01 - INFO - __main__ - Global step 1100 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=274
05/21/2022 11:30:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.80 on epoch=277
05/21/2022 11:30:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.93 on epoch=279
05/21/2022 11:30:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.87 on epoch=282
05/21/2022 11:30:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.82 on epoch=284
05/21/2022 11:30:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.83 on epoch=287
05/21/2022 11:30:09 - INFO - __main__ - Global step 1150 Train loss 0.85 Classification-F1 0.13067758749069247 on epoch=287
05/21/2022 11:30:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.85 on epoch=289
05/21/2022 11:30:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.84 on epoch=292
05/21/2022 11:30:13 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.80 on epoch=294
05/21/2022 11:30:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.84 on epoch=297
05/21/2022 11:30:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.94 on epoch=299
05/21/2022 11:30:17 - INFO - __main__ - Global step 1200 Train loss 0.85 Classification-F1 0.11111111111111112 on epoch=299
05/21/2022 11:30:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.87 on epoch=302
05/21/2022 11:30:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.90 on epoch=304
05/21/2022 11:30:21 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.88 on epoch=307
05/21/2022 11:30:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.81 on epoch=309
05/21/2022 11:30:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.89 on epoch=312
05/21/2022 11:30:24 - INFO - __main__ - Global step 1250 Train loss 0.87 Classification-F1 0.13067758749069247 on epoch=312
05/21/2022 11:30:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.84 on epoch=314
05/21/2022 11:30:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.86 on epoch=317
05/21/2022 11:30:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.81 on epoch=319
05/21/2022 11:30:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.89 on epoch=322
05/21/2022 11:30:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.80 on epoch=324
05/21/2022 11:30:32 - INFO - __main__ - Global step 1300 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=324
05/21/2022 11:30:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.87 on epoch=327
05/21/2022 11:30:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.85 on epoch=329
05/21/2022 11:30:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.89 on epoch=332
05/21/2022 11:30:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.80 on epoch=334
05/21/2022 11:30:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.84 on epoch=337
05/21/2022 11:30:40 - INFO - __main__ - Global step 1350 Train loss 0.85 Classification-F1 0.1622222222222222 on epoch=337
05/21/2022 11:30:40 - INFO - __main__ - Saving model with best Classification-F1: 0.15975177304964538 -> 0.1622222222222222 on epoch=337, global_step=1350
05/21/2022 11:30:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.87 on epoch=339
05/21/2022 11:30:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.75 on epoch=342
05/21/2022 11:30:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.86 on epoch=344
05/21/2022 11:30:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.72 on epoch=347
05/21/2022 11:30:47 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.93 on epoch=349
05/21/2022 11:30:48 - INFO - __main__ - Global step 1400 Train loss 0.83 Classification-F1 0.13067758749069247 on epoch=349
05/21/2022 11:30:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.81 on epoch=352
05/21/2022 11:30:50 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.95 on epoch=354
05/21/2022 11:30:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.81 on epoch=357
05/21/2022 11:30:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.94 on epoch=359
05/21/2022 11:30:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.88 on epoch=362
05/21/2022 11:30:55 - INFO - __main__ - Global step 1450 Train loss 0.88 Classification-F1 0.13197586726998492 on epoch=362
05/21/2022 11:30:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.85 on epoch=364
05/21/2022 11:30:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.88 on epoch=367
05/21/2022 11:30:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.78 on epoch=369
05/21/2022 11:31:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.80 on epoch=372
05/21/2022 11:31:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.89 on epoch=374
05/21/2022 11:31:03 - INFO - __main__ - Global step 1500 Train loss 0.84 Classification-F1 0.1581196581196581 on epoch=374
05/21/2022 11:31:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.89 on epoch=377
05/21/2022 11:31:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.80 on epoch=379
05/21/2022 11:31:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.90 on epoch=382
05/21/2022 11:31:08 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.89 on epoch=384
05/21/2022 11:31:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.89 on epoch=387
05/21/2022 11:31:10 - INFO - __main__ - Global step 1550 Train loss 0.87 Classification-F1 0.13197586726998492 on epoch=387
05/21/2022 11:31:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.76 on epoch=389
05/21/2022 11:31:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.88 on epoch=392
05/21/2022 11:31:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.78 on epoch=394
05/21/2022 11:31:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.84 on epoch=397
05/21/2022 11:31:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.81 on epoch=399
05/21/2022 11:31:18 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.10126582278481013 on epoch=399
05/21/2022 11:31:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.84 on epoch=402
05/21/2022 11:31:21 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.80 on epoch=404
05/21/2022 11:31:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.76 on epoch=407
05/21/2022 11:31:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.84 on epoch=409
05/21/2022 11:31:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.83 on epoch=412
05/21/2022 11:31:26 - INFO - __main__ - Global step 1650 Train loss 0.81 Classification-F1 0.13067758749069247 on epoch=412
05/21/2022 11:31:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.79 on epoch=414
05/21/2022 11:31:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.85 on epoch=417
05/21/2022 11:31:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.78 on epoch=419
05/21/2022 11:31:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.84 on epoch=422
05/21/2022 11:31:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.84 on epoch=424
05/21/2022 11:31:33 - INFO - __main__ - Global step 1700 Train loss 0.82 Classification-F1 0.18284347231715653 on epoch=424
05/21/2022 11:31:33 - INFO - __main__ - Saving model with best Classification-F1: 0.1622222222222222 -> 0.18284347231715653 on epoch=424, global_step=1700
05/21/2022 11:31:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.75 on epoch=427
05/21/2022 11:31:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.82 on epoch=429
05/21/2022 11:31:37 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.84 on epoch=432
05/21/2022 11:31:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.88 on epoch=434
05/21/2022 11:31:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.81 on epoch=437
05/21/2022 11:31:41 - INFO - __main__ - Global step 1750 Train loss 0.82 Classification-F1 0.24447174447174447 on epoch=437
05/21/2022 11:31:41 - INFO - __main__ - Saving model with best Classification-F1: 0.18284347231715653 -> 0.24447174447174447 on epoch=437, global_step=1750
05/21/2022 11:31:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.72 on epoch=439
05/21/2022 11:31:43 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.81 on epoch=442
05/21/2022 11:31:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.85 on epoch=444
05/21/2022 11:31:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.86 on epoch=447
05/21/2022 11:31:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.83 on epoch=449
05/21/2022 11:31:48 - INFO - __main__ - Global step 1800 Train loss 0.81 Classification-F1 0.20666666666666667 on epoch=449
05/21/2022 11:31:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.80 on epoch=452
05/21/2022 11:31:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.89 on epoch=454
05/21/2022 11:31:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.85 on epoch=457
05/21/2022 11:31:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.80 on epoch=459
05/21/2022 11:31:55 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.86 on epoch=462
05/21/2022 11:31:55 - INFO - __main__ - Global step 1850 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=462
05/21/2022 11:31:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.80 on epoch=464
05/21/2022 11:31:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.82 on epoch=467
05/21/2022 11:32:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.80 on epoch=469
05/21/2022 11:32:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.84 on epoch=472
05/21/2022 11:32:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.83 on epoch=474
05/21/2022 11:32:03 - INFO - __main__ - Global step 1900 Train loss 0.82 Classification-F1 0.15945165945165946 on epoch=474
05/21/2022 11:32:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.79 on epoch=477
05/21/2022 11:32:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.85 on epoch=479
05/21/2022 11:32:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.85 on epoch=482
05/21/2022 11:32:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.74 on epoch=484
05/21/2022 11:32:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.88 on epoch=487
05/21/2022 11:32:10 - INFO - __main__ - Global step 1950 Train loss 0.82 Classification-F1 0.1842105263157895 on epoch=487
05/21/2022 11:32:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.84 on epoch=489
05/21/2022 11:32:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.86 on epoch=492
05/21/2022 11:32:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.82 on epoch=494
05/21/2022 11:32:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.75 on epoch=497
05/21/2022 11:32:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.82 on epoch=499
05/21/2022 11:32:17 - INFO - __main__ - Global step 2000 Train loss 0.82 Classification-F1 0.13330786860198623 on epoch=499
05/21/2022 11:32:19 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.84 on epoch=502
05/21/2022 11:32:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.78 on epoch=504
05/21/2022 11:32:21 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.81 on epoch=507
05/21/2022 11:32:22 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.76 on epoch=509
05/21/2022 11:32:24 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.83 on epoch=512
05/21/2022 11:32:24 - INFO - __main__ - Global step 2050 Train loss 0.80 Classification-F1 0.1842105263157895 on epoch=512
05/21/2022 11:32:26 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.78 on epoch=514
05/21/2022 11:32:27 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.75 on epoch=517
05/21/2022 11:32:29 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.80 on epoch=519
05/21/2022 11:32:30 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.81 on epoch=522
05/21/2022 11:32:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.84 on epoch=524
05/21/2022 11:32:32 - INFO - __main__ - Global step 2100 Train loss 0.80 Classification-F1 0.13197586726998492 on epoch=524
05/21/2022 11:32:33 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.93 on epoch=527
05/21/2022 11:32:34 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.82 on epoch=529
05/21/2022 11:32:36 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.79 on epoch=532
05/21/2022 11:32:37 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.92 on epoch=534
05/21/2022 11:32:39 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.77 on epoch=537
05/21/2022 11:32:39 - INFO - __main__ - Global step 2150 Train loss 0.85 Classification-F1 0.230952380952381 on epoch=537
05/21/2022 11:32:41 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.80 on epoch=539
05/21/2022 11:32:42 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.82 on epoch=542
05/21/2022 11:32:43 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.81 on epoch=544
05/21/2022 11:32:45 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.83 on epoch=547
05/21/2022 11:32:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.88 on epoch=549
05/21/2022 11:32:46 - INFO - __main__ - Global step 2200 Train loss 0.83 Classification-F1 0.15596101454280342 on epoch=549
05/21/2022 11:32:48 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.80 on epoch=552
05/21/2022 11:32:49 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.85 on epoch=554
05/21/2022 11:32:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.81 on epoch=557
05/21/2022 11:32:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.79 on epoch=559
05/21/2022 11:32:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.76 on epoch=562
05/21/2022 11:32:53 - INFO - __main__ - Global step 2250 Train loss 0.80 Classification-F1 0.15945165945165946 on epoch=562
05/21/2022 11:32:55 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.87 on epoch=564
05/21/2022 11:32:56 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.83 on epoch=567
05/21/2022 11:32:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.84 on epoch=569
05/21/2022 11:32:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.88 on epoch=572
05/21/2022 11:33:00 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.84 on epoch=574
05/21/2022 11:33:01 - INFO - __main__ - Global step 2300 Train loss 0.85 Classification-F1 0.1842105263157895 on epoch=574
05/21/2022 11:33:02 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.74 on epoch=577
05/21/2022 11:33:03 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.77 on epoch=579
05/21/2022 11:33:04 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.82 on epoch=582
05/21/2022 11:33:06 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.81 on epoch=584
05/21/2022 11:33:08 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.81 on epoch=587
05/21/2022 11:33:08 - INFO - __main__ - Global step 2350 Train loss 0.79 Classification-F1 0.23751987281399045 on epoch=587
05/21/2022 11:33:10 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.81 on epoch=589
05/21/2022 11:33:11 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.78 on epoch=592
05/21/2022 11:33:13 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.80 on epoch=594
05/21/2022 11:33:15 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.76 on epoch=597
05/21/2022 11:33:16 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.77 on epoch=599
05/21/2022 11:33:17 - INFO - __main__ - Global step 2400 Train loss 0.78 Classification-F1 0.2081081081081081 on epoch=599
05/21/2022 11:33:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.80 on epoch=602
05/21/2022 11:33:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.80 on epoch=604
05/21/2022 11:33:21 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.88 on epoch=607
05/21/2022 11:33:23 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.87 on epoch=609
05/21/2022 11:33:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.81 on epoch=612
05/21/2022 11:33:25 - INFO - __main__ - Global step 2450 Train loss 0.83 Classification-F1 0.20666666666666667 on epoch=612
05/21/2022 11:33:26 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.83 on epoch=614
05/21/2022 11:33:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.75 on epoch=617
05/21/2022 11:33:30 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.78 on epoch=619
05/21/2022 11:33:31 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.83 on epoch=622
05/21/2022 11:33:33 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.81 on epoch=624
05/21/2022 11:33:33 - INFO - __main__ - Global step 2500 Train loss 0.80 Classification-F1 0.20334620334620335 on epoch=624
05/21/2022 11:33:35 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.79 on epoch=627
05/21/2022 11:33:36 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.85 on epoch=629
05/21/2022 11:33:37 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.86 on epoch=632
05/21/2022 11:33:39 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.86 on epoch=634
05/21/2022 11:33:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.83 on epoch=637
05/21/2022 11:33:41 - INFO - __main__ - Global step 2550 Train loss 0.84 Classification-F1 0.19082633053221287 on epoch=637
05/21/2022 11:33:42 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.82 on epoch=639
05/21/2022 11:33:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.77 on epoch=642
05/21/2022 11:33:45 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.84 on epoch=644
05/21/2022 11:33:46 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.84 on epoch=647
05/21/2022 11:33:47 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.84 on epoch=649
05/21/2022 11:33:48 - INFO - __main__ - Global step 2600 Train loss 0.82 Classification-F1 0.22322540473225405 on epoch=649
05/21/2022 11:33:49 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.80 on epoch=652
05/21/2022 11:33:51 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.86 on epoch=654
05/21/2022 11:33:52 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.87 on epoch=657
05/21/2022 11:33:54 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.84 on epoch=659
05/21/2022 11:33:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.79 on epoch=662
05/21/2022 11:33:56 - INFO - __main__ - Global step 2650 Train loss 0.83 Classification-F1 0.21927016645326503 on epoch=662
05/21/2022 11:33:57 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.82 on epoch=664
05/21/2022 11:33:59 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.78 on epoch=667
05/21/2022 11:34:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.86 on epoch=669
05/21/2022 11:34:01 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.81 on epoch=672
05/21/2022 11:34:03 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.84 on epoch=674
05/21/2022 11:34:03 - INFO - __main__ - Global step 2700 Train loss 0.82 Classification-F1 0.17635135135135135 on epoch=674
05/21/2022 11:34:05 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.78 on epoch=677
05/21/2022 11:34:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.76 on epoch=679
05/21/2022 11:34:07 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.81 on epoch=682
05/21/2022 11:34:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.74 on epoch=684
05/21/2022 11:34:10 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.79 on epoch=687
05/21/2022 11:34:11 - INFO - __main__ - Global step 2750 Train loss 0.78 Classification-F1 0.175 on epoch=687
05/21/2022 11:34:12 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.88 on epoch=689
05/21/2022 11:34:13 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.79 on epoch=692
05/21/2022 11:34:15 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.78 on epoch=694
05/21/2022 11:34:16 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.75 on epoch=697
05/21/2022 11:34:18 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.76 on epoch=699
05/21/2022 11:34:18 - INFO - __main__ - Global step 2800 Train loss 0.79 Classification-F1 0.18997945313734785 on epoch=699
05/21/2022 11:34:20 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.78 on epoch=702
05/21/2022 11:34:21 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.86 on epoch=704
05/21/2022 11:34:22 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.78 on epoch=707
05/21/2022 11:34:24 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.75 on epoch=709
05/21/2022 11:34:25 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.84 on epoch=712
05/21/2022 11:34:26 - INFO - __main__ - Global step 2850 Train loss 0.80 Classification-F1 0.17644927536231886 on epoch=712
05/21/2022 11:34:27 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.84 on epoch=714
05/21/2022 11:34:28 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.85 on epoch=717
05/21/2022 11:34:30 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.77 on epoch=719
05/21/2022 11:34:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.87 on epoch=722
05/21/2022 11:34:33 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.79 on epoch=724
05/21/2022 11:34:34 - INFO - __main__ - Global step 2900 Train loss 0.83 Classification-F1 0.19797782126549252 on epoch=724
05/21/2022 11:34:35 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.84 on epoch=727
05/21/2022 11:34:36 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.79 on epoch=729
05/21/2022 11:34:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.81 on epoch=732
05/21/2022 11:34:39 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.77 on epoch=734
05/21/2022 11:34:40 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.77 on epoch=737
05/21/2022 11:34:40 - INFO - __main__ - Global step 2950 Train loss 0.80 Classification-F1 0.2306338028169014 on epoch=737
05/21/2022 11:34:42 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.80 on epoch=739
05/21/2022 11:34:43 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.77 on epoch=742
05/21/2022 11:34:45 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.79 on epoch=744
05/21/2022 11:34:46 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.77 on epoch=747
05/21/2022 11:34:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.86 on epoch=749
05/21/2022 11:34:48 - INFO - __main__ - Global step 3000 Train loss 0.80 Classification-F1 0.19797782126549252 on epoch=749
05/21/2022 11:34:48 - INFO - __main__ - save last model!
05/21/2022 11:34:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 11:34:48 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 11:34:48 - INFO - __main__ - Printing 3 examples
05/21/2022 11:34:48 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 11:34:48 - INFO - __main__ - ['others']
05/21/2022 11:34:48 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 11:34:48 - INFO - __main__ - ['others']
05/21/2022 11:34:48 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 11:34:48 - INFO - __main__ - ['others']
05/21/2022 11:34:48 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:34:49 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:34:49 - INFO - __main__ - Printing 3 examples
05/21/2022 11:34:49 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/21/2022 11:34:49 - INFO - __main__ - ['others']
05/21/2022 11:34:49 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/21/2022 11:34:49 - INFO - __main__ - ['others']
05/21/2022 11:34:49 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/21/2022 11:34:49 - INFO - __main__ - ['others']
05/21/2022 11:34:49 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:34:49 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:34:49 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 11:34:49 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:34:49 - INFO - __main__ - Printing 3 examples
05/21/2022 11:34:49 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/21/2022 11:34:49 - INFO - __main__ - ['others']
05/21/2022 11:34:49 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/21/2022 11:34:49 - INFO - __main__ - ['others']
05/21/2022 11:34:49 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/21/2022 11:34:49 - INFO - __main__ - ['others']
05/21/2022 11:34:49 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:34:49 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:34:49 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 11:34:50 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:34:55 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 11:34:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 11:34:55 - INFO - __main__ - Starting training!
05/21/2022 11:34:57 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 11:35:43 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_42_0.2_8_predictions.txt
05/21/2022 11:35:43 - INFO - __main__ - Classification-F1 on test data: 0.1017
05/21/2022 11:35:43 - INFO - __main__ - prefix=emo_16_42, lr=0.2, bsz=8, dev_performance=0.24447174447174447, test_performance=0.10169664156978525
05/21/2022 11:35:43 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.5, bsz=8 ...
05/21/2022 11:35:44 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:35:44 - INFO - __main__ - Printing 3 examples
05/21/2022 11:35:44 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/21/2022 11:35:44 - INFO - __main__ - ['others']
05/21/2022 11:35:44 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/21/2022 11:35:44 - INFO - __main__ - ['others']
05/21/2022 11:35:44 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/21/2022 11:35:44 - INFO - __main__ - ['others']
05/21/2022 11:35:44 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:35:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:35:44 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 11:35:44 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:35:44 - INFO - __main__ - Printing 3 examples
05/21/2022 11:35:44 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/21/2022 11:35:44 - INFO - __main__ - ['others']
05/21/2022 11:35:44 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/21/2022 11:35:44 - INFO - __main__ - ['others']
05/21/2022 11:35:44 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/21/2022 11:35:44 - INFO - __main__ - ['others']
05/21/2022 11:35:44 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:35:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:35:45 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 11:35:51 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 11:35:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 11:35:51 - INFO - __main__ - Starting training!
05/21/2022 11:35:53 - INFO - __main__ - Step 10 Global step 10 Train loss 4.17 on epoch=2
05/21/2022 11:35:54 - INFO - __main__ - Step 20 Global step 20 Train loss 3.62 on epoch=4
05/21/2022 11:35:55 - INFO - __main__ - Step 30 Global step 30 Train loss 3.00 on epoch=7
05/21/2022 11:35:57 - INFO - __main__ - Step 40 Global step 40 Train loss 2.39 on epoch=9
05/21/2022 11:35:58 - INFO - __main__ - Step 50 Global step 50 Train loss 1.87 on epoch=12
05/21/2022 11:35:58 - INFO - __main__ - Global step 50 Train loss 3.01 Classification-F1 0.1 on epoch=12
05/21/2022 11:35:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/21/2022 11:36:00 - INFO - __main__ - Step 60 Global step 60 Train loss 1.45 on epoch=14
05/21/2022 11:36:01 - INFO - __main__ - Step 70 Global step 70 Train loss 1.41 on epoch=17
05/21/2022 11:36:03 - INFO - __main__ - Step 80 Global step 80 Train loss 1.22 on epoch=19
05/21/2022 11:36:04 - INFO - __main__ - Step 90 Global step 90 Train loss 1.10 on epoch=22
05/21/2022 11:36:05 - INFO - __main__ - Step 100 Global step 100 Train loss 0.95 on epoch=24
05/21/2022 11:36:06 - INFO - __main__ - Global step 100 Train loss 1.23 Classification-F1 0.1 on epoch=24
05/21/2022 11:36:07 - INFO - __main__ - Step 110 Global step 110 Train loss 1.06 on epoch=27
05/21/2022 11:36:09 - INFO - __main__ - Step 120 Global step 120 Train loss 1.01 on epoch=29
05/21/2022 11:36:11 - INFO - __main__ - Step 130 Global step 130 Train loss 1.09 on epoch=32
05/21/2022 11:36:12 - INFO - __main__ - Step 140 Global step 140 Train loss 0.98 on epoch=34
05/21/2022 11:36:13 - INFO - __main__ - Step 150 Global step 150 Train loss 1.01 on epoch=37
05/21/2022 11:36:14 - INFO - __main__ - Global step 150 Train loss 1.03 Classification-F1 0.13034188034188032 on epoch=37
05/21/2022 11:36:14 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.13034188034188032 on epoch=37, global_step=150
05/21/2022 11:36:15 - INFO - __main__ - Step 160 Global step 160 Train loss 1.02 on epoch=39
05/21/2022 11:36:17 - INFO - __main__ - Step 170 Global step 170 Train loss 1.02 on epoch=42
05/21/2022 11:36:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.86 on epoch=44
05/21/2022 11:36:19 - INFO - __main__ - Step 190 Global step 190 Train loss 1.04 on epoch=47
05/21/2022 11:36:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.97 on epoch=49
05/21/2022 11:36:21 - INFO - __main__ - Global step 200 Train loss 0.98 Classification-F1 0.1 on epoch=49
05/21/2022 11:36:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.92 on epoch=52
05/21/2022 11:36:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.92 on epoch=54
05/21/2022 11:36:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.96 on epoch=57
05/21/2022 11:36:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.97 on epoch=59
05/21/2022 11:36:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.87 on epoch=62
05/21/2022 11:36:29 - INFO - __main__ - Global step 250 Train loss 0.93 Classification-F1 0.1 on epoch=62
05/21/2022 11:36:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=64
05/21/2022 11:36:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.93 on epoch=67
05/21/2022 11:36:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.90 on epoch=69
05/21/2022 11:36:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.91 on epoch=72
05/21/2022 11:36:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.95 on epoch=74
05/21/2022 11:36:37 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.1 on epoch=74
05/21/2022 11:36:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.95 on epoch=77
05/21/2022 11:36:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.86 on epoch=79
05/21/2022 11:36:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.88 on epoch=82
05/21/2022 11:36:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.84 on epoch=84
05/21/2022 11:36:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.84 on epoch=87
05/21/2022 11:36:44 - INFO - __main__ - Global step 350 Train loss 0.88 Classification-F1 0.1 on epoch=87
05/21/2022 11:36:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.92 on epoch=89
05/21/2022 11:36:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.81 on epoch=92
05/21/2022 11:36:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.86 on epoch=94
05/21/2022 11:36:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=97
05/21/2022 11:36:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.82 on epoch=99
05/21/2022 11:36:52 - INFO - __main__ - Global step 400 Train loss 0.86 Classification-F1 0.17712418300653593 on epoch=99
05/21/2022 11:36:52 - INFO - __main__ - Saving model with best Classification-F1: 0.13034188034188032 -> 0.17712418300653593 on epoch=99, global_step=400
05/21/2022 11:36:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.79 on epoch=102
05/21/2022 11:36:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.81 on epoch=104
05/21/2022 11:36:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.85 on epoch=107
05/21/2022 11:36:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.92 on epoch=109
05/21/2022 11:36:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.88 on epoch=112
05/21/2022 11:37:00 - INFO - __main__ - Global step 450 Train loss 0.85 Classification-F1 0.1 on epoch=112
05/21/2022 11:37:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.83 on epoch=114
05/21/2022 11:37:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.82 on epoch=117
05/21/2022 11:37:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.81 on epoch=119
05/21/2022 11:37:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.79 on epoch=122
05/21/2022 11:37:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.81 on epoch=124
05/21/2022 11:37:07 - INFO - __main__ - Global step 500 Train loss 0.81 Classification-F1 0.15505279034690797 on epoch=124
05/21/2022 11:37:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.88 on epoch=127
05/21/2022 11:37:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.81 on epoch=129
05/21/2022 11:37:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.92 on epoch=132
05/21/2022 11:37:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.84 on epoch=134
05/21/2022 11:37:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=137
05/21/2022 11:37:15 - INFO - __main__ - Global step 550 Train loss 0.86 Classification-F1 0.09493670886075949 on epoch=137
05/21/2022 11:37:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.88 on epoch=139
05/21/2022 11:37:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.90 on epoch=142
05/21/2022 11:37:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.83 on epoch=144
05/21/2022 11:37:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.82 on epoch=147
05/21/2022 11:37:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.82 on epoch=149
05/21/2022 11:37:24 - INFO - __main__ - Global step 600 Train loss 0.85 Classification-F1 0.1 on epoch=149
05/21/2022 11:37:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.89 on epoch=152
05/21/2022 11:37:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.78 on epoch=154
05/21/2022 11:37:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.88 on epoch=157
05/21/2022 11:37:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.81 on epoch=159
05/21/2022 11:37:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.87 on epoch=162
05/21/2022 11:37:31 - INFO - __main__ - Global step 650 Train loss 0.84 Classification-F1 0.1 on epoch=162
05/21/2022 11:37:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.87 on epoch=164
05/21/2022 11:37:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.87 on epoch=167
05/21/2022 11:37:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.81 on epoch=169
05/21/2022 11:37:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.76 on epoch=172
05/21/2022 11:37:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.85 on epoch=174
05/21/2022 11:37:39 - INFO - __main__ - Global step 700 Train loss 0.83 Classification-F1 0.1 on epoch=174
05/21/2022 11:37:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.83 on epoch=177
05/21/2022 11:37:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.82 on epoch=179
05/21/2022 11:37:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.86 on epoch=182
05/21/2022 11:37:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.86 on epoch=184
05/21/2022 11:37:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.77 on epoch=187
05/21/2022 11:37:47 - INFO - __main__ - Global step 750 Train loss 0.83 Classification-F1 0.1 on epoch=187
05/21/2022 11:37:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.88 on epoch=189
05/21/2022 11:37:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.81 on epoch=192
05/21/2022 11:37:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.80 on epoch=194
05/21/2022 11:37:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.81 on epoch=197
05/21/2022 11:37:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.81 on epoch=199
05/21/2022 11:37:54 - INFO - __main__ - Global step 800 Train loss 0.82 Classification-F1 0.1 on epoch=199
05/21/2022 11:37:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.85 on epoch=202
05/21/2022 11:37:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.85 on epoch=204
05/21/2022 11:37:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=207
05/21/2022 11:38:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.86 on epoch=209
05/21/2022 11:38:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.78 on epoch=212
05/21/2022 11:38:02 - INFO - __main__ - Global step 850 Train loss 0.83 Classification-F1 0.1 on epoch=212
05/21/2022 11:38:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.85 on epoch=214
05/21/2022 11:38:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.85 on epoch=217
05/21/2022 11:38:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.83 on epoch=219
05/21/2022 11:38:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.83 on epoch=222
05/21/2022 11:38:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.80 on epoch=224
05/21/2022 11:38:10 - INFO - __main__ - Global step 900 Train loss 0.83 Classification-F1 0.1 on epoch=224
05/21/2022 11:38:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.86 on epoch=227
05/21/2022 11:38:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.89 on epoch=229
05/21/2022 11:38:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.74 on epoch=232
05/21/2022 11:38:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.78 on epoch=234
05/21/2022 11:38:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.88 on epoch=237
05/21/2022 11:38:17 - INFO - __main__ - Global step 950 Train loss 0.83 Classification-F1 0.14460784313725492 on epoch=237
05/21/2022 11:38:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.79 on epoch=239
05/21/2022 11:38:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.88 on epoch=242
05/21/2022 11:38:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.77 on epoch=244
05/21/2022 11:38:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.83 on epoch=247
05/21/2022 11:38:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.79 on epoch=249
05/21/2022 11:38:25 - INFO - __main__ - Global step 1000 Train loss 0.81 Classification-F1 0.1 on epoch=249
05/21/2022 11:38:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.75 on epoch=252
05/21/2022 11:38:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.86 on epoch=254
05/21/2022 11:38:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.85 on epoch=257
05/21/2022 11:38:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.82 on epoch=259
05/21/2022 11:38:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.86 on epoch=262
05/21/2022 11:38:32 - INFO - __main__ - Global step 1050 Train loss 0.83 Classification-F1 0.1422354557947778 on epoch=262
05/21/2022 11:38:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.80 on epoch=264
05/21/2022 11:38:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.78 on epoch=267
05/21/2022 11:38:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.79 on epoch=269
05/21/2022 11:38:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.84 on epoch=272
05/21/2022 11:38:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.77 on epoch=274
05/21/2022 11:38:40 - INFO - __main__ - Global step 1100 Train loss 0.80 Classification-F1 0.1 on epoch=274
05/21/2022 11:38:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.73 on epoch=277
05/21/2022 11:38:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.81 on epoch=279
05/21/2022 11:38:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.77 on epoch=282
05/21/2022 11:38:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.82 on epoch=284
05/21/2022 11:38:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.81 on epoch=287
05/21/2022 11:38:47 - INFO - __main__ - Global step 1150 Train loss 0.79 Classification-F1 0.2085137085137085 on epoch=287
05/21/2022 11:38:47 - INFO - __main__ - Saving model with best Classification-F1: 0.17712418300653593 -> 0.2085137085137085 on epoch=287, global_step=1150
05/21/2022 11:38:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.79 on epoch=289
05/21/2022 11:38:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.85 on epoch=292
05/21/2022 11:38:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.85 on epoch=294
05/21/2022 11:38:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.83 on epoch=297
05/21/2022 11:38:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.77 on epoch=299
05/21/2022 11:38:55 - INFO - __main__ - Global step 1200 Train loss 0.82 Classification-F1 0.10126582278481013 on epoch=299
05/21/2022 11:38:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.80 on epoch=302
05/21/2022 11:38:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.82 on epoch=304
05/21/2022 11:38:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.80 on epoch=307
05/21/2022 11:39:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.81 on epoch=309
05/21/2022 11:39:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.83 on epoch=312
05/21/2022 11:39:02 - INFO - __main__ - Global step 1250 Train loss 0.82 Classification-F1 0.19444444444444445 on epoch=312
05/21/2022 11:39:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.78 on epoch=314
05/21/2022 11:39:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.83 on epoch=317
05/21/2022 11:39:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.78 on epoch=319
05/21/2022 11:39:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.81 on epoch=322
05/21/2022 11:39:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.86 on epoch=324
05/21/2022 11:39:10 - INFO - __main__ - Global step 1300 Train loss 0.81 Classification-F1 0.1 on epoch=324
05/21/2022 11:39:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.84 on epoch=327
05/21/2022 11:39:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.82 on epoch=329
05/21/2022 11:39:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.84 on epoch=332
05/21/2022 11:39:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.89 on epoch=334
05/21/2022 11:39:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.82 on epoch=337
05/21/2022 11:39:18 - INFO - __main__ - Global step 1350 Train loss 0.84 Classification-F1 0.1095890410958904 on epoch=337
05/21/2022 11:39:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.83 on epoch=339
05/21/2022 11:39:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.74 on epoch=342
05/21/2022 11:39:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.74 on epoch=344
05/21/2022 11:39:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.76 on epoch=347
05/21/2022 11:39:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.79 on epoch=349
05/21/2022 11:39:26 - INFO - __main__ - Global step 1400 Train loss 0.77 Classification-F1 0.10126582278481013 on epoch=349
05/21/2022 11:39:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.77 on epoch=352
05/21/2022 11:39:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.75 on epoch=354
05/21/2022 11:39:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.73 on epoch=357
05/21/2022 11:39:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.80 on epoch=359
05/21/2022 11:39:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.76 on epoch=362
05/21/2022 11:39:34 - INFO - __main__ - Global step 1450 Train loss 0.76 Classification-F1 0.22377622377622375 on epoch=362
05/21/2022 11:39:34 - INFO - __main__ - Saving model with best Classification-F1: 0.2085137085137085 -> 0.22377622377622375 on epoch=362, global_step=1450
05/21/2022 11:39:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.75 on epoch=364
05/21/2022 11:39:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.78 on epoch=367
05/21/2022 11:39:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.85 on epoch=369
05/21/2022 11:39:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.85 on epoch=372
05/21/2022 11:39:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.81 on epoch=374
05/21/2022 11:39:42 - INFO - __main__ - Global step 1500 Train loss 0.81 Classification-F1 0.2375598086124402 on epoch=374
05/21/2022 11:39:42 - INFO - __main__ - Saving model with best Classification-F1: 0.22377622377622375 -> 0.2375598086124402 on epoch=374, global_step=1500
05/21/2022 11:39:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.79 on epoch=377
05/21/2022 11:39:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.82 on epoch=379
05/21/2022 11:39:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.82 on epoch=382
05/21/2022 11:39:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.75 on epoch=384
05/21/2022 11:39:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.83 on epoch=387
05/21/2022 11:39:50 - INFO - __main__ - Global step 1550 Train loss 0.80 Classification-F1 0.26572864615294706 on epoch=387
05/21/2022 11:39:50 - INFO - __main__ - Saving model with best Classification-F1: 0.2375598086124402 -> 0.26572864615294706 on epoch=387, global_step=1550
05/21/2022 11:39:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.73 on epoch=389
05/21/2022 11:39:53 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.80 on epoch=392
05/21/2022 11:39:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.80 on epoch=394
05/21/2022 11:39:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.77 on epoch=397
05/21/2022 11:39:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.77 on epoch=399
05/21/2022 11:39:57 - INFO - __main__ - Global step 1600 Train loss 0.77 Classification-F1 0.2610344827586207 on epoch=399
05/21/2022 11:39:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.82 on epoch=402
05/21/2022 11:40:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.76 on epoch=404
05/21/2022 11:40:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.80 on epoch=407
05/21/2022 11:40:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.75 on epoch=409
05/21/2022 11:40:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.84 on epoch=412
05/21/2022 11:40:05 - INFO - __main__ - Global step 1650 Train loss 0.79 Classification-F1 0.45157237398616706 on epoch=412
05/21/2022 11:40:05 - INFO - __main__ - Saving model with best Classification-F1: 0.26572864615294706 -> 0.45157237398616706 on epoch=412, global_step=1650
05/21/2022 11:40:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.77 on epoch=414
05/21/2022 11:40:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.78 on epoch=417
05/21/2022 11:40:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.72 on epoch=419
05/21/2022 11:40:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.79 on epoch=422
05/21/2022 11:40:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.74 on epoch=424
05/21/2022 11:40:12 - INFO - __main__ - Global step 1700 Train loss 0.76 Classification-F1 0.2416666666666667 on epoch=424
05/21/2022 11:40:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.78 on epoch=427
05/21/2022 11:40:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.79 on epoch=429
05/21/2022 11:40:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.80 on epoch=432
05/21/2022 11:40:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.75 on epoch=434
05/21/2022 11:40:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.76 on epoch=437
05/21/2022 11:40:20 - INFO - __main__ - Global step 1750 Train loss 0.78 Classification-F1 0.4458562271062271 on epoch=437
05/21/2022 11:40:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.80 on epoch=439
05/21/2022 11:40:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.71 on epoch=442
05/21/2022 11:40:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.79 on epoch=444
05/21/2022 11:40:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.82 on epoch=447
05/21/2022 11:40:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.72 on epoch=449
05/21/2022 11:40:28 - INFO - __main__ - Global step 1800 Train loss 0.77 Classification-F1 0.46711018905596813 on epoch=449
05/21/2022 11:40:28 - INFO - __main__ - Saving model with best Classification-F1: 0.45157237398616706 -> 0.46711018905596813 on epoch=449, global_step=1800
05/21/2022 11:40:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.75 on epoch=452
05/21/2022 11:40:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.78 on epoch=454
05/21/2022 11:40:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.71 on epoch=457
05/21/2022 11:40:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.79 on epoch=459
05/21/2022 11:40:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.82 on epoch=462
05/21/2022 11:40:36 - INFO - __main__ - Global step 1850 Train loss 0.77 Classification-F1 0.4533938019652305 on epoch=462
05/21/2022 11:40:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.74 on epoch=464
05/21/2022 11:40:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.71 on epoch=467
05/21/2022 11:40:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.72 on epoch=469
05/21/2022 11:40:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.74 on epoch=472
05/21/2022 11:40:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.73 on epoch=474
05/21/2022 11:40:43 - INFO - __main__ - Global step 1900 Train loss 0.73 Classification-F1 0.48836618876941457 on epoch=474
05/21/2022 11:40:43 - INFO - __main__ - Saving model with best Classification-F1: 0.46711018905596813 -> 0.48836618876941457 on epoch=474, global_step=1900
05/21/2022 11:40:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.72 on epoch=477
05/21/2022 11:40:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.77 on epoch=479
05/21/2022 11:40:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.72 on epoch=482
05/21/2022 11:40:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.72 on epoch=484
05/21/2022 11:40:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.69 on epoch=487
05/21/2022 11:40:51 - INFO - __main__ - Global step 1950 Train loss 0.72 Classification-F1 0.4438177368144273 on epoch=487
05/21/2022 11:40:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.74 on epoch=489
05/21/2022 11:40:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.72 on epoch=492
05/21/2022 11:40:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.71 on epoch=494
05/21/2022 11:40:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.72 on epoch=497
05/21/2022 11:40:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.75 on epoch=499
05/21/2022 11:40:58 - INFO - __main__ - Global step 2000 Train loss 0.73 Classification-F1 0.5103174603174603 on epoch=499
05/21/2022 11:40:58 - INFO - __main__ - Saving model with best Classification-F1: 0.48836618876941457 -> 0.5103174603174603 on epoch=499, global_step=2000
05/21/2022 11:40:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.67 on epoch=502
05/21/2022 11:41:01 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.73 on epoch=504
05/21/2022 11:41:02 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.72 on epoch=507
05/21/2022 11:41:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.68 on epoch=509
05/21/2022 11:41:05 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.69 on epoch=512
05/21/2022 11:41:06 - INFO - __main__ - Global step 2050 Train loss 0.70 Classification-F1 0.5089686495936495 on epoch=512
05/21/2022 11:41:07 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.66 on epoch=514
05/21/2022 11:41:09 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.66 on epoch=517
05/21/2022 11:41:10 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.68 on epoch=519
05/21/2022 11:41:12 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.73 on epoch=522
05/21/2022 11:41:13 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.68 on epoch=524
05/21/2022 11:41:14 - INFO - __main__ - Global step 2100 Train loss 0.68 Classification-F1 0.4750646412411118 on epoch=524
05/21/2022 11:41:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.69 on epoch=527
05/21/2022 11:41:16 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.74 on epoch=529
05/21/2022 11:41:18 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.71 on epoch=532
05/21/2022 11:41:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.69 on epoch=534
05/21/2022 11:41:20 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.72 on epoch=537
05/21/2022 11:41:21 - INFO - __main__ - Global step 2150 Train loss 0.71 Classification-F1 0.47826953748006373 on epoch=537
05/21/2022 11:41:22 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.66 on epoch=539
05/21/2022 11:41:24 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.63 on epoch=542
05/21/2022 11:41:25 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.63 on epoch=544
05/21/2022 11:41:27 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.66 on epoch=547
05/21/2022 11:41:28 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.71 on epoch=549
05/21/2022 11:41:29 - INFO - __main__ - Global step 2200 Train loss 0.66 Classification-F1 0.47596196373286154 on epoch=549
05/21/2022 11:41:30 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.72 on epoch=552
05/21/2022 11:41:31 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.66 on epoch=554
05/21/2022 11:41:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.66 on epoch=557
05/21/2022 11:41:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.64 on epoch=559
05/21/2022 11:41:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.67 on epoch=562
05/21/2022 11:41:36 - INFO - __main__ - Global step 2250 Train loss 0.67 Classification-F1 0.5218000243575691 on epoch=562
05/21/2022 11:41:36 - INFO - __main__ - Saving model with best Classification-F1: 0.5103174603174603 -> 0.5218000243575691 on epoch=562, global_step=2250
05/21/2022 11:41:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.68 on epoch=564
05/21/2022 11:41:39 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.70 on epoch=567
05/21/2022 11:41:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.68 on epoch=569
05/21/2022 11:41:41 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.64 on epoch=572
05/21/2022 11:41:43 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.69 on epoch=574
05/21/2022 11:41:43 - INFO - __main__ - Global step 2300 Train loss 0.68 Classification-F1 0.45149667405764965 on epoch=574
05/21/2022 11:41:44 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.64 on epoch=577
05/21/2022 11:41:46 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.65 on epoch=579
05/21/2022 11:41:47 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.69 on epoch=582
05/21/2022 11:41:49 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.65 on epoch=584
05/21/2022 11:41:50 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.58 on epoch=587
05/21/2022 11:41:51 - INFO - __main__ - Global step 2350 Train loss 0.64 Classification-F1 0.48713450292397664 on epoch=587
05/21/2022 11:41:52 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.68 on epoch=589
05/21/2022 11:41:54 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.63 on epoch=592
05/21/2022 11:41:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.63 on epoch=594
05/21/2022 11:41:56 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.62 on epoch=597
05/21/2022 11:41:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.60 on epoch=599
05/21/2022 11:41:58 - INFO - __main__ - Global step 2400 Train loss 0.63 Classification-F1 0.45467658327698895 on epoch=599
05/21/2022 11:42:00 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.60 on epoch=602
05/21/2022 11:42:01 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.56 on epoch=604
05/21/2022 11:42:02 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.59 on epoch=607
05/21/2022 11:42:04 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.62 on epoch=609
05/21/2022 11:42:05 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.54 on epoch=612
05/21/2022 11:42:06 - INFO - __main__ - Global step 2450 Train loss 0.58 Classification-F1 0.5586528326745719 on epoch=612
05/21/2022 11:42:06 - INFO - __main__ - Saving model with best Classification-F1: 0.5218000243575691 -> 0.5586528326745719 on epoch=612, global_step=2450
05/21/2022 11:42:07 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.62 on epoch=614
05/21/2022 11:42:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.55 on epoch=617
05/21/2022 11:42:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.58 on epoch=619
05/21/2022 11:42:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.63 on epoch=622
05/21/2022 11:42:13 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.63 on epoch=624
05/21/2022 11:42:14 - INFO - __main__ - Global step 2500 Train loss 0.60 Classification-F1 0.5548460144927536 on epoch=624
05/21/2022 11:42:15 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.57 on epoch=627
05/21/2022 11:42:17 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.63 on epoch=629
05/21/2022 11:42:18 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.57 on epoch=632
05/21/2022 11:42:20 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.70 on epoch=634
05/21/2022 11:42:21 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.62 on epoch=637
05/21/2022 11:42:22 - INFO - __main__ - Global step 2550 Train loss 0.62 Classification-F1 0.538949013949014 on epoch=637
05/21/2022 11:42:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.52 on epoch=639
05/21/2022 11:42:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.56 on epoch=642
05/21/2022 11:42:26 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.56 on epoch=644
05/21/2022 11:42:28 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.63 on epoch=647
05/21/2022 11:42:29 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.55 on epoch=649
05/21/2022 11:42:30 - INFO - __main__ - Global step 2600 Train loss 0.56 Classification-F1 0.4865216201423098 on epoch=649
05/21/2022 11:42:31 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.59 on epoch=652
05/21/2022 11:42:32 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.64 on epoch=654
05/21/2022 11:42:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.59 on epoch=657
05/21/2022 11:42:35 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.58 on epoch=659
05/21/2022 11:42:36 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.55 on epoch=662
05/21/2022 11:42:37 - INFO - __main__ - Global step 2650 Train loss 0.59 Classification-F1 0.5477063504446872 on epoch=662
05/21/2022 11:42:38 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.58 on epoch=664
05/21/2022 11:42:40 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.49 on epoch=667
05/21/2022 11:42:41 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.52 on epoch=669
05/21/2022 11:42:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.48 on epoch=672
05/21/2022 11:42:44 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.55 on epoch=674
05/21/2022 11:42:44 - INFO - __main__ - Global step 2700 Train loss 0.52 Classification-F1 0.5478571428571428 on epoch=674
05/21/2022 11:42:46 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.55 on epoch=677
05/21/2022 11:42:47 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.58 on epoch=679
05/21/2022 11:42:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.51 on epoch=682
05/21/2022 11:42:50 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.47 on epoch=684
05/21/2022 11:42:52 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.50 on epoch=687
05/21/2022 11:42:52 - INFO - __main__ - Global step 2750 Train loss 0.52 Classification-F1 0.5095424836601308 on epoch=687
05/21/2022 11:42:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.51 on epoch=689
05/21/2022 11:42:55 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.52 on epoch=692
05/21/2022 11:42:57 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.54 on epoch=694
05/21/2022 11:42:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.55 on epoch=697
05/21/2022 11:43:00 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.60 on epoch=699
05/21/2022 11:43:00 - INFO - __main__ - Global step 2800 Train loss 0.54 Classification-F1 0.524859943977591 on epoch=699
05/21/2022 11:43:02 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.41 on epoch=702
05/21/2022 11:43:03 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.49 on epoch=704
05/21/2022 11:43:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.49 on epoch=707
05/21/2022 11:43:05 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.55 on epoch=709
05/21/2022 11:43:07 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.51 on epoch=712
05/21/2022 11:43:07 - INFO - __main__ - Global step 2850 Train loss 0.49 Classification-F1 0.517153037357641 on epoch=712
05/21/2022 11:43:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.52 on epoch=714
05/21/2022 11:43:10 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.41 on epoch=717
05/21/2022 11:43:11 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.41 on epoch=719
05/21/2022 11:43:13 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.48 on epoch=722
05/21/2022 11:43:14 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.49 on epoch=724
05/21/2022 11:43:14 - INFO - __main__ - Global step 2900 Train loss 0.46 Classification-F1 0.5468494468494468 on epoch=724
05/21/2022 11:43:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.50 on epoch=727
05/21/2022 11:43:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.45 on epoch=729
05/21/2022 11:43:19 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.41 on epoch=732
05/21/2022 11:43:20 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.44 on epoch=734
05/21/2022 11:43:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.40 on epoch=737
05/21/2022 11:43:22 - INFO - __main__ - Global step 2950 Train loss 0.44 Classification-F1 0.5655735651084488 on epoch=737
05/21/2022 11:43:22 - INFO - __main__ - Saving model with best Classification-F1: 0.5586528326745719 -> 0.5655735651084488 on epoch=737, global_step=2950
05/21/2022 11:43:24 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.52 on epoch=739
05/21/2022 11:43:25 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.45 on epoch=742
05/21/2022 11:43:27 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.47 on epoch=744
05/21/2022 11:43:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.43 on epoch=747
05/21/2022 11:43:29 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.48 on epoch=749
05/21/2022 11:43:30 - INFO - __main__ - Global step 3000 Train loss 0.47 Classification-F1 0.5647865853658536 on epoch=749
05/21/2022 11:43:30 - INFO - __main__ - save last model!
05/21/2022 11:43:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 11:43:30 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 11:43:30 - INFO - __main__ - Printing 3 examples
05/21/2022 11:43:30 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 11:43:30 - INFO - __main__ - ['others']
05/21/2022 11:43:30 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 11:43:30 - INFO - __main__ - ['others']
05/21/2022 11:43:30 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 11:43:30 - INFO - __main__ - ['others']
05/21/2022 11:43:30 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:43:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:43:30 - INFO - __main__ - Printing 3 examples
05/21/2022 11:43:30 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/21/2022 11:43:30 - INFO - __main__ - ['others']
05/21/2022 11:43:30 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/21/2022 11:43:30 - INFO - __main__ - ['others']
05/21/2022 11:43:30 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/21/2022 11:43:30 - INFO - __main__ - ['others']
05/21/2022 11:43:30 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:43:31 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:43:31 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 11:43:31 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:43:31 - INFO - __main__ - Printing 3 examples
05/21/2022 11:43:31 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/21/2022 11:43:31 - INFO - __main__ - ['others']
05/21/2022 11:43:31 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/21/2022 11:43:31 - INFO - __main__ - ['others']
05/21/2022 11:43:31 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/21/2022 11:43:31 - INFO - __main__ - ['others']
05/21/2022 11:43:31 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:43:31 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:43:31 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 11:43:32 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:43:36 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 11:43:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 11:43:37 - INFO - __main__ - Starting training!
05/21/2022 11:43:38 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 11:44:24 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_87_0.5_8_predictions.txt
05/21/2022 11:44:24 - INFO - __main__ - Classification-F1 on test data: 0.1223
05/21/2022 11:44:25 - INFO - __main__ - prefix=emo_16_87, lr=0.5, bsz=8, dev_performance=0.5655735651084488, test_performance=0.12230081771829042
05/21/2022 11:44:25 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.4, bsz=8 ...
05/21/2022 11:44:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:44:26 - INFO - __main__ - Printing 3 examples
05/21/2022 11:44:26 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/21/2022 11:44:26 - INFO - __main__ - ['others']
05/21/2022 11:44:26 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/21/2022 11:44:26 - INFO - __main__ - ['others']
05/21/2022 11:44:26 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/21/2022 11:44:26 - INFO - __main__ - ['others']
05/21/2022 11:44:26 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:44:26 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:44:26 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 11:44:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:44:26 - INFO - __main__ - Printing 3 examples
05/21/2022 11:44:26 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/21/2022 11:44:26 - INFO - __main__ - ['others']
05/21/2022 11:44:26 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/21/2022 11:44:26 - INFO - __main__ - ['others']
05/21/2022 11:44:26 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/21/2022 11:44:26 - INFO - __main__ - ['others']
05/21/2022 11:44:26 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:44:26 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:44:26 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 11:44:32 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 11:44:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 11:44:32 - INFO - __main__ - Starting training!
05/21/2022 11:44:34 - INFO - __main__ - Step 10 Global step 10 Train loss 4.22 on epoch=2
05/21/2022 11:44:36 - INFO - __main__ - Step 20 Global step 20 Train loss 3.63 on epoch=4
05/21/2022 11:44:37 - INFO - __main__ - Step 30 Global step 30 Train loss 3.12 on epoch=7
05/21/2022 11:44:39 - INFO - __main__ - Step 40 Global step 40 Train loss 2.73 on epoch=9
05/21/2022 11:44:40 - INFO - __main__ - Step 50 Global step 50 Train loss 2.19 on epoch=12
05/21/2022 11:44:41 - INFO - __main__ - Global step 50 Train loss 3.17 Classification-F1 0.1 on epoch=12
05/21/2022 11:44:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/21/2022 11:44:42 - INFO - __main__ - Step 60 Global step 60 Train loss 1.86 on epoch=14
05/21/2022 11:44:43 - INFO - __main__ - Step 70 Global step 70 Train loss 1.68 on epoch=17
05/21/2022 11:44:45 - INFO - __main__ - Step 80 Global step 80 Train loss 1.33 on epoch=19
05/21/2022 11:44:46 - INFO - __main__ - Step 90 Global step 90 Train loss 1.31 on epoch=22
05/21/2022 11:44:47 - INFO - __main__ - Step 100 Global step 100 Train loss 1.03 on epoch=24
05/21/2022 11:44:48 - INFO - __main__ - Global step 100 Train loss 1.44 Classification-F1 0.1 on epoch=24
05/21/2022 11:44:49 - INFO - __main__ - Step 110 Global step 110 Train loss 1.16 on epoch=27
05/21/2022 11:44:50 - INFO - __main__ - Step 120 Global step 120 Train loss 1.11 on epoch=29
05/21/2022 11:44:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.99 on epoch=32
05/21/2022 11:44:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.98 on epoch=34
05/21/2022 11:44:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.98 on epoch=37
05/21/2022 11:44:55 - INFO - __main__ - Global step 150 Train loss 1.04 Classification-F1 0.1 on epoch=37
05/21/2022 11:44:57 - INFO - __main__ - Step 160 Global step 160 Train loss 1.01 on epoch=39
05/21/2022 11:44:58 - INFO - __main__ - Step 170 Global step 170 Train loss 1.04 on epoch=42
05/21/2022 11:45:00 - INFO - __main__ - Step 180 Global step 180 Train loss 1.02 on epoch=44
05/21/2022 11:45:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.93 on epoch=47
05/21/2022 11:45:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.98 on epoch=49
05/21/2022 11:45:03 - INFO - __main__ - Global step 200 Train loss 1.00 Classification-F1 0.1 on epoch=49
05/21/2022 11:45:04 - INFO - __main__ - Step 210 Global step 210 Train loss 1.05 on epoch=52
05/21/2022 11:45:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.96 on epoch=54
05/21/2022 11:45:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.94 on epoch=57
05/21/2022 11:45:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=59
05/21/2022 11:45:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.93 on epoch=62
05/21/2022 11:45:11 - INFO - __main__ - Global step 250 Train loss 0.95 Classification-F1 0.1476190476190476 on epoch=62
05/21/2022 11:45:11 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1476190476190476 on epoch=62, global_step=250
05/21/2022 11:45:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.98 on epoch=64
05/21/2022 11:45:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.95 on epoch=67
05/21/2022 11:45:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.94 on epoch=69
05/21/2022 11:45:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.83 on epoch=72
05/21/2022 11:45:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.87 on epoch=74
05/21/2022 11:45:18 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.1 on epoch=74
05/21/2022 11:45:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.93 on epoch=77
05/21/2022 11:45:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.97 on epoch=79
05/21/2022 11:45:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.94 on epoch=82
05/21/2022 11:45:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.93 on epoch=84
05/21/2022 11:45:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.93 on epoch=87
05/21/2022 11:45:26 - INFO - __main__ - Global step 350 Train loss 0.94 Classification-F1 0.13067758749069247 on epoch=87
05/21/2022 11:45:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.88 on epoch=89
05/21/2022 11:45:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.95 on epoch=92
05/21/2022 11:45:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.99 on epoch=94
05/21/2022 11:45:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.90 on epoch=97
05/21/2022 11:45:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.96 on epoch=99
05/21/2022 11:45:34 - INFO - __main__ - Global step 400 Train loss 0.93 Classification-F1 0.1 on epoch=99
05/21/2022 11:45:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.85 on epoch=102
05/21/2022 11:45:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.96 on epoch=104
05/21/2022 11:45:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.93 on epoch=107
05/21/2022 11:45:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.90 on epoch=109
05/21/2022 11:45:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.87 on epoch=112
05/21/2022 11:45:41 - INFO - __main__ - Global step 450 Train loss 0.90 Classification-F1 0.10126582278481013 on epoch=112
05/21/2022 11:45:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.91 on epoch=114
05/21/2022 11:45:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.93 on epoch=117
05/21/2022 11:45:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.87 on epoch=119
05/21/2022 11:45:47 - INFO - __main__ - Step 490 Global step 490 Train loss 1.04 on epoch=122
05/21/2022 11:45:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.92 on epoch=124
05/21/2022 11:45:49 - INFO - __main__ - Global step 500 Train loss 0.93 Classification-F1 0.1302118933697881 on epoch=124
05/21/2022 11:45:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.79 on epoch=127
05/21/2022 11:45:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.89 on epoch=129
05/21/2022 11:45:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.87 on epoch=132
05/21/2022 11:45:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.91 on epoch=134
05/21/2022 11:45:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.93 on epoch=137
05/21/2022 11:45:56 - INFO - __main__ - Global step 550 Train loss 0.88 Classification-F1 0.15620915032679739 on epoch=137
05/21/2022 11:45:56 - INFO - __main__ - Saving model with best Classification-F1: 0.1476190476190476 -> 0.15620915032679739 on epoch=137, global_step=550
05/21/2022 11:45:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.94 on epoch=139
05/21/2022 11:45:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.88 on epoch=142
05/21/2022 11:46:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.83 on epoch=144
05/21/2022 11:46:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.85 on epoch=147
05/21/2022 11:46:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.86 on epoch=149
05/21/2022 11:46:04 - INFO - __main__ - Global step 600 Train loss 0.87 Classification-F1 0.09493670886075949 on epoch=149
05/21/2022 11:46:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.84 on epoch=152
05/21/2022 11:46:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.86 on epoch=154
05/21/2022 11:46:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.84 on epoch=157
05/21/2022 11:46:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.86 on epoch=159
05/21/2022 11:46:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.89 on epoch=162
05/21/2022 11:46:11 - INFO - __main__ - Global step 650 Train loss 0.86 Classification-F1 0.1 on epoch=162
05/21/2022 11:46:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.80 on epoch=164
05/21/2022 11:46:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.86 on epoch=167
05/21/2022 11:46:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.85 on epoch=169
05/21/2022 11:46:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.89 on epoch=172
05/21/2022 11:46:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.81 on epoch=174
05/21/2022 11:46:18 - INFO - __main__ - Global step 700 Train loss 0.84 Classification-F1 0.1 on epoch=174
05/21/2022 11:46:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.80 on epoch=177
05/21/2022 11:46:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.78 on epoch=179
05/21/2022 11:46:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.82 on epoch=182
05/21/2022 11:46:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.82 on epoch=184
05/21/2022 11:46:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.88 on epoch=187
05/21/2022 11:46:26 - INFO - __main__ - Global step 750 Train loss 0.82 Classification-F1 0.13067758749069247 on epoch=187
05/21/2022 11:46:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.86 on epoch=189
05/21/2022 11:46:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.83 on epoch=192
05/21/2022 11:46:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.84 on epoch=194
05/21/2022 11:46:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.78 on epoch=197
05/21/2022 11:46:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.89 on epoch=199
05/21/2022 11:46:34 - INFO - __main__ - Global step 800 Train loss 0.84 Classification-F1 0.13067758749069247 on epoch=199
05/21/2022 11:46:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.86 on epoch=202
05/21/2022 11:46:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.88 on epoch=204
05/21/2022 11:46:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.81 on epoch=207
05/21/2022 11:46:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.83 on epoch=209
05/21/2022 11:46:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.83 on epoch=212
05/21/2022 11:46:41 - INFO - __main__ - Global step 850 Train loss 0.84 Classification-F1 0.1 on epoch=212
05/21/2022 11:46:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.84 on epoch=214
05/21/2022 11:46:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.78 on epoch=217
05/21/2022 11:46:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.86 on epoch=219
05/21/2022 11:46:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.88 on epoch=222
05/21/2022 11:46:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.78 on epoch=224
05/21/2022 11:46:49 - INFO - __main__ - Global step 900 Train loss 0.83 Classification-F1 0.1 on epoch=224
05/21/2022 11:46:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.82 on epoch=227
05/21/2022 11:46:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.82 on epoch=229
05/21/2022 11:46:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.90 on epoch=232
05/21/2022 11:46:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.88 on epoch=234
05/21/2022 11:46:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.84 on epoch=237
05/21/2022 11:46:56 - INFO - __main__ - Global step 950 Train loss 0.85 Classification-F1 0.1 on epoch=237
05/21/2022 11:46:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.82 on epoch=239
05/21/2022 11:46:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.81 on epoch=242
05/21/2022 11:47:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.82 on epoch=244
05/21/2022 11:47:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.90 on epoch=247
05/21/2022 11:47:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.88 on epoch=249
05/21/2022 11:47:04 - INFO - __main__ - Global step 1000 Train loss 0.85 Classification-F1 0.1 on epoch=249
05/21/2022 11:47:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.82 on epoch=252
05/21/2022 11:47:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.84 on epoch=254
05/21/2022 11:47:08 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.85 on epoch=257
05/21/2022 11:47:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.80 on epoch=259
05/21/2022 11:47:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.83 on epoch=262
05/21/2022 11:47:12 - INFO - __main__ - Global step 1050 Train loss 0.83 Classification-F1 0.1 on epoch=262
05/21/2022 11:47:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.84 on epoch=264
05/21/2022 11:47:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.77 on epoch=267
05/21/2022 11:47:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.80 on epoch=269
05/21/2022 11:47:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.87 on epoch=272
05/21/2022 11:47:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.75 on epoch=274
05/21/2022 11:47:20 - INFO - __main__ - Global step 1100 Train loss 0.80 Classification-F1 0.1500341763499658 on epoch=274
05/21/2022 11:47:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.83 on epoch=277
05/21/2022 11:47:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.81 on epoch=279
05/21/2022 11:47:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.77 on epoch=282
05/21/2022 11:47:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.76 on epoch=284
05/21/2022 11:47:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.81 on epoch=287
05/21/2022 11:47:26 - INFO - __main__ - Global step 1150 Train loss 0.80 Classification-F1 0.1 on epoch=287
05/21/2022 11:47:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.78 on epoch=289
05/21/2022 11:47:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.81 on epoch=292
05/21/2022 11:47:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.85 on epoch=294
05/21/2022 11:47:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.81 on epoch=297
05/21/2022 11:47:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.81 on epoch=299
05/21/2022 11:47:34 - INFO - __main__ - Global step 1200 Train loss 0.81 Classification-F1 0.10126582278481013 on epoch=299
05/21/2022 11:47:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.83 on epoch=302
05/21/2022 11:47:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.76 on epoch=304
05/21/2022 11:47:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.84 on epoch=307
05/21/2022 11:47:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.83 on epoch=309
05/21/2022 11:47:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.84 on epoch=312
05/21/2022 11:47:42 - INFO - __main__ - Global step 1250 Train loss 0.82 Classification-F1 0.1 on epoch=312
05/21/2022 11:47:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.85 on epoch=314
05/21/2022 11:47:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.76 on epoch=317
05/21/2022 11:47:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.80 on epoch=319
05/21/2022 11:47:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.85 on epoch=322
05/21/2022 11:47:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.79 on epoch=324
05/21/2022 11:47:50 - INFO - __main__ - Global step 1300 Train loss 0.81 Classification-F1 0.1 on epoch=324
05/21/2022 11:47:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.83 on epoch=327
05/21/2022 11:47:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.81 on epoch=329
05/21/2022 11:47:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.80 on epoch=332
05/21/2022 11:47:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.78 on epoch=334
05/21/2022 11:47:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.80 on epoch=337
05/21/2022 11:47:57 - INFO - __main__ - Global step 1350 Train loss 0.81 Classification-F1 0.1 on epoch=337
05/21/2022 11:47:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.75 on epoch=339
05/21/2022 11:48:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.75 on epoch=342
05/21/2022 11:48:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.80 on epoch=344
05/21/2022 11:48:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.81 on epoch=347
05/21/2022 11:48:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.83 on epoch=349
05/21/2022 11:48:04 - INFO - __main__ - Global step 1400 Train loss 0.79 Classification-F1 0.1 on epoch=349
05/21/2022 11:48:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.85 on epoch=352
05/21/2022 11:48:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.79 on epoch=354
05/21/2022 11:48:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.82 on epoch=357
05/21/2022 11:48:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.81 on epoch=359
05/21/2022 11:48:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.78 on epoch=362
05/21/2022 11:48:12 - INFO - __main__ - Global step 1450 Train loss 0.81 Classification-F1 0.1 on epoch=362
05/21/2022 11:48:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.81 on epoch=364
05/21/2022 11:48:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.82 on epoch=367
05/21/2022 11:48:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.78 on epoch=369
05/21/2022 11:48:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.85 on epoch=372
05/21/2022 11:48:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.78 on epoch=374
05/21/2022 11:48:19 - INFO - __main__ - Global step 1500 Train loss 0.81 Classification-F1 0.1 on epoch=374
05/21/2022 11:48:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.79 on epoch=377
05/21/2022 11:48:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.83 on epoch=379
05/21/2022 11:48:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.81 on epoch=382
05/21/2022 11:48:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.83 on epoch=384
05/21/2022 11:48:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.75 on epoch=387
05/21/2022 11:48:26 - INFO - __main__ - Global step 1550 Train loss 0.80 Classification-F1 0.14054336468129572 on epoch=387
05/21/2022 11:48:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.77 on epoch=389
05/21/2022 11:48:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.82 on epoch=392
05/21/2022 11:48:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.83 on epoch=394
05/21/2022 11:48:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.86 on epoch=397
05/21/2022 11:48:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.76 on epoch=399
05/21/2022 11:48:34 - INFO - __main__ - Global step 1600 Train loss 0.81 Classification-F1 0.1 on epoch=399
05/21/2022 11:48:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.81 on epoch=402
05/21/2022 11:48:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.87 on epoch=404
05/21/2022 11:48:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.79 on epoch=407
05/21/2022 11:48:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.84 on epoch=409
05/21/2022 11:48:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.80 on epoch=412
05/21/2022 11:48:40 - INFO - __main__ - Global step 1650 Train loss 0.82 Classification-F1 0.1697802197802198 on epoch=412
05/21/2022 11:48:40 - INFO - __main__ - Saving model with best Classification-F1: 0.15620915032679739 -> 0.1697802197802198 on epoch=412, global_step=1650
05/21/2022 11:48:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.79 on epoch=414
05/21/2022 11:48:43 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.74 on epoch=417
05/21/2022 11:48:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.83 on epoch=419
05/21/2022 11:48:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.78 on epoch=422
05/21/2022 11:48:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.82 on epoch=424
05/21/2022 11:48:48 - INFO - __main__ - Global step 1700 Train loss 0.79 Classification-F1 0.1 on epoch=424
05/21/2022 11:48:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.79 on epoch=427
05/21/2022 11:48:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.78 on epoch=429
05/21/2022 11:48:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.77 on epoch=432
05/21/2022 11:48:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.78 on epoch=434
05/21/2022 11:48:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.75 on epoch=437
05/21/2022 11:48:55 - INFO - __main__ - Global step 1750 Train loss 0.77 Classification-F1 0.16666666666666669 on epoch=437
05/21/2022 11:48:56 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.84 on epoch=439
05/21/2022 11:48:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.82 on epoch=442
05/21/2022 11:48:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.76 on epoch=444
05/21/2022 11:49:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.87 on epoch=447
05/21/2022 11:49:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.77 on epoch=449
05/21/2022 11:49:03 - INFO - __main__ - Global step 1800 Train loss 0.81 Classification-F1 0.10666666666666667 on epoch=449
05/21/2022 11:49:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.79 on epoch=452
05/21/2022 11:49:06 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.80 on epoch=454
05/21/2022 11:49:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.79 on epoch=457
05/21/2022 11:49:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.80 on epoch=459
05/21/2022 11:49:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.83 on epoch=462
05/21/2022 11:49:10 - INFO - __main__ - Global step 1850 Train loss 0.80 Classification-F1 0.1 on epoch=462
05/21/2022 11:49:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.83 on epoch=464
05/21/2022 11:49:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.78 on epoch=467
05/21/2022 11:49:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.81 on epoch=469
05/21/2022 11:49:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.76 on epoch=472
05/21/2022 11:49:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.83 on epoch=474
05/21/2022 11:49:18 - INFO - __main__ - Global step 1900 Train loss 0.80 Classification-F1 0.1 on epoch=474
05/21/2022 11:49:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.76 on epoch=477
05/21/2022 11:49:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.81 on epoch=479
05/21/2022 11:49:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.85 on epoch=482
05/21/2022 11:49:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.75 on epoch=484
05/21/2022 11:49:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.80 on epoch=487
05/21/2022 11:49:25 - INFO - __main__ - Global step 1950 Train loss 0.80 Classification-F1 0.15490196078431373 on epoch=487
05/21/2022 11:49:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.73 on epoch=489
05/21/2022 11:49:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.82 on epoch=492
05/21/2022 11:49:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.73 on epoch=494
05/21/2022 11:49:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.75 on epoch=497
05/21/2022 11:49:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.79 on epoch=499
05/21/2022 11:49:33 - INFO - __main__ - Global step 2000 Train loss 0.77 Classification-F1 0.10256410256410256 on epoch=499
05/21/2022 11:49:35 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.83 on epoch=502
05/21/2022 11:49:36 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.71 on epoch=504
05/21/2022 11:49:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.74 on epoch=507
05/21/2022 11:49:39 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.81 on epoch=509
05/21/2022 11:49:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.79 on epoch=512
05/21/2022 11:49:41 - INFO - __main__ - Global step 2050 Train loss 0.78 Classification-F1 0.17480643240023822 on epoch=512
05/21/2022 11:49:41 - INFO - __main__ - Saving model with best Classification-F1: 0.1697802197802198 -> 0.17480643240023822 on epoch=512, global_step=2050
05/21/2022 11:49:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.79 on epoch=514
05/21/2022 11:49:44 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.73 on epoch=517
05/21/2022 11:49:46 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.79 on epoch=519
05/21/2022 11:49:47 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.78 on epoch=522
05/21/2022 11:49:49 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.80 on epoch=524
05/21/2022 11:49:50 - INFO - __main__ - Global step 2100 Train loss 0.78 Classification-F1 0.10126582278481013 on epoch=524
05/21/2022 11:49:51 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.73 on epoch=527
05/21/2022 11:49:52 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.75 on epoch=529
05/21/2022 11:49:54 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.80 on epoch=532
05/21/2022 11:49:55 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.80 on epoch=534
05/21/2022 11:49:56 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.75 on epoch=537
05/21/2022 11:49:57 - INFO - __main__ - Global step 2150 Train loss 0.77 Classification-F1 0.1 on epoch=537
05/21/2022 11:49:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.75 on epoch=539
05/21/2022 11:49:59 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.71 on epoch=542
05/21/2022 11:50:01 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.81 on epoch=544
05/21/2022 11:50:02 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.77 on epoch=547
05/21/2022 11:50:03 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.75 on epoch=549
05/21/2022 11:50:04 - INFO - __main__ - Global step 2200 Train loss 0.76 Classification-F1 0.13034188034188032 on epoch=549
05/21/2022 11:50:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.74 on epoch=552
05/21/2022 11:50:07 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.75 on epoch=554
05/21/2022 11:50:08 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.74 on epoch=557
05/21/2022 11:50:10 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.78 on epoch=559
05/21/2022 11:50:11 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.79 on epoch=562
05/21/2022 11:50:12 - INFO - __main__ - Global step 2250 Train loss 0.76 Classification-F1 0.21554834054834054 on epoch=562
05/21/2022 11:50:12 - INFO - __main__ - Saving model with best Classification-F1: 0.17480643240023822 -> 0.21554834054834054 on epoch=562, global_step=2250
05/21/2022 11:50:13 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.79 on epoch=564
05/21/2022 11:50:14 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.75 on epoch=567
05/21/2022 11:50:16 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.82 on epoch=569
05/21/2022 11:50:17 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.73 on epoch=572
05/21/2022 11:50:18 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.70 on epoch=574
05/21/2022 11:50:19 - INFO - __main__ - Global step 2300 Train loss 0.76 Classification-F1 0.18445535125631823 on epoch=574
05/21/2022 11:50:20 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.77 on epoch=577
05/21/2022 11:50:21 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.78 on epoch=579
05/21/2022 11:50:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.76 on epoch=582
05/21/2022 11:50:24 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.71 on epoch=584
05/21/2022 11:50:25 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.72 on epoch=587
05/21/2022 11:50:26 - INFO - __main__ - Global step 2350 Train loss 0.75 Classification-F1 0.31845238095238093 on epoch=587
05/21/2022 11:50:26 - INFO - __main__ - Saving model with best Classification-F1: 0.21554834054834054 -> 0.31845238095238093 on epoch=587, global_step=2350
05/21/2022 11:50:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.72 on epoch=589
05/21/2022 11:50:29 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.76 on epoch=592
05/21/2022 11:50:30 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.74 on epoch=594
05/21/2022 11:50:32 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.74 on epoch=597
05/21/2022 11:50:33 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.73 on epoch=599
05/21/2022 11:50:34 - INFO - __main__ - Global step 2400 Train loss 0.74 Classification-F1 0.20974025974025973 on epoch=599
05/21/2022 11:50:35 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.79 on epoch=602
05/21/2022 11:50:36 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.78 on epoch=604
05/21/2022 11:50:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.74 on epoch=607
05/21/2022 11:50:39 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.77 on epoch=609
05/21/2022 11:50:40 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.72 on epoch=612
05/21/2022 11:50:41 - INFO - __main__ - Global step 2450 Train loss 0.76 Classification-F1 0.29098583877995643 on epoch=612
05/21/2022 11:50:42 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.73 on epoch=614
05/21/2022 11:50:44 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.77 on epoch=617
05/21/2022 11:50:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.78 on epoch=619
05/21/2022 11:50:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.84 on epoch=622
05/21/2022 11:50:48 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.73 on epoch=624
05/21/2022 11:50:48 - INFO - __main__ - Global step 2500 Train loss 0.77 Classification-F1 0.2535156044813639 on epoch=624
05/21/2022 11:50:50 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.78 on epoch=627
05/21/2022 11:50:51 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.77 on epoch=629
05/21/2022 11:50:53 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.73 on epoch=632
05/21/2022 11:50:54 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.76 on epoch=634
05/21/2022 11:50:55 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.73 on epoch=637
05/21/2022 11:50:56 - INFO - __main__ - Global step 2550 Train loss 0.75 Classification-F1 0.40428276573787414 on epoch=637
05/21/2022 11:50:56 - INFO - __main__ - Saving model with best Classification-F1: 0.31845238095238093 -> 0.40428276573787414 on epoch=637, global_step=2550
05/21/2022 11:50:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.76 on epoch=639
05/21/2022 11:50:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.70 on epoch=642
05/21/2022 11:51:00 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.73 on epoch=644
05/21/2022 11:51:01 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.73 on epoch=647
05/21/2022 11:51:02 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.76 on epoch=649
05/21/2022 11:51:03 - INFO - __main__ - Global step 2600 Train loss 0.74 Classification-F1 0.38361230671130236 on epoch=649
05/21/2022 11:51:04 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.76 on epoch=652
05/21/2022 11:51:06 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.71 on epoch=654
05/21/2022 11:51:07 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.78 on epoch=657
05/21/2022 11:51:08 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.71 on epoch=659
05/21/2022 11:51:10 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.75 on epoch=662
05/21/2022 11:51:10 - INFO - __main__ - Global step 2650 Train loss 0.74 Classification-F1 0.46142686804451505 on epoch=662
05/21/2022 11:51:10 - INFO - __main__ - Saving model with best Classification-F1: 0.40428276573787414 -> 0.46142686804451505 on epoch=662, global_step=2650
05/21/2022 11:51:12 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.70 on epoch=664
05/21/2022 11:51:13 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.70 on epoch=667
05/21/2022 11:51:15 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.70 on epoch=669
05/21/2022 11:51:16 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.78 on epoch=672
05/21/2022 11:51:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.72 on epoch=674
05/21/2022 11:51:18 - INFO - __main__ - Global step 2700 Train loss 0.72 Classification-F1 0.4291600484587428 on epoch=674
05/21/2022 11:51:20 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.68 on epoch=677
05/21/2022 11:51:21 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.70 on epoch=679
05/21/2022 11:51:22 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.70 on epoch=682
05/21/2022 11:51:24 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.71 on epoch=684
05/21/2022 11:51:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.74 on epoch=687
05/21/2022 11:51:26 - INFO - __main__ - Global step 2750 Train loss 0.70 Classification-F1 0.42410572337042923 on epoch=687
05/21/2022 11:51:27 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.75 on epoch=689
05/21/2022 11:51:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.72 on epoch=692
05/21/2022 11:51:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.76 on epoch=694
05/21/2022 11:51:32 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.73 on epoch=697
05/21/2022 11:51:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.82 on epoch=699
05/21/2022 11:51:34 - INFO - __main__ - Global step 2800 Train loss 0.76 Classification-F1 0.40140096618357496 on epoch=699
05/21/2022 11:51:36 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.74 on epoch=702
05/21/2022 11:51:37 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.65 on epoch=704
05/21/2022 11:51:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.71 on epoch=707
05/21/2022 11:51:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.71 on epoch=709
05/21/2022 11:51:41 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.74 on epoch=712
05/21/2022 11:51:42 - INFO - __main__ - Global step 2850 Train loss 0.71 Classification-F1 0.4199393114834291 on epoch=712
05/21/2022 11:51:43 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.69 on epoch=714
05/21/2022 11:51:45 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.75 on epoch=717
05/21/2022 11:51:46 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.71 on epoch=719
05/21/2022 11:51:48 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.72 on epoch=722
05/21/2022 11:51:49 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.74 on epoch=724
05/21/2022 11:51:50 - INFO - __main__ - Global step 2900 Train loss 0.72 Classification-F1 0.43405506549051054 on epoch=724
05/21/2022 11:51:52 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.69 on epoch=727
05/21/2022 11:51:53 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.73 on epoch=729
05/21/2022 11:51:55 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.67 on epoch=732
05/21/2022 11:51:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.70 on epoch=734
05/21/2022 11:51:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.68 on epoch=737
05/21/2022 11:51:58 - INFO - __main__ - Global step 2950 Train loss 0.69 Classification-F1 0.40733442802408315 on epoch=737
05/21/2022 11:52:00 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.71 on epoch=739
05/21/2022 11:52:01 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.72 on epoch=742
05/21/2022 11:52:03 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.71 on epoch=744
05/21/2022 11:52:04 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.68 on epoch=747
05/21/2022 11:52:06 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.66 on epoch=749
05/21/2022 11:52:07 - INFO - __main__ - Global step 3000 Train loss 0.70 Classification-F1 0.44435286935286933 on epoch=749
05/21/2022 11:52:07 - INFO - __main__ - save last model!
05/21/2022 11:52:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 11:52:07 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 11:52:07 - INFO - __main__ - Printing 3 examples
05/21/2022 11:52:07 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 11:52:07 - INFO - __main__ - ['others']
05/21/2022 11:52:07 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 11:52:07 - INFO - __main__ - ['others']
05/21/2022 11:52:07 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 11:52:07 - INFO - __main__ - ['others']
05/21/2022 11:52:07 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:52:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:52:07 - INFO - __main__ - Printing 3 examples
05/21/2022 11:52:07 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/21/2022 11:52:07 - INFO - __main__ - ['others']
05/21/2022 11:52:07 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/21/2022 11:52:07 - INFO - __main__ - ['others']
05/21/2022 11:52:07 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/21/2022 11:52:07 - INFO - __main__ - ['others']
05/21/2022 11:52:07 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:52:07 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:52:07 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 11:52:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:52:07 - INFO - __main__ - Printing 3 examples
05/21/2022 11:52:07 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/21/2022 11:52:07 - INFO - __main__ - ['others']
05/21/2022 11:52:07 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/21/2022 11:52:07 - INFO - __main__ - ['others']
05/21/2022 11:52:07 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/21/2022 11:52:07 - INFO - __main__ - ['others']
05/21/2022 11:52:07 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:52:07 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:52:07 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 11:52:09 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:52:14 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 11:52:14 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 11:52:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 11:52:14 - INFO - __main__ - Starting training!
05/21/2022 11:52:59 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_87_0.4_8_predictions.txt
05/21/2022 11:52:59 - INFO - __main__ - Classification-F1 on test data: 0.2629
05/21/2022 11:52:59 - INFO - __main__ - prefix=emo_16_87, lr=0.4, bsz=8, dev_performance=0.46142686804451505, test_performance=0.26286077165415145
05/21/2022 11:52:59 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.3, bsz=8 ...
05/21/2022 11:53:00 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:53:00 - INFO - __main__ - Printing 3 examples
05/21/2022 11:53:00 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/21/2022 11:53:00 - INFO - __main__ - ['others']
05/21/2022 11:53:00 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/21/2022 11:53:00 - INFO - __main__ - ['others']
05/21/2022 11:53:00 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/21/2022 11:53:00 - INFO - __main__ - ['others']
05/21/2022 11:53:00 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:53:00 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:53:00 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 11:53:00 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 11:53:00 - INFO - __main__ - Printing 3 examples
05/21/2022 11:53:00 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/21/2022 11:53:00 - INFO - __main__ - ['others']
05/21/2022 11:53:00 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/21/2022 11:53:00 - INFO - __main__ - ['others']
05/21/2022 11:53:00 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/21/2022 11:53:00 - INFO - __main__ - ['others']
05/21/2022 11:53:00 - INFO - __main__ - Tokenizing Input ...
05/21/2022 11:53:00 - INFO - __main__ - Tokenizing Output ...
05/21/2022 11:53:00 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 11:53:07 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 11:53:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 11:53:07 - INFO - __main__ - Starting training!
05/21/2022 11:53:09 - INFO - __main__ - Step 10 Global step 10 Train loss 4.24 on epoch=2
05/21/2022 11:53:10 - INFO - __main__ - Step 20 Global step 20 Train loss 3.92 on epoch=4
05/21/2022 11:53:12 - INFO - __main__ - Step 30 Global step 30 Train loss 3.44 on epoch=7
05/21/2022 11:53:13 - INFO - __main__ - Step 40 Global step 40 Train loss 3.12 on epoch=9
05/21/2022 11:53:15 - INFO - __main__ - Step 50 Global step 50 Train loss 2.59 on epoch=12
05/21/2022 11:53:16 - INFO - __main__ - Global step 50 Train loss 3.46 Classification-F1 0.1 on epoch=12
05/21/2022 11:53:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/21/2022 11:53:17 - INFO - __main__ - Step 60 Global step 60 Train loss 2.28 on epoch=14
05/21/2022 11:53:18 - INFO - __main__ - Step 70 Global step 70 Train loss 2.08 on epoch=17
05/21/2022 11:53:19 - INFO - __main__ - Step 80 Global step 80 Train loss 1.71 on epoch=19
05/21/2022 11:53:21 - INFO - __main__ - Step 90 Global step 90 Train loss 1.60 on epoch=22
05/21/2022 11:53:22 - INFO - __main__ - Step 100 Global step 100 Train loss 1.39 on epoch=24
05/21/2022 11:53:23 - INFO - __main__ - Global step 100 Train loss 1.81 Classification-F1 0.13034188034188032 on epoch=24
05/21/2022 11:53:23 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.13034188034188032 on epoch=24, global_step=100
05/21/2022 11:53:24 - INFO - __main__ - Step 110 Global step 110 Train loss 1.35 on epoch=27
05/21/2022 11:53:25 - INFO - __main__ - Step 120 Global step 120 Train loss 1.28 on epoch=29
05/21/2022 11:53:26 - INFO - __main__ - Step 130 Global step 130 Train loss 1.25 on epoch=32
05/21/2022 11:53:28 - INFO - __main__ - Step 140 Global step 140 Train loss 1.09 on epoch=34
05/21/2022 11:53:29 - INFO - __main__ - Step 150 Global step 150 Train loss 1.06 on epoch=37
05/21/2022 11:53:30 - INFO - __main__ - Global step 150 Train loss 1.21 Classification-F1 0.2813816127375449 on epoch=37
05/21/2022 11:53:30 - INFO - __main__ - Saving model with best Classification-F1: 0.13034188034188032 -> 0.2813816127375449 on epoch=37, global_step=150
05/21/2022 11:53:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.97 on epoch=39
05/21/2022 11:53:32 - INFO - __main__ - Step 170 Global step 170 Train loss 1.04 on epoch=42
05/21/2022 11:53:34 - INFO - __main__ - Step 180 Global step 180 Train loss 1.03 on epoch=44
05/21/2022 11:53:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.92 on epoch=47
05/21/2022 11:53:37 - INFO - __main__ - Step 200 Global step 200 Train loss 1.02 on epoch=49
05/21/2022 11:53:37 - INFO - __main__ - Global step 200 Train loss 0.99 Classification-F1 0.1 on epoch=49
05/21/2022 11:53:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.93 on epoch=52
05/21/2022 11:53:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.98 on epoch=54
05/21/2022 11:53:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.90 on epoch=57
05/21/2022 11:53:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.93 on epoch=59
05/21/2022 11:53:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.91 on epoch=62
05/21/2022 11:53:45 - INFO - __main__ - Global step 250 Train loss 0.93 Classification-F1 0.1 on epoch=62
05/21/2022 11:53:46 - INFO - __main__ - Step 260 Global step 260 Train loss 1.02 on epoch=64
05/21/2022 11:53:47 - INFO - __main__ - Step 270 Global step 270 Train loss 1.02 on epoch=67
05/21/2022 11:53:49 - INFO - __main__ - Step 280 Global step 280 Train loss 1.07 on epoch=69
05/21/2022 11:53:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.98 on epoch=72
05/21/2022 11:53:52 - INFO - __main__ - Step 300 Global step 300 Train loss 1.07 on epoch=74
05/21/2022 11:53:52 - INFO - __main__ - Global step 300 Train loss 1.03 Classification-F1 0.1 on epoch=74
05/21/2022 11:53:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.99 on epoch=77
05/21/2022 11:53:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.86 on epoch=79
05/21/2022 11:53:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.89 on epoch=82
05/21/2022 11:53:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.92 on epoch=84
05/21/2022 11:53:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.92 on epoch=87
05/21/2022 11:54:00 - INFO - __main__ - Global step 350 Train loss 0.92 Classification-F1 0.13067758749069247 on epoch=87
05/21/2022 11:54:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.86 on epoch=89
05/21/2022 11:54:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.89 on epoch=92
05/21/2022 11:54:04 - INFO - __main__ - Step 380 Global step 380 Train loss 1.05 on epoch=94
05/21/2022 11:54:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.92 on epoch=97
05/21/2022 11:54:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.91 on epoch=99
05/21/2022 11:54:08 - INFO - __main__ - Global step 400 Train loss 0.93 Classification-F1 0.19973009446693657 on epoch=99
05/21/2022 11:54:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.87 on epoch=102
05/21/2022 11:54:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.98 on epoch=104
05/21/2022 11:54:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.90 on epoch=107
05/21/2022 11:54:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.94 on epoch=109
05/21/2022 11:54:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.82 on epoch=112
05/21/2022 11:54:15 - INFO - __main__ - Global step 450 Train loss 0.90 Classification-F1 0.1 on epoch=112
05/21/2022 11:54:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.87 on epoch=114
05/21/2022 11:54:17 - INFO - __main__ - Step 470 Global step 470 Train loss 1.01 on epoch=117
05/21/2022 11:54:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.97 on epoch=119
05/21/2022 11:54:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.97 on epoch=122
05/21/2022 11:54:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.93 on epoch=124
05/21/2022 11:54:22 - INFO - __main__ - Global step 500 Train loss 0.95 Classification-F1 0.1 on epoch=124
05/21/2022 11:54:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.85 on epoch=127
05/21/2022 11:54:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.84 on epoch=129
05/21/2022 11:54:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.86 on epoch=132
05/21/2022 11:54:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.92 on epoch=134
05/21/2022 11:54:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.86 on epoch=137
05/21/2022 11:54:29 - INFO - __main__ - Global step 550 Train loss 0.87 Classification-F1 0.1 on epoch=137
05/21/2022 11:54:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.97 on epoch=139
05/21/2022 11:54:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.91 on epoch=142
05/21/2022 11:54:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.94 on epoch=144
05/21/2022 11:54:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.83 on epoch=147
05/21/2022 11:54:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.89 on epoch=149
05/21/2022 11:54:36 - INFO - __main__ - Global step 600 Train loss 0.91 Classification-F1 0.1 on epoch=149
05/21/2022 11:54:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.93 on epoch=152
05/21/2022 11:54:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.86 on epoch=154
05/21/2022 11:54:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.91 on epoch=157
05/21/2022 11:54:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.84 on epoch=159
05/21/2022 11:54:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.83 on epoch=162
05/21/2022 11:54:43 - INFO - __main__ - Global step 650 Train loss 0.87 Classification-F1 0.19542483660130716 on epoch=162
05/21/2022 11:54:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.85 on epoch=164
05/21/2022 11:54:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.88 on epoch=167
05/21/2022 11:54:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.85 on epoch=169
05/21/2022 11:54:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.86 on epoch=172
05/21/2022 11:54:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.87 on epoch=174
05/21/2022 11:54:50 - INFO - __main__ - Global step 700 Train loss 0.86 Classification-F1 0.1 on epoch=174
05/21/2022 11:54:51 - INFO - __main__ - Step 710 Global step 710 Train loss 0.88 on epoch=177
05/21/2022 11:54:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.79 on epoch=179
05/21/2022 11:54:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.85 on epoch=182
05/21/2022 11:54:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.84 on epoch=184
05/21/2022 11:54:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.92 on epoch=187
05/21/2022 11:54:57 - INFO - __main__ - Global step 750 Train loss 0.85 Classification-F1 0.10389610389610389 on epoch=187
05/21/2022 11:54:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.86 on epoch=189
05/21/2022 11:55:00 - INFO - __main__ - Step 770 Global step 770 Train loss 0.84 on epoch=192
05/21/2022 11:55:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.85 on epoch=194
05/21/2022 11:55:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.85 on epoch=197
05/21/2022 11:55:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.85 on epoch=199
05/21/2022 11:55:05 - INFO - __main__ - Global step 800 Train loss 0.85 Classification-F1 0.1 on epoch=199
05/21/2022 11:55:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.88 on epoch=202
05/21/2022 11:55:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.78 on epoch=204
05/21/2022 11:55:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=207
05/21/2022 11:55:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.86 on epoch=209
05/21/2022 11:55:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.89 on epoch=212
05/21/2022 11:55:13 - INFO - __main__ - Global step 850 Train loss 0.85 Classification-F1 0.1 on epoch=212
05/21/2022 11:55:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.90 on epoch=214
05/21/2022 11:55:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.92 on epoch=217
05/21/2022 11:55:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.87 on epoch=219
05/21/2022 11:55:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.88 on epoch=222
05/21/2022 11:55:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.85 on epoch=224
05/21/2022 11:55:21 - INFO - __main__ - Global step 900 Train loss 0.88 Classification-F1 0.1 on epoch=224
05/21/2022 11:55:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.79 on epoch=227
05/21/2022 11:55:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.83 on epoch=229
05/21/2022 11:55:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.83 on epoch=232
05/21/2022 11:55:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.84 on epoch=234
05/21/2022 11:55:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.81 on epoch=237
05/21/2022 11:55:29 - INFO - __main__ - Global step 950 Train loss 0.82 Classification-F1 0.1 on epoch=237
05/21/2022 11:55:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.81 on epoch=239
05/21/2022 11:55:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.80 on epoch=242
05/21/2022 11:55:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.83 on epoch=244
05/21/2022 11:55:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.84 on epoch=247
05/21/2022 11:55:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.79 on epoch=249
05/21/2022 11:55:36 - INFO - __main__ - Global step 1000 Train loss 0.82 Classification-F1 0.1 on epoch=249
05/21/2022 11:55:38 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.84 on epoch=252
05/21/2022 11:55:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.86 on epoch=254
05/21/2022 11:55:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.84 on epoch=257
05/21/2022 11:55:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.83 on epoch=259
05/21/2022 11:55:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.82 on epoch=262
05/21/2022 11:55:44 - INFO - __main__ - Global step 1050 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=262
05/21/2022 11:55:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.81 on epoch=264
05/21/2022 11:55:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.83 on epoch=267
05/21/2022 11:55:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.83 on epoch=269
05/21/2022 11:55:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.90 on epoch=272
05/21/2022 11:55:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.89 on epoch=274
05/21/2022 11:55:52 - INFO - __main__ - Global step 1100 Train loss 0.85 Classification-F1 0.1 on epoch=274
05/21/2022 11:55:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.77 on epoch=277
05/21/2022 11:55:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.83 on epoch=279
05/21/2022 11:55:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.81 on epoch=282
05/21/2022 11:55:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.82 on epoch=284
05/21/2022 11:55:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.83 on epoch=287
05/21/2022 11:56:00 - INFO - __main__ - Global step 1150 Train loss 0.81 Classification-F1 0.1 on epoch=287
05/21/2022 11:56:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.83 on epoch=289
05/21/2022 11:56:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.79 on epoch=292
05/21/2022 11:56:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.88 on epoch=294
05/21/2022 11:56:06 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.83 on epoch=297
05/21/2022 11:56:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.79 on epoch=299
05/21/2022 11:56:08 - INFO - __main__ - Global step 1200 Train loss 0.82 Classification-F1 0.1 on epoch=299
05/21/2022 11:56:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.78 on epoch=302
05/21/2022 11:56:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.80 on epoch=304
05/21/2022 11:56:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.89 on epoch=307
05/21/2022 11:56:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.80 on epoch=309
05/21/2022 11:56:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.80 on epoch=312
05/21/2022 11:56:15 - INFO - __main__ - Global step 1250 Train loss 0.82 Classification-F1 0.11111111111111112 on epoch=312
05/21/2022 11:56:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.81 on epoch=314
05/21/2022 11:56:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.79 on epoch=317
05/21/2022 11:56:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.78 on epoch=319
05/21/2022 11:56:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.86 on epoch=322
05/21/2022 11:56:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.73 on epoch=324
05/21/2022 11:56:23 - INFO - __main__ - Global step 1300 Train loss 0.79 Classification-F1 0.1996370235934664 on epoch=324
05/21/2022 11:56:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.81 on epoch=327
05/21/2022 11:56:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.82 on epoch=329
05/21/2022 11:56:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.78 on epoch=332
05/21/2022 11:56:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.82 on epoch=334
05/21/2022 11:56:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.79 on epoch=337
05/21/2022 11:56:31 - INFO - __main__ - Global step 1350 Train loss 0.80 Classification-F1 0.1 on epoch=337
05/21/2022 11:56:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.85 on epoch=339
05/21/2022 11:56:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.85 on epoch=342
05/21/2022 11:56:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.83 on epoch=344
05/21/2022 11:56:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.83 on epoch=347
05/21/2022 11:56:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.81 on epoch=349
05/21/2022 11:56:38 - INFO - __main__ - Global step 1400 Train loss 0.84 Classification-F1 0.1 on epoch=349
05/21/2022 11:56:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.82 on epoch=352
05/21/2022 11:56:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.81 on epoch=354
05/21/2022 11:56:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.77 on epoch=357
05/21/2022 11:56:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.79 on epoch=359
05/21/2022 11:56:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.84 on epoch=362
05/21/2022 11:56:45 - INFO - __main__ - Global step 1450 Train loss 0.81 Classification-F1 0.10126582278481013 on epoch=362
05/21/2022 11:56:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.87 on epoch=364
05/21/2022 11:56:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.79 on epoch=367
05/21/2022 11:56:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.80 on epoch=369
05/21/2022 11:56:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.87 on epoch=372
05/21/2022 11:56:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.85 on epoch=374
05/21/2022 11:56:53 - INFO - __main__ - Global step 1500 Train loss 0.84 Classification-F1 0.10126582278481013 on epoch=374
05/21/2022 11:56:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.79 on epoch=377
05/21/2022 11:56:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.85 on epoch=379
05/21/2022 11:56:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.79 on epoch=382
05/21/2022 11:56:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.80 on epoch=384
05/21/2022 11:57:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.81 on epoch=387
05/21/2022 11:57:01 - INFO - __main__ - Global step 1550 Train loss 0.81 Classification-F1 0.20920303605313093 on epoch=387
05/21/2022 11:57:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.80 on epoch=389
05/21/2022 11:57:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.75 on epoch=392
05/21/2022 11:57:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.82 on epoch=394
05/21/2022 11:57:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.82 on epoch=397
05/21/2022 11:57:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.82 on epoch=399
05/21/2022 11:57:08 - INFO - __main__ - Global step 1600 Train loss 0.80 Classification-F1 0.17112712300566135 on epoch=399
05/21/2022 11:57:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.76 on epoch=402
05/21/2022 11:57:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.77 on epoch=404
05/21/2022 11:57:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.76 on epoch=407
05/21/2022 11:57:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.77 on epoch=409
05/21/2022 11:57:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.82 on epoch=412
05/21/2022 11:57:16 - INFO - __main__ - Global step 1650 Train loss 0.78 Classification-F1 0.131328171530673 on epoch=412
05/21/2022 11:57:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.80 on epoch=414
05/21/2022 11:57:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.81 on epoch=417
05/21/2022 11:57:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.79 on epoch=419
05/21/2022 11:57:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.85 on epoch=422
05/21/2022 11:57:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.77 on epoch=424
05/21/2022 11:57:23 - INFO - __main__ - Global step 1700 Train loss 0.81 Classification-F1 0.10126582278481013 on epoch=424
05/21/2022 11:57:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.81 on epoch=427
05/21/2022 11:57:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.83 on epoch=429
05/21/2022 11:57:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.71 on epoch=432
05/21/2022 11:57:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.82 on epoch=434
05/21/2022 11:57:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.80 on epoch=437
05/21/2022 11:57:31 - INFO - __main__ - Global step 1750 Train loss 0.80 Classification-F1 0.10389610389610389 on epoch=437
05/21/2022 11:57:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.84 on epoch=439
05/21/2022 11:57:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.77 on epoch=442
05/21/2022 11:57:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.81 on epoch=444
05/21/2022 11:57:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.79 on epoch=447
05/21/2022 11:57:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.78 on epoch=449
05/21/2022 11:57:38 - INFO - __main__ - Global step 1800 Train loss 0.80 Classification-F1 0.1 on epoch=449
05/21/2022 11:57:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.75 on epoch=452
05/21/2022 11:57:41 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.81 on epoch=454
05/21/2022 11:57:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.82 on epoch=457
05/21/2022 11:57:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.87 on epoch=459
05/21/2022 11:57:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.87 on epoch=462
05/21/2022 11:57:46 - INFO - __main__ - Global step 1850 Train loss 0.82 Classification-F1 0.1527777777777778 on epoch=462
05/21/2022 11:57:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.77 on epoch=464
05/21/2022 11:57:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.78 on epoch=467
05/21/2022 11:57:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.77 on epoch=469
05/21/2022 11:57:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.82 on epoch=472
05/21/2022 11:57:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.78 on epoch=474
05/21/2022 11:57:54 - INFO - __main__ - Global step 1900 Train loss 0.78 Classification-F1 0.1 on epoch=474
05/21/2022 11:57:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.82 on epoch=477
05/21/2022 11:57:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.84 on epoch=479
05/21/2022 11:57:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.76 on epoch=482
05/21/2022 11:58:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.87 on epoch=484
05/21/2022 11:58:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.84 on epoch=487
05/21/2022 11:58:02 - INFO - __main__ - Global step 1950 Train loss 0.83 Classification-F1 0.17480643240023822 on epoch=487
05/21/2022 11:58:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.78 on epoch=489
05/21/2022 11:58:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.80 on epoch=492
05/21/2022 11:58:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.84 on epoch=494
05/21/2022 11:58:08 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.75 on epoch=497
05/21/2022 11:58:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.83 on epoch=499
05/21/2022 11:58:09 - INFO - __main__ - Global step 2000 Train loss 0.80 Classification-F1 0.1996370235934664 on epoch=499
05/21/2022 11:58:11 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.82 on epoch=502
05/21/2022 11:58:12 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.86 on epoch=504
05/21/2022 11:58:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.77 on epoch=507
05/21/2022 11:58:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.81 on epoch=509
05/21/2022 11:58:16 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.81 on epoch=512
05/21/2022 11:58:17 - INFO - __main__ - Global step 2050 Train loss 0.81 Classification-F1 0.10126582278481013 on epoch=512
05/21/2022 11:58:19 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.76 on epoch=514
05/21/2022 11:58:20 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.80 on epoch=517
05/21/2022 11:58:22 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.76 on epoch=519
05/21/2022 11:58:23 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.73 on epoch=522
05/21/2022 11:58:24 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.81 on epoch=524
05/21/2022 11:58:25 - INFO - __main__ - Global step 2100 Train loss 0.77 Classification-F1 0.1 on epoch=524
05/21/2022 11:58:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.79 on epoch=527
05/21/2022 11:58:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.84 on epoch=529
05/21/2022 11:58:29 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.77 on epoch=532
05/21/2022 11:58:31 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.80 on epoch=534
05/21/2022 11:58:32 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.81 on epoch=537
05/21/2022 11:58:33 - INFO - __main__ - Global step 2150 Train loss 0.80 Classification-F1 0.10256410256410256 on epoch=537
05/21/2022 11:58:34 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.81 on epoch=539
05/21/2022 11:58:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.77 on epoch=542
05/21/2022 11:58:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.81 on epoch=544
05/21/2022 11:58:39 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.77 on epoch=547
05/21/2022 11:58:40 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.85 on epoch=549
05/21/2022 11:58:41 - INFO - __main__ - Global step 2200 Train loss 0.80 Classification-F1 0.10126582278481013 on epoch=549
05/21/2022 11:58:42 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.90 on epoch=552
05/21/2022 11:58:43 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.79 on epoch=554
05/21/2022 11:58:44 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.80 on epoch=557
05/21/2022 11:58:46 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.76 on epoch=559
05/21/2022 11:58:47 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.81 on epoch=562
05/21/2022 11:58:47 - INFO - __main__ - Global step 2250 Train loss 0.81 Classification-F1 0.10126582278481013 on epoch=562
05/21/2022 11:58:49 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.82 on epoch=564
05/21/2022 11:58:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.77 on epoch=567
05/21/2022 11:58:51 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.78 on epoch=569
05/21/2022 11:58:53 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.80 on epoch=572
05/21/2022 11:58:54 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.81 on epoch=574
05/21/2022 11:58:55 - INFO - __main__ - Global step 2300 Train loss 0.80 Classification-F1 0.1302118933697881 on epoch=574
05/21/2022 11:58:57 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.78 on epoch=577
05/21/2022 11:58:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.79 on epoch=579
05/21/2022 11:59:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.81 on epoch=582
05/21/2022 11:59:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.76 on epoch=584
05/21/2022 11:59:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.75 on epoch=587
05/21/2022 11:59:03 - INFO - __main__ - Global step 2350 Train loss 0.78 Classification-F1 0.10126582278481013 on epoch=587
05/21/2022 11:59:05 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.80 on epoch=589
05/21/2022 11:59:06 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.80 on epoch=592
05/21/2022 11:59:07 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.76 on epoch=594
05/21/2022 11:59:09 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.81 on epoch=597
05/21/2022 11:59:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.81 on epoch=599
05/21/2022 11:59:11 - INFO - __main__ - Global step 2400 Train loss 0.80 Classification-F1 0.10126582278481013 on epoch=599
05/21/2022 11:59:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.75 on epoch=602
05/21/2022 11:59:14 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.74 on epoch=604
05/21/2022 11:59:15 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.81 on epoch=607
05/21/2022 11:59:16 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.73 on epoch=609
05/21/2022 11:59:18 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.79 on epoch=612
05/21/2022 11:59:18 - INFO - __main__ - Global step 2450 Train loss 0.76 Classification-F1 0.1242521859180856 on epoch=612
05/21/2022 11:59:20 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.81 on epoch=614
05/21/2022 11:59:21 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.82 on epoch=617
05/21/2022 11:59:22 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.75 on epoch=619
05/21/2022 11:59:23 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.79 on epoch=622
05/21/2022 11:59:25 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.73 on epoch=624
05/21/2022 11:59:26 - INFO - __main__ - Global step 2500 Train loss 0.78 Classification-F1 0.10389610389610389 on epoch=624
05/21/2022 11:59:27 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.73 on epoch=627
05/21/2022 11:59:29 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.77 on epoch=629
05/21/2022 11:59:30 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.78 on epoch=632
05/21/2022 11:59:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.75 on epoch=634
05/21/2022 11:59:33 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.75 on epoch=637
05/21/2022 11:59:34 - INFO - __main__ - Global step 2550 Train loss 0.76 Classification-F1 0.10126582278481013 on epoch=637
05/21/2022 11:59:35 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.81 on epoch=639
05/21/2022 11:59:37 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.82 on epoch=642
05/21/2022 11:59:38 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.86 on epoch=644
05/21/2022 11:59:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.81 on epoch=647
05/21/2022 11:59:41 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.76 on epoch=649
05/21/2022 11:59:42 - INFO - __main__ - Global step 2600 Train loss 0.81 Classification-F1 0.1 on epoch=649
05/21/2022 11:59:43 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.81 on epoch=652
05/21/2022 11:59:44 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.78 on epoch=654
05/21/2022 11:59:46 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.78 on epoch=657
05/21/2022 11:59:47 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.76 on epoch=659
05/21/2022 11:59:48 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.89 on epoch=662
05/21/2022 11:59:49 - INFO - __main__ - Global step 2650 Train loss 0.80 Classification-F1 0.10389610389610389 on epoch=662
05/21/2022 11:59:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.82 on epoch=664
05/21/2022 11:59:51 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.76 on epoch=667
05/21/2022 11:59:53 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.83 on epoch=669
05/21/2022 11:59:55 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.77 on epoch=672
05/21/2022 11:59:56 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.77 on epoch=674
05/21/2022 11:59:57 - INFO - __main__ - Global step 2700 Train loss 0.79 Classification-F1 0.1 on epoch=674
05/21/2022 11:59:58 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.79 on epoch=677
05/21/2022 12:00:00 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.83 on epoch=679
05/21/2022 12:00:02 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.72 on epoch=682
05/21/2022 12:00:03 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.78 on epoch=684
05/21/2022 12:00:05 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.76 on epoch=687
05/21/2022 12:00:05 - INFO - __main__ - Global step 2750 Train loss 0.77 Classification-F1 0.13558823529411765 on epoch=687
05/21/2022 12:00:07 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.76 on epoch=689
05/21/2022 12:00:08 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.80 on epoch=692
05/21/2022 12:00:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.81 on epoch=694
05/21/2022 12:00:12 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.74 on epoch=697
05/21/2022 12:00:13 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.80 on epoch=699
05/21/2022 12:00:13 - INFO - __main__ - Global step 2800 Train loss 0.78 Classification-F1 0.10126582278481013 on epoch=699
05/21/2022 12:00:15 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.76 on epoch=702
05/21/2022 12:00:16 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.76 on epoch=704
05/21/2022 12:00:18 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.73 on epoch=707
05/21/2022 12:00:19 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.82 on epoch=709
05/21/2022 12:00:21 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.80 on epoch=712
05/21/2022 12:00:21 - INFO - __main__ - Global step 2850 Train loss 0.77 Classification-F1 0.1511156186612576 on epoch=712
05/21/2022 12:00:22 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.72 on epoch=714
05/21/2022 12:00:24 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.73 on epoch=717
05/21/2022 12:00:25 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.78 on epoch=719
05/21/2022 12:00:27 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.75 on epoch=722
05/21/2022 12:00:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.78 on epoch=724
05/21/2022 12:00:29 - INFO - __main__ - Global step 2900 Train loss 0.75 Classification-F1 0.10666666666666667 on epoch=724
05/21/2022 12:00:30 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.81 on epoch=727
05/21/2022 12:00:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.78 on epoch=729
05/21/2022 12:00:32 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.78 on epoch=732
05/21/2022 12:00:34 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.78 on epoch=734
05/21/2022 12:00:35 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.77 on epoch=737
05/21/2022 12:00:36 - INFO - __main__ - Global step 2950 Train loss 0.78 Classification-F1 0.10256410256410256 on epoch=737
05/21/2022 12:00:37 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.79 on epoch=739
05/21/2022 12:00:39 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.78 on epoch=742
05/21/2022 12:00:40 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.77 on epoch=744
05/21/2022 12:00:42 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.75 on epoch=747
05/21/2022 12:00:43 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.81 on epoch=749
05/21/2022 12:00:43 - INFO - __main__ - Global step 3000 Train loss 0.78 Classification-F1 0.0945945945945946 on epoch=749
05/21/2022 12:00:43 - INFO - __main__ - save last model!
05/21/2022 12:00:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 12:00:43 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 12:00:43 - INFO - __main__ - Printing 3 examples
05/21/2022 12:00:43 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 12:00:43 - INFO - __main__ - ['others']
05/21/2022 12:00:43 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 12:00:43 - INFO - __main__ - ['others']
05/21/2022 12:00:43 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 12:00:43 - INFO - __main__ - ['others']
05/21/2022 12:00:43 - INFO - __main__ - Tokenizing Input ...
05/21/2022 12:00:44 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 12:00:44 - INFO - __main__ - Printing 3 examples
05/21/2022 12:00:44 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/21/2022 12:00:44 - INFO - __main__ - ['others']
05/21/2022 12:00:44 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/21/2022 12:00:44 - INFO - __main__ - ['others']
05/21/2022 12:00:44 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/21/2022 12:00:44 - INFO - __main__ - ['others']
05/21/2022 12:00:44 - INFO - __main__ - Tokenizing Input ...
05/21/2022 12:00:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 12:00:44 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 12:00:44 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 12:00:44 - INFO - __main__ - Printing 3 examples
05/21/2022 12:00:44 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/21/2022 12:00:44 - INFO - __main__ - ['others']
05/21/2022 12:00:44 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/21/2022 12:00:44 - INFO - __main__ - ['others']
05/21/2022 12:00:44 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/21/2022 12:00:44 - INFO - __main__ - ['others']
05/21/2022 12:00:44 - INFO - __main__ - Tokenizing Input ...
05/21/2022 12:00:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 12:00:44 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 12:00:46 - INFO - __main__ - Tokenizing Output ...
05/21/2022 12:00:50 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 12:00:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 12:00:50 - INFO - __main__ - Starting training!
05/21/2022 12:00:51 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 12:01:34 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_87_0.3_8_predictions.txt
05/21/2022 12:01:34 - INFO - __main__ - Classification-F1 on test data: 0.0795
05/21/2022 12:01:34 - INFO - __main__ - prefix=emo_16_87, lr=0.3, bsz=8, dev_performance=0.2813816127375449, test_performance=0.07947565088904372
05/21/2022 12:01:34 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.2, bsz=8 ...
05/21/2022 12:01:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 12:01:35 - INFO - __main__ - Printing 3 examples
05/21/2022 12:01:35 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/21/2022 12:01:35 - INFO - __main__ - ['others']
05/21/2022 12:01:35 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/21/2022 12:01:35 - INFO - __main__ - ['others']
05/21/2022 12:01:35 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/21/2022 12:01:35 - INFO - __main__ - ['others']
05/21/2022 12:01:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 12:01:35 - INFO - __main__ - Tokenizing Output ...
05/21/2022 12:01:35 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 12:01:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 12:01:35 - INFO - __main__ - Printing 3 examples
05/21/2022 12:01:35 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/21/2022 12:01:35 - INFO - __main__ - ['others']
05/21/2022 12:01:35 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/21/2022 12:01:35 - INFO - __main__ - ['others']
05/21/2022 12:01:35 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/21/2022 12:01:35 - INFO - __main__ - ['others']
05/21/2022 12:01:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 12:01:36 - INFO - __main__ - Tokenizing Output ...
05/21/2022 12:01:36 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 12:01:42 - INFO - __main__ - load prompt embedding from ckpt
05/21/2022 12:01:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/21/2022 12:01:42 - INFO - __main__ - Starting training!
05/21/2022 12:01:44 - INFO - __main__ - Step 10 Global step 10 Train loss 4.16 on epoch=2
05/21/2022 12:01:45 - INFO - __main__ - Step 20 Global step 20 Train loss 4.05 on epoch=4
05/21/2022 12:01:46 - INFO - __main__ - Step 30 Global step 30 Train loss 3.77 on epoch=7
05/21/2022 12:01:47 - INFO - __main__ - Step 40 Global step 40 Train loss 3.45 on epoch=9
05/21/2022 12:01:49 - INFO - __main__ - Step 50 Global step 50 Train loss 3.29 on epoch=12
05/21/2022 12:01:50 - INFO - __main__ - Global step 50 Train loss 3.74 Classification-F1 0.0 on epoch=12
05/21/2022 12:01:50 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
05/21/2022 12:01:51 - INFO - __main__ - Step 60 Global step 60 Train loss 2.94 on epoch=14
05/21/2022 12:01:53 - INFO - __main__ - Step 70 Global step 70 Train loss 2.80 on epoch=17
05/21/2022 12:01:54 - INFO - __main__ - Step 80 Global step 80 Train loss 2.56 on epoch=19
05/21/2022 12:01:56 - INFO - __main__ - Step 90 Global step 90 Train loss 2.34 on epoch=22
05/21/2022 12:01:57 - INFO - __main__ - Step 100 Global step 100 Train loss 2.20 on epoch=24
05/21/2022 12:01:58 - INFO - __main__ - Global step 100 Train loss 2.57 Classification-F1 0.1 on epoch=24
05/21/2022 12:01:58 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.1 on epoch=24, global_step=100
05/21/2022 12:02:00 - INFO - __main__ - Step 110 Global step 110 Train loss 2.02 on epoch=27
05/21/2022 12:02:01 - INFO - __main__ - Step 120 Global step 120 Train loss 1.80 on epoch=29
05/21/2022 12:02:02 - INFO - __main__ - Step 130 Global step 130 Train loss 1.77 on epoch=32
05/21/2022 12:02:04 - INFO - __main__ - Step 140 Global step 140 Train loss 1.55 on epoch=34
05/21/2022 12:02:05 - INFO - __main__ - Step 150 Global step 150 Train loss 1.63 on epoch=37
05/21/2022 12:02:06 - INFO - __main__ - Global step 150 Train loss 1.75 Classification-F1 0.1 on epoch=37
05/21/2022 12:02:07 - INFO - __main__ - Step 160 Global step 160 Train loss 1.46 on epoch=39
05/21/2022 12:02:09 - INFO - __main__ - Step 170 Global step 170 Train loss 1.43 on epoch=42
05/21/2022 12:02:10 - INFO - __main__ - Step 180 Global step 180 Train loss 1.30 on epoch=44
05/21/2022 12:02:12 - INFO - __main__ - Step 190 Global step 190 Train loss 1.37 on epoch=47
05/21/2022 12:02:13 - INFO - __main__ - Step 200 Global step 200 Train loss 1.11 on epoch=49
05/21/2022 12:02:14 - INFO - __main__ - Global step 200 Train loss 1.33 Classification-F1 0.1 on epoch=49
05/21/2022 12:02:15 - INFO - __main__ - Step 210 Global step 210 Train loss 1.23 on epoch=52
05/21/2022 12:02:16 - INFO - __main__ - Step 220 Global step 220 Train loss 1.12 on epoch=54
05/21/2022 12:02:18 - INFO - __main__ - Step 230 Global step 230 Train loss 1.06 on epoch=57
05/21/2022 12:02:19 - INFO - __main__ - Step 240 Global step 240 Train loss 1.11 on epoch=59
05/21/2022 12:02:21 - INFO - __main__ - Step 250 Global step 250 Train loss 1.14 on epoch=62
05/21/2022 12:02:21 - INFO - __main__ - Global step 250 Train loss 1.13 Classification-F1 0.1 on epoch=62
05/21/2022 12:02:23 - INFO - __main__ - Step 260 Global step 260 Train loss 1.00 on epoch=64
05/21/2022 12:02:24 - INFO - __main__ - Step 270 Global step 270 Train loss 1.06 on epoch=67
05/21/2022 12:02:25 - INFO - __main__ - Step 280 Global step 280 Train loss 1.07 on epoch=69
05/21/2022 12:02:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.91 on epoch=72
05/21/2022 12:02:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.98 on epoch=74
05/21/2022 12:02:29 - INFO - __main__ - Global step 300 Train loss 1.00 Classification-F1 0.1 on epoch=74
05/21/2022 12:02:30 - INFO - __main__ - Step 310 Global step 310 Train loss 1.02 on epoch=77
05/21/2022 12:02:32 - INFO - __main__ - Step 320 Global step 320 Train loss 1.03 on epoch=79
05/21/2022 12:02:33 - INFO - __main__ - Step 330 Global step 330 Train loss 1.00 on epoch=82
05/21/2022 12:02:34 - INFO - __main__ - Step 340 Global step 340 Train loss 1.04 on epoch=84
05/21/2022 12:02:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.92 on epoch=87
05/21/2022 12:02:36 - INFO - __main__ - Global step 350 Train loss 1.00 Classification-F1 0.15526315789473685 on epoch=87
05/21/2022 12:02:36 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.15526315789473685 on epoch=87, global_step=350
05/21/2022 12:02:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.97 on epoch=89
05/21/2022 12:02:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.92 on epoch=92
05/21/2022 12:02:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.93 on epoch=94
05/21/2022 12:02:42 - INFO - __main__ - Step 390 Global step 390 Train loss 1.05 on epoch=97
05/21/2022 12:02:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.95 on epoch=99
05/21/2022 12:02:44 - INFO - __main__ - Global step 400 Train loss 0.96 Classification-F1 0.1 on epoch=99
05/21/2022 12:02:45 - INFO - __main__ - Step 410 Global step 410 Train loss 1.05 on epoch=102
05/21/2022 12:02:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.92 on epoch=104
05/21/2022 12:02:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.93 on epoch=107
05/21/2022 12:02:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.93 on epoch=109
05/21/2022 12:02:51 - INFO - __main__ - Step 450 Global step 450 Train loss 1.04 on epoch=112
05/21/2022 12:02:52 - INFO - __main__ - Global step 450 Train loss 0.97 Classification-F1 0.1 on epoch=112
05/21/2022 12:02:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.96 on epoch=114
05/21/2022 12:02:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.94 on epoch=117
05/21/2022 12:02:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.89 on epoch=119
05/21/2022 12:02:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.93 on epoch=122
05/21/2022 12:03:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.92 on epoch=124
05/21/2022 12:03:00 - INFO - __main__ - Global step 500 Train loss 0.93 Classification-F1 0.1 on epoch=124
05/21/2022 12:03:01 - INFO - __main__ - Step 510 Global step 510 Train loss 1.04 on epoch=127
05/21/2022 12:03:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.83 on epoch=129
05/21/2022 12:03:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.90 on epoch=132
05/21/2022 12:03:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.98 on epoch=134
05/21/2022 12:03:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.93 on epoch=137
05/21/2022 12:03:07 - INFO - __main__ - Global step 550 Train loss 0.93 Classification-F1 0.1 on epoch=137
05/21/2022 12:03:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.95 on epoch=139
05/21/2022 12:03:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.97 on epoch=142
05/21/2022 12:03:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.83 on epoch=144
05/21/2022 12:03:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.98 on epoch=147
05/21/2022 12:03:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.87 on epoch=149
05/21/2022 12:03:15 - INFO - __main__ - Global step 600 Train loss 0.92 Classification-F1 0.1 on epoch=149
05/21/2022 12:03:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.99 on epoch=152
05/21/2022 12:03:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.86 on epoch=154
05/21/2022 12:03:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.97 on epoch=157
05/21/2022 12:03:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.91 on epoch=159
05/21/2022 12:03:22 - INFO - __main__ - Step 650 Global step 650 Train loss 1.01 on epoch=162
05/21/2022 12:03:22 - INFO - __main__ - Global step 650 Train loss 0.95 Classification-F1 0.13034188034188032 on epoch=162
05/21/2022 12:03:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.91 on epoch=164
05/21/2022 12:03:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.87 on epoch=167
05/21/2022 12:03:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.87 on epoch=169
05/21/2022 12:03:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.89 on epoch=172
05/21/2022 12:03:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.96 on epoch=174
05/21/2022 12:03:30 - INFO - __main__ - Global step 700 Train loss 0.90 Classification-F1 0.1 on epoch=174
05/21/2022 12:03:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.93 on epoch=177
05/21/2022 12:03:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.85 on epoch=179
05/21/2022 12:03:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.91 on epoch=182
05/21/2022 12:03:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.93 on epoch=184
05/21/2022 12:03:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.89 on epoch=187
05/21/2022 12:03:38 - INFO - __main__ - Global step 750 Train loss 0.90 Classification-F1 0.1 on epoch=187
05/21/2022 12:03:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.88 on epoch=189
05/21/2022 12:03:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.89 on epoch=192
05/21/2022 12:03:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.87 on epoch=194
05/21/2022 12:03:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.91 on epoch=197
05/21/2022 12:03:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.87 on epoch=199
05/21/2022 12:03:45 - INFO - __main__ - Global step 800 Train loss 0.88 Classification-F1 0.1 on epoch=199
05/21/2022 12:03:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.78 on epoch=202
05/21/2022 12:03:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.88 on epoch=204
05/21/2022 12:03:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.86 on epoch=207
05/21/2022 12:03:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.96 on epoch=209
05/21/2022 12:03:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.81 on epoch=212
05/21/2022 12:03:53 - INFO - __main__ - Global step 850 Train loss 0.86 Classification-F1 0.1 on epoch=212
05/21/2022 12:03:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.83 on epoch=214
05/21/2022 12:03:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.89 on epoch=217
05/21/2022 12:03:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.89 on epoch=219
05/21/2022 12:03:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.76 on epoch=222
05/21/2022 12:04:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.86 on epoch=224
05/21/2022 12:04:01 - INFO - __main__ - Global step 900 Train loss 0.85 Classification-F1 0.1 on epoch=224
05/21/2022 12:04:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.83 on epoch=227
05/21/2022 12:04:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.86 on epoch=229
05/21/2022 12:04:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.93 on epoch=232
05/21/2022 12:04:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.84 on epoch=234
05/21/2022 12:04:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.82 on epoch=237
05/21/2022 12:04:08 - INFO - __main__ - Global step 950 Train loss 0.86 Classification-F1 0.1 on epoch=237
05/21/2022 12:04:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.78 on epoch=239
05/21/2022 12:04:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.77 on epoch=242
05/21/2022 12:04:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.85 on epoch=244
05/21/2022 12:04:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.85 on epoch=247
05/21/2022 12:04:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.87 on epoch=249
05/21/2022 12:04:16 - INFO - __main__ - Global step 1000 Train loss 0.82 Classification-F1 0.1 on epoch=249
05/21/2022 12:04:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.92 on epoch=252
05/21/2022 12:04:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.78 on epoch=254
05/21/2022 12:04:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.85 on epoch=257
05/21/2022 12:04:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.85 on epoch=259
05/21/2022 12:04:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.80 on epoch=262
05/21/2022 12:04:24 - INFO - __main__ - Global step 1050 Train loss 0.84 Classification-F1 0.1 on epoch=262
05/21/2022 12:04:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.81 on epoch=264
05/21/2022 12:04:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.78 on epoch=267
05/21/2022 12:04:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.75 on epoch=269
05/21/2022 12:04:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.88 on epoch=272
05/21/2022 12:04:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.85 on epoch=274
05/21/2022 12:04:32 - INFO - __main__ - Global step 1100 Train loss 0.81 Classification-F1 0.1 on epoch=274
05/21/2022 12:04:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.82 on epoch=277
05/21/2022 12:04:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.86 on epoch=279
05/21/2022 12:04:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.85 on epoch=282
05/21/2022 12:04:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.89 on epoch=284
05/21/2022 12:04:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.91 on epoch=287
05/21/2022 12:04:39 - INFO - __main__ - Global step 1150 Train loss 0.86 Classification-F1 0.1 on epoch=287
05/21/2022 12:04:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.89 on epoch=289
05/21/2022 12:04:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.90 on epoch=292
05/21/2022 12:04:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.83 on epoch=294
05/21/2022 12:04:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.84 on epoch=297
05/21/2022 12:04:47 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.90 on epoch=299
05/21/2022 12:04:47 - INFO - __main__ - Global step 1200 Train loss 0.87 Classification-F1 0.1 on epoch=299
05/21/2022 12:04:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.89 on epoch=302
05/21/2022 12:04:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.84 on epoch=304
05/21/2022 12:04:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.86 on epoch=307
05/21/2022 12:04:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.92 on epoch=309
05/21/2022 12:04:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.76 on epoch=312
05/21/2022 12:04:56 - INFO - __main__ - Global step 1250 Train loss 0.85 Classification-F1 0.1 on epoch=312
05/21/2022 12:04:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.84 on epoch=314
05/21/2022 12:04:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.80 on epoch=317
05/21/2022 12:05:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.91 on epoch=319
05/21/2022 12:05:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.87 on epoch=322
05/21/2022 12:05:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.76 on epoch=324
05/21/2022 12:05:03 - INFO - __main__ - Global step 1300 Train loss 0.84 Classification-F1 0.1 on epoch=324
05/21/2022 12:05:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.83 on epoch=327
05/21/2022 12:05:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.84 on epoch=329
05/21/2022 12:05:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.79 on epoch=332
05/21/2022 12:05:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.80 on epoch=334
05/21/2022 12:05:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.82 on epoch=337
05/21/2022 12:05:11 - INFO - __main__ - Global step 1350 Train loss 0.82 Classification-F1 0.13034188034188032 on epoch=337
05/21/2022 12:05:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.83 on epoch=339
05/21/2022 12:05:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.87 on epoch=342
05/21/2022 12:05:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.82 on epoch=344
05/21/2022 12:05:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.85 on epoch=347
05/21/2022 12:05:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.84 on epoch=349
05/21/2022 12:05:18 - INFO - __main__ - Global step 1400 Train loss 0.84 Classification-F1 0.1 on epoch=349
05/21/2022 12:05:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.82 on epoch=352
05/21/2022 12:05:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.82 on epoch=354
05/21/2022 12:05:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.84 on epoch=357
05/21/2022 12:05:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.87 on epoch=359
05/21/2022 12:05:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.85 on epoch=362
05/21/2022 12:05:26 - INFO - __main__ - Global step 1450 Train loss 0.84 Classification-F1 0.1 on epoch=362
05/21/2022 12:05:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.82 on epoch=364
05/21/2022 12:05:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.86 on epoch=367
05/21/2022 12:05:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.82 on epoch=369
05/21/2022 12:05:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.83 on epoch=372
05/21/2022 12:05:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.78 on epoch=374
05/21/2022 12:05:33 - INFO - __main__ - Global step 1500 Train loss 0.82 Classification-F1 0.1 on epoch=374
05/21/2022 12:05:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.82 on epoch=377
05/21/2022 12:05:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.86 on epoch=379
05/21/2022 12:05:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.84 on epoch=382
05/21/2022 12:05:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.83 on epoch=384
05/21/2022 12:05:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.76 on epoch=387
05/21/2022 12:05:41 - INFO - __main__ - Global step 1550 Train loss 0.82 Classification-F1 0.1 on epoch=387
05/21/2022 12:05:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.80 on epoch=389
05/21/2022 12:05:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.80 on epoch=392
05/21/2022 12:05:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.79 on epoch=394
05/21/2022 12:05:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.79 on epoch=397
05/21/2022 12:05:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.83 on epoch=399
05/21/2022 12:05:48 - INFO - __main__ - Global step 1600 Train loss 0.80 Classification-F1 0.1 on epoch=399
05/21/2022 12:05:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.78 on epoch=402
05/21/2022 12:05:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.85 on epoch=404
05/21/2022 12:05:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.84 on epoch=407
05/21/2022 12:05:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.88 on epoch=409
05/21/2022 12:05:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.74 on epoch=412
05/21/2022 12:05:55 - INFO - __main__ - Global step 1650 Train loss 0.82 Classification-F1 0.10126582278481013 on epoch=412
05/21/2022 12:05:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.79 on epoch=414
05/21/2022 12:05:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.88 on epoch=417
05/21/2022 12:05:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.84 on epoch=419
05/21/2022 12:06:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.90 on epoch=422
05/21/2022 12:06:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.88 on epoch=424
05/21/2022 12:06:02 - INFO - __main__ - Global step 1700 Train loss 0.86 Classification-F1 0.1 on epoch=424
05/21/2022 12:06:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.85 on epoch=427
05/21/2022 12:06:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.90 on epoch=429
05/21/2022 12:06:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.83 on epoch=432
05/21/2022 12:06:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.80 on epoch=434
05/21/2022 12:06:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.81 on epoch=437
05/21/2022 12:06:10 - INFO - __main__ - Global step 1750 Train loss 0.84 Classification-F1 0.1 on epoch=437
05/21/2022 12:06:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.80 on epoch=439
05/21/2022 12:06:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.85 on epoch=442
05/21/2022 12:06:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.77 on epoch=444
05/21/2022 12:06:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.86 on epoch=447
05/21/2022 12:06:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.87 on epoch=449
05/21/2022 12:06:18 - INFO - __main__ - Global step 1800 Train loss 0.83 Classification-F1 0.1 on epoch=449
05/21/2022 12:06:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.81 on epoch=452
05/21/2022 12:06:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.74 on epoch=454
05/21/2022 12:06:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.80 on epoch=457
05/21/2022 12:06:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.78 on epoch=459
05/21/2022 12:06:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.86 on epoch=462
05/21/2022 12:06:26 - INFO - __main__ - Global step 1850 Train loss 0.80 Classification-F1 0.1 on epoch=462
05/21/2022 12:06:27 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.85 on epoch=464
05/21/2022 12:06:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.85 on epoch=467
05/21/2022 12:06:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.80 on epoch=469
05/21/2022 12:06:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.85 on epoch=472
05/21/2022 12:06:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.86 on epoch=474
05/21/2022 12:06:33 - INFO - __main__ - Global step 1900 Train loss 0.84 Classification-F1 0.1 on epoch=474
05/21/2022 12:06:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.77 on epoch=477
05/21/2022 12:06:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.86 on epoch=479
05/21/2022 12:06:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.79 on epoch=482
05/21/2022 12:06:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.83 on epoch=484
05/21/2022 12:06:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.83 on epoch=487
05/21/2022 12:06:41 - INFO - __main__ - Global step 1950 Train loss 0.81 Classification-F1 0.1 on epoch=487
05/21/2022 12:06:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.85 on epoch=489
05/21/2022 12:06:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.84 on epoch=492
05/21/2022 12:06:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.86 on epoch=494
05/21/2022 12:06:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.76 on epoch=497
05/21/2022 12:06:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.85 on epoch=499
05/21/2022 12:06:49 - INFO - __main__ - Global step 2000 Train loss 0.83 Classification-F1 0.1 on epoch=499
05/21/2022 12:06:50 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.79 on epoch=502
05/21/2022 12:06:52 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.79 on epoch=504
05/21/2022 12:06:53 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.86 on epoch=507
05/21/2022 12:06:55 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.84 on epoch=509
05/21/2022 12:06:56 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.82 on epoch=512
05/21/2022 12:06:57 - INFO - __main__ - Global step 2050 Train loss 0.82 Classification-F1 0.1 on epoch=512
05/21/2022 12:06:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.77 on epoch=514
05/21/2022 12:06:59 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.81 on epoch=517
05/21/2022 12:07:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.83 on epoch=519
05/21/2022 12:07:03 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.84 on epoch=522
05/21/2022 12:07:04 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.80 on epoch=524
05/21/2022 12:07:04 - INFO - __main__ - Global step 2100 Train loss 0.81 Classification-F1 0.1 on epoch=524
05/21/2022 12:07:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.86 on epoch=527
05/21/2022 12:07:07 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.79 on epoch=529
05/21/2022 12:07:09 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.81 on epoch=532
05/21/2022 12:07:10 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.80 on epoch=534
05/21/2022 12:07:11 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.84 on epoch=537
05/21/2022 12:07:12 - INFO - __main__ - Global step 2150 Train loss 0.82 Classification-F1 0.1 on epoch=537
05/21/2022 12:07:14 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.85 on epoch=539
05/21/2022 12:07:15 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.76 on epoch=542
05/21/2022 12:07:17 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.83 on epoch=544
05/21/2022 12:07:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.81 on epoch=547
05/21/2022 12:07:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.86 on epoch=549
05/21/2022 12:07:20 - INFO - __main__ - Global step 2200 Train loss 0.82 Classification-F1 0.1 on epoch=549
05/21/2022 12:07:22 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.80 on epoch=552
05/21/2022 12:07:23 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.80 on epoch=554
05/21/2022 12:07:25 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.81 on epoch=557
05/21/2022 12:07:26 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.78 on epoch=559
05/21/2022 12:07:27 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.89 on epoch=562
05/21/2022 12:07:28 - INFO - __main__ - Global step 2250 Train loss 0.82 Classification-F1 0.1 on epoch=562
05/21/2022 12:07:29 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.85 on epoch=564
05/21/2022 12:07:31 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.80 on epoch=567
05/21/2022 12:07:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.81 on epoch=569
05/21/2022 12:07:34 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.87 on epoch=572
05/21/2022 12:07:35 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.77 on epoch=574
05/21/2022 12:07:36 - INFO - __main__ - Global step 2300 Train loss 0.82 Classification-F1 0.1 on epoch=574
05/21/2022 12:07:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.88 on epoch=577
05/21/2022 12:07:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.81 on epoch=579
05/21/2022 12:07:40 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.87 on epoch=582
05/21/2022 12:07:41 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.81 on epoch=584
05/21/2022 12:07:42 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.79 on epoch=587
05/21/2022 12:07:43 - INFO - __main__ - Global step 2350 Train loss 0.83 Classification-F1 0.1 on epoch=587
05/21/2022 12:07:44 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.82 on epoch=589
05/21/2022 12:07:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.81 on epoch=592
05/21/2022 12:07:47 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.83 on epoch=594
05/21/2022 12:07:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.84 on epoch=597
05/21/2022 12:07:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.82 on epoch=599
05/21/2022 12:07:51 - INFO - __main__ - Global step 2400 Train loss 0.83 Classification-F1 0.1 on epoch=599
05/21/2022 12:07:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.83 on epoch=602
05/21/2022 12:07:53 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.85 on epoch=604
05/21/2022 12:07:55 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.78 on epoch=607
05/21/2022 12:07:56 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.82 on epoch=609
05/21/2022 12:07:58 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.82 on epoch=612
05/21/2022 12:07:58 - INFO - __main__ - Global step 2450 Train loss 0.82 Classification-F1 0.1 on epoch=612
05/21/2022 12:08:00 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.79 on epoch=614
05/21/2022 12:08:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.89 on epoch=617
05/21/2022 12:08:03 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.81 on epoch=619
05/21/2022 12:08:05 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.80 on epoch=622
05/21/2022 12:08:06 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.80 on epoch=624
05/21/2022 12:08:06 - INFO - __main__ - Global step 2500 Train loss 0.82 Classification-F1 0.1 on epoch=624
05/21/2022 12:08:08 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.86 on epoch=627
05/21/2022 12:08:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.83 on epoch=629
05/21/2022 12:08:11 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.80 on epoch=632
05/21/2022 12:08:13 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.81 on epoch=634
05/21/2022 12:08:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.74 on epoch=637
05/21/2022 12:08:15 - INFO - __main__ - Global step 2550 Train loss 0.81 Classification-F1 0.1 on epoch=637
05/21/2022 12:08:16 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.83 on epoch=639
05/21/2022 12:08:18 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.79 on epoch=642
05/21/2022 12:08:19 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.77 on epoch=644
05/21/2022 12:08:21 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.83 on epoch=647
05/21/2022 12:08:22 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.80 on epoch=649
05/21/2022 12:08:23 - INFO - __main__ - Global step 2600 Train loss 0.80 Classification-F1 0.1 on epoch=649
05/21/2022 12:08:24 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.86 on epoch=652
05/21/2022 12:08:26 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.74 on epoch=654
05/21/2022 12:08:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.83 on epoch=657
05/21/2022 12:08:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.81 on epoch=659
05/21/2022 12:08:30 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.79 on epoch=662
05/21/2022 12:08:31 - INFO - __main__ - Global step 2650 Train loss 0.81 Classification-F1 0.1 on epoch=662
05/21/2022 12:08:33 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.76 on epoch=664
05/21/2022 12:08:34 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.82 on epoch=667
05/21/2022 12:08:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.82 on epoch=669
05/21/2022 12:08:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.78 on epoch=672
05/21/2022 12:08:38 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.81 on epoch=674
05/21/2022 12:08:38 - INFO - __main__ - Global step 2700 Train loss 0.80 Classification-F1 0.1 on epoch=674
05/21/2022 12:08:40 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.77 on epoch=677
05/21/2022 12:08:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.76 on epoch=679
05/21/2022 12:08:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.84 on epoch=682
05/21/2022 12:08:44 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.73 on epoch=684
05/21/2022 12:08:45 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.86 on epoch=687
05/21/2022 12:08:46 - INFO - __main__ - Global step 2750 Train loss 0.79 Classification-F1 0.1 on epoch=687
05/21/2022 12:08:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.82 on epoch=689
05/21/2022 12:08:49 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.81 on epoch=692
05/21/2022 12:08:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.84 on epoch=694
05/21/2022 12:08:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.79 on epoch=697
05/21/2022 12:08:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.80 on epoch=699
05/21/2022 12:08:54 - INFO - __main__ - Global step 2800 Train loss 0.81 Classification-F1 0.1 on epoch=699
05/21/2022 12:08:56 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.76 on epoch=702
05/21/2022 12:08:57 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.84 on epoch=704
05/21/2022 12:08:58 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.81 on epoch=707
05/21/2022 12:09:00 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.82 on epoch=709
05/21/2022 12:09:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.75 on epoch=712
05/21/2022 12:09:01 - INFO - __main__ - Global step 2850 Train loss 0.80 Classification-F1 0.1 on epoch=712
05/21/2022 12:09:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.81 on epoch=714
05/21/2022 12:09:04 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.80 on epoch=717
05/21/2022 12:09:06 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.75 on epoch=719
05/21/2022 12:09:07 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.79 on epoch=722
05/21/2022 12:09:09 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.77 on epoch=724
05/21/2022 12:09:10 - INFO - __main__ - Global step 2900 Train loss 0.78 Classification-F1 0.1 on epoch=724
05/21/2022 12:09:11 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.78 on epoch=727
05/21/2022 12:09:12 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.84 on epoch=729
05/21/2022 12:09:14 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.82 on epoch=732
05/21/2022 12:09:15 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.86 on epoch=734
05/21/2022 12:09:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.78 on epoch=737
05/21/2022 12:09:17 - INFO - __main__ - Global step 2950 Train loss 0.82 Classification-F1 0.1 on epoch=737
05/21/2022 12:09:19 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.81 on epoch=739
05/21/2022 12:09:20 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.85 on epoch=742
05/21/2022 12:09:21 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.83 on epoch=744
05/21/2022 12:09:23 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.80 on epoch=747
05/21/2022 12:09:24 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.82 on epoch=749
05/21/2022 12:09:25 - INFO - __main__ - Global step 3000 Train loss 0.82 Classification-F1 0.1 on epoch=749
05/21/2022 12:09:25 - INFO - __main__ - save last model!
05/21/2022 12:09:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/21/2022 12:09:25 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 12:09:25 - INFO - __main__ - Printing 3 examples
05/21/2022 12:09:25 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 12:09:25 - INFO - __main__ - ['others']
05/21/2022 12:09:25 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 12:09:25 - INFO - __main__ - ['others']
05/21/2022 12:09:25 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 12:09:25 - INFO - __main__ - ['others']
05/21/2022 12:09:25 - INFO - __main__ - Tokenizing Input ...
05/21/2022 12:09:28 - INFO - __main__ - Tokenizing Output ...
05/21/2022 12:09:33 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 12:10:17 - INFO - __main__ - Saved prediction in models/T5-base-multitask-cls2cls-5e-1-4-20/singletask-emo/emo_16_87_0.2_8_predictions.txt
05/21/2022 12:10:17 - INFO - __main__ - Classification-F1 on test data: 0.0275
05/21/2022 12:10:17 - INFO - __main__ - prefix=emo_16_87, lr=0.2, bsz=8, dev_performance=0.15526315789473685, test_performance=0.02746894914165758
