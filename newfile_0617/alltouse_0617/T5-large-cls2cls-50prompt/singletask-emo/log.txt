05/30/2022 19:24:32 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-cls2cls-50prompt', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls-50prompt/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=50, cuda='6,7')
05/30/2022 19:24:32 - INFO - __main__ - models/T5-large-cls2cls-50prompt/singletask-emo
05/30/2022 19:24:32 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-cls2cls-50prompt', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls-50prompt/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=50, cuda='6,7')
05/30/2022 19:24:32 - INFO - __main__ - models/T5-large-cls2cls-50prompt/singletask-emo
05/30/2022 19:24:32 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/30/2022 19:24:32 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/30/2022 19:24:32 - INFO - __main__ - args.device: cuda:0
05/30/2022 19:24:32 - INFO - __main__ - Using 2 gpus
05/30/2022 19:24:32 - INFO - __main__ - args.device: cuda:1
05/30/2022 19:24:32 - INFO - __main__ - Using 2 gpus
05/30/2022 19:24:32 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/30/2022 19:24:32 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/30/2022 19:24:37 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
05/30/2022 19:24:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 19:24:38 - INFO - __main__ - Printing 3 examples
05/30/2022 19:24:38 - INFO - __main__ -  [emo] how cause yes am listening
05/30/2022 19:24:38 - INFO - __main__ - ['others']
05/30/2022 19:24:38 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/30/2022 19:24:38 - INFO - __main__ - ['others']
05/30/2022 19:24:38 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/30/2022 19:24:38 - INFO - __main__ - ['others']
05/30/2022 19:24:38 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:24:38 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:24:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 19:24:38 - INFO - __main__ - Printing 3 examples
05/30/2022 19:24:38 - INFO - __main__ -  [emo] how cause yes am listening
05/30/2022 19:24:38 - INFO - __main__ - ['others']
05/30/2022 19:24:38 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/30/2022 19:24:38 - INFO - __main__ - ['others']
05/30/2022 19:24:38 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/30/2022 19:24:38 - INFO - __main__ - ['others']
05/30/2022 19:24:38 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:24:38 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:24:38 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 19:24:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 19:24:38 - INFO - __main__ - Printing 3 examples
05/30/2022 19:24:38 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/30/2022 19:24:38 - INFO - __main__ - ['others']
05/30/2022 19:24:38 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/30/2022 19:24:38 - INFO - __main__ - ['others']
05/30/2022 19:24:38 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/30/2022 19:24:38 - INFO - __main__ - ['others']
05/30/2022 19:24:38 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:24:38 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 19:24:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 19:24:38 - INFO - __main__ - Printing 3 examples
05/30/2022 19:24:38 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/30/2022 19:24:38 - INFO - __main__ - ['others']
05/30/2022 19:24:38 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/30/2022 19:24:38 - INFO - __main__ - ['others']
05/30/2022 19:24:38 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/30/2022 19:24:38 - INFO - __main__ - ['others']
05/30/2022 19:24:38 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:24:38 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:24:38 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:24:38 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 19:24:38 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 19:24:56 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 19:24:56 - INFO - __main__ - task name: emo
05/30/2022 19:24:56 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 19:24:56 - INFO - __main__ - task name: emo
05/30/2022 19:24:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 19:24:57 - INFO - __main__ - Starting training!
05/30/2022 19:24:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 19:24:57 - INFO - __main__ - Starting training!
05/30/2022 19:25:00 - INFO - __main__ - Step 10 Global step 10 Train loss 7.50 on epoch=2
05/30/2022 19:25:02 - INFO - __main__ - Step 20 Global step 20 Train loss 3.97 on epoch=4
05/30/2022 19:25:04 - INFO - __main__ - Step 30 Global step 30 Train loss 2.17 on epoch=7
05/30/2022 19:25:07 - INFO - __main__ - Step 40 Global step 40 Train loss 1.59 on epoch=9
05/30/2022 19:25:09 - INFO - __main__ - Step 50 Global step 50 Train loss 1.16 on epoch=12
05/30/2022 19:25:10 - INFO - __main__ - Global step 50 Train loss 3.28 Classification-F1 0.13034188034188032 on epoch=12
05/30/2022 19:25:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13034188034188032 on epoch=12, global_step=50
05/30/2022 19:25:12 - INFO - __main__ - Step 60 Global step 60 Train loss 0.98 on epoch=14
05/30/2022 19:25:14 - INFO - __main__ - Step 70 Global step 70 Train loss 1.08 on epoch=17
05/30/2022 19:25:17 - INFO - __main__ - Step 80 Global step 80 Train loss 1.08 on epoch=19
05/30/2022 19:25:19 - INFO - __main__ - Step 90 Global step 90 Train loss 0.99 on epoch=22
05/30/2022 19:25:21 - INFO - __main__ - Step 100 Global step 100 Train loss 1.05 on epoch=24
05/30/2022 19:25:22 - INFO - __main__ - Global step 100 Train loss 1.03 Classification-F1 0.08974358974358974 on epoch=24
05/30/2022 19:25:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.96 on epoch=27
05/30/2022 19:25:27 - INFO - __main__ - Step 120 Global step 120 Train loss 1.01 on epoch=29
05/30/2022 19:25:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.99 on epoch=32
05/30/2022 19:25:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.90 on epoch=34
05/30/2022 19:25:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.97 on epoch=37
05/30/2022 19:25:35 - INFO - __main__ - Global step 150 Train loss 0.97 Classification-F1 0.09615384615384615 on epoch=37
05/30/2022 19:25:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.89 on epoch=39
05/30/2022 19:25:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.99 on epoch=42
05/30/2022 19:25:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.90 on epoch=44
05/30/2022 19:25:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.88 on epoch=47
05/30/2022 19:25:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.89 on epoch=49
05/30/2022 19:25:47 - INFO - __main__ - Global step 200 Train loss 0.91 Classification-F1 0.09493670886075949 on epoch=49
05/30/2022 19:25:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.89 on epoch=52
05/30/2022 19:25:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.93 on epoch=54
05/30/2022 19:25:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.83 on epoch=57
05/30/2022 19:25:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.99 on epoch=59
05/30/2022 19:25:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.86 on epoch=62
05/30/2022 19:25:59 - INFO - __main__ - Global step 250 Train loss 0.90 Classification-F1 0.1994421906693712 on epoch=62
05/30/2022 19:25:59 - INFO - __main__ - Saving model with best Classification-F1: 0.13034188034188032 -> 0.1994421906693712 on epoch=62, global_step=250
05/30/2022 19:26:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.92 on epoch=64
05/30/2022 19:26:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.92 on epoch=67
05/30/2022 19:26:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.79 on epoch=69
05/30/2022 19:26:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.93 on epoch=72
05/30/2022 19:26:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.88 on epoch=74
05/30/2022 19:26:11 - INFO - __main__ - Global step 300 Train loss 0.89 Classification-F1 0.32484814578005117 on epoch=74
05/30/2022 19:26:11 - INFO - __main__ - Saving model with best Classification-F1: 0.1994421906693712 -> 0.32484814578005117 on epoch=74, global_step=300
05/30/2022 19:26:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.86 on epoch=77
05/30/2022 19:26:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.85 on epoch=79
05/30/2022 19:26:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.86 on epoch=82
05/30/2022 19:26:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.73 on epoch=84
05/30/2022 19:26:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.83 on epoch=87
05/30/2022 19:26:24 - INFO - __main__ - Global step 350 Train loss 0.83 Classification-F1 0.19615384615384615 on epoch=87
05/30/2022 19:26:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.81 on epoch=89
05/30/2022 19:26:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.77 on epoch=92
05/30/2022 19:26:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.80 on epoch=94
05/30/2022 19:26:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.81 on epoch=97
05/30/2022 19:26:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.72 on epoch=99
05/30/2022 19:26:36 - INFO - __main__ - Global step 400 Train loss 0.78 Classification-F1 0.3391729797979798 on epoch=99
05/30/2022 19:26:36 - INFO - __main__ - Saving model with best Classification-F1: 0.32484814578005117 -> 0.3391729797979798 on epoch=99, global_step=400
05/30/2022 19:26:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.70 on epoch=102
05/30/2022 19:26:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.71 on epoch=104
05/30/2022 19:26:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.79 on epoch=107
05/30/2022 19:26:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.83 on epoch=109
05/30/2022 19:26:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.67 on epoch=112
05/30/2022 19:26:48 - INFO - __main__ - Global step 450 Train loss 0.74 Classification-F1 0.2285431959345003 on epoch=112
05/30/2022 19:26:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.67 on epoch=114
05/30/2022 19:26:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.62 on epoch=117
05/30/2022 19:26:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.53 on epoch=119
05/30/2022 19:26:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.62 on epoch=122
05/30/2022 19:27:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.55 on epoch=124
05/30/2022 19:27:01 - INFO - __main__ - Global step 500 Train loss 0.60 Classification-F1 0.3109083191850594 on epoch=124
05/30/2022 19:27:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=127
05/30/2022 19:27:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=129
05/30/2022 19:27:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=132
05/30/2022 19:27:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.56 on epoch=134
05/30/2022 19:27:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=137
05/30/2022 19:27:13 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.41172081492913276 on epoch=137
05/30/2022 19:27:13 - INFO - __main__ - Saving model with best Classification-F1: 0.3391729797979798 -> 0.41172081492913276 on epoch=137, global_step=550
05/30/2022 19:27:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=139
05/30/2022 19:27:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.32 on epoch=142
05/30/2022 19:27:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=144
05/30/2022 19:27:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=147
05/30/2022 19:27:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=149
05/30/2022 19:27:26 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.42042544937281773 on epoch=149
05/30/2022 19:27:26 - INFO - __main__ - Saving model with best Classification-F1: 0.41172081492913276 -> 0.42042544937281773 on epoch=149, global_step=600
05/30/2022 19:27:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.33 on epoch=152
05/30/2022 19:27:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.28 on epoch=154
05/30/2022 19:27:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=157
05/30/2022 19:27:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=159
05/30/2022 19:27:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=162
05/30/2022 19:27:38 - INFO - __main__ - Global step 650 Train loss 0.28 Classification-F1 0.37003915624605277 on epoch=162
05/30/2022 19:27:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=164
05/30/2022 19:27:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=167
05/30/2022 19:27:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.33 on epoch=169
05/30/2022 19:27:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=172
05/30/2022 19:27:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=174
05/30/2022 19:27:51 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.40191994996873043 on epoch=174
05/30/2022 19:27:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=177
05/30/2022 19:27:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=179
05/30/2022 19:27:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=182
05/30/2022 19:28:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=184
05/30/2022 19:28:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=187
05/30/2022 19:28:03 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.3912280701754386 on epoch=187
05/30/2022 19:28:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=189
05/30/2022 19:28:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=192
05/30/2022 19:28:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.11 on epoch=194
05/30/2022 19:28:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.12 on epoch=197
05/30/2022 19:28:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=199
05/30/2022 19:28:15 - INFO - __main__ - Global step 800 Train loss 0.16 Classification-F1 0.40007763975155275 on epoch=199
05/30/2022 19:28:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=202
05/30/2022 19:28:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.16 on epoch=204
05/30/2022 19:28:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.11 on epoch=207
05/30/2022 19:28:24 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=209
05/30/2022 19:28:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=212
05/30/2022 19:28:28 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.4271686194240542 on epoch=212
05/30/2022 19:28:28 - INFO - __main__ - Saving model with best Classification-F1: 0.42042544937281773 -> 0.4271686194240542 on epoch=212, global_step=850
05/30/2022 19:28:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.09 on epoch=214
05/30/2022 19:28:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=217
05/30/2022 19:28:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=219
05/30/2022 19:28:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=222
05/30/2022 19:28:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=224
05/30/2022 19:28:40 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.3578137651821862 on epoch=224
05/30/2022 19:28:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=227
05/30/2022 19:28:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.09 on epoch=229
05/30/2022 19:28:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=232
05/30/2022 19:28:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=234
05/30/2022 19:28:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=237
05/30/2022 19:28:52 - INFO - __main__ - Global step 950 Train loss 0.12 Classification-F1 0.4117218698614048 on epoch=237
05/30/2022 19:28:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=239
05/30/2022 19:28:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=242
05/30/2022 19:28:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=244
05/30/2022 19:29:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=247
05/30/2022 19:29:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=249
05/30/2022 19:29:05 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.40128992628992627 on epoch=249
05/30/2022 19:29:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=252
05/30/2022 19:29:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=254
05/30/2022 19:29:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=257
05/30/2022 19:29:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.13 on epoch=259
05/30/2022 19:29:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=262
05/30/2022 19:29:17 - INFO - __main__ - Global step 1050 Train loss 0.09 Classification-F1 0.43912180135626233 on epoch=262
05/30/2022 19:29:17 - INFO - __main__ - Saving model with best Classification-F1: 0.4271686194240542 -> 0.43912180135626233 on epoch=262, global_step=1050
05/30/2022 19:29:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=264
05/30/2022 19:29:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=267
05/30/2022 19:29:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=269
05/30/2022 19:29:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=272
05/30/2022 19:29:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=274
05/30/2022 19:29:31 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.43889632666722456 on epoch=274
05/30/2022 19:29:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=277
05/30/2022 19:29:36 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=279
05/30/2022 19:29:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
05/30/2022 19:29:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=284
05/30/2022 19:29:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=287
05/30/2022 19:29:44 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.4340017825311943 on epoch=287
05/30/2022 19:29:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=289
05/30/2022 19:29:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
05/30/2022 19:29:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=294
05/30/2022 19:29:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
05/30/2022 19:29:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=299
05/30/2022 19:29:57 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.448741134751773 on epoch=299
05/30/2022 19:29:57 - INFO - __main__ - Saving model with best Classification-F1: 0.43912180135626233 -> 0.448741134751773 on epoch=299, global_step=1200
05/30/2022 19:29:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
05/30/2022 19:30:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=304
05/30/2022 19:30:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=307
05/30/2022 19:30:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=309
05/30/2022 19:30:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=312
05/30/2022 19:30:10 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.4548951048951049 on epoch=312
05/30/2022 19:30:10 - INFO - __main__ - Saving model with best Classification-F1: 0.448741134751773 -> 0.4548951048951049 on epoch=312, global_step=1250
05/30/2022 19:30:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=314
05/30/2022 19:30:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
05/30/2022 19:30:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=319
05/30/2022 19:30:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
05/30/2022 19:30:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
05/30/2022 19:30:23 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.4431206171107994 on epoch=324
05/30/2022 19:30:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.13 on epoch=327
05/30/2022 19:30:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=329
05/30/2022 19:30:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
05/30/2022 19:30:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
05/30/2022 19:30:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
05/30/2022 19:30:36 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.4194124559341951 on epoch=337
05/30/2022 19:30:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=339
05/30/2022 19:30:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=342
05/30/2022 19:30:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=344
05/30/2022 19:30:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.09 on epoch=347
05/30/2022 19:30:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
05/30/2022 19:30:49 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.45989658489658486 on epoch=349
05/30/2022 19:30:49 - INFO - __main__ - Saving model with best Classification-F1: 0.4548951048951049 -> 0.45989658489658486 on epoch=349, global_step=1400
05/30/2022 19:30:51 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=352
05/30/2022 19:30:53 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=354
05/30/2022 19:30:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=357
05/30/2022 19:30:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=359
05/30/2022 19:31:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=362
05/30/2022 19:31:01 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.5292297290268894 on epoch=362
05/30/2022 19:31:01 - INFO - __main__ - Saving model with best Classification-F1: 0.45989658489658486 -> 0.5292297290268894 on epoch=362, global_step=1450
05/30/2022 19:31:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
05/30/2022 19:31:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=367
05/30/2022 19:31:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/30/2022 19:31:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
05/30/2022 19:31:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=374
05/30/2022 19:31:14 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.4139933166248956 on epoch=374
05/30/2022 19:31:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
05/30/2022 19:31:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
05/30/2022 19:31:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
05/30/2022 19:31:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/30/2022 19:31:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=387
05/30/2022 19:31:27 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.44365721997300955 on epoch=387
05/30/2022 19:31:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
05/30/2022 19:31:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=392
05/30/2022 19:31:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
05/30/2022 19:31:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
05/30/2022 19:31:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/30/2022 19:31:40 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.4224491289708681 on epoch=399
05/30/2022 19:31:42 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/30/2022 19:31:45 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=404
05/30/2022 19:31:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
05/30/2022 19:31:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
05/30/2022 19:31:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=412
05/30/2022 19:31:53 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.5719835907335907 on epoch=412
05/30/2022 19:31:53 - INFO - __main__ - Saving model with best Classification-F1: 0.5292297290268894 -> 0.5719835907335907 on epoch=412, global_step=1650
05/30/2022 19:31:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/30/2022 19:31:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=417
05/30/2022 19:32:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=419
05/30/2022 19:32:03 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=422
05/30/2022 19:32:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/30/2022 19:32:06 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.43723684210526315 on epoch=424
05/30/2022 19:32:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/30/2022 19:32:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/30/2022 19:32:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/30/2022 19:32:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=434
05/30/2022 19:32:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
05/30/2022 19:32:19 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.4616028708133971 on epoch=437
05/30/2022 19:32:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=439
05/30/2022 19:32:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
05/30/2022 19:32:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
05/30/2022 19:32:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
05/30/2022 19:32:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=449
05/30/2022 19:32:32 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.4598244348244348 on epoch=449
05/30/2022 19:32:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/30/2022 19:32:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=454
05/30/2022 19:32:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
05/30/2022 19:32:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=459
05/30/2022 19:32:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=462
05/30/2022 19:32:45 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.45460435801528964 on epoch=462
05/30/2022 19:32:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
05/30/2022 19:32:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/30/2022 19:32:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=469
05/30/2022 19:32:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/30/2022 19:32:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/30/2022 19:32:58 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.46630359952793077 on epoch=474
05/30/2022 19:33:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/30/2022 19:33:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=479
05/30/2022 19:33:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=482
05/30/2022 19:33:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=484
05/30/2022 19:33:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/30/2022 19:33:11 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.4780612244897959 on epoch=487
05/30/2022 19:33:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/30/2022 19:33:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=492
05/30/2022 19:33:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=494
05/30/2022 19:33:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/30/2022 19:33:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
05/30/2022 19:33:24 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.5093710940787403 on epoch=499
05/30/2022 19:33:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/30/2022 19:33:29 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/30/2022 19:33:32 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.19 on epoch=507
05/30/2022 19:33:34 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/30/2022 19:33:36 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/30/2022 19:33:38 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.4723281596452329 on epoch=512
05/30/2022 19:33:40 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=514
05/30/2022 19:33:42 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
05/30/2022 19:33:45 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/30/2022 19:33:47 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/30/2022 19:33:49 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
05/30/2022 19:33:51 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.5053198653198653 on epoch=524
05/30/2022 19:33:53 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
05/30/2022 19:33:55 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/30/2022 19:33:58 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
05/30/2022 19:34:00 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/30/2022 19:34:03 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=537
05/30/2022 19:34:04 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.5114176245210729 on epoch=537
05/30/2022 19:34:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
05/30/2022 19:34:09 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/30/2022 19:34:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/30/2022 19:34:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/30/2022 19:34:16 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/30/2022 19:34:17 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.46401808785529713 on epoch=549
05/30/2022 19:34:19 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/30/2022 19:34:22 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.09 on epoch=554
05/30/2022 19:34:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/30/2022 19:34:26 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.09 on epoch=559
05/30/2022 19:34:29 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=562
05/30/2022 19:34:30 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.498515499194847 on epoch=562
05/30/2022 19:34:32 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
05/30/2022 19:34:35 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/30/2022 19:34:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=569
05/30/2022 19:34:39 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/30/2022 19:34:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
05/30/2022 19:34:42 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.4637512512512513 on epoch=574
05/30/2022 19:34:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/30/2022 19:34:47 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/30/2022 19:34:49 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/30/2022 19:34:52 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=584
05/30/2022 19:34:54 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.09 on epoch=587
05/30/2022 19:34:55 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.5467967967967968 on epoch=587
05/30/2022 19:34:57 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/30/2022 19:35:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/30/2022 19:35:02 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/30/2022 19:35:04 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
05/30/2022 19:35:07 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
05/30/2022 19:35:08 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.5454795711388004 on epoch=599
05/30/2022 19:35:10 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
05/30/2022 19:35:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
05/30/2022 19:35:15 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=607
05/30/2022 19:35:17 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/30/2022 19:35:19 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/30/2022 19:35:20 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.4652411725582457 on epoch=612
05/30/2022 19:35:22 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=614
05/30/2022 19:35:25 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.12 on epoch=617
05/30/2022 19:35:27 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=619
05/30/2022 19:35:29 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/30/2022 19:35:32 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
05/30/2022 19:35:33 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.5618371212121211 on epoch=624
05/30/2022 19:35:35 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=627
05/30/2022 19:35:37 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/30/2022 19:35:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/30/2022 19:35:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/30/2022 19:35:44 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/30/2022 19:35:45 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.5298738000350904 on epoch=637
05/30/2022 19:35:47 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=639
05/30/2022 19:35:50 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/30/2022 19:35:52 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=644
05/30/2022 19:35:54 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/30/2022 19:35:57 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=649
05/30/2022 19:35:58 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.4721840659340659 on epoch=649
05/30/2022 19:36:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/30/2022 19:36:03 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=654
05/30/2022 19:36:05 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/30/2022 19:36:07 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/30/2022 19:36:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.08 on epoch=662
05/30/2022 19:36:10 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.43207577306414513 on epoch=662
05/30/2022 19:36:13 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=664
05/30/2022 19:36:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
05/30/2022 19:36:17 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/30/2022 19:36:19 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/30/2022 19:36:22 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
05/30/2022 19:36:23 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.440366085766966 on epoch=674
05/30/2022 19:36:25 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/30/2022 19:36:27 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/30/2022 19:36:30 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=682
05/30/2022 19:36:32 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/30/2022 19:36:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=687
05/30/2022 19:36:35 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.41133540372670807 on epoch=687
05/30/2022 19:36:38 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
05/30/2022 19:36:40 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/30/2022 19:36:42 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/30/2022 19:36:44 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/30/2022 19:36:47 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/30/2022 19:36:48 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.5035067873303167 on epoch=699
05/30/2022 19:36:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=702
05/30/2022 19:36:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
05/30/2022 19:36:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/30/2022 19:36:57 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/30/2022 19:36:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/30/2022 19:37:00 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.47120029728725377 on epoch=712
05/30/2022 19:37:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/30/2022 19:37:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/30/2022 19:37:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=719
05/30/2022 19:37:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
05/30/2022 19:37:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
05/30/2022 19:37:13 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.47363557247278176 on epoch=724
05/30/2022 19:37:15 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/30/2022 19:37:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/30/2022 19:37:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/30/2022 19:37:22 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.20 on epoch=734
05/30/2022 19:37:25 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/30/2022 19:37:26 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.4714214214214214 on epoch=737
05/30/2022 19:37:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/30/2022 19:37:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/30/2022 19:37:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/30/2022 19:37:35 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/30/2022 19:37:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/30/2022 19:37:39 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.4987006237006236 on epoch=749
05/30/2022 19:37:39 - INFO - __main__ - save last model!
05/30/2022 19:37:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 19:37:39 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 19:37:39 - INFO - __main__ - Printing 3 examples
05/30/2022 19:37:39 - INFO - __main__ -  [emo] how cause yes am listening
05/30/2022 19:37:39 - INFO - __main__ - ['others']
05/30/2022 19:37:39 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/30/2022 19:37:39 - INFO - __main__ - ['others']
05/30/2022 19:37:39 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/30/2022 19:37:39 - INFO - __main__ - ['others']
05/30/2022 19:37:39 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:37:39 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 19:37:39 - INFO - __main__ - Printing 3 examples
05/30/2022 19:37:39 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 19:37:39 - INFO - __main__ - ['others']
05/30/2022 19:37:39 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 19:37:39 - INFO - __main__ - ['others']
05/30/2022 19:37:39 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 19:37:39 - INFO - __main__ - ['others']
05/30/2022 19:37:39 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:37:39 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:37:39 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 19:37:39 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 19:37:39 - INFO - __main__ - Printing 3 examples
05/30/2022 19:37:39 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/30/2022 19:37:39 - INFO - __main__ - ['others']
05/30/2022 19:37:39 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/30/2022 19:37:39 - INFO - __main__ - ['others']
05/30/2022 19:37:39 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/30/2022 19:37:39 - INFO - __main__ - ['others']
05/30/2022 19:37:39 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:37:39 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:37:39 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 19:37:41 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:37:46 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 19:37:54 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 19:37:54 - INFO - __main__ - task name: emo
05/30/2022 19:37:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 19:37:55 - INFO - __main__ - Starting training!
05/30/2022 19:39:15 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_100_0.5_8_predictions.txt
05/30/2022 19:39:15 - INFO - __main__ - Classification-F1 on test data: 0.3738
05/30/2022 19:39:15 - INFO - __main__ - prefix=emo_16_100, lr=0.5, bsz=8, dev_performance=0.5719835907335907, test_performance=0.3738377087089034
05/30/2022 19:39:15 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.4, bsz=8 ...
05/30/2022 19:39:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 19:39:16 - INFO - __main__ - Printing 3 examples
05/30/2022 19:39:16 - INFO - __main__ -  [emo] how cause yes am listening
05/30/2022 19:39:16 - INFO - __main__ - ['others']
05/30/2022 19:39:16 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/30/2022 19:39:16 - INFO - __main__ - ['others']
05/30/2022 19:39:16 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/30/2022 19:39:16 - INFO - __main__ - ['others']
05/30/2022 19:39:16 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:39:16 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:39:16 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 19:39:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 19:39:16 - INFO - __main__ - Printing 3 examples
05/30/2022 19:39:16 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/30/2022 19:39:16 - INFO - __main__ - ['others']
05/30/2022 19:39:16 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/30/2022 19:39:16 - INFO - __main__ - ['others']
05/30/2022 19:39:16 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/30/2022 19:39:16 - INFO - __main__ - ['others']
05/30/2022 19:39:16 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:39:16 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:39:16 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 19:39:31 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 19:39:31 - INFO - __main__ - task name: emo
05/30/2022 19:39:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 19:39:32 - INFO - __main__ - Starting training!
05/30/2022 19:39:34 - INFO - __main__ - Step 10 Global step 10 Train loss 7.69 on epoch=2
05/30/2022 19:39:37 - INFO - __main__ - Step 20 Global step 20 Train loss 3.64 on epoch=4
05/30/2022 19:39:39 - INFO - __main__ - Step 30 Global step 30 Train loss 1.72 on epoch=7
05/30/2022 19:39:42 - INFO - __main__ - Step 40 Global step 40 Train loss 1.22 on epoch=9
05/30/2022 19:39:44 - INFO - __main__ - Step 50 Global step 50 Train loss 1.19 on epoch=12
05/30/2022 19:39:45 - INFO - __main__ - Global step 50 Train loss 3.09 Classification-F1 0.16223908918406071 on epoch=12
05/30/2022 19:39:45 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16223908918406071 on epoch=12, global_step=50
05/30/2022 19:39:47 - INFO - __main__ - Step 60 Global step 60 Train loss 1.04 on epoch=14
05/30/2022 19:39:50 - INFO - __main__ - Step 70 Global step 70 Train loss 1.06 on epoch=17
05/30/2022 19:39:52 - INFO - __main__ - Step 80 Global step 80 Train loss 1.14 on epoch=19
05/30/2022 19:39:55 - INFO - __main__ - Step 90 Global step 90 Train loss 1.02 on epoch=22
05/30/2022 19:39:57 - INFO - __main__ - Step 100 Global step 100 Train loss 1.01 on epoch=24
05/30/2022 19:39:58 - INFO - __main__ - Global step 100 Train loss 1.05 Classification-F1 0.09615384615384615 on epoch=24
05/30/2022 19:40:00 - INFO - __main__ - Step 110 Global step 110 Train loss 0.97 on epoch=27
05/30/2022 19:40:03 - INFO - __main__ - Step 120 Global step 120 Train loss 1.03 on epoch=29
05/30/2022 19:40:05 - INFO - __main__ - Step 130 Global step 130 Train loss 1.00 on epoch=32
05/30/2022 19:40:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.94 on epoch=34
05/30/2022 19:40:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.96 on epoch=37
05/30/2022 19:40:11 - INFO - __main__ - Global step 150 Train loss 0.98 Classification-F1 0.1 on epoch=37
05/30/2022 19:40:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.95 on epoch=39
05/30/2022 19:40:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.88 on epoch=42
05/30/2022 19:40:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.86 on epoch=44
05/30/2022 19:40:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.91 on epoch=47
05/30/2022 19:40:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.95 on epoch=49
05/30/2022 19:40:24 - INFO - __main__ - Global step 200 Train loss 0.91 Classification-F1 0.1 on epoch=49
05/30/2022 19:40:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.89 on epoch=52
05/30/2022 19:40:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.97 on epoch=54
05/30/2022 19:40:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.84 on epoch=57
05/30/2022 19:40:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.87 on epoch=59
05/30/2022 19:40:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.91 on epoch=62
05/30/2022 19:40:37 - INFO - __main__ - Global step 250 Train loss 0.89 Classification-F1 0.3044299450549451 on epoch=62
05/30/2022 19:40:37 - INFO - __main__ - Saving model with best Classification-F1: 0.16223908918406071 -> 0.3044299450549451 on epoch=62, global_step=250
05/30/2022 19:40:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=64
05/30/2022 19:40:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.87 on epoch=67
05/30/2022 19:40:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.83 on epoch=69
05/30/2022 19:40:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.92 on epoch=72
05/30/2022 19:40:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.85 on epoch=74
05/30/2022 19:40:51 - INFO - __main__ - Global step 300 Train loss 0.87 Classification-F1 0.2639705492875682 on epoch=74
05/30/2022 19:40:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.78 on epoch=77
05/30/2022 19:40:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.79 on epoch=79
05/30/2022 19:40:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.88 on epoch=82
05/30/2022 19:41:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.74 on epoch=84
05/30/2022 19:41:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.82 on epoch=87
05/30/2022 19:41:04 - INFO - __main__ - Global step 350 Train loss 0.80 Classification-F1 0.13083538083538082 on epoch=87
05/30/2022 19:41:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.79 on epoch=89
05/30/2022 19:41:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.71 on epoch=92
05/30/2022 19:41:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.74 on epoch=94
05/30/2022 19:41:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.77 on epoch=97
05/30/2022 19:41:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.70 on epoch=99
05/30/2022 19:41:17 - INFO - __main__ - Global step 400 Train loss 0.74 Classification-F1 0.41740650534255186 on epoch=99
05/30/2022 19:41:17 - INFO - __main__ - Saving model with best Classification-F1: 0.3044299450549451 -> 0.41740650534255186 on epoch=99, global_step=400
05/30/2022 19:41:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.69 on epoch=102
05/30/2022 19:41:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.65 on epoch=104
05/30/2022 19:41:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.75 on epoch=107
05/30/2022 19:41:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.72 on epoch=109
05/30/2022 19:41:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.75 on epoch=112
05/30/2022 19:41:30 - INFO - __main__ - Global step 450 Train loss 0.71 Classification-F1 0.459004329004329 on epoch=112
05/30/2022 19:41:30 - INFO - __main__ - Saving model with best Classification-F1: 0.41740650534255186 -> 0.459004329004329 on epoch=112, global_step=450
05/30/2022 19:41:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.60 on epoch=114
05/30/2022 19:41:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.64 on epoch=117
05/30/2022 19:41:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.65 on epoch=119
05/30/2022 19:41:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.72 on epoch=122
05/30/2022 19:41:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.61 on epoch=124
05/30/2022 19:41:43 - INFO - __main__ - Global step 500 Train loss 0.64 Classification-F1 0.46267799950726785 on epoch=124
05/30/2022 19:41:43 - INFO - __main__ - Saving model with best Classification-F1: 0.459004329004329 -> 0.46267799950726785 on epoch=124, global_step=500
05/30/2022 19:41:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.51 on epoch=127
05/30/2022 19:41:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.63 on epoch=129
05/30/2022 19:41:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.57 on epoch=132
05/30/2022 19:41:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.56 on epoch=134
05/30/2022 19:41:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.51 on epoch=137
05/30/2022 19:41:57 - INFO - __main__ - Global step 550 Train loss 0.56 Classification-F1 0.4767006802721089 on epoch=137
05/30/2022 19:41:57 - INFO - __main__ - Saving model with best Classification-F1: 0.46267799950726785 -> 0.4767006802721089 on epoch=137, global_step=550
05/30/2022 19:41:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.56 on epoch=139
05/30/2022 19:42:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.63 on epoch=142
05/30/2022 19:42:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.47 on epoch=144
05/30/2022 19:42:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.53 on epoch=147
05/30/2022 19:42:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=149
05/30/2022 19:42:10 - INFO - __main__ - Global step 600 Train loss 0.54 Classification-F1 0.47182795698924734 on epoch=149
05/30/2022 19:42:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=152
05/30/2022 19:42:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=154
05/30/2022 19:42:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.53 on epoch=157
05/30/2022 19:42:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.49 on epoch=159
05/30/2022 19:42:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.48 on epoch=162
05/30/2022 19:42:23 - INFO - __main__ - Global step 650 Train loss 0.49 Classification-F1 0.49112318840579705 on epoch=162
05/30/2022 19:42:23 - INFO - __main__ - Saving model with best Classification-F1: 0.4767006802721089 -> 0.49112318840579705 on epoch=162, global_step=650
05/30/2022 19:42:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=164
05/30/2022 19:42:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.34 on epoch=167
05/30/2022 19:42:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=169
05/30/2022 19:42:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=172
05/30/2022 19:42:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=174
05/30/2022 19:42:36 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.5017912428129147 on epoch=174
05/30/2022 19:42:36 - INFO - __main__ - Saving model with best Classification-F1: 0.49112318840579705 -> 0.5017912428129147 on epoch=174, global_step=700
05/30/2022 19:42:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=177
05/30/2022 19:42:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=179
05/30/2022 19:42:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=182
05/30/2022 19:42:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=184
05/30/2022 19:42:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=187
05/30/2022 19:42:50 - INFO - __main__ - Global step 750 Train loss 0.38 Classification-F1 0.5599831330381615 on epoch=187
05/30/2022 19:42:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5017912428129147 -> 0.5599831330381615 on epoch=187, global_step=750
05/30/2022 19:42:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=189
05/30/2022 19:42:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=192
05/30/2022 19:42:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.31 on epoch=194
05/30/2022 19:42:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=197
05/30/2022 19:43:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=199
05/30/2022 19:43:03 - INFO - __main__ - Global step 800 Train loss 0.35 Classification-F1 0.5902507402507403 on epoch=199
05/30/2022 19:43:03 - INFO - __main__ - Saving model with best Classification-F1: 0.5599831330381615 -> 0.5902507402507403 on epoch=199, global_step=800
05/30/2022 19:43:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=202
05/30/2022 19:43:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.24 on epoch=204
05/30/2022 19:43:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.28 on epoch=207
05/30/2022 19:43:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=209
05/30/2022 19:43:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=212
05/30/2022 19:43:16 - INFO - __main__ - Global step 850 Train loss 0.25 Classification-F1 0.518327531681569 on epoch=212
05/30/2022 19:43:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=214
05/30/2022 19:43:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=217
05/30/2022 19:43:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.24 on epoch=219
05/30/2022 19:43:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=222
05/30/2022 19:43:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.19 on epoch=224
05/30/2022 19:43:29 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.5205209895052474 on epoch=224
05/30/2022 19:43:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.25 on epoch=227
05/30/2022 19:43:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.33 on epoch=229
05/30/2022 19:43:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.27 on epoch=232
05/30/2022 19:43:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=234
05/30/2022 19:43:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=237
05/30/2022 19:43:43 - INFO - __main__ - Global step 950 Train loss 0.26 Classification-F1 0.5176724137931035 on epoch=237
05/30/2022 19:43:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=239
05/30/2022 19:43:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=242
05/30/2022 19:43:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=244
05/30/2022 19:43:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=247
05/30/2022 19:43:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=249
05/30/2022 19:43:56 - INFO - __main__ - Global step 1000 Train loss 0.17 Classification-F1 0.5873819163292847 on epoch=249
05/30/2022 19:43:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.16 on epoch=252
05/30/2022 19:44:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=254
05/30/2022 19:44:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.13 on epoch=257
05/30/2022 19:44:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.24 on epoch=259
05/30/2022 19:44:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=262
05/30/2022 19:44:09 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.6150568181818181 on epoch=262
05/30/2022 19:44:09 - INFO - __main__ - Saving model with best Classification-F1: 0.5902507402507403 -> 0.6150568181818181 on epoch=262, global_step=1050
05/30/2022 19:44:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=264
05/30/2022 19:44:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=267
05/30/2022 19:44:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=269
05/30/2022 19:44:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.13 on epoch=272
05/30/2022 19:44:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
05/30/2022 19:44:23 - INFO - __main__ - Global step 1100 Train loss 0.14 Classification-F1 0.5421401515151515 on epoch=274
05/30/2022 19:44:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.14 on epoch=277
05/30/2022 19:44:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=279
05/30/2022 19:44:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.15 on epoch=282
05/30/2022 19:44:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.14 on epoch=284
05/30/2022 19:44:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=287
05/30/2022 19:44:37 - INFO - __main__ - Global step 1150 Train loss 0.13 Classification-F1 0.6022496022496022 on epoch=287
05/30/2022 19:44:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=289
05/30/2022 19:44:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.12 on epoch=292
05/30/2022 19:44:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=294
05/30/2022 19:44:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=297
05/30/2022 19:44:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=299
05/30/2022 19:44:50 - INFO - __main__ - Global step 1200 Train loss 0.11 Classification-F1 0.5734486102133162 on epoch=299
05/30/2022 19:44:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=302
05/30/2022 19:44:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=304
05/30/2022 19:44:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=307
05/30/2022 19:45:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=309
05/30/2022 19:45:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=312
05/30/2022 19:45:04 - INFO - __main__ - Global step 1250 Train loss 0.09 Classification-F1 0.5716712262874666 on epoch=312
05/30/2022 19:45:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.09 on epoch=314
05/30/2022 19:45:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.13 on epoch=317
05/30/2022 19:45:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
05/30/2022 19:45:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=322
05/30/2022 19:45:16 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.13 on epoch=324
05/30/2022 19:45:17 - INFO - __main__ - Global step 1300 Train loss 0.09 Classification-F1 0.6263657245580745 on epoch=324
05/30/2022 19:45:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6150568181818181 -> 0.6263657245580745 on epoch=324, global_step=1300
05/30/2022 19:45:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=327
05/30/2022 19:45:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=329
05/30/2022 19:45:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=332
05/30/2022 19:45:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.13 on epoch=334
05/30/2022 19:45:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=337
05/30/2022 19:45:31 - INFO - __main__ - Global step 1350 Train loss 0.11 Classification-F1 0.5395021645021645 on epoch=337
05/30/2022 19:45:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=339
05/30/2022 19:45:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=342
05/30/2022 19:45:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=344
05/30/2022 19:45:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=347
05/30/2022 19:45:43 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=349
05/30/2022 19:45:44 - INFO - __main__ - Global step 1400 Train loss 0.09 Classification-F1 0.5826493723184492 on epoch=349
05/30/2022 19:45:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=352
05/30/2022 19:45:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=354
05/30/2022 19:45:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
05/30/2022 19:45:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=359
05/30/2022 19:45:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=362
05/30/2022 19:45:58 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.6149054172951232 on epoch=362
05/30/2022 19:46:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
05/30/2022 19:46:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=367
05/30/2022 19:46:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=369
05/30/2022 19:46:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=372
05/30/2022 19:46:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
05/30/2022 19:46:11 - INFO - __main__ - Global step 1500 Train loss 0.08 Classification-F1 0.5181000838895575 on epoch=374
05/30/2022 19:46:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=377
05/30/2022 19:46:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
05/30/2022 19:46:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=382
05/30/2022 19:46:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=384
05/30/2022 19:46:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/30/2022 19:46:24 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.5409387065637066 on epoch=387
05/30/2022 19:46:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
05/30/2022 19:46:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=392
05/30/2022 19:46:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=394
05/30/2022 19:46:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
05/30/2022 19:46:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=399
05/30/2022 19:46:37 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.5303571428571429 on epoch=399
05/30/2022 19:46:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
05/30/2022 19:46:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
05/30/2022 19:46:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/30/2022 19:46:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=409
05/30/2022 19:46:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=412
05/30/2022 19:46:50 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.5448479767000574 on epoch=412
05/30/2022 19:46:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.10 on epoch=414
05/30/2022 19:46:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=417
05/30/2022 19:46:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
05/30/2022 19:47:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
05/30/2022 19:47:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
05/30/2022 19:47:03 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.5553058783321941 on epoch=424
05/30/2022 19:47:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=427
05/30/2022 19:47:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
05/30/2022 19:47:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/30/2022 19:47:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
05/30/2022 19:47:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/30/2022 19:47:16 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.6436011904761905 on epoch=437
05/30/2022 19:47:16 - INFO - __main__ - Saving model with best Classification-F1: 0.6263657245580745 -> 0.6436011904761905 on epoch=437, global_step=1750
05/30/2022 19:47:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/30/2022 19:47:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=442
05/30/2022 19:47:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.11 on epoch=444
05/30/2022 19:47:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
05/30/2022 19:47:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=449
05/30/2022 19:47:29 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.5976333476333475 on epoch=449
05/30/2022 19:47:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=452
05/30/2022 19:47:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=454
05/30/2022 19:47:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/30/2022 19:47:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/30/2022 19:47:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=462
05/30/2022 19:47:42 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.60112227872239 on epoch=462
05/30/2022 19:47:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=464
05/30/2022 19:47:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/30/2022 19:47:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
05/30/2022 19:47:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/30/2022 19:47:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/30/2022 19:47:56 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.555330459770115 on epoch=474
05/30/2022 19:47:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/30/2022 19:48:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/30/2022 19:48:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/30/2022 19:48:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=484
05/30/2022 19:48:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=487
05/30/2022 19:48:09 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.5725 on epoch=487
05/30/2022 19:48:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/30/2022 19:48:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
05/30/2022 19:48:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/30/2022 19:48:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=497
05/30/2022 19:48:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=499
05/30/2022 19:48:22 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.5292460317460317 on epoch=499
05/30/2022 19:48:25 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=502
05/30/2022 19:48:27 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/30/2022 19:48:30 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/30/2022 19:48:32 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=509
05/30/2022 19:48:34 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/30/2022 19:48:36 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.5881820119352089 on epoch=512
05/30/2022 19:48:38 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=514
05/30/2022 19:48:40 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=517
05/30/2022 19:48:43 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/30/2022 19:48:45 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
05/30/2022 19:48:48 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/30/2022 19:48:49 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.5955436720142602 on epoch=524
05/30/2022 19:48:51 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
05/30/2022 19:48:54 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/30/2022 19:48:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
05/30/2022 19:48:59 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/30/2022 19:49:01 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=537
05/30/2022 19:49:02 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.6226689976689977 on epoch=537
05/30/2022 19:49:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=539
05/30/2022 19:49:07 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=542
05/30/2022 19:49:09 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/30/2022 19:49:12 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/30/2022 19:49:14 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
05/30/2022 19:49:15 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.5896506018525653 on epoch=549
05/30/2022 19:49:18 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=552
05/30/2022 19:49:20 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
05/30/2022 19:49:23 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
05/30/2022 19:49:25 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/30/2022 19:49:27 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=562
05/30/2022 19:49:29 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.5828185328185328 on epoch=562
05/30/2022 19:49:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/30/2022 19:49:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
05/30/2022 19:49:36 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=569
05/30/2022 19:49:38 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/30/2022 19:49:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.11 on epoch=574
05/30/2022 19:49:42 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.5750741390788598 on epoch=574
05/30/2022 19:49:44 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/30/2022 19:49:47 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/30/2022 19:49:49 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/30/2022 19:49:52 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/30/2022 19:49:54 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
05/30/2022 19:49:55 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.6088135423948866 on epoch=587
05/30/2022 19:49:58 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
05/30/2022 19:50:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
05/30/2022 19:50:03 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.07 on epoch=594
05/30/2022 19:50:05 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=597
05/30/2022 19:50:08 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/30/2022 19:50:09 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.5757917007917008 on epoch=599
05/30/2022 19:50:11 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=602
05/30/2022 19:50:13 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/30/2022 19:50:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=607
05/30/2022 19:50:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/30/2022 19:50:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
05/30/2022 19:50:22 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.6448168948168947 on epoch=612
05/30/2022 19:50:22 - INFO - __main__ - Saving model with best Classification-F1: 0.6436011904761905 -> 0.6448168948168947 on epoch=612, global_step=2450
05/30/2022 19:50:24 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/30/2022 19:50:27 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/30/2022 19:50:29 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/30/2022 19:50:32 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=622
05/30/2022 19:50:34 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/30/2022 19:50:35 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.6347751710654936 on epoch=624
05/30/2022 19:50:38 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/30/2022 19:50:40 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/30/2022 19:50:43 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/30/2022 19:50:45 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/30/2022 19:50:48 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/30/2022 19:50:49 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.5999266862170088 on epoch=637
05/30/2022 19:50:51 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=639
05/30/2022 19:50:54 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/30/2022 19:50:56 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=644
05/30/2022 19:50:59 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/30/2022 19:51:01 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/30/2022 19:51:02 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.6185828877005348 on epoch=649
05/30/2022 19:51:05 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=652
05/30/2022 19:51:07 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/30/2022 19:51:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/30/2022 19:51:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/30/2022 19:51:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
05/30/2022 19:51:15 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.6037519936204147 on epoch=662
05/30/2022 19:51:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/30/2022 19:51:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=667
05/30/2022 19:51:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/30/2022 19:51:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
05/30/2022 19:51:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=674
05/30/2022 19:51:29 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.5764513793747665 on epoch=674
05/30/2022 19:51:31 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/30/2022 19:51:34 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/30/2022 19:51:36 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
05/30/2022 19:51:39 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/30/2022 19:51:41 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
05/30/2022 19:51:42 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6373303167420814 on epoch=687
05/30/2022 19:51:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/30/2022 19:51:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=692
05/30/2022 19:51:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/30/2022 19:51:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
05/30/2022 19:51:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/30/2022 19:51:56 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6285126270713972 on epoch=699
05/30/2022 19:51:58 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/30/2022 19:52:00 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=704
05/30/2022 19:52:03 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=707
05/30/2022 19:52:05 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
05/30/2022 19:52:08 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/30/2022 19:52:09 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.5821428571428572 on epoch=712
05/30/2022 19:52:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/30/2022 19:52:14 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/30/2022 19:52:16 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/30/2022 19:52:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=722
05/30/2022 19:52:21 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/30/2022 19:52:22 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.5817583732057416 on epoch=724
05/30/2022 19:52:25 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
05/30/2022 19:52:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=729
05/30/2022 19:52:29 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/30/2022 19:52:32 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/30/2022 19:52:34 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/30/2022 19:52:35 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.5937615923096866 on epoch=737
05/30/2022 19:52:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
05/30/2022 19:52:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=742
05/30/2022 19:52:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/30/2022 19:52:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/30/2022 19:52:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
05/30/2022 19:52:49 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 19:52:49 - INFO - __main__ - Printing 3 examples
05/30/2022 19:52:49 - INFO - __main__ -  [emo] how cause yes am listening
05/30/2022 19:52:49 - INFO - __main__ - ['others']
05/30/2022 19:52:49 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/30/2022 19:52:49 - INFO - __main__ - ['others']
05/30/2022 19:52:49 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/30/2022 19:52:49 - INFO - __main__ - ['others']
05/30/2022 19:52:49 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:52:49 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:52:49 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 19:52:49 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 19:52:49 - INFO - __main__ - Printing 3 examples
05/30/2022 19:52:49 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/30/2022 19:52:49 - INFO - __main__ - ['others']
05/30/2022 19:52:49 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/30/2022 19:52:49 - INFO - __main__ - ['others']
05/30/2022 19:52:49 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/30/2022 19:52:49 - INFO - __main__ - ['others']
05/30/2022 19:52:49 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:52:49 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.567191523073876 on epoch=749
05/30/2022 19:52:49 - INFO - __main__ - save last model!
05/30/2022 19:52:49 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:52:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 19:52:49 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 19:52:49 - INFO - __main__ - Printing 3 examples
05/30/2022 19:52:49 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 19:52:49 - INFO - __main__ - ['others']
05/30/2022 19:52:49 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 19:52:49 - INFO - __main__ - ['others']
05/30/2022 19:52:49 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 19:52:49 - INFO - __main__ - ['others']
05/30/2022 19:52:49 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:52:49 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 19:52:51 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:52:56 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 19:53:04 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 19:53:04 - INFO - __main__ - task name: emo
05/30/2022 19:53:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 19:53:04 - INFO - __main__ - Starting training!
05/30/2022 19:54:27 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_100_0.4_8_predictions.txt
05/30/2022 19:54:27 - INFO - __main__ - Classification-F1 on test data: 0.2565
05/30/2022 19:54:27 - INFO - __main__ - prefix=emo_16_100, lr=0.4, bsz=8, dev_performance=0.6448168948168947, test_performance=0.25652236148523655
05/30/2022 19:54:27 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.3, bsz=8 ...
05/30/2022 19:54:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 19:54:28 - INFO - __main__ - Printing 3 examples
05/30/2022 19:54:28 - INFO - __main__ -  [emo] how cause yes am listening
05/30/2022 19:54:28 - INFO - __main__ - ['others']
05/30/2022 19:54:28 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/30/2022 19:54:28 - INFO - __main__ - ['others']
05/30/2022 19:54:28 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/30/2022 19:54:28 - INFO - __main__ - ['others']
05/30/2022 19:54:28 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:54:28 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:54:28 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 19:54:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 19:54:28 - INFO - __main__ - Printing 3 examples
05/30/2022 19:54:28 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/30/2022 19:54:28 - INFO - __main__ - ['others']
05/30/2022 19:54:28 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/30/2022 19:54:28 - INFO - __main__ - ['others']
05/30/2022 19:54:28 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/30/2022 19:54:28 - INFO - __main__ - ['others']
05/30/2022 19:54:28 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:54:28 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:54:28 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 19:54:43 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 19:54:43 - INFO - __main__ - task name: emo
05/30/2022 19:54:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 19:54:44 - INFO - __main__ - Starting training!
05/30/2022 19:54:46 - INFO - __main__ - Step 10 Global step 10 Train loss 7.89 on epoch=2
05/30/2022 19:54:49 - INFO - __main__ - Step 20 Global step 20 Train loss 5.93 on epoch=4
05/30/2022 19:54:51 - INFO - __main__ - Step 30 Global step 30 Train loss 3.71 on epoch=7
05/30/2022 19:54:53 - INFO - __main__ - Step 40 Global step 40 Train loss 2.27 on epoch=9
05/30/2022 19:54:56 - INFO - __main__ - Step 50 Global step 50 Train loss 1.55 on epoch=12
05/30/2022 19:54:56 - INFO - __main__ - Global step 50 Train loss 4.27 Classification-F1 0.2929032258064517 on epoch=12
05/30/2022 19:54:56 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2929032258064517 on epoch=12, global_step=50
05/30/2022 19:54:59 - INFO - __main__ - Step 60 Global step 60 Train loss 1.30 on epoch=14
05/30/2022 19:55:01 - INFO - __main__ - Step 70 Global step 70 Train loss 1.16 on epoch=17
05/30/2022 19:55:04 - INFO - __main__ - Step 80 Global step 80 Train loss 1.23 on epoch=19
05/30/2022 19:55:06 - INFO - __main__ - Step 90 Global step 90 Train loss 1.09 on epoch=22
05/30/2022 19:55:09 - INFO - __main__ - Step 100 Global step 100 Train loss 1.10 on epoch=24
05/30/2022 19:55:09 - INFO - __main__ - Global step 100 Train loss 1.18 Classification-F1 0.12032085561497327 on epoch=24
05/30/2022 19:55:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.99 on epoch=27
05/30/2022 19:55:14 - INFO - __main__ - Step 120 Global step 120 Train loss 1.01 on epoch=29
05/30/2022 19:55:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.95 on epoch=32
05/30/2022 19:55:19 - INFO - __main__ - Step 140 Global step 140 Train loss 1.03 on epoch=34
05/30/2022 19:55:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.90 on epoch=37
05/30/2022 19:55:22 - INFO - __main__ - Global step 150 Train loss 0.98 Classification-F1 0.13067758749069247 on epoch=37
05/30/2022 19:55:24 - INFO - __main__ - Step 160 Global step 160 Train loss 0.91 on epoch=39
05/30/2022 19:55:27 - INFO - __main__ - Step 170 Global step 170 Train loss 1.02 on epoch=42
05/30/2022 19:55:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.92 on epoch=44
05/30/2022 19:55:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.99 on epoch=47
05/30/2022 19:55:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.93 on epoch=49
05/30/2022 19:55:35 - INFO - __main__ - Global step 200 Train loss 0.95 Classification-F1 0.1255656108597285 on epoch=49
05/30/2022 19:55:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.82 on epoch=52
05/30/2022 19:55:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.95 on epoch=54
05/30/2022 19:55:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.88 on epoch=57
05/30/2022 19:55:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.95 on epoch=59
05/30/2022 19:55:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.93 on epoch=62
05/30/2022 19:55:47 - INFO - __main__ - Global step 250 Train loss 0.91 Classification-F1 0.17470760233918126 on epoch=62
05/30/2022 19:55:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.91 on epoch=64
05/30/2022 19:55:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.85 on epoch=67
05/30/2022 19:55:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.91 on epoch=69
05/30/2022 19:55:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.94 on epoch=72
05/30/2022 19:55:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.89 on epoch=74
05/30/2022 19:56:00 - INFO - __main__ - Global step 300 Train loss 0.90 Classification-F1 0.20088831297546345 on epoch=74
05/30/2022 19:56:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.77 on epoch=77
05/30/2022 19:56:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.89 on epoch=79
05/30/2022 19:56:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.83 on epoch=82
05/30/2022 19:56:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.85 on epoch=84
05/30/2022 19:56:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.86 on epoch=87
05/30/2022 19:56:13 - INFO - __main__ - Global step 350 Train loss 0.84 Classification-F1 0.11642156862745097 on epoch=87
05/30/2022 19:56:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.75 on epoch=89
05/30/2022 19:56:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.88 on epoch=92
05/30/2022 19:56:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.97 on epoch=94
05/30/2022 19:56:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.75 on epoch=97
05/30/2022 19:56:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.83 on epoch=99
05/30/2022 19:56:26 - INFO - __main__ - Global step 400 Train loss 0.84 Classification-F1 0.1779411764705882 on epoch=99
05/30/2022 19:56:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.82 on epoch=102
05/30/2022 19:56:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.81 on epoch=104
05/30/2022 19:56:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.76 on epoch=107
05/30/2022 19:56:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.81 on epoch=109
05/30/2022 19:56:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.80 on epoch=112
05/30/2022 19:56:38 - INFO - __main__ - Global step 450 Train loss 0.80 Classification-F1 0.2028133903133903 on epoch=112
05/30/2022 19:56:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.71 on epoch=114
05/30/2022 19:56:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.78 on epoch=117
05/30/2022 19:56:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.81 on epoch=119
05/30/2022 19:56:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.81 on epoch=122
05/30/2022 19:56:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.69 on epoch=124
05/30/2022 19:56:51 - INFO - __main__ - Global step 500 Train loss 0.76 Classification-F1 0.32582788832788834 on epoch=124
05/30/2022 19:56:51 - INFO - __main__ - Saving model with best Classification-F1: 0.2929032258064517 -> 0.32582788832788834 on epoch=124, global_step=500
05/30/2022 19:56:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.65 on epoch=127
05/30/2022 19:56:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.65 on epoch=129
05/30/2022 19:56:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.72 on epoch=132
05/30/2022 19:57:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.74 on epoch=134
05/30/2022 19:57:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.70 on epoch=137
05/30/2022 19:57:04 - INFO - __main__ - Global step 550 Train loss 0.69 Classification-F1 0.26568061568061563 on epoch=137
05/30/2022 19:57:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.67 on epoch=139
05/30/2022 19:57:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.68 on epoch=142
05/30/2022 19:57:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.76 on epoch=144
05/30/2022 19:57:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.71 on epoch=147
05/30/2022 19:57:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.66 on epoch=149
05/30/2022 19:57:17 - INFO - __main__ - Global step 600 Train loss 0.70 Classification-F1 0.19203577255423288 on epoch=149
05/30/2022 19:57:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.61 on epoch=152
05/30/2022 19:57:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.71 on epoch=154
05/30/2022 19:57:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.70 on epoch=157
05/30/2022 19:57:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.55 on epoch=159
05/30/2022 19:57:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.59 on epoch=162
05/30/2022 19:57:30 - INFO - __main__ - Global step 650 Train loss 0.63 Classification-F1 0.3805102305102305 on epoch=162
05/30/2022 19:57:30 - INFO - __main__ - Saving model with best Classification-F1: 0.32582788832788834 -> 0.3805102305102305 on epoch=162, global_step=650
05/30/2022 19:57:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.59 on epoch=164
05/30/2022 19:57:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.59 on epoch=167
05/30/2022 19:57:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.62 on epoch=169
05/30/2022 19:57:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.55 on epoch=172
05/30/2022 19:57:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.66 on epoch=174
05/30/2022 19:57:42 - INFO - __main__ - Global step 700 Train loss 0.60 Classification-F1 0.29942512875376937 on epoch=174
05/30/2022 19:57:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.56 on epoch=177
05/30/2022 19:57:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.57 on epoch=179
05/30/2022 19:57:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.59 on epoch=182
05/30/2022 19:57:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.59 on epoch=184
05/30/2022 19:57:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.63 on epoch=187
05/30/2022 19:57:55 - INFO - __main__ - Global step 750 Train loss 0.59 Classification-F1 0.23533781033781032 on epoch=187
05/30/2022 19:57:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=189
05/30/2022 19:58:00 - INFO - __main__ - Step 770 Global step 770 Train loss 0.57 on epoch=192
05/30/2022 19:58:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.53 on epoch=194
05/30/2022 19:58:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.53 on epoch=197
05/30/2022 19:58:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.50 on epoch=199
05/30/2022 19:58:08 - INFO - __main__ - Global step 800 Train loss 0.51 Classification-F1 0.34285067873303166 on epoch=199
05/30/2022 19:58:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.50 on epoch=202
05/30/2022 19:58:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.55 on epoch=204
05/30/2022 19:58:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=207
05/30/2022 19:58:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=209
05/30/2022 19:58:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=212
05/30/2022 19:58:20 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.24679089026915113 on epoch=212
05/30/2022 19:58:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=214
05/30/2022 19:58:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.47 on epoch=217
05/30/2022 19:58:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=219
05/30/2022 19:58:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=222
05/30/2022 19:58:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=224
05/30/2022 19:58:33 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.3053774928774929 on epoch=224
05/30/2022 19:58:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.53 on epoch=227
05/30/2022 19:58:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=229
05/30/2022 19:58:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.48 on epoch=232
05/30/2022 19:58:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=234
05/30/2022 19:58:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.47 on epoch=237
05/30/2022 19:58:46 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.42807125307125304 on epoch=237
05/30/2022 19:58:46 - INFO - __main__ - Saving model with best Classification-F1: 0.3805102305102305 -> 0.42807125307125304 on epoch=237, global_step=950
05/30/2022 19:58:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.47 on epoch=239
05/30/2022 19:58:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=242
05/30/2022 19:58:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.32 on epoch=244
05/30/2022 19:58:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=247
05/30/2022 19:58:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.32 on epoch=249
05/30/2022 19:58:59 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.4208995261984393 on epoch=249
05/30/2022 19:59:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=252
05/30/2022 19:59:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.39 on epoch=254
05/30/2022 19:59:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.29 on epoch=257
05/30/2022 19:59:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.33 on epoch=259
05/30/2022 19:59:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=262
05/30/2022 19:59:11 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.39828987150415723 on epoch=262
05/30/2022 19:59:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=264
05/30/2022 19:59:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.33 on epoch=267
05/30/2022 19:59:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=269
05/30/2022 19:59:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.26 on epoch=272
05/30/2022 19:59:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.22 on epoch=274
05/30/2022 19:59:24 - INFO - __main__ - Global step 1100 Train loss 0.27 Classification-F1 0.4261670761670761 on epoch=274
05/30/2022 19:59:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.23 on epoch=277
05/30/2022 19:59:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.33 on epoch=279
05/30/2022 19:59:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.26 on epoch=282
05/30/2022 19:59:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=284
05/30/2022 19:59:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=287
05/30/2022 19:59:37 - INFO - __main__ - Global step 1150 Train loss 0.26 Classification-F1 0.4286674347158218 on epoch=287
05/30/2022 19:59:37 - INFO - __main__ - Saving model with best Classification-F1: 0.42807125307125304 -> 0.4286674347158218 on epoch=287, global_step=1150
05/30/2022 19:59:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.29 on epoch=289
05/30/2022 19:59:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=292
05/30/2022 19:59:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=294
05/30/2022 19:59:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=297
05/30/2022 19:59:49 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.14 on epoch=299
05/30/2022 19:59:50 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.32671277997364956 on epoch=299
05/30/2022 19:59:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=302
05/30/2022 19:59:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=304
05/30/2022 19:59:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=307
05/30/2022 20:00:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.15 on epoch=309
05/30/2022 20:00:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=312
05/30/2022 20:00:03 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.4057088744588745 on epoch=312
05/30/2022 20:00:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=314
05/30/2022 20:00:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.10 on epoch=317
05/30/2022 20:00:10 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=319
05/30/2022 20:00:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.18 on epoch=322
05/30/2022 20:00:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=324
05/30/2022 20:00:16 - INFO - __main__ - Global step 1300 Train loss 0.14 Classification-F1 0.4412666902989484 on epoch=324
05/30/2022 20:00:16 - INFO - __main__ - Saving model with best Classification-F1: 0.4286674347158218 -> 0.4412666902989484 on epoch=324, global_step=1300
05/30/2022 20:00:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.17 on epoch=327
05/30/2022 20:00:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=329
05/30/2022 20:00:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=332
05/30/2022 20:00:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.11 on epoch=334
05/30/2022 20:00:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=337
05/30/2022 20:00:29 - INFO - __main__ - Global step 1350 Train loss 0.12 Classification-F1 0.4134946741854636 on epoch=337
05/30/2022 20:00:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.11 on epoch=339
05/30/2022 20:00:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=342
05/30/2022 20:00:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.11 on epoch=344
05/30/2022 20:00:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.08 on epoch=347
05/30/2022 20:00:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=349
05/30/2022 20:00:41 - INFO - __main__ - Global step 1400 Train loss 0.10 Classification-F1 0.4312666902989484 on epoch=349
05/30/2022 20:00:44 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=352
05/30/2022 20:00:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=354
05/30/2022 20:00:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=357
05/30/2022 20:00:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=359
05/30/2022 20:00:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=362
05/30/2022 20:00:54 - INFO - __main__ - Global step 1450 Train loss 0.14 Classification-F1 0.43818181818181823 on epoch=362
05/30/2022 20:00:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.11 on epoch=364
05/30/2022 20:00:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=367
05/30/2022 20:01:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=369
05/30/2022 20:01:04 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=372
05/30/2022 20:01:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=374
05/30/2022 20:01:07 - INFO - __main__ - Global step 1500 Train loss 0.12 Classification-F1 0.41184032503975115 on epoch=374
05/30/2022 20:01:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=377
05/30/2022 20:01:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=379
05/30/2022 20:01:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.11 on epoch=382
05/30/2022 20:01:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=384
05/30/2022 20:01:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=387
05/30/2022 20:01:20 - INFO - __main__ - Global step 1550 Train loss 0.09 Classification-F1 0.4750479224163434 on epoch=387
05/30/2022 20:01:20 - INFO - __main__ - Saving model with best Classification-F1: 0.4412666902989484 -> 0.4750479224163434 on epoch=387, global_step=1550
05/30/2022 20:01:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
05/30/2022 20:01:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.09 on epoch=392
05/30/2022 20:01:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=394
05/30/2022 20:01:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
05/30/2022 20:01:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
05/30/2022 20:01:33 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.49987730134788966 on epoch=399
05/30/2022 20:01:33 - INFO - __main__ - Saving model with best Classification-F1: 0.4750479224163434 -> 0.49987730134788966 on epoch=399, global_step=1600
05/30/2022 20:01:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=402
05/30/2022 20:01:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=404
05/30/2022 20:01:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.11 on epoch=407
05/30/2022 20:01:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=409
05/30/2022 20:01:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=412
05/30/2022 20:01:45 - INFO - __main__ - Global step 1650 Train loss 0.10 Classification-F1 0.4569365585543543 on epoch=412
05/30/2022 20:01:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=414
05/30/2022 20:01:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=417
05/30/2022 20:01:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=419
05/30/2022 20:01:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=422
05/30/2022 20:01:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=424
05/30/2022 20:01:58 - INFO - __main__ - Global step 1700 Train loss 0.07 Classification-F1 0.5265080108502624 on epoch=424
05/30/2022 20:01:58 - INFO - __main__ - Saving model with best Classification-F1: 0.49987730134788966 -> 0.5265080108502624 on epoch=424, global_step=1700
05/30/2022 20:02:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=427
05/30/2022 20:02:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.10 on epoch=429
05/30/2022 20:02:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=432
05/30/2022 20:02:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=434
05/30/2022 20:02:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=437
05/30/2022 20:02:11 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.44663001510827605 on epoch=437
05/30/2022 20:02:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=439
05/30/2022 20:02:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=442
05/30/2022 20:02:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.13 on epoch=444
05/30/2022 20:02:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=447
05/30/2022 20:02:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=449
05/30/2022 20:02:24 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.4876633986928105 on epoch=449
05/30/2022 20:02:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
05/30/2022 20:02:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=454
05/30/2022 20:02:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
05/30/2022 20:02:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=459
05/30/2022 20:02:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=462
05/30/2022 20:02:37 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.5043288084464554 on epoch=462
05/30/2022 20:02:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
05/30/2022 20:02:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.10 on epoch=467
05/30/2022 20:02:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
05/30/2022 20:02:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=472
05/30/2022 20:02:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=474
05/30/2022 20:02:50 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.47042124542124536 on epoch=474
05/30/2022 20:02:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=477
05/30/2022 20:02:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/30/2022 20:02:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=482
05/30/2022 20:02:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=484
05/30/2022 20:03:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=487
05/30/2022 20:03:02 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.4744790090580806 on epoch=487
05/30/2022 20:03:05 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/30/2022 20:03:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=492
05/30/2022 20:03:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
05/30/2022 20:03:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=497
05/30/2022 20:03:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=499
05/30/2022 20:03:15 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.5348679098679099 on epoch=499
05/30/2022 20:03:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5265080108502624 -> 0.5348679098679099 on epoch=499, global_step=2000
05/30/2022 20:03:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=502
05/30/2022 20:03:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=504
05/30/2022 20:03:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
05/30/2022 20:03:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
05/30/2022 20:03:27 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=512
05/30/2022 20:03:28 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.44861715098557203 on epoch=512
05/30/2022 20:03:31 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
05/30/2022 20:03:33 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=517
05/30/2022 20:03:35 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/30/2022 20:03:38 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.09 on epoch=522
05/30/2022 20:03:40 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=524
05/30/2022 20:03:41 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.49936507936507935 on epoch=524
05/30/2022 20:03:43 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=527
05/30/2022 20:03:46 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.18 on epoch=529
05/30/2022 20:03:48 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/30/2022 20:03:51 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/30/2022 20:03:53 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/30/2022 20:03:54 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.4788461538461538 on epoch=537
05/30/2022 20:03:56 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/30/2022 20:03:59 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
05/30/2022 20:04:01 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=544
05/30/2022 20:04:03 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/30/2022 20:04:06 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
05/30/2022 20:04:07 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.440979406939647 on epoch=549
05/30/2022 20:04:09 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/30/2022 20:04:12 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
05/30/2022 20:04:14 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=557
05/30/2022 20:04:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
05/30/2022 20:04:19 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/30/2022 20:04:20 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.48208354242837 on epoch=562
05/30/2022 20:04:22 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=564
05/30/2022 20:04:25 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
05/30/2022 20:04:27 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.11 on epoch=569
05/30/2022 20:04:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/30/2022 20:04:32 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=574
05/30/2022 20:04:33 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.4207307060755337 on epoch=574
05/30/2022 20:04:35 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=577
05/30/2022 20:04:37 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
05/30/2022 20:04:40 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/30/2022 20:04:42 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
05/30/2022 20:04:45 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=587
05/30/2022 20:04:46 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.4671634104199894 on epoch=587
05/30/2022 20:04:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
05/30/2022 20:04:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
05/30/2022 20:04:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=594
05/30/2022 20:04:55 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=597
05/30/2022 20:04:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/30/2022 20:04:59 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.5121034910508595 on epoch=599
05/30/2022 20:05:01 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/30/2022 20:05:03 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=604
05/30/2022 20:05:06 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/30/2022 20:05:08 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.10 on epoch=609
05/30/2022 20:05:11 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
05/30/2022 20:05:12 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.5166666666666666 on epoch=612
05/30/2022 20:05:14 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
05/30/2022 20:05:17 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/30/2022 20:05:19 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=619
05/30/2022 20:05:22 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/30/2022 20:05:24 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.13 on epoch=624
05/30/2022 20:05:25 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.4338205980066445 on epoch=624
05/30/2022 20:05:27 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/30/2022 20:05:30 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=629
05/30/2022 20:05:32 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/30/2022 20:05:34 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/30/2022 20:05:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
05/30/2022 20:05:38 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.4907407407407407 on epoch=637
05/30/2022 20:05:40 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/30/2022 20:05:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
05/30/2022 20:05:45 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/30/2022 20:05:48 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/30/2022 20:05:50 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/30/2022 20:05:51 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.4587763025263025 on epoch=649
05/30/2022 20:05:54 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=652
05/30/2022 20:05:56 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/30/2022 20:05:59 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/30/2022 20:06:01 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
05/30/2022 20:06:03 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=662
05/30/2022 20:06:05 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.521875 on epoch=662
05/30/2022 20:06:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=664
05/30/2022 20:06:10 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.31 on epoch=667
05/30/2022 20:06:12 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/30/2022 20:06:14 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.09 on epoch=672
05/30/2022 20:06:17 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
05/30/2022 20:06:18 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.5231679217713837 on epoch=674
05/30/2022 20:06:20 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/30/2022 20:06:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/30/2022 20:06:25 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/30/2022 20:06:27 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/30/2022 20:06:30 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/30/2022 20:06:31 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.4445642609242204 on epoch=687
05/30/2022 20:06:33 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/30/2022 20:06:36 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/30/2022 20:06:38 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/30/2022 20:06:41 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
05/30/2022 20:06:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/30/2022 20:06:44 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.5063035968299127 on epoch=699
05/30/2022 20:06:47 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/30/2022 20:06:49 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=704
05/30/2022 20:06:52 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=707
05/30/2022 20:06:54 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=709
05/30/2022 20:06:56 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.11 on epoch=712
05/30/2022 20:06:58 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.44656862745098036 on epoch=712
05/30/2022 20:07:00 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=714
05/30/2022 20:07:03 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.11 on epoch=717
05/30/2022 20:07:05 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
05/30/2022 20:07:07 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/30/2022 20:07:10 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
05/30/2022 20:07:11 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.5233200707338638 on epoch=724
05/30/2022 20:07:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/30/2022 20:07:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/30/2022 20:07:18 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/30/2022 20:07:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/30/2022 20:07:23 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/30/2022 20:07:25 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.572829131652661 on epoch=737
05/30/2022 20:07:25 - INFO - __main__ - Saving model with best Classification-F1: 0.5348679098679099 -> 0.572829131652661 on epoch=737, global_step=2950
05/30/2022 20:07:27 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=739
05/30/2022 20:07:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/30/2022 20:07:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
05/30/2022 20:07:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/30/2022 20:07:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/30/2022 20:07:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 20:07:38 - INFO - __main__ - Printing 3 examples
05/30/2022 20:07:38 - INFO - __main__ -  [emo] how cause yes am listening
05/30/2022 20:07:38 - INFO - __main__ - ['others']
05/30/2022 20:07:38 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/30/2022 20:07:38 - INFO - __main__ - ['others']
05/30/2022 20:07:38 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/30/2022 20:07:38 - INFO - __main__ - ['others']
05/30/2022 20:07:38 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:07:38 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:07:38 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 20:07:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 20:07:38 - INFO - __main__ - Printing 3 examples
05/30/2022 20:07:38 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/30/2022 20:07:38 - INFO - __main__ - ['others']
05/30/2022 20:07:38 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/30/2022 20:07:38 - INFO - __main__ - ['others']
05/30/2022 20:07:38 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/30/2022 20:07:38 - INFO - __main__ - ['others']
05/30/2022 20:07:38 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:07:38 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:07:38 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 20:07:38 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.5957375957375957 on epoch=749
05/30/2022 20:07:38 - INFO - __main__ - Saving model with best Classification-F1: 0.572829131652661 -> 0.5957375957375957 on epoch=749, global_step=3000
05/30/2022 20:07:38 - INFO - __main__ - save last model!
05/30/2022 20:07:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 20:07:38 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 20:07:38 - INFO - __main__ - Printing 3 examples
05/30/2022 20:07:38 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 20:07:38 - INFO - __main__ - ['others']
05/30/2022 20:07:38 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 20:07:38 - INFO - __main__ - ['others']
05/30/2022 20:07:38 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 20:07:38 - INFO - __main__ - ['others']
05/30/2022 20:07:38 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:07:40 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:07:45 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 20:07:57 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 20:07:57 - INFO - __main__ - task name: emo
05/30/2022 20:07:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 20:07:58 - INFO - __main__ - Starting training!
05/30/2022 20:09:08 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_100_0.3_8_predictions.txt
05/30/2022 20:09:08 - INFO - __main__ - Classification-F1 on test data: 0.3493
05/30/2022 20:09:08 - INFO - __main__ - prefix=emo_16_100, lr=0.3, bsz=8, dev_performance=0.5957375957375957, test_performance=0.34925842929695194
05/30/2022 20:09:08 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.2, bsz=8 ...
05/30/2022 20:09:09 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 20:09:09 - INFO - __main__ - Printing 3 examples
05/30/2022 20:09:09 - INFO - __main__ -  [emo] how cause yes am listening
05/30/2022 20:09:09 - INFO - __main__ - ['others']
05/30/2022 20:09:09 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/30/2022 20:09:09 - INFO - __main__ - ['others']
05/30/2022 20:09:09 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/30/2022 20:09:09 - INFO - __main__ - ['others']
05/30/2022 20:09:09 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:09:09 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:09:09 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 20:09:09 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 20:09:09 - INFO - __main__ - Printing 3 examples
05/30/2022 20:09:09 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/30/2022 20:09:09 - INFO - __main__ - ['others']
05/30/2022 20:09:09 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/30/2022 20:09:09 - INFO - __main__ - ['others']
05/30/2022 20:09:09 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/30/2022 20:09:09 - INFO - __main__ - ['others']
05/30/2022 20:09:09 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:09:09 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:09:09 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 20:09:24 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 20:09:24 - INFO - __main__ - task name: emo
05/30/2022 20:09:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 20:09:25 - INFO - __main__ - Starting training!
05/30/2022 20:09:27 - INFO - __main__ - Step 10 Global step 10 Train loss 7.99 on epoch=2
05/30/2022 20:09:30 - INFO - __main__ - Step 20 Global step 20 Train loss 6.18 on epoch=4
05/30/2022 20:09:32 - INFO - __main__ - Step 30 Global step 30 Train loss 3.87 on epoch=7
05/30/2022 20:09:35 - INFO - __main__ - Step 40 Global step 40 Train loss 2.55 on epoch=9
05/30/2022 20:09:37 - INFO - __main__ - Step 50 Global step 50 Train loss 1.97 on epoch=12
05/30/2022 20:09:38 - INFO - __main__ - Global step 50 Train loss 4.51 Classification-F1 0.2590438651641921 on epoch=12
05/30/2022 20:09:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2590438651641921 on epoch=12, global_step=50
05/30/2022 20:09:41 - INFO - __main__ - Step 60 Global step 60 Train loss 1.41 on epoch=14
05/30/2022 20:09:43 - INFO - __main__ - Step 70 Global step 70 Train loss 1.29 on epoch=17
05/30/2022 20:09:46 - INFO - __main__ - Step 80 Global step 80 Train loss 1.19 on epoch=19
05/30/2022 20:09:48 - INFO - __main__ - Step 90 Global step 90 Train loss 1.09 on epoch=22
05/30/2022 20:09:51 - INFO - __main__ - Step 100 Global step 100 Train loss 1.01 on epoch=24
05/30/2022 20:09:51 - INFO - __main__ - Global step 100 Train loss 1.20 Classification-F1 0.19208955223880597 on epoch=24
05/30/2022 20:09:54 - INFO - __main__ - Step 110 Global step 110 Train loss 1.05 on epoch=27
05/30/2022 20:09:56 - INFO - __main__ - Step 120 Global step 120 Train loss 1.08 on epoch=29
05/30/2022 20:09:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.96 on epoch=32
05/30/2022 20:10:01 - INFO - __main__ - Step 140 Global step 140 Train loss 1.01 on epoch=34
05/30/2022 20:10:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.89 on epoch=37
05/30/2022 20:10:05 - INFO - __main__ - Global step 150 Train loss 1.00 Classification-F1 0.17831215970961886 on epoch=37
05/30/2022 20:10:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.99 on epoch=39
05/30/2022 20:10:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.95 on epoch=42
05/30/2022 20:10:12 - INFO - __main__ - Step 180 Global step 180 Train loss 1.00 on epoch=44
05/30/2022 20:10:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.93 on epoch=47
05/30/2022 20:10:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.95 on epoch=49
05/30/2022 20:10:18 - INFO - __main__ - Global step 200 Train loss 0.96 Classification-F1 0.10126582278481013 on epoch=49
05/30/2022 20:10:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.88 on epoch=52
05/30/2022 20:10:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.78 on epoch=54
05/30/2022 20:10:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.91 on epoch=57
05/30/2022 20:10:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=59
05/30/2022 20:10:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.81 on epoch=62
05/30/2022 20:10:31 - INFO - __main__ - Global step 250 Train loss 0.85 Classification-F1 0.17092154420921546 on epoch=62
05/30/2022 20:10:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.81 on epoch=64
05/30/2022 20:10:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.91 on epoch=67
05/30/2022 20:10:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.88 on epoch=69
05/30/2022 20:10:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.88 on epoch=72
05/30/2022 20:10:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.80 on epoch=74
05/30/2022 20:10:44 - INFO - __main__ - Global step 300 Train loss 0.86 Classification-F1 0.24412442396313366 on epoch=74
05/30/2022 20:10:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.91 on epoch=77
05/30/2022 20:10:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.86 on epoch=79
05/30/2022 20:10:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.85 on epoch=82
05/30/2022 20:10:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.82 on epoch=84
05/30/2022 20:10:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.88 on epoch=87
05/30/2022 20:10:57 - INFO - __main__ - Global step 350 Train loss 0.86 Classification-F1 0.33963829434212706 on epoch=87
05/30/2022 20:10:58 - INFO - __main__ - Saving model with best Classification-F1: 0.2590438651641921 -> 0.33963829434212706 on epoch=87, global_step=350
05/30/2022 20:11:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.79 on epoch=89
05/30/2022 20:11:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.81 on epoch=92
05/30/2022 20:11:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.86 on epoch=94
05/30/2022 20:11:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.75 on epoch=97
05/30/2022 20:11:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.82 on epoch=99
05/30/2022 20:11:11 - INFO - __main__ - Global step 400 Train loss 0.81 Classification-F1 0.18969624776652771 on epoch=99
05/30/2022 20:11:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.82 on epoch=102
05/30/2022 20:11:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.75 on epoch=104
05/30/2022 20:11:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.81 on epoch=107
05/30/2022 20:11:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.83 on epoch=109
05/30/2022 20:11:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.82 on epoch=112
05/30/2022 20:11:24 - INFO - __main__ - Global step 450 Train loss 0.80 Classification-F1 0.3591954022988506 on epoch=112
05/30/2022 20:11:24 - INFO - __main__ - Saving model with best Classification-F1: 0.33963829434212706 -> 0.3591954022988506 on epoch=112, global_step=450
05/30/2022 20:11:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.74 on epoch=114
05/30/2022 20:11:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.72 on epoch=117
05/30/2022 20:11:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.71 on epoch=119
05/30/2022 20:11:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.72 on epoch=122
05/30/2022 20:11:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.73 on epoch=124
05/30/2022 20:11:37 - INFO - __main__ - Global step 500 Train loss 0.73 Classification-F1 0.19308915695127704 on epoch=124
05/30/2022 20:11:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.80 on epoch=127
05/30/2022 20:11:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.70 on epoch=129
05/30/2022 20:11:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.75 on epoch=132
05/30/2022 20:11:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.71 on epoch=134
05/30/2022 20:11:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.75 on epoch=137
05/30/2022 20:11:50 - INFO - __main__ - Global step 550 Train loss 0.74 Classification-F1 0.4932194616977225 on epoch=137
05/30/2022 20:11:50 - INFO - __main__ - Saving model with best Classification-F1: 0.3591954022988506 -> 0.4932194616977225 on epoch=137, global_step=550
05/30/2022 20:11:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.76 on epoch=139
05/30/2022 20:11:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.68 on epoch=142
05/30/2022 20:11:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.72 on epoch=144
05/30/2022 20:12:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.69 on epoch=147
05/30/2022 20:12:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.68 on epoch=149
05/30/2022 20:12:04 - INFO - __main__ - Global step 600 Train loss 0.71 Classification-F1 0.3089225589225589 on epoch=149
05/30/2022 20:12:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.65 on epoch=152
05/30/2022 20:12:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.69 on epoch=154
05/30/2022 20:12:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.58 on epoch=157
05/30/2022 20:12:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.68 on epoch=159
05/30/2022 20:12:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.63 on epoch=162
05/30/2022 20:12:17 - INFO - __main__ - Global step 650 Train loss 0.65 Classification-F1 0.5133249528598365 on epoch=162
05/30/2022 20:12:17 - INFO - __main__ - Saving model with best Classification-F1: 0.4932194616977225 -> 0.5133249528598365 on epoch=162, global_step=650
05/30/2022 20:12:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.63 on epoch=164
05/30/2022 20:12:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.56 on epoch=167
05/30/2022 20:12:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.58 on epoch=169
05/30/2022 20:12:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.67 on epoch=172
05/30/2022 20:12:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.56 on epoch=174
05/30/2022 20:12:30 - INFO - __main__ - Global step 700 Train loss 0.60 Classification-F1 0.41950500479912245 on epoch=174
05/30/2022 20:12:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.62 on epoch=177
05/30/2022 20:12:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.58 on epoch=179
05/30/2022 20:12:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.54 on epoch=182
05/30/2022 20:12:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.54 on epoch=184
05/30/2022 20:12:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.55 on epoch=187
05/30/2022 20:12:44 - INFO - __main__ - Global step 750 Train loss 0.57 Classification-F1 0.4067208180111406 on epoch=187
05/30/2022 20:12:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.59 on epoch=189
05/30/2022 20:12:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.56 on epoch=192
05/30/2022 20:12:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.53 on epoch=194
05/30/2022 20:12:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.52 on epoch=197
05/30/2022 20:12:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.48 on epoch=199
05/30/2022 20:12:57 - INFO - __main__ - Global step 800 Train loss 0.54 Classification-F1 0.38882734912146677 on epoch=199
05/30/2022 20:12:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.50 on epoch=202
05/30/2022 20:13:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.46 on epoch=204
05/30/2022 20:13:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.58 on epoch=207
05/30/2022 20:13:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.48 on epoch=209
05/30/2022 20:13:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.50 on epoch=212
05/30/2022 20:13:10 - INFO - __main__ - Global step 850 Train loss 0.50 Classification-F1 0.5584584584584584 on epoch=212
05/30/2022 20:13:10 - INFO - __main__ - Saving model with best Classification-F1: 0.5133249528598365 -> 0.5584584584584584 on epoch=212, global_step=850
05/30/2022 20:13:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=214
05/30/2022 20:13:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.51 on epoch=217
05/30/2022 20:13:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.49 on epoch=219
05/30/2022 20:13:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=222
05/30/2022 20:13:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.45 on epoch=224
05/30/2022 20:13:24 - INFO - __main__ - Global step 900 Train loss 0.46 Classification-F1 0.4710597826086957 on epoch=224
05/30/2022 20:13:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=227
05/30/2022 20:13:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=229
05/30/2022 20:13:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=232
05/30/2022 20:13:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=234
05/30/2022 20:13:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=237
05/30/2022 20:13:37 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.5531061692969871 on epoch=237
05/30/2022 20:13:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=239
05/30/2022 20:13:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=242
05/30/2022 20:13:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.31 on epoch=244
05/30/2022 20:13:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=247
05/30/2022 20:13:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=249
05/30/2022 20:13:50 - INFO - __main__ - Global step 1000 Train loss 0.36 Classification-F1 0.5324490906816421 on epoch=249
05/30/2022 20:13:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=252
05/30/2022 20:13:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=254
05/30/2022 20:13:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=257
05/30/2022 20:14:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.33 on epoch=259
05/30/2022 20:14:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=262
05/30/2022 20:14:03 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.5907859078590786 on epoch=262
05/30/2022 20:14:03 - INFO - __main__ - Saving model with best Classification-F1: 0.5584584584584584 -> 0.5907859078590786 on epoch=262, global_step=1050
05/30/2022 20:14:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=264
05/30/2022 20:14:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.28 on epoch=267
05/30/2022 20:14:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=269
05/30/2022 20:14:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.32 on epoch=272
05/30/2022 20:14:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.30 on epoch=274
05/30/2022 20:14:17 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.5386904761904762 on epoch=274
05/30/2022 20:14:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=277
05/30/2022 20:14:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.28 on epoch=279
05/30/2022 20:14:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.29 on epoch=282
05/30/2022 20:14:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.30 on epoch=284
05/30/2022 20:14:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.33 on epoch=287
05/30/2022 20:14:30 - INFO - __main__ - Global step 1150 Train loss 0.29 Classification-F1 0.5686309651826893 on epoch=287
05/30/2022 20:14:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.27 on epoch=289
05/30/2022 20:14:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.31 on epoch=292
05/30/2022 20:14:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=294
05/30/2022 20:14:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.27 on epoch=297
05/30/2022 20:14:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.34 on epoch=299
05/30/2022 20:14:43 - INFO - __main__ - Global step 1200 Train loss 0.27 Classification-F1 0.5899256407508471 on epoch=299
05/30/2022 20:14:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.25 on epoch=302
05/30/2022 20:14:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=304
05/30/2022 20:14:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.32 on epoch=307
05/30/2022 20:14:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=309
05/30/2022 20:14:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=312
05/30/2022 20:14:56 - INFO - __main__ - Global step 1250 Train loss 0.25 Classification-F1 0.5853174603174603 on epoch=312
05/30/2022 20:14:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.18 on epoch=314
05/30/2022 20:15:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.32 on epoch=317
05/30/2022 20:15:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.21 on epoch=319
05/30/2022 20:15:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=322
05/30/2022 20:15:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.14 on epoch=324
05/30/2022 20:15:10 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.568100358422939 on epoch=324
05/30/2022 20:15:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.14 on epoch=327
05/30/2022 20:15:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=329
05/30/2022 20:15:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.26 on epoch=332
05/30/2022 20:15:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=334
05/30/2022 20:15:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.22 on epoch=337
05/30/2022 20:15:23 - INFO - __main__ - Global step 1350 Train loss 0.21 Classification-F1 0.5650054466230937 on epoch=337
05/30/2022 20:15:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=339
05/30/2022 20:15:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.14 on epoch=342
05/30/2022 20:15:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=344
05/30/2022 20:15:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=347
05/30/2022 20:15:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.15 on epoch=349
05/30/2022 20:15:36 - INFO - __main__ - Global step 1400 Train loss 0.16 Classification-F1 0.5988052710962928 on epoch=349
05/30/2022 20:15:36 - INFO - __main__ - Saving model with best Classification-F1: 0.5907859078590786 -> 0.5988052710962928 on epoch=349, global_step=1400
05/30/2022 20:15:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=352
05/30/2022 20:15:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=354
05/30/2022 20:15:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=357
05/30/2022 20:15:46 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=359
05/30/2022 20:15:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.22 on epoch=362
05/30/2022 20:15:50 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.44575585704617965 on epoch=362
05/30/2022 20:15:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.17 on epoch=364
05/30/2022 20:15:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.16 on epoch=367
05/30/2022 20:15:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=369
05/30/2022 20:16:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=372
05/30/2022 20:16:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=374
05/30/2022 20:16:03 - INFO - __main__ - Global step 1500 Train loss 0.13 Classification-F1 0.6204060798548094 on epoch=374
05/30/2022 20:16:03 - INFO - __main__ - Saving model with best Classification-F1: 0.5988052710962928 -> 0.6204060798548094 on epoch=374, global_step=1500
05/30/2022 20:16:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=377
05/30/2022 20:16:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=379
05/30/2022 20:16:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.12 on epoch=382
05/30/2022 20:16:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=384
05/30/2022 20:16:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.09 on epoch=387
05/30/2022 20:16:16 - INFO - __main__ - Global step 1550 Train loss 0.13 Classification-F1 0.6153846153846154 on epoch=387
05/30/2022 20:16:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=389
05/30/2022 20:16:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=392
05/30/2022 20:16:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=394
05/30/2022 20:16:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.10 on epoch=397
05/30/2022 20:16:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=399
05/30/2022 20:16:29 - INFO - __main__ - Global step 1600 Train loss 0.11 Classification-F1 0.6145622469635628 on epoch=399
05/30/2022 20:16:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=402
05/30/2022 20:16:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.15 on epoch=404
05/30/2022 20:16:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=407
05/30/2022 20:16:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=409
05/30/2022 20:16:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=412
05/30/2022 20:16:43 - INFO - __main__ - Global step 1650 Train loss 0.11 Classification-F1 0.6803053053053053 on epoch=412
05/30/2022 20:16:43 - INFO - __main__ - Saving model with best Classification-F1: 0.6204060798548094 -> 0.6803053053053053 on epoch=412, global_step=1650
05/30/2022 20:16:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=414
05/30/2022 20:16:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
05/30/2022 20:16:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=419
05/30/2022 20:16:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=422
05/30/2022 20:16:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.17 on epoch=424
05/30/2022 20:16:56 - INFO - __main__ - Global step 1700 Train loss 0.11 Classification-F1 0.6229795972443031 on epoch=424
05/30/2022 20:16:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.15 on epoch=427
05/30/2022 20:17:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=429
05/30/2022 20:17:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=432
05/30/2022 20:17:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=434
05/30/2022 20:17:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=437
05/30/2022 20:17:09 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.6372232764073371 on epoch=437
05/30/2022 20:17:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=439
05/30/2022 20:17:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=442
05/30/2022 20:17:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.13 on epoch=444
05/30/2022 20:17:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=447
05/30/2022 20:17:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=449
05/30/2022 20:17:23 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.5206766206766207 on epoch=449
05/30/2022 20:17:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=452
05/30/2022 20:17:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.08 on epoch=454
05/30/2022 20:17:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=457
05/30/2022 20:17:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=459
05/30/2022 20:17:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=462
05/30/2022 20:17:36 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.6480082417582418 on epoch=462
05/30/2022 20:17:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
05/30/2022 20:17:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=467
05/30/2022 20:17:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=469
05/30/2022 20:17:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=472
05/30/2022 20:17:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=474
05/30/2022 20:17:50 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.428191632928475 on epoch=474
05/30/2022 20:17:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=477
05/30/2022 20:17:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
05/30/2022 20:17:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=482
05/30/2022 20:18:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
05/30/2022 20:18:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=487
05/30/2022 20:18:03 - INFO - __main__ - Global step 1950 Train loss 0.07 Classification-F1 0.6264204545454546 on epoch=487
05/30/2022 20:18:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
05/30/2022 20:18:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=492
05/30/2022 20:18:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
05/30/2022 20:18:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=497
05/30/2022 20:18:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=499
05/30/2022 20:18:16 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.5548420040850656 on epoch=499
05/30/2022 20:18:19 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.14 on epoch=502
05/30/2022 20:18:21 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
05/30/2022 20:18:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.09 on epoch=507
05/30/2022 20:18:26 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=509
05/30/2022 20:18:29 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=512
05/30/2022 20:18:30 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.6038899430740038 on epoch=512
05/30/2022 20:18:32 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=514
05/30/2022 20:18:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.08 on epoch=517
05/30/2022 20:18:37 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/30/2022 20:18:39 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
05/30/2022 20:18:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=524
05/30/2022 20:18:43 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.6604617604617604 on epoch=524
05/30/2022 20:18:45 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
05/30/2022 20:18:48 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
05/30/2022 20:18:50 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=532
05/30/2022 20:18:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
05/30/2022 20:18:55 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
05/30/2022 20:18:56 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.6517061416254963 on epoch=537
05/30/2022 20:18:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
05/30/2022 20:19:01 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=542
05/30/2022 20:19:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=544
05/30/2022 20:19:06 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=547
05/30/2022 20:19:08 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
05/30/2022 20:19:09 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.6319590336134454 on epoch=549
05/30/2022 20:19:12 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/30/2022 20:19:14 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
05/30/2022 20:19:17 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
05/30/2022 20:19:19 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=559
05/30/2022 20:19:22 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=562
05/30/2022 20:19:23 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.6321128876572424 on epoch=562
05/30/2022 20:19:25 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=564
05/30/2022 20:19:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
05/30/2022 20:19:30 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=569
05/30/2022 20:19:32 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
05/30/2022 20:19:35 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=574
05/30/2022 20:19:36 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.6302083333333334 on epoch=574
05/30/2022 20:19:38 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=577
05/30/2022 20:19:41 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=579
05/30/2022 20:19:43 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/30/2022 20:19:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=584
05/30/2022 20:19:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
05/30/2022 20:19:49 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.6165995564892623 on epoch=587
05/30/2022 20:19:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/30/2022 20:19:54 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=592
05/30/2022 20:19:56 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.11 on epoch=594
05/30/2022 20:19:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=597
05/30/2022 20:20:01 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=599
05/30/2022 20:20:02 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.6345311635626036 on epoch=599
05/30/2022 20:20:04 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.08 on epoch=602
05/30/2022 20:20:07 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=604
05/30/2022 20:20:09 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
05/30/2022 20:20:11 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/30/2022 20:20:14 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
05/30/2022 20:20:15 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.7354312354312355 on epoch=612
05/30/2022 20:20:15 - INFO - __main__ - Saving model with best Classification-F1: 0.6803053053053053 -> 0.7354312354312355 on epoch=612, global_step=2450
05/30/2022 20:20:17 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
05/30/2022 20:20:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=617
05/30/2022 20:20:22 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/30/2022 20:20:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=622
05/30/2022 20:20:27 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
05/30/2022 20:20:28 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.6011904761904763 on epoch=624
05/30/2022 20:20:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=627
05/30/2022 20:20:32 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=629
05/30/2022 20:20:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/30/2022 20:20:37 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/30/2022 20:20:39 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
05/30/2022 20:20:40 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.6798973285486444 on epoch=637
05/30/2022 20:20:43 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
05/30/2022 20:20:45 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/30/2022 20:20:48 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=644
05/30/2022 20:20:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=647
05/30/2022 20:20:52 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/30/2022 20:20:53 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.6230113636363637 on epoch=649
05/30/2022 20:20:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
05/30/2022 20:20:58 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.11 on epoch=654
05/30/2022 20:21:00 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=657
05/30/2022 20:21:03 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.09 on epoch=659
05/30/2022 20:21:05 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
05/30/2022 20:21:06 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.6597722960151802 on epoch=662
05/30/2022 20:21:08 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=664
05/30/2022 20:21:11 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=667
05/30/2022 20:21:13 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/30/2022 20:21:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=672
05/30/2022 20:21:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
05/30/2022 20:21:19 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.6670806739117934 on epoch=674
05/30/2022 20:21:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=677
05/30/2022 20:21:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=679
05/30/2022 20:21:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/30/2022 20:21:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/30/2022 20:21:31 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=687
05/30/2022 20:21:32 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.6694444444444444 on epoch=687
05/30/2022 20:21:34 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=689
05/30/2022 20:21:36 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=692
05/30/2022 20:21:39 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=694
05/30/2022 20:21:41 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
05/30/2022 20:21:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
05/30/2022 20:21:44 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.6694508448540707 on epoch=699
05/30/2022 20:21:47 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=702
05/30/2022 20:21:49 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=704
05/30/2022 20:21:51 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=707
05/30/2022 20:21:54 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=709
05/30/2022 20:21:56 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/30/2022 20:21:57 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.6694724135900606 on epoch=712
05/30/2022 20:21:59 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=714
05/30/2022 20:22:02 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=717
05/30/2022 20:22:04 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/30/2022 20:22:06 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/30/2022 20:22:09 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=724
05/30/2022 20:22:10 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.6092976617170165 on epoch=724
05/30/2022 20:22:12 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=727
05/30/2022 20:22:15 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=729
05/30/2022 20:22:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=732
05/30/2022 20:22:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
05/30/2022 20:22:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/30/2022 20:22:23 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.7061965811965812 on epoch=737
05/30/2022 20:22:25 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
05/30/2022 20:22:27 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=742
05/30/2022 20:22:30 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/30/2022 20:22:32 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/30/2022 20:22:35 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=749
05/30/2022 20:22:36 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.6772875816993463 on epoch=749
05/30/2022 20:22:36 - INFO - __main__ - save last model!
05/30/2022 20:22:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 20:22:36 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 20:22:36 - INFO - __main__ - Printing 3 examples
05/30/2022 20:22:36 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 20:22:36 - INFO - __main__ - ['others']
05/30/2022 20:22:36 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 20:22:36 - INFO - __main__ - ['others']
05/30/2022 20:22:36 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 20:22:36 - INFO - __main__ - ['others']
05/30/2022 20:22:36 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:22:36 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 20:22:36 - INFO - __main__ - Printing 3 examples
05/30/2022 20:22:36 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/30/2022 20:22:36 - INFO - __main__ - ['others']
05/30/2022 20:22:36 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/30/2022 20:22:36 - INFO - __main__ - ['others']
05/30/2022 20:22:36 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/30/2022 20:22:36 - INFO - __main__ - ['others']
05/30/2022 20:22:36 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:22:36 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:22:36 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 20:22:36 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 20:22:36 - INFO - __main__ - Printing 3 examples
05/30/2022 20:22:36 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/30/2022 20:22:36 - INFO - __main__ - ['others']
05/30/2022 20:22:36 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/30/2022 20:22:36 - INFO - __main__ - ['others']
05/30/2022 20:22:36 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/30/2022 20:22:36 - INFO - __main__ - ['others']
05/30/2022 20:22:36 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:22:36 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:22:36 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 20:22:38 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:22:43 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 20:22:51 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 20:22:51 - INFO - __main__ - task name: emo
05/30/2022 20:22:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 20:22:52 - INFO - __main__ - Starting training!
05/30/2022 20:24:13 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_100_0.2_8_predictions.txt
05/30/2022 20:24:13 - INFO - __main__ - Classification-F1 on test data: 0.2999
05/30/2022 20:24:14 - INFO - __main__ - prefix=emo_16_100, lr=0.2, bsz=8, dev_performance=0.7354312354312355, test_performance=0.29991250983805845
05/30/2022 20:24:14 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.5, bsz=8 ...
05/30/2022 20:24:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 20:24:15 - INFO - __main__ - Printing 3 examples
05/30/2022 20:24:15 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/30/2022 20:24:15 - INFO - __main__ - ['others']
05/30/2022 20:24:15 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/30/2022 20:24:15 - INFO - __main__ - ['others']
05/30/2022 20:24:15 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/30/2022 20:24:15 - INFO - __main__ - ['others']
05/30/2022 20:24:15 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:24:15 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:24:15 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 20:24:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 20:24:15 - INFO - __main__ - Printing 3 examples
05/30/2022 20:24:15 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/30/2022 20:24:15 - INFO - __main__ - ['others']
05/30/2022 20:24:15 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/30/2022 20:24:15 - INFO - __main__ - ['others']
05/30/2022 20:24:15 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/30/2022 20:24:15 - INFO - __main__ - ['others']
05/30/2022 20:24:15 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:24:15 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:24:15 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 20:24:30 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 20:24:30 - INFO - __main__ - task name: emo
05/30/2022 20:24:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 20:24:31 - INFO - __main__ - Starting training!
05/30/2022 20:24:33 - INFO - __main__ - Step 10 Global step 10 Train loss 7.58 on epoch=2
05/30/2022 20:24:36 - INFO - __main__ - Step 20 Global step 20 Train loss 3.78 on epoch=4
05/30/2022 20:24:38 - INFO - __main__ - Step 30 Global step 30 Train loss 1.80 on epoch=7
05/30/2022 20:24:40 - INFO - __main__ - Step 40 Global step 40 Train loss 1.19 on epoch=9
05/30/2022 20:24:43 - INFO - __main__ - Step 50 Global step 50 Train loss 1.06 on epoch=12
05/30/2022 20:24:44 - INFO - __main__ - Global step 50 Train loss 3.08 Classification-F1 0.1 on epoch=12
05/30/2022 20:24:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/30/2022 20:24:46 - INFO - __main__ - Step 60 Global step 60 Train loss 1.07 on epoch=14
05/30/2022 20:24:48 - INFO - __main__ - Step 70 Global step 70 Train loss 1.07 on epoch=17
05/30/2022 20:24:51 - INFO - __main__ - Step 80 Global step 80 Train loss 1.04 on epoch=19
05/30/2022 20:24:53 - INFO - __main__ - Step 90 Global step 90 Train loss 1.02 on epoch=22
05/30/2022 20:24:56 - INFO - __main__ - Step 100 Global step 100 Train loss 0.93 on epoch=24
05/30/2022 20:24:57 - INFO - __main__ - Global step 100 Train loss 1.03 Classification-F1 0.1 on epoch=24
05/30/2022 20:24:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.93 on epoch=27
05/30/2022 20:25:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.91 on epoch=29
05/30/2022 20:25:04 - INFO - __main__ - Step 130 Global step 130 Train loss 0.94 on epoch=32
05/30/2022 20:25:06 - INFO - __main__ - Step 140 Global step 140 Train loss 0.96 on epoch=34
05/30/2022 20:25:09 - INFO - __main__ - Step 150 Global step 150 Train loss 0.95 on epoch=37
05/30/2022 20:25:09 - INFO - __main__ - Global step 150 Train loss 0.94 Classification-F1 0.1 on epoch=37
05/30/2022 20:25:12 - INFO - __main__ - Step 160 Global step 160 Train loss 1.00 on epoch=39
05/30/2022 20:25:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.93 on epoch=42
05/30/2022 20:25:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.82 on epoch=44
05/30/2022 20:25:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.90 on epoch=47
05/30/2022 20:25:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.85 on epoch=49
05/30/2022 20:25:22 - INFO - __main__ - Global step 200 Train loss 0.90 Classification-F1 0.11485019539730784 on epoch=49
05/30/2022 20:25:22 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.11485019539730784 on epoch=49, global_step=200
05/30/2022 20:25:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.87 on epoch=52
05/30/2022 20:25:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.84 on epoch=54
05/30/2022 20:25:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.90 on epoch=57
05/30/2022 20:25:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.78 on epoch=59
05/30/2022 20:25:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.85 on epoch=62
05/30/2022 20:25:35 - INFO - __main__ - Global step 250 Train loss 0.85 Classification-F1 0.1 on epoch=62
05/30/2022 20:25:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.80 on epoch=64
05/30/2022 20:25:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.88 on epoch=67
05/30/2022 20:25:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.82 on epoch=69
05/30/2022 20:25:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.79 on epoch=72
05/30/2022 20:25:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.76 on epoch=74
05/30/2022 20:25:48 - INFO - __main__ - Global step 300 Train loss 0.81 Classification-F1 0.20920303605313093 on epoch=74
05/30/2022 20:25:48 - INFO - __main__ - Saving model with best Classification-F1: 0.11485019539730784 -> 0.20920303605313093 on epoch=74, global_step=300
05/30/2022 20:25:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.84 on epoch=77
05/30/2022 20:25:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.77 on epoch=79
05/30/2022 20:25:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.83 on epoch=82
05/30/2022 20:25:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.77 on epoch=84
05/30/2022 20:26:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.82 on epoch=87
05/30/2022 20:26:01 - INFO - __main__ - Global step 350 Train loss 0.81 Classification-F1 0.1 on epoch=87
05/30/2022 20:26:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.76 on epoch=89
05/30/2022 20:26:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.77 on epoch=92
05/30/2022 20:26:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.76 on epoch=94
05/30/2022 20:26:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.77 on epoch=97
05/30/2022 20:26:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.77 on epoch=99
05/30/2022 20:26:14 - INFO - __main__ - Global step 400 Train loss 0.77 Classification-F1 0.1873065015479876 on epoch=99
05/30/2022 20:26:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.70 on epoch=102
05/30/2022 20:26:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.83 on epoch=104
05/30/2022 20:26:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.74 on epoch=107
05/30/2022 20:26:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.79 on epoch=109
05/30/2022 20:26:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.74 on epoch=112
05/30/2022 20:26:27 - INFO - __main__ - Global step 450 Train loss 0.76 Classification-F1 0.35440298507462686 on epoch=112
05/30/2022 20:26:27 - INFO - __main__ - Saving model with best Classification-F1: 0.20920303605313093 -> 0.35440298507462686 on epoch=112, global_step=450
05/30/2022 20:26:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.63 on epoch=114
05/30/2022 20:26:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.68 on epoch=117
05/30/2022 20:26:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.62 on epoch=119
05/30/2022 20:26:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.60 on epoch=122
05/30/2022 20:26:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.57 on epoch=124
05/30/2022 20:26:40 - INFO - __main__ - Global step 500 Train loss 0.62 Classification-F1 0.38262987012987015 on epoch=124
05/30/2022 20:26:40 - INFO - __main__ - Saving model with best Classification-F1: 0.35440298507462686 -> 0.38262987012987015 on epoch=124, global_step=500
05/30/2022 20:26:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.68 on epoch=127
05/30/2022 20:26:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.61 on epoch=129
05/30/2022 20:26:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.54 on epoch=132
05/30/2022 20:26:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=134
05/30/2022 20:26:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.52 on epoch=137
05/30/2022 20:26:52 - INFO - __main__ - Global step 550 Train loss 0.57 Classification-F1 0.310106998264893 on epoch=137
05/30/2022 20:26:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=139
05/30/2022 20:26:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.47 on epoch=142
05/30/2022 20:27:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=144
05/30/2022 20:27:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=147
05/30/2022 20:27:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=149
05/30/2022 20:27:05 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.3214285714285714 on epoch=149
05/30/2022 20:27:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=152
05/30/2022 20:27:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=154
05/30/2022 20:27:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=157
05/30/2022 20:27:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.40 on epoch=159
05/30/2022 20:27:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=162
05/30/2022 20:27:18 - INFO - __main__ - Global step 650 Train loss 0.41 Classification-F1 0.6897435897435897 on epoch=162
05/30/2022 20:27:18 - INFO - __main__ - Saving model with best Classification-F1: 0.38262987012987015 -> 0.6897435897435897 on epoch=162, global_step=650
05/30/2022 20:27:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=164
05/30/2022 20:27:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.35 on epoch=167
05/30/2022 20:27:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=169
05/30/2022 20:27:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.29 on epoch=172
05/30/2022 20:27:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.29 on epoch=174
05/30/2022 20:27:31 - INFO - __main__ - Global step 700 Train loss 0.33 Classification-F1 0.7082390648567118 on epoch=174
05/30/2022 20:27:31 - INFO - __main__ - Saving model with best Classification-F1: 0.6897435897435897 -> 0.7082390648567118 on epoch=174, global_step=700
05/30/2022 20:27:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.35 on epoch=177
05/30/2022 20:27:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=179
05/30/2022 20:27:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.26 on epoch=182
05/30/2022 20:27:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.28 on epoch=184
05/30/2022 20:27:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=187
05/30/2022 20:27:44 - INFO - __main__ - Global step 750 Train loss 0.29 Classification-F1 0.5856676003734826 on epoch=187
05/30/2022 20:27:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.28 on epoch=189
05/30/2022 20:27:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=192
05/30/2022 20:27:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.16 on epoch=194
05/30/2022 20:27:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.32 on epoch=197
05/30/2022 20:27:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.28 on epoch=199
05/30/2022 20:27:57 - INFO - __main__ - Global step 800 Train loss 0.26 Classification-F1 0.5716004296455425 on epoch=199
05/30/2022 20:27:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=202
05/30/2022 20:28:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=204
05/30/2022 20:28:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=207
05/30/2022 20:28:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=209
05/30/2022 20:28:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.16 on epoch=212
05/30/2022 20:28:10 - INFO - __main__ - Global step 850 Train loss 0.23 Classification-F1 0.6572926530459392 on epoch=212
05/30/2022 20:28:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.14 on epoch=214
05/30/2022 20:28:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=217
05/30/2022 20:28:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.16 on epoch=219
05/30/2022 20:28:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.17 on epoch=222
05/30/2022 20:28:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=224
05/30/2022 20:28:23 - INFO - __main__ - Global step 900 Train loss 0.13 Classification-F1 0.6842128178335074 on epoch=224
05/30/2022 20:28:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.14 on epoch=227
05/30/2022 20:28:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.12 on epoch=229
05/30/2022 20:28:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=232
05/30/2022 20:28:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=234
05/30/2022 20:28:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=237
05/30/2022 20:28:36 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.6867009342922122 on epoch=237
05/30/2022 20:28:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=239
05/30/2022 20:28:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=242
05/30/2022 20:28:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=244
05/30/2022 20:28:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=247
05/30/2022 20:28:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=249
05/30/2022 20:28:48 - INFO - __main__ - Global step 1000 Train loss 0.13 Classification-F1 0.6717647058823529 on epoch=249
05/30/2022 20:28:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=252
05/30/2022 20:28:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=254
05/30/2022 20:28:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=257
05/30/2022 20:28:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.11 on epoch=259
05/30/2022 20:29:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=262
05/30/2022 20:29:01 - INFO - __main__ - Global step 1050 Train loss 0.09 Classification-F1 0.7639296187683284 on epoch=262
05/30/2022 20:29:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7082390648567118 -> 0.7639296187683284 on epoch=262, global_step=1050
05/30/2022 20:29:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=264
05/30/2022 20:29:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=267
05/30/2022 20:29:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.17 on epoch=269
05/30/2022 20:29:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=272
05/30/2022 20:29:13 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
05/30/2022 20:29:14 - INFO - __main__ - Global step 1100 Train loss 0.10 Classification-F1 0.6370822041553749 on epoch=274
05/30/2022 20:29:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.09 on epoch=277
05/30/2022 20:29:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=279
05/30/2022 20:29:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=282
05/30/2022 20:29:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=284
05/30/2022 20:29:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=287
05/30/2022 20:29:27 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.7355113636363637 on epoch=287
05/30/2022 20:29:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
05/30/2022 20:29:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
05/30/2022 20:29:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.11 on epoch=294
05/30/2022 20:29:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=297
05/30/2022 20:29:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.14 on epoch=299
05/30/2022 20:29:40 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.7513587487781036 on epoch=299
05/30/2022 20:29:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=302
05/30/2022 20:29:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=304
05/30/2022 20:29:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
05/30/2022 20:29:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=309
05/30/2022 20:29:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
05/30/2022 20:29:53 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.7763409961685823 on epoch=312
05/30/2022 20:29:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7639296187683284 -> 0.7763409961685823 on epoch=312, global_step=1250
05/30/2022 20:29:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=314
05/30/2022 20:29:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.12 on epoch=317
05/30/2022 20:30:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=319
05/30/2022 20:30:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=322
05/30/2022 20:30:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=324
05/30/2022 20:30:06 - INFO - __main__ - Global step 1300 Train loss 0.09 Classification-F1 0.7519726919107723 on epoch=324
05/30/2022 20:30:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=327
05/30/2022 20:30:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.20 on epoch=329
05/30/2022 20:30:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=332
05/30/2022 20:30:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=334
05/30/2022 20:30:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=337
05/30/2022 20:30:19 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.7499643493761141 on epoch=337
05/30/2022 20:30:21 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=339
05/30/2022 20:30:23 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
05/30/2022 20:30:26 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=344
05/30/2022 20:30:28 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=347
05/30/2022 20:30:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=349
05/30/2022 20:30:32 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.7198352259531348 on epoch=349
05/30/2022 20:30:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=352
05/30/2022 20:30:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
05/30/2022 20:30:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=357
05/30/2022 20:30:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=359
05/30/2022 20:30:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=362
05/30/2022 20:30:45 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.7053054981725727 on epoch=362
05/30/2022 20:30:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
05/30/2022 20:30:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
05/30/2022 20:30:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/30/2022 20:30:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=372
05/30/2022 20:30:57 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=374
05/30/2022 20:30:58 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.6394830444015528 on epoch=374
05/30/2022 20:31:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=377
05/30/2022 20:31:03 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
05/30/2022 20:31:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
05/30/2022 20:31:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=384
05/30/2022 20:31:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/30/2022 20:31:11 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.6903044871794871 on epoch=387
05/30/2022 20:31:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
05/30/2022 20:31:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
05/30/2022 20:31:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
05/30/2022 20:31:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.10 on epoch=397
05/30/2022 20:31:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.09 on epoch=399
05/30/2022 20:31:24 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.7494195992179864 on epoch=399
05/30/2022 20:31:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
05/30/2022 20:31:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/30/2022 20:31:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/30/2022 20:31:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/30/2022 20:31:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/30/2022 20:31:36 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7325960626185958 on epoch=412
05/30/2022 20:31:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
05/30/2022 20:31:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=417
05/30/2022 20:31:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/30/2022 20:31:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/30/2022 20:31:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=424
05/30/2022 20:31:49 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.7335160359353908 on epoch=424
05/30/2022 20:31:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/30/2022 20:31:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=429
05/30/2022 20:31:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/30/2022 20:31:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=434
05/30/2022 20:32:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=437
05/30/2022 20:32:02 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.7525573961057831 on epoch=437
05/30/2022 20:32:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
05/30/2022 20:32:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
05/30/2022 20:32:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
05/30/2022 20:32:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
05/30/2022 20:32:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/30/2022 20:32:15 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7038515406162464 on epoch=449
05/30/2022 20:32:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/30/2022 20:32:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/30/2022 20:32:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
05/30/2022 20:32:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
05/30/2022 20:32:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
05/30/2022 20:32:27 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6870535714285715 on epoch=462
05/30/2022 20:32:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/30/2022 20:32:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=467
05/30/2022 20:32:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/30/2022 20:32:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/30/2022 20:32:39 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/30/2022 20:32:40 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6702574274537703 on epoch=474
05/30/2022 20:32:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/30/2022 20:32:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/30/2022 20:32:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/30/2022 20:32:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
05/30/2022 20:32:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/30/2022 20:32:53 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6878694581280789 on epoch=487
05/30/2022 20:32:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/30/2022 20:32:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
05/30/2022 20:33:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/30/2022 20:33:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/30/2022 20:33:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/30/2022 20:33:06 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7003472222222222 on epoch=499
05/30/2022 20:33:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/30/2022 20:33:11 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=504
05/30/2022 20:33:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/30/2022 20:33:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/30/2022 20:33:18 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/30/2022 20:33:19 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6826798654244307 on epoch=512
05/30/2022 20:33:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
05/30/2022 20:33:24 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
05/30/2022 20:33:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/30/2022 20:33:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/30/2022 20:33:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/30/2022 20:33:32 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7334280303030304 on epoch=524
05/30/2022 20:33:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.09 on epoch=527
05/30/2022 20:33:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
05/30/2022 20:33:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
05/30/2022 20:33:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
05/30/2022 20:33:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/30/2022 20:33:45 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.675 on epoch=537
05/30/2022 20:33:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
05/30/2022 20:33:49 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/30/2022 20:33:52 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/30/2022 20:33:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.08 on epoch=547
05/30/2022 20:33:56 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/30/2022 20:33:57 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.7287525987525987 on epoch=549
05/30/2022 20:34:00 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/30/2022 20:34:02 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=554
05/30/2022 20:34:04 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=557
05/30/2022 20:34:07 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/30/2022 20:34:09 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/30/2022 20:34:10 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.703763440860215 on epoch=562
05/30/2022 20:34:13 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/30/2022 20:34:15 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/30/2022 20:34:17 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/30/2022 20:34:20 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/30/2022 20:34:22 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/30/2022 20:34:23 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.7841178307280995 on epoch=574
05/30/2022 20:34:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7763409961685823 -> 0.7841178307280995 on epoch=574, global_step=2300
05/30/2022 20:34:25 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/30/2022 20:34:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=579
05/30/2022 20:34:30 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/30/2022 20:34:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=584
05/30/2022 20:34:35 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=587
05/30/2022 20:34:36 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.7031314855858467 on epoch=587
05/30/2022 20:34:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=589
05/30/2022 20:34:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/30/2022 20:34:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/30/2022 20:34:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
05/30/2022 20:34:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/30/2022 20:34:49 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.705869350392677 on epoch=599
05/30/2022 20:34:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/30/2022 20:34:53 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
05/30/2022 20:34:56 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/30/2022 20:34:58 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=609
05/30/2022 20:35:00 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/30/2022 20:35:01 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7200157037230909 on epoch=612
05/30/2022 20:35:04 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=614
05/30/2022 20:35:06 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/30/2022 20:35:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/30/2022 20:35:11 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=622
05/30/2022 20:35:13 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/30/2022 20:35:14 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.7175213675213676 on epoch=624
05/30/2022 20:35:17 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/30/2022 20:35:19 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/30/2022 20:35:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/30/2022 20:35:24 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/30/2022 20:35:26 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/30/2022 20:35:27 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7487821565407773 on epoch=637
05/30/2022 20:35:30 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/30/2022 20:35:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/30/2022 20:35:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/30/2022 20:35:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/30/2022 20:35:39 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=649
05/30/2022 20:35:40 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7592664124227968 on epoch=649
05/30/2022 20:35:42 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.13 on epoch=652
05/30/2022 20:35:45 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/30/2022 20:35:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/30/2022 20:35:50 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/30/2022 20:35:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/30/2022 20:35:53 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.7337640518084066 on epoch=662
05/30/2022 20:35:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/30/2022 20:35:58 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
05/30/2022 20:36:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/30/2022 20:36:03 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/30/2022 20:36:05 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
05/30/2022 20:36:06 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7148648648648648 on epoch=674
05/30/2022 20:36:08 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/30/2022 20:36:10 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/30/2022 20:36:13 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/30/2022 20:36:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/30/2022 20:36:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/30/2022 20:36:19 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7141923379531562 on epoch=687
05/30/2022 20:36:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/30/2022 20:36:23 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/30/2022 20:36:26 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/30/2022 20:36:28 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/30/2022 20:36:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/30/2022 20:36:31 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7287031799899447 on epoch=699
05/30/2022 20:36:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/30/2022 20:36:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/30/2022 20:36:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/30/2022 20:36:41 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/30/2022 20:36:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/30/2022 20:36:44 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7509152272019919 on epoch=712
05/30/2022 20:36:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=714
05/30/2022 20:36:49 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/30/2022 20:36:51 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
05/30/2022 20:36:54 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/30/2022 20:36:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/30/2022 20:36:57 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6520146520146519 on epoch=724
05/30/2022 20:36:59 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/30/2022 20:37:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/30/2022 20:37:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/30/2022 20:37:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/30/2022 20:37:09 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/30/2022 20:37:10 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6999418773612321 on epoch=737
05/30/2022 20:37:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=739
05/30/2022 20:37:15 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/30/2022 20:37:17 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/30/2022 20:37:19 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=747
05/30/2022 20:37:22 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/30/2022 20:37:23 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.7132671250318309 on epoch=749
05/30/2022 20:37:23 - INFO - __main__ - save last model!
05/30/2022 20:37:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 20:37:23 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 20:37:23 - INFO - __main__ - Printing 3 examples
05/30/2022 20:37:23 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 20:37:23 - INFO - __main__ - ['others']
05/30/2022 20:37:23 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 20:37:23 - INFO - __main__ - ['others']
05/30/2022 20:37:23 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 20:37:23 - INFO - __main__ - ['others']
05/30/2022 20:37:23 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 20:37:23 - INFO - __main__ - Printing 3 examples
05/30/2022 20:37:23 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/30/2022 20:37:23 - INFO - __main__ - ['others']
05/30/2022 20:37:23 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/30/2022 20:37:23 - INFO - __main__ - ['others']
05/30/2022 20:37:23 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/30/2022 20:37:23 - INFO - __main__ - ['others']
05/30/2022 20:37:23 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:37:23 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:37:23 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:37:23 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 20:37:23 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 20:37:23 - INFO - __main__ - Printing 3 examples
05/30/2022 20:37:23 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/30/2022 20:37:23 - INFO - __main__ - ['others']
05/30/2022 20:37:23 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/30/2022 20:37:23 - INFO - __main__ - ['others']
05/30/2022 20:37:23 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/30/2022 20:37:23 - INFO - __main__ - ['others']
05/30/2022 20:37:23 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:37:23 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:37:23 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 20:37:25 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:37:30 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 20:37:38 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 20:37:38 - INFO - __main__ - task name: emo
05/30/2022 20:37:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 20:37:39 - INFO - __main__ - Starting training!
05/30/2022 20:38:54 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_13_0.5_8_predictions.txt
05/30/2022 20:38:54 - INFO - __main__ - Classification-F1 on test data: 0.3718
05/30/2022 20:38:55 - INFO - __main__ - prefix=emo_16_13, lr=0.5, bsz=8, dev_performance=0.7841178307280995, test_performance=0.3717558908190162
05/30/2022 20:38:55 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.4, bsz=8 ...
05/30/2022 20:38:56 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 20:38:56 - INFO - __main__ - Printing 3 examples
05/30/2022 20:38:56 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/30/2022 20:38:56 - INFO - __main__ - ['others']
05/30/2022 20:38:56 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/30/2022 20:38:56 - INFO - __main__ - ['others']
05/30/2022 20:38:56 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/30/2022 20:38:56 - INFO - __main__ - ['others']
05/30/2022 20:38:56 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:38:56 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:38:56 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 20:38:56 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 20:38:56 - INFO - __main__ - Printing 3 examples
05/30/2022 20:38:56 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/30/2022 20:38:56 - INFO - __main__ - ['others']
05/30/2022 20:38:56 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/30/2022 20:38:56 - INFO - __main__ - ['others']
05/30/2022 20:38:56 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/30/2022 20:38:56 - INFO - __main__ - ['others']
05/30/2022 20:38:56 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:38:56 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:38:56 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 20:39:10 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 20:39:10 - INFO - __main__ - task name: emo
05/30/2022 20:39:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 20:39:11 - INFO - __main__ - Starting training!
05/30/2022 20:39:14 - INFO - __main__ - Step 10 Global step 10 Train loss 7.55 on epoch=2
05/30/2022 20:39:17 - INFO - __main__ - Step 20 Global step 20 Train loss 4.44 on epoch=4
05/30/2022 20:39:19 - INFO - __main__ - Step 30 Global step 30 Train loss 2.24 on epoch=7
05/30/2022 20:39:22 - INFO - __main__ - Step 40 Global step 40 Train loss 1.41 on epoch=9
05/30/2022 20:39:24 - INFO - __main__ - Step 50 Global step 50 Train loss 1.45 on epoch=12
05/30/2022 20:39:25 - INFO - __main__ - Global step 50 Train loss 3.42 Classification-F1 0.17838216367628135 on epoch=12
05/30/2022 20:39:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.17838216367628135 on epoch=12, global_step=50
05/30/2022 20:39:27 - INFO - __main__ - Step 60 Global step 60 Train loss 1.15 on epoch=14
05/30/2022 20:39:30 - INFO - __main__ - Step 70 Global step 70 Train loss 1.13 on epoch=17
05/30/2022 20:39:32 - INFO - __main__ - Step 80 Global step 80 Train loss 1.13 on epoch=19
05/30/2022 20:39:35 - INFO - __main__ - Step 90 Global step 90 Train loss 1.07 on epoch=22
05/30/2022 20:39:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.92 on epoch=24
05/30/2022 20:39:38 - INFO - __main__ - Global step 100 Train loss 1.08 Classification-F1 0.1 on epoch=24
05/30/2022 20:39:41 - INFO - __main__ - Step 110 Global step 110 Train loss 1.05 on epoch=27
05/30/2022 20:39:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.96 on epoch=29
05/30/2022 20:39:46 - INFO - __main__ - Step 130 Global step 130 Train loss 1.11 on epoch=32
05/30/2022 20:39:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.95 on epoch=34
05/30/2022 20:39:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.86 on epoch=37
05/30/2022 20:39:51 - INFO - __main__ - Global step 150 Train loss 0.99 Classification-F1 0.1 on epoch=37
05/30/2022 20:39:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.98 on epoch=39
05/30/2022 20:39:56 - INFO - __main__ - Step 170 Global step 170 Train loss 1.00 on epoch=42
05/30/2022 20:39:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.99 on epoch=44
05/30/2022 20:40:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.91 on epoch=47
05/30/2022 20:40:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.94 on epoch=49
05/30/2022 20:40:05 - INFO - __main__ - Global step 200 Train loss 0.96 Classification-F1 0.1 on epoch=49
05/30/2022 20:40:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.89 on epoch=52
05/30/2022 20:40:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.85 on epoch=54
05/30/2022 20:40:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.93 on epoch=57
05/30/2022 20:40:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.89 on epoch=59
05/30/2022 20:40:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.92 on epoch=62
05/30/2022 20:40:18 - INFO - __main__ - Global step 250 Train loss 0.90 Classification-F1 0.3731962481962482 on epoch=62
05/30/2022 20:40:18 - INFO - __main__ - Saving model with best Classification-F1: 0.17838216367628135 -> 0.3731962481962482 on epoch=62, global_step=250
05/30/2022 20:40:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.84 on epoch=64
05/30/2022 20:40:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.82 on epoch=67
05/30/2022 20:40:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.73 on epoch=69
05/30/2022 20:40:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.84 on epoch=72
05/30/2022 20:40:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.70 on epoch=74
05/30/2022 20:40:31 - INFO - __main__ - Global step 300 Train loss 0.79 Classification-F1 0.41882005647300424 on epoch=74
05/30/2022 20:40:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3731962481962482 -> 0.41882005647300424 on epoch=74, global_step=300
05/30/2022 20:40:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.71 on epoch=77
05/30/2022 20:40:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.89 on epoch=79
05/30/2022 20:40:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.75 on epoch=82
05/30/2022 20:40:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.84 on epoch=84
05/30/2022 20:40:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.72 on epoch=87
05/30/2022 20:40:44 - INFO - __main__ - Global step 350 Train loss 0.78 Classification-F1 0.5027958152958153 on epoch=87
05/30/2022 20:40:44 - INFO - __main__ - Saving model with best Classification-F1: 0.41882005647300424 -> 0.5027958152958153 on epoch=87, global_step=350
05/30/2022 20:40:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.82 on epoch=89
05/30/2022 20:40:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.71 on epoch=92
05/30/2022 20:40:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.71 on epoch=94
05/30/2022 20:40:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.71 on epoch=97
05/30/2022 20:40:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.78 on epoch=99
05/30/2022 20:40:58 - INFO - __main__ - Global step 400 Train loss 0.74 Classification-F1 0.47743506493506493 on epoch=99
05/30/2022 20:41:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.69 on epoch=102
05/30/2022 20:41:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.68 on epoch=104
05/30/2022 20:41:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.70 on epoch=107
05/30/2022 20:41:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.61 on epoch=109
05/30/2022 20:41:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.72 on epoch=112
05/30/2022 20:41:11 - INFO - __main__ - Global step 450 Train loss 0.68 Classification-F1 0.4618373764600179 on epoch=112
05/30/2022 20:41:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.61 on epoch=114
05/30/2022 20:41:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.56 on epoch=117
05/30/2022 20:41:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.63 on epoch=119
05/30/2022 20:41:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.63 on epoch=122
05/30/2022 20:41:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.63 on epoch=124
05/30/2022 20:41:24 - INFO - __main__ - Global step 500 Train loss 0.61 Classification-F1 0.6060009148185856 on epoch=124
05/30/2022 20:41:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5027958152958153 -> 0.6060009148185856 on epoch=124, global_step=500
05/30/2022 20:41:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.63 on epoch=127
05/30/2022 20:41:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.58 on epoch=129
05/30/2022 20:41:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.61 on epoch=132
05/30/2022 20:41:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.56 on epoch=134
05/30/2022 20:41:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.54 on epoch=137
05/30/2022 20:41:38 - INFO - __main__ - Global step 550 Train loss 0.58 Classification-F1 0.6566807094190461 on epoch=137
05/30/2022 20:41:38 - INFO - __main__ - Saving model with best Classification-F1: 0.6060009148185856 -> 0.6566807094190461 on epoch=137, global_step=550
05/30/2022 20:41:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.58 on epoch=139
05/30/2022 20:41:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=142
05/30/2022 20:41:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=144
05/30/2022 20:41:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=147
05/30/2022 20:41:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=149
05/30/2022 20:41:51 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.5768842268842269 on epoch=149
05/30/2022 20:41:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=152
05/30/2022 20:41:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.50 on epoch=154
05/30/2022 20:41:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=157
05/30/2022 20:42:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.37 on epoch=159
05/30/2022 20:42:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=162
05/30/2022 20:42:04 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.6672453703703705 on epoch=162
05/30/2022 20:42:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6566807094190461 -> 0.6672453703703705 on epoch=162, global_step=650
05/30/2022 20:42:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=164
05/30/2022 20:42:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.34 on epoch=167
05/30/2022 20:42:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=169
05/30/2022 20:42:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=172
05/30/2022 20:42:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=174
05/30/2022 20:42:18 - INFO - __main__ - Global step 700 Train loss 0.31 Classification-F1 0.6808982683982684 on epoch=174
05/30/2022 20:42:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6672453703703705 -> 0.6808982683982684 on epoch=174, global_step=700
05/30/2022 20:42:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.28 on epoch=177
05/30/2022 20:42:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.27 on epoch=179
05/30/2022 20:42:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.35 on epoch=182
05/30/2022 20:42:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=184
05/30/2022 20:42:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.17 on epoch=187
05/30/2022 20:42:31 - INFO - __main__ - Global step 750 Train loss 0.26 Classification-F1 0.7176206645646374 on epoch=187
05/30/2022 20:42:31 - INFO - __main__ - Saving model with best Classification-F1: 0.6808982683982684 -> 0.7176206645646374 on epoch=187, global_step=750
05/30/2022 20:42:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.19 on epoch=189
05/30/2022 20:42:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.32 on epoch=192
05/30/2022 20:42:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=194
05/30/2022 20:42:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=197
05/30/2022 20:42:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.17 on epoch=199
05/30/2022 20:42:44 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.6879289215686275 on epoch=199
05/30/2022 20:42:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=202
05/30/2022 20:42:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.16 on epoch=204
05/30/2022 20:42:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=207
05/30/2022 20:42:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.33 on epoch=209
05/30/2022 20:42:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=212
05/30/2022 20:42:58 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.6877552177858439 on epoch=212
05/30/2022 20:43:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=214
05/30/2022 20:43:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=217
05/30/2022 20:43:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=219
05/30/2022 20:43:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=222
05/30/2022 20:43:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.11 on epoch=224
05/30/2022 20:43:11 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.6050527337716782 on epoch=224
05/30/2022 20:43:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=227
05/30/2022 20:43:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=229
05/30/2022 20:43:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=232
05/30/2022 20:43:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.13 on epoch=234
05/30/2022 20:43:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=237
05/30/2022 20:43:24 - INFO - __main__ - Global step 950 Train loss 0.16 Classification-F1 0.6996058558558558 on epoch=237
05/30/2022 20:43:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=239
05/30/2022 20:43:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=242
05/30/2022 20:43:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.15 on epoch=244
05/30/2022 20:43:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.13 on epoch=247
05/30/2022 20:43:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=249
05/30/2022 20:43:38 - INFO - __main__ - Global step 1000 Train loss 0.15 Classification-F1 0.736065286065286 on epoch=249
05/30/2022 20:43:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7176206645646374 -> 0.736065286065286 on epoch=249, global_step=1000
05/30/2022 20:43:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=252
05/30/2022 20:43:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=254
05/30/2022 20:43:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.11 on epoch=257
05/30/2022 20:43:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.13 on epoch=259
05/30/2022 20:43:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=262
05/30/2022 20:43:51 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.6596783205619412 on epoch=262
05/30/2022 20:43:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=264
05/30/2022 20:43:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=267
05/30/2022 20:43:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=269
05/30/2022 20:44:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=272
05/30/2022 20:44:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=274
05/30/2022 20:44:05 - INFO - __main__ - Global step 1100 Train loss 0.08 Classification-F1 0.7212987976058054 on epoch=274
05/30/2022 20:44:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=277
05/30/2022 20:44:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=279
05/30/2022 20:44:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
05/30/2022 20:44:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=284
05/30/2022 20:44:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=287
05/30/2022 20:44:18 - INFO - __main__ - Global step 1150 Train loss 0.06 Classification-F1 0.7080747955747956 on epoch=287
05/30/2022 20:44:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=289
05/30/2022 20:44:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
05/30/2022 20:44:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.17 on epoch=294
05/30/2022 20:44:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=297
05/30/2022 20:44:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=299
05/30/2022 20:44:32 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.6876689092059111 on epoch=299
05/30/2022 20:44:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.14 on epoch=302
05/30/2022 20:44:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
05/30/2022 20:44:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=307
05/30/2022 20:44:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=309
05/30/2022 20:44:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=312
05/30/2022 20:44:45 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.6692268305171531 on epoch=312
05/30/2022 20:44:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
05/30/2022 20:44:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.13 on epoch=317
05/30/2022 20:44:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=319
05/30/2022 20:44:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
05/30/2022 20:44:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=324
05/30/2022 20:44:59 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.6896551724137931 on epoch=324
05/30/2022 20:45:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=327
05/30/2022 20:45:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=329
05/30/2022 20:45:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=332
05/30/2022 20:45:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=334
05/30/2022 20:45:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
05/30/2022 20:45:12 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.7045613185660392 on epoch=337
05/30/2022 20:45:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=339
05/30/2022 20:45:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=342
05/30/2022 20:45:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
05/30/2022 20:45:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=347
05/30/2022 20:45:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
05/30/2022 20:45:25 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.7001242361642807 on epoch=349
05/30/2022 20:45:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=352
05/30/2022 20:45:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
05/30/2022 20:45:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=357
05/30/2022 20:45:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/30/2022 20:45:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=362
05/30/2022 20:45:38 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.6835634953282013 on epoch=362
05/30/2022 20:45:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
05/30/2022 20:45:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=367
05/30/2022 20:45:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=369
05/30/2022 20:45:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
05/30/2022 20:45:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
05/30/2022 20:45:52 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.6550331535435491 on epoch=374
05/30/2022 20:45:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
05/30/2022 20:45:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=379
05/30/2022 20:45:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
05/30/2022 20:46:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/30/2022 20:46:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
05/30/2022 20:46:05 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.6816730523627076 on epoch=387
05/30/2022 20:46:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
05/30/2022 20:46:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=392
05/30/2022 20:46:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
05/30/2022 20:46:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.19 on epoch=397
05/30/2022 20:46:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
05/30/2022 20:46:18 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.7050420168067227 on epoch=399
05/30/2022 20:46:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=402
05/30/2022 20:46:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
05/30/2022 20:46:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/30/2022 20:46:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=409
05/30/2022 20:46:30 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/30/2022 20:46:31 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.7045038468668892 on epoch=412
05/30/2022 20:46:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
05/30/2022 20:46:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=417
05/30/2022 20:46:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=419
05/30/2022 20:46:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
05/30/2022 20:46:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/30/2022 20:46:45 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.7194975147561353 on epoch=424
05/30/2022 20:46:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=427
05/30/2022 20:46:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
05/30/2022 20:46:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=432
05/30/2022 20:46:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
05/30/2022 20:46:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=437
05/30/2022 20:46:58 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.6462215320910973 on epoch=437
05/30/2022 20:47:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=439
05/30/2022 20:47:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/30/2022 20:47:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
05/30/2022 20:47:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/30/2022 20:47:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/30/2022 20:47:11 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.7043010752688172 on epoch=449
05/30/2022 20:47:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=452
05/30/2022 20:47:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/30/2022 20:47:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=457
05/30/2022 20:47:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/30/2022 20:47:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=462
05/30/2022 20:47:25 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.6885085502732561 on epoch=462
05/30/2022 20:47:27 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
05/30/2022 20:47:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/30/2022 20:47:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=469
05/30/2022 20:47:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/30/2022 20:47:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/30/2022 20:47:38 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.6888480392156863 on epoch=474
05/30/2022 20:47:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
05/30/2022 20:47:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/30/2022 20:47:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/30/2022 20:47:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/30/2022 20:47:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/30/2022 20:47:51 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.7351097178683385 on epoch=487
05/30/2022 20:47:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
05/30/2022 20:47:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=492
05/30/2022 20:47:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=494
05/30/2022 20:48:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/30/2022 20:48:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
05/30/2022 20:48:05 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.672720165652468 on epoch=499
05/30/2022 20:48:07 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
05/30/2022 20:48:09 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/30/2022 20:48:12 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.12 on epoch=507
05/30/2022 20:48:14 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=509
05/30/2022 20:48:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/30/2022 20:48:18 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.6872232764073372 on epoch=512
05/30/2022 20:48:20 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/30/2022 20:48:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/30/2022 20:48:25 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/30/2022 20:48:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/30/2022 20:48:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/30/2022 20:48:31 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.672043010752688 on epoch=524
05/30/2022 20:48:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/30/2022 20:48:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/30/2022 20:48:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=532
05/30/2022 20:48:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
05/30/2022 20:48:43 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/30/2022 20:48:45 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6700840336134455 on epoch=537
05/30/2022 20:48:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/30/2022 20:48:49 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/30/2022 20:48:52 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/30/2022 20:48:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=547
05/30/2022 20:48:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/30/2022 20:48:58 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6874260480054091 on epoch=549
05/30/2022 20:49:00 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=552
05/30/2022 20:49:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/30/2022 20:49:05 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/30/2022 20:49:08 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=559
05/30/2022 20:49:10 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=562
05/30/2022 20:49:11 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7068181169960924 on epoch=562
05/30/2022 20:49:14 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/30/2022 20:49:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
05/30/2022 20:49:18 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=569
05/30/2022 20:49:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/30/2022 20:49:23 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/30/2022 20:49:24 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7190972222222223 on epoch=574
05/30/2022 20:49:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/30/2022 20:49:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/30/2022 20:49:32 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/30/2022 20:49:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
05/30/2022 20:49:37 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/30/2022 20:49:38 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7040562596006145 on epoch=587
05/30/2022 20:49:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/30/2022 20:49:43 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=592
05/30/2022 20:49:45 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.12 on epoch=594
05/30/2022 20:49:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=597
05/30/2022 20:49:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/30/2022 20:49:51 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.7035414590858139 on epoch=599
05/30/2022 20:49:53 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=602
05/30/2022 20:49:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/30/2022 20:49:58 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/30/2022 20:50:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/30/2022 20:50:03 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
05/30/2022 20:50:04 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7172453842765029 on epoch=612
05/30/2022 20:50:07 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/30/2022 20:50:09 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/30/2022 20:50:11 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/30/2022 20:50:14 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/30/2022 20:50:16 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
05/30/2022 20:50:17 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7346183869016256 on epoch=624
05/30/2022 20:50:20 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/30/2022 20:50:22 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=629
05/30/2022 20:50:25 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/30/2022 20:50:27 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/30/2022 20:50:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/30/2022 20:50:31 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6705566097406706 on epoch=637
05/30/2022 20:50:33 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
05/30/2022 20:50:35 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/30/2022 20:50:38 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/30/2022 20:50:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/30/2022 20:50:43 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=649
05/30/2022 20:50:44 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6715686274509803 on epoch=649
05/30/2022 20:50:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.09 on epoch=652
05/30/2022 20:50:49 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/30/2022 20:50:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/30/2022 20:50:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.05 on epoch=659
05/30/2022 20:50:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=662
05/30/2022 20:50:57 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.6677841634738186 on epoch=662
05/30/2022 20:50:59 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/30/2022 20:51:02 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/30/2022 20:51:04 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
05/30/2022 20:51:07 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
05/30/2022 20:51:09 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=674
05/30/2022 20:51:10 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7191664590858139 on epoch=674
05/30/2022 20:51:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/30/2022 20:51:15 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/30/2022 20:51:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/30/2022 20:51:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/30/2022 20:51:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/30/2022 20:51:23 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.7018537299977463 on epoch=687
05/30/2022 20:51:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/30/2022 20:51:28 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/30/2022 20:51:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/30/2022 20:51:33 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/30/2022 20:51:35 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/30/2022 20:51:36 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.716017316017316 on epoch=699
05/30/2022 20:51:39 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=702
05/30/2022 20:51:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/30/2022 20:51:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/30/2022 20:51:46 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=709
05/30/2022 20:51:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/30/2022 20:51:50 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.7158180476491671 on epoch=712
05/30/2022 20:51:52 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/30/2022 20:51:54 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/30/2022 20:51:57 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/30/2022 20:51:59 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/30/2022 20:52:02 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/30/2022 20:52:03 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.717503078817734 on epoch=724
05/30/2022 20:52:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/30/2022 20:52:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/30/2022 20:52:10 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=732
05/30/2022 20:52:12 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/30/2022 20:52:15 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=737
05/30/2022 20:52:16 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.6706845238095238 on epoch=737
05/30/2022 20:52:18 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=739
05/30/2022 20:52:21 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/30/2022 20:52:23 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/30/2022 20:52:26 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/30/2022 20:52:28 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/30/2022 20:52:29 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.6562770562770563 on epoch=749
05/30/2022 20:52:29 - INFO - __main__ - save last model!
05/30/2022 20:52:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 20:52:29 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 20:52:29 - INFO - __main__ - Printing 3 examples
05/30/2022 20:52:29 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 20:52:29 - INFO - __main__ - ['others']
05/30/2022 20:52:29 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 20:52:29 - INFO - __main__ - ['others']
05/30/2022 20:52:29 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 20:52:29 - INFO - __main__ - ['others']
05/30/2022 20:52:29 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:52:29 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 20:52:29 - INFO - __main__ - Printing 3 examples
05/30/2022 20:52:29 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/30/2022 20:52:29 - INFO - __main__ - ['others']
05/30/2022 20:52:29 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/30/2022 20:52:29 - INFO - __main__ - ['others']
05/30/2022 20:52:29 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/30/2022 20:52:29 - INFO - __main__ - ['others']
05/30/2022 20:52:29 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:52:29 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:52:29 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 20:52:29 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 20:52:29 - INFO - __main__ - Printing 3 examples
05/30/2022 20:52:29 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/30/2022 20:52:29 - INFO - __main__ - ['others']
05/30/2022 20:52:29 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/30/2022 20:52:29 - INFO - __main__ - ['others']
05/30/2022 20:52:29 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/30/2022 20:52:29 - INFO - __main__ - ['others']
05/30/2022 20:52:29 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:52:29 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:52:29 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 20:52:31 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:52:36 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 20:52:48 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 20:52:48 - INFO - __main__ - task name: emo
05/30/2022 20:52:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 20:52:49 - INFO - __main__ - Starting training!
05/30/2022 20:54:07 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_13_0.4_8_predictions.txt
05/30/2022 20:54:07 - INFO - __main__ - Classification-F1 on test data: 0.2798
05/30/2022 20:54:07 - INFO - __main__ - prefix=emo_16_13, lr=0.4, bsz=8, dev_performance=0.736065286065286, test_performance=0.2798078078146522
05/30/2022 20:54:07 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.3, bsz=8 ...
05/30/2022 20:54:08 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 20:54:08 - INFO - __main__ - Printing 3 examples
05/30/2022 20:54:08 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/30/2022 20:54:08 - INFO - __main__ - ['others']
05/30/2022 20:54:08 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/30/2022 20:54:08 - INFO - __main__ - ['others']
05/30/2022 20:54:08 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/30/2022 20:54:08 - INFO - __main__ - ['others']
05/30/2022 20:54:08 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:54:08 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:54:08 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 20:54:08 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 20:54:08 - INFO - __main__ - Printing 3 examples
05/30/2022 20:54:08 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/30/2022 20:54:08 - INFO - __main__ - ['others']
05/30/2022 20:54:08 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/30/2022 20:54:08 - INFO - __main__ - ['others']
05/30/2022 20:54:08 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/30/2022 20:54:08 - INFO - __main__ - ['others']
05/30/2022 20:54:08 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:54:09 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:54:09 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 20:54:27 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 20:54:27 - INFO - __main__ - task name: emo
05/30/2022 20:54:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 20:54:28 - INFO - __main__ - Starting training!
05/30/2022 20:54:30 - INFO - __main__ - Step 10 Global step 10 Train loss 7.93 on epoch=2
05/30/2022 20:54:33 - INFO - __main__ - Step 20 Global step 20 Train loss 5.21 on epoch=4
05/30/2022 20:54:35 - INFO - __main__ - Step 30 Global step 30 Train loss 3.01 on epoch=7
05/30/2022 20:54:38 - INFO - __main__ - Step 40 Global step 40 Train loss 2.01 on epoch=9
05/30/2022 20:54:40 - INFO - __main__ - Step 50 Global step 50 Train loss 1.74 on epoch=12
05/30/2022 20:54:41 - INFO - __main__ - Global step 50 Train loss 3.98 Classification-F1 0.0821917808219178 on epoch=12
05/30/2022 20:54:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0821917808219178 on epoch=12, global_step=50
05/30/2022 20:54:44 - INFO - __main__ - Step 60 Global step 60 Train loss 1.39 on epoch=14
05/30/2022 20:54:46 - INFO - __main__ - Step 70 Global step 70 Train loss 1.33 on epoch=17
05/30/2022 20:54:49 - INFO - __main__ - Step 80 Global step 80 Train loss 1.14 on epoch=19
05/30/2022 20:54:51 - INFO - __main__ - Step 90 Global step 90 Train loss 1.04 on epoch=22
05/30/2022 20:54:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.97 on epoch=24
05/30/2022 20:54:54 - INFO - __main__ - Global step 100 Train loss 1.17 Classification-F1 0.16110780226325194 on epoch=24
05/30/2022 20:54:54 - INFO - __main__ - Saving model with best Classification-F1: 0.0821917808219178 -> 0.16110780226325194 on epoch=24, global_step=100
05/30/2022 20:54:57 - INFO - __main__ - Step 110 Global step 110 Train loss 1.05 on epoch=27
05/30/2022 20:54:59 - INFO - __main__ - Step 120 Global step 120 Train loss 0.98 on epoch=29
05/30/2022 20:55:02 - INFO - __main__ - Step 130 Global step 130 Train loss 1.11 on epoch=32
05/30/2022 20:55:04 - INFO - __main__ - Step 140 Global step 140 Train loss 1.04 on epoch=34
05/30/2022 20:55:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.94 on epoch=37
05/30/2022 20:55:07 - INFO - __main__ - Global step 150 Train loss 1.03 Classification-F1 0.09493670886075949 on epoch=37
05/30/2022 20:55:10 - INFO - __main__ - Step 160 Global step 160 Train loss 1.00 on epoch=39
05/30/2022 20:55:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.91 on epoch=42
05/30/2022 20:55:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.98 on epoch=44
05/30/2022 20:55:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.92 on epoch=47
05/30/2022 20:55:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.94 on epoch=49
05/30/2022 20:55:20 - INFO - __main__ - Global step 200 Train loss 0.95 Classification-F1 0.1343478260869565 on epoch=49
05/30/2022 20:55:23 - INFO - __main__ - Step 210 Global step 210 Train loss 1.00 on epoch=52
05/30/2022 20:55:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.95 on epoch=54
05/30/2022 20:55:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.82 on epoch=57
05/30/2022 20:55:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.93 on epoch=59
05/30/2022 20:55:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.96 on epoch=62
05/30/2022 20:55:33 - INFO - __main__ - Global step 250 Train loss 0.93 Classification-F1 0.1 on epoch=62
05/30/2022 20:55:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.87 on epoch=64
05/30/2022 20:55:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.90 on epoch=67
05/30/2022 20:55:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.97 on epoch=69
05/30/2022 20:55:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.89 on epoch=72
05/30/2022 20:55:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.94 on epoch=74
05/30/2022 20:55:47 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.19574752097635392 on epoch=74
05/30/2022 20:55:47 - INFO - __main__ - Saving model with best Classification-F1: 0.16110780226325194 -> 0.19574752097635392 on epoch=74, global_step=300
05/30/2022 20:55:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.91 on epoch=77
05/30/2022 20:55:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.84 on epoch=79
05/30/2022 20:55:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.85 on epoch=82
05/30/2022 20:55:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.80 on epoch=84
05/30/2022 20:55:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.83 on epoch=87
05/30/2022 20:56:00 - INFO - __main__ - Global step 350 Train loss 0.85 Classification-F1 0.1 on epoch=87
05/30/2022 20:56:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.81 on epoch=89
05/30/2022 20:56:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.83 on epoch=92
05/30/2022 20:56:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.87 on epoch=94
05/30/2022 20:56:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.88 on epoch=97
05/30/2022 20:56:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.87 on epoch=99
05/30/2022 20:56:13 - INFO - __main__ - Global step 400 Train loss 0.85 Classification-F1 0.17197802197802198 on epoch=99
05/30/2022 20:56:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.80 on epoch=102
05/30/2022 20:56:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.84 on epoch=104
05/30/2022 20:56:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.89 on epoch=107
05/30/2022 20:56:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.86 on epoch=109
05/30/2022 20:56:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.81 on epoch=112
05/30/2022 20:56:26 - INFO - __main__ - Global step 450 Train loss 0.84 Classification-F1 0.16108564637976402 on epoch=112
05/30/2022 20:56:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.80 on epoch=114
05/30/2022 20:56:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.83 on epoch=117
05/30/2022 20:56:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.77 on epoch=119
05/30/2022 20:56:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.79 on epoch=122
05/30/2022 20:56:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.79 on epoch=124
05/30/2022 20:56:39 - INFO - __main__ - Global step 500 Train loss 0.80 Classification-F1 0.1953125 on epoch=124
05/30/2022 20:56:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.84 on epoch=127
05/30/2022 20:56:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.79 on epoch=129
05/30/2022 20:56:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.73 on epoch=132
05/30/2022 20:56:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.77 on epoch=134
05/30/2022 20:56:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.75 on epoch=137
05/30/2022 20:56:52 - INFO - __main__ - Global step 550 Train loss 0.78 Classification-F1 0.24116798846431145 on epoch=137
05/30/2022 20:56:52 - INFO - __main__ - Saving model with best Classification-F1: 0.19574752097635392 -> 0.24116798846431145 on epoch=137, global_step=550
05/30/2022 20:56:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.77 on epoch=139
05/30/2022 20:56:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.78 on epoch=142
05/30/2022 20:56:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.73 on epoch=144
05/30/2022 20:57:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.79 on epoch=147
05/30/2022 20:57:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.69 on epoch=149
05/30/2022 20:57:05 - INFO - __main__ - Global step 600 Train loss 0.75 Classification-F1 0.3386191447053789 on epoch=149
05/30/2022 20:57:05 - INFO - __main__ - Saving model with best Classification-F1: 0.24116798846431145 -> 0.3386191447053789 on epoch=149, global_step=600
05/30/2022 20:57:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.76 on epoch=152
05/30/2022 20:57:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.67 on epoch=154
05/30/2022 20:57:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.75 on epoch=157
05/30/2022 20:57:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.70 on epoch=159
05/30/2022 20:57:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.61 on epoch=162
05/30/2022 20:57:18 - INFO - __main__ - Global step 650 Train loss 0.70 Classification-F1 0.39297495876443245 on epoch=162
05/30/2022 20:57:18 - INFO - __main__ - Saving model with best Classification-F1: 0.3386191447053789 -> 0.39297495876443245 on epoch=162, global_step=650
05/30/2022 20:57:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.70 on epoch=164
05/30/2022 20:57:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.70 on epoch=167
05/30/2022 20:57:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.67 on epoch=169
05/30/2022 20:57:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.67 on epoch=172
05/30/2022 20:57:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.62 on epoch=174
05/30/2022 20:57:31 - INFO - __main__ - Global step 700 Train loss 0.67 Classification-F1 0.5841727126980245 on epoch=174
05/30/2022 20:57:31 - INFO - __main__ - Saving model with best Classification-F1: 0.39297495876443245 -> 0.5841727126980245 on epoch=174, global_step=700
05/30/2022 20:57:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.55 on epoch=177
05/30/2022 20:57:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.61 on epoch=179
05/30/2022 20:57:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.57 on epoch=182
05/30/2022 20:57:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.55 on epoch=184
05/30/2022 20:57:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.53 on epoch=187
05/30/2022 20:57:45 - INFO - __main__ - Global step 750 Train loss 0.56 Classification-F1 0.7193181818181819 on epoch=187
05/30/2022 20:57:45 - INFO - __main__ - Saving model with best Classification-F1: 0.5841727126980245 -> 0.7193181818181819 on epoch=187, global_step=750
05/30/2022 20:57:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.61 on epoch=189
05/30/2022 20:57:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.50 on epoch=192
05/30/2022 20:57:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.50 on epoch=194
05/30/2022 20:57:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.54 on epoch=197
05/30/2022 20:57:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=199
05/30/2022 20:57:58 - INFO - __main__ - Global step 800 Train loss 0.51 Classification-F1 0.5638841647770219 on epoch=199
05/30/2022 20:58:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=202
05/30/2022 20:58:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.55 on epoch=204
05/30/2022 20:58:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.47 on epoch=207
05/30/2022 20:58:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=209
05/30/2022 20:58:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.55 on epoch=212
05/30/2022 20:58:11 - INFO - __main__ - Global step 850 Train loss 0.48 Classification-F1 0.6630952380952382 on epoch=212
05/30/2022 20:58:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.47 on epoch=214
05/30/2022 20:58:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.34 on epoch=217
05/30/2022 20:58:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=219
05/30/2022 20:58:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=222
05/30/2022 20:58:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.33 on epoch=224
05/30/2022 20:58:24 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.701220238095238 on epoch=224
05/30/2022 20:58:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.29 on epoch=227
05/30/2022 20:58:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=229
05/30/2022 20:58:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=232
05/30/2022 20:58:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.27 on epoch=234
05/30/2022 20:58:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.34 on epoch=237
05/30/2022 20:58:37 - INFO - __main__ - Global step 950 Train loss 0.34 Classification-F1 0.7202380952380952 on epoch=237
05/30/2022 20:58:37 - INFO - __main__ - Saving model with best Classification-F1: 0.7193181818181819 -> 0.7202380952380952 on epoch=237, global_step=950
05/30/2022 20:58:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.36 on epoch=239
05/30/2022 20:58:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=242
05/30/2022 20:58:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.25 on epoch=244
05/30/2022 20:58:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.29 on epoch=247
05/30/2022 20:58:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.26 on epoch=249
05/30/2022 20:58:50 - INFO - __main__ - Global step 1000 Train loss 0.30 Classification-F1 0.5661268556005399 on epoch=249
05/30/2022 20:58:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=252
05/30/2022 20:58:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.32 on epoch=254
05/30/2022 20:58:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=257
05/30/2022 20:59:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.26 on epoch=259
05/30/2022 20:59:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.30 on epoch=262
05/30/2022 20:59:03 - INFO - __main__ - Global step 1050 Train loss 0.29 Classification-F1 0.7339755077658302 on epoch=262
05/30/2022 20:59:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7202380952380952 -> 0.7339755077658302 on epoch=262, global_step=1050
05/30/2022 20:59:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.28 on epoch=264
05/30/2022 20:59:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=267
05/30/2022 20:59:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.28 on epoch=269
05/30/2022 20:59:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=272
05/30/2022 20:59:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=274
05/30/2022 20:59:17 - INFO - __main__ - Global step 1100 Train loss 0.26 Classification-F1 0.7654233870967742 on epoch=274
05/30/2022 20:59:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7339755077658302 -> 0.7654233870967742 on epoch=274, global_step=1100
05/30/2022 20:59:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.23 on epoch=277
05/30/2022 20:59:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=279
05/30/2022 20:59:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=282
05/30/2022 20:59:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=284
05/30/2022 20:59:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.23 on epoch=287
05/30/2022 20:59:30 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.7972757254649415 on epoch=287
05/30/2022 20:59:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7654233870967742 -> 0.7972757254649415 on epoch=287, global_step=1150
05/30/2022 20:59:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=289
05/30/2022 20:59:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=292
05/30/2022 20:59:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.27 on epoch=294
05/30/2022 20:59:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.16 on epoch=297
05/30/2022 20:59:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.17 on epoch=299
05/30/2022 20:59:43 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.753225806451613 on epoch=299
05/30/2022 20:59:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=302
05/30/2022 20:59:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=304
05/30/2022 20:59:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=307
05/30/2022 20:59:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=309
05/30/2022 20:59:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=312
05/30/2022 20:59:56 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.7370967741935485 on epoch=312
05/30/2022 20:59:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.11 on epoch=314
05/30/2022 21:00:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=317
05/30/2022 21:00:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=319
05/30/2022 21:00:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.17 on epoch=322
05/30/2022 21:00:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=324
05/30/2022 21:00:09 - INFO - __main__ - Global step 1300 Train loss 0.16 Classification-F1 0.7218571512689159 on epoch=324
05/30/2022 21:00:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.17 on epoch=327
05/30/2022 21:00:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=329
05/30/2022 21:00:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=332
05/30/2022 21:00:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=334
05/30/2022 21:00:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.13 on epoch=337
05/30/2022 21:00:22 - INFO - __main__ - Global step 1350 Train loss 0.15 Classification-F1 0.7329121329121329 on epoch=337
05/30/2022 21:00:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.13 on epoch=339
05/30/2022 21:00:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=342
05/30/2022 21:00:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=344
05/30/2022 21:00:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.09 on epoch=347
05/30/2022 21:00:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=349
05/30/2022 21:00:35 - INFO - __main__ - Global step 1400 Train loss 0.12 Classification-F1 0.7682027649769585 on epoch=349
05/30/2022 21:00:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=352
05/30/2022 21:00:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=354
05/30/2022 21:00:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=357
05/30/2022 21:00:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=359
05/30/2022 21:00:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=362
05/30/2022 21:00:48 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.7302065052065052 on epoch=362
05/30/2022 21:00:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=364
05/30/2022 21:00:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=367
05/30/2022 21:00:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=369
05/30/2022 21:00:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=372
05/30/2022 21:01:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.21 on epoch=374
05/30/2022 21:01:01 - INFO - __main__ - Global step 1500 Train loss 0.12 Classification-F1 0.7182539682539683 on epoch=374
05/30/2022 21:01:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=377
05/30/2022 21:01:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=379
05/30/2022 21:01:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.08 on epoch=382
05/30/2022 21:01:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.09 on epoch=384
05/30/2022 21:01:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=387
05/30/2022 21:01:14 - INFO - __main__ - Global step 1550 Train loss 0.07 Classification-F1 0.6789820954907162 on epoch=387
05/30/2022 21:01:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.14 on epoch=389
05/30/2022 21:01:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=392
05/30/2022 21:01:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=394
05/30/2022 21:01:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=397
05/30/2022 21:01:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=399
05/30/2022 21:01:28 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.6993120393120393 on epoch=399
05/30/2022 21:01:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=402
05/30/2022 21:01:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
05/30/2022 21:01:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=407
05/30/2022 21:01:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=409
05/30/2022 21:01:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=412
05/30/2022 21:01:41 - INFO - __main__ - Global step 1650 Train loss 0.10 Classification-F1 0.7652462121212121 on epoch=412
05/30/2022 21:01:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
05/30/2022 21:01:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=417
05/30/2022 21:01:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=419
05/30/2022 21:01:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=422
05/30/2022 21:01:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=424
05/30/2022 21:01:55 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.7472232069006263 on epoch=424
05/30/2022 21:01:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
05/30/2022 21:02:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
05/30/2022 21:02:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=432
05/30/2022 21:02:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=434
05/30/2022 21:02:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.10 on epoch=437
05/30/2022 21:02:08 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.765686274509804 on epoch=437
05/30/2022 21:02:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=439
05/30/2022 21:02:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
05/30/2022 21:02:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=444
05/30/2022 21:02:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=447
05/30/2022 21:02:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=449
05/30/2022 21:02:22 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.749343487394958 on epoch=449
05/30/2022 21:02:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=452
05/30/2022 21:02:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/30/2022 21:02:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=457
05/30/2022 21:02:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/30/2022 21:02:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=462
05/30/2022 21:02:35 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.7659944581280789 on epoch=462
05/30/2022 21:02:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
05/30/2022 21:02:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=467
05/30/2022 21:02:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=469
05/30/2022 21:02:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=472
05/30/2022 21:02:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=474
05/30/2022 21:02:48 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.7500946969696971 on epoch=474
05/30/2022 21:02:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=477
05/30/2022 21:02:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/30/2022 21:02:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=482
05/30/2022 21:02:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=484
05/30/2022 21:03:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
05/30/2022 21:03:01 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.778329938507914 on epoch=487
05/30/2022 21:03:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=489
05/30/2022 21:03:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=492
05/30/2022 21:03:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
05/30/2022 21:03:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=497
05/30/2022 21:03:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=499
05/30/2022 21:03:15 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.7804807071731513 on epoch=499
05/30/2022 21:03:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
05/30/2022 21:03:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
05/30/2022 21:03:22 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=507
05/30/2022 21:03:24 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/30/2022 21:03:27 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
05/30/2022 21:03:28 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.7344673645320197 on epoch=512
05/30/2022 21:03:30 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=514
05/30/2022 21:03:33 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=517
05/30/2022 21:03:35 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=519
05/30/2022 21:03:38 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/30/2022 21:03:40 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
05/30/2022 21:03:41 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.7515756302521008 on epoch=524
05/30/2022 21:03:43 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
05/30/2022 21:03:46 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/30/2022 21:03:48 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/30/2022 21:03:51 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=534
05/30/2022 21:03:53 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
05/30/2022 21:03:54 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.7364529682840877 on epoch=537
05/30/2022 21:03:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=539
05/30/2022 21:03:59 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.11 on epoch=542
05/30/2022 21:04:02 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/30/2022 21:04:04 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=547
05/30/2022 21:04:06 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
05/30/2022 21:04:07 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.7491557734204792 on epoch=549
05/30/2022 21:04:10 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/30/2022 21:04:12 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.08 on epoch=554
05/30/2022 21:04:15 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.07 on epoch=557
05/30/2022 21:04:17 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=559
05/30/2022 21:04:20 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=562
05/30/2022 21:04:21 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.797377954114197 on epoch=562
05/30/2022 21:04:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7972757254649415 -> 0.797377954114197 on epoch=562, global_step=2250
05/30/2022 21:04:23 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
05/30/2022 21:04:26 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/30/2022 21:04:28 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
05/30/2022 21:04:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=572
05/30/2022 21:04:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/30/2022 21:04:34 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.7977456011730205 on epoch=574
05/30/2022 21:04:34 - INFO - __main__ - Saving model with best Classification-F1: 0.797377954114197 -> 0.7977456011730205 on epoch=574, global_step=2300
05/30/2022 21:04:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=577
05/30/2022 21:04:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
05/30/2022 21:04:41 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.09 on epoch=582
05/30/2022 21:04:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
05/30/2022 21:04:46 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/30/2022 21:04:47 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.7633228840125392 on epoch=587
05/30/2022 21:04:50 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/30/2022 21:04:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/30/2022 21:04:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=594
05/30/2022 21:04:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/30/2022 21:05:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/30/2022 21:05:01 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7495230039550118 on epoch=599
05/30/2022 21:05:03 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
05/30/2022 21:05:06 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
05/30/2022 21:05:08 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=607
05/30/2022 21:05:11 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=609
05/30/2022 21:05:13 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/30/2022 21:05:14 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.7658846529814272 on epoch=612
05/30/2022 21:05:16 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=614
05/30/2022 21:05:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/30/2022 21:05:21 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/30/2022 21:05:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/30/2022 21:05:26 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=624
05/30/2022 21:05:27 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.7508596400437008 on epoch=624
05/30/2022 21:05:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/30/2022 21:05:32 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/30/2022 21:05:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=632
05/30/2022 21:05:37 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
05/30/2022 21:05:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
05/30/2022 21:05:41 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.7353942652329749 on epoch=637
05/30/2022 21:05:43 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/30/2022 21:05:46 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/30/2022 21:05:48 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/30/2022 21:05:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/30/2022 21:05:53 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
05/30/2022 21:05:54 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7168521462639109 on epoch=649
05/30/2022 21:05:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/30/2022 21:05:59 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/30/2022 21:06:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/30/2022 21:06:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/30/2022 21:06:06 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
05/30/2022 21:06:07 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7669701213818861 on epoch=662
05/30/2022 21:06:10 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=664
05/30/2022 21:06:12 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
05/30/2022 21:06:15 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
05/30/2022 21:06:17 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/30/2022 21:06:20 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/30/2022 21:06:21 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7517316017316018 on epoch=674
05/30/2022 21:06:23 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/30/2022 21:06:26 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/30/2022 21:06:28 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/30/2022 21:06:30 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/30/2022 21:06:33 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/30/2022 21:06:34 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.733230926779314 on epoch=687
05/30/2022 21:06:36 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/30/2022 21:06:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=692
05/30/2022 21:06:41 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/30/2022 21:06:44 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/30/2022 21:06:46 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/30/2022 21:06:47 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7495098039215686 on epoch=699
05/30/2022 21:06:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/30/2022 21:06:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.08 on epoch=704
05/30/2022 21:06:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=707
05/30/2022 21:06:57 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=709
05/30/2022 21:07:00 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/30/2022 21:07:01 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.7370236019429567 on epoch=712
05/30/2022 21:07:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/30/2022 21:07:06 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
05/30/2022 21:07:08 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/30/2022 21:07:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/30/2022 21:07:13 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/30/2022 21:07:14 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7346376050420169 on epoch=724
05/30/2022 21:07:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=727
05/30/2022 21:07:19 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/30/2022 21:07:21 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/30/2022 21:07:24 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/30/2022 21:07:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/30/2022 21:07:27 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.7481713688610241 on epoch=737
05/30/2022 21:07:29 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
05/30/2022 21:07:32 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/30/2022 21:07:34 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/30/2022 21:07:37 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/30/2022 21:07:39 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=749
05/30/2022 21:07:40 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7659610215053763 on epoch=749
05/30/2022 21:07:40 - INFO - __main__ - save last model!
05/30/2022 21:07:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 21:07:40 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 21:07:40 - INFO - __main__ - Printing 3 examples
05/30/2022 21:07:40 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 21:07:40 - INFO - __main__ - ['others']
05/30/2022 21:07:40 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 21:07:40 - INFO - __main__ - ['others']
05/30/2022 21:07:40 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 21:07:40 - INFO - __main__ - ['others']
05/30/2022 21:07:40 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:07:40 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 21:07:40 - INFO - __main__ - Printing 3 examples
05/30/2022 21:07:40 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/30/2022 21:07:40 - INFO - __main__ - ['others']
05/30/2022 21:07:40 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/30/2022 21:07:40 - INFO - __main__ - ['others']
05/30/2022 21:07:40 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/30/2022 21:07:40 - INFO - __main__ - ['others']
05/30/2022 21:07:40 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:07:40 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:07:40 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 21:07:40 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 21:07:40 - INFO - __main__ - Printing 3 examples
05/30/2022 21:07:40 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/30/2022 21:07:40 - INFO - __main__ - ['others']
05/30/2022 21:07:40 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/30/2022 21:07:40 - INFO - __main__ - ['others']
05/30/2022 21:07:40 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/30/2022 21:07:40 - INFO - __main__ - ['others']
05/30/2022 21:07:40 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:07:40 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:07:40 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 21:07:42 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:07:47 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 21:07:55 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 21:07:55 - INFO - __main__ - task name: emo
05/30/2022 21:07:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 21:07:56 - INFO - __main__ - Starting training!
05/30/2022 21:09:04 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_13_0.3_8_predictions.txt
05/30/2022 21:09:04 - INFO - __main__ - Classification-F1 on test data: 0.3830
05/30/2022 21:09:04 - INFO - __main__ - prefix=emo_16_13, lr=0.3, bsz=8, dev_performance=0.7977456011730205, test_performance=0.3830365243290102
05/30/2022 21:09:04 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.2, bsz=8 ...
05/30/2022 21:09:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 21:09:05 - INFO - __main__ - Printing 3 examples
05/30/2022 21:09:05 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/30/2022 21:09:05 - INFO - __main__ - ['others']
05/30/2022 21:09:05 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/30/2022 21:09:05 - INFO - __main__ - ['others']
05/30/2022 21:09:05 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/30/2022 21:09:05 - INFO - __main__ - ['others']
05/30/2022 21:09:05 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:09:05 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:09:05 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 21:09:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 21:09:05 - INFO - __main__ - Printing 3 examples
05/30/2022 21:09:05 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/30/2022 21:09:05 - INFO - __main__ - ['others']
05/30/2022 21:09:05 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/30/2022 21:09:05 - INFO - __main__ - ['others']
05/30/2022 21:09:05 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/30/2022 21:09:05 - INFO - __main__ - ['others']
05/30/2022 21:09:05 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:09:05 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:09:05 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 21:09:24 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 21:09:24 - INFO - __main__ - task name: emo
05/30/2022 21:09:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 21:09:25 - INFO - __main__ - Starting training!
05/30/2022 21:09:28 - INFO - __main__ - Step 10 Global step 10 Train loss 8.14 on epoch=2
05/30/2022 21:09:30 - INFO - __main__ - Step 20 Global step 20 Train loss 6.81 on epoch=4
05/30/2022 21:09:33 - INFO - __main__ - Step 30 Global step 30 Train loss 4.86 on epoch=7
05/30/2022 21:09:35 - INFO - __main__ - Step 40 Global step 40 Train loss 3.17 on epoch=9
05/30/2022 21:09:38 - INFO - __main__ - Step 50 Global step 50 Train loss 2.22 on epoch=12
05/30/2022 21:09:39 - INFO - __main__ - Global step 50 Train loss 5.04 Classification-F1 0.15512605042016808 on epoch=12
05/30/2022 21:09:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.15512605042016808 on epoch=12, global_step=50
05/30/2022 21:09:41 - INFO - __main__ - Step 60 Global step 60 Train loss 1.66 on epoch=14
05/30/2022 21:09:44 - INFO - __main__ - Step 70 Global step 70 Train loss 1.39 on epoch=17
05/30/2022 21:09:46 - INFO - __main__ - Step 80 Global step 80 Train loss 1.22 on epoch=19
05/30/2022 21:09:49 - INFO - __main__ - Step 90 Global step 90 Train loss 1.08 on epoch=22
05/30/2022 21:09:51 - INFO - __main__ - Step 100 Global step 100 Train loss 1.06 on epoch=24
05/30/2022 21:09:52 - INFO - __main__ - Global step 100 Train loss 1.28 Classification-F1 0.25787794661991037 on epoch=24
05/30/2022 21:09:52 - INFO - __main__ - Saving model with best Classification-F1: 0.15512605042016808 -> 0.25787794661991037 on epoch=24, global_step=100
05/30/2022 21:09:54 - INFO - __main__ - Step 110 Global step 110 Train loss 1.03 on epoch=27
05/30/2022 21:09:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.92 on epoch=29
05/30/2022 21:09:59 - INFO - __main__ - Step 130 Global step 130 Train loss 1.08 on epoch=32
05/30/2022 21:10:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.90 on epoch=34
05/30/2022 21:10:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.93 on epoch=37
05/30/2022 21:10:05 - INFO - __main__ - Global step 150 Train loss 0.97 Classification-F1 0.18406593406593408 on epoch=37
05/30/2022 21:10:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.97 on epoch=39
05/30/2022 21:10:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.97 on epoch=42
05/30/2022 21:10:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.89 on epoch=44
05/30/2022 21:10:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.93 on epoch=47
05/30/2022 21:10:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.79 on epoch=49
05/30/2022 21:10:18 - INFO - __main__ - Global step 200 Train loss 0.91 Classification-F1 0.16515426497277674 on epoch=49
05/30/2022 21:10:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.94 on epoch=52
05/30/2022 21:10:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.81 on epoch=54
05/30/2022 21:10:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.93 on epoch=57
05/30/2022 21:10:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.85 on epoch=59
05/30/2022 21:10:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.86 on epoch=62
05/30/2022 21:10:31 - INFO - __main__ - Global step 250 Train loss 0.88 Classification-F1 0.1 on epoch=62
05/30/2022 21:10:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.86 on epoch=64
05/30/2022 21:10:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.78 on epoch=67
05/30/2022 21:10:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.89 on epoch=69
05/30/2022 21:10:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.88 on epoch=72
05/30/2022 21:10:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.78 on epoch=74
05/30/2022 21:10:43 - INFO - __main__ - Global step 300 Train loss 0.84 Classification-F1 0.1 on epoch=74
05/30/2022 21:10:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.89 on epoch=77
05/30/2022 21:10:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.89 on epoch=79
05/30/2022 21:10:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.92 on epoch=82
05/30/2022 21:10:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.87 on epoch=84
05/30/2022 21:10:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.81 on epoch=87
05/30/2022 21:10:56 - INFO - __main__ - Global step 350 Train loss 0.88 Classification-F1 0.1 on epoch=87
05/30/2022 21:10:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.89 on epoch=89
05/30/2022 21:11:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.93 on epoch=92
05/30/2022 21:11:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.86 on epoch=94
05/30/2022 21:11:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.87 on epoch=97
05/30/2022 21:11:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.82 on epoch=99
05/30/2022 21:11:09 - INFO - __main__ - Global step 400 Train loss 0.87 Classification-F1 0.26533433886375063 on epoch=99
05/30/2022 21:11:09 - INFO - __main__ - Saving model with best Classification-F1: 0.25787794661991037 -> 0.26533433886375063 on epoch=99, global_step=400
05/30/2022 21:11:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.83 on epoch=102
05/30/2022 21:11:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.82 on epoch=104
05/30/2022 21:11:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.83 on epoch=107
05/30/2022 21:11:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.79 on epoch=109
05/30/2022 21:11:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.74 on epoch=112
05/30/2022 21:11:22 - INFO - __main__ - Global step 450 Train loss 0.80 Classification-F1 0.09615384615384615 on epoch=112
05/30/2022 21:11:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.86 on epoch=114
05/30/2022 21:11:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.78 on epoch=117
05/30/2022 21:11:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.87 on epoch=119
05/30/2022 21:11:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.88 on epoch=122
05/30/2022 21:11:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.80 on epoch=124
05/30/2022 21:11:35 - INFO - __main__ - Global step 500 Train loss 0.84 Classification-F1 0.43077436025673077 on epoch=124
05/30/2022 21:11:35 - INFO - __main__ - Saving model with best Classification-F1: 0.26533433886375063 -> 0.43077436025673077 on epoch=124, global_step=500
05/30/2022 21:11:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.79 on epoch=127
05/30/2022 21:11:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.81 on epoch=129
05/30/2022 21:11:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.85 on epoch=132
05/30/2022 21:11:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.87 on epoch=134
05/30/2022 21:11:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.78 on epoch=137
05/30/2022 21:11:48 - INFO - __main__ - Global step 550 Train loss 0.82 Classification-F1 0.1 on epoch=137
05/30/2022 21:11:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.76 on epoch=139
05/30/2022 21:11:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.80 on epoch=142
05/30/2022 21:11:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.79 on epoch=144
05/30/2022 21:11:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.81 on epoch=147
05/30/2022 21:12:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.72 on epoch=149
05/30/2022 21:12:01 - INFO - __main__ - Global step 600 Train loss 0.78 Classification-F1 0.4210038986354776 on epoch=149
05/30/2022 21:12:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.79 on epoch=152
05/30/2022 21:12:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.70 on epoch=154
05/30/2022 21:12:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.74 on epoch=157
05/30/2022 21:12:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.76 on epoch=159
05/30/2022 21:12:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.73 on epoch=162
05/30/2022 21:12:14 - INFO - __main__ - Global step 650 Train loss 0.75 Classification-F1 0.3177109440267335 on epoch=162
05/30/2022 21:12:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.77 on epoch=164
05/30/2022 21:12:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.63 on epoch=167
05/30/2022 21:12:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.68 on epoch=169
05/30/2022 21:12:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.84 on epoch=172
05/30/2022 21:12:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.65 on epoch=174
05/30/2022 21:12:27 - INFO - __main__ - Global step 700 Train loss 0.71 Classification-F1 0.5624326653738418 on epoch=174
05/30/2022 21:12:27 - INFO - __main__ - Saving model with best Classification-F1: 0.43077436025673077 -> 0.5624326653738418 on epoch=174, global_step=700
05/30/2022 21:12:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.63 on epoch=177
05/30/2022 21:12:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.65 on epoch=179
05/30/2022 21:12:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.61 on epoch=182
05/30/2022 21:12:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.69 on epoch=184
05/30/2022 21:12:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.73 on epoch=187
05/30/2022 21:12:40 - INFO - __main__ - Global step 750 Train loss 0.66 Classification-F1 0.5834757834757835 on epoch=187
05/30/2022 21:12:40 - INFO - __main__ - Saving model with best Classification-F1: 0.5624326653738418 -> 0.5834757834757835 on epoch=187, global_step=750
05/30/2022 21:12:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.73 on epoch=189
05/30/2022 21:12:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.60 on epoch=192
05/30/2022 21:12:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.60 on epoch=194
05/30/2022 21:12:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.59 on epoch=197
05/30/2022 21:12:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.63 on epoch=199
05/30/2022 21:12:53 - INFO - __main__ - Global step 800 Train loss 0.63 Classification-F1 0.7223473473473473 on epoch=199
05/30/2022 21:12:53 - INFO - __main__ - Saving model with best Classification-F1: 0.5834757834757835 -> 0.7223473473473473 on epoch=199, global_step=800
05/30/2022 21:12:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.62 on epoch=202
05/30/2022 21:12:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.67 on epoch=204
05/30/2022 21:13:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.65 on epoch=207
05/30/2022 21:13:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.67 on epoch=209
05/30/2022 21:13:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.61 on epoch=212
05/30/2022 21:13:06 - INFO - __main__ - Global step 850 Train loss 0.64 Classification-F1 0.6722559050145257 on epoch=212
05/30/2022 21:13:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.54 on epoch=214
05/30/2022 21:13:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.57 on epoch=217
05/30/2022 21:13:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.64 on epoch=219
05/30/2022 21:13:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.52 on epoch=222
05/30/2022 21:13:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.54 on epoch=224
05/30/2022 21:13:19 - INFO - __main__ - Global step 900 Train loss 0.56 Classification-F1 0.7414200824642252 on epoch=224
05/30/2022 21:13:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7223473473473473 -> 0.7414200824642252 on epoch=224, global_step=900
05/30/2022 21:13:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=227
05/30/2022 21:13:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.57 on epoch=229
05/30/2022 21:13:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.54 on epoch=232
05/30/2022 21:13:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.53 on epoch=234
05/30/2022 21:13:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.55 on epoch=237
05/30/2022 21:13:32 - INFO - __main__ - Global step 950 Train loss 0.53 Classification-F1 0.7153965034289578 on epoch=237
05/30/2022 21:13:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.48 on epoch=239
05/30/2022 21:13:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.52 on epoch=242
05/30/2022 21:13:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.48 on epoch=244
05/30/2022 21:13:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.47 on epoch=247
05/30/2022 21:13:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.61 on epoch=249
05/30/2022 21:13:45 - INFO - __main__ - Global step 1000 Train loss 0.51 Classification-F1 0.6752329749103942 on epoch=249
05/30/2022 21:13:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.49 on epoch=252
05/30/2022 21:13:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=254
05/30/2022 21:13:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=257
05/30/2022 21:13:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.47 on epoch=259
05/30/2022 21:13:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.44 on epoch=262
05/30/2022 21:13:58 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.683102146263911 on epoch=262
05/30/2022 21:14:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=264
05/30/2022 21:14:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=267
05/30/2022 21:14:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=269
05/30/2022 21:14:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=272
05/30/2022 21:14:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=274
05/30/2022 21:14:11 - INFO - __main__ - Global step 1100 Train loss 0.43 Classification-F1 0.7092775041050903 on epoch=274
05/30/2022 21:14:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.44 on epoch=277
05/30/2022 21:14:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=279
05/30/2022 21:14:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.36 on epoch=282
05/30/2022 21:14:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.34 on epoch=284
05/30/2022 21:14:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=287
05/30/2022 21:14:24 - INFO - __main__ - Global step 1150 Train loss 0.39 Classification-F1 0.719291101055807 on epoch=287
05/30/2022 21:14:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=289
05/30/2022 21:14:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.41 on epoch=292
05/30/2022 21:14:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=294
05/30/2022 21:14:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.29 on epoch=297
05/30/2022 21:14:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.34 on epoch=299
05/30/2022 21:14:37 - INFO - __main__ - Global step 1200 Train loss 0.35 Classification-F1 0.7341880341880341 on epoch=299
05/30/2022 21:14:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.42 on epoch=302
05/30/2022 21:14:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.30 on epoch=304
05/30/2022 21:14:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=307
05/30/2022 21:14:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.29 on epoch=309
05/30/2022 21:14:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=312
05/30/2022 21:14:50 - INFO - __main__ - Global step 1250 Train loss 0.34 Classification-F1 0.6889610389610389 on epoch=312
05/30/2022 21:14:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=314
05/30/2022 21:14:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=317
05/30/2022 21:14:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=319
05/30/2022 21:15:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=322
05/30/2022 21:15:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=324
05/30/2022 21:15:03 - INFO - __main__ - Global step 1300 Train loss 0.31 Classification-F1 0.7178030303030303 on epoch=324
05/30/2022 21:15:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.32 on epoch=327
05/30/2022 21:15:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.23 on epoch=329
05/30/2022 21:15:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.25 on epoch=332
05/30/2022 21:15:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.26 on epoch=334
05/30/2022 21:15:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.22 on epoch=337
05/30/2022 21:15:16 - INFO - __main__ - Global step 1350 Train loss 0.25 Classification-F1 0.6886712749615975 on epoch=337
05/30/2022 21:15:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.27 on epoch=339
05/30/2022 21:15:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.25 on epoch=342
05/30/2022 21:15:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=344
05/30/2022 21:15:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.20 on epoch=347
05/30/2022 21:15:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=349
05/30/2022 21:15:29 - INFO - __main__ - Global step 1400 Train loss 0.23 Classification-F1 0.6509294626941686 on epoch=349
05/30/2022 21:15:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=352
05/30/2022 21:15:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=354
05/30/2022 21:15:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.22 on epoch=357
05/30/2022 21:15:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=359
05/30/2022 21:15:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.16 on epoch=362
05/30/2022 21:15:41 - INFO - __main__ - Global step 1450 Train loss 0.17 Classification-F1 0.739958184785771 on epoch=362
05/30/2022 21:15:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=364
05/30/2022 21:15:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=367
05/30/2022 21:15:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.27 on epoch=369
05/30/2022 21:15:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.16 on epoch=372
05/30/2022 21:15:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.15 on epoch=374
05/30/2022 21:15:54 - INFO - __main__ - Global step 1500 Train loss 0.17 Classification-F1 0.6265147969001925 on epoch=374
05/30/2022 21:15:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.15 on epoch=377
05/30/2022 21:15:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=379
05/30/2022 21:16:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=382
05/30/2022 21:16:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=384
05/30/2022 21:16:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=387
05/30/2022 21:16:07 - INFO - __main__ - Global step 1550 Train loss 0.14 Classification-F1 0.6995979938723338 on epoch=387
05/30/2022 21:16:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=389
05/30/2022 21:16:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=392
05/30/2022 21:16:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=394
05/30/2022 21:16:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.26 on epoch=397
05/30/2022 21:16:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=399
05/30/2022 21:16:20 - INFO - __main__ - Global step 1600 Train loss 0.17 Classification-F1 0.6684246458440006 on epoch=399
05/30/2022 21:16:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.14 on epoch=402
05/30/2022 21:16:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=404
05/30/2022 21:16:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=407
05/30/2022 21:16:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.15 on epoch=409
05/30/2022 21:16:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.14 on epoch=412
05/30/2022 21:16:33 - INFO - __main__ - Global step 1650 Train loss 0.12 Classification-F1 0.6692335677904439 on epoch=412
05/30/2022 21:16:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=414
05/30/2022 21:16:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.11 on epoch=417
05/30/2022 21:16:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=419
05/30/2022 21:16:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=422
05/30/2022 21:16:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=424
05/30/2022 21:16:46 - INFO - __main__ - Global step 1700 Train loss 0.11 Classification-F1 0.6863308541807528 on epoch=424
05/30/2022 21:16:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=427
05/30/2022 21:16:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=429
05/30/2022 21:16:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.10 on epoch=432
05/30/2022 21:16:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=434
05/30/2022 21:16:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=437
05/30/2022 21:16:59 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.6245845805736913 on epoch=437
05/30/2022 21:17:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=439
05/30/2022 21:17:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=442
05/30/2022 21:17:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=444
05/30/2022 21:17:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=447
05/30/2022 21:17:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=449
05/30/2022 21:17:13 - INFO - __main__ - Global step 1800 Train loss 0.09 Classification-F1 0.6431275958659326 on epoch=449
05/30/2022 21:17:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.17 on epoch=452
05/30/2022 21:17:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=454
05/30/2022 21:17:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=457
05/30/2022 21:17:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=459
05/30/2022 21:17:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=462
05/30/2022 21:17:26 - INFO - __main__ - Global step 1850 Train loss 0.10 Classification-F1 0.629182754182754 on epoch=462
05/30/2022 21:17:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=464
05/30/2022 21:17:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=467
05/30/2022 21:17:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=469
05/30/2022 21:17:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=472
05/30/2022 21:17:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=474
05/30/2022 21:17:39 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.6375095492742552 on epoch=474
05/30/2022 21:17:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=477
05/30/2022 21:17:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=479
05/30/2022 21:17:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=482
05/30/2022 21:17:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.10 on epoch=484
05/30/2022 21:17:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=487
05/30/2022 21:17:52 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.6491185897435896 on epoch=487
05/30/2022 21:17:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=489
05/30/2022 21:17:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.11 on epoch=492
05/30/2022 21:17:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
05/30/2022 21:18:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=497
05/30/2022 21:18:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=499
05/30/2022 21:18:05 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.7336830712804017 on epoch=499
05/30/2022 21:18:07 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=502
05/30/2022 21:18:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=504
05/30/2022 21:18:12 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=507
05/30/2022 21:18:14 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=509
05/30/2022 21:18:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=512
05/30/2022 21:18:18 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.6447368421052632 on epoch=512
05/30/2022 21:18:20 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
05/30/2022 21:18:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=517
05/30/2022 21:18:25 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=519
05/30/2022 21:18:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=522
05/30/2022 21:18:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
05/30/2022 21:18:31 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.6773842808325568 on epoch=524
05/30/2022 21:18:33 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=527
05/30/2022 21:18:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.09 on epoch=529
05/30/2022 21:18:38 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=532
05/30/2022 21:18:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=534
05/30/2022 21:18:43 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
05/30/2022 21:18:44 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.6543355758725777 on epoch=537
05/30/2022 21:18:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=539
05/30/2022 21:18:49 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=542
05/30/2022 21:18:51 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/30/2022 21:18:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
05/30/2022 21:18:56 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.09 on epoch=549
05/30/2022 21:18:57 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.6411044951907349 on epoch=549
05/30/2022 21:19:00 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/30/2022 21:19:02 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
05/30/2022 21:19:04 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.11 on epoch=557
05/30/2022 21:19:07 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=559
05/30/2022 21:19:09 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
05/30/2022 21:19:10 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.6535539215686275 on epoch=562
05/30/2022 21:19:13 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.10 on epoch=564
05/30/2022 21:19:15 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
05/30/2022 21:19:17 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=569
05/30/2022 21:19:20 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=572
05/30/2022 21:19:22 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
05/30/2022 21:19:23 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.6891891891891893 on epoch=574
05/30/2022 21:19:26 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=577
05/30/2022 21:19:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=579
05/30/2022 21:19:30 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/30/2022 21:19:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=584
05/30/2022 21:19:35 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
05/30/2022 21:19:36 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.6425282569435795 on epoch=587
05/30/2022 21:19:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
05/30/2022 21:19:41 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=592
05/30/2022 21:19:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=594
05/30/2022 21:19:46 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=597
05/30/2022 21:19:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
05/30/2022 21:19:49 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.7067186644772853 on epoch=599
05/30/2022 21:19:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/30/2022 21:19:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=604
05/30/2022 21:19:56 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=607
05/30/2022 21:19:59 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=609
05/30/2022 21:20:01 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
05/30/2022 21:20:02 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.6894772851669404 on epoch=612
05/30/2022 21:20:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
05/30/2022 21:20:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=617
05/30/2022 21:20:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=619
05/30/2022 21:20:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=622
05/30/2022 21:20:14 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
05/30/2022 21:20:15 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.6734433941330493 on epoch=624
05/30/2022 21:20:18 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=627
05/30/2022 21:20:20 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.08 on epoch=629
05/30/2022 21:20:22 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=632
05/30/2022 21:20:25 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
05/30/2022 21:20:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=637
05/30/2022 21:20:28 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.747906624644667 on epoch=637
05/30/2022 21:20:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7414200824642252 -> 0.747906624644667 on epoch=637, global_step=2550
05/30/2022 21:20:31 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
05/30/2022 21:20:33 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=642
05/30/2022 21:20:36 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/30/2022 21:20:38 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.10 on epoch=647
05/30/2022 21:20:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/30/2022 21:20:41 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.6995429139825692 on epoch=649
05/30/2022 21:20:44 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.11 on epoch=652
05/30/2022 21:20:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=654
05/30/2022 21:20:49 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
05/30/2022 21:20:51 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=659
05/30/2022 21:20:54 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
05/30/2022 21:20:55 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.6662907268170427 on epoch=662
05/30/2022 21:20:57 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=664
05/30/2022 21:20:59 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=667
05/30/2022 21:21:02 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/30/2022 21:21:04 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=672
05/30/2022 21:21:07 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=674
05/30/2022 21:21:08 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.6564171122994652 on epoch=674
05/30/2022 21:21:10 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.07 on epoch=677
05/30/2022 21:21:12 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=679
05/30/2022 21:21:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=682
05/30/2022 21:21:17 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=684
05/30/2022 21:21:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/30/2022 21:21:21 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.6429648280561059 on epoch=687
05/30/2022 21:21:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
05/30/2022 21:21:25 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=692
05/30/2022 21:21:28 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=694
05/30/2022 21:21:30 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=697
05/30/2022 21:21:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/30/2022 21:21:34 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6727201656524678 on epoch=699
05/30/2022 21:21:36 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
05/30/2022 21:21:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/30/2022 21:21:41 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=707
05/30/2022 21:21:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.08 on epoch=709
05/30/2022 21:21:46 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/30/2022 21:21:47 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.654965211891208 on epoch=712
05/30/2022 21:21:49 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/30/2022 21:21:52 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/30/2022 21:21:54 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/30/2022 21:21:57 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=722
05/30/2022 21:21:59 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/30/2022 21:22:00 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7025122549019608 on epoch=724
05/30/2022 21:22:03 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/30/2022 21:22:05 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/30/2022 21:22:07 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/30/2022 21:22:10 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=734
05/30/2022 21:22:12 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=737
05/30/2022 21:22:13 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.6845046082949309 on epoch=737
05/30/2022 21:22:16 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/30/2022 21:22:18 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/30/2022 21:22:21 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
05/30/2022 21:22:23 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/30/2022 21:22:25 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=749
05/30/2022 21:22:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 21:22:27 - INFO - __main__ - Printing 3 examples
05/30/2022 21:22:27 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/30/2022 21:22:27 - INFO - __main__ - ['sad']
05/30/2022 21:22:27 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/30/2022 21:22:27 - INFO - __main__ - ['sad']
05/30/2022 21:22:27 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/30/2022 21:22:27 - INFO - __main__ - ['sad']
05/30/2022 21:22:27 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:22:27 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:22:27 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.6742002890899951 on epoch=749
05/30/2022 21:22:27 - INFO - __main__ - save last model!
05/30/2022 21:22:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 21:22:27 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 21:22:27 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 21:22:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 21:22:27 - INFO - __main__ - Printing 3 examples
05/30/2022 21:22:27 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/30/2022 21:22:27 - INFO - __main__ - ['sad']
05/30/2022 21:22:27 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/30/2022 21:22:27 - INFO - __main__ - ['sad']
05/30/2022 21:22:27 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/30/2022 21:22:27 - INFO - __main__ - ['sad']
05/30/2022 21:22:27 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:22:27 - INFO - __main__ - Printing 3 examples
05/30/2022 21:22:27 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 21:22:27 - INFO - __main__ - ['others']
05/30/2022 21:22:27 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 21:22:27 - INFO - __main__ - ['others']
05/30/2022 21:22:27 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 21:22:27 - INFO - __main__ - ['others']
05/30/2022 21:22:27 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:22:27 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:22:27 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 21:22:29 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:22:34 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 21:22:42 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 21:22:42 - INFO - __main__ - task name: emo
05/30/2022 21:22:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 21:22:43 - INFO - __main__ - Starting training!
05/30/2022 21:24:06 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_13_0.2_8_predictions.txt
05/30/2022 21:24:06 - INFO - __main__ - Classification-F1 on test data: 0.3024
05/30/2022 21:24:06 - INFO - __main__ - prefix=emo_16_13, lr=0.2, bsz=8, dev_performance=0.747906624644667, test_performance=0.3024435910668613
05/30/2022 21:24:06 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.5, bsz=8 ...
05/30/2022 21:24:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 21:24:07 - INFO - __main__ - Printing 3 examples
05/30/2022 21:24:07 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/30/2022 21:24:07 - INFO - __main__ - ['sad']
05/30/2022 21:24:07 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/30/2022 21:24:07 - INFO - __main__ - ['sad']
05/30/2022 21:24:07 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/30/2022 21:24:07 - INFO - __main__ - ['sad']
05/30/2022 21:24:07 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:24:07 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:24:07 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 21:24:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 21:24:07 - INFO - __main__ - Printing 3 examples
05/30/2022 21:24:07 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/30/2022 21:24:07 - INFO - __main__ - ['sad']
05/30/2022 21:24:07 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/30/2022 21:24:07 - INFO - __main__ - ['sad']
05/30/2022 21:24:07 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/30/2022 21:24:07 - INFO - __main__ - ['sad']
05/30/2022 21:24:07 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:24:07 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:24:08 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 21:24:23 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 21:24:23 - INFO - __main__ - task name: emo
05/30/2022 21:24:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 21:24:23 - INFO - __main__ - Starting training!
05/30/2022 21:24:26 - INFO - __main__ - Step 10 Global step 10 Train loss 6.88 on epoch=2
05/30/2022 21:24:29 - INFO - __main__ - Step 20 Global step 20 Train loss 3.22 on epoch=4
05/30/2022 21:24:31 - INFO - __main__ - Step 30 Global step 30 Train loss 1.91 on epoch=7
05/30/2022 21:24:33 - INFO - __main__ - Step 40 Global step 40 Train loss 1.25 on epoch=9
05/30/2022 21:24:36 - INFO - __main__ - Step 50 Global step 50 Train loss 1.22 on epoch=12
05/30/2022 21:24:36 - INFO - __main__ - Global step 50 Train loss 2.89 Classification-F1 0.1 on epoch=12
05/30/2022 21:24:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/30/2022 21:24:39 - INFO - __main__ - Step 60 Global step 60 Train loss 1.06 on epoch=14
05/30/2022 21:24:41 - INFO - __main__ - Step 70 Global step 70 Train loss 1.09 on epoch=17
05/30/2022 21:24:44 - INFO - __main__ - Step 80 Global step 80 Train loss 1.21 on epoch=19
05/30/2022 21:24:46 - INFO - __main__ - Step 90 Global step 90 Train loss 1.02 on epoch=22
05/30/2022 21:24:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.90 on epoch=24
05/30/2022 21:24:49 - INFO - __main__ - Global step 100 Train loss 1.05 Classification-F1 0.21153846153846154 on epoch=24
05/30/2022 21:24:49 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.21153846153846154 on epoch=24, global_step=100
05/30/2022 21:24:51 - INFO - __main__ - Step 110 Global step 110 Train loss 1.04 on epoch=27
05/30/2022 21:24:54 - INFO - __main__ - Step 120 Global step 120 Train loss 1.00 on epoch=29
05/30/2022 21:24:56 - INFO - __main__ - Step 130 Global step 130 Train loss 1.11 on epoch=32
05/30/2022 21:24:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.99 on epoch=34
05/30/2022 21:25:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.96 on epoch=37
05/30/2022 21:25:02 - INFO - __main__ - Global step 150 Train loss 1.02 Classification-F1 0.1 on epoch=37
05/30/2022 21:25:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.93 on epoch=39
05/30/2022 21:25:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.93 on epoch=42
05/30/2022 21:25:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.89 on epoch=44
05/30/2022 21:25:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.99 on epoch=47
05/30/2022 21:25:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.90 on epoch=49
05/30/2022 21:25:14 - INFO - __main__ - Global step 200 Train loss 0.93 Classification-F1 0.17142857142857143 on epoch=49
05/30/2022 21:25:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.91 on epoch=52
05/30/2022 21:25:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.87 on epoch=54
05/30/2022 21:25:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.86 on epoch=57
05/30/2022 21:25:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.95 on epoch=59
05/30/2022 21:25:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.87 on epoch=62
05/30/2022 21:25:27 - INFO - __main__ - Global step 250 Train loss 0.89 Classification-F1 0.1542857142857143 on epoch=62
05/30/2022 21:25:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.86 on epoch=64
05/30/2022 21:25:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.89 on epoch=67
05/30/2022 21:25:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.85 on epoch=69
05/30/2022 21:25:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.92 on epoch=72
05/30/2022 21:25:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.89 on epoch=74
05/30/2022 21:25:40 - INFO - __main__ - Global step 300 Train loss 0.88 Classification-F1 0.17240107310529845 on epoch=74
05/30/2022 21:25:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.93 on epoch=77
05/30/2022 21:25:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.90 on epoch=79
05/30/2022 21:25:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.89 on epoch=82
05/30/2022 21:25:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.90 on epoch=84
05/30/2022 21:25:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.84 on epoch=87
05/30/2022 21:25:53 - INFO - __main__ - Global step 350 Train loss 0.89 Classification-F1 0.1542857142857143 on epoch=87
05/30/2022 21:25:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.87 on epoch=89
05/30/2022 21:25:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.83 on epoch=92
05/30/2022 21:26:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.81 on epoch=94
05/30/2022 21:26:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.81 on epoch=97
05/30/2022 21:26:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.81 on epoch=99
05/30/2022 21:26:05 - INFO - __main__ - Global step 400 Train loss 0.83 Classification-F1 0.22369747899159664 on epoch=99
05/30/2022 21:26:05 - INFO - __main__ - Saving model with best Classification-F1: 0.21153846153846154 -> 0.22369747899159664 on epoch=99, global_step=400
05/30/2022 21:26:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.89 on epoch=102
05/30/2022 21:26:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.73 on epoch=104
05/30/2022 21:26:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.77 on epoch=107
05/30/2022 21:26:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.77 on epoch=109
05/30/2022 21:26:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.85 on epoch=112
05/30/2022 21:26:18 - INFO - __main__ - Global step 450 Train loss 0.80 Classification-F1 0.1 on epoch=112
05/30/2022 21:26:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.77 on epoch=114
05/30/2022 21:26:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.72 on epoch=117
05/30/2022 21:26:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.80 on epoch=119
05/30/2022 21:26:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.87 on epoch=122
05/30/2022 21:26:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.79 on epoch=124
05/30/2022 21:26:31 - INFO - __main__ - Global step 500 Train loss 0.79 Classification-F1 0.35342007616443705 on epoch=124
05/30/2022 21:26:31 - INFO - __main__ - Saving model with best Classification-F1: 0.22369747899159664 -> 0.35342007616443705 on epoch=124, global_step=500
05/30/2022 21:26:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.82 on epoch=127
05/30/2022 21:26:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.74 on epoch=129
05/30/2022 21:26:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.67 on epoch=132
05/30/2022 21:26:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.67 on epoch=134
05/30/2022 21:26:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.77 on epoch=137
05/30/2022 21:26:44 - INFO - __main__ - Global step 550 Train loss 0.73 Classification-F1 0.1 on epoch=137
05/30/2022 21:26:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.75 on epoch=139
05/30/2022 21:26:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.73 on epoch=142
05/30/2022 21:26:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.63 on epoch=144
05/30/2022 21:26:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.73 on epoch=147
05/30/2022 21:26:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.57 on epoch=149
05/30/2022 21:26:56 - INFO - __main__ - Global step 600 Train loss 0.68 Classification-F1 0.22215099715099712 on epoch=149
05/30/2022 21:26:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.66 on epoch=152
05/30/2022 21:27:01 - INFO - __main__ - Step 620 Global step 620 Train loss 0.56 on epoch=154
05/30/2022 21:27:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.58 on epoch=157
05/30/2022 21:27:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=159
05/30/2022 21:27:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.57 on epoch=162
05/30/2022 21:27:09 - INFO - __main__ - Global step 650 Train loss 0.58 Classification-F1 0.3537596040401715 on epoch=162
05/30/2022 21:27:09 - INFO - __main__ - Saving model with best Classification-F1: 0.35342007616443705 -> 0.3537596040401715 on epoch=162, global_step=650
05/30/2022 21:27:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.53 on epoch=164
05/30/2022 21:27:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.54 on epoch=167
05/30/2022 21:27:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.45 on epoch=169
05/30/2022 21:27:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=172
05/30/2022 21:27:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=174
05/30/2022 21:27:22 - INFO - __main__ - Global step 700 Train loss 0.48 Classification-F1 0.5414319849803721 on epoch=174
05/30/2022 21:27:22 - INFO - __main__ - Saving model with best Classification-F1: 0.3537596040401715 -> 0.5414319849803721 on epoch=174, global_step=700
05/30/2022 21:27:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=177
05/30/2022 21:27:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=179
05/30/2022 21:27:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.52 on epoch=182
05/30/2022 21:27:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=184
05/30/2022 21:27:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=187
05/30/2022 21:27:34 - INFO - __main__ - Global step 750 Train loss 0.46 Classification-F1 0.5171849838969405 on epoch=187
05/30/2022 21:27:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=189
05/30/2022 21:27:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.50 on epoch=192
05/30/2022 21:27:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=194
05/30/2022 21:27:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=197
05/30/2022 21:27:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=199
05/30/2022 21:27:47 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.5353896103896104 on epoch=199
05/30/2022 21:27:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=202
05/30/2022 21:27:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=204
05/30/2022 21:27:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=207
05/30/2022 21:27:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=209
05/30/2022 21:27:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=212
05/30/2022 21:28:00 - INFO - __main__ - Global step 850 Train loss 0.35 Classification-F1 0.5703690707837571 on epoch=212
05/30/2022 21:28:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5414319849803721 -> 0.5703690707837571 on epoch=212, global_step=850
05/30/2022 21:28:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=214
05/30/2022 21:28:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=217
05/30/2022 21:28:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.33 on epoch=219
05/30/2022 21:28:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=222
05/30/2022 21:28:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=224
05/30/2022 21:28:13 - INFO - __main__ - Global step 900 Train loss 0.25 Classification-F1 0.7611773157538878 on epoch=224
05/30/2022 21:28:13 - INFO - __main__ - Saving model with best Classification-F1: 0.5703690707837571 -> 0.7611773157538878 on epoch=224, global_step=900
05/30/2022 21:28:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=227
05/30/2022 21:28:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=229
05/30/2022 21:28:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.30 on epoch=232
05/30/2022 21:28:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.15 on epoch=234
05/30/2022 21:28:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=237
05/30/2022 21:28:25 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.6392534340481671 on epoch=237
05/30/2022 21:28:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=239
05/30/2022 21:28:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.28 on epoch=242
05/30/2022 21:28:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.25 on epoch=244
05/30/2022 21:28:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=247
05/30/2022 21:28:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=249
05/30/2022 21:28:38 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.7631448412698413 on epoch=249
05/30/2022 21:28:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7611773157538878 -> 0.7631448412698413 on epoch=249, global_step=1000
05/30/2022 21:28:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.19 on epoch=252
05/30/2022 21:28:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=254
05/30/2022 21:28:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=257
05/30/2022 21:28:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=259
05/30/2022 21:28:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=262
05/30/2022 21:28:51 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.5450169480530012 on epoch=262
05/30/2022 21:28:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.14 on epoch=264
05/30/2022 21:28:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=267
05/30/2022 21:28:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.28 on epoch=269
05/30/2022 21:29:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.13 on epoch=272
05/30/2022 21:29:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=274
05/30/2022 21:29:03 - INFO - __main__ - Global step 1100 Train loss 0.16 Classification-F1 0.778632004438456 on epoch=274
05/30/2022 21:29:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7631448412698413 -> 0.778632004438456 on epoch=274, global_step=1100
05/30/2022 21:29:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.09 on epoch=277
05/30/2022 21:29:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=279
05/30/2022 21:29:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=282
05/30/2022 21:29:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=284
05/30/2022 21:29:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=287
05/30/2022 21:29:16 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.6856939356939358 on epoch=287
05/30/2022 21:29:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=289
05/30/2022 21:29:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.15 on epoch=292
05/30/2022 21:29:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=294
05/30/2022 21:29:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=297
05/30/2022 21:29:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=299
05/30/2022 21:29:29 - INFO - __main__ - Global step 1200 Train loss 0.09 Classification-F1 0.6351852386173467 on epoch=299
05/30/2022 21:29:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=302
05/30/2022 21:29:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.17 on epoch=304
05/30/2022 21:29:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=307
05/30/2022 21:29:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
05/30/2022 21:29:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=312
05/30/2022 21:29:42 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.6560172986596403 on epoch=312
05/30/2022 21:29:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
05/30/2022 21:29:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=317
05/30/2022 21:29:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=319
05/30/2022 21:29:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=322
05/30/2022 21:29:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=324
05/30/2022 21:29:55 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.5899266201685556 on epoch=324
05/30/2022 21:29:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=327
05/30/2022 21:30:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=329
05/30/2022 21:30:02 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=332
05/30/2022 21:30:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/30/2022 21:30:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
05/30/2022 21:30:08 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.5684417136030038 on epoch=337
05/30/2022 21:30:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=339
05/30/2022 21:30:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
05/30/2022 21:30:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
05/30/2022 21:30:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.08 on epoch=347
05/30/2022 21:30:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=349
05/30/2022 21:30:21 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.6945404505888377 on epoch=349
05/30/2022 21:30:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
05/30/2022 21:30:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
05/30/2022 21:30:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
05/30/2022 21:30:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/30/2022 21:30:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=362
05/30/2022 21:30:33 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.715814393939394 on epoch=362
05/30/2022 21:30:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
05/30/2022 21:30:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
05/30/2022 21:30:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.09 on epoch=369
05/30/2022 21:30:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/30/2022 21:30:46 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=374
05/30/2022 21:30:47 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.7779858092358092 on epoch=374
05/30/2022 21:30:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
05/30/2022 21:30:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=379
05/30/2022 21:30:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=382
05/30/2022 21:30:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
05/30/2022 21:30:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=387
05/30/2022 21:31:00 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.7494195992179864 on epoch=387
05/30/2022 21:31:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=389
05/30/2022 21:31:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=392
05/30/2022 21:31:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=394
05/30/2022 21:31:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
05/30/2022 21:31:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.12 on epoch=399
05/30/2022 21:31:13 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.5911194978547594 on epoch=399
05/30/2022 21:31:15 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
05/30/2022 21:31:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=404
05/30/2022 21:31:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
05/30/2022 21:31:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=409
05/30/2022 21:31:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/30/2022 21:31:26 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.7963914152980436 on epoch=412
05/30/2022 21:31:26 - INFO - __main__ - Saving model with best Classification-F1: 0.778632004438456 -> 0.7963914152980436 on epoch=412, global_step=1650
05/30/2022 21:31:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/30/2022 21:31:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=417
05/30/2022 21:31:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/30/2022 21:31:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
05/30/2022 21:31:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=424
05/30/2022 21:31:39 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.6174573055028463 on epoch=424
05/30/2022 21:31:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/30/2022 21:31:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
05/30/2022 21:31:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
05/30/2022 21:31:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/30/2022 21:31:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=437
05/30/2022 21:31:52 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.7914845011619205 on epoch=437
05/30/2022 21:31:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=439
05/30/2022 21:31:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=442
05/30/2022 21:31:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/30/2022 21:32:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
05/30/2022 21:32:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/30/2022 21:32:05 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.7427375762859634 on epoch=449
05/30/2022 21:32:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/30/2022 21:32:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/30/2022 21:32:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=457
05/30/2022 21:32:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=459
05/30/2022 21:32:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/30/2022 21:32:18 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.7482078853046594 on epoch=462
05/30/2022 21:32:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=464
05/30/2022 21:32:23 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=467
05/30/2022 21:32:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=469
05/30/2022 21:32:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=472
05/30/2022 21:32:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/30/2022 21:32:31 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.6432142857142857 on epoch=474
05/30/2022 21:32:33 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/30/2022 21:32:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/30/2022 21:32:38 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=482
05/30/2022 21:32:41 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/30/2022 21:32:43 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/30/2022 21:32:44 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.7759652981427175 on epoch=487
05/30/2022 21:32:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
05/30/2022 21:32:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=492
05/30/2022 21:32:51 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/30/2022 21:32:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
05/30/2022 21:32:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/30/2022 21:32:57 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.7642610282696489 on epoch=499
05/30/2022 21:32:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=502
05/30/2022 21:33:02 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/30/2022 21:33:04 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=507
05/30/2022 21:33:07 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/30/2022 21:33:09 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
05/30/2022 21:33:10 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.7601981351981353 on epoch=512
05/30/2022 21:33:12 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/30/2022 21:33:15 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/30/2022 21:33:17 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/30/2022 21:33:20 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/30/2022 21:33:22 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/30/2022 21:33:23 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7447787081339713 on epoch=524
05/30/2022 21:33:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/30/2022 21:33:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/30/2022 21:33:30 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=532
05/30/2022 21:33:33 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/30/2022 21:33:35 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/30/2022 21:33:36 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.7652677357557781 on epoch=537
05/30/2022 21:33:39 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/30/2022 21:33:41 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/30/2022 21:33:44 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/30/2022 21:33:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
05/30/2022 21:33:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
05/30/2022 21:33:50 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.7660539215686275 on epoch=549
05/30/2022 21:33:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.13 on epoch=552
05/30/2022 21:33:55 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/30/2022 21:33:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/30/2022 21:33:59 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
05/30/2022 21:34:02 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/30/2022 21:34:03 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.8137927192820702 on epoch=562
05/30/2022 21:34:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7963914152980436 -> 0.8137927192820702 on epoch=562, global_step=2250
05/30/2022 21:34:05 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/30/2022 21:34:08 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
05/30/2022 21:34:10 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=569
05/30/2022 21:34:12 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=572
05/30/2022 21:34:15 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/30/2022 21:34:16 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.781300513036756 on epoch=574
05/30/2022 21:34:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=577
05/30/2022 21:34:20 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=579
05/30/2022 21:34:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/30/2022 21:34:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
05/30/2022 21:34:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/30/2022 21:34:29 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.780059004857392 on epoch=587
05/30/2022 21:34:31 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/30/2022 21:34:33 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=592
05/30/2022 21:34:36 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/30/2022 21:34:38 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/30/2022 21:34:41 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/30/2022 21:34:41 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.6615238720501879 on epoch=599
05/30/2022 21:34:44 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=602
05/30/2022 21:34:46 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/30/2022 21:34:49 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
05/30/2022 21:34:51 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/30/2022 21:34:54 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=612
05/30/2022 21:34:54 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7946898496240601 on epoch=612
05/30/2022 21:34:57 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/30/2022 21:34:59 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=617
05/30/2022 21:35:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=619
05/30/2022 21:35:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/30/2022 21:35:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/30/2022 21:35:07 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.8270833333333333 on epoch=624
05/30/2022 21:35:07 - INFO - __main__ - Saving model with best Classification-F1: 0.8137927192820702 -> 0.8270833333333333 on epoch=624, global_step=2500
05/30/2022 21:35:10 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/30/2022 21:35:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/30/2022 21:35:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/30/2022 21:35:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.07 on epoch=634
05/30/2022 21:35:20 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/30/2022 21:35:21 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.7777243589743589 on epoch=637
05/30/2022 21:35:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/30/2022 21:35:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/30/2022 21:35:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=644
05/30/2022 21:35:30 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.06 on epoch=647
05/30/2022 21:35:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/30/2022 21:35:34 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.7970232447817838 on epoch=649
05/30/2022 21:35:36 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/30/2022 21:35:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
05/30/2022 21:35:41 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/30/2022 21:35:43 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/30/2022 21:35:46 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/30/2022 21:35:47 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7777243589743589 on epoch=662
05/30/2022 21:35:49 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/30/2022 21:35:52 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/30/2022 21:35:54 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/30/2022 21:35:57 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
05/30/2022 21:35:59 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=674
05/30/2022 21:36:00 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.6043197720617076 on epoch=674
05/30/2022 21:36:02 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/30/2022 21:36:05 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/30/2022 21:36:07 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=682
05/30/2022 21:36:10 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/30/2022 21:36:12 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/30/2022 21:36:13 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7622132699996994 on epoch=687
05/30/2022 21:36:16 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
05/30/2022 21:36:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/30/2022 21:36:20 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/30/2022 21:36:23 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/30/2022 21:36:25 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/30/2022 21:36:26 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7279500964984836 on epoch=699
05/30/2022 21:36:29 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/30/2022 21:36:31 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/30/2022 21:36:34 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/30/2022 21:36:36 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/30/2022 21:36:39 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/30/2022 21:36:40 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7468434343434344 on epoch=712
05/30/2022 21:36:42 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/30/2022 21:36:44 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
05/30/2022 21:36:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/30/2022 21:36:49 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/30/2022 21:36:52 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/30/2022 21:36:53 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7587079057667293 on epoch=724
05/30/2022 21:36:55 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/30/2022 21:36:58 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/30/2022 21:37:00 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/30/2022 21:37:02 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/30/2022 21:37:05 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/30/2022 21:37:06 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7651348039215686 on epoch=737
05/30/2022 21:37:08 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/30/2022 21:37:11 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/30/2022 21:37:13 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/30/2022 21:37:16 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/30/2022 21:37:18 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
05/30/2022 21:37:19 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7495098039215686 on epoch=749
05/30/2022 21:37:19 - INFO - __main__ - save last model!
05/30/2022 21:37:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 21:37:19 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 21:37:19 - INFO - __main__ - Printing 3 examples
05/30/2022 21:37:19 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 21:37:19 - INFO - __main__ - ['others']
05/30/2022 21:37:19 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 21:37:19 - INFO - __main__ - ['others']
05/30/2022 21:37:19 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 21:37:19 - INFO - __main__ - ['others']
05/30/2022 21:37:19 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:37:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 21:37:19 - INFO - __main__ - Printing 3 examples
05/30/2022 21:37:19 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/30/2022 21:37:19 - INFO - __main__ - ['sad']
05/30/2022 21:37:19 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/30/2022 21:37:19 - INFO - __main__ - ['sad']
05/30/2022 21:37:19 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/30/2022 21:37:19 - INFO - __main__ - ['sad']
05/30/2022 21:37:19 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:37:19 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:37:19 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 21:37:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 21:37:19 - INFO - __main__ - Printing 3 examples
05/30/2022 21:37:19 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/30/2022 21:37:19 - INFO - __main__ - ['sad']
05/30/2022 21:37:19 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/30/2022 21:37:19 - INFO - __main__ - ['sad']
05/30/2022 21:37:19 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/30/2022 21:37:19 - INFO - __main__ - ['sad']
05/30/2022 21:37:19 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:37:19 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:37:19 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 21:37:21 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:37:27 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 21:37:38 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 21:37:38 - INFO - __main__ - task name: emo
05/30/2022 21:37:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 21:37:39 - INFO - __main__ - Starting training!
05/30/2022 21:38:57 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_21_0.5_8_predictions.txt
05/30/2022 21:38:57 - INFO - __main__ - Classification-F1 on test data: 0.2966
05/30/2022 21:38:57 - INFO - __main__ - prefix=emo_16_21, lr=0.5, bsz=8, dev_performance=0.8270833333333333, test_performance=0.29661629093019176
05/30/2022 21:38:57 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.4, bsz=8 ...
05/30/2022 21:38:58 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 21:38:58 - INFO - __main__ - Printing 3 examples
05/30/2022 21:38:58 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/30/2022 21:38:58 - INFO - __main__ - ['sad']
05/30/2022 21:38:58 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/30/2022 21:38:58 - INFO - __main__ - ['sad']
05/30/2022 21:38:58 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/30/2022 21:38:58 - INFO - __main__ - ['sad']
05/30/2022 21:38:58 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:38:58 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:38:58 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 21:38:58 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 21:38:58 - INFO - __main__ - Printing 3 examples
05/30/2022 21:38:58 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/30/2022 21:38:58 - INFO - __main__ - ['sad']
05/30/2022 21:38:58 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/30/2022 21:38:58 - INFO - __main__ - ['sad']
05/30/2022 21:38:58 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/30/2022 21:38:58 - INFO - __main__ - ['sad']
05/30/2022 21:38:58 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:38:58 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:38:58 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 21:39:13 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 21:39:13 - INFO - __main__ - task name: emo
05/30/2022 21:39:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 21:39:14 - INFO - __main__ - Starting training!
05/30/2022 21:39:17 - INFO - __main__ - Step 10 Global step 10 Train loss 7.62 on epoch=2
05/30/2022 21:39:19 - INFO - __main__ - Step 20 Global step 20 Train loss 4.42 on epoch=4
05/30/2022 21:39:21 - INFO - __main__ - Step 30 Global step 30 Train loss 2.48 on epoch=7
05/30/2022 21:39:24 - INFO - __main__ - Step 40 Global step 40 Train loss 1.55 on epoch=9
05/30/2022 21:39:26 - INFO - __main__ - Step 50 Global step 50 Train loss 1.48 on epoch=12
05/30/2022 21:39:27 - INFO - __main__ - Global step 50 Train loss 3.51 Classification-F1 0.1 on epoch=12
05/30/2022 21:39:27 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/30/2022 21:39:29 - INFO - __main__ - Step 60 Global step 60 Train loss 1.19 on epoch=14
05/30/2022 21:39:32 - INFO - __main__ - Step 70 Global step 70 Train loss 1.33 on epoch=17
05/30/2022 21:39:34 - INFO - __main__ - Step 80 Global step 80 Train loss 1.17 on epoch=19
05/30/2022 21:39:36 - INFO - __main__ - Step 90 Global step 90 Train loss 1.11 on epoch=22
05/30/2022 21:39:39 - INFO - __main__ - Step 100 Global step 100 Train loss 0.94 on epoch=24
05/30/2022 21:39:40 - INFO - __main__ - Global step 100 Train loss 1.15 Classification-F1 0.1769230769230769 on epoch=24
05/30/2022 21:39:40 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1769230769230769 on epoch=24, global_step=100
05/30/2022 21:39:42 - INFO - __main__ - Step 110 Global step 110 Train loss 1.06 on epoch=27
05/30/2022 21:39:44 - INFO - __main__ - Step 120 Global step 120 Train loss 1.05 on epoch=29
05/30/2022 21:39:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.97 on epoch=32
05/30/2022 21:39:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.90 on epoch=34
05/30/2022 21:39:52 - INFO - __main__ - Step 150 Global step 150 Train loss 1.02 on epoch=37
05/30/2022 21:39:52 - INFO - __main__ - Global step 150 Train loss 1.00 Classification-F1 0.19240895129442737 on epoch=37
05/30/2022 21:39:52 - INFO - __main__ - Saving model with best Classification-F1: 0.1769230769230769 -> 0.19240895129442737 on epoch=37, global_step=150
05/30/2022 21:39:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.91 on epoch=39
05/30/2022 21:39:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.91 on epoch=42
05/30/2022 21:40:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=44
05/30/2022 21:40:02 - INFO - __main__ - Step 190 Global step 190 Train loss 1.03 on epoch=47
05/30/2022 21:40:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.96 on epoch=49
05/30/2022 21:40:05 - INFO - __main__ - Global step 200 Train loss 0.94 Classification-F1 0.1 on epoch=49
05/30/2022 21:40:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.86 on epoch=52
05/30/2022 21:40:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.99 on epoch=54
05/30/2022 21:40:12 - INFO - __main__ - Step 230 Global step 230 Train loss 1.06 on epoch=57
05/30/2022 21:40:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.83 on epoch=59
05/30/2022 21:40:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.89 on epoch=62
05/30/2022 21:40:18 - INFO - __main__ - Global step 250 Train loss 0.93 Classification-F1 0.17142857142857143 on epoch=62
05/30/2022 21:40:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.89 on epoch=64
05/30/2022 21:40:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.88 on epoch=67
05/30/2022 21:40:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.83 on epoch=69
05/30/2022 21:40:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.87 on epoch=72
05/30/2022 21:40:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.93 on epoch=74
05/30/2022 21:40:31 - INFO - __main__ - Global step 300 Train loss 0.88 Classification-F1 0.18621163536417773 on epoch=74
05/30/2022 21:40:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.84 on epoch=77
05/30/2022 21:40:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.83 on epoch=79
05/30/2022 21:40:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.79 on epoch=82
05/30/2022 21:40:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.89 on epoch=84
05/30/2022 21:40:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.96 on epoch=87
05/30/2022 21:40:43 - INFO - __main__ - Global step 350 Train loss 0.86 Classification-F1 0.15356265356265356 on epoch=87
05/30/2022 21:40:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.80 on epoch=89
05/30/2022 21:40:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.84 on epoch=92
05/30/2022 21:40:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.80 on epoch=94
05/30/2022 21:40:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.85 on epoch=97
05/30/2022 21:40:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.81 on epoch=99
05/30/2022 21:40:56 - INFO - __main__ - Global step 400 Train loss 0.82 Classification-F1 0.31240289638122454 on epoch=99
05/30/2022 21:40:56 - INFO - __main__ - Saving model with best Classification-F1: 0.19240895129442737 -> 0.31240289638122454 on epoch=99, global_step=400
05/30/2022 21:40:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.75 on epoch=102
05/30/2022 21:41:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.78 on epoch=104
05/30/2022 21:41:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.83 on epoch=107
05/30/2022 21:41:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.79 on epoch=109
05/30/2022 21:41:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.80 on epoch=112
05/30/2022 21:41:09 - INFO - __main__ - Global step 450 Train loss 0.79 Classification-F1 0.2176382047071702 on epoch=112
05/30/2022 21:41:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.71 on epoch=114
05/30/2022 21:41:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.74 on epoch=117
05/30/2022 21:41:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.71 on epoch=119
05/30/2022 21:41:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.73 on epoch=122
05/30/2022 21:41:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.71 on epoch=124
05/30/2022 21:41:22 - INFO - __main__ - Global step 500 Train loss 0.72 Classification-F1 0.2740780077736599 on epoch=124
05/30/2022 21:41:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.75 on epoch=127
05/30/2022 21:41:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.70 on epoch=129
05/30/2022 21:41:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.80 on epoch=132
05/30/2022 21:41:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.68 on epoch=134
05/30/2022 21:41:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.67 on epoch=137
05/30/2022 21:41:34 - INFO - __main__ - Global step 550 Train loss 0.72 Classification-F1 0.15306730196545562 on epoch=137
05/30/2022 21:41:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.72 on epoch=139
05/30/2022 21:41:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.69 on epoch=142
05/30/2022 21:41:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.58 on epoch=144
05/30/2022 21:41:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.61 on epoch=147
05/30/2022 21:41:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.54 on epoch=149
05/30/2022 21:41:47 - INFO - __main__ - Global step 600 Train loss 0.63 Classification-F1 0.31631652661064424 on epoch=149
05/30/2022 21:41:47 - INFO - __main__ - Saving model with best Classification-F1: 0.31240289638122454 -> 0.31631652661064424 on epoch=149, global_step=600
05/30/2022 21:41:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.62 on epoch=152
05/30/2022 21:41:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.53 on epoch=154
05/30/2022 21:41:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.62 on epoch=157
05/30/2022 21:41:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.52 on epoch=159
05/30/2022 21:41:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.65 on epoch=162
05/30/2022 21:42:00 - INFO - __main__ - Global step 650 Train loss 0.59 Classification-F1 0.3962272408963585 on epoch=162
05/30/2022 21:42:00 - INFO - __main__ - Saving model with best Classification-F1: 0.31631652661064424 -> 0.3962272408963585 on epoch=162, global_step=650
05/30/2022 21:42:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=164
05/30/2022 21:42:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.51 on epoch=167
05/30/2022 21:42:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.57 on epoch=169
05/30/2022 21:42:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=172
05/30/2022 21:42:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.49 on epoch=174
05/30/2022 21:42:13 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.5217086445256635 on epoch=174
05/30/2022 21:42:13 - INFO - __main__ - Saving model with best Classification-F1: 0.3962272408963585 -> 0.5217086445256635 on epoch=174, global_step=700
05/30/2022 21:42:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=177
05/30/2022 21:42:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=179
05/30/2022 21:42:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=182
05/30/2022 21:42:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=184
05/30/2022 21:42:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=187
05/30/2022 21:42:25 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.42545132172791744 on epoch=187
05/30/2022 21:42:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.49 on epoch=189
05/30/2022 21:42:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.62 on epoch=192
05/30/2022 21:42:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=194
05/30/2022 21:42:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=197
05/30/2022 21:42:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=199
05/30/2022 21:42:38 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.6129201680672269 on epoch=199
05/30/2022 21:42:38 - INFO - __main__ - Saving model with best Classification-F1: 0.5217086445256635 -> 0.6129201680672269 on epoch=199, global_step=800
05/30/2022 21:42:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.35 on epoch=202
05/30/2022 21:42:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=204
05/30/2022 21:42:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=207
05/30/2022 21:42:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=209
05/30/2022 21:42:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=212
05/30/2022 21:42:51 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.4490915443745632 on epoch=212
05/30/2022 21:42:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=214
05/30/2022 21:42:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=217
05/30/2022 21:42:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.31 on epoch=219
05/30/2022 21:43:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.29 on epoch=222
05/30/2022 21:43:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.33 on epoch=224
05/30/2022 21:43:04 - INFO - __main__ - Global step 900 Train loss 0.32 Classification-F1 0.5074074074074073 on epoch=224
05/30/2022 21:43:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.26 on epoch=227
05/30/2022 21:43:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.25 on epoch=229
05/30/2022 21:43:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=232
05/30/2022 21:43:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.25 on epoch=234
05/30/2022 21:43:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.23 on epoch=237
05/30/2022 21:43:16 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.5869692391431521 on epoch=237
05/30/2022 21:43:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=239
05/30/2022 21:43:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.27 on epoch=242
05/30/2022 21:43:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=244
05/30/2022 21:43:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.26 on epoch=247
05/30/2022 21:43:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=249
05/30/2022 21:43:29 - INFO - __main__ - Global step 1000 Train loss 0.24 Classification-F1 0.6554908906882592 on epoch=249
05/30/2022 21:43:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6129201680672269 -> 0.6554908906882592 on epoch=249, global_step=1000
05/30/2022 21:43:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=252
05/30/2022 21:43:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=254
05/30/2022 21:43:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=257
05/30/2022 21:43:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.25 on epoch=259
05/30/2022 21:43:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.18 on epoch=262
05/30/2022 21:43:42 - INFO - __main__ - Global step 1050 Train loss 0.18 Classification-F1 0.6758878913078022 on epoch=262
05/30/2022 21:43:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6554908906882592 -> 0.6758878913078022 on epoch=262, global_step=1050
05/30/2022 21:43:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=264
05/30/2022 21:43:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=267
05/30/2022 21:43:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.27 on epoch=269
05/30/2022 21:43:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.11 on epoch=272
05/30/2022 21:43:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=274
05/30/2022 21:43:55 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.7321654449603633 on epoch=274
05/30/2022 21:43:55 - INFO - __main__ - Saving model with best Classification-F1: 0.6758878913078022 -> 0.7321654449603633 on epoch=274, global_step=1100
05/30/2022 21:43:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.18 on epoch=277
05/30/2022 21:43:59 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.13 on epoch=279
05/30/2022 21:44:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.17 on epoch=282
05/30/2022 21:44:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=284
05/30/2022 21:44:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=287
05/30/2022 21:44:07 - INFO - __main__ - Global step 1150 Train loss 0.15 Classification-F1 0.6589635854341738 on epoch=287
05/30/2022 21:44:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=289
05/30/2022 21:44:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.12 on epoch=292
05/30/2022 21:44:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=294
05/30/2022 21:44:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=297
05/30/2022 21:44:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.13 on epoch=299
05/30/2022 21:44:20 - INFO - __main__ - Global step 1200 Train loss 0.14 Classification-F1 0.6678661716481515 on epoch=299
05/30/2022 21:44:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.15 on epoch=302
05/30/2022 21:44:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.12 on epoch=304
05/30/2022 21:44:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=307
05/30/2022 21:44:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=309
05/30/2022 21:44:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=312
05/30/2022 21:44:33 - INFO - __main__ - Global step 1250 Train loss 0.13 Classification-F1 0.6381410869958868 on epoch=312
05/30/2022 21:44:35 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=314
05/30/2022 21:44:38 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=317
05/30/2022 21:44:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=319
05/30/2022 21:44:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=322
05/30/2022 21:44:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=324
05/30/2022 21:44:46 - INFO - __main__ - Global step 1300 Train loss 0.10 Classification-F1 0.6426520847573479 on epoch=324
05/30/2022 21:44:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.11 on epoch=327
05/30/2022 21:44:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.09 on epoch=329
05/30/2022 21:44:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.12 on epoch=332
05/30/2022 21:44:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=334
05/30/2022 21:44:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=337
05/30/2022 21:44:58 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.6837525987525988 on epoch=337
05/30/2022 21:45:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
05/30/2022 21:45:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=342
05/30/2022 21:45:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=344
05/30/2022 21:45:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.09 on epoch=347
05/30/2022 21:45:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.13 on epoch=349
05/30/2022 21:45:11 - INFO - __main__ - Global step 1400 Train loss 0.08 Classification-F1 0.7374999999999999 on epoch=349
05/30/2022 21:45:11 - INFO - __main__ - Saving model with best Classification-F1: 0.7321654449603633 -> 0.7374999999999999 on epoch=349, global_step=1400
05/30/2022 21:45:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=352
05/30/2022 21:45:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.13 on epoch=354
05/30/2022 21:45:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=357
05/30/2022 21:45:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=359
05/30/2022 21:45:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=362
05/30/2022 21:45:24 - INFO - __main__ - Global step 1450 Train loss 0.10 Classification-F1 0.6396918299092212 on epoch=362
05/30/2022 21:45:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=364
05/30/2022 21:45:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=367
05/30/2022 21:45:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=369
05/30/2022 21:45:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=372
05/30/2022 21:45:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.08 on epoch=374
05/30/2022 21:45:37 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.6516840882694541 on epoch=374
05/30/2022 21:45:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=377
05/30/2022 21:45:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
05/30/2022 21:45:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
05/30/2022 21:45:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.09 on epoch=384
05/30/2022 21:45:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=387
05/30/2022 21:45:50 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.7054616828810376 on epoch=387
05/30/2022 21:45:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.16 on epoch=389
05/30/2022 21:45:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.09 on epoch=392
05/30/2022 21:45:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=394
05/30/2022 21:45:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
05/30/2022 21:46:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
05/30/2022 21:46:02 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.7529358901062755 on epoch=399
05/30/2022 21:46:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7374999999999999 -> 0.7529358901062755 on epoch=399, global_step=1600
05/30/2022 21:46:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=402
05/30/2022 21:46:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=404
05/30/2022 21:46:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
05/30/2022 21:46:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=409
05/30/2022 21:46:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
05/30/2022 21:46:15 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.6751602564102565 on epoch=412
05/30/2022 21:46:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
05/30/2022 21:46:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
05/30/2022 21:46:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=419
05/30/2022 21:46:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/30/2022 21:46:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=424
05/30/2022 21:46:28 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.6899613899613899 on epoch=424
05/30/2022 21:46:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/30/2022 21:46:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
05/30/2022 21:46:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=432
05/30/2022 21:46:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
05/30/2022 21:46:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
05/30/2022 21:46:41 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.7353942652329748 on epoch=437
05/30/2022 21:46:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
05/30/2022 21:46:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=442
05/30/2022 21:46:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=444
05/30/2022 21:46:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
05/30/2022 21:46:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
05/30/2022 21:46:54 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.6752294021056698 on epoch=449
05/30/2022 21:46:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.11 on epoch=452
05/30/2022 21:46:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.08 on epoch=454
05/30/2022 21:47:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/30/2022 21:47:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=459
05/30/2022 21:47:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
05/30/2022 21:47:07 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.6522435897435898 on epoch=462
05/30/2022 21:47:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/30/2022 21:47:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/30/2022 21:47:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=469
05/30/2022 21:47:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/30/2022 21:47:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
05/30/2022 21:47:20 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.7215503246753247 on epoch=474
05/30/2022 21:47:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
05/30/2022 21:47:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/30/2022 21:47:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=482
05/30/2022 21:47:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
05/30/2022 21:47:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/30/2022 21:47:33 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.7068627450980393 on epoch=487
05/30/2022 21:47:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/30/2022 21:47:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
05/30/2022 21:47:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/30/2022 21:47:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/30/2022 21:47:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/30/2022 21:47:46 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.6581541218637993 on epoch=499
05/30/2022 21:47:48 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/30/2022 21:47:51 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.14 on epoch=504
05/30/2022 21:47:53 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
05/30/2022 21:47:56 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/30/2022 21:47:58 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.16 on epoch=512
05/30/2022 21:47:59 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.6756270239133142 on epoch=512
05/30/2022 21:48:02 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
05/30/2022 21:48:04 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
05/30/2022 21:48:07 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/30/2022 21:48:09 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/30/2022 21:48:12 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/30/2022 21:48:12 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.6707142857142858 on epoch=524
05/30/2022 21:48:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/30/2022 21:48:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=529
05/30/2022 21:48:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=532
05/30/2022 21:48:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/30/2022 21:48:25 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/30/2022 21:48:26 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.674503657262278 on epoch=537
05/30/2022 21:48:28 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.10 on epoch=539
05/30/2022 21:48:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/30/2022 21:48:33 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=544
05/30/2022 21:48:36 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=547
05/30/2022 21:48:38 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/30/2022 21:48:39 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.7070412035782054 on epoch=549
05/30/2022 21:48:41 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=552
05/30/2022 21:48:44 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/30/2022 21:48:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/30/2022 21:48:49 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=559
05/30/2022 21:48:51 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.23 on epoch=562
05/30/2022 21:48:52 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.7032147311346422 on epoch=562
05/30/2022 21:48:55 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=564
05/30/2022 21:48:57 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/30/2022 21:49:00 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=569
05/30/2022 21:49:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/30/2022 21:49:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
05/30/2022 21:49:06 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.6922600884294432 on epoch=574
05/30/2022 21:49:08 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/30/2022 21:49:11 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
05/30/2022 21:49:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/30/2022 21:49:15 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
05/30/2022 21:49:18 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=587
05/30/2022 21:49:19 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.751456736909323 on epoch=587
05/30/2022 21:49:21 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/30/2022 21:49:24 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=592
05/30/2022 21:49:26 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/30/2022 21:49:29 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
05/30/2022 21:49:31 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=599
05/30/2022 21:49:32 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.6379310344827587 on epoch=599
05/30/2022 21:49:35 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=602
05/30/2022 21:49:37 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=604
05/30/2022 21:49:39 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/30/2022 21:49:42 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/30/2022 21:49:44 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/30/2022 21:49:45 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7070645363408521 on epoch=612
05/30/2022 21:49:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=614
05/30/2022 21:49:50 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=617
05/30/2022 21:49:53 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=619
05/30/2022 21:49:55 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=622
05/30/2022 21:49:58 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/30/2022 21:49:59 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.7225232198142415 on epoch=624
05/30/2022 21:50:01 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
05/30/2022 21:50:04 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/30/2022 21:50:06 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=632
05/30/2022 21:50:09 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/30/2022 21:50:11 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=637
05/30/2022 21:50:12 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.7078817733990148 on epoch=637
05/30/2022 21:50:15 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=639
05/30/2022 21:50:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/30/2022 21:50:20 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
05/30/2022 21:50:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/30/2022 21:50:24 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
05/30/2022 21:50:25 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.7336814257866889 on epoch=649
05/30/2022 21:50:28 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/30/2022 21:50:30 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.05 on epoch=654
05/30/2022 21:50:33 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
05/30/2022 21:50:35 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/30/2022 21:50:38 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
05/30/2022 21:50:39 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.7219446047032254 on epoch=662
05/30/2022 21:50:41 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/30/2022 21:50:44 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/30/2022 21:50:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/30/2022 21:50:49 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/30/2022 21:50:51 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/30/2022 21:50:52 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7336814257866889 on epoch=674
05/30/2022 21:50:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/30/2022 21:50:57 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/30/2022 21:50:59 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/30/2022 21:51:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/30/2022 21:51:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
05/30/2022 21:51:05 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6862704565030145 on epoch=687
05/30/2022 21:51:08 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
05/30/2022 21:51:10 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
05/30/2022 21:51:13 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/30/2022 21:51:15 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/30/2022 21:51:18 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/30/2022 21:51:19 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.6926326871126648 on epoch=699
05/30/2022 21:51:21 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/30/2022 21:51:24 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
05/30/2022 21:51:26 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/30/2022 21:51:29 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/30/2022 21:51:31 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/30/2022 21:51:32 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.736830159782746 on epoch=712
05/30/2022 21:51:35 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/30/2022 21:51:37 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/30/2022 21:51:40 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
05/30/2022 21:51:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/30/2022 21:51:45 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/30/2022 21:51:46 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7508771929824561 on epoch=724
05/30/2022 21:51:48 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/30/2022 21:51:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/30/2022 21:51:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/30/2022 21:51:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/30/2022 21:51:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/30/2022 21:51:59 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7200878468954108 on epoch=737
05/30/2022 21:52:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=739
05/30/2022 21:52:04 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=742
05/30/2022 21:52:06 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/30/2022 21:52:09 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/30/2022 21:52:11 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/30/2022 21:52:13 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.7838872504537204 on epoch=749
05/30/2022 21:52:13 - INFO - __main__ - Saving model with best Classification-F1: 0.7529358901062755 -> 0.7838872504537204 on epoch=749, global_step=3000
05/30/2022 21:52:13 - INFO - __main__ - save last model!
05/30/2022 21:52:13 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 21:52:13 - INFO - __main__ - Printing 3 examples
05/30/2022 21:52:13 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/30/2022 21:52:13 - INFO - __main__ - ['sad']
05/30/2022 21:52:13 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/30/2022 21:52:13 - INFO - __main__ - ['sad']
05/30/2022 21:52:13 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/30/2022 21:52:13 - INFO - __main__ - ['sad']
05/30/2022 21:52:13 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:52:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 21:52:13 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:52:13 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 21:52:13 - INFO - __main__ - Printing 3 examples
05/30/2022 21:52:13 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 21:52:13 - INFO - __main__ - ['others']
05/30/2022 21:52:13 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 21:52:13 - INFO - __main__ - ['others']
05/30/2022 21:52:13 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 21:52:13 - INFO - __main__ - ['others']
05/30/2022 21:52:13 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:52:13 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 21:52:13 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 21:52:13 - INFO - __main__ - Printing 3 examples
05/30/2022 21:52:13 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/30/2022 21:52:13 - INFO - __main__ - ['sad']
05/30/2022 21:52:13 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/30/2022 21:52:13 - INFO - __main__ - ['sad']
05/30/2022 21:52:13 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/30/2022 21:52:13 - INFO - __main__ - ['sad']
05/30/2022 21:52:13 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:52:13 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:52:13 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 21:52:15 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:52:20 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 21:52:28 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 21:52:28 - INFO - __main__ - task name: emo
05/30/2022 21:52:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 21:52:29 - INFO - __main__ - Starting training!
05/30/2022 21:53:52 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_21_0.4_8_predictions.txt
05/30/2022 21:53:52 - INFO - __main__ - Classification-F1 on test data: 0.3867
05/30/2022 21:53:53 - INFO - __main__ - prefix=emo_16_21, lr=0.4, bsz=8, dev_performance=0.7838872504537204, test_performance=0.3867252946974218
05/30/2022 21:53:53 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.3, bsz=8 ...
05/30/2022 21:53:54 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 21:53:54 - INFO - __main__ - Printing 3 examples
05/30/2022 21:53:54 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/30/2022 21:53:54 - INFO - __main__ - ['sad']
05/30/2022 21:53:54 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/30/2022 21:53:54 - INFO - __main__ - ['sad']
05/30/2022 21:53:54 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/30/2022 21:53:54 - INFO - __main__ - ['sad']
05/30/2022 21:53:54 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:53:54 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:53:54 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 21:53:54 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 21:53:54 - INFO - __main__ - Printing 3 examples
05/30/2022 21:53:54 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/30/2022 21:53:54 - INFO - __main__ - ['sad']
05/30/2022 21:53:54 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/30/2022 21:53:54 - INFO - __main__ - ['sad']
05/30/2022 21:53:54 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/30/2022 21:53:54 - INFO - __main__ - ['sad']
05/30/2022 21:53:54 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:53:54 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:53:54 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 21:54:12 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 21:54:12 - INFO - __main__ - task name: emo
05/30/2022 21:54:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 21:54:13 - INFO - __main__ - Starting training!
05/30/2022 21:54:16 - INFO - __main__ - Step 10 Global step 10 Train loss 7.54 on epoch=2
05/30/2022 21:54:18 - INFO - __main__ - Step 20 Global step 20 Train loss 4.87 on epoch=4
05/30/2022 21:54:21 - INFO - __main__ - Step 30 Global step 30 Train loss 3.23 on epoch=7
05/30/2022 21:54:23 - INFO - __main__ - Step 40 Global step 40 Train loss 2.10 on epoch=9
05/30/2022 21:54:25 - INFO - __main__ - Step 50 Global step 50 Train loss 1.69 on epoch=12
05/30/2022 21:54:26 - INFO - __main__ - Global step 50 Train loss 3.89 Classification-F1 0.11861768368617684 on epoch=12
05/30/2022 21:54:26 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.11861768368617684 on epoch=12, global_step=50
05/30/2022 21:54:29 - INFO - __main__ - Step 60 Global step 60 Train loss 1.43 on epoch=14
05/30/2022 21:54:31 - INFO - __main__ - Step 70 Global step 70 Train loss 1.22 on epoch=17
05/30/2022 21:54:33 - INFO - __main__ - Step 80 Global step 80 Train loss 1.28 on epoch=19
05/30/2022 21:54:36 - INFO - __main__ - Step 90 Global step 90 Train loss 1.06 on epoch=22
05/30/2022 21:54:38 - INFO - __main__ - Step 100 Global step 100 Train loss 1.16 on epoch=24
05/30/2022 21:54:39 - INFO - __main__ - Global step 100 Train loss 1.23 Classification-F1 0.1795366795366795 on epoch=24
05/30/2022 21:54:39 - INFO - __main__ - Saving model with best Classification-F1: 0.11861768368617684 -> 0.1795366795366795 on epoch=24, global_step=100
05/30/2022 21:54:42 - INFO - __main__ - Step 110 Global step 110 Train loss 1.19 on epoch=27
05/30/2022 21:54:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.94 on epoch=29
05/30/2022 21:54:46 - INFO - __main__ - Step 130 Global step 130 Train loss 1.13 on epoch=32
05/30/2022 21:54:49 - INFO - __main__ - Step 140 Global step 140 Train loss 1.07 on epoch=34
05/30/2022 21:54:51 - INFO - __main__ - Step 150 Global step 150 Train loss 1.08 on epoch=37
05/30/2022 21:54:52 - INFO - __main__ - Global step 150 Train loss 1.08 Classification-F1 0.15412912912912913 on epoch=37
05/30/2022 21:54:55 - INFO - __main__ - Step 160 Global step 160 Train loss 1.05 on epoch=39
05/30/2022 21:54:57 - INFO - __main__ - Step 170 Global step 170 Train loss 1.12 on epoch=42
05/30/2022 21:54:59 - INFO - __main__ - Step 180 Global step 180 Train loss 1.00 on epoch=44
05/30/2022 21:55:02 - INFO - __main__ - Step 190 Global step 190 Train loss 1.06 on epoch=47
05/30/2022 21:55:04 - INFO - __main__ - Step 200 Global step 200 Train loss 1.01 on epoch=49
05/30/2022 21:55:05 - INFO - __main__ - Global step 200 Train loss 1.05 Classification-F1 0.27615910562088686 on epoch=49
05/30/2022 21:55:05 - INFO - __main__ - Saving model with best Classification-F1: 0.1795366795366795 -> 0.27615910562088686 on epoch=49, global_step=200
05/30/2022 21:55:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.96 on epoch=52
05/30/2022 21:55:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.90 on epoch=54
05/30/2022 21:55:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.89 on epoch=57
05/30/2022 21:55:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.91 on epoch=59
05/30/2022 21:55:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.94 on epoch=62
05/30/2022 21:55:18 - INFO - __main__ - Global step 250 Train loss 0.92 Classification-F1 0.2158482142857143 on epoch=62
05/30/2022 21:55:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.91 on epoch=64
05/30/2022 21:55:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.90 on epoch=67
05/30/2022 21:55:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.82 on epoch=69
05/30/2022 21:55:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.94 on epoch=72
05/30/2022 21:55:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.98 on epoch=74
05/30/2022 21:55:31 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.18308823529411763 on epoch=74
05/30/2022 21:55:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.94 on epoch=77
05/30/2022 21:55:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.88 on epoch=79
05/30/2022 21:55:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.92 on epoch=82
05/30/2022 21:55:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.87 on epoch=84
05/30/2022 21:55:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.79 on epoch=87
05/30/2022 21:55:44 - INFO - __main__ - Global step 350 Train loss 0.88 Classification-F1 0.13330786860198623 on epoch=87
05/30/2022 21:55:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.97 on epoch=89
05/30/2022 21:55:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.85 on epoch=92
05/30/2022 21:55:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.85 on epoch=94
05/30/2022 21:55:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.81 on epoch=97
05/30/2022 21:55:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.88 on epoch=99
05/30/2022 21:55:57 - INFO - __main__ - Global step 400 Train loss 0.87 Classification-F1 0.24421086160216596 on epoch=99
05/30/2022 21:55:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.83 on epoch=102
05/30/2022 21:56:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.97 on epoch=104
05/30/2022 21:56:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.82 on epoch=107
05/30/2022 21:56:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.88 on epoch=109
05/30/2022 21:56:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.90 on epoch=112
05/30/2022 21:56:10 - INFO - __main__ - Global step 450 Train loss 0.88 Classification-F1 0.17319032876308416 on epoch=112
05/30/2022 21:56:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.85 on epoch=114
05/30/2022 21:56:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.86 on epoch=117
05/30/2022 21:56:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.80 on epoch=119
05/30/2022 21:56:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.82 on epoch=122
05/30/2022 21:56:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.78 on epoch=124
05/30/2022 21:56:23 - INFO - __main__ - Global step 500 Train loss 0.82 Classification-F1 0.25549124788255223 on epoch=124
05/30/2022 21:56:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.78 on epoch=127
05/30/2022 21:56:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.72 on epoch=129
05/30/2022 21:56:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.89 on epoch=132
05/30/2022 21:56:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.84 on epoch=134
05/30/2022 21:56:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=137
05/30/2022 21:56:36 - INFO - __main__ - Global step 550 Train loss 0.81 Classification-F1 0.12546699875467 on epoch=137
05/30/2022 21:56:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.74 on epoch=139
05/30/2022 21:56:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.72 on epoch=142
05/30/2022 21:56:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.77 on epoch=144
05/30/2022 21:56:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.73 on epoch=147
05/30/2022 21:56:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.75 on epoch=149
05/30/2022 21:56:48 - INFO - __main__ - Global step 600 Train loss 0.74 Classification-F1 0.3243612078977932 on epoch=149
05/30/2022 21:56:49 - INFO - __main__ - Saving model with best Classification-F1: 0.27615910562088686 -> 0.3243612078977932 on epoch=149, global_step=600
05/30/2022 21:56:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.80 on epoch=152
05/30/2022 21:56:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.65 on epoch=154
05/30/2022 21:56:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.69 on epoch=157
05/30/2022 21:56:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.70 on epoch=159
05/30/2022 21:57:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.64 on epoch=162
05/30/2022 21:57:02 - INFO - __main__ - Global step 650 Train loss 0.69 Classification-F1 0.12635135135135137 on epoch=162
05/30/2022 21:57:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.77 on epoch=164
05/30/2022 21:57:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.81 on epoch=167
05/30/2022 21:57:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.71 on epoch=169
05/30/2022 21:57:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.71 on epoch=172
05/30/2022 21:57:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.65 on epoch=174
05/30/2022 21:57:14 - INFO - __main__ - Global step 700 Train loss 0.73 Classification-F1 0.40719768170426063 on epoch=174
05/30/2022 21:57:14 - INFO - __main__ - Saving model with best Classification-F1: 0.3243612078977932 -> 0.40719768170426063 on epoch=174, global_step=700
05/30/2022 21:57:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.82 on epoch=177
05/30/2022 21:57:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.74 on epoch=179
05/30/2022 21:57:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.76 on epoch=182
05/30/2022 21:57:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.76 on epoch=184
05/30/2022 21:57:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.80 on epoch=187
05/30/2022 21:57:27 - INFO - __main__ - Global step 750 Train loss 0.78 Classification-F1 0.17760603618782506 on epoch=187
05/30/2022 21:57:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.70 on epoch=189
05/30/2022 21:57:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.58 on epoch=192
05/30/2022 21:57:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.60 on epoch=194
05/30/2022 21:57:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.70 on epoch=197
05/30/2022 21:57:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=199
05/30/2022 21:57:40 - INFO - __main__ - Global step 800 Train loss 0.61 Classification-F1 0.49281135531135534 on epoch=199
05/30/2022 21:57:40 - INFO - __main__ - Saving model with best Classification-F1: 0.40719768170426063 -> 0.49281135531135534 on epoch=199, global_step=800
05/30/2022 21:57:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.54 on epoch=202
05/30/2022 21:57:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=204
05/30/2022 21:57:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.57 on epoch=207
05/30/2022 21:57:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.57 on epoch=209
05/30/2022 21:57:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=212
05/30/2022 21:57:53 - INFO - __main__ - Global step 850 Train loss 0.53 Classification-F1 0.45266557766557763 on epoch=212
05/30/2022 21:57:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=214
05/30/2022 21:57:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.56 on epoch=217
05/30/2022 21:58:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=219
05/30/2022 21:58:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=222
05/30/2022 21:58:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=224
05/30/2022 21:58:06 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.3889952153110048 on epoch=224
05/30/2022 21:58:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=227
05/30/2022 21:58:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.34 on epoch=229
05/30/2022 21:58:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=232
05/30/2022 21:58:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=234
05/30/2022 21:58:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.35 on epoch=237
05/30/2022 21:58:19 - INFO - __main__ - Global step 950 Train loss 0.34 Classification-F1 0.4407347554406378 on epoch=237
05/30/2022 21:58:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.32 on epoch=239
05/30/2022 21:58:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.32 on epoch=242
05/30/2022 21:58:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=244
05/30/2022 21:58:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=247
05/30/2022 21:58:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=249
05/30/2022 21:58:32 - INFO - __main__ - Global step 1000 Train loss 0.32 Classification-F1 0.588375350140056 on epoch=249
05/30/2022 21:58:32 - INFO - __main__ - Saving model with best Classification-F1: 0.49281135531135534 -> 0.588375350140056 on epoch=249, global_step=1000
05/30/2022 21:58:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.28 on epoch=252
05/30/2022 21:58:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=254
05/30/2022 21:58:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.25 on epoch=257
05/30/2022 21:58:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=259
05/30/2022 21:58:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=262
05/30/2022 21:58:46 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.550894584139265 on epoch=262
05/30/2022 21:58:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.15 on epoch=264
05/30/2022 21:58:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=267
05/30/2022 21:58:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=269
05/30/2022 21:58:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.32 on epoch=272
05/30/2022 21:58:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=274
05/30/2022 21:58:59 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.5913249341238471 on epoch=274
05/30/2022 21:58:59 - INFO - __main__ - Saving model with best Classification-F1: 0.588375350140056 -> 0.5913249341238471 on epoch=274, global_step=1100
05/30/2022 21:59:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=277
05/30/2022 21:59:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.15 on epoch=279
05/30/2022 21:59:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=282
05/30/2022 21:59:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=284
05/30/2022 21:59:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=287
05/30/2022 21:59:12 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.5850649350649351 on epoch=287
05/30/2022 21:59:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=289
05/30/2022 21:59:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=292
05/30/2022 21:59:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=294
05/30/2022 21:59:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=297
05/30/2022 21:59:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.13 on epoch=299
05/30/2022 21:59:25 - INFO - __main__ - Global step 1200 Train loss 0.14 Classification-F1 0.6351803539019963 on epoch=299
05/30/2022 21:59:25 - INFO - __main__ - Saving model with best Classification-F1: 0.5913249341238471 -> 0.6351803539019963 on epoch=299, global_step=1200
05/30/2022 21:59:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=302
05/30/2022 21:59:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.12 on epoch=304
05/30/2022 21:59:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=307
05/30/2022 21:59:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=309
05/30/2022 21:59:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=312
05/30/2022 21:59:38 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.5793783422459893 on epoch=312
05/30/2022 21:59:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.13 on epoch=314
05/30/2022 21:59:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.11 on epoch=317
05/30/2022 21:59:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=319
05/30/2022 21:59:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=322
05/30/2022 21:59:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=324
05/30/2022 21:59:51 - INFO - __main__ - Global step 1300 Train loss 0.10 Classification-F1 0.542317331791016 on epoch=324
05/30/2022 21:59:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=327
05/30/2022 21:59:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.09 on epoch=329
05/30/2022 21:59:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=332
05/30/2022 22:00:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.09 on epoch=334
05/30/2022 22:00:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=337
05/30/2022 22:00:04 - INFO - __main__ - Global step 1350 Train loss 0.12 Classification-F1 0.6636051373954599 on epoch=337
05/30/2022 22:00:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6351803539019963 -> 0.6636051373954599 on epoch=337, global_step=1350
05/30/2022 22:00:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.11 on epoch=339
05/30/2022 22:00:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
05/30/2022 22:00:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
05/30/2022 22:00:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
05/30/2022 22:00:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=349
05/30/2022 22:00:17 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.6779688460750155 on epoch=349
05/30/2022 22:00:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6636051373954599 -> 0.6779688460750155 on epoch=349, global_step=1400
05/30/2022 22:00:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=352
05/30/2022 22:00:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=354
05/30/2022 22:00:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=357
05/30/2022 22:00:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=359
05/30/2022 22:00:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.12 on epoch=362
05/30/2022 22:00:30 - INFO - __main__ - Global step 1450 Train loss 0.10 Classification-F1 0.6277700539265251 on epoch=362
05/30/2022 22:00:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=364
05/30/2022 22:00:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=367
05/30/2022 22:00:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/30/2022 22:00:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=372
05/30/2022 22:00:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=374
05/30/2022 22:00:43 - INFO - __main__ - Global step 1500 Train loss 0.08 Classification-F1 0.6128947368421053 on epoch=374
05/30/2022 22:00:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=377
05/30/2022 22:00:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=379
05/30/2022 22:00:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
05/30/2022 22:00:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=384
05/30/2022 22:00:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
05/30/2022 22:00:56 - INFO - __main__ - Global step 1550 Train loss 0.07 Classification-F1 0.6498525864379523 on epoch=387
05/30/2022 22:00:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
05/30/2022 22:01:01 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=392
05/30/2022 22:01:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
05/30/2022 22:01:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=397
05/30/2022 22:01:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/30/2022 22:01:09 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.684855943476633 on epoch=399
05/30/2022 22:01:09 - INFO - __main__ - Saving model with best Classification-F1: 0.6779688460750155 -> 0.684855943476633 on epoch=399, global_step=1600
05/30/2022 22:01:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=402
05/30/2022 22:01:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
05/30/2022 22:01:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
05/30/2022 22:01:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=409
05/30/2022 22:01:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=412
05/30/2022 22:01:23 - INFO - __main__ - Global step 1650 Train loss 0.07 Classification-F1 0.666256157635468 on epoch=412
05/30/2022 22:01:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
05/30/2022 22:01:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=417
05/30/2022 22:01:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/30/2022 22:01:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=422
05/30/2022 22:01:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=424
05/30/2022 22:01:36 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.667087542087542 on epoch=424
05/30/2022 22:01:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/30/2022 22:01:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
05/30/2022 22:01:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=432
05/30/2022 22:01:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
05/30/2022 22:01:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
05/30/2022 22:01:50 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.610393772893773 on epoch=437
05/30/2022 22:01:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/30/2022 22:01:54 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
05/30/2022 22:01:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
05/30/2022 22:01:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=447
05/30/2022 22:02:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/30/2022 22:02:03 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.6284992784992786 on epoch=449
05/30/2022 22:02:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=452
05/30/2022 22:02:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=454
05/30/2022 22:02:10 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=457
05/30/2022 22:02:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/30/2022 22:02:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
05/30/2022 22:02:16 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.6069965839786813 on epoch=462
05/30/2022 22:02:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/30/2022 22:02:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=467
05/30/2022 22:02:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=469
05/30/2022 22:02:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
05/30/2022 22:02:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.11 on epoch=474
05/30/2022 22:02:29 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.6976740376740377 on epoch=474
05/30/2022 22:02:29 - INFO - __main__ - Saving model with best Classification-F1: 0.684855943476633 -> 0.6976740376740377 on epoch=474, global_step=1900
05/30/2022 22:02:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=477
05/30/2022 22:02:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
05/30/2022 22:02:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=482
05/30/2022 22:02:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=484
05/30/2022 22:02:42 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=487
05/30/2022 22:02:43 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.6795598259012892 on epoch=487
05/30/2022 22:02:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/30/2022 22:02:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=492
05/30/2022 22:02:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/30/2022 22:02:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=497
05/30/2022 22:02:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
05/30/2022 22:02:56 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.6518401267144126 on epoch=499
05/30/2022 22:02:58 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=502
05/30/2022 22:03:01 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=504
05/30/2022 22:03:03 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/30/2022 22:03:06 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
05/30/2022 22:03:09 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/30/2022 22:03:10 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.613782991202346 on epoch=512
05/30/2022 22:03:12 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/30/2022 22:03:15 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=517
05/30/2022 22:03:17 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/30/2022 22:03:19 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/30/2022 22:03:22 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=524
05/30/2022 22:03:23 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.6876223689126915 on epoch=524
05/30/2022 22:03:25 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/30/2022 22:03:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/30/2022 22:03:30 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/30/2022 22:03:33 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=534
05/30/2022 22:03:35 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
05/30/2022 22:03:36 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.6710008741258741 on epoch=537
05/30/2022 22:03:39 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=539
05/30/2022 22:03:41 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/30/2022 22:03:43 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=544
05/30/2022 22:03:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.10 on epoch=547
05/30/2022 22:03:48 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/30/2022 22:03:49 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.7185067130300395 on epoch=549
05/30/2022 22:03:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6976740376740377 -> 0.7185067130300395 on epoch=549, global_step=2200
05/30/2022 22:03:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=552
05/30/2022 22:03:54 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
05/30/2022 22:03:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
05/30/2022 22:03:59 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/30/2022 22:04:02 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/30/2022 22:04:03 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7020050125313284 on epoch=562
05/30/2022 22:04:05 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/30/2022 22:04:08 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/30/2022 22:04:10 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=569
05/30/2022 22:04:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/30/2022 22:04:15 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/30/2022 22:04:17 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7040293040293042 on epoch=574
05/30/2022 22:04:19 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=577
05/30/2022 22:04:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=579
05/30/2022 22:04:24 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
05/30/2022 22:04:26 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
05/30/2022 22:04:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/30/2022 22:04:30 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.6286931818181818 on epoch=587
05/30/2022 22:04:33 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/30/2022 22:04:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=592
05/30/2022 22:04:37 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=594
05/30/2022 22:04:40 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=597
05/30/2022 22:04:42 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=599
05/30/2022 22:04:44 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.6809789672692899 on epoch=599
05/30/2022 22:04:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/30/2022 22:04:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
05/30/2022 22:04:51 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/30/2022 22:04:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/30/2022 22:04:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
05/30/2022 22:04:57 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.6560447454844007 on epoch=612
05/30/2022 22:04:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/30/2022 22:05:02 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/30/2022 22:05:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=619
05/30/2022 22:05:07 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/30/2022 22:05:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/30/2022 22:05:10 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.6690116995073891 on epoch=624
05/30/2022 22:05:13 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/30/2022 22:05:15 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/30/2022 22:05:18 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/30/2022 22:05:20 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/30/2022 22:05:23 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
05/30/2022 22:05:24 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6257479580460947 on epoch=637
05/30/2022 22:05:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/30/2022 22:05:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
05/30/2022 22:05:32 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/30/2022 22:05:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=647
05/30/2022 22:05:37 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/30/2022 22:05:38 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.7201213818860878 on epoch=649
05/30/2022 22:05:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7185067130300395 -> 0.7201213818860878 on epoch=649, global_step=2600
05/30/2022 22:05:40 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/30/2022 22:05:43 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/30/2022 22:05:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/30/2022 22:05:48 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/30/2022 22:05:50 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/30/2022 22:05:52 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7154761904761906 on epoch=662
05/30/2022 22:05:54 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=664
05/30/2022 22:05:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/30/2022 22:05:59 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
05/30/2022 22:06:01 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=672
05/30/2022 22:06:04 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/30/2022 22:06:05 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.705219298245614 on epoch=674
05/30/2022 22:06:07 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/30/2022 22:06:10 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/30/2022 22:06:12 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/30/2022 22:06:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/30/2022 22:06:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/30/2022 22:06:19 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6972942168188349 on epoch=687
05/30/2022 22:06:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/30/2022 22:06:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=692
05/30/2022 22:06:26 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=694
05/30/2022 22:06:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/30/2022 22:06:31 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/30/2022 22:06:32 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.6648767898767899 on epoch=699
05/30/2022 22:06:35 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/30/2022 22:06:37 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/30/2022 22:06:40 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/30/2022 22:06:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/30/2022 22:06:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/30/2022 22:06:46 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6709670545877443 on epoch=712
05/30/2022 22:06:49 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/30/2022 22:06:51 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/30/2022 22:06:54 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/30/2022 22:06:56 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/30/2022 22:06:59 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/30/2022 22:07:00 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.6834864940128098 on epoch=724
05/30/2022 22:07:02 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/30/2022 22:07:05 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
05/30/2022 22:07:07 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
05/30/2022 22:07:10 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
05/30/2022 22:07:12 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/30/2022 22:07:13 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6694372944372945 on epoch=737
05/30/2022 22:07:16 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/30/2022 22:07:18 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/30/2022 22:07:20 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/30/2022 22:07:23 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
05/30/2022 22:07:25 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/30/2022 22:07:26 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6680851610658379 on epoch=749
05/30/2022 22:07:26 - INFO - __main__ - save last model!
05/30/2022 22:07:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 22:07:26 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 22:07:26 - INFO - __main__ - Printing 3 examples
05/30/2022 22:07:26 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 22:07:26 - INFO - __main__ - ['others']
05/30/2022 22:07:26 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 22:07:26 - INFO - __main__ - ['others']
05/30/2022 22:07:26 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 22:07:26 - INFO - __main__ - ['others']
05/30/2022 22:07:26 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:07:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:07:26 - INFO - __main__ - Printing 3 examples
05/30/2022 22:07:26 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/30/2022 22:07:26 - INFO - __main__ - ['sad']
05/30/2022 22:07:26 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/30/2022 22:07:26 - INFO - __main__ - ['sad']
05/30/2022 22:07:26 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/30/2022 22:07:26 - INFO - __main__ - ['sad']
05/30/2022 22:07:26 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:07:26 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:07:27 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 22:07:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:07:27 - INFO - __main__ - Printing 3 examples
05/30/2022 22:07:27 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/30/2022 22:07:27 - INFO - __main__ - ['sad']
05/30/2022 22:07:27 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/30/2022 22:07:27 - INFO - __main__ - ['sad']
05/30/2022 22:07:27 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/30/2022 22:07:27 - INFO - __main__ - ['sad']
05/30/2022 22:07:27 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:07:27 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:07:27 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 22:07:29 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:07:34 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 22:07:45 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 22:07:45 - INFO - __main__ - task name: emo
05/30/2022 22:07:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 22:07:46 - INFO - __main__ - Starting training!
05/30/2022 22:09:01 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_21_0.3_8_predictions.txt
05/30/2022 22:09:01 - INFO - __main__ - Classification-F1 on test data: 0.3426
05/30/2022 22:09:01 - INFO - __main__ - prefix=emo_16_21, lr=0.3, bsz=8, dev_performance=0.7201213818860878, test_performance=0.34263654373760705
05/30/2022 22:09:01 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.2, bsz=8 ...
05/30/2022 22:09:02 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:09:02 - INFO - __main__ - Printing 3 examples
05/30/2022 22:09:02 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/30/2022 22:09:02 - INFO - __main__ - ['sad']
05/30/2022 22:09:02 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/30/2022 22:09:02 - INFO - __main__ - ['sad']
05/30/2022 22:09:02 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/30/2022 22:09:02 - INFO - __main__ - ['sad']
05/30/2022 22:09:02 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:09:02 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:09:02 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 22:09:02 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:09:02 - INFO - __main__ - Printing 3 examples
05/30/2022 22:09:02 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/30/2022 22:09:02 - INFO - __main__ - ['sad']
05/30/2022 22:09:02 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/30/2022 22:09:02 - INFO - __main__ - ['sad']
05/30/2022 22:09:02 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/30/2022 22:09:02 - INFO - __main__ - ['sad']
05/30/2022 22:09:02 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:09:02 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:09:02 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 22:09:21 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 22:09:21 - INFO - __main__ - task name: emo
05/30/2022 22:09:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 22:09:22 - INFO - __main__ - Starting training!
05/30/2022 22:09:24 - INFO - __main__ - Step 10 Global step 10 Train loss 7.89 on epoch=2
05/30/2022 22:09:27 - INFO - __main__ - Step 20 Global step 20 Train loss 6.84 on epoch=4
05/30/2022 22:09:29 - INFO - __main__ - Step 30 Global step 30 Train loss 4.86 on epoch=7
05/30/2022 22:09:32 - INFO - __main__ - Step 40 Global step 40 Train loss 3.35 on epoch=9
05/30/2022 22:09:34 - INFO - __main__ - Step 50 Global step 50 Train loss 2.51 on epoch=12
05/30/2022 22:09:35 - INFO - __main__ - Global step 50 Train loss 5.09 Classification-F1 0.1 on epoch=12
05/30/2022 22:09:35 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/30/2022 22:09:37 - INFO - __main__ - Step 60 Global step 60 Train loss 1.91 on epoch=14
05/30/2022 22:09:40 - INFO - __main__ - Step 70 Global step 70 Train loss 1.75 on epoch=17
05/30/2022 22:09:42 - INFO - __main__ - Step 80 Global step 80 Train loss 1.64 on epoch=19
05/30/2022 22:09:45 - INFO - __main__ - Step 90 Global step 90 Train loss 1.21 on epoch=22
05/30/2022 22:09:47 - INFO - __main__ - Step 100 Global step 100 Train loss 1.31 on epoch=24
05/30/2022 22:09:48 - INFO - __main__ - Global step 100 Train loss 1.56 Classification-F1 0.18535613397901535 on epoch=24
05/30/2022 22:09:48 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.18535613397901535 on epoch=24, global_step=100
05/30/2022 22:09:50 - INFO - __main__ - Step 110 Global step 110 Train loss 1.16 on epoch=27
05/30/2022 22:09:53 - INFO - __main__ - Step 120 Global step 120 Train loss 1.16 on epoch=29
05/30/2022 22:09:55 - INFO - __main__ - Step 130 Global step 130 Train loss 1.03 on epoch=32
05/30/2022 22:09:58 - INFO - __main__ - Step 140 Global step 140 Train loss 1.14 on epoch=34
05/30/2022 22:10:00 - INFO - __main__ - Step 150 Global step 150 Train loss 1.04 on epoch=37
05/30/2022 22:10:01 - INFO - __main__ - Global step 150 Train loss 1.11 Classification-F1 0.11330409356725146 on epoch=37
05/30/2022 22:10:03 - INFO - __main__ - Step 160 Global step 160 Train loss 1.00 on epoch=39
05/30/2022 22:10:06 - INFO - __main__ - Step 170 Global step 170 Train loss 1.00 on epoch=42
05/30/2022 22:10:08 - INFO - __main__ - Step 180 Global step 180 Train loss 1.15 on epoch=44
05/30/2022 22:10:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.99 on epoch=47
05/30/2022 22:10:13 - INFO - __main__ - Step 200 Global step 200 Train loss 1.02 on epoch=49
05/30/2022 22:10:14 - INFO - __main__ - Global step 200 Train loss 1.03 Classification-F1 0.10135135135135136 on epoch=49
05/30/2022 22:10:16 - INFO - __main__ - Step 210 Global step 210 Train loss 1.03 on epoch=52
05/30/2022 22:10:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.98 on epoch=54
05/30/2022 22:10:21 - INFO - __main__ - Step 230 Global step 230 Train loss 1.05 on epoch=57
05/30/2022 22:10:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.93 on epoch=59
05/30/2022 22:10:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.93 on epoch=62
05/30/2022 22:10:27 - INFO - __main__ - Global step 250 Train loss 0.98 Classification-F1 0.09615384615384615 on epoch=62
05/30/2022 22:10:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.86 on epoch=64
05/30/2022 22:10:32 - INFO - __main__ - Step 270 Global step 270 Train loss 1.02 on epoch=67
05/30/2022 22:10:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.92 on epoch=69
05/30/2022 22:10:37 - INFO - __main__ - Step 290 Global step 290 Train loss 1.03 on epoch=72
05/30/2022 22:10:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.91 on epoch=74
05/30/2022 22:10:40 - INFO - __main__ - Global step 300 Train loss 0.95 Classification-F1 0.13067758749069247 on epoch=74
05/30/2022 22:10:42 - INFO - __main__ - Step 310 Global step 310 Train loss 1.00 on epoch=77
05/30/2022 22:10:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.98 on epoch=79
05/30/2022 22:10:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.92 on epoch=82
05/30/2022 22:10:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.91 on epoch=84
05/30/2022 22:10:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.83 on epoch=87
05/30/2022 22:10:53 - INFO - __main__ - Global step 350 Train loss 0.93 Classification-F1 0.09615384615384615 on epoch=87
05/30/2022 22:10:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.91 on epoch=89
05/30/2022 22:10:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.90 on epoch=92
05/30/2022 22:11:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.88 on epoch=94
05/30/2022 22:11:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.94 on epoch=97
05/30/2022 22:11:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.85 on epoch=99
05/30/2022 22:11:06 - INFO - __main__ - Global step 400 Train loss 0.90 Classification-F1 0.16696230598669623 on epoch=99
05/30/2022 22:11:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.88 on epoch=102
05/30/2022 22:11:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.91 on epoch=104
05/30/2022 22:11:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.97 on epoch=107
05/30/2022 22:11:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.88 on epoch=109
05/30/2022 22:11:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.79 on epoch=112
05/30/2022 22:11:18 - INFO - __main__ - Global step 450 Train loss 0.88 Classification-F1 0.12518037518037517 on epoch=112
05/30/2022 22:11:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.84 on epoch=114
05/30/2022 22:11:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.97 on epoch=117
05/30/2022 22:11:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.87 on epoch=119
05/30/2022 22:11:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.82 on epoch=122
05/30/2022 22:11:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.84 on epoch=124
05/30/2022 22:11:31 - INFO - __main__ - Global step 500 Train loss 0.87 Classification-F1 0.15791680430970956 on epoch=124
05/30/2022 22:11:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.84 on epoch=127
05/30/2022 22:11:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.94 on epoch=129
05/30/2022 22:11:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.92 on epoch=132
05/30/2022 22:11:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.72 on epoch=134
05/30/2022 22:11:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.79 on epoch=137
05/30/2022 22:11:45 - INFO - __main__ - Global step 550 Train loss 0.84 Classification-F1 0.12393162393162392 on epoch=137
05/30/2022 22:11:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.80 on epoch=139
05/30/2022 22:11:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.89 on epoch=142
05/30/2022 22:11:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.90 on epoch=144
05/30/2022 22:11:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.85 on epoch=147
05/30/2022 22:11:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.81 on epoch=149
05/30/2022 22:11:58 - INFO - __main__ - Global step 600 Train loss 0.85 Classification-F1 0.23610278194236214 on epoch=149
05/30/2022 22:11:58 - INFO - __main__ - Saving model with best Classification-F1: 0.18535613397901535 -> 0.23610278194236214 on epoch=149, global_step=600
05/30/2022 22:12:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.80 on epoch=152
05/30/2022 22:12:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.78 on epoch=154
05/30/2022 22:12:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.85 on epoch=157
05/30/2022 22:12:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.79 on epoch=159
05/30/2022 22:12:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.78 on epoch=162
05/30/2022 22:12:11 - INFO - __main__ - Global step 650 Train loss 0.80 Classification-F1 0.15441176470588236 on epoch=162
05/30/2022 22:12:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.74 on epoch=164
05/30/2022 22:12:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.89 on epoch=167
05/30/2022 22:12:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.75 on epoch=169
05/30/2022 22:12:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.77 on epoch=172
05/30/2022 22:12:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.71 on epoch=174
05/30/2022 22:12:25 - INFO - __main__ - Global step 700 Train loss 0.77 Classification-F1 0.25 on epoch=174
05/30/2022 22:12:25 - INFO - __main__ - Saving model with best Classification-F1: 0.23610278194236214 -> 0.25 on epoch=174, global_step=700
05/30/2022 22:12:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.73 on epoch=177
05/30/2022 22:12:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.79 on epoch=179
05/30/2022 22:12:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.73 on epoch=182
05/30/2022 22:12:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.80 on epoch=184
05/30/2022 22:12:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.72 on epoch=187
05/30/2022 22:12:38 - INFO - __main__ - Global step 750 Train loss 0.75 Classification-F1 0.1566166493734536 on epoch=187
05/30/2022 22:12:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.74 on epoch=189
05/30/2022 22:12:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.76 on epoch=192
05/30/2022 22:12:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.63 on epoch=194
05/30/2022 22:12:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.74 on epoch=197
05/30/2022 22:12:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.76 on epoch=199
05/30/2022 22:12:51 - INFO - __main__ - Global step 800 Train loss 0.73 Classification-F1 0.21886214114832536 on epoch=199
05/30/2022 22:12:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.80 on epoch=202
05/30/2022 22:12:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.81 on epoch=204
05/30/2022 22:12:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.69 on epoch=207
05/30/2022 22:13:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.77 on epoch=209
05/30/2022 22:13:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.68 on epoch=212
05/30/2022 22:13:04 - INFO - __main__ - Global step 850 Train loss 0.75 Classification-F1 0.2026660466558165 on epoch=212
05/30/2022 22:13:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.63 on epoch=214
05/30/2022 22:13:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.65 on epoch=217
05/30/2022 22:13:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.74 on epoch=219
05/30/2022 22:13:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.64 on epoch=222
05/30/2022 22:13:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.65 on epoch=224
05/30/2022 22:13:17 - INFO - __main__ - Global step 900 Train loss 0.66 Classification-F1 0.3433208020050126 on epoch=224
05/30/2022 22:13:17 - INFO - __main__ - Saving model with best Classification-F1: 0.25 -> 0.3433208020050126 on epoch=224, global_step=900
05/30/2022 22:13:20 - INFO - __main__ - Step 910 Global step 910 Train loss 0.71 on epoch=227
05/30/2022 22:13:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.63 on epoch=229
05/30/2022 22:13:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.67 on epoch=232
05/30/2022 22:13:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.65 on epoch=234
05/30/2022 22:13:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.59 on epoch=237
05/30/2022 22:13:31 - INFO - __main__ - Global step 950 Train loss 0.65 Classification-F1 0.2501909854851031 on epoch=237
05/30/2022 22:13:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.62 on epoch=239
05/30/2022 22:13:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.70 on epoch=242
05/30/2022 22:13:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.68 on epoch=244
05/30/2022 22:13:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.68 on epoch=247
05/30/2022 22:13:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.56 on epoch=249
05/30/2022 22:13:44 - INFO - __main__ - Global step 1000 Train loss 0.65 Classification-F1 0.4512828407224959 on epoch=249
05/30/2022 22:13:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3433208020050126 -> 0.4512828407224959 on epoch=249, global_step=1000
05/30/2022 22:13:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.56 on epoch=252
05/30/2022 22:13:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.65 on epoch=254
05/30/2022 22:13:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.67 on epoch=257
05/30/2022 22:13:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.62 on epoch=259
05/30/2022 22:13:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.63 on epoch=262
05/30/2022 22:13:57 - INFO - __main__ - Global step 1050 Train loss 0.62 Classification-F1 0.35944761708768497 on epoch=262
05/30/2022 22:14:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.61 on epoch=264
05/30/2022 22:14:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.68 on epoch=267
05/30/2022 22:14:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.63 on epoch=269
05/30/2022 22:14:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.54 on epoch=272
05/30/2022 22:14:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.50 on epoch=274
05/30/2022 22:14:10 - INFO - __main__ - Global step 1100 Train loss 0.59 Classification-F1 0.41544898940543223 on epoch=274
05/30/2022 22:14:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.69 on epoch=277
05/30/2022 22:14:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.65 on epoch=279
05/30/2022 22:14:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.59 on epoch=282
05/30/2022 22:14:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.62 on epoch=284
05/30/2022 22:14:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.46 on epoch=287
05/30/2022 22:14:23 - INFO - __main__ - Global step 1150 Train loss 0.60 Classification-F1 0.40751879699248117 on epoch=287
05/30/2022 22:14:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.50 on epoch=289
05/30/2022 22:14:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.51 on epoch=292
05/30/2022 22:14:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.65 on epoch=294
05/30/2022 22:14:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.49 on epoch=297
05/30/2022 22:14:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.47 on epoch=299
05/30/2022 22:14:37 - INFO - __main__ - Global step 1200 Train loss 0.52 Classification-F1 0.5595999115826703 on epoch=299
05/30/2022 22:14:37 - INFO - __main__ - Saving model with best Classification-F1: 0.4512828407224959 -> 0.5595999115826703 on epoch=299, global_step=1200
05/30/2022 22:14:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.59 on epoch=302
05/30/2022 22:14:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=304
05/30/2022 22:14:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.49 on epoch=307
05/30/2022 22:14:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.45 on epoch=309
05/30/2022 22:14:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=312
05/30/2022 22:14:50 - INFO - __main__ - Global step 1250 Train loss 0.50 Classification-F1 0.5424622304232499 on epoch=312
05/30/2022 22:14:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.45 on epoch=314
05/30/2022 22:14:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.58 on epoch=317
05/30/2022 22:14:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.44 on epoch=319
05/30/2022 22:15:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=322
05/30/2022 22:15:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=324
05/30/2022 22:15:03 - INFO - __main__ - Global step 1300 Train loss 0.46 Classification-F1 0.5161513924671819 on epoch=324
05/30/2022 22:15:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.44 on epoch=327
05/30/2022 22:15:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.52 on epoch=329
05/30/2022 22:15:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=332
05/30/2022 22:15:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.33 on epoch=334
05/30/2022 22:15:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.47 on epoch=337
05/30/2022 22:15:16 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.4908694246929541 on epoch=337
05/30/2022 22:15:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=339
05/30/2022 22:15:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=342
05/30/2022 22:15:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=344
05/30/2022 22:15:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.54 on epoch=347
05/30/2022 22:15:29 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=349
05/30/2022 22:15:29 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.48252259436469963 on epoch=349
05/30/2022 22:15:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=352
05/30/2022 22:15:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.35 on epoch=354
05/30/2022 22:15:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.32 on epoch=357
05/30/2022 22:15:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.29 on epoch=359
05/30/2022 22:15:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.27 on epoch=362
05/30/2022 22:15:43 - INFO - __main__ - Global step 1450 Train loss 0.33 Classification-F1 0.48923633969716923 on epoch=362
05/30/2022 22:15:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=364
05/30/2022 22:15:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.29 on epoch=367
05/30/2022 22:15:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=369
05/30/2022 22:15:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.38 on epoch=372
05/30/2022 22:15:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=374
05/30/2022 22:15:56 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.5393746870728968 on epoch=374
05/30/2022 22:15:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=377
05/30/2022 22:16:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.29 on epoch=379
05/30/2022 22:16:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.31 on epoch=382
05/30/2022 22:16:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.25 on epoch=384
05/30/2022 22:16:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=387
05/30/2022 22:16:08 - INFO - __main__ - Global step 1550 Train loss 0.29 Classification-F1 0.481478578892372 on epoch=387
05/30/2022 22:16:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=389
05/30/2022 22:16:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.30 on epoch=392
05/30/2022 22:16:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=394
05/30/2022 22:16:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.32 on epoch=397
05/30/2022 22:16:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.35 on epoch=399
05/30/2022 22:16:21 - INFO - __main__ - Global step 1600 Train loss 0.28 Classification-F1 0.6390595140595141 on epoch=399
05/30/2022 22:16:21 - INFO - __main__ - Saving model with best Classification-F1: 0.5595999115826703 -> 0.6390595140595141 on epoch=399, global_step=1600
05/30/2022 22:16:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.27 on epoch=402
05/30/2022 22:16:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=404
05/30/2022 22:16:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=407
05/30/2022 22:16:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=409
05/30/2022 22:16:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=412
05/30/2022 22:16:34 - INFO - __main__ - Global step 1650 Train loss 0.24 Classification-F1 0.5133600281897683 on epoch=412
05/30/2022 22:16:37 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=414
05/30/2022 22:16:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.20 on epoch=417
05/30/2022 22:16:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=419
05/30/2022 22:16:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=422
05/30/2022 22:16:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=424
05/30/2022 22:16:47 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.594180343331447 on epoch=424
05/30/2022 22:16:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=427
05/30/2022 22:16:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=429
05/30/2022 22:16:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.19 on epoch=432
05/30/2022 22:16:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=434
05/30/2022 22:16:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=437
05/30/2022 22:17:00 - INFO - __main__ - Global step 1750 Train loss 0.18 Classification-F1 0.6138143921763739 on epoch=437
05/30/2022 22:17:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.13 on epoch=439
05/30/2022 22:17:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.24 on epoch=442
05/30/2022 22:17:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.23 on epoch=444
05/30/2022 22:17:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=447
05/30/2022 22:17:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=449
05/30/2022 22:17:13 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.621515002927955 on epoch=449
05/30/2022 22:17:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.25 on epoch=452
05/30/2022 22:17:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=454
05/30/2022 22:17:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.13 on epoch=457
05/30/2022 22:17:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=459
05/30/2022 22:17:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=462
05/30/2022 22:17:26 - INFO - __main__ - Global step 1850 Train loss 0.15 Classification-F1 0.6700980392156862 on epoch=462
05/30/2022 22:17:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6390595140595141 -> 0.6700980392156862 on epoch=462, global_step=1850
05/30/2022 22:17:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.17 on epoch=464
05/30/2022 22:17:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.16 on epoch=467
05/30/2022 22:17:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=469
05/30/2022 22:17:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.12 on epoch=472
05/30/2022 22:17:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=474
05/30/2022 22:17:39 - INFO - __main__ - Global step 1900 Train loss 0.13 Classification-F1 0.6335443292339844 on epoch=474
05/30/2022 22:17:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.27 on epoch=477
05/30/2022 22:17:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=479
05/30/2022 22:17:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.13 on epoch=482
05/30/2022 22:17:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.10 on epoch=484
05/30/2022 22:17:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=487
05/30/2022 22:17:52 - INFO - __main__ - Global step 1950 Train loss 0.12 Classification-F1 0.6594272844272844 on epoch=487
05/30/2022 22:17:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=489
05/30/2022 22:17:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=492
05/30/2022 22:18:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=494
05/30/2022 22:18:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=497
05/30/2022 22:18:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=499
05/30/2022 22:18:05 - INFO - __main__ - Global step 2000 Train loss 0.10 Classification-F1 0.6424470367057575 on epoch=499
05/30/2022 22:18:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.10 on epoch=502
05/30/2022 22:18:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=504
05/30/2022 22:18:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
05/30/2022 22:18:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
05/30/2022 22:18:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=512
05/30/2022 22:18:18 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.637982387982388 on epoch=512
05/30/2022 22:18:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.18 on epoch=514
05/30/2022 22:18:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.13 on epoch=517
05/30/2022 22:18:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.11 on epoch=519
05/30/2022 22:18:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=522
05/30/2022 22:18:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.11 on epoch=524
05/30/2022 22:18:32 - INFO - __main__ - Global step 2100 Train loss 0.13 Classification-F1 0.566297428062134 on epoch=524
05/30/2022 22:18:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.11 on epoch=527
05/30/2022 22:18:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=529
05/30/2022 22:18:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.12 on epoch=532
05/30/2022 22:18:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
05/30/2022 22:18:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=537
05/30/2022 22:18:45 - INFO - __main__ - Global step 2150 Train loss 0.08 Classification-F1 0.6713599299806197 on epoch=537
05/30/2022 22:18:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6700980392156862 -> 0.6713599299806197 on epoch=537, global_step=2150
05/30/2022 22:18:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=539
05/30/2022 22:18:49 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.09 on epoch=542
05/30/2022 22:18:52 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.08 on epoch=544
05/30/2022 22:18:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=547
05/30/2022 22:18:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=549
05/30/2022 22:18:58 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.6051266928876037 on epoch=549
05/30/2022 22:19:00 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/30/2022 22:19:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
05/30/2022 22:19:05 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=557
05/30/2022 22:19:08 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.10 on epoch=559
05/30/2022 22:19:10 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.09 on epoch=562
05/30/2022 22:19:11 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.5876623376623377 on epoch=562
05/30/2022 22:19:14 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.09 on epoch=564
05/30/2022 22:19:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.09 on epoch=567
05/30/2022 22:19:18 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.09 on epoch=569
05/30/2022 22:19:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.13 on epoch=572
05/30/2022 22:19:23 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=574
05/30/2022 22:19:24 - INFO - __main__ - Global step 2300 Train loss 0.09 Classification-F1 0.6310785133861794 on epoch=574
05/30/2022 22:19:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=577
05/30/2022 22:19:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=579
05/30/2022 22:19:31 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=582
05/30/2022 22:19:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=584
05/30/2022 22:19:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=587
05/30/2022 22:19:37 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.6040927739090285 on epoch=587
05/30/2022 22:19:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=589
05/30/2022 22:19:42 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=592
05/30/2022 22:19:44 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=594
05/30/2022 22:19:47 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=597
05/30/2022 22:19:49 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=599
05/30/2022 22:19:50 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.6373954599761051 on epoch=599
05/30/2022 22:19:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
05/30/2022 22:19:55 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.14 on epoch=604
05/30/2022 22:19:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=607
05/30/2022 22:20:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
05/30/2022 22:20:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
05/30/2022 22:20:03 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.6319499341238471 on epoch=612
05/30/2022 22:20:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
05/30/2022 22:20:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
05/30/2022 22:20:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=619
05/30/2022 22:20:13 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=622
05/30/2022 22:20:15 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=624
05/30/2022 22:20:16 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.7042471042471043 on epoch=624
05/30/2022 22:20:16 - INFO - __main__ - Saving model with best Classification-F1: 0.6713599299806197 -> 0.7042471042471043 on epoch=624, global_step=2500
05/30/2022 22:20:19 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
05/30/2022 22:20:21 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=629
05/30/2022 22:20:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.09 on epoch=632
05/30/2022 22:20:26 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=634
05/30/2022 22:20:28 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=637
05/30/2022 22:20:29 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.6742656449553002 on epoch=637
05/30/2022 22:20:32 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=639
05/30/2022 22:20:34 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=642
05/30/2022 22:20:36 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=644
05/30/2022 22:20:39 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.06 on epoch=647
05/30/2022 22:20:41 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=649
05/30/2022 22:20:42 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.7345503958407184 on epoch=649
05/30/2022 22:20:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7042471042471043 -> 0.7345503958407184 on epoch=649, global_step=2600
05/30/2022 22:20:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.08 on epoch=652
05/30/2022 22:20:47 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=654
05/30/2022 22:20:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
05/30/2022 22:20:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=659
05/30/2022 22:20:54 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=662
05/30/2022 22:20:55 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.6560365853658537 on epoch=662
05/30/2022 22:20:58 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=664
05/30/2022 22:21:00 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.13 on epoch=667
05/30/2022 22:21:03 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=669
05/30/2022 22:21:05 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=672
05/30/2022 22:21:07 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
05/30/2022 22:21:08 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.7181712962962963 on epoch=674
05/30/2022 22:21:11 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=677
05/30/2022 22:21:13 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=679
05/30/2022 22:21:16 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=682
05/30/2022 22:21:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/30/2022 22:21:21 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
05/30/2022 22:21:22 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.6592054887987566 on epoch=687
05/30/2022 22:21:24 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=689
05/30/2022 22:21:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
05/30/2022 22:21:29 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=694
05/30/2022 22:21:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/30/2022 22:21:34 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/30/2022 22:21:35 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.6851457688338494 on epoch=699
05/30/2022 22:21:37 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
05/30/2022 22:21:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=704
05/30/2022 22:21:42 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=707
05/30/2022 22:21:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
05/30/2022 22:21:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.14 on epoch=712
05/30/2022 22:21:48 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.6888489782886335 on epoch=712
05/30/2022 22:21:50 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=714
05/30/2022 22:21:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
05/30/2022 22:21:55 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
05/30/2022 22:21:57 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
05/30/2022 22:22:00 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
05/30/2022 22:22:01 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.7160195910195909 on epoch=724
05/30/2022 22:22:03 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
05/30/2022 22:22:06 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=729
05/30/2022 22:22:08 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
05/30/2022 22:22:10 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=734
05/30/2022 22:22:13 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=737
05/30/2022 22:22:14 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.6402432712215321 on epoch=737
05/30/2022 22:22:16 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
05/30/2022 22:22:19 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.09 on epoch=742
05/30/2022 22:22:21 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=744
05/30/2022 22:22:24 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/30/2022 22:22:26 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
05/30/2022 22:22:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:22:27 - INFO - __main__ - Printing 3 examples
05/30/2022 22:22:27 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/30/2022 22:22:27 - INFO - __main__ - ['happy']
05/30/2022 22:22:27 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/30/2022 22:22:27 - INFO - __main__ - ['happy']
05/30/2022 22:22:27 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/30/2022 22:22:27 - INFO - __main__ - ['happy']
05/30/2022 22:22:27 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:22:27 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.7196704913438785 on epoch=749
05/30/2022 22:22:27 - INFO - __main__ - save last model!
05/30/2022 22:22:27 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:22:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 22:22:27 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 22:22:27 - INFO - __main__ - Printing 3 examples
05/30/2022 22:22:27 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 22:22:27 - INFO - __main__ - ['others']
05/30/2022 22:22:27 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 22:22:27 - INFO - __main__ - ['others']
05/30/2022 22:22:27 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 22:22:27 - INFO - __main__ - ['others']
05/30/2022 22:22:27 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:22:27 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 22:22:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:22:27 - INFO - __main__ - Printing 3 examples
05/30/2022 22:22:27 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/30/2022 22:22:27 - INFO - __main__ - ['happy']
05/30/2022 22:22:27 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/30/2022 22:22:27 - INFO - __main__ - ['happy']
05/30/2022 22:22:27 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/30/2022 22:22:27 - INFO - __main__ - ['happy']
05/30/2022 22:22:27 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:22:27 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:22:27 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 22:22:29 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:22:35 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 22:22:42 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 22:22:42 - INFO - __main__ - task name: emo
05/30/2022 22:22:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 22:22:43 - INFO - __main__ - Starting training!
05/30/2022 22:24:06 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_21_0.2_8_predictions.txt
05/30/2022 22:24:06 - INFO - __main__ - Classification-F1 on test data: 0.3348
05/30/2022 22:24:06 - INFO - __main__ - prefix=emo_16_21, lr=0.2, bsz=8, dev_performance=0.7345503958407184, test_performance=0.33480295501953383
05/30/2022 22:24:06 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.5, bsz=8 ...
05/30/2022 22:24:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:24:07 - INFO - __main__ - Printing 3 examples
05/30/2022 22:24:07 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/30/2022 22:24:07 - INFO - __main__ - ['happy']
05/30/2022 22:24:07 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/30/2022 22:24:07 - INFO - __main__ - ['happy']
05/30/2022 22:24:07 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/30/2022 22:24:07 - INFO - __main__ - ['happy']
05/30/2022 22:24:07 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:24:07 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:24:07 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 22:24:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:24:07 - INFO - __main__ - Printing 3 examples
05/30/2022 22:24:07 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/30/2022 22:24:07 - INFO - __main__ - ['happy']
05/30/2022 22:24:07 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/30/2022 22:24:07 - INFO - __main__ - ['happy']
05/30/2022 22:24:07 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/30/2022 22:24:07 - INFO - __main__ - ['happy']
05/30/2022 22:24:07 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:24:07 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:24:07 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 22:24:26 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 22:24:26 - INFO - __main__ - task name: emo
05/30/2022 22:24:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 22:24:27 - INFO - __main__ - Starting training!
05/30/2022 22:24:29 - INFO - __main__ - Step 10 Global step 10 Train loss 7.16 on epoch=2
05/30/2022 22:24:32 - INFO - __main__ - Step 20 Global step 20 Train loss 3.25 on epoch=4
05/30/2022 22:24:34 - INFO - __main__ - Step 30 Global step 30 Train loss 1.46 on epoch=7
05/30/2022 22:24:37 - INFO - __main__ - Step 40 Global step 40 Train loss 1.35 on epoch=9
05/30/2022 22:24:39 - INFO - __main__ - Step 50 Global step 50 Train loss 1.19 on epoch=12
05/30/2022 22:24:40 - INFO - __main__ - Global step 50 Train loss 2.88 Classification-F1 0.1581196581196581 on epoch=12
05/30/2022 22:24:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1581196581196581 on epoch=12, global_step=50
05/30/2022 22:24:42 - INFO - __main__ - Step 60 Global step 60 Train loss 1.05 on epoch=14
05/30/2022 22:24:45 - INFO - __main__ - Step 70 Global step 70 Train loss 1.01 on epoch=17
05/30/2022 22:24:47 - INFO - __main__ - Step 80 Global step 80 Train loss 0.93 on epoch=19
05/30/2022 22:24:50 - INFO - __main__ - Step 90 Global step 90 Train loss 1.02 on epoch=22
05/30/2022 22:24:52 - INFO - __main__ - Step 100 Global step 100 Train loss 1.02 on epoch=24
05/30/2022 22:24:53 - INFO - __main__ - Global step 100 Train loss 1.00 Classification-F1 0.1 on epoch=24
05/30/2022 22:24:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.93 on epoch=27
05/30/2022 22:24:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.98 on epoch=29
05/30/2022 22:25:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.96 on epoch=32
05/30/2022 22:25:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.96 on epoch=34
05/30/2022 22:25:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.96 on epoch=37
05/30/2022 22:25:06 - INFO - __main__ - Global step 150 Train loss 0.96 Classification-F1 0.15135135135135136 on epoch=37
05/30/2022 22:25:08 - INFO - __main__ - Step 160 Global step 160 Train loss 0.94 on epoch=39
05/30/2022 22:25:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.91 on epoch=42
05/30/2022 22:25:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.90 on epoch=44
05/30/2022 22:25:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.89 on epoch=47
05/30/2022 22:25:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.94 on epoch=49
05/30/2022 22:25:19 - INFO - __main__ - Global step 200 Train loss 0.91 Classification-F1 0.1 on epoch=49
05/30/2022 22:25:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.89 on epoch=52
05/30/2022 22:25:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.89 on epoch=54
05/30/2022 22:25:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.89 on epoch=57
05/30/2022 22:25:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.85 on epoch=59
05/30/2022 22:25:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.86 on epoch=62
05/30/2022 22:25:32 - INFO - __main__ - Global step 250 Train loss 0.88 Classification-F1 0.1581196581196581 on epoch=62
05/30/2022 22:25:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.81 on epoch=64
05/30/2022 22:25:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.82 on epoch=67
05/30/2022 22:25:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.83 on epoch=69
05/30/2022 22:25:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.95 on epoch=72
05/30/2022 22:25:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.86 on epoch=74
05/30/2022 22:25:44 - INFO - __main__ - Global step 300 Train loss 0.85 Classification-F1 0.2882175412663217 on epoch=74
05/30/2022 22:25:44 - INFO - __main__ - Saving model with best Classification-F1: 0.1581196581196581 -> 0.2882175412663217 on epoch=74, global_step=300
05/30/2022 22:25:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.88 on epoch=77
05/30/2022 22:25:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.92 on epoch=79
05/30/2022 22:25:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.78 on epoch=82
05/30/2022 22:25:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.88 on epoch=84
05/30/2022 22:25:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.82 on epoch=87
05/30/2022 22:25:57 - INFO - __main__ - Global step 350 Train loss 0.85 Classification-F1 0.3886262513904338 on epoch=87
05/30/2022 22:25:57 - INFO - __main__ - Saving model with best Classification-F1: 0.2882175412663217 -> 0.3886262513904338 on epoch=87, global_step=350
05/30/2022 22:26:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.90 on epoch=89
05/30/2022 22:26:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.83 on epoch=92
05/30/2022 22:26:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.84 on epoch=94
05/30/2022 22:26:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.82 on epoch=97
05/30/2022 22:26:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.91 on epoch=99
05/30/2022 22:26:10 - INFO - __main__ - Global step 400 Train loss 0.86 Classification-F1 0.19186446508427935 on epoch=99
05/30/2022 22:26:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.79 on epoch=102
05/30/2022 22:26:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.83 on epoch=104
05/30/2022 22:26:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.72 on epoch=107
05/30/2022 22:26:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.82 on epoch=109
05/30/2022 22:26:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.77 on epoch=112
05/30/2022 22:26:23 - INFO - __main__ - Global step 450 Train loss 0.79 Classification-F1 0.41828236824983733 on epoch=112
05/30/2022 22:26:23 - INFO - __main__ - Saving model with best Classification-F1: 0.3886262513904338 -> 0.41828236824983733 on epoch=112, global_step=450
05/30/2022 22:26:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.87 on epoch=114
05/30/2022 22:26:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.76 on epoch=117
05/30/2022 22:26:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.75 on epoch=119
05/30/2022 22:26:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.70 on epoch=122
05/30/2022 22:26:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.74 on epoch=124
05/30/2022 22:26:36 - INFO - __main__ - Global step 500 Train loss 0.77 Classification-F1 0.47820845216489505 on epoch=124
05/30/2022 22:26:36 - INFO - __main__ - Saving model with best Classification-F1: 0.41828236824983733 -> 0.47820845216489505 on epoch=124, global_step=500
05/30/2022 22:26:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.74 on epoch=127
05/30/2022 22:26:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.69 on epoch=129
05/30/2022 22:26:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.71 on epoch=132
05/30/2022 22:26:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.64 on epoch=134
05/30/2022 22:26:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.70 on epoch=137
05/30/2022 22:26:49 - INFO - __main__ - Global step 550 Train loss 0.69 Classification-F1 0.4475525395197138 on epoch=137
05/30/2022 22:26:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.66 on epoch=139
05/30/2022 22:26:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.60 on epoch=142
05/30/2022 22:26:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.65 on epoch=144
05/30/2022 22:26:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.61 on epoch=147
05/30/2022 22:27:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.69 on epoch=149
05/30/2022 22:27:02 - INFO - __main__ - Global step 600 Train loss 0.64 Classification-F1 0.7292626728110599 on epoch=149
05/30/2022 22:27:02 - INFO - __main__ - Saving model with best Classification-F1: 0.47820845216489505 -> 0.7292626728110599 on epoch=149, global_step=600
05/30/2022 22:27:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.69 on epoch=152
05/30/2022 22:27:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.68 on epoch=154
05/30/2022 22:27:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.52 on epoch=157
05/30/2022 22:27:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.59 on epoch=159
05/30/2022 22:27:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.61 on epoch=162
05/30/2022 22:27:15 - INFO - __main__ - Global step 650 Train loss 0.62 Classification-F1 0.5539734883113491 on epoch=162
05/30/2022 22:27:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=164
05/30/2022 22:27:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.69 on epoch=167
05/30/2022 22:27:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=169
05/30/2022 22:27:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.54 on epoch=172
05/30/2022 22:27:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.52 on epoch=174
05/30/2022 22:27:28 - INFO - __main__ - Global step 700 Train loss 0.56 Classification-F1 0.6764488808227466 on epoch=174
05/30/2022 22:27:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=177
05/30/2022 22:27:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.56 on epoch=179
05/30/2022 22:27:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.45 on epoch=182
05/30/2022 22:27:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.54 on epoch=184
05/30/2022 22:27:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=187
05/30/2022 22:27:41 - INFO - __main__ - Global step 750 Train loss 0.50 Classification-F1 0.6979108691268401 on epoch=187
05/30/2022 22:27:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=189
05/30/2022 22:27:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=192
05/30/2022 22:27:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=194
05/30/2022 22:27:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=197
05/30/2022 22:27:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=199
05/30/2022 22:27:54 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.6908977943460701 on epoch=199
05/30/2022 22:27:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.27 on epoch=202
05/30/2022 22:27:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=204
05/30/2022 22:28:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=207
05/30/2022 22:28:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=209
05/30/2022 22:28:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=212
05/30/2022 22:28:07 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.6742063492063493 on epoch=212
05/30/2022 22:28:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.33 on epoch=214
05/30/2022 22:28:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.35 on epoch=217
05/30/2022 22:28:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=219
05/30/2022 22:28:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.19 on epoch=222
05/30/2022 22:28:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=224
05/30/2022 22:28:20 - INFO - __main__ - Global step 900 Train loss 0.29 Classification-F1 0.7236636178861788 on epoch=224
05/30/2022 22:28:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=227
05/30/2022 22:28:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=229
05/30/2022 22:28:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=232
05/30/2022 22:28:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=234
05/30/2022 22:28:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=237
05/30/2022 22:28:33 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.7106328154604017 on epoch=237
05/30/2022 22:28:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.16 on epoch=239
05/30/2022 22:28:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.17 on epoch=242
05/30/2022 22:28:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=244
05/30/2022 22:28:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=247
05/30/2022 22:28:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.26 on epoch=249
05/30/2022 22:28:46 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.7452749301025163 on epoch=249
05/30/2022 22:28:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7292626728110599 -> 0.7452749301025163 on epoch=249, global_step=1000
05/30/2022 22:28:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=252
05/30/2022 22:28:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.16 on epoch=254
05/30/2022 22:28:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=257
05/30/2022 22:28:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=259
05/30/2022 22:28:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.17 on epoch=262
05/30/2022 22:28:59 - INFO - __main__ - Global step 1050 Train loss 0.18 Classification-F1 0.6524702908681715 on epoch=262
05/30/2022 22:29:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.17 on epoch=264
05/30/2022 22:29:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=267
05/30/2022 22:29:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=269
05/30/2022 22:29:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=272
05/30/2022 22:29:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.14 on epoch=274
05/30/2022 22:29:12 - INFO - __main__ - Global step 1100 Train loss 0.16 Classification-F1 0.7444444444444444 on epoch=274
05/30/2022 22:29:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.12 on epoch=277
05/30/2022 22:29:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=279
05/30/2022 22:29:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.17 on epoch=282
05/30/2022 22:29:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=284
05/30/2022 22:29:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.14 on epoch=287
05/30/2022 22:29:25 - INFO - __main__ - Global step 1150 Train loss 0.14 Classification-F1 0.763892489845303 on epoch=287
05/30/2022 22:29:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7452749301025163 -> 0.763892489845303 on epoch=287, global_step=1150
05/30/2022 22:29:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.27 on epoch=289
05/30/2022 22:29:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=292
05/30/2022 22:29:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=294
05/30/2022 22:29:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.12 on epoch=297
05/30/2022 22:29:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.13 on epoch=299
05/30/2022 22:29:38 - INFO - __main__ - Global step 1200 Train loss 0.16 Classification-F1 0.6858001265022138 on epoch=299
05/30/2022 22:29:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=302
05/30/2022 22:29:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=304
05/30/2022 22:29:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=307
05/30/2022 22:29:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=309
05/30/2022 22:29:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=312
05/30/2022 22:29:51 - INFO - __main__ - Global step 1250 Train loss 0.09 Classification-F1 0.7166818688557819 on epoch=312
05/30/2022 22:29:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=314
05/30/2022 22:29:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.14 on epoch=317
05/30/2022 22:29:59 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=319
05/30/2022 22:30:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=322
05/30/2022 22:30:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=324
05/30/2022 22:30:05 - INFO - __main__ - Global step 1300 Train loss 0.10 Classification-F1 0.7157581453634085 on epoch=324
05/30/2022 22:30:07 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=327
05/30/2022 22:30:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=329
05/30/2022 22:30:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=332
05/30/2022 22:30:14 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=334
05/30/2022 22:30:17 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
05/30/2022 22:30:18 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.7134836209206918 on epoch=337
05/30/2022 22:30:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=339
05/30/2022 22:30:22 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=342
05/30/2022 22:30:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
05/30/2022 22:30:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.08 on epoch=347
05/30/2022 22:30:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=349
05/30/2022 22:30:31 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.723740525523208 on epoch=349
05/30/2022 22:30:33 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=352
05/30/2022 22:30:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.07 on epoch=354
05/30/2022 22:30:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=357
05/30/2022 22:30:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=359
05/30/2022 22:30:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/30/2022 22:30:44 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.7651745495495496 on epoch=362
05/30/2022 22:30:44 - INFO - __main__ - Saving model with best Classification-F1: 0.763892489845303 -> 0.7651745495495496 on epoch=362, global_step=1450
05/30/2022 22:30:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
05/30/2022 22:30:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
05/30/2022 22:30:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
05/30/2022 22:30:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
05/30/2022 22:30:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
05/30/2022 22:30:57 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.7611897611897612 on epoch=374
05/30/2022 22:30:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
05/30/2022 22:31:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/30/2022 22:31:04 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=382
05/30/2022 22:31:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
05/30/2022 22:31:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=387
05/30/2022 22:31:10 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.6513373205223119 on epoch=387
05/30/2022 22:31:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
05/30/2022 22:31:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/30/2022 22:31:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
05/30/2022 22:31:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=397
05/30/2022 22:31:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=399
05/30/2022 22:31:23 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.620940170940171 on epoch=399
05/30/2022 22:31:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=402
05/30/2022 22:31:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
05/30/2022 22:31:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
05/30/2022 22:31:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/30/2022 22:31:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/30/2022 22:31:36 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7195212165227157 on epoch=412
05/30/2022 22:31:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
05/30/2022 22:31:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.08 on epoch=417
05/30/2022 22:31:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=419
05/30/2022 22:31:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
05/30/2022 22:31:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/30/2022 22:31:49 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.7314491911266106 on epoch=424
05/30/2022 22:31:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
05/30/2022 22:31:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
05/30/2022 22:31:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/30/2022 22:31:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/30/2022 22:32:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=437
05/30/2022 22:32:02 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.6804292929292929 on epoch=437
05/30/2022 22:32:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/30/2022 22:32:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.27 on epoch=442
05/30/2022 22:32:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
05/30/2022 22:32:12 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=447
05/30/2022 22:32:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=449
05/30/2022 22:32:15 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.7153619411683927 on epoch=449
05/30/2022 22:32:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=452
05/30/2022 22:32:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/30/2022 22:32:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
05/30/2022 22:32:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/30/2022 22:32:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=462
05/30/2022 22:32:28 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.6879913255624831 on epoch=462
05/30/2022 22:32:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/30/2022 22:32:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=467
05/30/2022 22:32:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
05/30/2022 22:32:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=472
05/30/2022 22:32:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/30/2022 22:32:41 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.674066924066924 on epoch=474
05/30/2022 22:32:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
05/30/2022 22:32:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/30/2022 22:32:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/30/2022 22:32:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
05/30/2022 22:32:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
05/30/2022 22:32:55 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.690029761904762 on epoch=487
05/30/2022 22:32:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=489
05/30/2022 22:33:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/30/2022 22:33:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/30/2022 22:33:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/30/2022 22:33:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/30/2022 22:33:08 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.740843324527535 on epoch=499
05/30/2022 22:33:10 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
05/30/2022 22:33:13 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/30/2022 22:33:15 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
05/30/2022 22:33:18 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
05/30/2022 22:33:20 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
05/30/2022 22:33:21 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.6485632183908047 on epoch=512
05/30/2022 22:33:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
05/30/2022 22:33:26 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/30/2022 22:33:29 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
05/30/2022 22:33:31 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/30/2022 22:33:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
05/30/2022 22:33:34 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.7124869119775741 on epoch=524
05/30/2022 22:33:37 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/30/2022 22:33:39 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/30/2022 22:33:42 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/30/2022 22:33:44 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/30/2022 22:33:46 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/30/2022 22:33:47 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6841891891891893 on epoch=537
05/30/2022 22:33:50 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/30/2022 22:33:52 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/30/2022 22:33:55 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=544
05/30/2022 22:33:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/30/2022 22:34:00 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/30/2022 22:34:01 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.7034869431643626 on epoch=549
05/30/2022 22:34:03 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/30/2022 22:34:06 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
05/30/2022 22:34:08 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.09 on epoch=557
05/30/2022 22:34:11 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/30/2022 22:34:13 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
05/30/2022 22:34:14 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.6995862369337978 on epoch=562
05/30/2022 22:34:16 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=564
05/30/2022 22:34:19 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/30/2022 22:34:21 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/30/2022 22:34:24 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/30/2022 22:34:26 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/30/2022 22:34:27 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7600831280788177 on epoch=574
05/30/2022 22:34:30 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/30/2022 22:34:32 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=579
05/30/2022 22:34:35 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/30/2022 22:34:37 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/30/2022 22:34:40 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/30/2022 22:34:41 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6538461538461539 on epoch=587
05/30/2022 22:34:43 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
05/30/2022 22:34:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/30/2022 22:34:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/30/2022 22:34:50 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/30/2022 22:34:53 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/30/2022 22:34:54 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7297646033129904 on epoch=599
05/30/2022 22:34:56 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/30/2022 22:34:59 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=604
05/30/2022 22:35:01 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/30/2022 22:35:04 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/30/2022 22:35:06 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=612
05/30/2022 22:35:07 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7001051051051052 on epoch=612
05/30/2022 22:35:09 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
05/30/2022 22:35:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=617
05/30/2022 22:35:14 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/30/2022 22:35:17 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/30/2022 22:35:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/30/2022 22:35:20 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.6365440115440115 on epoch=624
05/30/2022 22:35:23 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
05/30/2022 22:35:25 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/30/2022 22:35:28 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=632
05/30/2022 22:35:30 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/30/2022 22:35:33 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/30/2022 22:35:33 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6495661260278951 on epoch=637
05/30/2022 22:35:36 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=639
05/30/2022 22:35:38 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=642
05/30/2022 22:35:41 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/30/2022 22:35:43 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/30/2022 22:35:46 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/30/2022 22:35:47 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.6812697106814755 on epoch=649
05/30/2022 22:35:49 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/30/2022 22:35:52 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=654
05/30/2022 22:35:54 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/30/2022 22:35:56 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/30/2022 22:35:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/30/2022 22:36:00 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7513782051282052 on epoch=662
05/30/2022 22:36:02 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/30/2022 22:36:05 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/30/2022 22:36:07 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/30/2022 22:36:10 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
05/30/2022 22:36:12 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/30/2022 22:36:13 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6774602209384817 on epoch=674
05/30/2022 22:36:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/30/2022 22:36:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=679
05/30/2022 22:36:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/30/2022 22:36:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/30/2022 22:36:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/30/2022 22:36:26 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.7220622514740163 on epoch=687
05/30/2022 22:36:28 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/30/2022 22:36:31 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/30/2022 22:36:33 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/30/2022 22:36:36 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/30/2022 22:36:38 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/30/2022 22:36:39 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6973610496830311 on epoch=699
05/30/2022 22:36:42 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/30/2022 22:36:44 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/30/2022 22:36:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/30/2022 22:36:49 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/30/2022 22:36:51 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/30/2022 22:36:52 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6937017231134878 on epoch=712
05/30/2022 22:36:55 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.07 on epoch=714
05/30/2022 22:36:57 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/30/2022 22:37:00 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/30/2022 22:37:02 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/30/2022 22:37:05 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/30/2022 22:37:06 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.7004249815225425 on epoch=724
05/30/2022 22:37:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.06 on epoch=727
05/30/2022 22:37:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=729
05/30/2022 22:37:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/30/2022 22:37:15 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/30/2022 22:37:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/30/2022 22:37:19 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.7212402539988747 on epoch=737
05/30/2022 22:37:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/30/2022 22:37:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
05/30/2022 22:37:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/30/2022 22:37:29 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/30/2022 22:37:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/30/2022 22:37:32 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7084431269213879 on epoch=749
05/30/2022 22:37:32 - INFO - __main__ - save last model!
05/30/2022 22:37:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 22:37:32 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 22:37:32 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:37:32 - INFO - __main__ - Printing 3 examples
05/30/2022 22:37:32 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/30/2022 22:37:32 - INFO - __main__ - ['happy']
05/30/2022 22:37:32 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/30/2022 22:37:32 - INFO - __main__ - ['happy']
05/30/2022 22:37:32 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/30/2022 22:37:32 - INFO - __main__ - ['happy']
05/30/2022 22:37:32 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:37:32 - INFO - __main__ - Printing 3 examples
05/30/2022 22:37:32 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 22:37:32 - INFO - __main__ - ['others']
05/30/2022 22:37:32 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 22:37:32 - INFO - __main__ - ['others']
05/30/2022 22:37:32 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 22:37:32 - INFO - __main__ - ['others']
05/30/2022 22:37:32 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:37:32 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:37:32 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 22:37:32 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:37:32 - INFO - __main__ - Printing 3 examples
05/30/2022 22:37:32 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/30/2022 22:37:32 - INFO - __main__ - ['happy']
05/30/2022 22:37:32 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/30/2022 22:37:32 - INFO - __main__ - ['happy']
05/30/2022 22:37:32 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/30/2022 22:37:32 - INFO - __main__ - ['happy']
05/30/2022 22:37:32 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:37:32 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:37:33 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 22:37:35 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:37:40 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 22:37:47 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 22:37:47 - INFO - __main__ - task name: emo
05/30/2022 22:37:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 22:37:48 - INFO - __main__ - Starting training!
05/30/2022 22:39:13 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_42_0.5_8_predictions.txt
05/30/2022 22:39:13 - INFO - __main__ - Classification-F1 on test data: 0.1322
05/30/2022 22:39:14 - INFO - __main__ - prefix=emo_16_42, lr=0.5, bsz=8, dev_performance=0.7651745495495496, test_performance=0.1322237447216403
05/30/2022 22:39:14 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.4, bsz=8 ...
05/30/2022 22:39:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:39:15 - INFO - __main__ - Printing 3 examples
05/30/2022 22:39:15 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/30/2022 22:39:15 - INFO - __main__ - ['happy']
05/30/2022 22:39:15 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/30/2022 22:39:15 - INFO - __main__ - ['happy']
05/30/2022 22:39:15 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/30/2022 22:39:15 - INFO - __main__ - ['happy']
05/30/2022 22:39:15 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:39:15 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:39:15 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 22:39:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:39:15 - INFO - __main__ - Printing 3 examples
05/30/2022 22:39:15 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/30/2022 22:39:15 - INFO - __main__ - ['happy']
05/30/2022 22:39:15 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/30/2022 22:39:15 - INFO - __main__ - ['happy']
05/30/2022 22:39:15 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/30/2022 22:39:15 - INFO - __main__ - ['happy']
05/30/2022 22:39:15 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:39:15 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:39:15 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 22:39:30 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 22:39:30 - INFO - __main__ - task name: emo
05/30/2022 22:39:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 22:39:31 - INFO - __main__ - Starting training!
05/30/2022 22:39:33 - INFO - __main__ - Step 10 Global step 10 Train loss 6.93 on epoch=2
05/30/2022 22:39:35 - INFO - __main__ - Step 20 Global step 20 Train loss 3.72 on epoch=4
05/30/2022 22:39:38 - INFO - __main__ - Step 30 Global step 30 Train loss 2.15 on epoch=7
05/30/2022 22:39:40 - INFO - __main__ - Step 40 Global step 40 Train loss 1.54 on epoch=9
05/30/2022 22:39:43 - INFO - __main__ - Step 50 Global step 50 Train loss 1.25 on epoch=12
05/30/2022 22:39:44 - INFO - __main__ - Global step 50 Train loss 3.12 Classification-F1 0.1 on epoch=12
05/30/2022 22:39:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/30/2022 22:39:46 - INFO - __main__ - Step 60 Global step 60 Train loss 1.34 on epoch=14
05/30/2022 22:39:49 - INFO - __main__ - Step 70 Global step 70 Train loss 1.19 on epoch=17
05/30/2022 22:39:51 - INFO - __main__ - Step 80 Global step 80 Train loss 1.03 on epoch=19
05/30/2022 22:39:53 - INFO - __main__ - Step 90 Global step 90 Train loss 1.15 on epoch=22
05/30/2022 22:39:56 - INFO - __main__ - Step 100 Global step 100 Train loss 1.06 on epoch=24
05/30/2022 22:39:56 - INFO - __main__ - Global step 100 Train loss 1.16 Classification-F1 0.1 on epoch=24
05/30/2022 22:39:59 - INFO - __main__ - Step 110 Global step 110 Train loss 1.00 on epoch=27
05/30/2022 22:40:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.97 on epoch=29
05/30/2022 22:40:03 - INFO - __main__ - Step 130 Global step 130 Train loss 1.00 on epoch=32
05/30/2022 22:40:06 - INFO - __main__ - Step 140 Global step 140 Train loss 0.98 on epoch=34
05/30/2022 22:40:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.96 on epoch=37
05/30/2022 22:40:09 - INFO - __main__ - Global step 150 Train loss 0.98 Classification-F1 0.1 on epoch=37
05/30/2022 22:40:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.92 on epoch=39
05/30/2022 22:40:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.93 on epoch=42
05/30/2022 22:40:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.98 on epoch=44
05/30/2022 22:40:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.98 on epoch=47
05/30/2022 22:40:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.89 on epoch=49
05/30/2022 22:40:22 - INFO - __main__ - Global step 200 Train loss 0.94 Classification-F1 0.10126582278481013 on epoch=49
05/30/2022 22:40:22 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.10126582278481013 on epoch=49, global_step=200
05/30/2022 22:40:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.85 on epoch=52
05/30/2022 22:40:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.97 on epoch=54
05/30/2022 22:40:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.87 on epoch=57
05/30/2022 22:40:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.93 on epoch=59
05/30/2022 22:40:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.91 on epoch=62
05/30/2022 22:40:34 - INFO - __main__ - Global step 250 Train loss 0.90 Classification-F1 0.3950553025374105 on epoch=62
05/30/2022 22:40:34 - INFO - __main__ - Saving model with best Classification-F1: 0.10126582278481013 -> 0.3950553025374105 on epoch=62, global_step=250
05/30/2022 22:40:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.84 on epoch=64
05/30/2022 22:40:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.94 on epoch=67
05/30/2022 22:40:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.90 on epoch=69
05/30/2022 22:40:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.85 on epoch=72
05/30/2022 22:40:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.85 on epoch=74
05/30/2022 22:40:47 - INFO - __main__ - Global step 300 Train loss 0.88 Classification-F1 0.13067758749069247 on epoch=74
05/30/2022 22:40:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.89 on epoch=77
05/30/2022 22:40:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.81 on epoch=79
05/30/2022 22:40:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.83 on epoch=82
05/30/2022 22:40:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.81 on epoch=84
05/30/2022 22:40:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.79 on epoch=87
05/30/2022 22:41:00 - INFO - __main__ - Global step 350 Train loss 0.83 Classification-F1 0.2945476436188511 on epoch=87
05/30/2022 22:41:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.90 on epoch=89
05/30/2022 22:41:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.89 on epoch=92
05/30/2022 22:41:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.93 on epoch=94
05/30/2022 22:41:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.99 on epoch=97
05/30/2022 22:41:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.89 on epoch=99
05/30/2022 22:41:13 - INFO - __main__ - Global step 400 Train loss 0.92 Classification-F1 0.24077540106951872 on epoch=99
05/30/2022 22:41:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.88 on epoch=102
05/30/2022 22:41:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.78 on epoch=104
05/30/2022 22:41:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.79 on epoch=107
05/30/2022 22:41:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.81 on epoch=109
05/30/2022 22:41:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.85 on epoch=112
05/30/2022 22:41:26 - INFO - __main__ - Global step 450 Train loss 0.82 Classification-F1 0.3542171105730427 on epoch=112
05/30/2022 22:41:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.81 on epoch=114
05/30/2022 22:41:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.88 on epoch=117
05/30/2022 22:41:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.83 on epoch=119
05/30/2022 22:41:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.75 on epoch=122
05/30/2022 22:41:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.75 on epoch=124
05/30/2022 22:41:38 - INFO - __main__ - Global step 500 Train loss 0.81 Classification-F1 0.36242315189683605 on epoch=124
05/30/2022 22:41:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.74 on epoch=127
05/30/2022 22:41:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.74 on epoch=129
05/30/2022 22:41:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.83 on epoch=132
05/30/2022 22:41:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.80 on epoch=134
05/30/2022 22:41:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.73 on epoch=137
05/30/2022 22:41:51 - INFO - __main__ - Global step 550 Train loss 0.77 Classification-F1 0.5556355166179947 on epoch=137
05/30/2022 22:41:51 - INFO - __main__ - Saving model with best Classification-F1: 0.3950553025374105 -> 0.5556355166179947 on epoch=137, global_step=550
05/30/2022 22:41:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.66 on epoch=139
05/30/2022 22:41:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.67 on epoch=142
05/30/2022 22:41:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.71 on epoch=144
05/30/2022 22:42:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.74 on epoch=147
05/30/2022 22:42:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.57 on epoch=149
05/30/2022 22:42:04 - INFO - __main__ - Global step 600 Train loss 0.67 Classification-F1 0.6212643678160921 on epoch=149
05/30/2022 22:42:04 - INFO - __main__ - Saving model with best Classification-F1: 0.5556355166179947 -> 0.6212643678160921 on epoch=149, global_step=600
05/30/2022 22:42:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.64 on epoch=152
05/30/2022 22:42:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.62 on epoch=154
05/30/2022 22:42:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.58 on epoch=157
05/30/2022 22:42:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.60 on epoch=159
05/30/2022 22:42:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.60 on epoch=162
05/30/2022 22:42:16 - INFO - __main__ - Global step 650 Train loss 0.61 Classification-F1 0.5122906698564593 on epoch=162
05/30/2022 22:42:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.65 on epoch=164
05/30/2022 22:42:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.55 on epoch=167
05/30/2022 22:42:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.52 on epoch=169
05/30/2022 22:42:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.54 on epoch=172
05/30/2022 22:42:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=174
05/30/2022 22:42:29 - INFO - __main__ - Global step 700 Train loss 0.55 Classification-F1 0.5924329501915708 on epoch=174
05/30/2022 22:42:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.54 on epoch=177
05/30/2022 22:42:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.53 on epoch=179
05/30/2022 22:42:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.52 on epoch=182
05/30/2022 22:42:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.54 on epoch=184
05/30/2022 22:42:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=187
05/30/2022 22:42:41 - INFO - __main__ - Global step 750 Train loss 0.52 Classification-F1 0.7108253588516746 on epoch=187
05/30/2022 22:42:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6212643678160921 -> 0.7108253588516746 on epoch=187, global_step=750
05/30/2022 22:42:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=189
05/30/2022 22:42:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=192
05/30/2022 22:42:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=194
05/30/2022 22:42:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=197
05/30/2022 22:42:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=199
05/30/2022 22:42:54 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.7134618352784736 on epoch=199
05/30/2022 22:42:54 - INFO - __main__ - Saving model with best Classification-F1: 0.7108253588516746 -> 0.7134618352784736 on epoch=199, global_step=800
05/30/2022 22:42:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.51 on epoch=202
05/30/2022 22:42:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=204
05/30/2022 22:43:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.50 on epoch=207
05/30/2022 22:43:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=209
05/30/2022 22:43:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=212
05/30/2022 22:43:07 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.7283434959349593 on epoch=212
05/30/2022 22:43:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7134618352784736 -> 0.7283434959349593 on epoch=212, global_step=850
05/30/2022 22:43:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.37 on epoch=214
05/30/2022 22:43:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=217
05/30/2022 22:43:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=219
05/30/2022 22:43:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=222
05/30/2022 22:43:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=224
05/30/2022 22:43:19 - INFO - __main__ - Global step 900 Train loss 0.35 Classification-F1 0.7124938423645321 on epoch=224
05/30/2022 22:43:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.26 on epoch=227
05/30/2022 22:43:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.30 on epoch=229
05/30/2022 22:43:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.39 on epoch=232
05/30/2022 22:43:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.19 on epoch=234
05/30/2022 22:43:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.32 on epoch=237
05/30/2022 22:43:32 - INFO - __main__ - Global step 950 Train loss 0.29 Classification-F1 0.7093963831867057 on epoch=237
05/30/2022 22:43:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=239
05/30/2022 22:43:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.28 on epoch=242
05/30/2022 22:43:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=244
05/30/2022 22:43:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=247
05/30/2022 22:43:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=249
05/30/2022 22:43:45 - INFO - __main__ - Global step 1000 Train loss 0.27 Classification-F1 0.7146253974562798 on epoch=249
05/30/2022 22:43:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.28 on epoch=252
05/30/2022 22:43:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.24 on epoch=254
05/30/2022 22:43:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=257
05/30/2022 22:43:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=259
05/30/2022 22:43:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=262
05/30/2022 22:43:57 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.6948529411764706 on epoch=262
05/30/2022 22:44:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=264
05/30/2022 22:44:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.22 on epoch=267
05/30/2022 22:44:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=269
05/30/2022 22:44:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=272
05/30/2022 22:44:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=274
05/30/2022 22:44:10 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.6507142857142858 on epoch=274
05/30/2022 22:44:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=277
05/30/2022 22:44:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.15 on epoch=279
05/30/2022 22:44:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.32 on epoch=282
05/30/2022 22:44:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=284
05/30/2022 22:44:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=287
05/30/2022 22:44:23 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.7387012987012987 on epoch=287
05/30/2022 22:44:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7283434959349593 -> 0.7387012987012987 on epoch=287, global_step=1150
05/30/2022 22:44:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.14 on epoch=289
05/30/2022 22:44:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.14 on epoch=292
05/30/2022 22:44:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=294
05/30/2022 22:44:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=297
05/30/2022 22:44:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=299
05/30/2022 22:44:36 - INFO - __main__ - Global step 1200 Train loss 0.15 Classification-F1 0.7450198412698413 on epoch=299
05/30/2022 22:44:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7387012987012987 -> 0.7450198412698413 on epoch=299, global_step=1200
05/30/2022 22:44:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=302
05/30/2022 22:44:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=304
05/30/2022 22:44:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.16 on epoch=307
05/30/2022 22:44:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=309
05/30/2022 22:44:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.18 on epoch=312
05/30/2022 22:44:50 - INFO - __main__ - Global step 1250 Train loss 0.14 Classification-F1 0.7464808558558558 on epoch=312
05/30/2022 22:44:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7450198412698413 -> 0.7464808558558558 on epoch=312, global_step=1250
05/30/2022 22:44:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.12 on epoch=314
05/30/2022 22:44:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=317
05/30/2022 22:44:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.13 on epoch=319
05/30/2022 22:45:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=322
05/30/2022 22:45:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=324
05/30/2022 22:45:03 - INFO - __main__ - Global step 1300 Train loss 0.14 Classification-F1 0.7444361615613987 on epoch=324
05/30/2022 22:45:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=327
05/30/2022 22:45:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=329
05/30/2022 22:45:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=332
05/30/2022 22:45:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=334
05/30/2022 22:45:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=337
05/30/2022 22:45:16 - INFO - __main__ - Global step 1350 Train loss 0.15 Classification-F1 0.7644089247590089 on epoch=337
05/30/2022 22:45:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7464808558558558 -> 0.7644089247590089 on epoch=337, global_step=1350
05/30/2022 22:45:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.12 on epoch=339
05/30/2022 22:45:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=342
05/30/2022 22:45:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=344
05/30/2022 22:45:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=347
05/30/2022 22:45:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=349
05/30/2022 22:45:29 - INFO - __main__ - Global step 1400 Train loss 0.12 Classification-F1 0.716469298245614 on epoch=349
05/30/2022 22:45:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.16 on epoch=352
05/30/2022 22:45:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=354
05/30/2022 22:45:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
05/30/2022 22:45:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.09 on epoch=359
05/30/2022 22:45:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=362
05/30/2022 22:45:42 - INFO - __main__ - Global step 1450 Train loss 0.11 Classification-F1 0.728671679197995 on epoch=362
05/30/2022 22:45:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
05/30/2022 22:45:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
05/30/2022 22:45:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=369
05/30/2022 22:45:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
05/30/2022 22:45:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=374
05/30/2022 22:45:56 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.7274974481871034 on epoch=374
05/30/2022 22:45:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.15 on epoch=377
05/30/2022 22:46:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=379
05/30/2022 22:46:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=382
05/30/2022 22:46:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=384
05/30/2022 22:46:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=387
05/30/2022 22:46:09 - INFO - __main__ - Global step 1550 Train loss 0.07 Classification-F1 0.778108563762214 on epoch=387
05/30/2022 22:46:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7644089247590089 -> 0.778108563762214 on epoch=387, global_step=1550
05/30/2022 22:46:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=389
05/30/2022 22:46:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=392
05/30/2022 22:46:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
05/30/2022 22:46:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=397
05/30/2022 22:46:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
05/30/2022 22:46:22 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.6527972027972028 on epoch=399
05/30/2022 22:46:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=402
05/30/2022 22:46:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
05/30/2022 22:46:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/30/2022 22:46:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=409
05/30/2022 22:46:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=412
05/30/2022 22:46:35 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.6528024193548387 on epoch=412
05/30/2022 22:46:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=414
05/30/2022 22:46:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=417
05/30/2022 22:46:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=419
05/30/2022 22:46:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
05/30/2022 22:46:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=424
05/30/2022 22:46:48 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.7349927237448461 on epoch=424
05/30/2022 22:46:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=427
05/30/2022 22:46:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
05/30/2022 22:46:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=432
05/30/2022 22:46:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
05/30/2022 22:47:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=437
05/30/2022 22:47:02 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.7147763599376502 on epoch=437
05/30/2022 22:47:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
05/30/2022 22:47:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=442
05/30/2022 22:47:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=444
05/30/2022 22:47:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=447
05/30/2022 22:47:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.14 on epoch=449
05/30/2022 22:47:15 - INFO - __main__ - Global step 1800 Train loss 0.11 Classification-F1 0.6798433048433048 on epoch=449
05/30/2022 22:47:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=452
05/30/2022 22:47:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=454
05/30/2022 22:47:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=457
05/30/2022 22:47:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.09 on epoch=459
05/30/2022 22:47:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=462
05/30/2022 22:47:28 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.7487334574028122 on epoch=462
05/30/2022 22:47:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=464
05/30/2022 22:47:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=467
05/30/2022 22:47:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=469
05/30/2022 22:47:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/30/2022 22:47:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/30/2022 22:47:40 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.7620909556393428 on epoch=474
05/30/2022 22:47:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/30/2022 22:47:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/30/2022 22:47:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=482
05/30/2022 22:47:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=484
05/30/2022 22:47:53 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/30/2022 22:47:53 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.7771762021762021 on epoch=487
05/30/2022 22:47:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/30/2022 22:47:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
05/30/2022 22:48:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=494
05/30/2022 22:48:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
05/30/2022 22:48:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=499
05/30/2022 22:48:07 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.6771586223623576 on epoch=499
05/30/2022 22:48:09 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=502
05/30/2022 22:48:12 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=504
05/30/2022 22:48:14 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
05/30/2022 22:48:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=509
05/30/2022 22:48:19 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=512
05/30/2022 22:48:20 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.7500718390804598 on epoch=512
05/30/2022 22:48:22 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
05/30/2022 22:48:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
05/30/2022 22:48:27 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=519
05/30/2022 22:48:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
05/30/2022 22:48:32 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
05/30/2022 22:48:33 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.7005115935262994 on epoch=524
05/30/2022 22:48:35 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
05/30/2022 22:48:38 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=529
05/30/2022 22:48:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=532
05/30/2022 22:48:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
05/30/2022 22:48:45 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/30/2022 22:48:46 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.7327697155283363 on epoch=537
05/30/2022 22:48:48 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
05/30/2022 22:48:51 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
05/30/2022 22:48:53 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=544
05/30/2022 22:48:56 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.12 on epoch=547
05/30/2022 22:48:58 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
05/30/2022 22:48:59 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.7485923423423423 on epoch=549
05/30/2022 22:49:02 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=552
05/30/2022 22:49:04 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.10 on epoch=554
05/30/2022 22:49:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
05/30/2022 22:49:09 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/30/2022 22:49:11 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/30/2022 22:49:12 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.6971403221403222 on epoch=562
05/30/2022 22:49:15 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=564
05/30/2022 22:49:17 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/30/2022 22:49:20 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=569
05/30/2022 22:49:22 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.07 on epoch=572
05/30/2022 22:49:24 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=574
05/30/2022 22:49:25 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.7311487854251013 on epoch=574
05/30/2022 22:49:28 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=577
05/30/2022 22:49:30 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
05/30/2022 22:49:33 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
05/30/2022 22:49:35 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=584
05/30/2022 22:49:38 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/30/2022 22:49:38 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.7500718390804598 on epoch=587
05/30/2022 22:49:41 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
05/30/2022 22:49:43 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.09 on epoch=592
05/30/2022 22:49:46 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=594
05/30/2022 22:49:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/30/2022 22:49:51 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/30/2022 22:49:52 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.7583471487066814 on epoch=599
05/30/2022 22:49:54 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
05/30/2022 22:49:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/30/2022 22:49:59 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=607
05/30/2022 22:50:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=609
05/30/2022 22:50:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/30/2022 22:50:05 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.7290602217117748 on epoch=612
05/30/2022 22:50:07 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/30/2022 22:50:09 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/30/2022 22:50:12 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=619
05/30/2022 22:50:14 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=622
05/30/2022 22:50:17 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/30/2022 22:50:18 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.72926267281106 on epoch=624
05/30/2022 22:50:20 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/30/2022 22:50:23 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=629
05/30/2022 22:50:25 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/30/2022 22:50:28 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/30/2022 22:50:30 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=637
05/30/2022 22:50:31 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.7288203540525522 on epoch=637
05/30/2022 22:50:33 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.07 on epoch=639
05/30/2022 22:50:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=642
05/30/2022 22:50:38 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=644
05/30/2022 22:50:41 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=647
05/30/2022 22:50:43 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/30/2022 22:50:44 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.7155237854251013 on epoch=649
05/30/2022 22:50:47 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/30/2022 22:50:49 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/30/2022 22:50:52 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/30/2022 22:50:54 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
05/30/2022 22:50:57 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=662
05/30/2022 22:50:58 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.7347306077856363 on epoch=662
05/30/2022 22:51:00 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/30/2022 22:51:02 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.19 on epoch=667
05/30/2022 22:51:05 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=669
05/30/2022 22:51:07 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/30/2022 22:51:10 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/30/2022 22:51:11 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.7625405844155844 on epoch=674
05/30/2022 22:51:13 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/30/2022 22:51:16 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.13 on epoch=679
05/30/2022 22:51:18 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.09 on epoch=682
05/30/2022 22:51:21 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=684
05/30/2022 22:51:23 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=687
05/30/2022 22:51:24 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.7327503974562798 on epoch=687
05/30/2022 22:51:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/30/2022 22:51:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/30/2022 22:51:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/30/2022 22:51:34 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=697
05/30/2022 22:51:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/30/2022 22:51:37 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7475108225108226 on epoch=699
05/30/2022 22:51:39 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=702
05/30/2022 22:51:42 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
05/30/2022 22:51:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=707
05/30/2022 22:51:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/30/2022 22:51:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/30/2022 22:51:50 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7164942398208118 on epoch=712
05/30/2022 22:51:53 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/30/2022 22:51:55 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/30/2022 22:51:58 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/30/2022 22:52:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/30/2022 22:52:02 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/30/2022 22:52:03 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7102003588516748 on epoch=724
05/30/2022 22:52:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/30/2022 22:52:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/30/2022 22:52:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/30/2022 22:52:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/30/2022 22:52:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/30/2022 22:52:17 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7324929971988795 on epoch=737
05/30/2022 22:52:19 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=739
05/30/2022 22:52:21 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/30/2022 22:52:24 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/30/2022 22:52:26 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/30/2022 22:52:29 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/30/2022 22:52:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:52:30 - INFO - __main__ - Printing 3 examples
05/30/2022 22:52:30 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/30/2022 22:52:30 - INFO - __main__ - ['happy']
05/30/2022 22:52:30 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/30/2022 22:52:30 - INFO - __main__ - ['happy']
05/30/2022 22:52:30 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/30/2022 22:52:30 - INFO - __main__ - ['happy']
05/30/2022 22:52:30 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:52:30 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:52:30 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7270817417876243 on epoch=749
05/30/2022 22:52:30 - INFO - __main__ - save last model!
05/30/2022 22:52:30 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 22:52:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:52:30 - INFO - __main__ - Printing 3 examples
05/30/2022 22:52:30 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/30/2022 22:52:30 - INFO - __main__ - ['happy']
05/30/2022 22:52:30 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/30/2022 22:52:30 - INFO - __main__ - ['happy']
05/30/2022 22:52:30 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/30/2022 22:52:30 - INFO - __main__ - ['happy']
05/30/2022 22:52:30 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:52:30 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:52:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 22:52:30 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 22:52:30 - INFO - __main__ - Printing 3 examples
05/30/2022 22:52:30 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 22:52:30 - INFO - __main__ - ['others']
05/30/2022 22:52:30 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 22:52:30 - INFO - __main__ - ['others']
05/30/2022 22:52:30 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 22:52:30 - INFO - __main__ - ['others']
05/30/2022 22:52:30 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:52:30 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 22:52:32 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:52:37 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 22:52:45 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 22:52:45 - INFO - __main__ - task name: emo
05/30/2022 22:52:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 22:52:46 - INFO - __main__ - Starting training!
05/30/2022 22:54:36 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_42_0.4_8_predictions.txt
05/30/2022 22:54:36 - INFO - __main__ - Classification-F1 on test data: 0.3963
05/30/2022 22:54:37 - INFO - __main__ - prefix=emo_16_42, lr=0.4, bsz=8, dev_performance=0.778108563762214, test_performance=0.3962588236833296
05/30/2022 22:54:37 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.3, bsz=8 ...
05/30/2022 22:54:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:54:38 - INFO - __main__ - Printing 3 examples
05/30/2022 22:54:38 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/30/2022 22:54:38 - INFO - __main__ - ['happy']
05/30/2022 22:54:38 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/30/2022 22:54:38 - INFO - __main__ - ['happy']
05/30/2022 22:54:38 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/30/2022 22:54:38 - INFO - __main__ - ['happy']
05/30/2022 22:54:38 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:54:38 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:54:38 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 22:54:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 22:54:38 - INFO - __main__ - Printing 3 examples
05/30/2022 22:54:38 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/30/2022 22:54:38 - INFO - __main__ - ['happy']
05/30/2022 22:54:38 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/30/2022 22:54:38 - INFO - __main__ - ['happy']
05/30/2022 22:54:38 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/30/2022 22:54:38 - INFO - __main__ - ['happy']
05/30/2022 22:54:38 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:54:38 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:54:38 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 22:54:56 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 22:54:56 - INFO - __main__ - task name: emo
05/30/2022 22:54:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 22:54:57 - INFO - __main__ - Starting training!
05/30/2022 22:55:00 - INFO - __main__ - Step 10 Global step 10 Train loss 7.84 on epoch=2
05/30/2022 22:55:02 - INFO - __main__ - Step 20 Global step 20 Train loss 5.86 on epoch=4
05/30/2022 22:55:05 - INFO - __main__ - Step 30 Global step 30 Train loss 3.41 on epoch=7
05/30/2022 22:55:07 - INFO - __main__ - Step 40 Global step 40 Train loss 2.32 on epoch=9
05/30/2022 22:55:10 - INFO - __main__ - Step 50 Global step 50 Train loss 1.75 on epoch=12
05/30/2022 22:55:10 - INFO - __main__ - Global step 50 Train loss 4.24 Classification-F1 0.22498316498316498 on epoch=12
05/30/2022 22:55:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.22498316498316498 on epoch=12, global_step=50
05/30/2022 22:55:13 - INFO - __main__ - Step 60 Global step 60 Train loss 1.47 on epoch=14
05/30/2022 22:55:15 - INFO - __main__ - Step 70 Global step 70 Train loss 1.32 on epoch=17
05/30/2022 22:55:18 - INFO - __main__ - Step 80 Global step 80 Train loss 1.16 on epoch=19
05/30/2022 22:55:20 - INFO - __main__ - Step 90 Global step 90 Train loss 1.13 on epoch=22
05/30/2022 22:55:23 - INFO - __main__ - Step 100 Global step 100 Train loss 1.16 on epoch=24
05/30/2022 22:55:23 - INFO - __main__ - Global step 100 Train loss 1.25 Classification-F1 0.1332923832923833 on epoch=24
05/30/2022 22:55:26 - INFO - __main__ - Step 110 Global step 110 Train loss 1.05 on epoch=27
05/30/2022 22:55:28 - INFO - __main__ - Step 120 Global step 120 Train loss 1.06 on epoch=29
05/30/2022 22:55:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.95 on epoch=32
05/30/2022 22:55:33 - INFO - __main__ - Step 140 Global step 140 Train loss 1.12 on epoch=34
05/30/2022 22:55:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.97 on epoch=37
05/30/2022 22:55:36 - INFO - __main__ - Global step 150 Train loss 1.03 Classification-F1 0.13197586726998492 on epoch=37
05/30/2022 22:55:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.98 on epoch=39
05/30/2022 22:55:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.93 on epoch=42
05/30/2022 22:55:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.95 on epoch=44
05/30/2022 22:55:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.95 on epoch=47
05/30/2022 22:55:49 - INFO - __main__ - Step 200 Global step 200 Train loss 1.04 on epoch=49
05/30/2022 22:55:49 - INFO - __main__ - Global step 200 Train loss 0.97 Classification-F1 0.20373514431239387 on epoch=49
05/30/2022 22:55:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.95 on epoch=52
05/30/2022 22:55:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.95 on epoch=54
05/30/2022 22:55:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.87 on epoch=57
05/30/2022 22:55:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.91 on epoch=59
05/30/2022 22:56:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.89 on epoch=62
05/30/2022 22:56:02 - INFO - __main__ - Global step 250 Train loss 0.91 Classification-F1 0.31816239316239314 on epoch=62
05/30/2022 22:56:02 - INFO - __main__ - Saving model with best Classification-F1: 0.22498316498316498 -> 0.31816239316239314 on epoch=62, global_step=250
05/30/2022 22:56:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.89 on epoch=64
05/30/2022 22:56:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.84 on epoch=67
05/30/2022 22:56:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.95 on epoch=69
05/30/2022 22:56:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.85 on epoch=72
05/30/2022 22:56:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.89 on epoch=74
05/30/2022 22:56:15 - INFO - __main__ - Global step 300 Train loss 0.88 Classification-F1 0.28746697746697747 on epoch=74
05/30/2022 22:56:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.84 on epoch=77
05/30/2022 22:56:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.86 on epoch=79
05/30/2022 22:56:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.88 on epoch=82
05/30/2022 22:56:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.94 on epoch=84
05/30/2022 22:56:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.83 on epoch=87
05/30/2022 22:56:28 - INFO - __main__ - Global step 350 Train loss 0.87 Classification-F1 0.3891547049441787 on epoch=87
05/30/2022 22:56:28 - INFO - __main__ - Saving model with best Classification-F1: 0.31816239316239314 -> 0.3891547049441787 on epoch=87, global_step=350
05/30/2022 22:56:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.81 on epoch=89
05/30/2022 22:56:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.80 on epoch=92
05/30/2022 22:56:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.77 on epoch=94
05/30/2022 22:56:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.77 on epoch=97
05/30/2022 22:56:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.87 on epoch=99
05/30/2022 22:56:40 - INFO - __main__ - Global step 400 Train loss 0.81 Classification-F1 0.23020487264673312 on epoch=99
05/30/2022 22:56:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.78 on epoch=102
05/30/2022 22:56:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.68 on epoch=104
05/30/2022 22:56:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.68 on epoch=107
05/30/2022 22:56:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.80 on epoch=109
05/30/2022 22:56:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.74 on epoch=112
05/30/2022 22:56:53 - INFO - __main__ - Global step 450 Train loss 0.74 Classification-F1 0.3897435897435897 on epoch=112
05/30/2022 22:56:53 - INFO - __main__ - Saving model with best Classification-F1: 0.3891547049441787 -> 0.3897435897435897 on epoch=112, global_step=450
05/30/2022 22:56:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.66 on epoch=114
05/30/2022 22:56:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.74 on epoch=117
05/30/2022 22:57:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.74 on epoch=119
05/30/2022 22:57:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.67 on epoch=122
05/30/2022 22:57:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.70 on epoch=124
05/30/2022 22:57:06 - INFO - __main__ - Global step 500 Train loss 0.70 Classification-F1 0.2992424242424242 on epoch=124
05/30/2022 22:57:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.60 on epoch=127
05/30/2022 22:57:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.74 on epoch=129
05/30/2022 22:57:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.68 on epoch=132
05/30/2022 22:57:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.58 on epoch=134
05/30/2022 22:57:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.61 on epoch=137
05/30/2022 22:57:19 - INFO - __main__ - Global step 550 Train loss 0.64 Classification-F1 0.3054082714740191 on epoch=137
05/30/2022 22:57:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.64 on epoch=139
05/30/2022 22:57:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.63 on epoch=142
05/30/2022 22:57:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.66 on epoch=144
05/30/2022 22:57:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.55 on epoch=147
05/30/2022 22:57:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.54 on epoch=149
05/30/2022 22:57:31 - INFO - __main__ - Global step 600 Train loss 0.60 Classification-F1 0.4939464674758792 on epoch=149
05/30/2022 22:57:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3897435897435897 -> 0.4939464674758792 on epoch=149, global_step=600
05/30/2022 22:57:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=152
05/30/2022 22:57:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.65 on epoch=154
05/30/2022 22:57:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=157
05/30/2022 22:57:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=159
05/30/2022 22:57:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.47 on epoch=162
05/30/2022 22:57:44 - INFO - __main__ - Global step 650 Train loss 0.50 Classification-F1 0.3557692307692308 on epoch=162
05/30/2022 22:57:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.56 on epoch=164
05/30/2022 22:57:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=167
05/30/2022 22:57:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.50 on epoch=169
05/30/2022 22:57:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=172
05/30/2022 22:57:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.51 on epoch=174
05/30/2022 22:57:57 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.39851913256168575 on epoch=174
05/30/2022 22:58:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=177
05/30/2022 22:58:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.41 on epoch=179
05/30/2022 22:58:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=182
05/30/2022 22:58:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=184
05/30/2022 22:58:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=187
05/30/2022 22:58:10 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.44872958257713247 on epoch=187
05/30/2022 22:58:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=189
05/30/2022 22:58:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.35 on epoch=192
05/30/2022 22:58:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=194
05/30/2022 22:58:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.31 on epoch=197
05/30/2022 22:58:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.32 on epoch=199
05/30/2022 22:58:23 - INFO - __main__ - Global step 800 Train loss 0.35 Classification-F1 0.5779264418377321 on epoch=199
05/30/2022 22:58:23 - INFO - __main__ - Saving model with best Classification-F1: 0.4939464674758792 -> 0.5779264418377321 on epoch=199, global_step=800
05/30/2022 22:58:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=202
05/30/2022 22:58:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.30 on epoch=204
05/30/2022 22:58:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.31 on epoch=207
05/30/2022 22:58:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=209
05/30/2022 22:58:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.30 on epoch=212
05/30/2022 22:58:36 - INFO - __main__ - Global step 850 Train loss 0.32 Classification-F1 0.4619781071393974 on epoch=212
05/30/2022 22:58:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.31 on epoch=214
05/30/2022 22:58:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=217
05/30/2022 22:58:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=219
05/30/2022 22:58:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.27 on epoch=222
05/30/2022 22:58:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=224
05/30/2022 22:58:49 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.46178488178488175 on epoch=224
05/30/2022 22:58:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=227
05/30/2022 22:58:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=229
05/30/2022 22:58:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.28 on epoch=232
05/30/2022 22:58:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.32 on epoch=234
05/30/2022 22:59:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=237
05/30/2022 22:59:02 - INFO - __main__ - Global step 950 Train loss 0.29 Classification-F1 0.3306263534899217 on epoch=237
05/30/2022 22:59:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=239
05/30/2022 22:59:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=242
05/30/2022 22:59:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=244
05/30/2022 22:59:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.23 on epoch=247
05/30/2022 22:59:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.31 on epoch=249
05/30/2022 22:59:15 - INFO - __main__ - Global step 1000 Train loss 0.24 Classification-F1 0.4784843205574913 on epoch=249
05/30/2022 22:59:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=252
05/30/2022 22:59:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.24 on epoch=254
05/30/2022 22:59:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.18 on epoch=257
05/30/2022 22:59:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=259
05/30/2022 22:59:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.11 on epoch=262
05/30/2022 22:59:28 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.5533725235338138 on epoch=262
05/30/2022 22:59:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=264
05/30/2022 22:59:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=267
05/30/2022 22:59:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=269
05/30/2022 22:59:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=272
05/30/2022 22:59:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.15 on epoch=274
05/30/2022 22:59:40 - INFO - __main__ - Global step 1100 Train loss 0.18 Classification-F1 0.48715725806451615 on epoch=274
05/30/2022 22:59:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.15 on epoch=277
05/30/2022 22:59:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=279
05/30/2022 22:59:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=282
05/30/2022 22:59:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.22 on epoch=284
05/30/2022 22:59:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=287
05/30/2022 22:59:53 - INFO - __main__ - Global step 1150 Train loss 0.15 Classification-F1 0.49459956279809214 on epoch=287
05/30/2022 22:59:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=289
05/30/2022 22:59:58 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.12 on epoch=292
05/30/2022 23:00:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.15 on epoch=294
05/30/2022 23:00:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.11 on epoch=297
05/30/2022 23:00:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.13 on epoch=299
05/30/2022 23:00:06 - INFO - __main__ - Global step 1200 Train loss 0.15 Classification-F1 0.45667295667295665 on epoch=299
05/30/2022 23:00:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.14 on epoch=302
05/30/2022 23:00:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=304
05/30/2022 23:00:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.12 on epoch=307
05/30/2022 23:00:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.08 on epoch=309
05/30/2022 23:00:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=312
05/30/2022 23:00:20 - INFO - __main__ - Global step 1250 Train loss 0.10 Classification-F1 0.5137746855014408 on epoch=312
05/30/2022 23:00:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=314
05/30/2022 23:00:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=317
05/30/2022 23:00:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=319
05/30/2022 23:00:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=322
05/30/2022 23:00:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=324
05/30/2022 23:00:33 - INFO - __main__ - Global step 1300 Train loss 0.10 Classification-F1 0.4639133238023155 on epoch=324
05/30/2022 23:00:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=327
05/30/2022 23:00:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=329
05/30/2022 23:00:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=332
05/30/2022 23:00:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=334
05/30/2022 23:00:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=337
05/30/2022 23:00:45 - INFO - __main__ - Global step 1350 Train loss 0.07 Classification-F1 0.5898178398178399 on epoch=337
05/30/2022 23:00:45 - INFO - __main__ - Saving model with best Classification-F1: 0.5779264418377321 -> 0.5898178398178399 on epoch=337, global_step=1350
05/30/2022 23:00:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=339
05/30/2022 23:00:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=342
05/30/2022 23:00:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=344
05/30/2022 23:00:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=347
05/30/2022 23:00:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.15 on epoch=349
05/30/2022 23:00:58 - INFO - __main__ - Global step 1400 Train loss 0.10 Classification-F1 0.5605137395459976 on epoch=349
05/30/2022 23:01:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.11 on epoch=352
05/30/2022 23:01:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=354
05/30/2022 23:01:06 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=357
05/30/2022 23:01:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.11 on epoch=359
05/30/2022 23:01:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=362
05/30/2022 23:01:11 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.5648265460030166 on epoch=362
05/30/2022 23:01:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=364
05/30/2022 23:01:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=367
05/30/2022 23:01:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=369
05/30/2022 23:01:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
05/30/2022 23:01:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
05/30/2022 23:01:25 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.574748330609857 on epoch=374
05/30/2022 23:01:27 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=377
05/30/2022 23:01:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
05/30/2022 23:01:32 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=382
05/30/2022 23:01:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=384
05/30/2022 23:01:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/30/2022 23:01:38 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.5332519332519332 on epoch=387
05/30/2022 23:01:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
05/30/2022 23:01:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=392
05/30/2022 23:01:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
05/30/2022 23:01:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=397
05/30/2022 23:01:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
05/30/2022 23:01:51 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.5907651033386327 on epoch=399
05/30/2022 23:01:51 - INFO - __main__ - Saving model with best Classification-F1: 0.5898178398178399 -> 0.5907651033386327 on epoch=399, global_step=1600
05/30/2022 23:01:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=402
05/30/2022 23:01:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/30/2022 23:01:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=407
05/30/2022 23:02:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=409
05/30/2022 23:02:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
05/30/2022 23:02:04 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.5491298648921153 on epoch=412
05/30/2022 23:02:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=414
05/30/2022 23:02:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.08 on epoch=417
05/30/2022 23:02:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
05/30/2022 23:02:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
05/30/2022 23:02:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=424
05/30/2022 23:02:17 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.53203419298754 on epoch=424
05/30/2022 23:02:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
05/30/2022 23:02:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
05/30/2022 23:02:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
05/30/2022 23:02:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=434
05/30/2022 23:02:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
05/30/2022 23:02:30 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.5642678969291872 on epoch=437
05/30/2022 23:02:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=439
05/30/2022 23:02:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
05/30/2022 23:02:37 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
05/30/2022 23:02:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
05/30/2022 23:02:42 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=449
05/30/2022 23:02:43 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.48672402159244266 on epoch=449
05/30/2022 23:02:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=452
05/30/2022 23:02:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/30/2022 23:02:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=457
05/30/2022 23:02:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/30/2022 23:02:55 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=462
05/30/2022 23:02:56 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.5100427350427351 on epoch=462
05/30/2022 23:02:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=464
05/30/2022 23:03:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=467
05/30/2022 23:03:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
05/30/2022 23:03:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/30/2022 23:03:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/30/2022 23:03:09 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.5723588309795206 on epoch=474
05/30/2022 23:03:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/30/2022 23:03:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/30/2022 23:03:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/30/2022 23:03:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=484
05/30/2022 23:03:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=487
05/30/2022 23:03:22 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.5616091667704571 on epoch=487
05/30/2022 23:03:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
05/30/2022 23:03:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=492
05/30/2022 23:03:29 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/30/2022 23:03:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/30/2022 23:03:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=499
05/30/2022 23:03:35 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.5358333333333334 on epoch=499
05/30/2022 23:03:37 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
05/30/2022 23:03:40 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=504
05/30/2022 23:03:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=507
05/30/2022 23:03:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
05/30/2022 23:03:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
05/30/2022 23:03:48 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.6532604650251709 on epoch=512
05/30/2022 23:03:48 - INFO - __main__ - Saving model with best Classification-F1: 0.5907651033386327 -> 0.6532604650251709 on epoch=512, global_step=2050
05/30/2022 23:03:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
05/30/2022 23:03:53 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=517
05/30/2022 23:03:55 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/30/2022 23:03:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/30/2022 23:04:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/30/2022 23:04:01 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.5036774628879892 on epoch=524
05/30/2022 23:04:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/30/2022 23:04:06 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/30/2022 23:04:08 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/30/2022 23:04:10 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
05/30/2022 23:04:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.09 on epoch=537
05/30/2022 23:04:14 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.5223749119097956 on epoch=537
05/30/2022 23:04:16 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=539
05/30/2022 23:04:19 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=542
05/30/2022 23:04:21 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/30/2022 23:04:24 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=547
05/30/2022 23:04:26 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
05/30/2022 23:04:27 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.5585948604241288 on epoch=549
05/30/2022 23:04:29 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/30/2022 23:04:32 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/30/2022 23:04:34 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=557
05/30/2022 23:04:37 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.08 on epoch=559
05/30/2022 23:04:39 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
05/30/2022 23:04:40 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.5937121212121212 on epoch=562
05/30/2022 23:04:43 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=564
05/30/2022 23:04:45 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/30/2022 23:04:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=569
05/30/2022 23:04:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
05/30/2022 23:04:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=574
05/30/2022 23:04:53 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.5154091063974785 on epoch=574
05/30/2022 23:04:56 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=577
05/30/2022 23:04:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.07 on epoch=579
05/30/2022 23:05:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
05/30/2022 23:05:03 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/30/2022 23:05:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=587
05/30/2022 23:05:06 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.6028631611795912 on epoch=587
05/30/2022 23:05:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
05/30/2022 23:05:11 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/30/2022 23:05:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/30/2022 23:05:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
05/30/2022 23:05:18 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/30/2022 23:05:20 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.655366692131398 on epoch=599
05/30/2022 23:05:20 - INFO - __main__ - Saving model with best Classification-F1: 0.6532604650251709 -> 0.655366692131398 on epoch=599, global_step=2400
05/30/2022 23:05:22 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/30/2022 23:05:24 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/30/2022 23:05:27 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/30/2022 23:05:29 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=609
05/30/2022 23:05:32 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
05/30/2022 23:05:33 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6528708133971292 on epoch=612
05/30/2022 23:05:35 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=614
05/30/2022 23:05:37 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=617
05/30/2022 23:05:40 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=619
05/30/2022 23:05:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/30/2022 23:05:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/30/2022 23:05:46 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.6693986193986193 on epoch=624
05/30/2022 23:05:46 - INFO - __main__ - Saving model with best Classification-F1: 0.655366692131398 -> 0.6693986193986193 on epoch=624, global_step=2500
05/30/2022 23:05:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/30/2022 23:05:51 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/30/2022 23:05:53 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/30/2022 23:05:55 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/30/2022 23:05:58 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/30/2022 23:05:59 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6596585146597824 on epoch=637
05/30/2022 23:06:01 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=639
05/30/2022 23:06:04 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/30/2022 23:06:06 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/30/2022 23:06:09 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/30/2022 23:06:11 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=649
05/30/2022 23:06:12 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.626984126984127 on epoch=649
05/30/2022 23:06:14 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/30/2022 23:06:17 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/30/2022 23:06:19 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/30/2022 23:06:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/30/2022 23:06:24 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=662
05/30/2022 23:06:25 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.6147216951296648 on epoch=662
05/30/2022 23:06:28 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=664
05/30/2022 23:06:30 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/30/2022 23:06:32 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=669
05/30/2022 23:06:35 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/30/2022 23:06:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/30/2022 23:06:38 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.6384120494242852 on epoch=674
05/30/2022 23:06:41 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/30/2022 23:06:43 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/30/2022 23:06:45 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
05/30/2022 23:06:48 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/30/2022 23:06:50 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/30/2022 23:06:51 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6596585146597824 on epoch=687
05/30/2022 23:06:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/30/2022 23:06:56 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/30/2022 23:06:58 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/30/2022 23:07:01 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/30/2022 23:07:03 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
05/30/2022 23:07:04 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.5815072615719167 on epoch=699
05/30/2022 23:07:07 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/30/2022 23:07:09 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/30/2022 23:07:12 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/30/2022 23:07:14 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/30/2022 23:07:16 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=712
05/30/2022 23:07:17 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.5989884017267384 on epoch=712
05/30/2022 23:07:20 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=714
05/30/2022 23:07:22 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
05/30/2022 23:07:25 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/30/2022 23:07:27 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/30/2022 23:07:29 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/30/2022 23:07:30 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.5849647301260203 on epoch=724
05/30/2022 23:07:33 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=727
05/30/2022 23:07:35 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/30/2022 23:07:38 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/30/2022 23:07:40 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/30/2022 23:07:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/30/2022 23:07:44 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.5361656348152972 on epoch=737
05/30/2022 23:07:46 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/30/2022 23:07:48 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/30/2022 23:07:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=744
05/30/2022 23:07:53 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/30/2022 23:07:56 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=749
05/30/2022 23:07:57 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.6115650478553705 on epoch=749
05/30/2022 23:07:57 - INFO - __main__ - save last model!
05/30/2022 23:07:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 23:07:57 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 23:07:57 - INFO - __main__ - Printing 3 examples
05/30/2022 23:07:57 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 23:07:57 - INFO - __main__ - ['others']
05/30/2022 23:07:57 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 23:07:57 - INFO - __main__ - ['others']
05/30/2022 23:07:57 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 23:07:57 - INFO - __main__ - ['others']
05/30/2022 23:07:57 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:07:57 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:07:57 - INFO - __main__ - Printing 3 examples
05/30/2022 23:07:57 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/30/2022 23:07:57 - INFO - __main__ - ['happy']
05/30/2022 23:07:57 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/30/2022 23:07:57 - INFO - __main__ - ['happy']
05/30/2022 23:07:57 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/30/2022 23:07:57 - INFO - __main__ - ['happy']
05/30/2022 23:07:57 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:07:57 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:07:57 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 23:07:57 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:07:57 - INFO - __main__ - Printing 3 examples
05/30/2022 23:07:57 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/30/2022 23:07:57 - INFO - __main__ - ['happy']
05/30/2022 23:07:57 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/30/2022 23:07:57 - INFO - __main__ - ['happy']
05/30/2022 23:07:57 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/30/2022 23:07:57 - INFO - __main__ - ['happy']
05/30/2022 23:07:57 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:07:57 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:07:57 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:07:59 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:08:04 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 23:08:15 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 23:08:15 - INFO - __main__ - task name: emo
05/30/2022 23:08:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 23:08:16 - INFO - __main__ - Starting training!
05/30/2022 23:09:32 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_42_0.3_8_predictions.txt
05/30/2022 23:09:32 - INFO - __main__ - Classification-F1 on test data: 0.3939
05/30/2022 23:09:32 - INFO - __main__ - prefix=emo_16_42, lr=0.3, bsz=8, dev_performance=0.6693986193986193, test_performance=0.3938637282129449
05/30/2022 23:09:32 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.2, bsz=8 ...
05/30/2022 23:09:33 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:09:33 - INFO - __main__ - Printing 3 examples
05/30/2022 23:09:33 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/30/2022 23:09:33 - INFO - __main__ - ['happy']
05/30/2022 23:09:33 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/30/2022 23:09:33 - INFO - __main__ - ['happy']
05/30/2022 23:09:33 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/30/2022 23:09:33 - INFO - __main__ - ['happy']
05/30/2022 23:09:33 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:09:33 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:09:33 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 23:09:33 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:09:33 - INFO - __main__ - Printing 3 examples
05/30/2022 23:09:33 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/30/2022 23:09:33 - INFO - __main__ - ['happy']
05/30/2022 23:09:33 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/30/2022 23:09:33 - INFO - __main__ - ['happy']
05/30/2022 23:09:33 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/30/2022 23:09:33 - INFO - __main__ - ['happy']
05/30/2022 23:09:33 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:09:33 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:09:33 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:09:52 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 23:09:52 - INFO - __main__ - task name: emo
05/30/2022 23:09:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 23:09:52 - INFO - __main__ - Starting training!
05/30/2022 23:09:55 - INFO - __main__ - Step 10 Global step 10 Train loss 8.15 on epoch=2
05/30/2022 23:09:57 - INFO - __main__ - Step 20 Global step 20 Train loss 6.90 on epoch=4
05/30/2022 23:10:00 - INFO - __main__ - Step 30 Global step 30 Train loss 4.48 on epoch=7
05/30/2022 23:10:02 - INFO - __main__ - Step 40 Global step 40 Train loss 3.01 on epoch=9
05/30/2022 23:10:04 - INFO - __main__ - Step 50 Global step 50 Train loss 2.27 on epoch=12
05/30/2022 23:10:06 - INFO - __main__ - Global step 50 Train loss 4.96 Classification-F1 0.19971537001897532 on epoch=12
05/30/2022 23:10:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.19971537001897532 on epoch=12, global_step=50
05/30/2022 23:10:08 - INFO - __main__ - Step 60 Global step 60 Train loss 1.89 on epoch=14
05/30/2022 23:10:10 - INFO - __main__ - Step 70 Global step 70 Train loss 1.56 on epoch=17
05/30/2022 23:10:13 - INFO - __main__ - Step 80 Global step 80 Train loss 1.42 on epoch=19
05/30/2022 23:10:15 - INFO - __main__ - Step 90 Global step 90 Train loss 1.31 on epoch=22
05/30/2022 23:10:18 - INFO - __main__ - Step 100 Global step 100 Train loss 1.29 on epoch=24
05/30/2022 23:10:18 - INFO - __main__ - Global step 100 Train loss 1.49 Classification-F1 0.1788094548164093 on epoch=24
05/30/2022 23:10:21 - INFO - __main__ - Step 110 Global step 110 Train loss 1.32 on epoch=27
05/30/2022 23:10:23 - INFO - __main__ - Step 120 Global step 120 Train loss 1.14 on epoch=29
05/30/2022 23:10:26 - INFO - __main__ - Step 130 Global step 130 Train loss 1.12 on epoch=32
05/30/2022 23:10:28 - INFO - __main__ - Step 140 Global step 140 Train loss 1.10 on epoch=34
05/30/2022 23:10:30 - INFO - __main__ - Step 150 Global step 150 Train loss 1.08 on epoch=37
05/30/2022 23:10:31 - INFO - __main__ - Global step 150 Train loss 1.15 Classification-F1 0.20226331639375117 on epoch=37
05/30/2022 23:10:31 - INFO - __main__ - Saving model with best Classification-F1: 0.19971537001897532 -> 0.20226331639375117 on epoch=37, global_step=150
05/30/2022 23:10:34 - INFO - __main__ - Step 160 Global step 160 Train loss 1.04 on epoch=39
05/30/2022 23:10:36 - INFO - __main__ - Step 170 Global step 170 Train loss 1.11 on epoch=42
05/30/2022 23:10:38 - INFO - __main__ - Step 180 Global step 180 Train loss 1.00 on epoch=44
05/30/2022 23:10:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.97 on epoch=47
05/30/2022 23:10:43 - INFO - __main__ - Step 200 Global step 200 Train loss 1.01 on epoch=49
05/30/2022 23:10:44 - INFO - __main__ - Global step 200 Train loss 1.03 Classification-F1 0.1732142857142857 on epoch=49
05/30/2022 23:10:46 - INFO - __main__ - Step 210 Global step 210 Train loss 1.02 on epoch=52
05/30/2022 23:10:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.94 on epoch=54
05/30/2022 23:10:51 - INFO - __main__ - Step 230 Global step 230 Train loss 1.01 on epoch=57
05/30/2022 23:10:54 - INFO - __main__ - Step 240 Global step 240 Train loss 1.07 on epoch=59
05/30/2022 23:10:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.94 on epoch=62
05/30/2022 23:10:57 - INFO - __main__ - Global step 250 Train loss 0.99 Classification-F1 0.13067758749069247 on epoch=62
05/30/2022 23:10:59 - INFO - __main__ - Step 260 Global step 260 Train loss 1.08 on epoch=64
05/30/2022 23:11:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.96 on epoch=67
05/30/2022 23:11:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.95 on epoch=69
05/30/2022 23:11:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.95 on epoch=72
05/30/2022 23:11:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.91 on epoch=74
05/30/2022 23:11:09 - INFO - __main__ - Global step 300 Train loss 0.97 Classification-F1 0.21830916927899688 on epoch=74
05/30/2022 23:11:09 - INFO - __main__ - Saving model with best Classification-F1: 0.20226331639375117 -> 0.21830916927899688 on epoch=74, global_step=300
05/30/2022 23:11:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.83 on epoch=77
05/30/2022 23:11:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.94 on epoch=79
05/30/2022 23:11:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.99 on epoch=82
05/30/2022 23:11:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.83 on epoch=84
05/30/2022 23:11:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.90 on epoch=87
05/30/2022 23:11:22 - INFO - __main__ - Global step 350 Train loss 0.90 Classification-F1 0.2604347826086957 on epoch=87
05/30/2022 23:11:22 - INFO - __main__ - Saving model with best Classification-F1: 0.21830916927899688 -> 0.2604347826086957 on epoch=87, global_step=350
05/30/2022 23:11:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.95 on epoch=89
05/30/2022 23:11:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.90 on epoch=92
05/30/2022 23:11:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.92 on epoch=94
05/30/2022 23:11:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.90 on epoch=97
05/30/2022 23:11:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.97 on epoch=99
05/30/2022 23:11:35 - INFO - __main__ - Global step 400 Train loss 0.93 Classification-F1 0.17269812185066422 on epoch=99
05/30/2022 23:11:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.92 on epoch=102
05/30/2022 23:11:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.92 on epoch=104
05/30/2022 23:11:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.89 on epoch=107
05/30/2022 23:11:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.95 on epoch=109
05/30/2022 23:11:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.95 on epoch=112
05/30/2022 23:11:48 - INFO - __main__ - Global step 450 Train loss 0.93 Classification-F1 0.35545068928950163 on epoch=112
05/30/2022 23:11:48 - INFO - __main__ - Saving model with best Classification-F1: 0.2604347826086957 -> 0.35545068928950163 on epoch=112, global_step=450
05/30/2022 23:11:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.92 on epoch=114
05/30/2022 23:11:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.86 on epoch=117
05/30/2022 23:11:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.95 on epoch=119
05/30/2022 23:11:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.98 on epoch=122
05/30/2022 23:12:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.88 on epoch=124
05/30/2022 23:12:00 - INFO - __main__ - Global step 500 Train loss 0.92 Classification-F1 0.2124819624819625 on epoch=124
05/30/2022 23:12:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.85 on epoch=127
05/30/2022 23:12:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.91 on epoch=129
05/30/2022 23:12:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.91 on epoch=132
05/30/2022 23:12:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.88 on epoch=134
05/30/2022 23:12:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=137
05/30/2022 23:12:13 - INFO - __main__ - Global step 550 Train loss 0.87 Classification-F1 0.3521174863387978 on epoch=137
05/30/2022 23:12:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.96 on epoch=139
05/30/2022 23:12:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.88 on epoch=142
05/30/2022 23:12:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.83 on epoch=144
05/30/2022 23:12:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.84 on epoch=147
05/30/2022 23:12:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.83 on epoch=149
05/30/2022 23:12:26 - INFO - __main__ - Global step 600 Train loss 0.87 Classification-F1 0.3116319444444444 on epoch=149
05/30/2022 23:12:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.88 on epoch=152
05/30/2022 23:12:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.91 on epoch=154
05/30/2022 23:12:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.86 on epoch=157
05/30/2022 23:12:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.80 on epoch=159
05/30/2022 23:12:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.86 on epoch=162
05/30/2022 23:12:39 - INFO - __main__ - Global step 650 Train loss 0.86 Classification-F1 0.3020833333333333 on epoch=162
05/30/2022 23:12:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.83 on epoch=164
05/30/2022 23:12:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.86 on epoch=167
05/30/2022 23:12:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.92 on epoch=169
05/30/2022 23:12:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.84 on epoch=172
05/30/2022 23:12:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.76 on epoch=174
05/30/2022 23:12:51 - INFO - __main__ - Global step 700 Train loss 0.84 Classification-F1 0.37592016833113506 on epoch=174
05/30/2022 23:12:51 - INFO - __main__ - Saving model with best Classification-F1: 0.35545068928950163 -> 0.37592016833113506 on epoch=174, global_step=700
05/30/2022 23:12:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.82 on epoch=177
05/30/2022 23:12:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.86 on epoch=179
05/30/2022 23:12:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.91 on epoch=182
05/30/2022 23:13:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.82 on epoch=184
05/30/2022 23:13:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.80 on epoch=187
05/30/2022 23:13:04 - INFO - __main__ - Global step 750 Train loss 0.84 Classification-F1 0.3921763720150817 on epoch=187
05/30/2022 23:13:04 - INFO - __main__ - Saving model with best Classification-F1: 0.37592016833113506 -> 0.3921763720150817 on epoch=187, global_step=750
05/30/2022 23:13:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.86 on epoch=189
05/30/2022 23:13:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.82 on epoch=192
05/30/2022 23:13:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.87 on epoch=194
05/30/2022 23:13:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.80 on epoch=197
05/30/2022 23:13:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.77 on epoch=199
05/30/2022 23:13:17 - INFO - __main__ - Global step 800 Train loss 0.82 Classification-F1 0.3839285714285715 on epoch=199
05/30/2022 23:13:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.73 on epoch=202
05/30/2022 23:13:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.75 on epoch=204
05/30/2022 23:13:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.64 on epoch=207
05/30/2022 23:13:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.74 on epoch=209
05/30/2022 23:13:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.69 on epoch=212
05/30/2022 23:13:30 - INFO - __main__ - Global step 850 Train loss 0.71 Classification-F1 0.3463705450733753 on epoch=212
05/30/2022 23:13:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.80 on epoch=214
05/30/2022 23:13:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.65 on epoch=217
05/30/2022 23:13:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.78 on epoch=219
05/30/2022 23:13:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.65 on epoch=222
05/30/2022 23:13:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.63 on epoch=224
05/30/2022 23:13:42 - INFO - __main__ - Global step 900 Train loss 0.70 Classification-F1 0.508224669178016 on epoch=224
05/30/2022 23:13:42 - INFO - __main__ - Saving model with best Classification-F1: 0.3921763720150817 -> 0.508224669178016 on epoch=224, global_step=900
05/30/2022 23:13:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.72 on epoch=227
05/30/2022 23:13:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.64 on epoch=229
05/30/2022 23:13:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.67 on epoch=232
05/30/2022 23:13:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.60 on epoch=234
05/30/2022 23:13:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.71 on epoch=237
05/30/2022 23:13:55 - INFO - __main__ - Global step 950 Train loss 0.67 Classification-F1 0.4013241660300484 on epoch=237
05/30/2022 23:13:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.70 on epoch=239
05/30/2022 23:14:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.64 on epoch=242
05/30/2022 23:14:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.61 on epoch=244
05/30/2022 23:14:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.61 on epoch=247
05/30/2022 23:14:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.62 on epoch=249
05/30/2022 23:14:08 - INFO - __main__ - Global step 1000 Train loss 0.64 Classification-F1 0.544147465437788 on epoch=249
05/30/2022 23:14:08 - INFO - __main__ - Saving model with best Classification-F1: 0.508224669178016 -> 0.544147465437788 on epoch=249, global_step=1000
05/30/2022 23:14:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.55 on epoch=252
05/30/2022 23:14:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.51 on epoch=254
05/30/2022 23:14:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.66 on epoch=257
05/30/2022 23:14:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.58 on epoch=259
05/30/2022 23:14:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.55 on epoch=262
05/30/2022 23:14:21 - INFO - __main__ - Global step 1050 Train loss 0.57 Classification-F1 0.5112539766702014 on epoch=262
05/30/2022 23:14:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=264
05/30/2022 23:14:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=267
05/30/2022 23:14:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=269
05/30/2022 23:14:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=272
05/30/2022 23:14:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=274
05/30/2022 23:14:33 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.5041374989197132 on epoch=274
05/30/2022 23:14:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.49 on epoch=277
05/30/2022 23:14:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=279
05/30/2022 23:14:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=282
05/30/2022 23:14:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.34 on epoch=284
05/30/2022 23:14:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=287
05/30/2022 23:14:46 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.4792513687862525 on epoch=287
05/30/2022 23:14:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=289
05/30/2022 23:14:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=292
05/30/2022 23:14:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.31 on epoch=294
05/30/2022 23:14:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.48 on epoch=297
05/30/2022 23:14:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=299
05/30/2022 23:14:58 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.47956709956709953 on epoch=299
05/30/2022 23:15:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=302
05/30/2022 23:15:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.33 on epoch=304
05/30/2022 23:15:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=307
05/30/2022 23:15:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.30 on epoch=309
05/30/2022 23:15:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=312
05/30/2022 23:15:11 - INFO - __main__ - Global step 1250 Train loss 0.33 Classification-F1 0.4889853363690573 on epoch=312
05/30/2022 23:15:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.28 on epoch=314
05/30/2022 23:15:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.36 on epoch=317
05/30/2022 23:15:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.28 on epoch=319
05/30/2022 23:15:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=322
05/30/2022 23:15:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=324
05/30/2022 23:15:24 - INFO - __main__ - Global step 1300 Train loss 0.27 Classification-F1 0.5371620716448302 on epoch=324
05/30/2022 23:15:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=327
05/30/2022 23:15:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.28 on epoch=329
05/30/2022 23:15:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=332
05/30/2022 23:15:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=334
05/30/2022 23:15:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=337
05/30/2022 23:15:37 - INFO - __main__ - Global step 1350 Train loss 0.24 Classification-F1 0.5435064935064935 on epoch=337
05/30/2022 23:15:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=339
05/30/2022 23:15:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=342
05/30/2022 23:15:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=344
05/30/2022 23:15:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=347
05/30/2022 23:15:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=349
05/30/2022 23:15:50 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.5704184704184705 on epoch=349
05/30/2022 23:15:50 - INFO - __main__ - Saving model with best Classification-F1: 0.544147465437788 -> 0.5704184704184705 on epoch=349, global_step=1400
05/30/2022 23:15:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=352
05/30/2022 23:15:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=354
05/30/2022 23:15:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.21 on epoch=357
05/30/2022 23:15:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.13 on epoch=359
05/30/2022 23:16:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.18 on epoch=362
05/30/2022 23:16:03 - INFO - __main__ - Global step 1450 Train loss 0.18 Classification-F1 0.5337346258077965 on epoch=362
05/30/2022 23:16:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=364
05/30/2022 23:16:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.11 on epoch=367
05/30/2022 23:16:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=369
05/30/2022 23:16:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=372
05/30/2022 23:16:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=374
05/30/2022 23:16:16 - INFO - __main__ - Global step 1500 Train loss 0.15 Classification-F1 0.5337346258077965 on epoch=374
05/30/2022 23:16:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.15 on epoch=377
05/30/2022 23:16:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.15 on epoch=379
05/30/2022 23:16:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=382
05/30/2022 23:16:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=384
05/30/2022 23:16:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=387
05/30/2022 23:16:29 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.5710239651416121 on epoch=387
05/30/2022 23:16:29 - INFO - __main__ - Saving model with best Classification-F1: 0.5704184704184705 -> 0.5710239651416121 on epoch=387, global_step=1550
05/30/2022 23:16:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.09 on epoch=389
05/30/2022 23:16:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=392
05/30/2022 23:16:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=394
05/30/2022 23:16:39 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=397
05/30/2022 23:16:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=399
05/30/2022 23:16:42 - INFO - __main__ - Global step 1600 Train loss 0.13 Classification-F1 0.6083333333333334 on epoch=399
05/30/2022 23:16:42 - INFO - __main__ - Saving model with best Classification-F1: 0.5710239651416121 -> 0.6083333333333334 on epoch=399, global_step=1600
05/30/2022 23:16:44 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.18 on epoch=402
05/30/2022 23:16:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=404
05/30/2022 23:16:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=407
05/30/2022 23:16:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=409
05/30/2022 23:16:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=412
05/30/2022 23:16:55 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.5540542228885557 on epoch=412
05/30/2022 23:16:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=414
05/30/2022 23:17:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.08 on epoch=417
05/30/2022 23:17:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=419
05/30/2022 23:17:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=422
05/30/2022 23:17:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=424
05/30/2022 23:17:08 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.598119918699187 on epoch=424
05/30/2022 23:17:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.10 on epoch=427
05/30/2022 23:17:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=429
05/30/2022 23:17:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.16 on epoch=432
05/30/2022 23:17:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=434
05/30/2022 23:17:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=437
05/30/2022 23:17:21 - INFO - __main__ - Global step 1750 Train loss 0.16 Classification-F1 0.605518018018018 on epoch=437
05/30/2022 23:17:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=439
05/30/2022 23:17:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=442
05/30/2022 23:17:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=444
05/30/2022 23:17:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=447
05/30/2022 23:17:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=449
05/30/2022 23:17:34 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.665319055944056 on epoch=449
05/30/2022 23:17:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6083333333333334 -> 0.665319055944056 on epoch=449, global_step=1800
05/30/2022 23:17:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=452
05/30/2022 23:17:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=454
05/30/2022 23:17:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=457
05/30/2022 23:17:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
05/30/2022 23:17:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=462
05/30/2022 23:17:47 - INFO - __main__ - Global step 1850 Train loss 0.07 Classification-F1 0.5186486486486487 on epoch=462
05/30/2022 23:17:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.09 on epoch=464
05/30/2022 23:17:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=467
05/30/2022 23:17:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=469
05/30/2022 23:17:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=472
05/30/2022 23:17:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=474
05/30/2022 23:18:00 - INFO - __main__ - Global step 1900 Train loss 0.10 Classification-F1 0.6285714285714286 on epoch=474
05/30/2022 23:18:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=477
05/30/2022 23:18:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=479
05/30/2022 23:18:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=482
05/30/2022 23:18:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=484
05/30/2022 23:18:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=487
05/30/2022 23:18:13 - INFO - __main__ - Global step 1950 Train loss 0.10 Classification-F1 0.5631584341053495 on epoch=487
05/30/2022 23:18:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=489
05/30/2022 23:18:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=492
05/30/2022 23:18:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
05/30/2022 23:18:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=497
05/30/2022 23:18:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
05/30/2022 23:18:26 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.5664795672799146 on epoch=499
05/30/2022 23:18:28 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=502
05/30/2022 23:18:31 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=504
05/30/2022 23:18:33 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.12 on epoch=507
05/30/2022 23:18:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=509
05/30/2022 23:18:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=512
05/30/2022 23:18:39 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.6332375055285273 on epoch=512
05/30/2022 23:18:41 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=514
05/30/2022 23:18:44 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.10 on epoch=517
05/30/2022 23:18:46 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=519
05/30/2022 23:18:48 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=522
05/30/2022 23:18:51 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
05/30/2022 23:18:52 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.5594017094017094 on epoch=524
05/30/2022 23:18:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
05/30/2022 23:18:57 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
05/30/2022 23:18:59 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
05/30/2022 23:19:01 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=534
05/30/2022 23:19:04 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=537
05/30/2022 23:19:05 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.5735433858464616 on epoch=537
05/30/2022 23:19:07 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/30/2022 23:19:10 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
05/30/2022 23:19:12 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.07 on epoch=544
05/30/2022 23:19:14 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=547
05/30/2022 23:19:17 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
05/30/2022 23:19:18 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.5947916666666666 on epoch=549
05/30/2022 23:19:20 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=552
05/30/2022 23:19:23 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
05/30/2022 23:19:25 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.07 on epoch=557
05/30/2022 23:19:28 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
05/30/2022 23:19:30 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
05/30/2022 23:19:31 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.5094919786096257 on epoch=562
05/30/2022 23:19:33 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.09 on epoch=564
05/30/2022 23:19:36 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
05/30/2022 23:19:38 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/30/2022 23:19:41 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=572
05/30/2022 23:19:43 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
05/30/2022 23:19:44 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.6095430107526882 on epoch=574
05/30/2022 23:19:47 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/30/2022 23:19:49 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
05/30/2022 23:19:52 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/30/2022 23:19:54 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/30/2022 23:19:56 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=587
05/30/2022 23:19:58 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.5775171065493646 on epoch=587
05/30/2022 23:20:00 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
05/30/2022 23:20:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
05/30/2022 23:20:05 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
05/30/2022 23:20:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
05/30/2022 23:20:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
05/30/2022 23:20:11 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.5870096697682905 on epoch=599
05/30/2022 23:20:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=602
05/30/2022 23:20:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=604
05/30/2022 23:20:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=607
05/30/2022 23:20:20 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
05/30/2022 23:20:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=612
05/30/2022 23:20:24 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.589291101055807 on epoch=612
05/30/2022 23:20:26 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
05/30/2022 23:20:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/30/2022 23:20:31 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/30/2022 23:20:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=622
05/30/2022 23:20:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
05/30/2022 23:20:37 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.5665674603174603 on epoch=624
05/30/2022 23:20:40 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=627
05/30/2022 23:20:42 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=629
05/30/2022 23:20:44 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=632
05/30/2022 23:20:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=634
05/30/2022 23:20:49 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/30/2022 23:20:50 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.5951612903225807 on epoch=637
05/30/2022 23:20:52 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=639
05/30/2022 23:20:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/30/2022 23:20:57 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
05/30/2022 23:21:00 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
05/30/2022 23:21:02 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/30/2022 23:21:03 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.5927712639109698 on epoch=649
05/30/2022 23:21:05 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=652
05/30/2022 23:21:08 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=654
05/30/2022 23:21:10 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=657
05/30/2022 23:21:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
05/30/2022 23:21:15 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=662
05/30/2022 23:21:16 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.5548467944053683 on epoch=662
05/30/2022 23:21:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/30/2022 23:21:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=667
05/30/2022 23:21:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
05/30/2022 23:21:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/30/2022 23:21:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/30/2022 23:21:29 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.5825938767115237 on epoch=674
05/30/2022 23:21:31 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=677
05/30/2022 23:21:34 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=679
05/30/2022 23:21:36 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/30/2022 23:21:38 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/30/2022 23:21:41 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
05/30/2022 23:21:42 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.6085412953060012 on epoch=687
05/30/2022 23:21:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/30/2022 23:21:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
05/30/2022 23:21:49 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=694
05/30/2022 23:21:51 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
05/30/2022 23:21:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=699
05/30/2022 23:21:55 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.5951479892656364 on epoch=699
05/30/2022 23:21:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=702
05/30/2022 23:22:00 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=704
05/30/2022 23:22:02 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=707
05/30/2022 23:22:05 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=709
05/30/2022 23:22:07 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
05/30/2022 23:22:08 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.5761365051687632 on epoch=712
05/30/2022 23:22:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/30/2022 23:22:13 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.12 on epoch=717
05/30/2022 23:22:15 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/30/2022 23:22:18 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/30/2022 23:22:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/30/2022 23:22:21 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.6085412953060012 on epoch=724
05/30/2022 23:22:24 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=727
05/30/2022 23:22:26 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
05/30/2022 23:22:29 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.07 on epoch=732
05/30/2022 23:22:31 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
05/30/2022 23:22:33 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/30/2022 23:22:34 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.5740896358543417 on epoch=737
05/30/2022 23:22:37 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/30/2022 23:22:39 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/30/2022 23:22:42 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
05/30/2022 23:22:44 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.08 on epoch=747
05/30/2022 23:22:46 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=749
05/30/2022 23:22:48 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.5943722943722944 on epoch=749
05/30/2022 23:22:48 - INFO - __main__ - save last model!
05/30/2022 23:22:48 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:22:48 - INFO - __main__ - Printing 3 examples
05/30/2022 23:22:48 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/30/2022 23:22:48 - INFO - __main__ - ['others']
05/30/2022 23:22:48 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/30/2022 23:22:48 - INFO - __main__ - ['others']
05/30/2022 23:22:48 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/30/2022 23:22:48 - INFO - __main__ - ['others']
05/30/2022 23:22:48 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:22:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 23:22:48 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 23:22:48 - INFO - __main__ - Printing 3 examples
05/30/2022 23:22:48 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 23:22:48 - INFO - __main__ - ['others']
05/30/2022 23:22:48 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 23:22:48 - INFO - __main__ - ['others']
05/30/2022 23:22:48 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 23:22:48 - INFO - __main__ - ['others']
05/30/2022 23:22:48 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:22:48 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:22:48 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 23:22:48 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:22:48 - INFO - __main__ - Printing 3 examples
05/30/2022 23:22:48 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/30/2022 23:22:48 - INFO - __main__ - ['others']
05/30/2022 23:22:48 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/30/2022 23:22:48 - INFO - __main__ - ['others']
05/30/2022 23:22:48 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/30/2022 23:22:48 - INFO - __main__ - ['others']
05/30/2022 23:22:48 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:22:48 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:22:48 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:22:50 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:22:55 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 23:23:03 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 23:23:03 - INFO - __main__ - task name: emo
05/30/2022 23:23:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 23:23:04 - INFO - __main__ - Starting training!
05/30/2022 23:24:28 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_42_0.2_8_predictions.txt
05/30/2022 23:24:28 - INFO - __main__ - Classification-F1 on test data: 0.2809
05/30/2022 23:24:29 - INFO - __main__ - prefix=emo_16_42, lr=0.2, bsz=8, dev_performance=0.665319055944056, test_performance=0.2809425141231096
05/30/2022 23:24:29 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.5, bsz=8 ...
05/30/2022 23:24:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:24:30 - INFO - __main__ - Printing 3 examples
05/30/2022 23:24:30 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/30/2022 23:24:30 - INFO - __main__ - ['others']
05/30/2022 23:24:30 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/30/2022 23:24:30 - INFO - __main__ - ['others']
05/30/2022 23:24:30 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/30/2022 23:24:30 - INFO - __main__ - ['others']
05/30/2022 23:24:30 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:24:30 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:24:30 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 23:24:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:24:30 - INFO - __main__ - Printing 3 examples
05/30/2022 23:24:30 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/30/2022 23:24:30 - INFO - __main__ - ['others']
05/30/2022 23:24:30 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/30/2022 23:24:30 - INFO - __main__ - ['others']
05/30/2022 23:24:30 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/30/2022 23:24:30 - INFO - __main__ - ['others']
05/30/2022 23:24:30 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:24:30 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:24:30 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:24:45 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 23:24:45 - INFO - __main__ - task name: emo
05/30/2022 23:24:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 23:24:46 - INFO - __main__ - Starting training!
05/30/2022 23:24:49 - INFO - __main__ - Step 10 Global step 10 Train loss 6.90 on epoch=2
05/30/2022 23:24:51 - INFO - __main__ - Step 20 Global step 20 Train loss 2.97 on epoch=4
05/30/2022 23:24:54 - INFO - __main__ - Step 30 Global step 30 Train loss 1.78 on epoch=7
05/30/2022 23:24:56 - INFO - __main__ - Step 40 Global step 40 Train loss 1.45 on epoch=9
05/30/2022 23:24:58 - INFO - __main__ - Step 50 Global step 50 Train loss 1.26 on epoch=12
05/30/2022 23:24:59 - INFO - __main__ - Global step 50 Train loss 2.87 Classification-F1 0.1 on epoch=12
05/30/2022 23:24:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/30/2022 23:25:02 - INFO - __main__ - Step 60 Global step 60 Train loss 1.20 on epoch=14
05/30/2022 23:25:04 - INFO - __main__ - Step 70 Global step 70 Train loss 1.13 on epoch=17
05/30/2022 23:25:07 - INFO - __main__ - Step 80 Global step 80 Train loss 1.14 on epoch=19
05/30/2022 23:25:09 - INFO - __main__ - Step 90 Global step 90 Train loss 1.02 on epoch=22
05/30/2022 23:25:11 - INFO - __main__ - Step 100 Global step 100 Train loss 1.04 on epoch=24
05/30/2022 23:25:12 - INFO - __main__ - Global step 100 Train loss 1.11 Classification-F1 0.09615384615384615 on epoch=24
05/30/2022 23:25:15 - INFO - __main__ - Step 110 Global step 110 Train loss 1.12 on epoch=27
05/30/2022 23:25:17 - INFO - __main__ - Step 120 Global step 120 Train loss 1.07 on epoch=29
05/30/2022 23:25:19 - INFO - __main__ - Step 130 Global step 130 Train loss 1.06 on epoch=32
05/30/2022 23:25:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.90 on epoch=34
05/30/2022 23:25:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.94 on epoch=37
05/30/2022 23:25:25 - INFO - __main__ - Global step 150 Train loss 1.02 Classification-F1 0.13067758749069247 on epoch=37
05/30/2022 23:25:25 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.13067758749069247 on epoch=37, global_step=150
05/30/2022 23:25:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.90 on epoch=39
05/30/2022 23:25:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.96 on epoch=42
05/30/2022 23:25:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.99 on epoch=44
05/30/2022 23:25:35 - INFO - __main__ - Step 190 Global step 190 Train loss 1.02 on epoch=47
05/30/2022 23:25:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.85 on epoch=49
05/30/2022 23:25:38 - INFO - __main__ - Global step 200 Train loss 0.94 Classification-F1 0.1237183868762816 on epoch=49
05/30/2022 23:25:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.96 on epoch=52
05/30/2022 23:25:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.91 on epoch=54
05/30/2022 23:25:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.92 on epoch=57
05/30/2022 23:25:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.87 on epoch=59
05/30/2022 23:25:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.76 on epoch=62
05/30/2022 23:25:51 - INFO - __main__ - Global step 250 Train loss 0.88 Classification-F1 0.1 on epoch=62
05/30/2022 23:25:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.90 on epoch=64
05/30/2022 23:25:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.87 on epoch=67
05/30/2022 23:25:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.86 on epoch=69
05/30/2022 23:26:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.84 on epoch=72
05/30/2022 23:26:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.86 on epoch=74
05/30/2022 23:26:04 - INFO - __main__ - Global step 300 Train loss 0.87 Classification-F1 0.18969624776652771 on epoch=74
05/30/2022 23:26:04 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.18969624776652771 on epoch=74, global_step=300
05/30/2022 23:26:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.92 on epoch=77
05/30/2022 23:26:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.79 on epoch=79
05/30/2022 23:26:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.77 on epoch=82
05/30/2022 23:26:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.76 on epoch=84
05/30/2022 23:26:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.80 on epoch=87
05/30/2022 23:26:17 - INFO - __main__ - Global step 350 Train loss 0.81 Classification-F1 0.16109154929577466 on epoch=87
05/30/2022 23:26:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.69 on epoch=89
05/30/2022 23:26:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.78 on epoch=92
05/30/2022 23:26:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.72 on epoch=94
05/30/2022 23:26:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.70 on epoch=97
05/30/2022 23:26:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.66 on epoch=99
05/30/2022 23:26:30 - INFO - __main__ - Global step 400 Train loss 0.71 Classification-F1 0.2570048309178744 on epoch=99
05/30/2022 23:26:30 - INFO - __main__ - Saving model with best Classification-F1: 0.18969624776652771 -> 0.2570048309178744 on epoch=99, global_step=400
05/30/2022 23:26:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.70 on epoch=102
05/30/2022 23:26:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.60 on epoch=104
05/30/2022 23:26:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.64 on epoch=107
05/30/2022 23:26:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.67 on epoch=109
05/30/2022 23:26:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.56 on epoch=112
05/30/2022 23:26:43 - INFO - __main__ - Global step 450 Train loss 0.64 Classification-F1 0.6447616971810519 on epoch=112
05/30/2022 23:26:43 - INFO - __main__ - Saving model with best Classification-F1: 0.2570048309178744 -> 0.6447616971810519 on epoch=112, global_step=450
05/30/2022 23:26:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.55 on epoch=114
05/30/2022 23:26:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=117
05/30/2022 23:26:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.49 on epoch=119
05/30/2022 23:26:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.52 on epoch=122
05/30/2022 23:26:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.48 on epoch=124
05/30/2022 23:26:56 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.3896153846153846 on epoch=124
05/30/2022 23:26:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.53 on epoch=127
05/30/2022 23:27:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.52 on epoch=129
05/30/2022 23:27:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.56 on epoch=132
05/30/2022 23:27:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=134
05/30/2022 23:27:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=137
05/30/2022 23:27:08 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.4785539215686274 on epoch=137
05/30/2022 23:27:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.35 on epoch=139
05/30/2022 23:27:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=142
05/30/2022 23:27:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=144
05/30/2022 23:27:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=147
05/30/2022 23:27:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=149
05/30/2022 23:27:21 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.5763023594909862 on epoch=149
05/30/2022 23:27:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.32 on epoch=152
05/30/2022 23:27:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.28 on epoch=154
05/30/2022 23:27:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=157
05/30/2022 23:27:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=159
05/30/2022 23:27:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=162
05/30/2022 23:27:34 - INFO - __main__ - Global step 650 Train loss 0.31 Classification-F1 0.6930552232854865 on epoch=162
05/30/2022 23:27:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6447616971810519 -> 0.6930552232854865 on epoch=162, global_step=650
05/30/2022 23:27:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=164
05/30/2022 23:27:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=167
05/30/2022 23:27:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.29 on epoch=169
05/30/2022 23:27:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.31 on epoch=172
05/30/2022 23:27:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=174
05/30/2022 23:27:47 - INFO - __main__ - Global step 700 Train loss 0.27 Classification-F1 0.620600414078675 on epoch=174
05/30/2022 23:27:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=177
05/30/2022 23:27:52 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=179
05/30/2022 23:27:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=182
05/30/2022 23:27:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=184
05/30/2022 23:27:59 - INFO - __main__ - Step 750 Global step 750 Train loss 0.31 on epoch=187
05/30/2022 23:28:00 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.6829032696489592 on epoch=187
05/30/2022 23:28:03 - INFO - __main__ - Step 760 Global step 760 Train loss 0.19 on epoch=189
05/30/2022 23:28:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=192
05/30/2022 23:28:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=194
05/30/2022 23:28:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.17 on epoch=197
05/30/2022 23:28:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=199
05/30/2022 23:28:13 - INFO - __main__ - Global step 800 Train loss 0.18 Classification-F1 0.6055945352385842 on epoch=199
05/30/2022 23:28:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=202
05/30/2022 23:28:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=204
05/30/2022 23:28:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=207
05/30/2022 23:28:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.13 on epoch=209
05/30/2022 23:28:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=212
05/30/2022 23:28:26 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.6509454949944382 on epoch=212
05/30/2022 23:28:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=214
05/30/2022 23:28:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=217
05/30/2022 23:28:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=219
05/30/2022 23:28:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.12 on epoch=222
05/30/2022 23:28:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=224
05/30/2022 23:28:39 - INFO - __main__ - Global step 900 Train loss 0.11 Classification-F1 0.6509454949944382 on epoch=224
05/30/2022 23:28:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.13 on epoch=227
05/30/2022 23:28:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=229
05/30/2022 23:28:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=232
05/30/2022 23:28:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.12 on epoch=234
05/30/2022 23:28:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=237
05/30/2022 23:28:52 - INFO - __main__ - Global step 950 Train loss 0.10 Classification-F1 0.6826612903225806 on epoch=237
05/30/2022 23:28:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=239
05/30/2022 23:28:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.16 on epoch=242
05/30/2022 23:28:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.08 on epoch=244
05/30/2022 23:29:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=247
05/30/2022 23:29:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=249
05/30/2022 23:29:05 - INFO - __main__ - Global step 1000 Train loss 0.09 Classification-F1 0.6329967770457203 on epoch=249
05/30/2022 23:29:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=252
05/30/2022 23:29:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.10 on epoch=254
05/30/2022 23:29:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=257
05/30/2022 23:29:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=259
05/30/2022 23:29:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=262
05/30/2022 23:29:18 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.6675824175824177 on epoch=262
05/30/2022 23:29:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=264
05/30/2022 23:29:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=267
05/30/2022 23:29:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
05/30/2022 23:29:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=272
05/30/2022 23:29:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
05/30/2022 23:29:31 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.6717948717948719 on epoch=274
05/30/2022 23:29:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=277
05/30/2022 23:29:36 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=279
05/30/2022 23:29:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=282
05/30/2022 23:29:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.16 on epoch=284
05/30/2022 23:29:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
05/30/2022 23:29:44 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.6712885154061624 on epoch=287
05/30/2022 23:29:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=289
05/30/2022 23:29:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=292
05/30/2022 23:29:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
05/30/2022 23:29:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=297
05/30/2022 23:29:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=299
05/30/2022 23:29:57 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.6895502645502645 on epoch=299
05/30/2022 23:29:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=302
05/30/2022 23:30:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
05/30/2022 23:30:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
05/30/2022 23:30:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=309
05/30/2022 23:30:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
05/30/2022 23:30:10 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.5739486584107327 on epoch=312
05/30/2022 23:30:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=314
05/30/2022 23:30:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
05/30/2022 23:30:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=319
05/30/2022 23:30:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=322
05/30/2022 23:30:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
05/30/2022 23:30:23 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.679223125564589 on epoch=324
05/30/2022 23:30:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=327
05/30/2022 23:30:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
05/30/2022 23:30:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=332
05/30/2022 23:30:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
05/30/2022 23:30:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=337
05/30/2022 23:30:36 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.6748101042218688 on epoch=337
05/30/2022 23:30:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=339
05/30/2022 23:30:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
05/30/2022 23:30:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
05/30/2022 23:30:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
05/30/2022 23:30:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
05/30/2022 23:30:49 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.6646202236719477 on epoch=349
05/30/2022 23:30:51 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=352
05/30/2022 23:30:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
05/30/2022 23:30:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=357
05/30/2022 23:30:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=359
05/30/2022 23:31:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
05/30/2022 23:31:02 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.6808366546269772 on epoch=362
05/30/2022 23:31:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
05/30/2022 23:31:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
05/30/2022 23:31:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/30/2022 23:31:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
05/30/2022 23:31:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=374
05/30/2022 23:31:15 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.6281882868089765 on epoch=374
05/30/2022 23:31:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
05/30/2022 23:31:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/30/2022 23:31:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
05/30/2022 23:31:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
05/30/2022 23:31:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=387
05/30/2022 23:31:28 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.6638401559454192 on epoch=387
05/30/2022 23:31:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
05/30/2022 23:31:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=392
05/30/2022 23:31:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=394
05/30/2022 23:31:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/30/2022 23:31:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
05/30/2022 23:31:41 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.6615384615384615 on epoch=399
05/30/2022 23:31:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.13 on epoch=402
05/30/2022 23:31:45 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
05/30/2022 23:31:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/30/2022 23:31:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/30/2022 23:31:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/30/2022 23:31:54 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.6766899766899767 on epoch=412
05/30/2022 23:31:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=414
05/30/2022 23:31:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
05/30/2022 23:32:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/30/2022 23:32:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=422
05/30/2022 23:32:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
05/30/2022 23:32:07 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.6557966423820083 on epoch=424
05/30/2022 23:32:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/30/2022 23:32:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
05/30/2022 23:32:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/30/2022 23:32:17 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/30/2022 23:32:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
05/30/2022 23:32:20 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.6622596153846154 on epoch=437
05/30/2022 23:32:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=439
05/30/2022 23:32:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
05/30/2022 23:32:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/30/2022 23:32:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/30/2022 23:32:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=449
05/30/2022 23:32:34 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.6683787038388557 on epoch=449
05/30/2022 23:32:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/30/2022 23:32:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=454
05/30/2022 23:32:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
05/30/2022 23:32:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/30/2022 23:32:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.06 on epoch=462
05/30/2022 23:32:47 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.666285403050109 on epoch=462
05/30/2022 23:32:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/30/2022 23:32:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/30/2022 23:32:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
05/30/2022 23:32:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/30/2022 23:33:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
05/30/2022 23:33:01 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7112566810842672 on epoch=474
05/30/2022 23:33:01 - INFO - __main__ - Saving model with best Classification-F1: 0.6930552232854865 -> 0.7112566810842672 on epoch=474, global_step=1900
05/30/2022 23:33:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
05/30/2022 23:33:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/30/2022 23:33:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/30/2022 23:33:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/30/2022 23:33:13 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/30/2022 23:33:14 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6704382183908046 on epoch=487
05/30/2022 23:33:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=489
05/30/2022 23:33:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/30/2022 23:33:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/30/2022 23:33:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/30/2022 23:33:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/30/2022 23:33:28 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.6983301181577043 on epoch=499
05/30/2022 23:33:30 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/30/2022 23:33:33 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
05/30/2022 23:33:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
05/30/2022 23:33:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
05/30/2022 23:33:40 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
05/30/2022 23:33:41 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.7418677148278667 on epoch=512
05/30/2022 23:33:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7112566810842672 -> 0.7418677148278667 on epoch=512, global_step=2050
05/30/2022 23:33:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=514
05/30/2022 23:33:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/30/2022 23:33:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/30/2022 23:33:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/30/2022 23:33:54 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/30/2022 23:33:55 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.6820489844683393 on epoch=524
05/30/2022 23:33:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
05/30/2022 23:33:59 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=529
05/30/2022 23:34:02 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/30/2022 23:34:04 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/30/2022 23:34:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/30/2022 23:34:08 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6646945646945647 on epoch=537
05/30/2022 23:34:10 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/30/2022 23:34:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/30/2022 23:34:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/30/2022 23:34:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
05/30/2022 23:34:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
05/30/2022 23:34:21 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6654411764705883 on epoch=549
05/30/2022 23:34:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/30/2022 23:34:26 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/30/2022 23:34:29 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
05/30/2022 23:34:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/30/2022 23:34:34 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/30/2022 23:34:35 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7042105263157894 on epoch=562
05/30/2022 23:34:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/30/2022 23:34:40 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/30/2022 23:34:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/30/2022 23:34:45 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/30/2022 23:34:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/30/2022 23:34:48 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.6899248120300752 on epoch=574
05/30/2022 23:34:51 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=577
05/30/2022 23:34:53 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/30/2022 23:34:56 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/30/2022 23:34:58 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/30/2022 23:35:01 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/30/2022 23:35:02 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.6946065881549753 on epoch=587
05/30/2022 23:35:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/30/2022 23:35:07 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/30/2022 23:35:09 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
05/30/2022 23:35:12 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
05/30/2022 23:35:14 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=599
05/30/2022 23:35:15 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.6866866028708134 on epoch=599
05/30/2022 23:35:17 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/30/2022 23:35:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/30/2022 23:35:22 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/30/2022 23:35:25 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/30/2022 23:35:27 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/30/2022 23:35:28 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.6749617106153608 on epoch=612
05/30/2022 23:35:31 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
05/30/2022 23:35:33 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/30/2022 23:35:36 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/30/2022 23:35:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/30/2022 23:35:41 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/30/2022 23:35:42 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6622596153846154 on epoch=624
05/30/2022 23:35:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/30/2022 23:35:47 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.10 on epoch=629
05/30/2022 23:35:49 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/30/2022 23:35:52 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=634
05/30/2022 23:35:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/30/2022 23:35:55 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.6487179487179487 on epoch=637
05/30/2022 23:35:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/30/2022 23:36:00 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/30/2022 23:36:02 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/30/2022 23:36:05 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=647
05/30/2022 23:36:07 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
05/30/2022 23:36:08 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6742295016478579 on epoch=649
05/30/2022 23:36:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/30/2022 23:36:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/30/2022 23:36:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
05/30/2022 23:36:18 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/30/2022 23:36:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/30/2022 23:36:22 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7284780578898226 on epoch=662
05/30/2022 23:36:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/30/2022 23:36:27 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/30/2022 23:36:29 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/30/2022 23:36:32 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=672
05/30/2022 23:36:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/30/2022 23:36:35 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6809918091168091 on epoch=674
05/30/2022 23:36:38 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
05/30/2022 23:36:40 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=679
05/30/2022 23:36:43 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=682
05/30/2022 23:36:45 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/30/2022 23:36:47 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/30/2022 23:36:48 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.7042105263157894 on epoch=687
05/30/2022 23:36:51 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/30/2022 23:36:53 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/30/2022 23:36:56 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/30/2022 23:36:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/30/2022 23:37:01 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/30/2022 23:37:02 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6783671399594321 on epoch=699
05/30/2022 23:37:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=702
05/30/2022 23:37:07 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/30/2022 23:37:09 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=707
05/30/2022 23:37:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/30/2022 23:37:14 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/30/2022 23:37:15 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.6903846153846154 on epoch=712
05/30/2022 23:37:18 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/30/2022 23:37:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/30/2022 23:37:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/30/2022 23:37:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/30/2022 23:37:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
05/30/2022 23:37:28 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7119131782577923 on epoch=724
05/30/2022 23:37:31 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=727
05/30/2022 23:37:34 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=729
05/30/2022 23:37:36 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/30/2022 23:37:38 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/30/2022 23:37:41 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/30/2022 23:37:42 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7115261813537676 on epoch=737
05/30/2022 23:37:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/30/2022 23:37:47 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/30/2022 23:37:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
05/30/2022 23:37:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/30/2022 23:37:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/30/2022 23:37:55 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7119131782577923 on epoch=749
05/30/2022 23:37:55 - INFO - __main__ - save last model!
05/30/2022 23:37:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 23:37:55 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 23:37:55 - INFO - __main__ - Printing 3 examples
05/30/2022 23:37:55 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 23:37:55 - INFO - __main__ - ['others']
05/30/2022 23:37:55 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 23:37:55 - INFO - __main__ - ['others']
05/30/2022 23:37:55 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 23:37:55 - INFO - __main__ - ['others']
05/30/2022 23:37:55 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:37:56 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:37:56 - INFO - __main__ - Printing 3 examples
05/30/2022 23:37:56 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/30/2022 23:37:56 - INFO - __main__ - ['others']
05/30/2022 23:37:56 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/30/2022 23:37:56 - INFO - __main__ - ['others']
05/30/2022 23:37:56 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/30/2022 23:37:56 - INFO - __main__ - ['others']
05/30/2022 23:37:56 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:37:56 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:37:56 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 23:37:56 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:37:56 - INFO - __main__ - Printing 3 examples
05/30/2022 23:37:56 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/30/2022 23:37:56 - INFO - __main__ - ['others']
05/30/2022 23:37:56 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/30/2022 23:37:56 - INFO - __main__ - ['others']
05/30/2022 23:37:56 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/30/2022 23:37:56 - INFO - __main__ - ['others']
05/30/2022 23:37:56 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:37:56 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:37:56 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:37:58 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:38:03 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 23:38:11 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 23:38:11 - INFO - __main__ - task name: emo
05/30/2022 23:38:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 23:38:11 - INFO - __main__ - Starting training!
05/30/2022 23:39:25 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_87_0.5_8_predictions.txt
05/30/2022 23:39:25 - INFO - __main__ - Classification-F1 on test data: 0.4056
05/30/2022 23:39:25 - INFO - __main__ - prefix=emo_16_87, lr=0.5, bsz=8, dev_performance=0.7418677148278667, test_performance=0.40560803417407
05/30/2022 23:39:25 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.4, bsz=8 ...
05/30/2022 23:39:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:39:26 - INFO - __main__ - Printing 3 examples
05/30/2022 23:39:26 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/30/2022 23:39:26 - INFO - __main__ - ['others']
05/30/2022 23:39:26 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/30/2022 23:39:26 - INFO - __main__ - ['others']
05/30/2022 23:39:26 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/30/2022 23:39:26 - INFO - __main__ - ['others']
05/30/2022 23:39:26 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:39:26 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:39:26 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 23:39:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:39:26 - INFO - __main__ - Printing 3 examples
05/30/2022 23:39:26 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/30/2022 23:39:26 - INFO - __main__ - ['others']
05/30/2022 23:39:26 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/30/2022 23:39:26 - INFO - __main__ - ['others']
05/30/2022 23:39:26 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/30/2022 23:39:26 - INFO - __main__ - ['others']
05/30/2022 23:39:26 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:39:26 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:39:26 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:39:41 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 23:39:41 - INFO - __main__ - task name: emo
05/30/2022 23:39:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 23:39:42 - INFO - __main__ - Starting training!
05/30/2022 23:39:44 - INFO - __main__ - Step 10 Global step 10 Train loss 7.29 on epoch=2
05/30/2022 23:39:47 - INFO - __main__ - Step 20 Global step 20 Train loss 3.97 on epoch=4
05/30/2022 23:39:49 - INFO - __main__ - Step 30 Global step 30 Train loss 2.25 on epoch=7
05/30/2022 23:39:52 - INFO - __main__ - Step 40 Global step 40 Train loss 1.53 on epoch=9
05/30/2022 23:39:54 - INFO - __main__ - Step 50 Global step 50 Train loss 1.35 on epoch=12
05/30/2022 23:39:55 - INFO - __main__ - Global step 50 Train loss 3.28 Classification-F1 0.1 on epoch=12
05/30/2022 23:39:55 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/30/2022 23:39:57 - INFO - __main__ - Step 60 Global step 60 Train loss 1.30 on epoch=14
05/30/2022 23:40:00 - INFO - __main__ - Step 70 Global step 70 Train loss 1.14 on epoch=17
05/30/2022 23:40:02 - INFO - __main__ - Step 80 Global step 80 Train loss 1.05 on epoch=19
05/30/2022 23:40:05 - INFO - __main__ - Step 90 Global step 90 Train loss 1.10 on epoch=22
05/30/2022 23:40:07 - INFO - __main__ - Step 100 Global step 100 Train loss 1.02 on epoch=24
05/30/2022 23:40:08 - INFO - __main__ - Global step 100 Train loss 1.12 Classification-F1 0.1 on epoch=24
05/30/2022 23:40:11 - INFO - __main__ - Step 110 Global step 110 Train loss 1.16 on epoch=27
05/30/2022 23:40:13 - INFO - __main__ - Step 120 Global step 120 Train loss 1.00 on epoch=29
05/30/2022 23:40:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.98 on epoch=32
05/30/2022 23:40:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.96 on epoch=34
05/30/2022 23:40:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.96 on epoch=37
05/30/2022 23:40:21 - INFO - __main__ - Global step 150 Train loss 1.01 Classification-F1 0.1 on epoch=37
05/30/2022 23:40:24 - INFO - __main__ - Step 160 Global step 160 Train loss 0.89 on epoch=39
05/30/2022 23:40:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.92 on epoch=42
05/30/2022 23:40:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.91 on epoch=44
05/30/2022 23:40:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.90 on epoch=47
05/30/2022 23:40:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.98 on epoch=49
05/30/2022 23:40:34 - INFO - __main__ - Global step 200 Train loss 0.92 Classification-F1 0.13067758749069247 on epoch=49
05/30/2022 23:40:34 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.13067758749069247 on epoch=49, global_step=200
05/30/2022 23:40:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.93 on epoch=52
05/30/2022 23:40:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.91 on epoch=54
05/30/2022 23:40:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.87 on epoch=57
05/30/2022 23:40:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.90 on epoch=59
05/30/2022 23:40:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.91 on epoch=62
05/30/2022 23:40:47 - INFO - __main__ - Global step 250 Train loss 0.90 Classification-F1 0.1 on epoch=62
05/30/2022 23:40:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.89 on epoch=64
05/30/2022 23:40:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.90 on epoch=67
05/30/2022 23:40:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.89 on epoch=69
05/30/2022 23:40:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.85 on epoch=72
05/30/2022 23:40:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.88 on epoch=74
05/30/2022 23:41:00 - INFO - __main__ - Global step 300 Train loss 0.88 Classification-F1 0.09868421052631579 on epoch=74
05/30/2022 23:41:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.92 on epoch=77
05/30/2022 23:41:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.86 on epoch=79
05/30/2022 23:41:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.79 on epoch=82
05/30/2022 23:41:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.88 on epoch=84
05/30/2022 23:41:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.84 on epoch=87
05/30/2022 23:41:13 - INFO - __main__ - Global step 350 Train loss 0.86 Classification-F1 0.10126582278481013 on epoch=87
05/30/2022 23:41:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.89 on epoch=89
05/30/2022 23:41:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.83 on epoch=92
05/30/2022 23:41:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.86 on epoch=94
05/30/2022 23:41:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.82 on epoch=97
05/30/2022 23:41:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.85 on epoch=99
05/30/2022 23:41:26 - INFO - __main__ - Global step 400 Train loss 0.85 Classification-F1 0.2569444444444445 on epoch=99
05/30/2022 23:41:26 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.2569444444444445 on epoch=99, global_step=400
05/30/2022 23:41:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.81 on epoch=102
05/30/2022 23:41:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.81 on epoch=104
05/30/2022 23:41:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.76 on epoch=107
05/30/2022 23:41:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.70 on epoch=109
05/30/2022 23:41:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.77 on epoch=112
05/30/2022 23:41:39 - INFO - __main__ - Global step 450 Train loss 0.77 Classification-F1 0.3114975845410628 on epoch=112
05/30/2022 23:41:40 - INFO - __main__ - Saving model with best Classification-F1: 0.2569444444444445 -> 0.3114975845410628 on epoch=112, global_step=450
05/30/2022 23:41:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.80 on epoch=114
05/30/2022 23:41:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.78 on epoch=117
05/30/2022 23:41:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.77 on epoch=119
05/30/2022 23:41:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.76 on epoch=122
05/30/2022 23:41:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.80 on epoch=124
05/30/2022 23:41:53 - INFO - __main__ - Global step 500 Train loss 0.78 Classification-F1 0.23588588588588588 on epoch=124
05/30/2022 23:41:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.68 on epoch=127
05/30/2022 23:41:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.77 on epoch=129
05/30/2022 23:42:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.72 on epoch=132
05/30/2022 23:42:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.63 on epoch=134
05/30/2022 23:42:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.62 on epoch=137
05/30/2022 23:42:06 - INFO - __main__ - Global step 550 Train loss 0.68 Classification-F1 0.40175505955969615 on epoch=137
05/30/2022 23:42:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3114975845410628 -> 0.40175505955969615 on epoch=137, global_step=550
05/30/2022 23:42:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.64 on epoch=139
05/30/2022 23:42:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.60 on epoch=142
05/30/2022 23:42:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.51 on epoch=144
05/30/2022 23:42:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.52 on epoch=147
05/30/2022 23:42:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.52 on epoch=149
05/30/2022 23:42:19 - INFO - __main__ - Global step 600 Train loss 0.56 Classification-F1 0.576256308975203 on epoch=149
05/30/2022 23:42:19 - INFO - __main__ - Saving model with best Classification-F1: 0.40175505955969615 -> 0.576256308975203 on epoch=149, global_step=600
05/30/2022 23:42:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=152
05/30/2022 23:42:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=154
05/30/2022 23:42:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=157
05/30/2022 23:42:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=159
05/30/2022 23:42:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=162
05/30/2022 23:42:32 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.6301097178683386 on epoch=162
05/30/2022 23:42:32 - INFO - __main__ - Saving model with best Classification-F1: 0.576256308975203 -> 0.6301097178683386 on epoch=162, global_step=650
05/30/2022 23:42:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=164
05/30/2022 23:42:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=167
05/30/2022 23:42:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.34 on epoch=169
05/30/2022 23:42:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=172
05/30/2022 23:42:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=174
05/30/2022 23:42:45 - INFO - __main__ - Global step 700 Train loss 0.31 Classification-F1 0.7027375762859633 on epoch=174
05/30/2022 23:42:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6301097178683386 -> 0.7027375762859633 on epoch=174, global_step=700
05/30/2022 23:42:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=177
05/30/2022 23:42:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.27 on epoch=179
05/30/2022 23:42:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.32 on epoch=182
05/30/2022 23:42:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=184
05/30/2022 23:42:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=187
05/30/2022 23:42:58 - INFO - __main__ - Global step 750 Train loss 0.25 Classification-F1 0.6078571428571429 on epoch=187
05/30/2022 23:43:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.33 on epoch=189
05/30/2022 23:43:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.27 on epoch=192
05/30/2022 23:43:05 - INFO - __main__ - Step 780 Global step 780 Train loss 0.17 on epoch=194
05/30/2022 23:43:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.15 on epoch=197
05/30/2022 23:43:10 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=199
05/30/2022 23:43:11 - INFO - __main__ - Global step 800 Train loss 0.23 Classification-F1 0.6702380952380952 on epoch=199
05/30/2022 23:43:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.13 on epoch=202
05/30/2022 23:43:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=204
05/30/2022 23:43:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=207
05/30/2022 23:43:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.14 on epoch=209
05/30/2022 23:43:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.11 on epoch=212
05/30/2022 23:43:24 - INFO - __main__ - Global step 850 Train loss 0.15 Classification-F1 0.6975508501509614 on epoch=212
05/30/2022 23:43:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.13 on epoch=214
05/30/2022 23:43:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=217
05/30/2022 23:43:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=219
05/30/2022 23:43:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=222
05/30/2022 23:43:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.15 on epoch=224
05/30/2022 23:43:37 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.6820224950136738 on epoch=224
05/30/2022 23:43:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=227
05/30/2022 23:43:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=229
05/30/2022 23:43:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=232
05/30/2022 23:43:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=234
05/30/2022 23:43:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=237
05/30/2022 23:43:50 - INFO - __main__ - Global step 950 Train loss 0.07 Classification-F1 0.7174631180223285 on epoch=237
05/30/2022 23:43:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7027375762859633 -> 0.7174631180223285 on epoch=237, global_step=950
05/30/2022 23:43:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=239
05/30/2022 23:43:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=242
05/30/2022 23:43:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=244
05/30/2022 23:44:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=247
05/30/2022 23:44:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=249
05/30/2022 23:44:04 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.648310143174027 on epoch=249
05/30/2022 23:44:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
05/30/2022 23:44:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=254
05/30/2022 23:44:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.15 on epoch=257
05/30/2022 23:44:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=259
05/30/2022 23:44:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=262
05/30/2022 23:44:17 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.657928834235842 on epoch=262
05/30/2022 23:44:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=264
05/30/2022 23:44:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.10 on epoch=267
05/30/2022 23:44:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=269
05/30/2022 23:44:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=272
05/30/2022 23:44:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
05/30/2022 23:44:30 - INFO - __main__ - Global step 1100 Train loss 0.10 Classification-F1 0.693954248366013 on epoch=274
05/30/2022 23:44:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=277
05/30/2022 23:44:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
05/30/2022 23:44:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
05/30/2022 23:44:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=284
05/30/2022 23:44:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=287
05/30/2022 23:44:43 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.7089161231374181 on epoch=287
05/30/2022 23:44:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=289
05/30/2022 23:44:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=292
05/30/2022 23:44:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
05/30/2022 23:44:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
05/30/2022 23:44:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=299
05/30/2022 23:44:56 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.693954248366013 on epoch=299
05/30/2022 23:44:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=302
05/30/2022 23:45:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=304
05/30/2022 23:45:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
05/30/2022 23:45:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=309
05/30/2022 23:45:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
05/30/2022 23:45:10 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.6676587301587301 on epoch=312
05/30/2022 23:45:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=314
05/30/2022 23:45:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
05/30/2022 23:45:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.10 on epoch=319
05/30/2022 23:45:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
05/30/2022 23:45:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
05/30/2022 23:45:23 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.6822649572649573 on epoch=324
05/30/2022 23:45:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
05/30/2022 23:45:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=329
05/30/2022 23:45:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
05/30/2022 23:45:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/30/2022 23:45:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
05/30/2022 23:45:36 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.7340232683982684 on epoch=337
05/30/2022 23:45:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7174631180223285 -> 0.7340232683982684 on epoch=337, global_step=1350
05/30/2022 23:45:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
05/30/2022 23:45:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
05/30/2022 23:45:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
05/30/2022 23:45:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
05/30/2022 23:45:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
05/30/2022 23:45:49 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.7598039215686274 on epoch=349
05/30/2022 23:45:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7340232683982684 -> 0.7598039215686274 on epoch=349, global_step=1400
05/30/2022 23:45:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
05/30/2022 23:45:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/30/2022 23:45:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.15 on epoch=357
05/30/2022 23:45:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/30/2022 23:46:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/30/2022 23:46:03 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.6839806435394671 on epoch=362
05/30/2022 23:46:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
05/30/2022 23:46:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/30/2022 23:46:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=369
05/30/2022 23:46:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/30/2022 23:46:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=374
05/30/2022 23:46:16 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.7634622082897945 on epoch=374
05/30/2022 23:46:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7598039215686274 -> 0.7634622082897945 on epoch=374, global_step=1500
05/30/2022 23:46:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
05/30/2022 23:46:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
05/30/2022 23:46:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
05/30/2022 23:46:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
05/30/2022 23:46:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/30/2022 23:46:30 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.7046594982078853 on epoch=387
05/30/2022 23:46:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
05/30/2022 23:46:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=392
05/30/2022 23:46:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
05/30/2022 23:46:39 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/30/2022 23:46:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/30/2022 23:46:43 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.7483106931382794 on epoch=399
05/30/2022 23:46:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/30/2022 23:46:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
05/30/2022 23:46:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/30/2022 23:46:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=409
05/30/2022 23:46:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/30/2022 23:46:56 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7263592543904 on epoch=412
05/30/2022 23:46:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
05/30/2022 23:47:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=417
05/30/2022 23:47:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/30/2022 23:47:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
05/30/2022 23:47:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
05/30/2022 23:47:10 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.7049862955935062 on epoch=424
05/30/2022 23:47:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/30/2022 23:47:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/30/2022 23:47:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/30/2022 23:47:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/30/2022 23:47:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=437
05/30/2022 23:47:23 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.7032711205037181 on epoch=437
05/30/2022 23:47:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/30/2022 23:47:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/30/2022 23:47:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/30/2022 23:47:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/30/2022 23:47:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/30/2022 23:47:37 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.7131046508132158 on epoch=449
05/30/2022 23:47:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=452
05/30/2022 23:47:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/30/2022 23:47:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/30/2022 23:47:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
05/30/2022 23:47:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
05/30/2022 23:47:50 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.7346554651367017 on epoch=462
05/30/2022 23:47:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
05/30/2022 23:47:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/30/2022 23:47:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
05/30/2022 23:48:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/30/2022 23:48:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/30/2022 23:48:04 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7055555555555555 on epoch=474
05/30/2022 23:48:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
05/30/2022 23:48:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
05/30/2022 23:48:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/30/2022 23:48:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
05/30/2022 23:48:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
05/30/2022 23:48:17 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.712055281882868 on epoch=487
05/30/2022 23:48:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.09 on epoch=489
05/30/2022 23:48:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/30/2022 23:48:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
05/30/2022 23:48:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/30/2022 23:48:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
05/30/2022 23:48:31 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.6619444444444444 on epoch=499
05/30/2022 23:48:33 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/30/2022 23:48:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/30/2022 23:48:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/30/2022 23:48:40 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/30/2022 23:48:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/30/2022 23:48:44 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6332467532467533 on epoch=512
05/30/2022 23:48:46 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/30/2022 23:48:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
05/30/2022 23:48:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/30/2022 23:48:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/30/2022 23:48:56 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/30/2022 23:48:57 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.6443381180223285 on epoch=524
05/30/2022 23:49:00 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/30/2022 23:49:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/30/2022 23:49:05 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/30/2022 23:49:07 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/30/2022 23:49:10 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/30/2022 23:49:11 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7448275862068965 on epoch=537
05/30/2022 23:49:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/30/2022 23:49:16 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/30/2022 23:49:18 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/30/2022 23:49:21 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/30/2022 23:49:23 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/30/2022 23:49:25 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.703377446925834 on epoch=549
05/30/2022 23:49:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/30/2022 23:49:29 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/30/2022 23:49:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/30/2022 23:49:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/30/2022 23:49:37 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/30/2022 23:49:38 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.684773775380986 on epoch=562
05/30/2022 23:49:41 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=564
05/30/2022 23:49:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/30/2022 23:49:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/30/2022 23:49:48 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=572
05/30/2022 23:49:50 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=574
05/30/2022 23:49:51 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.7122622115015624 on epoch=574
05/30/2022 23:49:54 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/30/2022 23:49:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=579
05/30/2022 23:49:59 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/30/2022 23:50:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/30/2022 23:50:04 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/30/2022 23:50:05 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6841466776950648 on epoch=587
05/30/2022 23:50:07 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/30/2022 23:50:10 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=592
05/30/2022 23:50:12 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/30/2022 23:50:15 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=597
05/30/2022 23:50:17 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/30/2022 23:50:18 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.6839806435394671 on epoch=599
05/30/2022 23:50:21 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/30/2022 23:50:23 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=604
05/30/2022 23:50:26 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/30/2022 23:50:28 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/30/2022 23:50:31 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/30/2022 23:50:32 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7049862955935062 on epoch=612
05/30/2022 23:50:34 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/30/2022 23:50:37 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/30/2022 23:50:39 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/30/2022 23:50:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/30/2022 23:50:44 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/30/2022 23:50:45 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6619444444444444 on epoch=624
05/30/2022 23:50:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/30/2022 23:50:50 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/30/2022 23:50:53 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/30/2022 23:50:55 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=634
05/30/2022 23:50:58 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
05/30/2022 23:50:59 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6478334597236266 on epoch=637
05/30/2022 23:51:01 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=639
05/30/2022 23:51:04 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.08 on epoch=642
05/30/2022 23:51:06 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/30/2022 23:51:09 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.12 on epoch=647
05/30/2022 23:51:11 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/30/2022 23:51:12 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.6740841073271414 on epoch=649
05/30/2022 23:51:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=652
05/30/2022 23:51:17 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/30/2022 23:51:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/30/2022 23:51:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/30/2022 23:51:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/30/2022 23:51:26 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6839806435394671 on epoch=662
05/30/2022 23:51:28 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/30/2022 23:51:31 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/30/2022 23:51:33 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/30/2022 23:51:36 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/30/2022 23:51:38 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/30/2022 23:51:39 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6864677770749877 on epoch=674
05/30/2022 23:51:42 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/30/2022 23:51:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/30/2022 23:51:47 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/30/2022 23:51:49 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/30/2022 23:51:52 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/30/2022 23:51:53 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.684773775380986 on epoch=687
05/30/2022 23:51:55 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=689
05/30/2022 23:51:58 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/30/2022 23:52:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=694
05/30/2022 23:52:03 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/30/2022 23:52:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/30/2022 23:52:06 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6914767708885355 on epoch=699
05/30/2022 23:52:09 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/30/2022 23:52:11 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/30/2022 23:52:14 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/30/2022 23:52:16 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/30/2022 23:52:19 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/30/2022 23:52:20 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6981663929939792 on epoch=712
05/30/2022 23:52:22 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/30/2022 23:52:25 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/30/2022 23:52:27 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/30/2022 23:52:30 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/30/2022 23:52:32 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/30/2022 23:52:33 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.6902804132405651 on epoch=724
05/30/2022 23:52:36 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/30/2022 23:52:38 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/30/2022 23:52:41 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/30/2022 23:52:43 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
05/30/2022 23:52:46 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=737
05/30/2022 23:52:47 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6416863602347473 on epoch=737
05/30/2022 23:52:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
05/30/2022 23:52:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/30/2022 23:52:54 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/30/2022 23:52:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/30/2022 23:52:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/30/2022 23:53:00 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7263409961685823 on epoch=749
05/30/2022 23:53:00 - INFO - __main__ - save last model!
05/30/2022 23:53:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 23:53:00 - INFO - __main__ - Start tokenizing ... 5509 instances
05/30/2022 23:53:00 - INFO - __main__ - Printing 3 examples
05/30/2022 23:53:00 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/30/2022 23:53:00 - INFO - __main__ - ['others']
05/30/2022 23:53:00 - INFO - __main__ -  [emo] what you like very little things ok
05/30/2022 23:53:00 - INFO - __main__ - ['others']
05/30/2022 23:53:00 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/30/2022 23:53:00 - INFO - __main__ - ['others']
05/30/2022 23:53:00 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:53:00 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:53:00 - INFO - __main__ - Printing 3 examples
05/30/2022 23:53:00 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/30/2022 23:53:00 - INFO - __main__ - ['others']
05/30/2022 23:53:00 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/30/2022 23:53:00 - INFO - __main__ - ['others']
05/30/2022 23:53:00 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/30/2022 23:53:00 - INFO - __main__ - ['others']
05/30/2022 23:53:00 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:53:00 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:53:00 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 23:53:00 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:53:00 - INFO - __main__ - Printing 3 examples
05/30/2022 23:53:00 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/30/2022 23:53:00 - INFO - __main__ - ['others']
05/30/2022 23:53:00 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/30/2022 23:53:00 - INFO - __main__ - ['others']
05/30/2022 23:53:00 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/30/2022 23:53:00 - INFO - __main__ - ['others']
05/30/2022 23:53:00 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:53:00 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:53:01 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:53:02 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:53:08 - INFO - __main__ - Loaded 5509 examples from test data
05/30/2022 23:53:20 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 23:53:20 - INFO - __main__ - task name: emo
05/30/2022 23:53:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 23:53:21 - INFO - __main__ - Starting training!
05/30/2022 23:54:38 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_87_0.4_8_predictions.txt
05/30/2022 23:54:38 - INFO - __main__ - Classification-F1 on test data: 0.4391
05/30/2022 23:54:38 - INFO - __main__ - prefix=emo_16_87, lr=0.4, bsz=8, dev_performance=0.7634622082897945, test_performance=0.43905713558809206
05/30/2022 23:54:38 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.3, bsz=8 ...
05/30/2022 23:54:39 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:54:39 - INFO - __main__ - Printing 3 examples
05/30/2022 23:54:39 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/30/2022 23:54:39 - INFO - __main__ - ['others']
05/30/2022 23:54:39 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/30/2022 23:54:39 - INFO - __main__ - ['others']
05/30/2022 23:54:39 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/30/2022 23:54:39 - INFO - __main__ - ['others']
05/30/2022 23:54:39 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:54:39 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:54:39 - INFO - __main__ - Loaded 64 examples from train data
05/30/2022 23:54:39 - INFO - __main__ - Start tokenizing ... 64 instances
05/30/2022 23:54:39 - INFO - __main__ - Printing 3 examples
05/30/2022 23:54:39 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/30/2022 23:54:39 - INFO - __main__ - ['others']
05/30/2022 23:54:39 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/30/2022 23:54:39 - INFO - __main__ - ['others']
05/30/2022 23:54:39 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/30/2022 23:54:39 - INFO - __main__ - ['others']
05/30/2022 23:54:39 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:54:39 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:54:39 - INFO - __main__ - Loaded 64 examples from dev data
05/30/2022 23:54:54 - INFO - __main__ - try to initialize prompt embeddings
05/30/2022 23:54:54 - INFO - __main__ - task name: emo
05/30/2022 23:54:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/30/2022 23:54:55 - INFO - __main__ - Starting training!
05/30/2022 23:54:57 - INFO - __main__ - Step 10 Global step 10 Train loss 7.63 on epoch=2
05/30/2022 23:55:00 - INFO - __main__ - Step 20 Global step 20 Train loss 4.90 on epoch=4
05/30/2022 23:55:02 - INFO - __main__ - Step 30 Global step 30 Train loss 2.90 on epoch=7
05/30/2022 23:55:05 - INFO - __main__ - Step 40 Global step 40 Train loss 2.23 on epoch=9
05/30/2022 23:55:07 - INFO - __main__ - Step 50 Global step 50 Train loss 1.74 on epoch=12
05/30/2022 23:55:08 - INFO - __main__ - Global step 50 Train loss 3.88 Classification-F1 0.14695071010860483 on epoch=12
05/30/2022 23:55:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.14695071010860483 on epoch=12, global_step=50
05/30/2022 23:55:10 - INFO - __main__ - Step 60 Global step 60 Train loss 1.48 on epoch=14
05/30/2022 23:55:13 - INFO - __main__ - Step 70 Global step 70 Train loss 1.22 on epoch=17
05/30/2022 23:55:15 - INFO - __main__ - Step 80 Global step 80 Train loss 1.22 on epoch=19
05/30/2022 23:55:18 - INFO - __main__ - Step 90 Global step 90 Train loss 1.10 on epoch=22
05/30/2022 23:55:20 - INFO - __main__ - Step 100 Global step 100 Train loss 1.12 on epoch=24
05/30/2022 23:55:21 - INFO - __main__ - Global step 100 Train loss 1.23 Classification-F1 0.17773705909299126 on epoch=24
05/30/2022 23:55:21 - INFO - __main__ - Saving model with best Classification-F1: 0.14695071010860483 -> 0.17773705909299126 on epoch=24, global_step=100
05/30/2022 23:55:24 - INFO - __main__ - Step 110 Global step 110 Train loss 1.11 on epoch=27
05/30/2022 23:55:26 - INFO - __main__ - Step 120 Global step 120 Train loss 1.02 on epoch=29
05/30/2022 23:55:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.99 on epoch=32
05/30/2022 23:55:31 - INFO - __main__ - Step 140 Global step 140 Train loss 1.12 on epoch=34
05/30/2022 23:55:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.97 on epoch=37
05/30/2022 23:55:34 - INFO - __main__ - Global step 150 Train loss 1.04 Classification-F1 0.22769567597153803 on epoch=37
05/30/2022 23:55:34 - INFO - __main__ - Saving model with best Classification-F1: 0.17773705909299126 -> 0.22769567597153803 on epoch=37, global_step=150
05/30/2022 23:55:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.93 on epoch=39
05/30/2022 23:55:39 - INFO - __main__ - Step 170 Global step 170 Train loss 1.00 on epoch=42
05/30/2022 23:55:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.92 on epoch=44
05/30/2022 23:55:44 - INFO - __main__ - Step 190 Global step 190 Train loss 1.03 on epoch=47
05/30/2022 23:55:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.95 on epoch=49
05/30/2022 23:55:47 - INFO - __main__ - Global step 200 Train loss 0.96 Classification-F1 0.16483516483516486 on epoch=49
05/30/2022 23:55:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.98 on epoch=52
05/30/2022 23:55:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.93 on epoch=54
05/30/2022 23:55:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.93 on epoch=57
05/30/2022 23:55:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.91 on epoch=59
05/30/2022 23:55:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.91 on epoch=62
05/30/2022 23:56:00 - INFO - __main__ - Global step 250 Train loss 0.93 Classification-F1 0.19924242424242425 on epoch=62
05/30/2022 23:56:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.94 on epoch=64
05/30/2022 23:56:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.93 on epoch=67
05/30/2022 23:56:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.94 on epoch=69
05/30/2022 23:56:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.83 on epoch=72
05/30/2022 23:56:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.89 on epoch=74
05/30/2022 23:56:13 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.2734375 on epoch=74
05/30/2022 23:56:13 - INFO - __main__ - Saving model with best Classification-F1: 0.22769567597153803 -> 0.2734375 on epoch=74, global_step=300
05/30/2022 23:56:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.93 on epoch=77
05/30/2022 23:56:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.81 on epoch=79
05/30/2022 23:56:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.93 on epoch=82
05/30/2022 23:56:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.90 on epoch=84
05/30/2022 23:56:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.87 on epoch=87
05/30/2022 23:56:26 - INFO - __main__ - Global step 350 Train loss 0.89 Classification-F1 0.24615384615384614 on epoch=87
05/30/2022 23:56:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.87 on epoch=89
05/30/2022 23:56:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.87 on epoch=92
05/30/2022 23:56:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.92 on epoch=94
05/30/2022 23:56:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.81 on epoch=97
05/30/2022 23:56:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.73 on epoch=99
05/30/2022 23:56:40 - INFO - __main__ - Global step 400 Train loss 0.84 Classification-F1 0.2621763931548925 on epoch=99
05/30/2022 23:56:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.78 on epoch=102
05/30/2022 23:56:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.82 on epoch=104
05/30/2022 23:56:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.80 on epoch=107
05/30/2022 23:56:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.77 on epoch=109
05/30/2022 23:56:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.79 on epoch=112
05/30/2022 23:56:53 - INFO - __main__ - Global step 450 Train loss 0.79 Classification-F1 0.369281045751634 on epoch=112
05/30/2022 23:56:53 - INFO - __main__ - Saving model with best Classification-F1: 0.2734375 -> 0.369281045751634 on epoch=112, global_step=450
05/30/2022 23:56:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.74 on epoch=114
05/30/2022 23:56:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.80 on epoch=117
05/30/2022 23:57:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.77 on epoch=119
05/30/2022 23:57:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.66 on epoch=122
05/30/2022 23:57:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.71 on epoch=124
05/30/2022 23:57:06 - INFO - __main__ - Global step 500 Train loss 0.74 Classification-F1 0.23782608695652174 on epoch=124
05/30/2022 23:57:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.79 on epoch=127
05/30/2022 23:57:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.80 on epoch=129
05/30/2022 23:57:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.61 on epoch=132
05/30/2022 23:57:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.66 on epoch=134
05/30/2022 23:57:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.73 on epoch=137
05/30/2022 23:57:19 - INFO - __main__ - Global step 550 Train loss 0.72 Classification-F1 0.4655990698166059 on epoch=137
05/30/2022 23:57:19 - INFO - __main__ - Saving model with best Classification-F1: 0.369281045751634 -> 0.4655990698166059 on epoch=137, global_step=550
05/30/2022 23:57:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.76 on epoch=139
05/30/2022 23:57:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.71 on epoch=142
05/30/2022 23:57:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.56 on epoch=144
05/30/2022 23:57:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.63 on epoch=147
05/30/2022 23:57:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.59 on epoch=149
05/30/2022 23:57:32 - INFO - __main__ - Global step 600 Train loss 0.65 Classification-F1 0.4777590090090089 on epoch=149
05/30/2022 23:57:32 - INFO - __main__ - Saving model with best Classification-F1: 0.4655990698166059 -> 0.4777590090090089 on epoch=149, global_step=600
05/30/2022 23:57:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.59 on epoch=152
05/30/2022 23:57:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.57 on epoch=154
05/30/2022 23:57:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.63 on epoch=157
05/30/2022 23:57:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.55 on epoch=159
05/30/2022 23:57:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.58 on epoch=162
05/30/2022 23:57:45 - INFO - __main__ - Global step 650 Train loss 0.58 Classification-F1 0.6323542003985553 on epoch=162
05/30/2022 23:57:45 - INFO - __main__ - Saving model with best Classification-F1: 0.4777590090090089 -> 0.6323542003985553 on epoch=162, global_step=650
05/30/2022 23:57:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=164
05/30/2022 23:57:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.54 on epoch=167
05/30/2022 23:57:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=169
05/30/2022 23:57:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=172
05/30/2022 23:57:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.61 on epoch=174
05/30/2022 23:57:58 - INFO - __main__ - Global step 700 Train loss 0.52 Classification-F1 0.5209752321981425 on epoch=174
05/30/2022 23:58:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.53 on epoch=177
05/30/2022 23:58:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=179
05/30/2022 23:58:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.53 on epoch=182
05/30/2022 23:58:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=184
05/30/2022 23:58:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=187
05/30/2022 23:58:11 - INFO - __main__ - Global step 750 Train loss 0.50 Classification-F1 0.6841108452950558 on epoch=187
05/30/2022 23:58:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6323542003985553 -> 0.6841108452950558 on epoch=187, global_step=750
05/30/2022 23:58:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.48 on epoch=189
05/30/2022 23:58:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=192
05/30/2022 23:58:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=194
05/30/2022 23:58:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=197
05/30/2022 23:58:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=199
05/30/2022 23:58:25 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.5680239898989898 on epoch=199
05/30/2022 23:58:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=202
05/30/2022 23:58:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=204
05/30/2022 23:58:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=207
05/30/2022 23:58:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.33 on epoch=209
05/30/2022 23:58:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.33 on epoch=212
05/30/2022 23:58:38 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.5782438282438283 on epoch=212
05/30/2022 23:58:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.37 on epoch=214
05/30/2022 23:58:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.34 on epoch=217
05/30/2022 23:58:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=219
05/30/2022 23:58:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.31 on epoch=222
05/30/2022 23:58:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=224
05/30/2022 23:58:51 - INFO - __main__ - Global step 900 Train loss 0.35 Classification-F1 0.5624034749034749 on epoch=224
05/30/2022 23:58:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=227
05/30/2022 23:58:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.29 on epoch=229
05/30/2022 23:58:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.35 on epoch=232
05/30/2022 23:59:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.29 on epoch=234
05/30/2022 23:59:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=237
05/30/2022 23:59:04 - INFO - __main__ - Global step 950 Train loss 0.32 Classification-F1 0.6701415701415702 on epoch=237
05/30/2022 23:59:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=239
05/30/2022 23:59:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.27 on epoch=242
05/30/2022 23:59:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=244
05/30/2022 23:59:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.30 on epoch=247
05/30/2022 23:59:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.30 on epoch=249
05/30/2022 23:59:17 - INFO - __main__ - Global step 1000 Train loss 0.28 Classification-F1 0.6753104116822044 on epoch=249
05/30/2022 23:59:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.30 on epoch=252
05/30/2022 23:59:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=254
05/30/2022 23:59:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=257
05/30/2022 23:59:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.24 on epoch=259
05/30/2022 23:59:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.27 on epoch=262
05/30/2022 23:59:30 - INFO - __main__ - Global step 1050 Train loss 0.30 Classification-F1 0.6571789321789322 on epoch=262
05/30/2022 23:59:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.23 on epoch=264
05/30/2022 23:59:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=267
05/30/2022 23:59:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=269
05/30/2022 23:59:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=272
05/30/2022 23:59:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.29 on epoch=274
05/30/2022 23:59:44 - INFO - __main__ - Global step 1100 Train loss 0.24 Classification-F1 0.6571969696969697 on epoch=274
05/30/2022 23:59:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.25 on epoch=277
05/30/2022 23:59:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.17 on epoch=279
05/30/2022 23:59:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=282
05/30/2022 23:59:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=284
05/30/2022 23:59:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.22 on epoch=287
05/30/2022 23:59:57 - INFO - __main__ - Global step 1150 Train loss 0.19 Classification-F1 0.7054125816993464 on epoch=287
05/30/2022 23:59:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6841108452950558 -> 0.7054125816993464 on epoch=287, global_step=1150
05/30/2022 23:59:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.26 on epoch=289
05/31/2022 00:00:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=292
05/31/2022 00:00:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=294
05/31/2022 00:00:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.15 on epoch=297
05/31/2022 00:00:09 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.15 on epoch=299
05/31/2022 00:00:10 - INFO - __main__ - Global step 1200 Train loss 0.18 Classification-F1 0.7007530834914611 on epoch=299
05/31/2022 00:00:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.14 on epoch=302
05/31/2022 00:00:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.14 on epoch=304
05/31/2022 00:00:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=307
05/31/2022 00:00:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=309
05/31/2022 00:00:22 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=312
05/31/2022 00:00:23 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.7083680518463127 on epoch=312
05/31/2022 00:00:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7054125816993464 -> 0.7083680518463127 on epoch=312, global_step=1250
05/31/2022 00:00:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.10 on epoch=314
05/31/2022 00:00:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.13 on epoch=317
05/31/2022 00:00:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=319
05/31/2022 00:00:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.11 on epoch=322
05/31/2022 00:00:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=324
05/31/2022 00:00:37 - INFO - __main__ - Global step 1300 Train loss 0.13 Classification-F1 0.7036805555555555 on epoch=324
05/31/2022 00:00:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.13 on epoch=327
05/31/2022 00:00:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.29 on epoch=329
05/31/2022 00:00:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=332
05/31/2022 00:00:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.09 on epoch=334
05/31/2022 00:00:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=337
05/31/2022 00:00:50 - INFO - __main__ - Global step 1350 Train loss 0.15 Classification-F1 0.6885483870967742 on epoch=337
05/31/2022 00:00:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=339
05/31/2022 00:00:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=342
05/31/2022 00:00:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.12 on epoch=344
05/31/2022 00:01:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=347
05/31/2022 00:01:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=349
05/31/2022 00:01:03 - INFO - __main__ - Global step 1400 Train loss 0.09 Classification-F1 0.7176767676767677 on epoch=349
05/31/2022 00:01:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7083680518463127 -> 0.7176767676767677 on epoch=349, global_step=1400
05/31/2022 00:01:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.10 on epoch=352
05/31/2022 00:01:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
05/31/2022 00:01:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=357
05/31/2022 00:01:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=359
05/31/2022 00:01:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.12 on epoch=362
05/31/2022 00:01:16 - INFO - __main__ - Global step 1450 Train loss 0.09 Classification-F1 0.675202843751231 on epoch=362
05/31/2022 00:01:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=364
05/31/2022 00:01:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.11 on epoch=367
05/31/2022 00:01:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.09 on epoch=369
05/31/2022 00:01:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=372
05/31/2022 00:01:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=374
05/31/2022 00:01:30 - INFO - __main__ - Global step 1500 Train loss 0.11 Classification-F1 0.6894885580369452 on epoch=374
05/31/2022 00:01:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=377
05/31/2022 00:01:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=379
05/31/2022 00:01:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=382
05/31/2022 00:01:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=384
05/31/2022 00:01:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=387
05/31/2022 00:01:44 - INFO - __main__ - Global step 1550 Train loss 0.07 Classification-F1 0.6885989010989011 on epoch=387
05/31/2022 00:01:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=389
05/31/2022 00:01:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=392
05/31/2022 00:01:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=394
05/31/2022 00:01:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
05/31/2022 00:01:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=399
05/31/2022 00:01:57 - INFO - __main__ - Global step 1600 Train loss 0.09 Classification-F1 0.6602016418592505 on epoch=399
05/31/2022 00:01:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=402
05/31/2022 00:02:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=404
05/31/2022 00:02:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.12 on epoch=407
05/31/2022 00:02:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=409
05/31/2022 00:02:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=412
05/31/2022 00:02:10 - INFO - __main__ - Global step 1650 Train loss 0.10 Classification-F1 0.7036805555555555 on epoch=412
05/31/2022 00:02:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
05/31/2022 00:02:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=417
05/31/2022 00:02:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=419
05/31/2022 00:02:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
05/31/2022 00:02:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=424
05/31/2022 00:02:23 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.711866125760649 on epoch=424
05/31/2022 00:02:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.16 on epoch=427
05/31/2022 00:02:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
05/31/2022 00:02:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=432
05/31/2022 00:02:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.14 on epoch=434
05/31/2022 00:02:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=437
05/31/2022 00:02:36 - INFO - __main__ - Global step 1750 Train loss 0.12 Classification-F1 0.703853238265003 on epoch=437
05/31/2022 00:02:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
05/31/2022 00:02:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=442
05/31/2022 00:02:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
05/31/2022 00:02:45 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=447
05/31/2022 00:02:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
05/31/2022 00:02:49 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.7058327497665733 on epoch=449
05/31/2022 00:02:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=452
05/31/2022 00:02:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=454
05/31/2022 00:02:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=457
05/31/2022 00:02:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=459
05/31/2022 00:03:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=462
05/31/2022 00:03:02 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.6660173160173161 on epoch=462
05/31/2022 00:03:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.14 on epoch=464
05/31/2022 00:03:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=467
05/31/2022 00:03:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=469
05/31/2022 00:03:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=472
05/31/2022 00:03:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=474
05/31/2022 00:03:15 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.6944099378881987 on epoch=474
05/31/2022 00:03:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=477
05/31/2022 00:03:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
05/31/2022 00:03:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=482
05/31/2022 00:03:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/31/2022 00:03:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=487
05/31/2022 00:03:28 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.6943165672065927 on epoch=487
05/31/2022 00:03:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/31/2022 00:03:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
05/31/2022 00:03:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=494
05/31/2022 00:03:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=497
05/31/2022 00:03:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=499
05/31/2022 00:03:41 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.695133560670645 on epoch=499
05/31/2022 00:03:43 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
05/31/2022 00:03:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=504
05/31/2022 00:03:48 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
05/31/2022 00:03:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
05/31/2022 00:03:53 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=512
05/31/2022 00:03:54 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.7313025210084033 on epoch=512
05/31/2022 00:03:54 - INFO - __main__ - Saving model with best Classification-F1: 0.7176767676767677 -> 0.7313025210084033 on epoch=512, global_step=2050
05/31/2022 00:03:57 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/31/2022 00:03:59 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
05/31/2022 00:04:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/31/2022 00:04:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
05/31/2022 00:04:06 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
05/31/2022 00:04:07 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.7176767676767677 on epoch=524
05/31/2022 00:04:10 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/31/2022 00:04:12 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/31/2022 00:04:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=532
05/31/2022 00:04:17 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/31/2022 00:04:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
05/31/2022 00:04:20 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.7026839826839827 on epoch=537
05/31/2022 00:04:23 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.06 on epoch=539
05/31/2022 00:04:25 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
05/31/2022 00:04:27 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.12 on epoch=544
05/31/2022 00:04:30 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=547
05/31/2022 00:04:32 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=549
05/31/2022 00:04:33 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.6951335606706451 on epoch=549
05/31/2022 00:04:36 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/31/2022 00:04:38 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=554
05/31/2022 00:04:40 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=557
05/31/2022 00:04:43 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
05/31/2022 00:04:45 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=562
05/31/2022 00:04:46 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.6885483870967742 on epoch=562
05/31/2022 00:04:49 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
05/31/2022 00:04:51 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=567
05/31/2022 00:04:53 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/31/2022 00:04:56 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.16 on epoch=572
05/31/2022 00:04:58 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.14 on epoch=574
05/31/2022 00:04:59 - INFO - __main__ - Global step 2300 Train loss 0.08 Classification-F1 0.6724671195259431 on epoch=574
05/31/2022 00:05:02 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/31/2022 00:05:04 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
05/31/2022 00:05:06 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/31/2022 00:05:09 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/31/2022 00:05:11 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.11 on epoch=587
05/31/2022 00:05:12 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.6662831097613706 on epoch=587
05/31/2022 00:05:14 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
05/31/2022 00:05:17 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=592
05/31/2022 00:05:19 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=594
05/31/2022 00:05:22 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/31/2022 00:05:24 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=599
05/31/2022 00:05:25 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.7031499202551834 on epoch=599
05/31/2022 00:05:28 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
05/31/2022 00:05:30 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/31/2022 00:05:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/31/2022 00:05:35 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.07 on epoch=609
05/31/2022 00:05:37 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/31/2022 00:05:38 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7043965838083485 on epoch=612
05/31/2022 00:05:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/31/2022 00:05:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=617
05/31/2022 00:05:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/31/2022 00:05:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=622
05/31/2022 00:05:50 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/31/2022 00:05:51 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.7042239010989011 on epoch=624
05/31/2022 00:05:54 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/31/2022 00:05:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=629
05/31/2022 00:05:59 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/31/2022 00:06:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.10 on epoch=634
05/31/2022 00:06:03 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/31/2022 00:06:04 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.6827731092436975 on epoch=637
05/31/2022 00:06:07 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/31/2022 00:06:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/31/2022 00:06:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=644
05/31/2022 00:06:14 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.11 on epoch=647
05/31/2022 00:06:16 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=649
05/31/2022 00:06:17 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.671067821067821 on epoch=649
05/31/2022 00:06:20 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/31/2022 00:06:22 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=654
05/31/2022 00:06:24 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/31/2022 00:06:27 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/31/2022 00:06:29 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=662
05/31/2022 00:06:30 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.6486568712850994 on epoch=662
05/31/2022 00:06:33 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.10 on epoch=664
05/31/2022 00:06:35 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
05/31/2022 00:06:37 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/31/2022 00:06:40 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/31/2022 00:06:42 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/31/2022 00:06:43 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.6951335606706451 on epoch=674
05/31/2022 00:06:46 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/31/2022 00:06:48 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/31/2022 00:06:50 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
05/31/2022 00:06:53 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/31/2022 00:06:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=687
05/31/2022 00:06:56 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.6951335606706451 on epoch=687
05/31/2022 00:06:59 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
05/31/2022 00:07:01 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/31/2022 00:07:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=694
05/31/2022 00:07:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=697
05/31/2022 00:07:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/31/2022 00:07:09 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.6577612371730018 on epoch=699
05/31/2022 00:07:12 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/31/2022 00:07:14 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/31/2022 00:07:17 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=707
05/31/2022 00:07:19 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/31/2022 00:07:21 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/31/2022 00:07:22 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6942102407844769 on epoch=712
05/31/2022 00:07:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/31/2022 00:07:27 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=717
05/31/2022 00:07:30 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/31/2022 00:07:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=722
05/31/2022 00:07:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/31/2022 00:07:35 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.7036805555555555 on epoch=724
05/31/2022 00:07:38 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=727
05/31/2022 00:07:40 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/31/2022 00:07:43 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
05/31/2022 00:07:45 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
05/31/2022 00:07:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/31/2022 00:07:48 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.7176767676767677 on epoch=737
05/31/2022 00:07:51 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/31/2022 00:07:53 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/31/2022 00:07:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=744
05/31/2022 00:07:58 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=747
05/31/2022 00:08:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/31/2022 00:08:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:08:01 - INFO - __main__ - Printing 3 examples
05/31/2022 00:08:01 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/31/2022 00:08:01 - INFO - __main__ - ['others']
05/31/2022 00:08:01 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/31/2022 00:08:01 - INFO - __main__ - ['others']
05/31/2022 00:08:01 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/31/2022 00:08:01 - INFO - __main__ - ['others']
05/31/2022 00:08:01 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:08:02 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.6686388339920949 on epoch=749
05/31/2022 00:08:02 - INFO - __main__ - save last model!
05/31/2022 00:08:02 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:08:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 00:08:02 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 00:08:02 - INFO - __main__ - Printing 3 examples
05/31/2022 00:08:02 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 00:08:02 - INFO - __main__ - ['others']
05/31/2022 00:08:02 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 00:08:02 - INFO - __main__ - ['others']
05/31/2022 00:08:02 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 00:08:02 - INFO - __main__ - ['others']
05/31/2022 00:08:02 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:08:02 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 00:08:02 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:08:02 - INFO - __main__ - Printing 3 examples
05/31/2022 00:08:02 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/31/2022 00:08:02 - INFO - __main__ - ['others']
05/31/2022 00:08:02 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/31/2022 00:08:02 - INFO - __main__ - ['others']
05/31/2022 00:08:02 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/31/2022 00:08:02 - INFO - __main__ - ['others']
05/31/2022 00:08:02 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:08:02 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:08:02 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 00:08:04 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:08:09 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 00:08:20 - INFO - __main__ - try to initialize prompt embeddings
05/31/2022 00:08:20 - INFO - __main__ - task name: emo
05/31/2022 00:08:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/31/2022 00:08:21 - INFO - __main__ - Starting training!
05/31/2022 00:09:42 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_87_0.3_8_predictions.txt
05/31/2022 00:09:42 - INFO - __main__ - Classification-F1 on test data: 0.3943
05/31/2022 00:09:42 - INFO - __main__ - prefix=emo_16_87, lr=0.3, bsz=8, dev_performance=0.7313025210084033, test_performance=0.3942515401973372
05/31/2022 00:09:42 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.2, bsz=8 ...
05/31/2022 00:09:43 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:09:43 - INFO - __main__ - Printing 3 examples
05/31/2022 00:09:43 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/31/2022 00:09:43 - INFO - __main__ - ['others']
05/31/2022 00:09:43 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/31/2022 00:09:43 - INFO - __main__ - ['others']
05/31/2022 00:09:43 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/31/2022 00:09:43 - INFO - __main__ - ['others']
05/31/2022 00:09:43 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:09:43 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:09:43 - INFO - __main__ - Loaded 64 examples from train data
05/31/2022 00:09:43 - INFO - __main__ - Start tokenizing ... 64 instances
05/31/2022 00:09:43 - INFO - __main__ - Printing 3 examples
05/31/2022 00:09:43 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/31/2022 00:09:43 - INFO - __main__ - ['others']
05/31/2022 00:09:43 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/31/2022 00:09:43 - INFO - __main__ - ['others']
05/31/2022 00:09:43 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/31/2022 00:09:43 - INFO - __main__ - ['others']
05/31/2022 00:09:43 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:09:43 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:09:43 - INFO - __main__ - Loaded 64 examples from dev data
05/31/2022 00:09:58 - INFO - __main__ - try to initialize prompt embeddings
05/31/2022 00:09:58 - INFO - __main__ - task name: emo
05/31/2022 00:09:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/31/2022 00:09:59 - INFO - __main__ - Starting training!
05/31/2022 00:10:02 - INFO - __main__ - Step 10 Global step 10 Train loss 7.90 on epoch=2
05/31/2022 00:10:04 - INFO - __main__ - Step 20 Global step 20 Train loss 6.32 on epoch=4
05/31/2022 00:10:07 - INFO - __main__ - Step 30 Global step 30 Train loss 4.61 on epoch=7
05/31/2022 00:10:10 - INFO - __main__ - Step 40 Global step 40 Train loss 2.91 on epoch=9
05/31/2022 00:10:12 - INFO - __main__ - Step 50 Global step 50 Train loss 2.34 on epoch=12
05/31/2022 00:10:13 - INFO - __main__ - Global step 50 Train loss 4.82 Classification-F1 0.1 on epoch=12
05/31/2022 00:10:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/31/2022 00:10:15 - INFO - __main__ - Step 60 Global step 60 Train loss 1.83 on epoch=14
05/31/2022 00:10:18 - INFO - __main__ - Step 70 Global step 70 Train loss 1.70 on epoch=17
05/31/2022 00:10:20 - INFO - __main__ - Step 80 Global step 80 Train loss 1.44 on epoch=19
05/31/2022 00:10:23 - INFO - __main__ - Step 90 Global step 90 Train loss 1.34 on epoch=22
05/31/2022 00:10:25 - INFO - __main__ - Step 100 Global step 100 Train loss 1.21 on epoch=24
05/31/2022 00:10:26 - INFO - __main__ - Global step 100 Train loss 1.50 Classification-F1 0.08783783783783784 on epoch=24
05/31/2022 00:10:29 - INFO - __main__ - Step 110 Global step 110 Train loss 1.31 on epoch=27
05/31/2022 00:10:31 - INFO - __main__ - Step 120 Global step 120 Train loss 1.19 on epoch=29
05/31/2022 00:10:34 - INFO - __main__ - Step 130 Global step 130 Train loss 1.10 on epoch=32
05/31/2022 00:10:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.96 on epoch=34
05/31/2022 00:10:39 - INFO - __main__ - Step 150 Global step 150 Train loss 1.04 on epoch=37
05/31/2022 00:10:39 - INFO - __main__ - Global step 150 Train loss 1.12 Classification-F1 0.1 on epoch=37
05/31/2022 00:10:42 - INFO - __main__ - Step 160 Global step 160 Train loss 1.03 on epoch=39
05/31/2022 00:10:44 - INFO - __main__ - Step 170 Global step 170 Train loss 1.01 on epoch=42
05/31/2022 00:10:47 - INFO - __main__ - Step 180 Global step 180 Train loss 1.16 on epoch=44
05/31/2022 00:10:49 - INFO - __main__ - Step 190 Global step 190 Train loss 1.12 on epoch=47
05/31/2022 00:10:52 - INFO - __main__ - Step 200 Global step 200 Train loss 1.00 on epoch=49
05/31/2022 00:10:52 - INFO - __main__ - Global step 200 Train loss 1.06 Classification-F1 0.12635135135135137 on epoch=49
05/31/2022 00:10:53 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.12635135135135137 on epoch=49, global_step=200
05/31/2022 00:10:55 - INFO - __main__ - Step 210 Global step 210 Train loss 1.02 on epoch=52
05/31/2022 00:10:57 - INFO - __main__ - Step 220 Global step 220 Train loss 1.05 on epoch=54
05/31/2022 00:11:00 - INFO - __main__ - Step 230 Global step 230 Train loss 1.01 on epoch=57
05/31/2022 00:11:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.97 on epoch=59
05/31/2022 00:11:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.93 on epoch=62
05/31/2022 00:11:06 - INFO - __main__ - Global step 250 Train loss 0.99 Classification-F1 0.13026315789473686 on epoch=62
05/31/2022 00:11:06 - INFO - __main__ - Saving model with best Classification-F1: 0.12635135135135137 -> 0.13026315789473686 on epoch=62, global_step=250
05/31/2022 00:11:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.91 on epoch=64
05/31/2022 00:11:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.95 on epoch=67
05/31/2022 00:11:13 - INFO - __main__ - Step 280 Global step 280 Train loss 1.00 on epoch=69
05/31/2022 00:11:15 - INFO - __main__ - Step 290 Global step 290 Train loss 1.01 on epoch=72
05/31/2022 00:11:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.94 on epoch=74
05/31/2022 00:11:19 - INFO - __main__ - Global step 300 Train loss 0.96 Classification-F1 0.24789661319073086 on epoch=74
05/31/2022 00:11:19 - INFO - __main__ - Saving model with best Classification-F1: 0.13026315789473686 -> 0.24789661319073086 on epoch=74, global_step=300
05/31/2022 00:11:21 - INFO - __main__ - Step 310 Global step 310 Train loss 1.00 on epoch=77
05/31/2022 00:11:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.95 on epoch=79
05/31/2022 00:11:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.93 on epoch=82
05/31/2022 00:11:29 - INFO - __main__ - Step 340 Global step 340 Train loss 1.02 on epoch=84
05/31/2022 00:11:31 - INFO - __main__ - Step 350 Global step 350 Train loss 1.03 on epoch=87
05/31/2022 00:11:32 - INFO - __main__ - Global step 350 Train loss 0.99 Classification-F1 0.13444444444444445 on epoch=87
05/31/2022 00:11:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.81 on epoch=89
05/31/2022 00:11:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.98 on epoch=92
05/31/2022 00:11:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.94 on epoch=94
05/31/2022 00:11:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.91 on epoch=97
05/31/2022 00:11:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.84 on epoch=99
05/31/2022 00:11:45 - INFO - __main__ - Global step 400 Train loss 0.90 Classification-F1 0.0974025974025974 on epoch=99
05/31/2022 00:11:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.91 on epoch=102
05/31/2022 00:11:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.85 on epoch=104
05/31/2022 00:11:52 - INFO - __main__ - Step 430 Global step 430 Train loss 1.02 on epoch=107
05/31/2022 00:11:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.89 on epoch=109
05/31/2022 00:11:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.87 on epoch=112
05/31/2022 00:11:58 - INFO - __main__ - Global step 450 Train loss 0.91 Classification-F1 0.1 on epoch=112
05/31/2022 00:12:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.82 on epoch=114
05/31/2022 00:12:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.87 on epoch=117
05/31/2022 00:12:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.96 on epoch=119
05/31/2022 00:12:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.88 on epoch=122
05/31/2022 00:12:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.81 on epoch=124
05/31/2022 00:12:11 - INFO - __main__ - Global step 500 Train loss 0.87 Classification-F1 0.28601953601953606 on epoch=124
05/31/2022 00:12:11 - INFO - __main__ - Saving model with best Classification-F1: 0.24789661319073086 -> 0.28601953601953606 on epoch=124, global_step=500
05/31/2022 00:12:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.90 on epoch=127
05/31/2022 00:12:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.78 on epoch=129
05/31/2022 00:12:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.91 on epoch=132
05/31/2022 00:12:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.81 on epoch=134
05/31/2022 00:12:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.88 on epoch=137
05/31/2022 00:12:24 - INFO - __main__ - Global step 550 Train loss 0.86 Classification-F1 0.3541224128540305 on epoch=137
05/31/2022 00:12:24 - INFO - __main__ - Saving model with best Classification-F1: 0.28601953601953606 -> 0.3541224128540305 on epoch=137, global_step=550
05/31/2022 00:12:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.83 on epoch=139
05/31/2022 00:12:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.82 on epoch=142
05/31/2022 00:12:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.85 on epoch=144
05/31/2022 00:12:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.78 on epoch=147
05/31/2022 00:12:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.86 on epoch=149
05/31/2022 00:12:37 - INFO - __main__ - Global step 600 Train loss 0.83 Classification-F1 0.21655462184873953 on epoch=149
05/31/2022 00:12:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.84 on epoch=152
05/31/2022 00:12:42 - INFO - __main__ - Step 620 Global step 620 Train loss 0.78 on epoch=154
05/31/2022 00:12:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.88 on epoch=157
05/31/2022 00:12:47 - INFO - __main__ - Step 640 Global step 640 Train loss 0.77 on epoch=159
05/31/2022 00:12:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.89 on epoch=162
05/31/2022 00:12:50 - INFO - __main__ - Global step 650 Train loss 0.83 Classification-F1 0.41605003112356054 on epoch=162
05/31/2022 00:12:50 - INFO - __main__ - Saving model with best Classification-F1: 0.3541224128540305 -> 0.41605003112356054 on epoch=162, global_step=650
05/31/2022 00:12:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.77 on epoch=164
05/31/2022 00:12:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.82 on epoch=167
05/31/2022 00:12:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.82 on epoch=169
05/31/2022 00:13:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.75 on epoch=172
05/31/2022 00:13:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.87 on epoch=174
05/31/2022 00:13:04 - INFO - __main__ - Global step 700 Train loss 0.81 Classification-F1 0.26666666666666666 on epoch=174
05/31/2022 00:13:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.76 on epoch=177
05/31/2022 00:13:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.78 on epoch=179
05/31/2022 00:13:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.76 on epoch=182
05/31/2022 00:13:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.69 on epoch=184
05/31/2022 00:13:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.74 on epoch=187
05/31/2022 00:13:17 - INFO - __main__ - Global step 750 Train loss 0.75 Classification-F1 0.3648592629440782 on epoch=187
05/31/2022 00:13:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.72 on epoch=189
05/31/2022 00:13:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.68 on epoch=192
05/31/2022 00:13:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.66 on epoch=194
05/31/2022 00:13:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.76 on epoch=197
05/31/2022 00:13:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.75 on epoch=199
05/31/2022 00:13:30 - INFO - __main__ - Global step 800 Train loss 0.71 Classification-F1 0.3364661654135338 on epoch=199
05/31/2022 00:13:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.66 on epoch=202
05/31/2022 00:13:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.73 on epoch=204
05/31/2022 00:13:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.73 on epoch=207
05/31/2022 00:13:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.70 on epoch=209
05/31/2022 00:13:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.65 on epoch=212
05/31/2022 00:13:43 - INFO - __main__ - Global step 850 Train loss 0.69 Classification-F1 0.341144186783132 on epoch=212
05/31/2022 00:13:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.61 on epoch=214
05/31/2022 00:13:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.64 on epoch=217
05/31/2022 00:13:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.62 on epoch=219
05/31/2022 00:13:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.67 on epoch=222
05/31/2022 00:13:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.61 on epoch=224
05/31/2022 00:13:56 - INFO - __main__ - Global step 900 Train loss 0.63 Classification-F1 0.3765277777777778 on epoch=224
05/31/2022 00:13:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.60 on epoch=227
05/31/2022 00:14:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.54 on epoch=229
05/31/2022 00:14:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.70 on epoch=232
05/31/2022 00:14:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.60 on epoch=234
05/31/2022 00:14:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.65 on epoch=237
05/31/2022 00:14:09 - INFO - __main__ - Global step 950 Train loss 0.62 Classification-F1 0.42009864815491416 on epoch=237
05/31/2022 00:14:09 - INFO - __main__ - Saving model with best Classification-F1: 0.41605003112356054 -> 0.42009864815491416 on epoch=237, global_step=950
05/31/2022 00:14:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.52 on epoch=239
05/31/2022 00:14:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.60 on epoch=242
05/31/2022 00:14:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.50 on epoch=244
05/31/2022 00:14:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.68 on epoch=247
05/31/2022 00:14:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.54 on epoch=249
05/31/2022 00:14:22 - INFO - __main__ - Global step 1000 Train loss 0.57 Classification-F1 0.40071684587813616 on epoch=249
05/31/2022 00:14:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.57 on epoch=252
05/31/2022 00:14:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.58 on epoch=254
05/31/2022 00:14:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.55 on epoch=257
05/31/2022 00:14:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.51 on epoch=259
05/31/2022 00:14:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.57 on epoch=262
05/31/2022 00:14:35 - INFO - __main__ - Global step 1050 Train loss 0.56 Classification-F1 0.5136904761904761 on epoch=262
05/31/2022 00:14:35 - INFO - __main__ - Saving model with best Classification-F1: 0.42009864815491416 -> 0.5136904761904761 on epoch=262, global_step=1050
05/31/2022 00:14:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=264
05/31/2022 00:14:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=267
05/31/2022 00:14:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=269
05/31/2022 00:14:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.62 on epoch=272
05/31/2022 00:14:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.54 on epoch=274
05/31/2022 00:14:49 - INFO - __main__ - Global step 1100 Train loss 0.52 Classification-F1 0.384379619163551 on epoch=274
05/31/2022 00:14:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.54 on epoch=277
05/31/2022 00:14:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=279
05/31/2022 00:14:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.50 on epoch=282
05/31/2022 00:14:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.47 on epoch=284
05/31/2022 00:15:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.47 on epoch=287
05/31/2022 00:15:02 - INFO - __main__ - Global step 1150 Train loss 0.48 Classification-F1 0.5298620562336586 on epoch=287
05/31/2022 00:15:02 - INFO - __main__ - Saving model with best Classification-F1: 0.5136904761904761 -> 0.5298620562336586 on epoch=287, global_step=1150
05/31/2022 00:15:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=289
05/31/2022 00:15:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=292
05/31/2022 00:15:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=294
05/31/2022 00:15:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=297
05/31/2022 00:15:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=299
05/31/2022 00:15:15 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.5389005183413078 on epoch=299
05/31/2022 00:15:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5298620562336586 -> 0.5389005183413078 on epoch=299, global_step=1200
05/31/2022 00:15:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.48 on epoch=302
05/31/2022 00:15:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.48 on epoch=304
05/31/2022 00:15:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=307
05/31/2022 00:15:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=309
05/31/2022 00:15:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=312
05/31/2022 00:15:28 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.540189705811546 on epoch=312
05/31/2022 00:15:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5389005183413078 -> 0.540189705811546 on epoch=312, global_step=1250
05/31/2022 00:15:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=314
05/31/2022 00:15:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=317
05/31/2022 00:15:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.27 on epoch=319
05/31/2022 00:15:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=322
05/31/2022 00:15:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=324
05/31/2022 00:15:41 - INFO - __main__ - Global step 1300 Train loss 0.35 Classification-F1 0.6166720534367593 on epoch=324
05/31/2022 00:15:41 - INFO - __main__ - Saving model with best Classification-F1: 0.540189705811546 -> 0.6166720534367593 on epoch=324, global_step=1300
05/31/2022 00:15:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.44 on epoch=327
05/31/2022 00:15:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=329
05/31/2022 00:15:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.26 on epoch=332
05/31/2022 00:15:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=334
05/31/2022 00:15:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.35 on epoch=337
05/31/2022 00:15:54 - INFO - __main__ - Global step 1350 Train loss 0.34 Classification-F1 0.6299603174603174 on epoch=337
05/31/2022 00:15:54 - INFO - __main__ - Saving model with best Classification-F1: 0.6166720534367593 -> 0.6299603174603174 on epoch=337, global_step=1350
05/31/2022 00:15:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=339
05/31/2022 00:15:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=342
05/31/2022 00:16:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.29 on epoch=344
05/31/2022 00:16:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.30 on epoch=347
05/31/2022 00:16:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.30 on epoch=349
05/31/2022 00:16:08 - INFO - __main__ - Global step 1400 Train loss 0.30 Classification-F1 0.6436783804430863 on epoch=349
05/31/2022 00:16:08 - INFO - __main__ - Saving model with best Classification-F1: 0.6299603174603174 -> 0.6436783804430863 on epoch=349, global_step=1400
05/31/2022 00:16:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.33 on epoch=352
05/31/2022 00:16:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=354
05/31/2022 00:16:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.32 on epoch=357
05/31/2022 00:16:17 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=359
05/31/2022 00:16:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.26 on epoch=362
05/31/2022 00:16:21 - INFO - __main__ - Global step 1450 Train loss 0.28 Classification-F1 0.5869565217391304 on epoch=362
05/31/2022 00:16:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.26 on epoch=364
05/31/2022 00:16:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.25 on epoch=367
05/31/2022 00:16:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.25 on epoch=369
05/31/2022 00:16:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.23 on epoch=372
05/31/2022 00:16:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=374
05/31/2022 00:16:34 - INFO - __main__ - Global step 1500 Train loss 0.24 Classification-F1 0.5529168200220832 on epoch=374
05/31/2022 00:16:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.28 on epoch=377
05/31/2022 00:16:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=379
05/31/2022 00:16:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.23 on epoch=382
05/31/2022 00:16:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.27 on epoch=384
05/31/2022 00:16:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.24 on epoch=387
05/31/2022 00:16:47 - INFO - __main__ - Global step 1550 Train loss 0.25 Classification-F1 0.6384924512873696 on epoch=387
05/31/2022 00:16:50 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.29 on epoch=389
05/31/2022 00:16:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.27 on epoch=392
05/31/2022 00:16:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.25 on epoch=394
05/31/2022 00:16:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.24 on epoch=397
05/31/2022 00:16:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=399
05/31/2022 00:17:00 - INFO - __main__ - Global step 1600 Train loss 0.26 Classification-F1 0.6042717086834735 on epoch=399
05/31/2022 00:17:03 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.23 on epoch=402
05/31/2022 00:17:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=404
05/31/2022 00:17:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.28 on epoch=407
05/31/2022 00:17:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.19 on epoch=409
05/31/2022 00:17:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=412
05/31/2022 00:17:13 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.5749965554952766 on epoch=412
05/31/2022 00:17:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.26 on epoch=414
05/31/2022 00:17:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=417
05/31/2022 00:17:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.17 on epoch=419
05/31/2022 00:17:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.16 on epoch=422
05/31/2022 00:17:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.23 on epoch=424
05/31/2022 00:17:26 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.626216872558336 on epoch=424
05/31/2022 00:17:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=427
05/31/2022 00:17:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.13 on epoch=429
05/31/2022 00:17:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.22 on epoch=432
05/31/2022 00:17:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=434
05/31/2022 00:17:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=437
05/31/2022 00:17:40 - INFO - __main__ - Global step 1750 Train loss 0.20 Classification-F1 0.6283783783783783 on epoch=437
05/31/2022 00:17:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.13 on epoch=439
05/31/2022 00:17:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=442
05/31/2022 00:17:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.21 on epoch=444
05/31/2022 00:17:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=447
05/31/2022 00:17:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=449
05/31/2022 00:17:53 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.5552889576883385 on epoch=449
05/31/2022 00:17:55 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=452
05/31/2022 00:17:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.19 on epoch=454
05/31/2022 00:18:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=457
05/31/2022 00:18:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.09 on epoch=459
05/31/2022 00:18:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=462
05/31/2022 00:18:06 - INFO - __main__ - Global step 1850 Train loss 0.18 Classification-F1 0.5906451272304931 on epoch=462
05/31/2022 00:18:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.14 on epoch=464
05/31/2022 00:18:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=467
05/31/2022 00:18:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=469
05/31/2022 00:18:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=472
05/31/2022 00:18:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=474
05/31/2022 00:18:19 - INFO - __main__ - Global step 1900 Train loss 0.15 Classification-F1 0.561181362497152 on epoch=474
05/31/2022 00:18:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=477
05/31/2022 00:18:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.15 on epoch=479
05/31/2022 00:18:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.17 on epoch=482
05/31/2022 00:18:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=484
05/31/2022 00:18:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=487
05/31/2022 00:18:32 - INFO - __main__ - Global step 1950 Train loss 0.14 Classification-F1 0.6174068712850994 on epoch=487
05/31/2022 00:18:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=489
05/31/2022 00:18:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.13 on epoch=492
05/31/2022 00:18:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=494
05/31/2022 00:18:42 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=497
05/31/2022 00:18:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.14 on epoch=499
05/31/2022 00:18:45 - INFO - __main__ - Global step 2000 Train loss 0.12 Classification-F1 0.6174068712850994 on epoch=499
05/31/2022 00:18:48 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.12 on epoch=502
05/31/2022 00:18:50 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.22 on epoch=504
05/31/2022 00:18:53 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.14 on epoch=507
05/31/2022 00:18:55 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=509
05/31/2022 00:18:58 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.17 on epoch=512
05/31/2022 00:18:59 - INFO - __main__ - Global step 2050 Train loss 0.17 Classification-F1 0.5969793322734499 on epoch=512
05/31/2022 00:19:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.15 on epoch=514
05/31/2022 00:19:04 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.12 on epoch=517
05/31/2022 00:19:06 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.11 on epoch=519
05/31/2022 00:19:09 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.11 on epoch=522
05/31/2022 00:19:11 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.11 on epoch=524
05/31/2022 00:19:12 - INFO - __main__ - Global step 2100 Train loss 0.12 Classification-F1 0.6790587256329617 on epoch=524
05/31/2022 00:19:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6436783804430863 -> 0.6790587256329617 on epoch=524, global_step=2100
05/31/2022 00:19:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.15 on epoch=527
05/31/2022 00:19:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=529
05/31/2022 00:19:19 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.09 on epoch=532
05/31/2022 00:19:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=534
05/31/2022 00:19:24 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=537
05/31/2022 00:19:25 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.6713629507747155 on epoch=537
05/31/2022 00:19:28 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.12 on epoch=539
05/31/2022 00:19:30 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=542
05/31/2022 00:19:32 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.08 on epoch=544
05/31/2022 00:19:35 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=547
05/31/2022 00:19:37 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.12 on epoch=549
05/31/2022 00:19:38 - INFO - __main__ - Global step 2200 Train loss 0.09 Classification-F1 0.6437747035573123 on epoch=549
05/31/2022 00:19:41 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.12 on epoch=552
05/31/2022 00:19:43 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.12 on epoch=554
05/31/2022 00:19:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.11 on epoch=557
05/31/2022 00:19:48 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.12 on epoch=559
05/31/2022 00:19:50 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=562
05/31/2022 00:19:51 - INFO - __main__ - Global step 2250 Train loss 0.11 Classification-F1 0.6549669417316477 on epoch=562
05/31/2022 00:19:54 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.15 on epoch=564
05/31/2022 00:19:56 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.10 on epoch=567
05/31/2022 00:19:59 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.10 on epoch=569
05/31/2022 00:20:01 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=572
05/31/2022 00:20:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.12 on epoch=574
05/31/2022 00:20:04 - INFO - __main__ - Global step 2300 Train loss 0.11 Classification-F1 0.6790587256329617 on epoch=574
05/31/2022 00:20:07 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=577
05/31/2022 00:20:09 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.07 on epoch=579
05/31/2022 00:20:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.20 on epoch=582
05/31/2022 00:20:14 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=584
05/31/2022 00:20:17 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=587
05/31/2022 00:20:17 - INFO - __main__ - Global step 2350 Train loss 0.09 Classification-F1 0.6455624355005161 on epoch=587
05/31/2022 00:20:20 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
05/31/2022 00:20:22 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=592
05/31/2022 00:20:25 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.13 on epoch=594
05/31/2022 00:20:27 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.10 on epoch=597
05/31/2022 00:20:30 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=599
05/31/2022 00:20:30 - INFO - __main__ - Global step 2400 Train loss 0.07 Classification-F1 0.6520128507736691 on epoch=599
05/31/2022 00:20:33 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.13 on epoch=602
05/31/2022 00:20:35 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=604
05/31/2022 00:20:38 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.10 on epoch=607
05/31/2022 00:20:40 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=609
05/31/2022 00:20:43 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=612
05/31/2022 00:20:44 - INFO - __main__ - Global step 2450 Train loss 0.10 Classification-F1 0.6433029689608637 on epoch=612
05/31/2022 00:20:46 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=614
05/31/2022 00:20:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=617
05/31/2022 00:20:51 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=619
05/31/2022 00:20:53 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=622
05/31/2022 00:20:56 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.12 on epoch=624
05/31/2022 00:20:57 - INFO - __main__ - Global step 2500 Train loss 0.09 Classification-F1 0.6416666666666666 on epoch=624
05/31/2022 00:20:59 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.09 on epoch=627
05/31/2022 00:21:02 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=629
05/31/2022 00:21:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.07 on epoch=632
05/31/2022 00:21:07 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=634
05/31/2022 00:21:09 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=637
05/31/2022 00:21:10 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.6977742448330684 on epoch=637
05/31/2022 00:21:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6790587256329617 -> 0.6977742448330684 on epoch=637, global_step=2550
05/31/2022 00:21:13 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=639
05/31/2022 00:21:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=642
05/31/2022 00:21:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=644
05/31/2022 00:21:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.12 on epoch=647
05/31/2022 00:21:22 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=649
05/31/2022 00:21:23 - INFO - __main__ - Global step 2600 Train loss 0.08 Classification-F1 0.6652432712215322 on epoch=649
05/31/2022 00:21:26 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
05/31/2022 00:21:28 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=654
05/31/2022 00:21:31 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=657
05/31/2022 00:21:33 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=659
05/31/2022 00:21:36 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=662
05/31/2022 00:21:37 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.7077532077532078 on epoch=662
05/31/2022 00:21:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6977742448330684 -> 0.7077532077532078 on epoch=662, global_step=2650
05/31/2022 00:21:39 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=664
05/31/2022 00:21:42 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
05/31/2022 00:21:44 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=669
05/31/2022 00:21:47 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=672
05/31/2022 00:21:49 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=674
05/31/2022 00:21:50 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.6655138339920948 on epoch=674
05/31/2022 00:21:53 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=677
05/31/2022 00:21:55 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.16 on epoch=679
05/31/2022 00:21:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=682
05/31/2022 00:22:00 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=684
05/31/2022 00:22:02 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.11 on epoch=687
05/31/2022 00:22:03 - INFO - __main__ - Global step 2750 Train loss 0.08 Classification-F1 0.6581069475806318 on epoch=687
05/31/2022 00:22:06 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
05/31/2022 00:22:08 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.07 on epoch=692
05/31/2022 00:22:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/31/2022 00:22:13 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
05/31/2022 00:22:16 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=699
05/31/2022 00:22:17 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.7159493712192595 on epoch=699
05/31/2022 00:22:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7077532077532078 -> 0.7159493712192595 on epoch=699, global_step=2800
05/31/2022 00:22:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=702
05/31/2022 00:22:22 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
05/31/2022 00:22:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=707
05/31/2022 00:22:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=709
05/31/2022 00:22:29 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.09 on epoch=712
05/31/2022 00:22:30 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.7158214679953812 on epoch=712
05/31/2022 00:22:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=714
05/31/2022 00:22:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=717
05/31/2022 00:22:37 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=719
05/31/2022 00:22:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=722
05/31/2022 00:22:42 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.13 on epoch=724
05/31/2022 00:22:43 - INFO - __main__ - Global step 2900 Train loss 0.08 Classification-F1 0.6581069475806318 on epoch=724
05/31/2022 00:22:46 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.07 on epoch=727
05/31/2022 00:22:48 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=729
05/31/2022 00:22:51 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
05/31/2022 00:22:53 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=734
05/31/2022 00:22:56 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=737
05/31/2022 00:22:57 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.715848870260635 on epoch=737
05/31/2022 00:22:59 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=739
05/31/2022 00:23:02 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.11 on epoch=742
05/31/2022 00:23:04 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=744
05/31/2022 00:23:07 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=747
05/31/2022 00:23:09 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.16 on epoch=749
05/31/2022 00:23:10 - INFO - __main__ - Global step 3000 Train loss 0.08 Classification-F1 0.7014189455046231 on epoch=749
05/31/2022 00:23:10 - INFO - __main__ - save last model!
05/31/2022 00:23:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 00:23:10 - INFO - __main__ - Start tokenizing ... 5509 instances
05/31/2022 00:23:10 - INFO - __main__ - Printing 3 examples
05/31/2022 00:23:10 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/31/2022 00:23:10 - INFO - __main__ - ['others']
05/31/2022 00:23:10 - INFO - __main__ -  [emo] what you like very little things ok
05/31/2022 00:23:10 - INFO - __main__ - ['others']
05/31/2022 00:23:10 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/31/2022 00:23:10 - INFO - __main__ - ['others']
05/31/2022 00:23:10 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:23:12 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:23:17 - INFO - __main__ - Loaded 5509 examples from test data
05/31/2022 00:24:30 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-50prompt/singletask-emo/emo_16_87_0.2_8_predictions.txt
05/31/2022 00:24:30 - INFO - __main__ - Classification-F1 on test data: 0.3770
05/31/2022 00:24:30 - INFO - __main__ - prefix=emo_16_87, lr=0.2, bsz=8, dev_performance=0.7159493712192595, test_performance=0.37698912519898176
