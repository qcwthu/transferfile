05/21/2022 21:26:59 - INFO - __main__ - Namespace(task_dir='data_64/ethos-religion/', task_name='ethos-religion', identifier='T5-large-ft-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:26:59 - INFO - __main__ - models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion
05/21/2022 21:26:59 - INFO - __main__ - Namespace(task_dir='data_64/ethos-religion/', task_name='ethos-religion', identifier='T5-large-ft-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:26:59 - INFO - __main__ - models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion
05/21/2022 21:27:01 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:27:01 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:27:01 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:27:01 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:27:01 - INFO - __main__ - Using 2 gpus
05/21/2022 21:27:01 - INFO - __main__ - Using 2 gpus
05/21/2022 21:27:01 - INFO - __main__ - Fine-tuning the following samples: ['ethos-religion_64_100', 'ethos-religion_64_13', 'ethos-religion_64_21', 'ethos-religion_64_42', 'ethos-religion_64_87']
05/21/2022 21:27:01 - INFO - __main__ - Fine-tuning the following samples: ['ethos-religion_64_100', 'ethos-religion_64_13', 'ethos-religion_64_21', 'ethos-religion_64_42', 'ethos-religion_64_87']
05/21/2022 21:27:06 - INFO - __main__ - Running ... prefix=ethos-religion_64_100, lr=0.0005, bsz=8 ...
06/02/2022 11:00:25 - INFO - __main__ - Namespace(task_dir='data_64/ethos-religion/', task_name='ethos-religion', identifier='T5-large-ft-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
06/02/2022 11:00:25 - INFO - __main__ - models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion
06/02/2022 11:00:25 - INFO - __main__ - Namespace(task_dir='data_64/ethos-religion/', task_name='ethos-religion', identifier='T5-large-ft-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
06/02/2022 11:00:25 - INFO - __main__ - models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion
06/02/2022 11:00:26 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/02/2022 11:00:26 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/02/2022 11:00:26 - INFO - __main__ - args.device: cuda:0
06/02/2022 11:00:26 - INFO - __main__ - Using 2 gpus
06/02/2022 11:00:26 - INFO - __main__ - Fine-tuning the following samples: ['ethos-religion_64_100', 'ethos-religion_64_13', 'ethos-religion_64_21', 'ethos-religion_64_42', 'ethos-religion_64_87']
06/02/2022 11:00:26 - INFO - __main__ - args.device: cuda:1
06/02/2022 11:00:26 - INFO - __main__ - Using 2 gpus
06/02/2022 11:00:26 - INFO - __main__ - Fine-tuning the following samples: ['ethos-religion_64_100', 'ethos-religion_64_13', 'ethos-religion_64_21', 'ethos-religion_64_42', 'ethos-religion_64_87']
06/02/2022 11:00:31 - INFO - __main__ - Running ... prefix=ethos-religion_64_100, lr=0.0005, bsz=8 ...
06/02/2022 11:00:32 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 11:00:32 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 11:00:32 - INFO - __main__ - Printing 3 examples
06/02/2022 11:00:32 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
06/02/2022 11:00:32 - INFO - __main__ - Printing 3 examples
06/02/2022 11:00:32 - INFO - __main__ - ['false']
06/02/2022 11:00:32 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
06/02/2022 11:00:32 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
06/02/2022 11:00:32 - INFO - __main__ - ['false']
06/02/2022 11:00:32 - INFO - __main__ - ['false']
06/02/2022 11:00:32 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
06/02/2022 11:00:32 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
06/02/2022 11:00:32 - INFO - __main__ - ['false']
06/02/2022 11:00:32 - INFO - __main__ - ['false']
06/02/2022 11:00:32 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
06/02/2022 11:00:32 - INFO - __main__ - ['false']
06/02/2022 11:00:32 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:00:32 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:00:32 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:00:32 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:00:32 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 11:00:32 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:00:32 - INFO - __main__ - Printing 3 examples
06/02/2022 11:00:32 - INFO - __main__ -  [ethos-religion] I really don’t like your accent. I don’t want to call support and call to rats like you. I want a cute lady with British accent to answer my call. You are third world pieces with no job to this planet
06/02/2022 11:00:32 - INFO - __main__ - ['false']
06/02/2022 11:00:32 - INFO - __main__ -  [ethos-religion] Humans with third leg I hope to distinct soon, with much love
06/02/2022 11:00:32 - INFO - __main__ - ['false']
06/02/2022 11:00:32 - INFO - __main__ -  [ethos-religion] I would pull you over for your race, if I was a cop.
06/02/2022 11:00:32 - INFO - __main__ - ['false']
06/02/2022 11:00:32 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:00:32 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 11:00:32 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:00:32 - INFO - __main__ - Printing 3 examples
06/02/2022 11:00:32 - INFO - __main__ -  [ethos-religion] I really don’t like your accent. I don’t want to call support and call to rats like you. I want a cute lady with British accent to answer my call. You are third world pieces with no job to this planet
06/02/2022 11:00:32 - INFO - __main__ - ['false']
06/02/2022 11:00:32 - INFO - __main__ -  [ethos-religion] Humans with third leg I hope to distinct soon, with much love
06/02/2022 11:00:32 - INFO - __main__ - ['false']
06/02/2022 11:00:32 - INFO - __main__ -  [ethos-religion] I would pull you over for your race, if I was a cop.
06/02/2022 11:00:32 - INFO - __main__ - ['false']
06/02/2022 11:00:32 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:00:32 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:00:32 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:00:32 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:00:32 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:00:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 11:00:45 - INFO - __main__ - Starting training!
06/02/2022 11:00:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 11:00:46 - INFO - __main__ - Starting training!
06/02/2022 11:00:51 - INFO - __main__ - Step 10 Global step 10 Train loss 23.054356 on epoch=1
06/02/2022 11:00:56 - INFO - __main__ - Step 20 Global step 20 Train loss 18.843504 on epoch=2
06/02/2022 11:01:01 - INFO - __main__ - Step 30 Global step 30 Train loss 16.013254 on epoch=3
06/02/2022 11:01:06 - INFO - __main__ - Step 40 Global step 40 Train loss 14.293869 on epoch=4
06/02/2022 11:01:11 - INFO - __main__ - Step 50 Global step 50 Train loss 12.676726 on epoch=6
06/02/2022 11:01:11 - INFO - __main__ - Global step 50 Train loss 16.976341 Classification-F1 0.0 on epoch=6
06/02/2022 11:01:17 - INFO - __main__ - Step 60 Global step 60 Train loss 9.422496 on epoch=7
06/02/2022 11:01:22 - INFO - __main__ - Step 70 Global step 70 Train loss 3.355233 on epoch=8
06/02/2022 11:01:28 - INFO - __main__ - Step 80 Global step 80 Train loss 2.559531 on epoch=9
06/02/2022 11:01:33 - INFO - __main__ - Step 90 Global step 90 Train loss 2.240528 on epoch=11
06/02/2022 11:01:38 - INFO - __main__ - Step 100 Global step 100 Train loss 1.491941 on epoch=12
06/02/2022 11:01:39 - INFO - __main__ - Global step 100 Train loss 3.813946 Classification-F1 1.0 on epoch=12
06/02/2022 11:01:45 - INFO - __main__ - Step 110 Global step 110 Train loss 1.940695 on epoch=13
06/02/2022 11:01:50 - INFO - __main__ - Step 120 Global step 120 Train loss 2.315276 on epoch=14
06/02/2022 11:01:55 - INFO - __main__ - Step 130 Global step 130 Train loss 1.932491 on epoch=16
06/02/2022 11:02:01 - INFO - __main__ - Step 140 Global step 140 Train loss 1.888658 on epoch=17
06/02/2022 11:02:06 - INFO - __main__ - Step 150 Global step 150 Train loss 1.360174 on epoch=18
06/02/2022 11:02:06 - INFO - __main__ - Global step 150 Train loss 1.887459 Classification-F1 1.0 on epoch=18
06/02/2022 11:02:11 - INFO - __main__ - Step 160 Global step 160 Train loss 1.038241 on epoch=19
06/02/2022 11:02:17 - INFO - __main__ - Step 170 Global step 170 Train loss 1.020095 on epoch=21
06/02/2022 11:02:22 - INFO - __main__ - Step 180 Global step 180 Train loss 1.061789 on epoch=22
06/02/2022 11:02:27 - INFO - __main__ - Step 190 Global step 190 Train loss 0.967429 on epoch=23
06/02/2022 11:02:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.842348 on epoch=24
06/02/2022 11:02:33 - INFO - __main__ - Global step 200 Train loss 0.985981 Classification-F1 0.42342342342342343 on epoch=24
06/02/2022 11:02:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.697709 on epoch=26
06/02/2022 11:02:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.602252 on epoch=27
06/02/2022 11:02:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.587509 on epoch=28
06/02/2022 11:02:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.521712 on epoch=29
06/02/2022 11:02:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.503167 on epoch=31
06/02/2022 11:03:00 - INFO - __main__ - Global step 250 Train loss 0.582470 Classification-F1 0.07246376811594203 on epoch=31
06/02/2022 11:03:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.485843 on epoch=32
06/02/2022 11:03:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.375150 on epoch=33
06/02/2022 11:03:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.372193 on epoch=34
06/02/2022 11:03:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.276715 on epoch=36
06/02/2022 11:03:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.250324 on epoch=37
06/02/2022 11:03:26 - INFO - __main__ - Global step 300 Train loss 0.352045 Classification-F1 0.36633663366336633 on epoch=37
06/02/2022 11:03:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.260775 on epoch=38
06/02/2022 11:03:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.221185 on epoch=39
06/02/2022 11:03:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.174869 on epoch=41
06/02/2022 11:03:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.190593 on epoch=42
06/02/2022 11:03:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.180532 on epoch=43
06/02/2022 11:03:53 - INFO - __main__ - Global step 350 Train loss 0.205591 Classification-F1 1.0 on epoch=43
06/02/2022 11:03:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.162462 on epoch=44
06/02/2022 11:04:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.177051 on epoch=46
06/02/2022 11:04:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.127586 on epoch=47
06/02/2022 11:04:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.116952 on epoch=48
06/02/2022 11:04:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.132382 on epoch=49
06/02/2022 11:04:19 - INFO - __main__ - Global step 400 Train loss 0.143287 Classification-F1 0.4576271186440678 on epoch=49
06/02/2022 11:04:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.103375 on epoch=51
06/02/2022 11:04:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.075339 on epoch=52
06/02/2022 11:04:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.091592 on epoch=53
06/02/2022 11:04:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.046289 on epoch=54
06/02/2022 11:04:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.065237 on epoch=56
06/02/2022 11:04:45 - INFO - __main__ - Global step 450 Train loss 0.076367 Classification-F1 0.41818181818181815 on epoch=56
06/02/2022 11:04:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.076904 on epoch=57
06/02/2022 11:04:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.115212 on epoch=58
06/02/2022 11:05:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.050786 on epoch=59
06/02/2022 11:05:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.122153 on epoch=61
06/02/2022 11:05:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.145487 on epoch=62
06/02/2022 11:05:11 - INFO - __main__ - Global step 500 Train loss 0.102109 Classification-F1 0.49606299212598426 on epoch=62
06/02/2022 11:05:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.074498 on epoch=63
06/02/2022 11:05:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.049823 on epoch=64
06/02/2022 11:05:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.027742 on epoch=66
06/02/2022 11:05:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.007430 on epoch=67
06/02/2022 11:05:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.044321 on epoch=68
06/02/2022 11:05:38 - INFO - __main__ - Global step 550 Train loss 0.040763 Classification-F1 0.47107438016528924 on epoch=68
06/02/2022 11:05:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.002041 on epoch=69
06/02/2022 11:05:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.011767 on epoch=71
06/02/2022 11:05:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.010057 on epoch=72
06/02/2022 11:05:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001505 on epoch=73
06/02/2022 11:06:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.030790 on epoch=74
06/02/2022 11:06:04 - INFO - __main__ - Global step 600 Train loss 0.011232 Classification-F1 0.4796747967479675 on epoch=74
06/02/2022 11:06:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.010518 on epoch=76
06/02/2022 11:06:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.002198 on epoch=77
06/02/2022 11:06:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.012287 on epoch=78
06/02/2022 11:06:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001014 on epoch=79
06/02/2022 11:06:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001553 on epoch=81
06/02/2022 11:06:30 - INFO - __main__ - Global step 650 Train loss 0.005514 Classification-F1 0.47107438016528924 on epoch=81
06/02/2022 11:06:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.001141 on epoch=82
06/02/2022 11:06:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.001008 on epoch=83
06/02/2022 11:06:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000765 on epoch=84
06/02/2022 11:06:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.015907 on epoch=86
06/02/2022 11:06:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.184364 on epoch=87
06/02/2022 11:06:56 - INFO - __main__ - Global step 700 Train loss 0.040637 Classification-F1 0.47107438016528924 on epoch=87
06/02/2022 11:07:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.012281 on epoch=88
06/02/2022 11:07:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.008345 on epoch=89
06/02/2022 11:07:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.011982 on epoch=91
06/02/2022 11:07:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.014824 on epoch=92
06/02/2022 11:07:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.011226 on epoch=93
06/02/2022 11:07:22 - INFO - __main__ - Global step 750 Train loss 0.011732 Classification-F1 1.0 on epoch=93
06/02/2022 11:07:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.012034 on epoch=94
06/02/2022 11:07:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.016738 on epoch=96
06/02/2022 11:07:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.002764 on epoch=97
06/02/2022 11:07:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.051538 on epoch=98
06/02/2022 11:07:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.116068 on epoch=99
06/02/2022 11:07:49 - INFO - __main__ - Global step 800 Train loss 0.039828 Classification-F1 0.015384615384615385 on epoch=99
06/02/2022 11:07:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.119789 on epoch=101
06/02/2022 11:08:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.171672 on epoch=102
06/02/2022 11:08:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.022565 on epoch=103
06/02/2022 11:08:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.002450 on epoch=104
06/02/2022 11:08:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.008905 on epoch=106
06/02/2022 11:08:16 - INFO - __main__ - Global step 850 Train loss 0.065076 Classification-F1 0.488 on epoch=106
06/02/2022 11:08:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.002333 on epoch=107
06/02/2022 11:08:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.002746 on epoch=108
06/02/2022 11:08:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001269 on epoch=109
06/02/2022 11:08:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000484 on epoch=111
06/02/2022 11:08:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.002065 on epoch=112
06/02/2022 11:08:43 - INFO - __main__ - Global step 900 Train loss 0.001779 Classification-F1 0.49206349206349204 on epoch=112
06/02/2022 11:08:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.009770 on epoch=113
06/02/2022 11:08:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.003168 on epoch=114
06/02/2022 11:08:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000292 on epoch=116
06/02/2022 11:09:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000177 on epoch=117
06/02/2022 11:09:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000192 on epoch=118
06/02/2022 11:09:10 - INFO - __main__ - Global step 950 Train loss 0.002720 Classification-F1 0.4838709677419355 on epoch=118
06/02/2022 11:09:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000742 on epoch=119
06/02/2022 11:09:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000593 on epoch=121
06/02/2022 11:09:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000077 on epoch=122
06/02/2022 11:09:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000188 on epoch=123
06/02/2022 11:09:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000186 on epoch=124
06/02/2022 11:09:37 - INFO - __main__ - Global step 1000 Train loss 0.000357 Classification-F1 0.488 on epoch=124
06/02/2022 11:09:37 - INFO - __main__ - save last model!
06/02/2022 11:09:37 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 11:09:37 - INFO - __main__ - Printing 3 examples
06/02/2022 11:09:37 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
06/02/2022 11:09:37 - INFO - __main__ - ['false']
06/02/2022 11:09:37 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
06/02/2022 11:09:37 - INFO - __main__ - ['false']
06/02/2022 11:09:37 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
06/02/2022 11:09:37 - INFO - __main__ - ['false']
06/02/2022 11:09:37 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:09:37 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:09:37 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 11:09:37 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:09:37 - INFO - __main__ - Printing 3 examples
06/02/2022 11:09:37 - INFO - __main__ -  [ethos-religion] I really don’t like your accent. I don’t want to call support and call to rats like you. I want a cute lady with British accent to answer my call. You are third world pieces with no job to this planet
06/02/2022 11:09:37 - INFO - __main__ - ['false']
06/02/2022 11:09:37 - INFO - __main__ -  [ethos-religion] Humans with third leg I hope to distinct soon, with much love
06/02/2022 11:09:37 - INFO - __main__ - ['false']
06/02/2022 11:09:37 - INFO - __main__ -  [ethos-religion] I would pull you over for your race, if I was a cop.
06/02/2022 11:09:37 - INFO - __main__ - ['false']
06/02/2022 11:09:37 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:09:37 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:09:37 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:09:44 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 11:09:44 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 11:09:44 - INFO - __main__ - Printing 3 examples
06/02/2022 11:09:44 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 11:09:44 - INFO - __main__ - ['false']
06/02/2022 11:09:44 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 11:09:44 - INFO - __main__ - ['false']
06/02/2022 11:09:44 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 11:09:44 - INFO - __main__ - ['true']
06/02/2022 11:09:44 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:09:44 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:09:44 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 11:09:46 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_100_0.0005_8_predictions.txt
06/02/2022 11:09:46 - INFO - __main__ - Classification-F1 on test data: 0.4387
06/02/2022 11:09:46 - INFO - __main__ - prefix=ethos-religion_64_100, lr=0.0005, bsz=8, dev_performance=1.0, test_performance=0.4387096774193549
06/02/2022 11:09:46 - INFO - __main__ - Running ... prefix=ethos-religion_64_100, lr=0.0003, bsz=8 ...
06/02/2022 11:09:47 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 11:09:47 - INFO - __main__ - Printing 3 examples
06/02/2022 11:09:47 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
06/02/2022 11:09:47 - INFO - __main__ - ['false']
06/02/2022 11:09:47 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
06/02/2022 11:09:47 - INFO - __main__ - ['false']
06/02/2022 11:09:47 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
06/02/2022 11:09:47 - INFO - __main__ - ['false']
06/02/2022 11:09:47 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:09:47 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:09:47 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 11:09:47 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:09:47 - INFO - __main__ - Printing 3 examples
06/02/2022 11:09:47 - INFO - __main__ -  [ethos-religion] I really don’t like your accent. I don’t want to call support and call to rats like you. I want a cute lady with British accent to answer my call. You are third world pieces with no job to this planet
06/02/2022 11:09:47 - INFO - __main__ - ['false']
06/02/2022 11:09:47 - INFO - __main__ -  [ethos-religion] Humans with third leg I hope to distinct soon, with much love
06/02/2022 11:09:47 - INFO - __main__ - ['false']
06/02/2022 11:09:47 - INFO - __main__ -  [ethos-religion] I would pull you over for your race, if I was a cop.
06/02/2022 11:09:47 - INFO - __main__ - ['false']
06/02/2022 11:09:47 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:09:47 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:09:48 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:09:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 11:09:50 - INFO - __main__ - Starting training!
06/02/2022 11:09:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 11:09:59 - INFO - __main__ - Starting training!
06/02/2022 11:10:03 - INFO - __main__ - Step 10 Global step 10 Train loss 23.725954 on epoch=1
06/02/2022 11:10:08 - INFO - __main__ - Step 20 Global step 20 Train loss 19.364088 on epoch=2
06/02/2022 11:10:13 - INFO - __main__ - Step 30 Global step 30 Train loss 17.060862 on epoch=3
06/02/2022 11:10:18 - INFO - __main__ - Step 40 Global step 40 Train loss 15.947225 on epoch=4
06/02/2022 11:10:23 - INFO - __main__ - Step 50 Global step 50 Train loss 14.729297 on epoch=6
06/02/2022 11:10:37 - INFO - __main__ - Global step 50 Train loss 18.165485 Classification-F1 0.0 on epoch=6
06/02/2022 11:10:43 - INFO - __main__ - Step 60 Global step 60 Train loss 13.524981 on epoch=7
06/02/2022 11:10:48 - INFO - __main__ - Step 70 Global step 70 Train loss 12.544812 on epoch=8
06/02/2022 11:10:53 - INFO - __main__ - Step 80 Global step 80 Train loss 10.297644 on epoch=9
06/02/2022 11:10:58 - INFO - __main__ - Step 90 Global step 90 Train loss 5.940723 on epoch=11
06/02/2022 11:11:03 - INFO - __main__ - Step 100 Global step 100 Train loss 2.156682 on epoch=12
06/02/2022 11:11:04 - INFO - __main__ - Global step 100 Train loss 8.892968 Classification-F1 0.030303030303030304 on epoch=12
06/02/2022 11:11:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.916112 on epoch=13
06/02/2022 11:11:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.510933 on epoch=14
06/02/2022 11:11:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.564136 on epoch=16
06/02/2022 11:11:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.604364 on epoch=17
06/02/2022 11:11:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.468180 on epoch=18
06/02/2022 11:11:31 - INFO - __main__ - Global step 150 Train loss 0.612745 Classification-F1 0.35353535353535354 on epoch=18
06/02/2022 11:11:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.467458 on epoch=19
06/02/2022 11:11:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.531117 on epoch=21
06/02/2022 11:11:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.497239 on epoch=22
06/02/2022 11:11:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.392056 on epoch=23
06/02/2022 11:11:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.485264 on epoch=24
06/02/2022 11:11:58 - INFO - __main__ - Global step 200 Train loss 0.474627 Classification-F1 0.13513513513513514 on epoch=24
06/02/2022 11:12:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.455837 on epoch=26
06/02/2022 11:12:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.460341 on epoch=27
06/02/2022 11:12:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.372745 on epoch=28
06/02/2022 11:12:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.414266 on epoch=29
06/02/2022 11:12:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.444243 on epoch=31
06/02/2022 11:12:25 - INFO - __main__ - Global step 250 Train loss 0.429486 Classification-F1 0.4576271186440678 on epoch=31
06/02/2022 11:12:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.373790 on epoch=32
06/02/2022 11:12:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.400585 on epoch=33
06/02/2022 11:12:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.395384 on epoch=34
06/02/2022 11:12:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.433632 on epoch=36
06/02/2022 11:12:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.333189 on epoch=37
06/02/2022 11:12:52 - INFO - __main__ - Global step 300 Train loss 0.387316 Classification-F1 0.030303030303030304 on epoch=37
06/02/2022 11:12:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.416547 on epoch=38
06/02/2022 11:13:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.373826 on epoch=39
06/02/2022 11:13:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.340062 on epoch=41
06/02/2022 11:13:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.372670 on epoch=42
06/02/2022 11:13:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.360126 on epoch=43
06/02/2022 11:13:19 - INFO - __main__ - Global step 350 Train loss 0.372646 Classification-F1 0.4666666666666667 on epoch=43
06/02/2022 11:13:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.354196 on epoch=44
06/02/2022 11:13:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.358481 on epoch=46
06/02/2022 11:13:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.375244 on epoch=47
06/02/2022 11:13:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.289257 on epoch=48
06/02/2022 11:13:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.299172 on epoch=49
06/02/2022 11:13:46 - INFO - __main__ - Global step 400 Train loss 0.335270 Classification-F1 0.14666666666666667 on epoch=49
06/02/2022 11:13:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.324097 on epoch=51
06/02/2022 11:13:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.290988 on epoch=52
06/02/2022 11:14:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.296294 on epoch=53
06/02/2022 11:14:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.276217 on epoch=54
06/02/2022 11:14:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.259883 on epoch=56
06/02/2022 11:14:12 - INFO - __main__ - Global step 450 Train loss 0.289496 Classification-F1 1.0 on epoch=56
06/02/2022 11:14:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.301869 on epoch=57
06/02/2022 11:14:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.310795 on epoch=58
06/02/2022 11:14:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.280572 on epoch=59
06/02/2022 11:14:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.262361 on epoch=61
06/02/2022 11:14:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.197109 on epoch=62
06/02/2022 11:14:39 - INFO - __main__ - Global step 500 Train loss 0.270541 Classification-F1 0.452991452991453 on epoch=62
06/02/2022 11:14:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.223170 on epoch=63
06/02/2022 11:14:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.227103 on epoch=64
06/02/2022 11:14:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.252871 on epoch=66
06/02/2022 11:15:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.244583 on epoch=67
06/02/2022 11:15:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.180865 on epoch=68
06/02/2022 11:15:05 - INFO - __main__ - Global step 550 Train loss 0.225719 Classification-F1 0.41818181818181815 on epoch=68
06/02/2022 11:15:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.154218 on epoch=69
06/02/2022 11:15:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.182291 on epoch=71
06/02/2022 11:15:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.241359 on epoch=72
06/02/2022 11:15:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.180354 on epoch=73
06/02/2022 11:15:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.179872 on epoch=74
06/02/2022 11:15:31 - INFO - __main__ - Global step 600 Train loss 0.187619 Classification-F1 0.49606299212598426 on epoch=74
06/02/2022 11:15:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.157791 on epoch=76
06/02/2022 11:15:42 - INFO - __main__ - Step 620 Global step 620 Train loss 0.171869 on epoch=77
06/02/2022 11:15:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.134227 on epoch=78
06/02/2022 11:15:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.154793 on epoch=79
06/02/2022 11:15:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.175015 on epoch=81
06/02/2022 11:15:58 - INFO - __main__ - Global step 650 Train loss 0.158739 Classification-F1 0.46218487394957986 on epoch=81
06/02/2022 11:16:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.155891 on epoch=82
06/02/2022 11:16:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.155555 on epoch=83
06/02/2022 11:16:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.176576 on epoch=84
06/02/2022 11:16:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.094634 on epoch=86
06/02/2022 11:16:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.157389 on epoch=87
06/02/2022 11:16:24 - INFO - __main__ - Global step 700 Train loss 0.148009 Classification-F1 0.47107438016528924 on epoch=87
06/02/2022 11:16:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.128667 on epoch=88
06/02/2022 11:16:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.150072 on epoch=89
06/02/2022 11:16:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.143186 on epoch=91
06/02/2022 11:16:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.172123 on epoch=92
06/02/2022 11:16:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.126845 on epoch=93
06/02/2022 11:16:50 - INFO - __main__ - Global step 750 Train loss 0.144179 Classification-F1 0.43859649122807015 on epoch=93
06/02/2022 11:16:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.099303 on epoch=94
06/02/2022 11:17:00 - INFO - __main__ - Step 770 Global step 770 Train loss 0.131951 on epoch=96
06/02/2022 11:17:05 - INFO - __main__ - Step 780 Global step 780 Train loss 0.093750 on epoch=97
06/02/2022 11:17:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.093938 on epoch=98
06/02/2022 11:17:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.156927 on epoch=99
06/02/2022 11:17:16 - INFO - __main__ - Global step 800 Train loss 0.115174 Classification-F1 0.49606299212598426 on epoch=99
06/02/2022 11:17:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.108480 on epoch=101
06/02/2022 11:17:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.117334 on epoch=102
06/02/2022 11:17:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.112246 on epoch=103
06/02/2022 11:17:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.100204 on epoch=104
06/02/2022 11:17:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.054619 on epoch=106
06/02/2022 11:17:42 - INFO - __main__ - Global step 850 Train loss 0.098577 Classification-F1 0.46218487394957986 on epoch=106
06/02/2022 11:17:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.052157 on epoch=107
06/02/2022 11:17:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.043497 on epoch=108
06/02/2022 11:17:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.081504 on epoch=109
06/02/2022 11:18:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.092628 on epoch=111
06/02/2022 11:18:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.084156 on epoch=112
06/02/2022 11:18:09 - INFO - __main__ - Global step 900 Train loss 0.070788 Classification-F1 0.4482758620689655 on epoch=112
06/02/2022 11:18:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.061987 on epoch=113
06/02/2022 11:18:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.047364 on epoch=114
06/02/2022 11:18:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.028885 on epoch=116
06/02/2022 11:18:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.034620 on epoch=117
06/02/2022 11:18:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.068225 on epoch=118
06/02/2022 11:18:35 - INFO - __main__ - Global step 950 Train loss 0.048216 Classification-F1 0.23809523809523808 on epoch=118
06/02/2022 11:18:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.065277 on epoch=119
06/02/2022 11:18:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.083437 on epoch=121
06/02/2022 11:18:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.087116 on epoch=122
06/02/2022 11:18:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.140906 on epoch=123
06/02/2022 11:19:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.048499 on epoch=124
06/02/2022 11:19:01 - INFO - __main__ - Global step 1000 Train loss 0.085047 Classification-F1 1.0 on epoch=124
06/02/2022 11:19:01 - INFO - __main__ - save last model!
06/02/2022 11:19:02 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 11:19:02 - INFO - __main__ - Printing 3 examples
06/02/2022 11:19:02 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
06/02/2022 11:19:02 - INFO - __main__ - ['false']
06/02/2022 11:19:02 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
06/02/2022 11:19:02 - INFO - __main__ - ['false']
06/02/2022 11:19:02 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
06/02/2022 11:19:02 - INFO - __main__ - ['false']
06/02/2022 11:19:02 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:19:02 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:19:02 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 11:19:02 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:19:02 - INFO - __main__ - Printing 3 examples
06/02/2022 11:19:02 - INFO - __main__ -  [ethos-religion] I really don’t like your accent. I don’t want to call support and call to rats like you. I want a cute lady with British accent to answer my call. You are third world pieces with no job to this planet
06/02/2022 11:19:02 - INFO - __main__ - ['false']
06/02/2022 11:19:02 - INFO - __main__ -  [ethos-religion] Humans with third leg I hope to distinct soon, with much love
06/02/2022 11:19:02 - INFO - __main__ - ['false']
06/02/2022 11:19:02 - INFO - __main__ -  [ethos-religion] I would pull you over for your race, if I was a cop.
06/02/2022 11:19:02 - INFO - __main__ - ['false']
06/02/2022 11:19:02 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:19:02 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:19:02 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:19:08 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 11:19:09 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 11:19:09 - INFO - __main__ - Printing 3 examples
06/02/2022 11:19:09 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 11:19:09 - INFO - __main__ - ['false']
06/02/2022 11:19:09 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 11:19:09 - INFO - __main__ - ['false']
06/02/2022 11:19:09 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 11:19:09 - INFO - __main__ - ['true']
06/02/2022 11:19:09 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:19:09 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:19:09 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 11:19:11 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_100_0.0003_8_predictions.txt
06/02/2022 11:19:11 - INFO - __main__ - Classification-F1 on test data: 0.8990
06/02/2022 11:19:11 - INFO - __main__ - prefix=ethos-religion_64_100, lr=0.0003, bsz=8, dev_performance=1.0, test_performance=0.8989938080495357
06/02/2022 11:19:11 - INFO - __main__ - Running ... prefix=ethos-religion_64_100, lr=0.0002, bsz=8 ...
06/02/2022 11:19:12 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 11:19:12 - INFO - __main__ - Printing 3 examples
06/02/2022 11:19:12 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
06/02/2022 11:19:12 - INFO - __main__ - ['false']
06/02/2022 11:19:12 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
06/02/2022 11:19:12 - INFO - __main__ - ['false']
06/02/2022 11:19:12 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
06/02/2022 11:19:12 - INFO - __main__ - ['false']
06/02/2022 11:19:12 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:19:12 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:19:12 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 11:19:12 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:19:12 - INFO - __main__ - Printing 3 examples
06/02/2022 11:19:12 - INFO - __main__ -  [ethos-religion] I really don’t like your accent. I don’t want to call support and call to rats like you. I want a cute lady with British accent to answer my call. You are third world pieces with no job to this planet
06/02/2022 11:19:12 - INFO - __main__ - ['false']
06/02/2022 11:19:12 - INFO - __main__ -  [ethos-religion] Humans with third leg I hope to distinct soon, with much love
06/02/2022 11:19:12 - INFO - __main__ - ['false']
06/02/2022 11:19:12 - INFO - __main__ -  [ethos-religion] I would pull you over for your race, if I was a cop.
06/02/2022 11:19:12 - INFO - __main__ - ['false']
06/02/2022 11:19:12 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:19:12 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:19:12 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:19:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 11:19:15 - INFO - __main__ - Starting training!
06/02/2022 11:19:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 11:19:23 - INFO - __main__ - Starting training!
06/02/2022 11:19:28 - INFO - __main__ - Step 10 Global step 10 Train loss 23.507202 on epoch=1
06/02/2022 11:19:32 - INFO - __main__ - Step 20 Global step 20 Train loss 20.850117 on epoch=2
06/02/2022 11:19:37 - INFO - __main__ - Step 30 Global step 30 Train loss 18.381372 on epoch=3
06/02/2022 11:19:43 - INFO - __main__ - Step 40 Global step 40 Train loss 16.967785 on epoch=4
06/02/2022 11:19:48 - INFO - __main__ - Step 50 Global step 50 Train loss 16.636543 on epoch=6
06/02/2022 11:19:50 - INFO - __main__ - Global step 50 Train loss 19.268602 Classification-F1 0.0 on epoch=6
06/02/2022 11:19:56 - INFO - __main__ - Step 60 Global step 60 Train loss 14.803858 on epoch=7
06/02/2022 11:20:01 - INFO - __main__ - Step 70 Global step 70 Train loss 14.805341 on epoch=8
06/02/2022 11:20:06 - INFO - __main__ - Step 80 Global step 80 Train loss 13.608478 on epoch=9
06/02/2022 11:20:11 - INFO - __main__ - Step 90 Global step 90 Train loss 13.242085 on epoch=11
06/02/2022 11:20:16 - INFO - __main__ - Step 100 Global step 100 Train loss 12.561220 on epoch=12
06/02/2022 11:20:17 - INFO - __main__ - Global step 100 Train loss 13.804195 Classification-F1 0.0 on epoch=12
06/02/2022 11:20:22 - INFO - __main__ - Step 110 Global step 110 Train loss 10.976514 on epoch=13
06/02/2022 11:20:27 - INFO - __main__ - Step 120 Global step 120 Train loss 9.536238 on epoch=14
06/02/2022 11:20:32 - INFO - __main__ - Step 130 Global step 130 Train loss 4.902508 on epoch=16
06/02/2022 11:20:37 - INFO - __main__ - Step 140 Global step 140 Train loss 4.237487 on epoch=17
06/02/2022 11:20:42 - INFO - __main__ - Step 150 Global step 150 Train loss 3.917801 on epoch=18
06/02/2022 11:20:43 - INFO - __main__ - Global step 150 Train loss 6.714110 Classification-F1 0.20987654320987653 on epoch=18
06/02/2022 11:20:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.993332 on epoch=19
06/02/2022 11:20:55 - INFO - __main__ - Step 170 Global step 170 Train loss 1.063588 on epoch=21
06/02/2022 11:21:00 - INFO - __main__ - Step 180 Global step 180 Train loss 1.054609 on epoch=22
06/02/2022 11:21:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.831548 on epoch=23
06/02/2022 11:21:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.791236 on epoch=24
06/02/2022 11:21:11 - INFO - __main__ - Global step 200 Train loss 0.946863 Classification-F1 0.36 on epoch=24
06/02/2022 11:21:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.673380 on epoch=26
06/02/2022 11:21:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.635977 on epoch=27
06/02/2022 11:21:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.462374 on epoch=28
06/02/2022 11:21:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.554124 on epoch=29
06/02/2022 11:21:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.486528 on epoch=31
06/02/2022 11:21:39 - INFO - __main__ - Global step 250 Train loss 0.562477 Classification-F1 0.28888888888888886 on epoch=31
06/02/2022 11:21:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.403534 on epoch=32
06/02/2022 11:21:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.403535 on epoch=33
06/02/2022 11:21:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.310193 on epoch=34
06/02/2022 11:22:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.240867 on epoch=36
06/02/2022 11:22:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.310848 on epoch=37
06/02/2022 11:22:06 - INFO - __main__ - Global step 300 Train loss 0.333795 Classification-F1 0.49606299212598426 on epoch=37
06/02/2022 11:22:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.515248 on epoch=38
06/02/2022 11:22:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.309991 on epoch=39
06/02/2022 11:22:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.410274 on epoch=41
06/02/2022 11:22:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.464136 on epoch=42
06/02/2022 11:22:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.224407 on epoch=43
06/02/2022 11:22:34 - INFO - __main__ - Global step 350 Train loss 0.384811 Classification-F1 0.49606299212598426 on epoch=43
06/02/2022 11:22:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.638939 on epoch=44
06/02/2022 11:22:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.663721 on epoch=46
06/02/2022 11:22:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.221542 on epoch=47
06/02/2022 11:22:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.386254 on epoch=48
06/02/2022 11:23:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.138830 on epoch=49
06/02/2022 11:23:01 - INFO - __main__ - Global step 400 Train loss 0.409857 Classification-F1 0.49606299212598426 on epoch=49
06/02/2022 11:23:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.318857 on epoch=51
06/02/2022 11:23:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.091588 on epoch=52
06/02/2022 11:23:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.093162 on epoch=53
06/02/2022 11:23:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.082937 on epoch=54
06/02/2022 11:23:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.146688 on epoch=56
06/02/2022 11:23:27 - INFO - __main__ - Global step 450 Train loss 0.146647 Classification-F1 0.4074074074074074 on epoch=56
06/02/2022 11:23:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.047814 on epoch=57
06/02/2022 11:23:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.098568 on epoch=58
06/02/2022 11:23:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.067730 on epoch=59
06/02/2022 11:23:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.142872 on epoch=61
06/02/2022 11:23:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.076192 on epoch=62
06/02/2022 11:23:54 - INFO - __main__ - Global step 500 Train loss 0.086635 Classification-F1 0.4796747967479675 on epoch=62
06/02/2022 11:23:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.060472 on epoch=63
06/02/2022 11:24:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.050050 on epoch=64
06/02/2022 11:24:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.040389 on epoch=66
06/02/2022 11:24:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.060108 on epoch=67
06/02/2022 11:24:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.075295 on epoch=68
06/02/2022 11:24:21 - INFO - __main__ - Global step 550 Train loss 0.057263 Classification-F1 0.49606299212598426 on epoch=68
06/02/2022 11:24:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.052640 on epoch=69
06/02/2022 11:24:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.051944 on epoch=71
06/02/2022 11:24:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.033400 on epoch=72
06/02/2022 11:24:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.078677 on epoch=73
06/02/2022 11:24:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.032934 on epoch=74
06/02/2022 11:24:47 - INFO - __main__ - Global step 600 Train loss 0.049919 Classification-F1 0.488 on epoch=74
06/02/2022 11:24:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.058966 on epoch=76
06/02/2022 11:24:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.026654 on epoch=77
06/02/2022 11:25:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.059408 on epoch=78
06/02/2022 11:25:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.026299 on epoch=79
06/02/2022 11:25:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.034673 on epoch=81
06/02/2022 11:25:13 - INFO - __main__ - Global step 650 Train loss 0.041200 Classification-F1 0.42857142857142855 on epoch=81
06/02/2022 11:25:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.021819 on epoch=82
06/02/2022 11:25:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.023551 on epoch=83
06/02/2022 11:25:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.016063 on epoch=84
06/02/2022 11:25:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.027162 on epoch=86
06/02/2022 11:25:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.043457 on epoch=87
06/02/2022 11:25:40 - INFO - __main__ - Global step 700 Train loss 0.026410 Classification-F1 0.49206349206349204 on epoch=87
06/02/2022 11:25:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.017165 on epoch=88
06/02/2022 11:25:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.010373 on epoch=89
06/02/2022 11:25:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.010926 on epoch=91
06/02/2022 11:26:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.019485 on epoch=92
06/02/2022 11:26:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.012199 on epoch=93
06/02/2022 11:26:06 - INFO - __main__ - Global step 750 Train loss 0.014029 Classification-F1 0.488 on epoch=93
06/02/2022 11:26:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.008853 on epoch=94
06/02/2022 11:26:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.011759 on epoch=96
06/02/2022 11:26:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.014886 on epoch=97
06/02/2022 11:26:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.029624 on epoch=98
06/02/2022 11:26:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.015041 on epoch=99
06/02/2022 11:26:32 - INFO - __main__ - Global step 800 Train loss 0.016033 Classification-F1 0.488 on epoch=99
06/02/2022 11:26:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.009490 on epoch=101
06/02/2022 11:26:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.004005 on epoch=102
06/02/2022 11:26:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.011512 on epoch=103
06/02/2022 11:26:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.004682 on epoch=104
06/02/2022 11:26:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.008368 on epoch=106
06/02/2022 11:26:59 - INFO - __main__ - Global step 850 Train loss 0.007611 Classification-F1 0.4838709677419355 on epoch=106
06/02/2022 11:27:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.001496 on epoch=107
06/02/2022 11:27:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.001444 on epoch=108
06/02/2022 11:27:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000659 on epoch=109
06/02/2022 11:27:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.036190 on epoch=111
06/02/2022 11:27:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.003794 on epoch=112
06/02/2022 11:27:25 - INFO - __main__ - Global step 900 Train loss 0.008716 Classification-F1 0.488 on epoch=112
06/02/2022 11:27:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.028037 on epoch=113
06/02/2022 11:27:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.001170 on epoch=114
06/02/2022 11:27:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.001899 on epoch=116
06/02/2022 11:27:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.001474 on epoch=117
06/02/2022 11:27:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.001438 on epoch=118
06/02/2022 11:27:52 - INFO - __main__ - Global step 950 Train loss 0.006804 Classification-F1 0.488 on epoch=118
06/02/2022 11:27:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.007999 on epoch=119
06/02/2022 11:28:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000697 on epoch=121
06/02/2022 11:28:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.005563 on epoch=122
06/02/2022 11:28:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.001529 on epoch=123
06/02/2022 11:28:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.019143 on epoch=124
06/02/2022 11:28:18 - INFO - __main__ - Global step 1000 Train loss 0.006986 Classification-F1 0.49206349206349204 on epoch=124
06/02/2022 11:28:18 - INFO - __main__ - save last model!
06/02/2022 11:28:18 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 11:28:18 - INFO - __main__ - Printing 3 examples
06/02/2022 11:28:18 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
06/02/2022 11:28:18 - INFO - __main__ - ['false']
06/02/2022 11:28:18 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
06/02/2022 11:28:18 - INFO - __main__ - ['false']
06/02/2022 11:28:18 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
06/02/2022 11:28:18 - INFO - __main__ - ['false']
06/02/2022 11:28:18 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:28:18 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:28:19 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 11:28:19 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:28:19 - INFO - __main__ - Printing 3 examples
06/02/2022 11:28:19 - INFO - __main__ -  [ethos-religion] I really don’t like your accent. I don’t want to call support and call to rats like you. I want a cute lady with British accent to answer my call. You are third world pieces with no job to this planet
06/02/2022 11:28:19 - INFO - __main__ - ['false']
06/02/2022 11:28:19 - INFO - __main__ -  [ethos-religion] Humans with third leg I hope to distinct soon, with much love
06/02/2022 11:28:19 - INFO - __main__ - ['false']
06/02/2022 11:28:19 - INFO - __main__ -  [ethos-religion] I would pull you over for your race, if I was a cop.
06/02/2022 11:28:19 - INFO - __main__ - ['false']
06/02/2022 11:28:19 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:28:19 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:28:19 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:28:25 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 11:28:26 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 11:28:26 - INFO - __main__ - Printing 3 examples
06/02/2022 11:28:26 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 11:28:26 - INFO - __main__ - ['false']
06/02/2022 11:28:26 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 11:28:26 - INFO - __main__ - ['false']
06/02/2022 11:28:26 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 11:28:26 - INFO - __main__ - ['true']
06/02/2022 11:28:26 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:28:26 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:28:26 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 11:28:27 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_100_0.0002_8_predictions.txt
06/02/2022 11:28:27 - INFO - __main__ - Classification-F1 on test data: 0.8745
06/02/2022 11:28:28 - INFO - __main__ - prefix=ethos-religion_64_100, lr=0.0002, bsz=8, dev_performance=0.49606299212598426, test_performance=0.8744588744588745
06/02/2022 11:28:28 - INFO - __main__ - Running ... prefix=ethos-religion_64_100, lr=0.0001, bsz=8 ...
06/02/2022 11:28:28 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 11:28:28 - INFO - __main__ - Printing 3 examples
06/02/2022 11:28:28 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped”.
06/02/2022 11:28:28 - INFO - __main__ - ['false']
06/02/2022 11:28:28 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
06/02/2022 11:28:28 - INFO - __main__ - ['false']
06/02/2022 11:28:28 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
06/02/2022 11:28:28 - INFO - __main__ - ['false']
06/02/2022 11:28:28 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:28:29 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:28:29 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 11:28:29 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:28:29 - INFO - __main__ - Printing 3 examples
06/02/2022 11:28:29 - INFO - __main__ -  [ethos-religion] I really don’t like your accent. I don’t want to call support and call to rats like you. I want a cute lady with British accent to answer my call. You are third world pieces with no job to this planet
06/02/2022 11:28:29 - INFO - __main__ - ['false']
06/02/2022 11:28:29 - INFO - __main__ -  [ethos-religion] Humans with third leg I hope to distinct soon, with much love
06/02/2022 11:28:29 - INFO - __main__ - ['false']
06/02/2022 11:28:29 - INFO - __main__ -  [ethos-religion] I would pull you over for your race, if I was a cop.
06/02/2022 11:28:29 - INFO - __main__ - ['false']
06/02/2022 11:28:29 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:28:29 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:28:29 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:28:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 11:28:31 - INFO - __main__ - Starting training!
06/02/2022 11:28:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 11:28:42 - INFO - __main__ - Starting training!
06/02/2022 11:28:46 - INFO - __main__ - Step 10 Global step 10 Train loss 24.207672 on epoch=1
06/02/2022 11:28:51 - INFO - __main__ - Step 20 Global step 20 Train loss 22.593985 on epoch=2
06/02/2022 11:28:56 - INFO - __main__ - Step 30 Global step 30 Train loss 20.004709 on epoch=3
06/02/2022 11:29:01 - INFO - __main__ - Step 40 Global step 40 Train loss 18.251133 on epoch=4
06/02/2022 11:29:06 - INFO - __main__ - Step 50 Global step 50 Train loss 17.598003 on epoch=6
06/02/2022 11:29:25 - INFO - __main__ - Global step 50 Train loss 20.531101 Classification-F1 0.0 on epoch=6
06/02/2022 11:29:30 - INFO - __main__ - Step 60 Global step 60 Train loss 17.367199 on epoch=7
06/02/2022 11:29:35 - INFO - __main__ - Step 70 Global step 70 Train loss 17.386173 on epoch=8
06/02/2022 11:29:40 - INFO - __main__ - Step 80 Global step 80 Train loss 16.452932 on epoch=9
06/02/2022 11:29:45 - INFO - __main__ - Step 90 Global step 90 Train loss 17.073572 on epoch=11
06/02/2022 11:29:50 - INFO - __main__ - Step 100 Global step 100 Train loss 15.547841 on epoch=12
06/02/2022 11:30:06 - INFO - __main__ - Global step 100 Train loss 16.765545 Classification-F1 0.0 on epoch=12
06/02/2022 11:30:11 - INFO - __main__ - Step 110 Global step 110 Train loss 15.406682 on epoch=13
06/02/2022 11:30:16 - INFO - __main__ - Step 120 Global step 120 Train loss 14.829190 on epoch=14
06/02/2022 11:30:21 - INFO - __main__ - Step 130 Global step 130 Train loss 14.495135 on epoch=16
06/02/2022 11:30:26 - INFO - __main__ - Step 140 Global step 140 Train loss 14.677500 on epoch=17
06/02/2022 11:30:31 - INFO - __main__ - Step 150 Global step 150 Train loss 14.111239 on epoch=18
06/02/2022 11:30:48 - INFO - __main__ - Global step 150 Train loss 14.703950 Classification-F1 0.0 on epoch=18
06/02/2022 11:30:53 - INFO - __main__ - Step 160 Global step 160 Train loss 13.299438 on epoch=19
06/02/2022 11:30:58 - INFO - __main__ - Step 170 Global step 170 Train loss 13.061862 on epoch=21
06/02/2022 11:31:03 - INFO - __main__ - Step 180 Global step 180 Train loss 12.388132 on epoch=22
06/02/2022 11:31:08 - INFO - __main__ - Step 190 Global step 190 Train loss 12.079249 on epoch=23
06/02/2022 11:31:14 - INFO - __main__ - Step 200 Global step 200 Train loss 11.652545 on epoch=24
06/02/2022 11:31:29 - INFO - __main__ - Global step 200 Train loss 12.496243 Classification-F1 0.0 on epoch=24
06/02/2022 11:31:34 - INFO - __main__ - Step 210 Global step 210 Train loss 10.505354 on epoch=26
06/02/2022 11:31:39 - INFO - __main__ - Step 220 Global step 220 Train loss 10.312159 on epoch=27
06/02/2022 11:31:44 - INFO - __main__ - Step 230 Global step 230 Train loss 9.180963 on epoch=28
06/02/2022 11:31:49 - INFO - __main__ - Step 240 Global step 240 Train loss 7.879546 on epoch=29
06/02/2022 11:31:54 - INFO - __main__ - Step 250 Global step 250 Train loss 4.673872 on epoch=31
06/02/2022 11:31:55 - INFO - __main__ - Global step 250 Train loss 8.510379 Classification-F1 0.488 on epoch=31
06/02/2022 11:32:02 - INFO - __main__ - Step 260 Global step 260 Train loss 5.326822 on epoch=32
06/02/2022 11:32:07 - INFO - __main__ - Step 270 Global step 270 Train loss 4.187915 on epoch=33
06/02/2022 11:32:12 - INFO - __main__ - Step 280 Global step 280 Train loss 3.205084 on epoch=34
06/02/2022 11:32:17 - INFO - __main__ - Step 290 Global step 290 Train loss 1.939239 on epoch=36
06/02/2022 11:32:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.735890 on epoch=37
06/02/2022 11:32:23 - INFO - __main__ - Global step 300 Train loss 3.078990 Classification-F1 0.46218487394957986 on epoch=37
06/02/2022 11:32:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.771843 on epoch=38
06/02/2022 11:32:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.643662 on epoch=39
06/02/2022 11:32:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.600300 on epoch=41
06/02/2022 11:32:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.601123 on epoch=42
06/02/2022 11:32:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.515166 on epoch=43
06/02/2022 11:32:49 - INFO - __main__ - Global step 350 Train loss 0.626419 Classification-F1 0.42342342342342343 on epoch=43
06/02/2022 11:32:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.606875 on epoch=44
06/02/2022 11:32:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.980230 on epoch=46
06/02/2022 11:33:04 - INFO - __main__ - Step 380 Global step 380 Train loss 1.252227 on epoch=47
06/02/2022 11:33:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.636933 on epoch=48
06/02/2022 11:33:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.444786 on epoch=49
06/02/2022 11:33:15 - INFO - __main__ - Global step 400 Train loss 0.784210 Classification-F1 0.4336283185840708 on epoch=49
06/02/2022 11:33:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.658582 on epoch=51
06/02/2022 11:33:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.464790 on epoch=52
06/02/2022 11:33:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.431084 on epoch=53
06/02/2022 11:33:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.298354 on epoch=54
06/02/2022 11:33:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.350445 on epoch=56
06/02/2022 11:33:41 - INFO - __main__ - Global step 450 Train loss 0.440651 Classification-F1 0.3469387755102041 on epoch=56
06/02/2022 11:33:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.328877 on epoch=57
06/02/2022 11:33:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.298736 on epoch=58
06/02/2022 11:33:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.292517 on epoch=59
06/02/2022 11:34:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.324261 on epoch=61
06/02/2022 11:34:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.325790 on epoch=62
06/02/2022 11:34:07 - INFO - __main__ - Global step 500 Train loss 0.314036 Classification-F1 0.4838709677419355 on epoch=62
06/02/2022 11:34:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.266358 on epoch=63
06/02/2022 11:34:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.300679 on epoch=64
06/02/2022 11:34:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.313679 on epoch=66
06/02/2022 11:34:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.269030 on epoch=67
06/02/2022 11:34:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.309456 on epoch=68
06/02/2022 11:34:33 - INFO - __main__ - Global step 550 Train loss 0.291841 Classification-F1 0.47107438016528924 on epoch=68
06/02/2022 11:34:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.332910 on epoch=69
06/02/2022 11:34:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.286672 on epoch=71
06/02/2022 11:34:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.240766 on epoch=72
06/02/2022 11:34:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.450166 on epoch=73
06/02/2022 11:34:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.213362 on epoch=74
06/02/2022 11:35:00 - INFO - __main__ - Global step 600 Train loss 0.304775 Classification-F1 0.4666666666666667 on epoch=74
06/02/2022 11:35:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.304158 on epoch=76
06/02/2022 11:35:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.239803 on epoch=77
06/02/2022 11:35:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.280724 on epoch=78
06/02/2022 11:35:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.287136 on epoch=79
06/02/2022 11:35:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.388130 on epoch=81
06/02/2022 11:35:26 - INFO - __main__ - Global step 650 Train loss 0.299990 Classification-F1 0.4336283185840708 on epoch=81
06/02/2022 11:35:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.286496 on epoch=82
06/02/2022 11:35:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.303089 on epoch=83
06/02/2022 11:35:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.334653 on epoch=84
06/02/2022 11:35:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.303200 on epoch=86
06/02/2022 11:35:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.435436 on epoch=87
06/02/2022 11:35:52 - INFO - __main__ - Global step 700 Train loss 0.332575 Classification-F1 0.41284403669724773 on epoch=87
06/02/2022 11:35:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.236984 on epoch=88
06/02/2022 11:36:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.248699 on epoch=89
06/02/2022 11:36:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.374817 on epoch=91
06/02/2022 11:36:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.300437 on epoch=92
06/02/2022 11:36:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.262618 on epoch=93
06/02/2022 11:36:18 - INFO - __main__ - Global step 750 Train loss 0.284711 Classification-F1 0.47107438016528924 on epoch=93
06/02/2022 11:36:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.262440 on epoch=94
06/02/2022 11:36:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.333493 on epoch=96
06/02/2022 11:36:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.349261 on epoch=97
06/02/2022 11:36:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.257314 on epoch=98
06/02/2022 11:36:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.386457 on epoch=99
06/02/2022 11:36:44 - INFO - __main__ - Global step 800 Train loss 0.317793 Classification-F1 0.47107438016528924 on epoch=99
06/02/2022 11:36:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.264020 on epoch=101
06/02/2022 11:36:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.362183 on epoch=102
06/02/2022 11:37:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.238188 on epoch=103
06/02/2022 11:37:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.213918 on epoch=104
06/02/2022 11:37:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.224655 on epoch=106
06/02/2022 11:37:10 - INFO - __main__ - Global step 850 Train loss 0.260593 Classification-F1 0.41284403669724773 on epoch=106
06/02/2022 11:37:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.308045 on epoch=107
06/02/2022 11:37:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.234801 on epoch=108
06/02/2022 11:37:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.259027 on epoch=109
06/02/2022 11:37:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.249009 on epoch=111
06/02/2022 11:37:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.317212 on epoch=112
06/02/2022 11:37:37 - INFO - __main__ - Global step 900 Train loss 0.273619 Classification-F1 0.4434782608695652 on epoch=112
06/02/2022 11:37:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.257861 on epoch=113
06/02/2022 11:37:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.200245 on epoch=114
06/02/2022 11:37:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.263206 on epoch=116
06/02/2022 11:37:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.317747 on epoch=117
06/02/2022 11:38:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.255982 on epoch=118
06/02/2022 11:38:03 - INFO - __main__ - Global step 950 Train loss 0.259008 Classification-F1 1.0 on epoch=118
06/02/2022 11:38:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.235927 on epoch=119
06/02/2022 11:38:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.176662 on epoch=121
06/02/2022 11:38:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.227566 on epoch=122
06/02/2022 11:38:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.245089 on epoch=123
06/02/2022 11:38:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.165740 on epoch=124
06/02/2022 11:38:29 - INFO - __main__ - Global step 1000 Train loss 0.210197 Classification-F1 0.49206349206349204 on epoch=124
06/02/2022 11:38:29 - INFO - __main__ - save last model!
06/02/2022 11:38:30 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 11:38:30 - INFO - __main__ - Printing 3 examples
06/02/2022 11:38:30 - INFO - __main__ -  [ethos-religion] Disability centres all around the world. That’s were our money goes. Let’s take em down
06/02/2022 11:38:30 - INFO - __main__ - ['false']
06/02/2022 11:38:30 - INFO - __main__ -  [ethos-religion] FUCK GAYS disgusting
06/02/2022 11:38:30 - INFO - __main__ - ['false']
06/02/2022 11:38:30 - INFO - __main__ -  [ethos-religion] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
06/02/2022 11:38:30 - INFO - __main__ - ['false']
06/02/2022 11:38:30 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:38:30 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:38:30 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 11:38:30 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:38:30 - INFO - __main__ - Printing 3 examples
06/02/2022 11:38:30 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/02/2022 11:38:30 - INFO - __main__ - ['false']
06/02/2022 11:38:30 - INFO - __main__ -  [ethos-religion] All people are fucked up because of these queer thing because y'all realize how shitty gender roles are and how absurd is to see it in reverse genders roles
06/02/2022 11:38:30 - INFO - __main__ - ['false']
06/02/2022 11:38:30 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
06/02/2022 11:38:30 - INFO - __main__ - ['false']
06/02/2022 11:38:30 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:38:30 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:38:30 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:38:38 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 11:38:38 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 11:38:38 - INFO - __main__ - Printing 3 examples
06/02/2022 11:38:38 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 11:38:38 - INFO - __main__ - ['false']
06/02/2022 11:38:38 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 11:38:38 - INFO - __main__ - ['false']
06/02/2022 11:38:38 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 11:38:38 - INFO - __main__ - ['true']
06/02/2022 11:38:38 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:38:38 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:38:38 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 11:38:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_100_0.0001_8_predictions.txt
06/02/2022 11:38:40 - INFO - __main__ - Classification-F1 on test data: 0.9058
06/02/2022 11:38:41 - INFO - __main__ - prefix=ethos-religion_64_100, lr=0.0001, bsz=8, dev_performance=1.0, test_performance=0.9058441558441558
06/02/2022 11:38:41 - INFO - __main__ - Running ... prefix=ethos-religion_64_13, lr=0.0005, bsz=8 ...
06/02/2022 11:38:42 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 11:38:42 - INFO - __main__ - Printing 3 examples
06/02/2022 11:38:42 - INFO - __main__ -  [ethos-religion] Disability centres all around the world. That’s were our money goes. Let’s take em down
06/02/2022 11:38:42 - INFO - __main__ - ['false']
06/02/2022 11:38:42 - INFO - __main__ -  [ethos-religion] FUCK GAYS disgusting
06/02/2022 11:38:42 - INFO - __main__ - ['false']
06/02/2022 11:38:42 - INFO - __main__ -  [ethos-religion] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
06/02/2022 11:38:42 - INFO - __main__ - ['false']
06/02/2022 11:38:42 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:38:42 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:38:42 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 11:38:42 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:38:42 - INFO - __main__ - Printing 3 examples
06/02/2022 11:38:42 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/02/2022 11:38:42 - INFO - __main__ - ['false']
06/02/2022 11:38:42 - INFO - __main__ -  [ethos-religion] All people are fucked up because of these queer thing because y'all realize how shitty gender roles are and how absurd is to see it in reverse genders roles
06/02/2022 11:38:42 - INFO - __main__ - ['false']
06/02/2022 11:38:42 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
06/02/2022 11:38:42 - INFO - __main__ - ['false']
06/02/2022 11:38:42 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:38:42 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:38:42 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:38:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 11:38:43 - INFO - __main__ - Starting training!
06/02/2022 11:38:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 11:38:55 - INFO - __main__ - Starting training!
06/02/2022 11:38:59 - INFO - __main__ - Step 10 Global step 10 Train loss 23.757626 on epoch=1
06/02/2022 11:39:04 - INFO - __main__ - Step 20 Global step 20 Train loss 18.388378 on epoch=2
06/02/2022 11:39:09 - INFO - __main__ - Step 30 Global step 30 Train loss 16.725142 on epoch=3
06/02/2022 11:39:14 - INFO - __main__ - Step 40 Global step 40 Train loss 14.196655 on epoch=4
06/02/2022 11:39:19 - INFO - __main__ - Step 50 Global step 50 Train loss 11.828601 on epoch=6
06/02/2022 11:39:20 - INFO - __main__ - Global step 50 Train loss 16.979280 Classification-F1 0.0 on epoch=6
06/02/2022 11:39:25 - INFO - __main__ - Step 60 Global step 60 Train loss 8.980101 on epoch=7
06/02/2022 11:39:30 - INFO - __main__ - Step 70 Global step 70 Train loss 4.749309 on epoch=8
06/02/2022 11:39:35 - INFO - __main__ - Step 80 Global step 80 Train loss 2.774466 on epoch=9
06/02/2022 11:39:40 - INFO - __main__ - Step 90 Global step 90 Train loss 3.116344 on epoch=11
06/02/2022 11:39:45 - INFO - __main__ - Step 100 Global step 100 Train loss 2.740673 on epoch=12
06/02/2022 11:39:46 - INFO - __main__ - Global step 100 Train loss 4.472178 Classification-F1 1.0 on epoch=12
06/02/2022 11:39:52 - INFO - __main__ - Step 110 Global step 110 Train loss 3.034386 on epoch=13
06/02/2022 11:39:57 - INFO - __main__ - Step 120 Global step 120 Train loss 2.477458 on epoch=14
06/02/2022 11:40:02 - INFO - __main__ - Step 130 Global step 130 Train loss 1.867341 on epoch=16
06/02/2022 11:40:07 - INFO - __main__ - Step 140 Global step 140 Train loss 2.493418 on epoch=17
06/02/2022 11:40:12 - INFO - __main__ - Step 150 Global step 150 Train loss 2.354001 on epoch=18
06/02/2022 11:40:13 - INFO - __main__ - Global step 150 Train loss 2.445321 Classification-F1 0.43859649122807015 on epoch=18
06/02/2022 11:40:18 - INFO - __main__ - Step 160 Global step 160 Train loss 1.497488 on epoch=19
06/02/2022 11:40:23 - INFO - __main__ - Step 170 Global step 170 Train loss 1.720790 on epoch=21
06/02/2022 11:40:28 - INFO - __main__ - Step 180 Global step 180 Train loss 2.045855 on epoch=22
06/02/2022 11:40:33 - INFO - __main__ - Step 190 Global step 190 Train loss 1.402294 on epoch=23
06/02/2022 11:40:38 - INFO - __main__ - Step 200 Global step 200 Train loss 1.447191 on epoch=24
06/02/2022 11:40:39 - INFO - __main__ - Global step 200 Train loss 1.622724 Classification-F1 0.0 on epoch=24
06/02/2022 11:40:44 - INFO - __main__ - Step 210 Global step 210 Train loss 1.909501 on epoch=26
06/02/2022 11:40:49 - INFO - __main__ - Step 220 Global step 220 Train loss 1.503648 on epoch=27
06/02/2022 11:40:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.943549 on epoch=28
06/02/2022 11:41:00 - INFO - __main__ - Step 240 Global step 240 Train loss 1.192739 on epoch=29
06/02/2022 11:41:05 - INFO - __main__ - Step 250 Global step 250 Train loss 1.137981 on epoch=31
06/02/2022 11:41:06 - INFO - __main__ - Global step 250 Train loss 1.337484 Classification-F1 0.452991452991453 on epoch=31
06/02/2022 11:41:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.896380 on epoch=32
06/02/2022 11:41:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.946525 on epoch=33
06/02/2022 11:41:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.720242 on epoch=34
06/02/2022 11:41:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.548250 on epoch=36
06/02/2022 11:41:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.601307 on epoch=37
06/02/2022 11:41:32 - INFO - __main__ - Global step 300 Train loss 0.742541 Classification-F1 1.0 on epoch=37
06/02/2022 11:41:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.691257 on epoch=38
06/02/2022 11:41:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.718943 on epoch=39
06/02/2022 11:41:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.635501 on epoch=41
06/02/2022 11:41:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.624949 on epoch=42
06/02/2022 11:41:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.553832 on epoch=43
06/02/2022 11:41:58 - INFO - __main__ - Global step 350 Train loss 0.644897 Classification-F1 1.0 on epoch=43
06/02/2022 11:42:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.492366 on epoch=44
06/02/2022 11:42:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.466815 on epoch=46
06/02/2022 11:42:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.523537 on epoch=47
06/02/2022 11:42:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.624453 on epoch=48
06/02/2022 11:42:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.504263 on epoch=49
06/02/2022 11:42:24 - INFO - __main__ - Global step 400 Train loss 0.522287 Classification-F1 1.0 on epoch=49
06/02/2022 11:42:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.389226 on epoch=51
06/02/2022 11:42:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.490759 on epoch=52
06/02/2022 11:42:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.388498 on epoch=53
06/02/2022 11:42:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.503258 on epoch=54
06/02/2022 11:42:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.464315 on epoch=56
06/02/2022 11:42:50 - INFO - __main__ - Global step 450 Train loss 0.447211 Classification-F1 0.0 on epoch=56
06/02/2022 11:42:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.483260 on epoch=57
06/02/2022 11:43:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.444110 on epoch=58
06/02/2022 11:43:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.408845 on epoch=59
06/02/2022 11:43:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.403602 on epoch=61
06/02/2022 11:43:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.430691 on epoch=62
06/02/2022 11:43:16 - INFO - __main__ - Global step 500 Train loss 0.434102 Classification-F1 1.0 on epoch=62
06/02/2022 11:43:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.416479 on epoch=63
06/02/2022 11:43:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.377810 on epoch=64
06/02/2022 11:43:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.421529 on epoch=66
06/02/2022 11:43:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.369432 on epoch=67
06/02/2022 11:43:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.432680 on epoch=68
06/02/2022 11:43:42 - INFO - __main__ - Global step 550 Train loss 0.403586 Classification-F1 1.0 on epoch=68
06/02/2022 11:43:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.418392 on epoch=69
06/02/2022 11:43:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.402193 on epoch=71
06/02/2022 11:43:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.404007 on epoch=72
06/02/2022 11:44:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.434629 on epoch=73
06/02/2022 11:44:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.348366 on epoch=74
06/02/2022 11:44:08 - INFO - __main__ - Global step 600 Train loss 0.401517 Classification-F1 1.0 on epoch=74
06/02/2022 11:44:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.409196 on epoch=76
06/02/2022 11:44:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.392619 on epoch=77
06/02/2022 11:44:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.415553 on epoch=78
06/02/2022 11:44:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.371048 on epoch=79
06/02/2022 11:44:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.370617 on epoch=81
06/02/2022 11:44:34 - INFO - __main__ - Global step 650 Train loss 0.391806 Classification-F1 0.0 on epoch=81
06/02/2022 11:44:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.374149 on epoch=82
06/02/2022 11:44:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.434624 on epoch=83
06/02/2022 11:44:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.414058 on epoch=84
06/02/2022 11:44:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.365688 on epoch=86
06/02/2022 11:45:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.372850 on epoch=87
06/02/2022 11:45:01 - INFO - __main__ - Global step 700 Train loss 0.392274 Classification-F1 1.0 on epoch=87
06/02/2022 11:45:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.373220 on epoch=88
06/02/2022 11:45:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.385340 on epoch=89
06/02/2022 11:45:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.398487 on epoch=91
06/02/2022 11:45:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.381293 on epoch=92
06/02/2022 11:45:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.368741 on epoch=93
06/02/2022 11:45:27 - INFO - __main__ - Global step 750 Train loss 0.381416 Classification-F1 0.49606299212598426 on epoch=93
06/02/2022 11:45:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.323240 on epoch=94
06/02/2022 11:45:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.376336 on epoch=96
06/02/2022 11:45:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.371185 on epoch=97
06/02/2022 11:45:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.355074 on epoch=98
06/02/2022 11:45:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.341322 on epoch=99
06/02/2022 11:45:53 - INFO - __main__ - Global step 800 Train loss 0.353431 Classification-F1 0.13513513513513514 on epoch=99
06/02/2022 11:45:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.348976 on epoch=101
06/02/2022 11:46:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.365383 on epoch=102
06/02/2022 11:46:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.365861 on epoch=103
06/02/2022 11:46:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.373878 on epoch=104
06/02/2022 11:46:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.379593 on epoch=106
06/02/2022 11:46:19 - INFO - __main__ - Global step 850 Train loss 0.366738 Classification-F1 0.0 on epoch=106
06/02/2022 11:46:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.353848 on epoch=107
06/02/2022 11:46:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.372133 on epoch=108
06/02/2022 11:46:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.323029 on epoch=109
06/02/2022 11:46:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.324137 on epoch=111
06/02/2022 11:46:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.311064 on epoch=112
06/02/2022 11:46:45 - INFO - __main__ - Global step 900 Train loss 0.336842 Classification-F1 0.488 on epoch=112
06/02/2022 11:46:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.307959 on epoch=113
06/02/2022 11:46:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.313572 on epoch=114
06/02/2022 11:47:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.280664 on epoch=116
06/02/2022 11:47:05 - INFO - __main__ - Step 940 Global step 940 Train loss 2.392211 on epoch=117
06/02/2022 11:47:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.930131 on epoch=118
06/02/2022 11:47:11 - INFO - __main__ - Global step 950 Train loss 0.844908 Classification-F1 0.015384615384615385 on epoch=118
06/02/2022 11:47:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.395580 on epoch=119
06/02/2022 11:47:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.365676 on epoch=121
06/02/2022 11:47:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.351334 on epoch=122
06/02/2022 11:47:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.351785 on epoch=123
06/02/2022 11:47:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.271142 on epoch=124
06/02/2022 11:47:37 - INFO - __main__ - Global step 1000 Train loss 0.347103 Classification-F1 0.4666666666666667 on epoch=124
06/02/2022 11:47:37 - INFO - __main__ - save last model!
06/02/2022 11:47:38 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 11:47:38 - INFO - __main__ - Printing 3 examples
06/02/2022 11:47:38 - INFO - __main__ -  [ethos-religion] Disability centres all around the world. That’s were our money goes. Let’s take em down
06/02/2022 11:47:38 - INFO - __main__ - ['false']
06/02/2022 11:47:38 - INFO - __main__ -  [ethos-religion] FUCK GAYS disgusting
06/02/2022 11:47:38 - INFO - __main__ - ['false']
06/02/2022 11:47:38 - INFO - __main__ -  [ethos-religion] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
06/02/2022 11:47:38 - INFO - __main__ - ['false']
06/02/2022 11:47:38 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:47:38 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:47:38 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 11:47:38 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:47:38 - INFO - __main__ - Printing 3 examples
06/02/2022 11:47:38 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/02/2022 11:47:38 - INFO - __main__ - ['false']
06/02/2022 11:47:38 - INFO - __main__ -  [ethos-religion] All people are fucked up because of these queer thing because y'all realize how shitty gender roles are and how absurd is to see it in reverse genders roles
06/02/2022 11:47:38 - INFO - __main__ - ['false']
06/02/2022 11:47:38 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
06/02/2022 11:47:38 - INFO - __main__ - ['false']
06/02/2022 11:47:38 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:47:38 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:47:38 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:47:44 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 11:47:45 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 11:47:45 - INFO - __main__ - Printing 3 examples
06/02/2022 11:47:45 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 11:47:45 - INFO - __main__ - ['false']
06/02/2022 11:47:45 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 11:47:45 - INFO - __main__ - ['false']
06/02/2022 11:47:45 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 11:47:45 - INFO - __main__ - ['true']
06/02/2022 11:47:45 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:47:45 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:47:45 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 11:47:46 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_13_0.0005_8_predictions.txt
06/02/2022 11:47:46 - INFO - __main__ - Classification-F1 on test data: 0.4387
06/02/2022 11:47:47 - INFO - __main__ - prefix=ethos-religion_64_13, lr=0.0005, bsz=8, dev_performance=1.0, test_performance=0.4387096774193549
06/02/2022 11:47:47 - INFO - __main__ - Running ... prefix=ethos-religion_64_13, lr=0.0003, bsz=8 ...
06/02/2022 11:47:48 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 11:47:48 - INFO - __main__ - Printing 3 examples
06/02/2022 11:47:48 - INFO - __main__ -  [ethos-religion] Disability centres all around the world. That’s were our money goes. Let’s take em down
06/02/2022 11:47:48 - INFO - __main__ - ['false']
06/02/2022 11:47:48 - INFO - __main__ -  [ethos-religion] FUCK GAYS disgusting
06/02/2022 11:47:48 - INFO - __main__ - ['false']
06/02/2022 11:47:48 - INFO - __main__ -  [ethos-religion] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
06/02/2022 11:47:48 - INFO - __main__ - ['false']
06/02/2022 11:47:48 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:47:48 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:47:48 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 11:47:48 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:47:48 - INFO - __main__ - Printing 3 examples
06/02/2022 11:47:48 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/02/2022 11:47:48 - INFO - __main__ - ['false']
06/02/2022 11:47:48 - INFO - __main__ -  [ethos-religion] All people are fucked up because of these queer thing because y'all realize how shitty gender roles are and how absurd is to see it in reverse genders roles
06/02/2022 11:47:48 - INFO - __main__ - ['false']
06/02/2022 11:47:48 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
06/02/2022 11:47:48 - INFO - __main__ - ['false']
06/02/2022 11:47:48 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:47:48 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:47:48 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:47:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 11:47:51 - INFO - __main__ - Starting training!
06/02/2022 11:48:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 11:48:01 - INFO - __main__ - Starting training!
06/02/2022 11:48:05 - INFO - __main__ - Step 10 Global step 10 Train loss 22.811523 on epoch=1
06/02/2022 11:48:10 - INFO - __main__ - Step 20 Global step 20 Train loss 18.452501 on epoch=2
06/02/2022 11:48:15 - INFO - __main__ - Step 30 Global step 30 Train loss 17.266825 on epoch=3
06/02/2022 11:48:21 - INFO - __main__ - Step 40 Global step 40 Train loss 15.452242 on epoch=4
06/02/2022 11:48:26 - INFO - __main__ - Step 50 Global step 50 Train loss 14.882403 on epoch=6
06/02/2022 11:48:27 - INFO - __main__ - Global step 50 Train loss 17.773100 Classification-F1 0.0 on epoch=6
06/02/2022 11:48:33 - INFO - __main__ - Step 60 Global step 60 Train loss 13.376666 on epoch=7
06/02/2022 11:48:38 - INFO - __main__ - Step 70 Global step 70 Train loss 12.948726 on epoch=8
06/02/2022 11:48:43 - INFO - __main__ - Step 80 Global step 80 Train loss 10.057441 on epoch=9
06/02/2022 11:48:49 - INFO - __main__ - Step 90 Global step 90 Train loss 6.747826 on epoch=11
06/02/2022 11:48:54 - INFO - __main__ - Step 100 Global step 100 Train loss 1.964689 on epoch=12
06/02/2022 11:48:55 - INFO - __main__ - Global step 100 Train loss 9.019069 Classification-F1 0.49606299212598426 on epoch=12
06/02/2022 11:49:01 - INFO - __main__ - Step 110 Global step 110 Train loss 1.023816 on epoch=13
06/02/2022 11:49:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.503560 on epoch=14
06/02/2022 11:49:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.342547 on epoch=16
06/02/2022 11:49:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.382408 on epoch=17
06/02/2022 11:49:22 - INFO - __main__ - Step 150 Global step 150 Train loss 1.294531 on epoch=18
06/02/2022 11:49:23 - INFO - __main__ - Global step 150 Train loss 0.709372 Classification-F1 0.2442244224422442 on epoch=18
06/02/2022 11:49:28 - INFO - __main__ - Step 160 Global step 160 Train loss 1.068881 on epoch=19
06/02/2022 11:49:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.474985 on epoch=21
06/02/2022 11:49:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.260066 on epoch=22
06/02/2022 11:49:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.123381 on epoch=23
06/02/2022 11:49:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.113346 on epoch=24
06/02/2022 11:49:50 - INFO - __main__ - Global step 200 Train loss 0.408131 Classification-F1 0.43859649122807015 on epoch=24
06/02/2022 11:49:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.198951 on epoch=26
06/02/2022 11:50:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.098424 on epoch=27
06/02/2022 11:50:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.147543 on epoch=28
06/02/2022 11:50:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.192930 on epoch=29
06/02/2022 11:50:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.145021 on epoch=31
06/02/2022 11:50:17 - INFO - __main__ - Global step 250 Train loss 0.156574 Classification-F1 0.46218487394957986 on epoch=31
06/02/2022 11:50:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.194470 on epoch=32
06/02/2022 11:50:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.061486 on epoch=33
06/02/2022 11:50:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.087212 on epoch=34
06/02/2022 11:50:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.123099 on epoch=36
06/02/2022 11:50:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.105149 on epoch=37
06/02/2022 11:50:44 - INFO - __main__ - Global step 300 Train loss 0.114283 Classification-F1 0.47540983606557374 on epoch=37
06/02/2022 11:50:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.070409 on epoch=38
06/02/2022 11:50:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.563515 on epoch=39
06/02/2022 11:51:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.285945 on epoch=41
06/02/2022 11:51:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.215551 on epoch=42
06/02/2022 11:51:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.043615 on epoch=43
06/02/2022 11:51:11 - INFO - __main__ - Global step 350 Train loss 0.235807 Classification-F1 0.46218487394957986 on epoch=43
06/02/2022 11:51:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.033906 on epoch=44
06/02/2022 11:51:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.005288 on epoch=46
06/02/2022 11:51:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.047084 on epoch=47
06/02/2022 11:51:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.015177 on epoch=48
06/02/2022 11:51:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.007144 on epoch=49
06/02/2022 11:51:38 - INFO - __main__ - Global step 400 Train loss 0.021720 Classification-F1 0.47107438016528924 on epoch=49
06/02/2022 11:51:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.016732 on epoch=51
06/02/2022 11:51:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.017644 on epoch=52
06/02/2022 11:51:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.009649 on epoch=53
06/02/2022 11:51:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.008878 on epoch=54
06/02/2022 11:52:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.021640 on epoch=56
06/02/2022 11:52:05 - INFO - __main__ - Global step 450 Train loss 0.014909 Classification-F1 0.4796747967479675 on epoch=56
06/02/2022 11:52:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000936 on epoch=57
06/02/2022 11:52:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.019277 on epoch=58
06/02/2022 11:52:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.009737 on epoch=59
06/02/2022 11:52:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.003041 on epoch=61
06/02/2022 11:52:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000348 on epoch=62
06/02/2022 11:52:32 - INFO - __main__ - Global step 500 Train loss 0.006667 Classification-F1 0.47107438016528924 on epoch=62
06/02/2022 11:52:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.040933 on epoch=63
06/02/2022 11:52:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.029318 on epoch=64
06/02/2022 11:52:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000535 on epoch=66
06/02/2022 11:52:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.009438 on epoch=67
06/02/2022 11:52:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.004077 on epoch=68
06/02/2022 11:52:59 - INFO - __main__ - Global step 550 Train loss 0.016860 Classification-F1 0.47540983606557374 on epoch=68
06/02/2022 11:53:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000174 on epoch=69
06/02/2022 11:53:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.024454 on epoch=71
06/02/2022 11:53:15 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000951 on epoch=72
06/02/2022 11:53:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000915 on epoch=73
06/02/2022 11:53:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000299 on epoch=74
06/02/2022 11:53:26 - INFO - __main__ - Global step 600 Train loss 0.005359 Classification-F1 0.47107438016528924 on epoch=74
06/02/2022 11:53:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.001688 on epoch=76
06/02/2022 11:53:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.005960 on epoch=77
06/02/2022 11:53:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000419 on epoch=78
06/02/2022 11:53:47 - INFO - __main__ - Step 640 Global step 640 Train loss 0.014676 on epoch=79
06/02/2022 11:53:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.064755 on epoch=81
06/02/2022 11:53:53 - INFO - __main__ - Global step 650 Train loss 0.017499 Classification-F1 0.4666666666666667 on epoch=81
06/02/2022 11:53:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000545 on epoch=82
06/02/2022 11:54:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000561 on epoch=83
06/02/2022 11:54:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000267 on epoch=84
06/02/2022 11:54:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.006261 on epoch=86
06/02/2022 11:54:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.021018 on epoch=87
06/02/2022 11:54:20 - INFO - __main__ - Global step 700 Train loss 0.005730 Classification-F1 0.47107438016528924 on epoch=87
06/02/2022 11:54:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000093 on epoch=88
06/02/2022 11:54:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.040662 on epoch=89
06/02/2022 11:54:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000318 on epoch=91
06/02/2022 11:54:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000246 on epoch=92
06/02/2022 11:54:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000352 on epoch=93
06/02/2022 11:54:47 - INFO - __main__ - Global step 750 Train loss 0.008334 Classification-F1 0.47107438016528924 on epoch=93
06/02/2022 11:54:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000799 on epoch=94
06/02/2022 11:54:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000075 on epoch=96
06/02/2022 11:55:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.027875 on epoch=97
06/02/2022 11:55:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.031887 on epoch=98
06/02/2022 11:55:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.001058 on epoch=99
06/02/2022 11:55:14 - INFO - __main__ - Global step 800 Train loss 0.012339 Classification-F1 0.47107438016528924 on epoch=99
06/02/2022 11:55:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000105 on epoch=101
06/02/2022 11:55:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.007483 on epoch=102
06/02/2022 11:55:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000630 on epoch=103
06/02/2022 11:55:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.002421 on epoch=104
06/02/2022 11:55:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000540 on epoch=106
06/02/2022 11:55:41 - INFO - __main__ - Global step 850 Train loss 0.002236 Classification-F1 0.47107438016528924 on epoch=106
06/02/2022 11:55:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000215 on epoch=107
06/02/2022 11:55:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000125 on epoch=108
06/02/2022 11:55:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000284 on epoch=109
06/02/2022 11:56:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.007218 on epoch=111
06/02/2022 11:56:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000367 on epoch=112
06/02/2022 11:56:07 - INFO - __main__ - Global step 900 Train loss 0.001642 Classification-F1 0.4666666666666667 on epoch=112
06/02/2022 11:56:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.002620 on epoch=113
06/02/2022 11:56:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000947 on epoch=114
06/02/2022 11:56:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000137 on epoch=116
06/02/2022 11:56:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.001181 on epoch=117
06/02/2022 11:56:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000065 on epoch=118
06/02/2022 11:56:34 - INFO - __main__ - Global step 950 Train loss 0.000990 Classification-F1 0.47107438016528924 on epoch=118
06/02/2022 11:56:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.018504 on epoch=119
06/02/2022 11:56:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000143 on epoch=121
06/02/2022 11:56:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.021857 on epoch=122
06/02/2022 11:56:54 - INFO - __main__ - Step 990 Global step 990 Train loss 0.005351 on epoch=123
06/02/2022 11:56:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000060 on epoch=124
06/02/2022 11:57:00 - INFO - __main__ - Global step 1000 Train loss 0.009183 Classification-F1 0.4666666666666667 on epoch=124
06/02/2022 11:57:00 - INFO - __main__ - save last model!
06/02/2022 11:57:01 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 11:57:01 - INFO - __main__ - Printing 3 examples
06/02/2022 11:57:01 - INFO - __main__ -  [ethos-religion] Disability centres all around the world. That’s were our money goes. Let’s take em down
06/02/2022 11:57:01 - INFO - __main__ - ['false']
06/02/2022 11:57:01 - INFO - __main__ -  [ethos-religion] FUCK GAYS disgusting
06/02/2022 11:57:01 - INFO - __main__ - ['false']
06/02/2022 11:57:01 - INFO - __main__ -  [ethos-religion] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
06/02/2022 11:57:01 - INFO - __main__ - ['false']
06/02/2022 11:57:01 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:57:01 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:57:01 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 11:57:01 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:57:01 - INFO - __main__ - Printing 3 examples
06/02/2022 11:57:01 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/02/2022 11:57:01 - INFO - __main__ - ['false']
06/02/2022 11:57:01 - INFO - __main__ -  [ethos-religion] All people are fucked up because of these queer thing because y'all realize how shitty gender roles are and how absurd is to see it in reverse genders roles
06/02/2022 11:57:01 - INFO - __main__ - ['false']
06/02/2022 11:57:01 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
06/02/2022 11:57:01 - INFO - __main__ - ['false']
06/02/2022 11:57:01 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:57:01 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:57:01 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:57:07 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 11:57:08 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 11:57:08 - INFO - __main__ - Printing 3 examples
06/02/2022 11:57:08 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 11:57:08 - INFO - __main__ - ['false']
06/02/2022 11:57:08 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 11:57:08 - INFO - __main__ - ['false']
06/02/2022 11:57:08 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 11:57:08 - INFO - __main__ - ['true']
06/02/2022 11:57:08 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:57:08 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:57:08 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 11:57:09 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_13_0.0003_8_predictions.txt
06/02/2022 11:57:09 - INFO - __main__ - Classification-F1 on test data: 0.4276
06/02/2022 11:57:10 - INFO - __main__ - prefix=ethos-religion_64_13, lr=0.0003, bsz=8, dev_performance=0.49606299212598426, test_performance=0.4276315789473684
06/02/2022 11:57:10 - INFO - __main__ - Running ... prefix=ethos-religion_64_13, lr=0.0002, bsz=8 ...
06/02/2022 11:57:10 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 11:57:10 - INFO - __main__ - Printing 3 examples
06/02/2022 11:57:10 - INFO - __main__ -  [ethos-religion] Disability centres all around the world. That’s were our money goes. Let’s take em down
06/02/2022 11:57:10 - INFO - __main__ - ['false']
06/02/2022 11:57:10 - INFO - __main__ -  [ethos-religion] FUCK GAYS disgusting
06/02/2022 11:57:10 - INFO - __main__ - ['false']
06/02/2022 11:57:10 - INFO - __main__ -  [ethos-religion] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
06/02/2022 11:57:10 - INFO - __main__ - ['false']
06/02/2022 11:57:10 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:57:10 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:57:11 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 11:57:11 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:57:11 - INFO - __main__ - Printing 3 examples
06/02/2022 11:57:11 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/02/2022 11:57:11 - INFO - __main__ - ['false']
06/02/2022 11:57:11 - INFO - __main__ -  [ethos-religion] All people are fucked up because of these queer thing because y'all realize how shitty gender roles are and how absurd is to see it in reverse genders roles
06/02/2022 11:57:11 - INFO - __main__ - ['false']
06/02/2022 11:57:11 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
06/02/2022 11:57:11 - INFO - __main__ - ['false']
06/02/2022 11:57:11 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:57:11 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:57:11 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:57:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 11:57:12 - INFO - __main__ - Starting training!
06/02/2022 11:57:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 11:57:22 - INFO - __main__ - Starting training!
06/02/2022 11:57:26 - INFO - __main__ - Step 10 Global step 10 Train loss 23.798307 on epoch=1
06/02/2022 11:57:31 - INFO - __main__ - Step 20 Global step 20 Train loss 19.822392 on epoch=2
06/02/2022 11:57:36 - INFO - __main__ - Step 30 Global step 30 Train loss 17.604424 on epoch=3
06/02/2022 11:57:40 - INFO - __main__ - Step 40 Global step 40 Train loss 17.619207 on epoch=4
06/02/2022 11:57:46 - INFO - __main__ - Step 50 Global step 50 Train loss 16.626200 on epoch=6
06/02/2022 11:57:52 - INFO - __main__ - Global step 50 Train loss 19.094107 Classification-F1 0.0 on epoch=6
06/02/2022 11:57:57 - INFO - __main__ - Step 60 Global step 60 Train loss 16.022757 on epoch=7
06/02/2022 11:58:02 - INFO - __main__ - Step 70 Global step 70 Train loss 15.302649 on epoch=8
06/02/2022 11:58:07 - INFO - __main__ - Step 80 Global step 80 Train loss 13.918432 on epoch=9
06/02/2022 11:58:12 - INFO - __main__ - Step 90 Global step 90 Train loss 13.789258 on epoch=11
06/02/2022 11:58:17 - INFO - __main__ - Step 100 Global step 100 Train loss 12.913040 on epoch=12
06/02/2022 11:58:21 - INFO - __main__ - Global step 100 Train loss 14.389226 Classification-F1 0.0 on epoch=12
06/02/2022 11:58:26 - INFO - __main__ - Step 110 Global step 110 Train loss 11.758304 on epoch=13
06/02/2022 11:58:31 - INFO - __main__ - Step 120 Global step 120 Train loss 10.268325 on epoch=14
06/02/2022 11:58:36 - INFO - __main__ - Step 130 Global step 130 Train loss 5.991844 on epoch=16
06/02/2022 11:58:41 - INFO - __main__ - Step 140 Global step 140 Train loss 2.850039 on epoch=17
06/02/2022 11:58:46 - INFO - __main__ - Step 150 Global step 150 Train loss 2.816218 on epoch=18
06/02/2022 11:58:47 - INFO - __main__ - Global step 150 Train loss 6.736946 Classification-F1 0.19259259259259257 on epoch=18
06/02/2022 11:58:53 - INFO - __main__ - Step 160 Global step 160 Train loss 1.718678 on epoch=19
06/02/2022 11:58:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.968623 on epoch=21
06/02/2022 11:59:03 - INFO - __main__ - Step 180 Global step 180 Train loss 1.091105 on epoch=22
06/02/2022 11:59:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.678667 on epoch=23
06/02/2022 11:59:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.639749 on epoch=24
06/02/2022 11:59:14 - INFO - __main__ - Global step 200 Train loss 1.019364 Classification-F1 0.4434782608695652 on epoch=24
06/02/2022 11:59:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.587008 on epoch=26
06/02/2022 11:59:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.526125 on epoch=27
06/02/2022 11:59:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.399253 on epoch=28
06/02/2022 11:59:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.378291 on epoch=29
06/02/2022 11:59:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.400987 on epoch=31
06/02/2022 11:59:40 - INFO - __main__ - Global step 250 Train loss 0.458333 Classification-F1 0.15789473684210525 on epoch=31
06/02/2022 11:59:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.386203 on epoch=32
06/02/2022 11:59:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.321809 on epoch=33
06/02/2022 11:59:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.340646 on epoch=34
06/02/2022 12:00:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.288881 on epoch=36
06/02/2022 12:00:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.165117 on epoch=37
06/02/2022 12:00:06 - INFO - __main__ - Global step 300 Train loss 0.300531 Classification-F1 0.4796747967479675 on epoch=37
06/02/2022 12:00:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.265729 on epoch=38
06/02/2022 12:00:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.147069 on epoch=39
06/02/2022 12:00:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.217354 on epoch=41
06/02/2022 12:00:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.176226 on epoch=42
06/02/2022 12:00:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.181967 on epoch=43
06/02/2022 12:00:33 - INFO - __main__ - Global step 350 Train loss 0.197669 Classification-F1 0.40186915887850466 on epoch=43
06/02/2022 12:00:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.192363 on epoch=44
06/02/2022 12:00:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.157485 on epoch=46
06/02/2022 12:00:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.156105 on epoch=47
06/02/2022 12:00:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.123218 on epoch=48
06/02/2022 12:00:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.114172 on epoch=49
06/02/2022 12:00:59 - INFO - __main__ - Global step 400 Train loss 0.148669 Classification-F1 0.40186915887850466 on epoch=49
06/02/2022 12:01:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.071127 on epoch=51
06/02/2022 12:01:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.104627 on epoch=52
06/02/2022 12:01:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.093131 on epoch=53
06/02/2022 12:01:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.123306 on epoch=54
06/02/2022 12:01:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.109896 on epoch=56
06/02/2022 12:01:25 - INFO - __main__ - Global step 450 Train loss 0.100418 Classification-F1 0.46218487394957986 on epoch=56
06/02/2022 12:01:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.088550 on epoch=57
06/02/2022 12:01:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.056836 on epoch=58
06/02/2022 12:01:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.061857 on epoch=59
06/02/2022 12:01:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.050838 on epoch=61
06/02/2022 12:01:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.081092 on epoch=62
06/02/2022 12:01:51 - INFO - __main__ - Global step 500 Train loss 0.067835 Classification-F1 0.47540983606557374 on epoch=62
06/02/2022 12:01:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.067714 on epoch=63
06/02/2022 12:02:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.046552 on epoch=64
06/02/2022 12:02:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.056371 on epoch=66
06/02/2022 12:02:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.055434 on epoch=67
06/02/2022 12:02:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.049917 on epoch=68
06/02/2022 12:02:16 - INFO - __main__ - Global step 550 Train loss 0.055198 Classification-F1 0.47107438016528924 on epoch=68
06/02/2022 12:02:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.054224 on epoch=69
06/02/2022 12:02:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.048541 on epoch=71
06/02/2022 12:02:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.084252 on epoch=72
06/02/2022 12:02:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.027955 on epoch=73
06/02/2022 12:02:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.023067 on epoch=74
06/02/2022 12:02:42 - INFO - __main__ - Global step 600 Train loss 0.047608 Classification-F1 0.46218487394957986 on epoch=74
06/02/2022 12:02:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.031564 on epoch=76
06/02/2022 12:02:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.013760 on epoch=77
06/02/2022 12:02:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.014892 on epoch=78
06/02/2022 12:03:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.011228 on epoch=79
06/02/2022 12:03:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.024571 on epoch=81
06/02/2022 12:03:08 - INFO - __main__ - Global step 650 Train loss 0.019203 Classification-F1 0.47540983606557374 on epoch=81
06/02/2022 12:03:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.013736 on epoch=82
06/02/2022 12:03:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.057242 on epoch=83
06/02/2022 12:03:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.010566 on epoch=84
06/02/2022 12:03:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.003939 on epoch=86
06/02/2022 12:03:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.036863 on epoch=87
06/02/2022 12:03:34 - INFO - __main__ - Global step 700 Train loss 0.024469 Classification-F1 0.47540983606557374 on epoch=87
06/02/2022 12:03:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.004429 on epoch=88
06/02/2022 12:03:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.033938 on epoch=89
06/02/2022 12:03:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.083784 on epoch=91
06/02/2022 12:03:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.017723 on epoch=92
06/02/2022 12:03:59 - INFO - __main__ - Step 750 Global step 750 Train loss 0.007184 on epoch=93
06/02/2022 12:04:00 - INFO - __main__ - Global step 750 Train loss 0.029412 Classification-F1 0.47107438016528924 on epoch=93
06/02/2022 12:04:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.004125 on epoch=94
06/02/2022 12:04:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.024117 on epoch=96
06/02/2022 12:04:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.001951 on epoch=97
06/02/2022 12:04:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.013528 on epoch=98
06/02/2022 12:04:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.029928 on epoch=99
06/02/2022 12:04:26 - INFO - __main__ - Global step 800 Train loss 0.014730 Classification-F1 0.4666666666666667 on epoch=99
06/02/2022 12:04:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.008053 on epoch=101
06/02/2022 12:04:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.031466 on epoch=102
06/02/2022 12:04:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.010058 on epoch=103
06/02/2022 12:04:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.014063 on epoch=104
06/02/2022 12:04:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.003047 on epoch=106
06/02/2022 12:04:51 - INFO - __main__ - Global step 850 Train loss 0.013337 Classification-F1 0.4666666666666667 on epoch=106
06/02/2022 12:04:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.002527 on epoch=107
06/02/2022 12:05:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.016256 on epoch=108
06/02/2022 12:05:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.005817 on epoch=109
06/02/2022 12:05:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000397 on epoch=111
06/02/2022 12:05:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.007007 on epoch=112
06/02/2022 12:05:17 - INFO - __main__ - Global step 900 Train loss 0.006401 Classification-F1 0.47540983606557374 on epoch=112
06/02/2022 12:05:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.001522 on epoch=113
06/02/2022 12:05:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000783 on epoch=114
06/02/2022 12:05:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000378 on epoch=116
06/02/2022 12:05:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.006754 on epoch=117
06/02/2022 12:05:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000971 on epoch=118
06/02/2022 12:05:43 - INFO - __main__ - Global step 950 Train loss 0.002082 Classification-F1 0.4796747967479675 on epoch=118
06/02/2022 12:05:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.003067 on epoch=119
06/02/2022 12:05:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000376 on epoch=121
06/02/2022 12:05:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.032973 on epoch=122
06/02/2022 12:06:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.007392 on epoch=123
06/02/2022 12:06:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.001911 on epoch=124
06/02/2022 12:06:09 - INFO - __main__ - Global step 1000 Train loss 0.009144 Classification-F1 0.47107438016528924 on epoch=124
06/02/2022 12:06:09 - INFO - __main__ - save last model!
06/02/2022 12:06:09 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 12:06:09 - INFO - __main__ - Printing 3 examples
06/02/2022 12:06:09 - INFO - __main__ -  [ethos-religion] Disability centres all around the world. That’s were our money goes. Let’s take em down
06/02/2022 12:06:09 - INFO - __main__ - ['false']
06/02/2022 12:06:09 - INFO - __main__ -  [ethos-religion] FUCK GAYS disgusting
06/02/2022 12:06:09 - INFO - __main__ - ['false']
06/02/2022 12:06:09 - INFO - __main__ -  [ethos-religion] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
06/02/2022 12:06:09 - INFO - __main__ - ['false']
06/02/2022 12:06:09 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:06:09 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:06:09 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 12:06:09 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:06:09 - INFO - __main__ - Printing 3 examples
06/02/2022 12:06:09 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/02/2022 12:06:09 - INFO - __main__ - ['false']
06/02/2022 12:06:09 - INFO - __main__ -  [ethos-religion] All people are fucked up because of these queer thing because y'all realize how shitty gender roles are and how absurd is to see it in reverse genders roles
06/02/2022 12:06:09 - INFO - __main__ - ['false']
06/02/2022 12:06:09 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
06/02/2022 12:06:09 - INFO - __main__ - ['false']
06/02/2022 12:06:09 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:06:09 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:06:10 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:06:16 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 12:06:17 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 12:06:17 - INFO - __main__ - Printing 3 examples
06/02/2022 12:06:17 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 12:06:17 - INFO - __main__ - ['false']
06/02/2022 12:06:17 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 12:06:17 - INFO - __main__ - ['false']
06/02/2022 12:06:17 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 12:06:17 - INFO - __main__ - ['true']
06/02/2022 12:06:17 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:06:17 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:06:17 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 12:06:18 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_13_0.0002_8_predictions.txt
06/02/2022 12:06:18 - INFO - __main__ - Classification-F1 on test data: 0.9351
06/02/2022 12:06:19 - INFO - __main__ - prefix=ethos-religion_64_13, lr=0.0002, bsz=8, dev_performance=0.4796747967479675, test_performance=0.9350746268656716
06/02/2022 12:06:19 - INFO - __main__ - Running ... prefix=ethos-religion_64_13, lr=0.0001, bsz=8 ...
06/02/2022 12:06:19 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 12:06:19 - INFO - __main__ - Printing 3 examples
06/02/2022 12:06:19 - INFO - __main__ -  [ethos-religion] Disability centres all around the world. That’s were our money goes. Let’s take em down
06/02/2022 12:06:19 - INFO - __main__ - ['false']
06/02/2022 12:06:19 - INFO - __main__ -  [ethos-religion] FUCK GAYS disgusting
06/02/2022 12:06:19 - INFO - __main__ - ['false']
06/02/2022 12:06:19 - INFO - __main__ -  [ethos-religion] transgenders need help...they are sick in the head. Civilization has messed up the world. Soon enough, people will claim to have the right to change to animals and it will be seen as human rights or right to choose
06/02/2022 12:06:19 - INFO - __main__ - ['false']
06/02/2022 12:06:19 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:06:20 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:06:20 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 12:06:20 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:06:20 - INFO - __main__ - Printing 3 examples
06/02/2022 12:06:20 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/02/2022 12:06:20 - INFO - __main__ - ['false']
06/02/2022 12:06:20 - INFO - __main__ -  [ethos-religion] All people are fucked up because of these queer thing because y'all realize how shitty gender roles are and how absurd is to see it in reverse genders roles
06/02/2022 12:06:20 - INFO - __main__ - ['false']
06/02/2022 12:06:20 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
06/02/2022 12:06:20 - INFO - __main__ - ['false']
06/02/2022 12:06:20 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:06:20 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:06:20 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:06:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 12:06:22 - INFO - __main__ - Starting training!
06/02/2022 12:06:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 12:06:30 - INFO - __main__ - Starting training!
06/02/2022 12:06:35 - INFO - __main__ - Step 10 Global step 10 Train loss 24.693186 on epoch=1
06/02/2022 12:06:40 - INFO - __main__ - Step 20 Global step 20 Train loss 21.857819 on epoch=2
06/02/2022 12:06:45 - INFO - __main__ - Step 30 Global step 30 Train loss 18.659008 on epoch=3
06/02/2022 12:06:50 - INFO - __main__ - Step 40 Global step 40 Train loss 17.918036 on epoch=4
06/02/2022 12:06:55 - INFO - __main__ - Step 50 Global step 50 Train loss 18.803474 on epoch=6
06/02/2022 12:07:15 - INFO - __main__ - Global step 50 Train loss 20.386305 Classification-F1 0.0 on epoch=6
06/02/2022 12:07:21 - INFO - __main__ - Step 60 Global step 60 Train loss 16.153709 on epoch=7
06/02/2022 12:07:26 - INFO - __main__ - Step 70 Global step 70 Train loss 16.446150 on epoch=8
06/02/2022 12:07:31 - INFO - __main__ - Step 80 Global step 80 Train loss 16.058302 on epoch=9
06/02/2022 12:07:36 - INFO - __main__ - Step 90 Global step 90 Train loss 16.138861 on epoch=11
06/02/2022 12:07:41 - INFO - __main__ - Step 100 Global step 100 Train loss 15.132322 on epoch=12
06/02/2022 12:07:52 - INFO - __main__ - Global step 100 Train loss 15.985868 Classification-F1 0.0 on epoch=12
06/02/2022 12:07:57 - INFO - __main__ - Step 110 Global step 110 Train loss 15.340891 on epoch=13
06/02/2022 12:08:02 - INFO - __main__ - Step 120 Global step 120 Train loss 15.336710 on epoch=14
06/02/2022 12:08:07 - INFO - __main__ - Step 130 Global step 130 Train loss 14.418956 on epoch=16
06/02/2022 12:08:12 - INFO - __main__ - Step 140 Global step 140 Train loss 14.089602 on epoch=17
06/02/2022 12:08:17 - INFO - __main__ - Step 150 Global step 150 Train loss 13.860112 on epoch=18
06/02/2022 12:08:29 - INFO - __main__ - Global step 150 Train loss 14.609253 Classification-F1 0.0 on epoch=18
06/02/2022 12:08:34 - INFO - __main__ - Step 160 Global step 160 Train loss 13.180316 on epoch=19
06/02/2022 12:08:39 - INFO - __main__ - Step 170 Global step 170 Train loss 12.719349 on epoch=21
06/02/2022 12:08:44 - INFO - __main__ - Step 180 Global step 180 Train loss 12.936045 on epoch=22
06/02/2022 12:08:50 - INFO - __main__ - Step 190 Global step 190 Train loss 11.496984 on epoch=23
06/02/2022 12:08:55 - INFO - __main__ - Step 200 Global step 200 Train loss 11.908401 on epoch=24
06/02/2022 12:09:08 - INFO - __main__ - Global step 200 Train loss 12.448219 Classification-F1 0.0 on epoch=24
06/02/2022 12:09:13 - INFO - __main__ - Step 210 Global step 210 Train loss 9.971143 on epoch=26
06/02/2022 12:09:18 - INFO - __main__ - Step 220 Global step 220 Train loss 9.270082 on epoch=27
06/02/2022 12:09:23 - INFO - __main__ - Step 230 Global step 230 Train loss 6.768637 on epoch=28
06/02/2022 12:09:28 - INFO - __main__ - Step 240 Global step 240 Train loss 5.690276 on epoch=29
06/02/2022 12:09:33 - INFO - __main__ - Step 250 Global step 250 Train loss 4.991786 on epoch=31
06/02/2022 12:09:34 - INFO - __main__ - Global step 250 Train loss 7.338385 Classification-F1 1.0 on epoch=31
06/02/2022 12:09:41 - INFO - __main__ - Step 260 Global step 260 Train loss 3.856092 on epoch=32
06/02/2022 12:09:46 - INFO - __main__ - Step 270 Global step 270 Train loss 3.955048 on epoch=33
06/02/2022 12:09:51 - INFO - __main__ - Step 280 Global step 280 Train loss 3.286394 on epoch=34
06/02/2022 12:09:56 - INFO - __main__ - Step 290 Global step 290 Train loss 3.118233 on epoch=36
06/02/2022 12:10:01 - INFO - __main__ - Step 300 Global step 300 Train loss 3.427364 on epoch=37
06/02/2022 12:10:02 - INFO - __main__ - Global step 300 Train loss 3.528626 Classification-F1 1.0 on epoch=37
06/02/2022 12:10:07 - INFO - __main__ - Step 310 Global step 310 Train loss 3.335290 on epoch=38
06/02/2022 12:10:13 - INFO - __main__ - Step 320 Global step 320 Train loss 2.690531 on epoch=39
06/02/2022 12:10:18 - INFO - __main__ - Step 330 Global step 330 Train loss 2.156339 on epoch=41
06/02/2022 12:10:23 - INFO - __main__ - Step 340 Global step 340 Train loss 1.771098 on epoch=42
06/02/2022 12:10:28 - INFO - __main__ - Step 350 Global step 350 Train loss 1.105482 on epoch=43
06/02/2022 12:10:29 - INFO - __main__ - Global step 350 Train loss 2.211748 Classification-F1 0.47107438016528924 on epoch=43
06/02/2022 12:10:34 - INFO - __main__ - Step 360 Global step 360 Train loss 1.253375 on epoch=44
06/02/2022 12:10:39 - INFO - __main__ - Step 370 Global step 370 Train loss 1.226224 on epoch=46
06/02/2022 12:10:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.487045 on epoch=47
06/02/2022 12:10:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.484946 on epoch=48
06/02/2022 12:10:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.507751 on epoch=49
06/02/2022 12:10:56 - INFO - __main__ - Global step 400 Train loss 0.791868 Classification-F1 0.4576271186440678 on epoch=49
06/02/2022 12:11:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.353695 on epoch=51
06/02/2022 12:11:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.350719 on epoch=52
06/02/2022 12:11:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.394488 on epoch=53
06/02/2022 12:11:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.324260 on epoch=54
06/02/2022 12:11:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.269365 on epoch=56
06/02/2022 12:11:22 - INFO - __main__ - Global step 450 Train loss 0.338505 Classification-F1 0.4434782608695652 on epoch=56
06/02/2022 12:11:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.322891 on epoch=57
06/02/2022 12:11:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.205443 on epoch=58
06/02/2022 12:11:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.228793 on epoch=59
06/02/2022 12:11:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.187900 on epoch=61
06/02/2022 12:11:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.188374 on epoch=62
06/02/2022 12:11:48 - INFO - __main__ - Global step 500 Train loss 0.226680 Classification-F1 0.488 on epoch=62
06/02/2022 12:11:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.224568 on epoch=63
06/02/2022 12:11:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.229140 on epoch=64
06/02/2022 12:12:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.150028 on epoch=66
06/02/2022 12:12:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.174392 on epoch=67
06/02/2022 12:12:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.168310 on epoch=68
06/02/2022 12:12:15 - INFO - __main__ - Global step 550 Train loss 0.189287 Classification-F1 0.4796747967479675 on epoch=68
06/02/2022 12:12:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.160594 on epoch=69
06/02/2022 12:12:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.133248 on epoch=71
06/02/2022 12:12:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.148237 on epoch=72
06/02/2022 12:12:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.170973 on epoch=73
06/02/2022 12:12:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.080892 on epoch=74
06/02/2022 12:12:41 - INFO - __main__ - Global step 600 Train loss 0.138789 Classification-F1 0.4796747967479675 on epoch=74
06/02/2022 12:12:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.118736 on epoch=76
06/02/2022 12:12:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.187436 on epoch=77
06/02/2022 12:12:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.080164 on epoch=78
06/02/2022 12:13:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.106363 on epoch=79
06/02/2022 12:13:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.072227 on epoch=81
06/02/2022 12:13:08 - INFO - __main__ - Global step 650 Train loss 0.112985 Classification-F1 0.4576271186440678 on epoch=81
06/02/2022 12:13:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.167667 on epoch=82
06/02/2022 12:13:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.107369 on epoch=83
06/02/2022 12:13:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.091146 on epoch=84
06/02/2022 12:13:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.102131 on epoch=86
06/02/2022 12:13:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.067567 on epoch=87
06/02/2022 12:13:34 - INFO - __main__ - Global step 700 Train loss 0.107176 Classification-F1 0.47540983606557374 on epoch=87
06/02/2022 12:13:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.111628 on epoch=88
06/02/2022 12:13:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.096924 on epoch=89
06/02/2022 12:13:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.091832 on epoch=91
06/02/2022 12:13:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.084820 on epoch=92
06/02/2022 12:13:59 - INFO - __main__ - Step 750 Global step 750 Train loss 0.056679 on epoch=93
06/02/2022 12:14:00 - INFO - __main__ - Global step 750 Train loss 0.088377 Classification-F1 0.4576271186440678 on epoch=93
06/02/2022 12:14:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.045757 on epoch=94
06/02/2022 12:14:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.044130 on epoch=96
06/02/2022 12:14:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.034955 on epoch=97
06/02/2022 12:14:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.064801 on epoch=98
06/02/2022 12:14:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.085694 on epoch=99
06/02/2022 12:14:27 - INFO - __main__ - Global step 800 Train loss 0.055067 Classification-F1 0.47540983606557374 on epoch=99
06/02/2022 12:14:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.034683 on epoch=101
06/02/2022 12:14:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.077570 on epoch=102
06/02/2022 12:14:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.045303 on epoch=103
06/02/2022 12:14:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.042166 on epoch=104
06/02/2022 12:14:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.033258 on epoch=106
06/02/2022 12:14:53 - INFO - __main__ - Global step 850 Train loss 0.046596 Classification-F1 0.47540983606557374 on epoch=106
06/02/2022 12:14:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.042013 on epoch=107
06/02/2022 12:15:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.037157 on epoch=108
06/02/2022 12:15:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.035366 on epoch=109
06/02/2022 12:15:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.017103 on epoch=111
06/02/2022 12:15:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.038020 on epoch=112
06/02/2022 12:15:20 - INFO - __main__ - Global step 900 Train loss 0.033932 Classification-F1 0.47540983606557374 on epoch=112
06/02/2022 12:15:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.044511 on epoch=113
06/02/2022 12:15:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.016726 on epoch=114
06/02/2022 12:15:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.023340 on epoch=116
06/02/2022 12:15:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.021444 on epoch=117
06/02/2022 12:15:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.053685 on epoch=118
06/02/2022 12:15:46 - INFO - __main__ - Global step 950 Train loss 0.031941 Classification-F1 0.4666666666666667 on epoch=118
06/02/2022 12:15:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.028884 on epoch=119
06/02/2022 12:15:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.012608 on epoch=121
06/02/2022 12:16:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.036373 on epoch=122
06/02/2022 12:16:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.016091 on epoch=123
06/02/2022 12:16:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.039695 on epoch=124
06/02/2022 12:16:12 - INFO - __main__ - Global step 1000 Train loss 0.026730 Classification-F1 0.47540983606557374 on epoch=124
06/02/2022 12:16:12 - INFO - __main__ - save last model!
06/02/2022 12:16:13 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 12:16:13 - INFO - __main__ - Printing 3 examples
06/02/2022 12:16:13 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/02/2022 12:16:13 - INFO - __main__ - ['false']
06/02/2022 12:16:13 - INFO - __main__ -  [ethos-religion] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
06/02/2022 12:16:13 - INFO - __main__ - ['false']
06/02/2022 12:16:13 - INFO - __main__ -  [ethos-religion] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
06/02/2022 12:16:13 - INFO - __main__ - ['false']
06/02/2022 12:16:13 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:16:13 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:16:13 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 12:16:13 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:16:13 - INFO - __main__ - Printing 3 examples
06/02/2022 12:16:13 - INFO - __main__ -  [ethos-religion] On todays episode of "Guess That Gender"
06/02/2022 12:16:13 - INFO - __main__ - ['false']
06/02/2022 12:16:13 - INFO - __main__ -  [ethos-religion] The Whites Have dislike the documentary because they are mad at  the truth. #WhiteDevils
06/02/2022 12:16:13 - INFO - __main__ - ['false']
06/02/2022 12:16:13 - INFO - __main__ -  [ethos-religion] Women are made for making babies and cooking dinner and nothing else!!!
06/02/2022 12:16:13 - INFO - __main__ - ['false']
06/02/2022 12:16:13 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:16:13 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:16:13 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:16:19 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 12:16:20 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 12:16:20 - INFO - __main__ - Printing 3 examples
06/02/2022 12:16:20 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 12:16:20 - INFO - __main__ - ['false']
06/02/2022 12:16:20 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 12:16:20 - INFO - __main__ - ['false']
06/02/2022 12:16:20 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 12:16:20 - INFO - __main__ - ['true']
06/02/2022 12:16:20 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:16:20 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:16:20 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 12:16:22 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_13_0.0001_8_predictions.txt
06/02/2022 12:16:22 - INFO - __main__ - Classification-F1 on test data: 0.4387
06/02/2022 12:16:22 - INFO - __main__ - prefix=ethos-religion_64_13, lr=0.0001, bsz=8, dev_performance=1.0, test_performance=0.4387096774193549
06/02/2022 12:16:22 - INFO - __main__ - Running ... prefix=ethos-religion_64_21, lr=0.0005, bsz=8 ...
06/02/2022 12:16:23 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 12:16:23 - INFO - __main__ - Printing 3 examples
06/02/2022 12:16:23 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/02/2022 12:16:23 - INFO - __main__ - ['false']
06/02/2022 12:16:23 - INFO - __main__ -  [ethos-religion] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
06/02/2022 12:16:23 - INFO - __main__ - ['false']
06/02/2022 12:16:23 - INFO - __main__ -  [ethos-religion] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
06/02/2022 12:16:23 - INFO - __main__ - ['false']
06/02/2022 12:16:23 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:16:23 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:16:23 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 12:16:23 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:16:23 - INFO - __main__ - Printing 3 examples
06/02/2022 12:16:23 - INFO - __main__ -  [ethos-religion] On todays episode of "Guess That Gender"
06/02/2022 12:16:23 - INFO - __main__ - ['false']
06/02/2022 12:16:23 - INFO - __main__ -  [ethos-religion] The Whites Have dislike the documentary because they are mad at  the truth. #WhiteDevils
06/02/2022 12:16:23 - INFO - __main__ - ['false']
06/02/2022 12:16:23 - INFO - __main__ -  [ethos-religion] Women are made for making babies and cooking dinner and nothing else!!!
06/02/2022 12:16:23 - INFO - __main__ - ['false']
06/02/2022 12:16:23 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:16:24 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:16:24 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:16:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 12:16:24 - INFO - __main__ - Starting training!
06/02/2022 12:16:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 12:16:36 - INFO - __main__ - Starting training!
06/02/2022 12:16:41 - INFO - __main__ - Step 10 Global step 10 Train loss 23.728495 on epoch=1
06/02/2022 12:16:46 - INFO - __main__ - Step 20 Global step 20 Train loss 19.326826 on epoch=2
06/02/2022 12:16:51 - INFO - __main__ - Step 30 Global step 30 Train loss 16.545219 on epoch=3
06/02/2022 12:16:56 - INFO - __main__ - Step 40 Global step 40 Train loss 14.721234 on epoch=4
06/02/2022 12:17:01 - INFO - __main__ - Step 50 Global step 50 Train loss 12.847084 on epoch=6
06/02/2022 12:17:11 - INFO - __main__ - Global step 50 Train loss 17.433771 Classification-F1 0.0009049773755656109 on epoch=6
06/02/2022 12:17:17 - INFO - __main__ - Step 60 Global step 60 Train loss 7.610159 on epoch=7
06/02/2022 12:17:22 - INFO - __main__ - Step 70 Global step 70 Train loss 1.169451 on epoch=8
06/02/2022 12:17:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.718420 on epoch=9
06/02/2022 12:17:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.548805 on epoch=11
06/02/2022 12:17:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.496977 on epoch=12
06/02/2022 12:17:38 - INFO - __main__ - Global step 100 Train loss 2.108762 Classification-F1 0.04477611940298507 on epoch=12
06/02/2022 12:17:45 - INFO - __main__ - Step 110 Global step 110 Train loss 0.473705 on epoch=13
06/02/2022 12:17:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.532840 on epoch=14
06/02/2022 12:17:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.669982 on epoch=16
06/02/2022 12:18:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.491586 on epoch=17
06/02/2022 12:18:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.494952 on epoch=18
06/02/2022 12:18:06 - INFO - __main__ - Global step 150 Train loss 0.532613 Classification-F1 1.0 on epoch=18
06/02/2022 12:18:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.438298 on epoch=19
06/02/2022 12:18:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.384403 on epoch=21
06/02/2022 12:18:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.489802 on epoch=22
06/02/2022 12:18:28 - INFO - __main__ - Step 190 Global step 190 Train loss 0.489674 on epoch=23
06/02/2022 12:18:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.470198 on epoch=24
06/02/2022 12:18:33 - INFO - __main__ - Global step 200 Train loss 0.454475 Classification-F1 0.47107438016528924 on epoch=24
06/02/2022 12:18:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.414903 on epoch=26
06/02/2022 12:18:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.481799 on epoch=27
06/02/2022 12:18:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.349444 on epoch=28
06/02/2022 12:18:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.396660 on epoch=29
06/02/2022 12:18:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.433717 on epoch=31
06/02/2022 12:19:00 - INFO - __main__ - Global step 250 Train loss 0.415305 Classification-F1 0.13513513513513514 on epoch=31
06/02/2022 12:19:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.479894 on epoch=32
06/02/2022 12:19:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.383352 on epoch=33
06/02/2022 12:19:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.395494 on epoch=34
06/02/2022 12:19:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.420865 on epoch=36
06/02/2022 12:19:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.404331 on epoch=37
06/02/2022 12:19:26 - INFO - __main__ - Global step 300 Train loss 0.416787 Classification-F1 0.1111111111111111 on epoch=37
06/02/2022 12:19:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.378381 on epoch=38
06/02/2022 12:19:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.387942 on epoch=39
06/02/2022 12:19:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.380937 on epoch=41
06/02/2022 12:19:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.397327 on epoch=42
06/02/2022 12:19:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.387994 on epoch=43
06/02/2022 12:19:53 - INFO - __main__ - Global step 350 Train loss 0.386516 Classification-F1 1.0 on epoch=43
06/02/2022 12:19:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.361435 on epoch=44
06/02/2022 12:20:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.447936 on epoch=46
06/02/2022 12:20:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.397375 on epoch=47
06/02/2022 12:20:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.339582 on epoch=48
06/02/2022 12:20:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.362663 on epoch=49
06/02/2022 12:20:19 - INFO - __main__ - Global step 400 Train loss 0.381798 Classification-F1 0.21951219512195122 on epoch=49
06/02/2022 12:20:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.379403 on epoch=51
06/02/2022 12:20:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.381324 on epoch=52
06/02/2022 12:20:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.355395 on epoch=53
06/02/2022 12:20:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.273028 on epoch=54
06/02/2022 12:20:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.342679 on epoch=56
06/02/2022 12:20:46 - INFO - __main__ - Global step 450 Train loss 0.346366 Classification-F1 0.14666666666666667 on epoch=56
06/02/2022 12:20:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.404528 on epoch=57
06/02/2022 12:20:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.341435 on epoch=58
06/02/2022 12:21:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.266888 on epoch=59
06/02/2022 12:21:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.351882 on epoch=61
06/02/2022 12:21:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.287686 on epoch=62
06/02/2022 12:21:12 - INFO - __main__ - Global step 500 Train loss 0.330484 Classification-F1 0.20987654320987653 on epoch=62
06/02/2022 12:21:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.246027 on epoch=63
06/02/2022 12:21:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.228575 on epoch=64
06/02/2022 12:21:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.177461 on epoch=66
06/02/2022 12:21:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.184822 on epoch=67
06/02/2022 12:21:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.186305 on epoch=68
06/02/2022 12:21:39 - INFO - __main__ - Global step 550 Train loss 0.204638 Classification-F1 0.4838709677419355 on epoch=68
06/02/2022 12:21:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.164645 on epoch=69
06/02/2022 12:21:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.157966 on epoch=71
06/02/2022 12:21:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.156043 on epoch=72
06/02/2022 12:22:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.193753 on epoch=73
06/02/2022 12:22:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.101647 on epoch=74
06/02/2022 12:22:06 - INFO - __main__ - Global step 600 Train loss 0.154811 Classification-F1 0.452991452991453 on epoch=74
06/02/2022 12:22:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.128214 on epoch=76
06/02/2022 12:22:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.030448 on epoch=77
06/02/2022 12:22:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.132795 on epoch=78
06/02/2022 12:22:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.031244 on epoch=79
06/02/2022 12:22:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.110393 on epoch=81
06/02/2022 12:22:32 - INFO - __main__ - Global step 650 Train loss 0.086619 Classification-F1 0.39622641509433965 on epoch=81
06/02/2022 12:22:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.086937 on epoch=82
06/02/2022 12:22:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.058537 on epoch=83
06/02/2022 12:22:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.072562 on epoch=84
06/02/2022 12:22:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.079809 on epoch=86
06/02/2022 12:22:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.098718 on epoch=87
06/02/2022 12:22:59 - INFO - __main__ - Global step 700 Train loss 0.079313 Classification-F1 0.4796747967479675 on epoch=87
06/02/2022 12:23:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.040976 on epoch=88
06/02/2022 12:23:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.016605 on epoch=89
06/02/2022 12:23:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.112712 on epoch=91
06/02/2022 12:23:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.053883 on epoch=92
06/02/2022 12:23:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.080038 on epoch=93
06/02/2022 12:23:25 - INFO - __main__ - Global step 750 Train loss 0.060843 Classification-F1 0.43859649122807015 on epoch=93
06/02/2022 12:23:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.125359 on epoch=94
06/02/2022 12:23:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.044422 on epoch=96
06/02/2022 12:23:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.075707 on epoch=97
06/02/2022 12:23:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.055374 on epoch=98
06/02/2022 12:23:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.171776 on epoch=99
06/02/2022 12:23:52 - INFO - __main__ - Global step 800 Train loss 0.094527 Classification-F1 0.38461538461538464 on epoch=99
06/02/2022 12:23:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.112112 on epoch=101
06/02/2022 12:24:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.045792 on epoch=102
06/02/2022 12:24:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.093016 on epoch=103
06/02/2022 12:24:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.167618 on epoch=104
06/02/2022 12:24:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.929244 on epoch=106
06/02/2022 12:24:18 - INFO - __main__ - Global step 850 Train loss 0.269556 Classification-F1 0.14666666666666667 on epoch=106
06/02/2022 12:24:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.186846 on epoch=107
06/02/2022 12:24:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.214380 on epoch=108
06/02/2022 12:24:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.039700 on epoch=109
06/02/2022 12:24:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.077802 on epoch=111
06/02/2022 12:24:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.061495 on epoch=112
06/02/2022 12:24:45 - INFO - __main__ - Global step 900 Train loss 0.116045 Classification-F1 0.4796747967479675 on epoch=112
06/02/2022 12:24:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.115773 on epoch=113
06/02/2022 12:24:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.017072 on epoch=114
06/02/2022 12:25:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.040989 on epoch=116
06/02/2022 12:25:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.013977 on epoch=117
06/02/2022 12:25:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.009819 on epoch=118
06/02/2022 12:25:12 - INFO - __main__ - Global step 950 Train loss 0.039526 Classification-F1 0.46218487394957986 on epoch=118
06/02/2022 12:25:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.017780 on epoch=119
06/02/2022 12:25:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.016808 on epoch=121
06/02/2022 12:25:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.009692 on epoch=122
06/02/2022 12:25:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.016465 on epoch=123
06/02/2022 12:25:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.005789 on epoch=124
06/02/2022 12:25:38 - INFO - __main__ - Global step 1000 Train loss 0.013307 Classification-F1 0.452991452991453 on epoch=124
06/02/2022 12:25:38 - INFO - __main__ - save last model!
06/02/2022 12:25:39 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 12:25:39 - INFO - __main__ - Printing 3 examples
06/02/2022 12:25:39 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/02/2022 12:25:39 - INFO - __main__ - ['false']
06/02/2022 12:25:39 - INFO - __main__ -  [ethos-religion] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
06/02/2022 12:25:39 - INFO - __main__ - ['false']
06/02/2022 12:25:39 - INFO - __main__ -  [ethos-religion] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
06/02/2022 12:25:39 - INFO - __main__ - ['false']
06/02/2022 12:25:39 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:25:39 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:25:39 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 12:25:39 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:25:39 - INFO - __main__ - Printing 3 examples
06/02/2022 12:25:39 - INFO - __main__ -  [ethos-religion] On todays episode of "Guess That Gender"
06/02/2022 12:25:39 - INFO - __main__ - ['false']
06/02/2022 12:25:39 - INFO - __main__ -  [ethos-religion] The Whites Have dislike the documentary because they are mad at  the truth. #WhiteDevils
06/02/2022 12:25:39 - INFO - __main__ - ['false']
06/02/2022 12:25:39 - INFO - __main__ -  [ethos-religion] Women are made for making babies and cooking dinner and nothing else!!!
06/02/2022 12:25:39 - INFO - __main__ - ['false']
06/02/2022 12:25:39 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:25:39 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:25:39 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:25:46 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 12:25:47 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 12:25:47 - INFO - __main__ - Printing 3 examples
06/02/2022 12:25:47 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 12:25:47 - INFO - __main__ - ['false']
06/02/2022 12:25:47 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 12:25:47 - INFO - __main__ - ['false']
06/02/2022 12:25:47 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 12:25:47 - INFO - __main__ - ['true']
06/02/2022 12:25:47 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:25:47 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:25:47 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 12:25:48 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_21_0.0005_8_predictions.txt
06/02/2022 12:25:48 - INFO - __main__ - Classification-F1 on test data: 0.4387
06/02/2022 12:25:49 - INFO - __main__ - prefix=ethos-religion_64_21, lr=0.0005, bsz=8, dev_performance=1.0, test_performance=0.4387096774193549
06/02/2022 12:25:49 - INFO - __main__ - Running ... prefix=ethos-religion_64_21, lr=0.0003, bsz=8 ...
06/02/2022 12:25:49 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 12:25:49 - INFO - __main__ - Printing 3 examples
06/02/2022 12:25:49 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/02/2022 12:25:49 - INFO - __main__ - ['false']
06/02/2022 12:25:49 - INFO - __main__ -  [ethos-religion] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
06/02/2022 12:25:49 - INFO - __main__ - ['false']
06/02/2022 12:25:49 - INFO - __main__ -  [ethos-religion] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
06/02/2022 12:25:49 - INFO - __main__ - ['false']
06/02/2022 12:25:49 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:25:50 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:25:50 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 12:25:50 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:25:50 - INFO - __main__ - Printing 3 examples
06/02/2022 12:25:50 - INFO - __main__ -  [ethos-religion] On todays episode of "Guess That Gender"
06/02/2022 12:25:50 - INFO - __main__ - ['false']
06/02/2022 12:25:50 - INFO - __main__ -  [ethos-religion] The Whites Have dislike the documentary because they are mad at  the truth. #WhiteDevils
06/02/2022 12:25:50 - INFO - __main__ - ['false']
06/02/2022 12:25:50 - INFO - __main__ -  [ethos-religion] Women are made for making babies and cooking dinner and nothing else!!!
06/02/2022 12:25:50 - INFO - __main__ - ['false']
06/02/2022 12:25:50 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:25:50 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:25:50 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:25:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 12:25:50 - INFO - __main__ - Starting training!
06/02/2022 12:26:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 12:26:01 - INFO - __main__ - Starting training!
06/02/2022 12:26:05 - INFO - __main__ - Step 10 Global step 10 Train loss 24.943155 on epoch=1
06/02/2022 12:26:10 - INFO - __main__ - Step 20 Global step 20 Train loss 19.087713 on epoch=2
06/02/2022 12:26:16 - INFO - __main__ - Step 30 Global step 30 Train loss 18.383282 on epoch=3
06/02/2022 12:26:21 - INFO - __main__ - Step 40 Global step 40 Train loss 16.544577 on epoch=4
06/02/2022 12:26:26 - INFO - __main__ - Step 50 Global step 50 Train loss 14.377771 on epoch=6
06/02/2022 12:26:41 - INFO - __main__ - Global step 50 Train loss 18.667299 Classification-F1 0.0 on epoch=6
06/02/2022 12:26:47 - INFO - __main__ - Step 60 Global step 60 Train loss 13.957906 on epoch=7
06/02/2022 12:26:53 - INFO - __main__ - Step 70 Global step 70 Train loss 12.953783 on epoch=8
06/02/2022 12:26:58 - INFO - __main__ - Step 80 Global step 80 Train loss 11.605207 on epoch=9
06/02/2022 12:27:03 - INFO - __main__ - Step 90 Global step 90 Train loss 8.542623 on epoch=11
06/02/2022 12:27:09 - INFO - __main__ - Step 100 Global step 100 Train loss 4.692665 on epoch=12
06/02/2022 12:27:11 - INFO - __main__ - Global step 100 Train loss 10.350436 Classification-F1 1.0 on epoch=12
06/02/2022 12:27:17 - INFO - __main__ - Step 110 Global step 110 Train loss 3.491767 on epoch=13
06/02/2022 12:27:22 - INFO - __main__ - Step 120 Global step 120 Train loss 3.056495 on epoch=14
06/02/2022 12:27:27 - INFO - __main__ - Step 130 Global step 130 Train loss 3.606424 on epoch=16
06/02/2022 12:27:33 - INFO - __main__ - Step 140 Global step 140 Train loss 1.207062 on epoch=17
06/02/2022 12:27:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.719292 on epoch=18
06/02/2022 12:27:39 - INFO - __main__ - Global step 150 Train loss 2.416208 Classification-F1 1.0 on epoch=18
06/02/2022 12:27:44 - INFO - __main__ - Step 160 Global step 160 Train loss 0.605611 on epoch=19
06/02/2022 12:27:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.333579 on epoch=21
06/02/2022 12:27:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.354837 on epoch=22
06/02/2022 12:28:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.332059 on epoch=23
06/02/2022 12:28:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.239431 on epoch=24
06/02/2022 12:28:06 - INFO - __main__ - Global step 200 Train loss 0.373103 Classification-F1 0.4666666666666667 on epoch=24
06/02/2022 12:28:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.212085 on epoch=26
06/02/2022 12:28:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.205949 on epoch=27
06/02/2022 12:28:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.312094 on epoch=28
06/02/2022 12:28:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.593372 on epoch=29
06/02/2022 12:28:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.338581 on epoch=31
06/02/2022 12:28:33 - INFO - __main__ - Global step 250 Train loss 0.332416 Classification-F1 0.13513513513513514 on epoch=31
06/02/2022 12:28:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.303292 on epoch=32
06/02/2022 12:28:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.284811 on epoch=33
06/02/2022 12:28:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.311995 on epoch=34
06/02/2022 12:28:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.221569 on epoch=36
06/02/2022 12:29:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.152835 on epoch=37
06/02/2022 12:29:00 - INFO - __main__ - Global step 300 Train loss 0.254901 Classification-F1 0.452991452991453 on epoch=37
06/02/2022 12:29:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.290385 on epoch=38
06/02/2022 12:29:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.422345 on epoch=39
06/02/2022 12:29:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.327916 on epoch=41
06/02/2022 12:29:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.468068 on epoch=42
06/02/2022 12:29:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.365858 on epoch=43
06/02/2022 12:29:27 - INFO - __main__ - Global step 350 Train loss 0.374915 Classification-F1 0.452991452991453 on epoch=43
06/02/2022 12:29:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.452643 on epoch=44
06/02/2022 12:29:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.304013 on epoch=46
06/02/2022 12:29:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.155838 on epoch=47
06/02/2022 12:29:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.131431 on epoch=48
06/02/2022 12:29:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.107101 on epoch=49
06/02/2022 12:29:55 - INFO - __main__ - Global step 400 Train loss 0.230205 Classification-F1 0.46218487394957986 on epoch=49
06/02/2022 12:30:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.132766 on epoch=51
06/02/2022 12:30:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.070755 on epoch=52
06/02/2022 12:30:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.130148 on epoch=53
06/02/2022 12:30:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.064830 on epoch=54
06/02/2022 12:30:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.087506 on epoch=56
06/02/2022 12:30:22 - INFO - __main__ - Global step 450 Train loss 0.097201 Classification-F1 0.43859649122807015 on epoch=56
06/02/2022 12:30:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.110455 on epoch=57
06/02/2022 12:30:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.119232 on epoch=58
06/02/2022 12:30:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.099692 on epoch=59
06/02/2022 12:30:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.054008 on epoch=61
06/02/2022 12:30:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.070455 on epoch=62
06/02/2022 12:30:49 - INFO - __main__ - Global step 500 Train loss 0.090768 Classification-F1 0.4796747967479675 on epoch=62
06/02/2022 12:30:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.099599 on epoch=63
06/02/2022 12:31:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.025739 on epoch=64
06/02/2022 12:31:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.017219 on epoch=66
06/02/2022 12:31:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.017030 on epoch=67
06/02/2022 12:31:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.059608 on epoch=68
06/02/2022 12:31:16 - INFO - __main__ - Global step 550 Train loss 0.043839 Classification-F1 0.452991452991453 on epoch=68
06/02/2022 12:31:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.033686 on epoch=69
06/02/2022 12:31:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.114137 on epoch=71
06/02/2022 12:31:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.031836 on epoch=72
06/02/2022 12:31:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.032971 on epoch=73
06/02/2022 12:31:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.028086 on epoch=74
06/02/2022 12:31:44 - INFO - __main__ - Global step 600 Train loss 0.048143 Classification-F1 0.46218487394957986 on epoch=74
06/02/2022 12:31:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.038385 on epoch=76
06/02/2022 12:31:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.021860 on epoch=77
06/02/2022 12:31:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.042112 on epoch=78
06/02/2022 12:32:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.006884 on epoch=79
06/02/2022 12:32:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.018164 on epoch=81
06/02/2022 12:32:11 - INFO - __main__ - Global step 650 Train loss 0.025481 Classification-F1 0.452991452991453 on epoch=81
06/02/2022 12:32:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.005939 on epoch=82
06/02/2022 12:32:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.002581 on epoch=83
06/02/2022 12:32:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.026626 on epoch=84
06/02/2022 12:32:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.002328 on epoch=86
06/02/2022 12:32:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.042800 on epoch=87
06/02/2022 12:32:38 - INFO - __main__ - Global step 700 Train loss 0.016055 Classification-F1 0.47107438016528924 on epoch=87
06/02/2022 12:32:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.007385 on epoch=88
06/02/2022 12:32:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.005859 on epoch=89
06/02/2022 12:32:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.014454 on epoch=91
06/02/2022 12:33:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.014394 on epoch=92
06/02/2022 12:33:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001073 on epoch=93
06/02/2022 12:33:06 - INFO - __main__ - Global step 750 Train loss 0.008633 Classification-F1 0.47540983606557374 on epoch=93
06/02/2022 12:33:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.007737 on epoch=94
06/02/2022 12:33:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.007742 on epoch=96
06/02/2022 12:33:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.017814 on epoch=97
06/02/2022 12:33:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.010884 on epoch=98
06/02/2022 12:33:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000208 on epoch=99
06/02/2022 12:33:32 - INFO - __main__ - Global step 800 Train loss 0.008877 Classification-F1 0.47540983606557374 on epoch=99
06/02/2022 12:33:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.039043 on epoch=101
06/02/2022 12:33:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.003675 on epoch=102
06/02/2022 12:33:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.046370 on epoch=103
06/02/2022 12:33:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.026362 on epoch=104
06/02/2022 12:33:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.039719 on epoch=106
06/02/2022 12:33:59 - INFO - __main__ - Global step 850 Train loss 0.031034 Classification-F1 0.47540983606557374 on epoch=106
06/02/2022 12:34:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.036415 on epoch=107
06/02/2022 12:34:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.003647 on epoch=108
06/02/2022 12:34:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000829 on epoch=109
06/02/2022 12:34:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000368 on epoch=111
06/02/2022 12:34:25 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000609 on epoch=112
06/02/2022 12:34:26 - INFO - __main__ - Global step 900 Train loss 0.008374 Classification-F1 0.4838709677419355 on epoch=112
06/02/2022 12:34:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000724 on epoch=113
06/02/2022 12:34:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000314 on epoch=114
06/02/2022 12:34:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.001538 on epoch=116
06/02/2022 12:34:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.001359 on epoch=117
06/02/2022 12:34:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000200 on epoch=118
06/02/2022 12:34:52 - INFO - __main__ - Global step 950 Train loss 0.000827 Classification-F1 0.4838709677419355 on epoch=118
06/02/2022 12:34:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000105 on epoch=119
06/02/2022 12:35:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000385 on epoch=121
06/02/2022 12:35:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.002005 on epoch=122
06/02/2022 12:35:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.001060 on epoch=123
06/02/2022 12:35:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000429 on epoch=124
06/02/2022 12:35:19 - INFO - __main__ - Global step 1000 Train loss 0.000797 Classification-F1 0.4838709677419355 on epoch=124
06/02/2022 12:35:19 - INFO - __main__ - save last model!
06/02/2022 12:35:19 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 12:35:19 - INFO - __main__ - Printing 3 examples
06/02/2022 12:35:19 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/02/2022 12:35:19 - INFO - __main__ - ['false']
06/02/2022 12:35:19 - INFO - __main__ -  [ethos-religion] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
06/02/2022 12:35:19 - INFO - __main__ - ['false']
06/02/2022 12:35:19 - INFO - __main__ -  [ethos-religion] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
06/02/2022 12:35:19 - INFO - __main__ - ['false']
06/02/2022 12:35:19 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:35:19 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:35:20 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 12:35:20 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:35:20 - INFO - __main__ - Printing 3 examples
06/02/2022 12:35:20 - INFO - __main__ -  [ethos-religion] On todays episode of "Guess That Gender"
06/02/2022 12:35:20 - INFO - __main__ - ['false']
06/02/2022 12:35:20 - INFO - __main__ -  [ethos-religion] The Whites Have dislike the documentary because they are mad at  the truth. #WhiteDevils
06/02/2022 12:35:20 - INFO - __main__ - ['false']
06/02/2022 12:35:20 - INFO - __main__ -  [ethos-religion] Women are made for making babies and cooking dinner and nothing else!!!
06/02/2022 12:35:20 - INFO - __main__ - ['false']
06/02/2022 12:35:20 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:35:20 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:35:20 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:35:26 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 12:35:27 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 12:35:27 - INFO - __main__ - Printing 3 examples
06/02/2022 12:35:27 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 12:35:27 - INFO - __main__ - ['false']
06/02/2022 12:35:27 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 12:35:27 - INFO - __main__ - ['false']
06/02/2022 12:35:27 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 12:35:27 - INFO - __main__ - ['true']
06/02/2022 12:35:27 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:35:27 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:35:27 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 12:35:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 12:35:31 - INFO - __main__ - Starting training!
06/02/2022 12:35:32 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_21_0.0003_8_predictions.txt
06/02/2022 12:35:32 - INFO - __main__ - Classification-F1 on test data: 0.4387
06/02/2022 12:35:33 - INFO - __main__ - prefix=ethos-religion_64_21, lr=0.0003, bsz=8, dev_performance=1.0, test_performance=0.4387096774193549
06/02/2022 12:35:33 - INFO - __main__ - Running ... prefix=ethos-religion_64_21, lr=0.0002, bsz=8 ...
06/02/2022 12:35:34 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 12:35:34 - INFO - __main__ - Printing 3 examples
06/02/2022 12:35:34 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/02/2022 12:35:34 - INFO - __main__ - ['false']
06/02/2022 12:35:34 - INFO - __main__ -  [ethos-religion] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
06/02/2022 12:35:34 - INFO - __main__ - ['false']
06/02/2022 12:35:34 - INFO - __main__ -  [ethos-religion] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
06/02/2022 12:35:34 - INFO - __main__ - ['false']
06/02/2022 12:35:34 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:35:34 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:35:34 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 12:35:34 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:35:34 - INFO - __main__ - Printing 3 examples
06/02/2022 12:35:34 - INFO - __main__ -  [ethos-religion] On todays episode of "Guess That Gender"
06/02/2022 12:35:34 - INFO - __main__ - ['false']
06/02/2022 12:35:34 - INFO - __main__ -  [ethos-religion] The Whites Have dislike the documentary because they are mad at  the truth. #WhiteDevils
06/02/2022 12:35:34 - INFO - __main__ - ['false']
06/02/2022 12:35:34 - INFO - __main__ -  [ethos-religion] Women are made for making babies and cooking dinner and nothing else!!!
06/02/2022 12:35:34 - INFO - __main__ - ['false']
06/02/2022 12:35:34 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:35:34 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:35:34 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:35:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 12:35:45 - INFO - __main__ - Starting training!
06/02/2022 12:35:50 - INFO - __main__ - Step 10 Global step 10 Train loss 23.919189 on epoch=1
06/02/2022 12:35:54 - INFO - __main__ - Step 20 Global step 20 Train loss 20.056871 on epoch=2
06/02/2022 12:36:00 - INFO - __main__ - Step 30 Global step 30 Train loss 18.551426 on epoch=3
06/02/2022 12:36:05 - INFO - __main__ - Step 40 Global step 40 Train loss 17.475800 on epoch=4
06/02/2022 12:36:10 - INFO - __main__ - Step 50 Global step 50 Train loss 15.968771 on epoch=6
06/02/2022 12:36:29 - INFO - __main__ - Global step 50 Train loss 19.194410 Classification-F1 0.0 on epoch=6
06/02/2022 12:36:35 - INFO - __main__ - Step 60 Global step 60 Train loss 14.838402 on epoch=7
06/02/2022 12:36:40 - INFO - __main__ - Step 70 Global step 70 Train loss 14.177748 on epoch=8
06/02/2022 12:36:45 - INFO - __main__ - Step 80 Global step 80 Train loss 13.629595 on epoch=9
06/02/2022 12:36:50 - INFO - __main__ - Step 90 Global step 90 Train loss 13.510844 on epoch=11
06/02/2022 12:36:55 - INFO - __main__ - Step 100 Global step 100 Train loss 12.292218 on epoch=12
06/02/2022 12:37:11 - INFO - __main__ - Global step 100 Train loss 13.689761 Classification-F1 0.0 on epoch=12
06/02/2022 12:37:16 - INFO - __main__ - Step 110 Global step 110 Train loss 10.422752 on epoch=13
06/02/2022 12:37:22 - INFO - __main__ - Step 120 Global step 120 Train loss 8.964899 on epoch=14
06/02/2022 12:37:27 - INFO - __main__ - Step 130 Global step 130 Train loss 4.394125 on epoch=16
06/02/2022 12:37:32 - INFO - __main__ - Step 140 Global step 140 Train loss 1.226431 on epoch=17
06/02/2022 12:37:37 - INFO - __main__ - Step 150 Global step 150 Train loss 0.477879 on epoch=18
06/02/2022 12:37:38 - INFO - __main__ - Global step 150 Train loss 5.097218 Classification-F1 1.0 on epoch=18
06/02/2022 12:37:44 - INFO - __main__ - Step 160 Global step 160 Train loss 0.954753 on epoch=19
06/02/2022 12:37:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.480084 on epoch=21
06/02/2022 12:37:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.465223 on epoch=22
06/02/2022 12:37:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.376001 on epoch=23
06/02/2022 12:38:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.420136 on epoch=24
06/02/2022 12:38:05 - INFO - __main__ - Global step 200 Train loss 0.539239 Classification-F1 0.452991452991453 on epoch=24
06/02/2022 12:38:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.332603 on epoch=26
06/02/2022 12:38:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.292193 on epoch=27
06/02/2022 12:38:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.282224 on epoch=28
06/02/2022 12:38:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.252848 on epoch=29
06/02/2022 12:38:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.245130 on epoch=31
06/02/2022 12:38:31 - INFO - __main__ - Global step 250 Train loss 0.281000 Classification-F1 0.3402061855670103 on epoch=31
06/02/2022 12:38:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.225678 on epoch=32
06/02/2022 12:38:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.255128 on epoch=33
06/02/2022 12:38:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.235857 on epoch=34
06/02/2022 12:38:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.199314 on epoch=36
06/02/2022 12:38:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.161953 on epoch=37
06/02/2022 12:38:58 - INFO - __main__ - Global step 300 Train loss 0.215586 Classification-F1 0.3191489361702128 on epoch=37
06/02/2022 12:39:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.199561 on epoch=38
06/02/2022 12:39:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.140751 on epoch=39
06/02/2022 12:39:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.176659 on epoch=41
06/02/2022 12:39:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.086657 on epoch=42
06/02/2022 12:39:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.185169 on epoch=43
06/02/2022 12:39:24 - INFO - __main__ - Global step 350 Train loss 0.157759 Classification-F1 0.4838709677419355 on epoch=43
06/02/2022 12:39:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.117940 on epoch=44
06/02/2022 12:39:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.065671 on epoch=46
06/02/2022 12:39:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.361692 on epoch=47
06/02/2022 12:39:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.292503 on epoch=48
06/02/2022 12:39:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.085329 on epoch=49
06/02/2022 12:39:51 - INFO - __main__ - Global step 400 Train loss 0.184627 Classification-F1 0.488 on epoch=49
06/02/2022 12:39:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.076377 on epoch=51
06/02/2022 12:40:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.107282 on epoch=52
06/02/2022 12:40:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.097890 on epoch=53
06/02/2022 12:40:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.162233 on epoch=54
06/02/2022 12:40:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.125369 on epoch=56
06/02/2022 12:40:17 - INFO - __main__ - Global step 450 Train loss 0.113830 Classification-F1 0.43859649122807015 on epoch=56
06/02/2022 12:40:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.057363 on epoch=57
06/02/2022 12:40:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.075773 on epoch=58
06/02/2022 12:40:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.114388 on epoch=59
06/02/2022 12:40:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.124038 on epoch=61
06/02/2022 12:40:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.021139 on epoch=62
06/02/2022 12:40:43 - INFO - __main__ - Global step 500 Train loss 0.078540 Classification-F1 0.4796747967479675 on epoch=62
06/02/2022 12:40:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.032161 on epoch=63
06/02/2022 12:40:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.031707 on epoch=64
06/02/2022 12:40:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.031479 on epoch=66
06/02/2022 12:41:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.004998 on epoch=67
06/02/2022 12:41:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.010661 on epoch=68
06/02/2022 12:41:10 - INFO - __main__ - Global step 550 Train loss 0.022201 Classification-F1 0.4796747967479675 on epoch=68
06/02/2022 12:41:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.010970 on epoch=69
06/02/2022 12:41:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.024055 on epoch=71
06/02/2022 12:41:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.006465 on epoch=72
06/02/2022 12:41:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.010962 on epoch=73
06/02/2022 12:41:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.041019 on epoch=74
06/02/2022 12:41:36 - INFO - __main__ - Global step 600 Train loss 0.018694 Classification-F1 0.4838709677419355 on epoch=74
06/02/2022 12:41:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.151018 on epoch=76
06/02/2022 12:41:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.078894 on epoch=77
06/02/2022 12:41:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.028349 on epoch=78
06/02/2022 12:41:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.173764 on epoch=79
06/02/2022 12:42:02 - INFO - __main__ - Step 650 Global step 650 Train loss 0.087279 on epoch=81
06/02/2022 12:42:03 - INFO - __main__ - Global step 650 Train loss 0.103861 Classification-F1 0.4838709677419355 on epoch=81
06/02/2022 12:42:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.106503 on epoch=82
06/02/2022 12:42:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.309523 on epoch=83
06/02/2022 12:42:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.564381 on epoch=84
06/02/2022 12:42:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.293603 on epoch=86
06/02/2022 12:42:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.285171 on epoch=87
06/02/2022 12:42:29 - INFO - __main__ - Global step 700 Train loss 0.311836 Classification-F1 0.2 on epoch=87
06/02/2022 12:42:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.328403 on epoch=88
06/02/2022 12:42:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.298488 on epoch=89
06/02/2022 12:42:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.176102 on epoch=91
06/02/2022 12:42:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.230572 on epoch=92
06/02/2022 12:42:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.234337 on epoch=93
06/02/2022 12:42:56 - INFO - __main__ - Global step 750 Train loss 0.253581 Classification-F1 0.4796747967479675 on epoch=93
06/02/2022 12:43:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.126575 on epoch=94
06/02/2022 12:43:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.191741 on epoch=96
06/02/2022 12:43:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.155717 on epoch=97
06/02/2022 12:43:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.070927 on epoch=98
06/02/2022 12:43:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.102273 on epoch=99
06/02/2022 12:43:23 - INFO - __main__ - Global step 800 Train loss 0.129447 Classification-F1 0.4666666666666667 on epoch=99
06/02/2022 12:43:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.073351 on epoch=101
06/02/2022 12:43:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.034681 on epoch=102
06/02/2022 12:43:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.053508 on epoch=103
06/02/2022 12:43:44 - INFO - __main__ - Step 840 Global step 840 Train loss 0.008677 on epoch=104
06/02/2022 12:43:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.093863 on epoch=106
06/02/2022 12:43:50 - INFO - __main__ - Global step 850 Train loss 0.052816 Classification-F1 0.2967032967032967 on epoch=106
06/02/2022 12:43:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.038302 on epoch=107
06/02/2022 12:44:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.173041 on epoch=108
06/02/2022 12:44:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.113939 on epoch=109
06/02/2022 12:44:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.135019 on epoch=111
06/02/2022 12:44:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.051684 on epoch=112
06/02/2022 12:44:16 - INFO - __main__ - Global step 900 Train loss 0.102397 Classification-F1 0.452991452991453 on epoch=112
06/02/2022 12:44:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.088942 on epoch=113
06/02/2022 12:44:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.036640 on epoch=114
06/02/2022 12:44:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.053082 on epoch=116
06/02/2022 12:44:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.083791 on epoch=117
06/02/2022 12:44:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.052108 on epoch=118
06/02/2022 12:44:43 - INFO - __main__ - Global step 950 Train loss 0.062913 Classification-F1 0.4666666666666667 on epoch=118
06/02/2022 12:44:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.026824 on epoch=119
06/02/2022 12:44:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.027537 on epoch=121
06/02/2022 12:44:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.002478 on epoch=122
06/02/2022 12:45:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.050087 on epoch=123
06/02/2022 12:45:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.031348 on epoch=124
06/02/2022 12:45:10 - INFO - __main__ - Global step 1000 Train loss 0.027655 Classification-F1 0.4666666666666667 on epoch=124
06/02/2022 12:45:10 - INFO - __main__ - save last model!
06/02/2022 12:45:11 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 12:45:11 - INFO - __main__ - Printing 3 examples
06/02/2022 12:45:11 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/02/2022 12:45:11 - INFO - __main__ - ['false']
06/02/2022 12:45:11 - INFO - __main__ -  [ethos-religion] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
06/02/2022 12:45:11 - INFO - __main__ - ['false']
06/02/2022 12:45:11 - INFO - __main__ -  [ethos-religion] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
06/02/2022 12:45:11 - INFO - __main__ - ['false']
06/02/2022 12:45:11 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:45:11 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:45:11 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 12:45:11 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:45:11 - INFO - __main__ - Printing 3 examples
06/02/2022 12:45:11 - INFO - __main__ -  [ethos-religion] On todays episode of "Guess That Gender"
06/02/2022 12:45:11 - INFO - __main__ - ['false']
06/02/2022 12:45:11 - INFO - __main__ -  [ethos-religion] The Whites Have dislike the documentary because they are mad at  the truth. #WhiteDevils
06/02/2022 12:45:11 - INFO - __main__ - ['false']
06/02/2022 12:45:11 - INFO - __main__ -  [ethos-religion] Women are made for making babies and cooking dinner and nothing else!!!
06/02/2022 12:45:11 - INFO - __main__ - ['false']
06/02/2022 12:45:11 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:45:11 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:45:11 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:45:17 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 12:45:18 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 12:45:18 - INFO - __main__ - Printing 3 examples
06/02/2022 12:45:18 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 12:45:18 - INFO - __main__ - ['false']
06/02/2022 12:45:18 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 12:45:18 - INFO - __main__ - ['false']
06/02/2022 12:45:18 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 12:45:18 - INFO - __main__ - ['true']
06/02/2022 12:45:18 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:45:18 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:45:18 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 12:45:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_21_0.0002_8_predictions.txt
06/02/2022 12:45:20 - INFO - __main__ - Classification-F1 on test data: 0.6497
06/02/2022 12:45:20 - INFO - __main__ - prefix=ethos-religion_64_21, lr=0.0002, bsz=8, dev_performance=1.0, test_performance=0.6496644295302013
06/02/2022 12:45:20 - INFO - __main__ - Running ... prefix=ethos-religion_64_21, lr=0.0001, bsz=8 ...
06/02/2022 12:45:21 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 12:45:21 - INFO - __main__ - Printing 3 examples
06/02/2022 12:45:21 - INFO - __main__ -  [ethos-religion] You are so lucky you have a girlfriend. I bet your asian penis is smaller than my thumb
06/02/2022 12:45:21 - INFO - __main__ - ['false']
06/02/2022 12:45:21 - INFO - __main__ -  [ethos-religion] FUCK THAT HOMO DUMB ASS PRICK!  SHOULDA LET HIM GET FUCKING EATEN!  ASSHOKE FUCKING WORTHLESS DICK.... NOBODY WOULD HAVE MISSED HIM!
06/02/2022 12:45:21 - INFO - __main__ - ['false']
06/02/2022 12:45:21 - INFO - __main__ -  [ethos-religion] Hope you fucking die bitches gay people should not exist all you do is lick one another's ass
06/02/2022 12:45:21 - INFO - __main__ - ['false']
06/02/2022 12:45:21 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:45:21 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:45:21 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 12:45:21 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:45:21 - INFO - __main__ - Printing 3 examples
06/02/2022 12:45:21 - INFO - __main__ -  [ethos-religion] On todays episode of "Guess That Gender"
06/02/2022 12:45:21 - INFO - __main__ - ['false']
06/02/2022 12:45:21 - INFO - __main__ -  [ethos-religion] The Whites Have dislike the documentary because they are mad at  the truth. #WhiteDevils
06/02/2022 12:45:21 - INFO - __main__ - ['false']
06/02/2022 12:45:21 - INFO - __main__ -  [ethos-religion] Women are made for making babies and cooking dinner and nothing else!!!
06/02/2022 12:45:21 - INFO - __main__ - ['false']
06/02/2022 12:45:21 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:45:21 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:45:21 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:45:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 12:45:24 - INFO - __main__ - Starting training!
06/02/2022 12:45:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 12:45:32 - INFO - __main__ - Starting training!
06/02/2022 12:45:37 - INFO - __main__ - Step 10 Global step 10 Train loss 23.881733 on epoch=1
06/02/2022 12:45:42 - INFO - __main__ - Step 20 Global step 20 Train loss 22.806265 on epoch=2
06/02/2022 12:45:47 - INFO - __main__ - Step 30 Global step 30 Train loss 20.260096 on epoch=3
06/02/2022 12:45:52 - INFO - __main__ - Step 40 Global step 40 Train loss 18.303593 on epoch=4
06/02/2022 12:45:57 - INFO - __main__ - Step 50 Global step 50 Train loss 18.478609 on epoch=6
06/02/2022 12:46:17 - INFO - __main__ - Global step 50 Train loss 20.746059 Classification-F1 0.0 on epoch=6
06/02/2022 12:46:23 - INFO - __main__ - Step 60 Global step 60 Train loss 17.524330 on epoch=7
06/02/2022 12:46:28 - INFO - __main__ - Step 70 Global step 70 Train loss 17.149075 on epoch=8
06/02/2022 12:46:33 - INFO - __main__ - Step 80 Global step 80 Train loss 16.415810 on epoch=9
06/02/2022 12:46:38 - INFO - __main__ - Step 90 Global step 90 Train loss 15.369222 on epoch=11
06/02/2022 12:46:44 - INFO - __main__ - Step 100 Global step 100 Train loss 15.614439 on epoch=12
06/02/2022 12:47:03 - INFO - __main__ - Global step 100 Train loss 16.414574 Classification-F1 0.0 on epoch=12
06/02/2022 12:47:08 - INFO - __main__ - Step 110 Global step 110 Train loss 15.948834 on epoch=13
06/02/2022 12:47:13 - INFO - __main__ - Step 120 Global step 120 Train loss 15.202188 on epoch=14
06/02/2022 12:47:18 - INFO - __main__ - Step 130 Global step 130 Train loss 14.588722 on epoch=16
06/02/2022 12:47:23 - INFO - __main__ - Step 140 Global step 140 Train loss 14.153468 on epoch=17
06/02/2022 12:47:28 - INFO - __main__ - Step 150 Global step 150 Train loss 13.730913 on epoch=18
06/02/2022 12:47:47 - INFO - __main__ - Global step 150 Train loss 14.724826 Classification-F1 0.0 on epoch=18
06/02/2022 12:47:52 - INFO - __main__ - Step 160 Global step 160 Train loss 12.946973 on epoch=19
06/02/2022 12:47:57 - INFO - __main__ - Step 170 Global step 170 Train loss 13.483055 on epoch=21
06/02/2022 12:48:02 - INFO - __main__ - Step 180 Global step 180 Train loss 12.980524 on epoch=22
06/02/2022 12:48:07 - INFO - __main__ - Step 190 Global step 190 Train loss 13.092847 on epoch=23
06/02/2022 12:48:12 - INFO - __main__ - Step 200 Global step 200 Train loss 10.775732 on epoch=24
06/02/2022 12:48:31 - INFO - __main__ - Global step 200 Train loss 12.655827 Classification-F1 0.0 on epoch=24
06/02/2022 12:48:36 - INFO - __main__ - Step 210 Global step 210 Train loss 11.586719 on epoch=26
06/02/2022 12:48:41 - INFO - __main__ - Step 220 Global step 220 Train loss 10.039411 on epoch=27
06/02/2022 12:48:46 - INFO - __main__ - Step 230 Global step 230 Train loss 8.901533 on epoch=28
06/02/2022 12:48:52 - INFO - __main__ - Step 240 Global step 240 Train loss 7.314548 on epoch=29
06/02/2022 12:48:57 - INFO - __main__ - Step 250 Global step 250 Train loss 4.829803 on epoch=31
06/02/2022 12:49:16 - INFO - __main__ - Global step 250 Train loss 8.534403 Classification-F1 0.0012121212121212121 on epoch=31
06/02/2022 12:49:21 - INFO - __main__ - Step 260 Global step 260 Train loss 4.824468 on epoch=32
06/02/2022 12:49:27 - INFO - __main__ - Step 270 Global step 270 Train loss 3.852476 on epoch=33
06/02/2022 12:49:32 - INFO - __main__ - Step 280 Global step 280 Train loss 4.474574 on epoch=34
06/02/2022 12:49:37 - INFO - __main__ - Step 290 Global step 290 Train loss 3.334930 on epoch=36
06/02/2022 12:49:42 - INFO - __main__ - Step 300 Global step 300 Train loss 1.335212 on epoch=37
06/02/2022 12:49:43 - INFO - __main__ - Global step 300 Train loss 3.564332 Classification-F1 0.47540983606557374 on epoch=37
06/02/2022 12:49:48 - INFO - __main__ - Step 310 Global step 310 Train loss 1.138286 on epoch=38
06/02/2022 12:49:53 - INFO - __main__ - Step 320 Global step 320 Train loss 1.429004 on epoch=39
06/02/2022 12:49:59 - INFO - __main__ - Step 330 Global step 330 Train loss 1.994625 on epoch=41
06/02/2022 12:50:04 - INFO - __main__ - Step 340 Global step 340 Train loss 1.010637 on epoch=42
06/02/2022 12:50:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.866406 on epoch=43
06/02/2022 12:50:10 - INFO - __main__ - Global step 350 Train loss 1.287792 Classification-F1 0.41818181818181815 on epoch=43
06/02/2022 12:50:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.593966 on epoch=44
06/02/2022 12:50:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.833313 on epoch=46
06/02/2022 12:50:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.671700 on epoch=47
06/02/2022 12:50:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.707858 on epoch=48
06/02/2022 12:50:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.386192 on epoch=49
06/02/2022 12:50:36 - INFO - __main__ - Global step 400 Train loss 0.638606 Classification-F1 0.488 on epoch=49
06/02/2022 12:50:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.394926 on epoch=51
06/02/2022 12:50:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.384080 on epoch=52
06/02/2022 12:50:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.331041 on epoch=53
06/02/2022 12:50:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.515428 on epoch=54
06/02/2022 12:51:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.399163 on epoch=56
06/02/2022 12:51:04 - INFO - __main__ - Global step 450 Train loss 0.404927 Classification-F1 0.43859649122807015 on epoch=56
06/02/2022 12:51:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.269878 on epoch=57
06/02/2022 12:51:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.300702 on epoch=58
06/02/2022 12:51:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.354707 on epoch=59
06/02/2022 12:51:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.286096 on epoch=61
06/02/2022 12:51:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.225979 on epoch=62
06/02/2022 12:51:31 - INFO - __main__ - Global step 500 Train loss 0.287472 Classification-F1 0.46218487394957986 on epoch=62
06/02/2022 12:51:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.184388 on epoch=63
06/02/2022 12:51:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.257432 on epoch=64
06/02/2022 12:51:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.238233 on epoch=66
06/02/2022 12:51:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.223413 on epoch=67
06/02/2022 12:51:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.260632 on epoch=68
06/02/2022 12:51:57 - INFO - __main__ - Global step 550 Train loss 0.232819 Classification-F1 0.47540983606557374 on epoch=68
06/02/2022 12:52:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.240334 on epoch=69
06/02/2022 12:52:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.182898 on epoch=71
06/02/2022 12:52:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.167010 on epoch=72
06/02/2022 12:52:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.211104 on epoch=73
06/02/2022 12:52:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.148090 on epoch=74
06/02/2022 12:52:24 - INFO - __main__ - Global step 600 Train loss 0.189887 Classification-F1 0.47540983606557374 on epoch=74
06/02/2022 12:52:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.097336 on epoch=76
06/02/2022 12:52:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.123615 on epoch=77
06/02/2022 12:52:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.088678 on epoch=78
06/02/2022 12:52:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.140476 on epoch=79
06/02/2022 12:52:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.150639 on epoch=81
06/02/2022 12:52:51 - INFO - __main__ - Global step 650 Train loss 0.120149 Classification-F1 0.46218487394957986 on epoch=81
06/02/2022 12:52:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.104408 on epoch=82
06/02/2022 12:53:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.098886 on epoch=83
06/02/2022 12:53:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.203147 on epoch=84
06/02/2022 12:53:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.123719 on epoch=86
06/02/2022 12:53:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.101305 on epoch=87
06/02/2022 12:53:18 - INFO - __main__ - Global step 700 Train loss 0.126293 Classification-F1 0.4666666666666667 on epoch=87
06/02/2022 12:53:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.074523 on epoch=88
06/02/2022 12:53:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.130625 on epoch=89
06/02/2022 12:53:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.130535 on epoch=91
06/02/2022 12:53:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.055079 on epoch=92
06/02/2022 12:53:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.071175 on epoch=93
06/02/2022 12:53:45 - INFO - __main__ - Global step 750 Train loss 0.092388 Classification-F1 0.46218487394957986 on epoch=93
06/02/2022 12:53:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.074593 on epoch=94
06/02/2022 12:53:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.095140 on epoch=96
06/02/2022 12:54:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.108787 on epoch=97
06/02/2022 12:54:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.094392 on epoch=98
06/02/2022 12:54:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.057084 on epoch=99
06/02/2022 12:54:11 - INFO - __main__ - Global step 800 Train loss 0.085999 Classification-F1 0.46218487394957986 on epoch=99
06/02/2022 12:54:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.028965 on epoch=101
06/02/2022 12:54:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.069866 on epoch=102
06/02/2022 12:54:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.077526 on epoch=103
06/02/2022 12:54:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.088482 on epoch=104
06/02/2022 12:54:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.032815 on epoch=106
06/02/2022 12:54:38 - INFO - __main__ - Global step 850 Train loss 0.059531 Classification-F1 0.4796747967479675 on epoch=106
06/02/2022 12:54:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.068676 on epoch=107
06/02/2022 12:54:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.058134 on epoch=108
06/02/2022 12:54:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.072605 on epoch=109
06/02/2022 12:54:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.063734 on epoch=111
06/02/2022 12:55:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.043648 on epoch=112
06/02/2022 12:55:05 - INFO - __main__ - Global step 900 Train loss 0.061359 Classification-F1 0.47540983606557374 on epoch=112
06/02/2022 12:55:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.046984 on epoch=113
06/02/2022 12:55:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.027989 on epoch=114
06/02/2022 12:55:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.055046 on epoch=116
06/02/2022 12:55:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.056310 on epoch=117
06/02/2022 12:55:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.051986 on epoch=118
06/02/2022 12:55:32 - INFO - __main__ - Global step 950 Train loss 0.047663 Classification-F1 0.452991452991453 on epoch=118
06/02/2022 12:55:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.019663 on epoch=119
06/02/2022 12:55:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.054767 on epoch=121
06/02/2022 12:55:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.015524 on epoch=122
06/02/2022 12:55:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.026933 on epoch=123
06/02/2022 12:55:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.018216 on epoch=124
06/02/2022 12:55:59 - INFO - __main__ - Global step 1000 Train loss 0.027021 Classification-F1 0.4666666666666667 on epoch=124
06/02/2022 12:55:59 - INFO - __main__ - save last model!
06/02/2022 12:55:59 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 12:55:59 - INFO - __main__ - Printing 3 examples
06/02/2022 12:55:59 - INFO - __main__ -  [ethos-religion] Women deserve to be abused, I guess.
06/02/2022 12:55:59 - INFO - __main__ - ['false']
06/02/2022 12:55:59 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
06/02/2022 12:55:59 - INFO - __main__ - ['false']
06/02/2022 12:55:59 - INFO - __main__ -  [ethos-religion] The only thing more repulsive than a nigger is a nigger faggot. Self-hating whites shower images of cross-dressed niggers with praise because the modern left fetishizes all forms of depravity.
06/02/2022 12:55:59 - INFO - __main__ - ['false']
06/02/2022 12:55:59 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:55:59 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:56:00 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 12:56:00 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:56:00 - INFO - __main__ - Printing 3 examples
06/02/2022 12:56:00 - INFO - __main__ -  [ethos-religion] Down syndrome? Problematic people must be dead
06/02/2022 12:56:00 - INFO - __main__ - ['false']
06/02/2022 12:56:00 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
06/02/2022 12:56:00 - INFO - __main__ - ['false']
06/02/2022 12:56:00 - INFO - __main__ -  [ethos-religion] Ugly long chin brainless problematic mother fucker he looks retarded
06/02/2022 12:56:00 - INFO - __main__ - ['false']
06/02/2022 12:56:00 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:56:00 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:56:00 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:56:06 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 12:56:06 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 12:56:06 - INFO - __main__ - Printing 3 examples
06/02/2022 12:56:06 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 12:56:06 - INFO - __main__ - ['false']
06/02/2022 12:56:06 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 12:56:06 - INFO - __main__ - ['false']
06/02/2022 12:56:06 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 12:56:06 - INFO - __main__ - ['true']
06/02/2022 12:56:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:56:06 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:56:07 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 12:56:08 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_21_0.0001_8_predictions.txt
06/02/2022 12:56:08 - INFO - __main__ - Classification-F1 on test data: 0.9142
06/02/2022 12:56:09 - INFO - __main__ - prefix=ethos-religion_64_21, lr=0.0001, bsz=8, dev_performance=0.488, test_performance=0.9141842572499508
06/02/2022 12:56:09 - INFO - __main__ - Running ... prefix=ethos-religion_64_42, lr=0.0005, bsz=8 ...
06/02/2022 12:56:09 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 12:56:09 - INFO - __main__ - Printing 3 examples
06/02/2022 12:56:09 - INFO - __main__ -  [ethos-religion] Women deserve to be abused, I guess.
06/02/2022 12:56:09 - INFO - __main__ - ['false']
06/02/2022 12:56:09 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
06/02/2022 12:56:09 - INFO - __main__ - ['false']
06/02/2022 12:56:09 - INFO - __main__ -  [ethos-religion] The only thing more repulsive than a nigger is a nigger faggot. Self-hating whites shower images of cross-dressed niggers with praise because the modern left fetishizes all forms of depravity.
06/02/2022 12:56:09 - INFO - __main__ - ['false']
06/02/2022 12:56:09 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:56:10 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:56:10 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 12:56:10 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:56:10 - INFO - __main__ - Printing 3 examples
06/02/2022 12:56:10 - INFO - __main__ -  [ethos-religion] Down syndrome? Problematic people must be dead
06/02/2022 12:56:10 - INFO - __main__ - ['false']
06/02/2022 12:56:10 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
06/02/2022 12:56:10 - INFO - __main__ - ['false']
06/02/2022 12:56:10 - INFO - __main__ -  [ethos-religion] Ugly long chin brainless problematic mother fucker he looks retarded
06/02/2022 12:56:10 - INFO - __main__ - ['false']
06/02/2022 12:56:10 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:56:10 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:56:10 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:56:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 12:56:12 - INFO - __main__ - Starting training!
06/02/2022 12:56:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 12:56:20 - INFO - __main__ - Starting training!
06/02/2022 12:56:25 - INFO - __main__ - Step 10 Global step 10 Train loss 23.016678 on epoch=1
06/02/2022 12:56:30 - INFO - __main__ - Step 20 Global step 20 Train loss 18.342062 on epoch=2
06/02/2022 12:56:36 - INFO - __main__ - Step 30 Global step 30 Train loss 15.807574 on epoch=3
06/02/2022 12:56:41 - INFO - __main__ - Step 40 Global step 40 Train loss 13.204778 on epoch=4
06/02/2022 12:56:46 - INFO - __main__ - Step 50 Global step 50 Train loss 11.143589 on epoch=6
06/02/2022 12:56:47 - INFO - __main__ - Global step 50 Train loss 16.302937 Classification-F1 0.0 on epoch=6
06/02/2022 12:56:53 - INFO - __main__ - Step 60 Global step 60 Train loss 4.552765 on epoch=7
06/02/2022 12:56:58 - INFO - __main__ - Step 70 Global step 70 Train loss 1.020141 on epoch=8
06/02/2022 12:57:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.590108 on epoch=9
06/02/2022 12:57:08 - INFO - __main__ - Step 90 Global step 90 Train loss 0.475325 on epoch=11
06/02/2022 12:57:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.339265 on epoch=12
06/02/2022 12:57:14 - INFO - __main__ - Global step 100 Train loss 1.395521 Classification-F1 0.15789473684210525 on epoch=12
06/02/2022 12:57:21 - INFO - __main__ - Step 110 Global step 110 Train loss 0.276299 on epoch=13
06/02/2022 12:57:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.256162 on epoch=14
06/02/2022 12:57:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.191396 on epoch=16
06/02/2022 12:57:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.148450 on epoch=17
06/02/2022 12:57:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.127024 on epoch=18
06/02/2022 12:57:42 - INFO - __main__ - Global step 150 Train loss 0.199866 Classification-F1 0.4838709677419355 on epoch=18
06/02/2022 12:57:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.204833 on epoch=19
06/02/2022 12:57:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.098415 on epoch=21
06/02/2022 12:57:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.097122 on epoch=22
06/02/2022 12:58:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.096446 on epoch=23
06/02/2022 12:58:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.040008 on epoch=24
06/02/2022 12:58:10 - INFO - __main__ - Global step 200 Train loss 0.107365 Classification-F1 0.488 on epoch=24
06/02/2022 12:58:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.045914 on epoch=26
06/02/2022 12:58:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.086443 on epoch=27
06/02/2022 12:58:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.057548 on epoch=28
06/02/2022 12:58:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.096214 on epoch=29
06/02/2022 12:58:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.015543 on epoch=31
06/02/2022 12:58:38 - INFO - __main__ - Global step 250 Train loss 0.060332 Classification-F1 0.49206349206349204 on epoch=31
06/02/2022 12:58:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.093083 on epoch=32
06/02/2022 12:58:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.089844 on epoch=33
06/02/2022 12:58:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.019927 on epoch=34
06/02/2022 12:59:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.001828 on epoch=36
06/02/2022 12:59:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.001046 on epoch=37
06/02/2022 12:59:06 - INFO - __main__ - Global step 300 Train loss 0.041146 Classification-F1 0.488 on epoch=37
06/02/2022 12:59:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.006011 on epoch=38
06/02/2022 12:59:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.006115 on epoch=39
06/02/2022 12:59:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000793 on epoch=41
06/02/2022 12:59:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000588 on epoch=42
06/02/2022 12:59:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.002633 on epoch=43
06/02/2022 12:59:32 - INFO - __main__ - Global step 350 Train loss 0.003228 Classification-F1 0.49206349206349204 on epoch=43
06/02/2022 12:59:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.002045 on epoch=44
06/02/2022 12:59:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000171 on epoch=46
06/02/2022 12:59:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000075 on epoch=47
06/02/2022 12:59:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.035846 on epoch=48
06/02/2022 12:59:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.004821 on epoch=49
06/02/2022 12:59:59 - INFO - __main__ - Global step 400 Train loss 0.008592 Classification-F1 0.488 on epoch=49
06/02/2022 13:00:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.001684 on epoch=51
06/02/2022 13:00:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.004430 on epoch=52
06/02/2022 13:00:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.041316 on epoch=53
06/02/2022 13:00:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.013671 on epoch=54
06/02/2022 13:00:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.004825 on epoch=56
06/02/2022 13:00:25 - INFO - __main__ - Global step 450 Train loss 0.013185 Classification-F1 0.49206349206349204 on epoch=56
06/02/2022 13:00:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000208 on epoch=57
06/02/2022 13:00:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001538 on epoch=58
06/02/2022 13:00:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.013032 on epoch=59
06/02/2022 13:00:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001010 on epoch=61
06/02/2022 13:00:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000916 on epoch=62
06/02/2022 13:00:52 - INFO - __main__ - Global step 500 Train loss 0.003341 Classification-F1 0.4796747967479675 on epoch=62
06/02/2022 13:00:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000752 on epoch=63
06/02/2022 13:01:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000240 on epoch=64
06/02/2022 13:01:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000403 on epoch=66
06/02/2022 13:01:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000273 on epoch=67
06/02/2022 13:01:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000082 on epoch=68
06/02/2022 13:01:19 - INFO - __main__ - Global step 550 Train loss 0.000350 Classification-F1 0.49206349206349204 on epoch=68
06/02/2022 13:01:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.038147 on epoch=69
06/02/2022 13:01:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.001194 on epoch=71
06/02/2022 13:01:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000308 on epoch=72
06/02/2022 13:01:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000427 on epoch=73
06/02/2022 13:01:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000052 on epoch=74
06/02/2022 13:01:45 - INFO - __main__ - Global step 600 Train loss 0.008025 Classification-F1 0.488 on epoch=74
06/02/2022 13:01:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000129 on epoch=76
06/02/2022 13:01:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000038 on epoch=77
06/02/2022 13:02:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000029 on epoch=78
06/02/2022 13:02:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001274 on epoch=79
06/02/2022 13:02:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000070 on epoch=81
06/02/2022 13:02:12 - INFO - __main__ - Global step 650 Train loss 0.000308 Classification-F1 0.488 on epoch=81
06/02/2022 13:02:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000029 on epoch=82
06/02/2022 13:02:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000088 on epoch=83
06/02/2022 13:02:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000145 on epoch=84
06/02/2022 13:02:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.013649 on epoch=86
06/02/2022 13:02:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000157 on epoch=87
06/02/2022 13:02:39 - INFO - __main__ - Global step 700 Train loss 0.002814 Classification-F1 0.49606299212598426 on epoch=87
06/02/2022 13:02:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000226 on epoch=88
06/02/2022 13:02:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000110 on epoch=89
06/02/2022 13:02:56 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000096 on epoch=91
06/02/2022 13:03:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.019276 on epoch=92
06/02/2022 13:03:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000160 on epoch=93
06/02/2022 13:03:07 - INFO - __main__ - Global step 750 Train loss 0.003973 Classification-F1 0.488 on epoch=93
06/02/2022 13:03:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000073 on epoch=94
06/02/2022 13:03:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000247 on epoch=96
06/02/2022 13:03:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.018117 on epoch=97
06/02/2022 13:03:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.099276 on epoch=98
06/02/2022 13:03:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000438 on epoch=99
06/02/2022 13:03:34 - INFO - __main__ - Global step 800 Train loss 0.023630 Classification-F1 0.49206349206349204 on epoch=99
06/02/2022 13:03:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.015862 on epoch=101
06/02/2022 13:03:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000438 on epoch=102
06/02/2022 13:03:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.001149 on epoch=103
06/02/2022 13:03:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000509 on epoch=104
06/02/2022 13:04:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000657 on epoch=106
06/02/2022 13:04:01 - INFO - __main__ - Global step 850 Train loss 0.003723 Classification-F1 0.49206349206349204 on epoch=106
06/02/2022 13:04:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000022 on epoch=107
06/02/2022 13:04:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.001342 on epoch=108
06/02/2022 13:04:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000484 on epoch=109
06/02/2022 13:04:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000192 on epoch=111
06/02/2022 13:04:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000019 on epoch=112
06/02/2022 13:04:27 - INFO - __main__ - Global step 900 Train loss 0.000412 Classification-F1 0.49206349206349204 on epoch=112
06/02/2022 13:04:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000029 on epoch=113
06/02/2022 13:04:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.031635 on epoch=114
06/02/2022 13:04:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.141961 on epoch=116
06/02/2022 13:04:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.071460 on epoch=117
06/02/2022 13:04:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.002900 on epoch=118
06/02/2022 13:04:54 - INFO - __main__ - Global step 950 Train loss 0.049597 Classification-F1 0.488 on epoch=118
06/02/2022 13:04:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000304 on epoch=119
06/02/2022 13:05:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000088 on epoch=121
06/02/2022 13:05:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.016439 on epoch=122
06/02/2022 13:05:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000094 on epoch=123
06/02/2022 13:05:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000212 on epoch=124
06/02/2022 13:05:20 - INFO - __main__ - Global step 1000 Train loss 0.003427 Classification-F1 0.488 on epoch=124
06/02/2022 13:05:20 - INFO - __main__ - save last model!
06/02/2022 13:05:21 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 13:05:21 - INFO - __main__ - Printing 3 examples
06/02/2022 13:05:21 - INFO - __main__ -  [ethos-religion] Women deserve to be abused, I guess.
06/02/2022 13:05:21 - INFO - __main__ - ['false']
06/02/2022 13:05:21 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
06/02/2022 13:05:21 - INFO - __main__ - ['false']
06/02/2022 13:05:21 - INFO - __main__ -  [ethos-religion] The only thing more repulsive than a nigger is a nigger faggot. Self-hating whites shower images of cross-dressed niggers with praise because the modern left fetishizes all forms of depravity.
06/02/2022 13:05:21 - INFO - __main__ - ['false']
06/02/2022 13:05:21 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:05:21 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:05:21 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 13:05:21 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:05:21 - INFO - __main__ - Printing 3 examples
06/02/2022 13:05:21 - INFO - __main__ -  [ethos-religion] Down syndrome? Problematic people must be dead
06/02/2022 13:05:21 - INFO - __main__ - ['false']
06/02/2022 13:05:21 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
06/02/2022 13:05:21 - INFO - __main__ - ['false']
06/02/2022 13:05:21 - INFO - __main__ -  [ethos-religion] Ugly long chin brainless problematic mother fucker he looks retarded
06/02/2022 13:05:21 - INFO - __main__ - ['false']
06/02/2022 13:05:21 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:05:21 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:05:21 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:05:28 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 13:05:28 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 13:05:28 - INFO - __main__ - Printing 3 examples
06/02/2022 13:05:28 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 13:05:28 - INFO - __main__ - ['false']
06/02/2022 13:05:28 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 13:05:28 - INFO - __main__ - ['false']
06/02/2022 13:05:28 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 13:05:28 - INFO - __main__ - ['true']
06/02/2022 13:05:28 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:05:28 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:05:28 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 13:05:30 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_42_0.0005_8_predictions.txt
06/02/2022 13:05:30 - INFO - __main__ - Classification-F1 on test data: 0.9675
06/02/2022 13:05:30 - INFO - __main__ - prefix=ethos-religion_64_42, lr=0.0005, bsz=8, dev_performance=0.49606299212598426, test_performance=0.9675373134328358
06/02/2022 13:05:30 - INFO - __main__ - Running ... prefix=ethos-religion_64_42, lr=0.0003, bsz=8 ...
06/02/2022 13:05:31 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 13:05:31 - INFO - __main__ - Printing 3 examples
06/02/2022 13:05:31 - INFO - __main__ -  [ethos-religion] Women deserve to be abused, I guess.
06/02/2022 13:05:31 - INFO - __main__ - ['false']
06/02/2022 13:05:31 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
06/02/2022 13:05:31 - INFO - __main__ - ['false']
06/02/2022 13:05:31 - INFO - __main__ -  [ethos-religion] The only thing more repulsive than a nigger is a nigger faggot. Self-hating whites shower images of cross-dressed niggers with praise because the modern left fetishizes all forms of depravity.
06/02/2022 13:05:31 - INFO - __main__ - ['false']
06/02/2022 13:05:31 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:05:31 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:05:31 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 13:05:31 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:05:31 - INFO - __main__ - Printing 3 examples
06/02/2022 13:05:31 - INFO - __main__ -  [ethos-religion] Down syndrome? Problematic people must be dead
06/02/2022 13:05:31 - INFO - __main__ - ['false']
06/02/2022 13:05:31 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
06/02/2022 13:05:31 - INFO - __main__ - ['false']
06/02/2022 13:05:31 - INFO - __main__ -  [ethos-religion] Ugly long chin brainless problematic mother fucker he looks retarded
06/02/2022 13:05:31 - INFO - __main__ - ['false']
06/02/2022 13:05:31 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:05:31 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:05:32 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:05:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 13:05:32 - INFO - __main__ - Starting training!
06/02/2022 13:05:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 13:05:44 - INFO - __main__ - Starting training!
06/02/2022 13:05:49 - INFO - __main__ - Step 10 Global step 10 Train loss 23.901665 on epoch=1
06/02/2022 13:05:54 - INFO - __main__ - Step 20 Global step 20 Train loss 19.670656 on epoch=2
06/02/2022 13:05:59 - INFO - __main__ - Step 30 Global step 30 Train loss 17.896879 on epoch=3
06/02/2022 13:06:04 - INFO - __main__ - Step 40 Global step 40 Train loss 16.609882 on epoch=4
06/02/2022 13:06:09 - INFO - __main__ - Step 50 Global step 50 Train loss 15.400264 on epoch=6
06/02/2022 13:06:10 - INFO - __main__ - Global step 50 Train loss 18.695869 Classification-F1 0.0 on epoch=6
06/02/2022 13:06:16 - INFO - __main__ - Step 60 Global step 60 Train loss 14.203992 on epoch=7
06/02/2022 13:06:21 - INFO - __main__ - Step 70 Global step 70 Train loss 12.291258 on epoch=8
06/02/2022 13:06:26 - INFO - __main__ - Step 80 Global step 80 Train loss 10.825171 on epoch=9
06/02/2022 13:06:31 - INFO - __main__ - Step 90 Global step 90 Train loss 6.536356 on epoch=11
06/02/2022 13:06:36 - INFO - __main__ - Step 100 Global step 100 Train loss 3.842836 on epoch=12
06/02/2022 13:06:37 - INFO - __main__ - Global step 100 Train loss 9.539923 Classification-F1 0.015384615384615385 on epoch=12
06/02/2022 13:06:42 - INFO - __main__ - Step 110 Global step 110 Train loss 1.630269 on epoch=13
06/02/2022 13:06:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.570943 on epoch=14
06/02/2022 13:06:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.552081 on epoch=16
06/02/2022 13:06:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.511229 on epoch=17
06/02/2022 13:07:03 - INFO - __main__ - Step 150 Global step 150 Train loss 0.483450 on epoch=18
06/02/2022 13:07:04 - INFO - __main__ - Global step 150 Train loss 0.749595 Classification-F1 0.3191489361702128 on epoch=18
06/02/2022 13:07:09 - INFO - __main__ - Step 160 Global step 160 Train loss 0.401708 on epoch=19
06/02/2022 13:07:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.440827 on epoch=21
06/02/2022 13:07:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.464634 on epoch=22
06/02/2022 13:07:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.410308 on epoch=23
06/02/2022 13:07:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.366624 on epoch=24
06/02/2022 13:07:31 - INFO - __main__ - Global step 200 Train loss 0.416820 Classification-F1 0.09859154929577464 on epoch=24
06/02/2022 13:07:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.407078 on epoch=26
06/02/2022 13:07:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.488958 on epoch=27
06/02/2022 13:07:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.402127 on epoch=28
06/02/2022 13:07:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.455533 on epoch=29
06/02/2022 13:07:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.426569 on epoch=31
06/02/2022 13:07:57 - INFO - __main__ - Global step 250 Train loss 0.436053 Classification-F1 0.13513513513513514 on epoch=31
06/02/2022 13:08:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.490626 on epoch=32
06/02/2022 13:08:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.367195 on epoch=33
06/02/2022 13:08:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.369309 on epoch=34
06/02/2022 13:08:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.349564 on epoch=36
06/02/2022 13:08:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.306014 on epoch=37
06/02/2022 13:08:24 - INFO - __main__ - Global step 300 Train loss 0.376542 Classification-F1 0.4666666666666667 on epoch=37
06/02/2022 13:08:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.269099 on epoch=38
06/02/2022 13:08:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.283894 on epoch=39
06/02/2022 13:08:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.269305 on epoch=41
06/02/2022 13:08:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.202531 on epoch=42
06/02/2022 13:08:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.163246 on epoch=43
06/02/2022 13:08:51 - INFO - __main__ - Global step 350 Train loss 0.237615 Classification-F1 0.46218487394957986 on epoch=43
06/02/2022 13:08:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.148402 on epoch=44
06/02/2022 13:09:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.192051 on epoch=46
06/02/2022 13:09:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.292396 on epoch=47
06/02/2022 13:09:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.169981 on epoch=48
06/02/2022 13:09:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.261923 on epoch=49
06/02/2022 13:09:17 - INFO - __main__ - Global step 400 Train loss 0.212951 Classification-F1 0.3402061855670103 on epoch=49
06/02/2022 13:09:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.367736 on epoch=51
06/02/2022 13:09:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.340914 on epoch=52
06/02/2022 13:09:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.273087 on epoch=53
06/02/2022 13:09:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.304481 on epoch=54
06/02/2022 13:09:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.303477 on epoch=56
06/02/2022 13:09:44 - INFO - __main__ - Global step 450 Train loss 0.317939 Classification-F1 0.2289156626506024 on epoch=56
06/02/2022 13:09:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.321025 on epoch=57
06/02/2022 13:09:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.235411 on epoch=58
06/02/2022 13:09:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.342494 on epoch=59
06/02/2022 13:10:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.313992 on epoch=61
06/02/2022 13:10:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.565439 on epoch=62
06/02/2022 13:10:10 - INFO - __main__ - Global step 500 Train loss 0.355672 Classification-F1 0.04477611940298507 on epoch=62
06/02/2022 13:10:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.256869 on epoch=63
06/02/2022 13:10:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.183014 on epoch=64
06/02/2022 13:10:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.214233 on epoch=66
06/02/2022 13:10:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.394633 on epoch=67
06/02/2022 13:10:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.313028 on epoch=68
06/02/2022 13:10:36 - INFO - __main__ - Global step 550 Train loss 0.272355 Classification-F1 0.49606299212598426 on epoch=68
06/02/2022 13:10:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.364008 on epoch=69
06/02/2022 13:10:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.207475 on epoch=71
06/02/2022 13:10:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.237106 on epoch=72
06/02/2022 13:10:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.192594 on epoch=73
06/02/2022 13:11:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.253720 on epoch=74
06/02/2022 13:11:03 - INFO - __main__ - Global step 600 Train loss 0.250981 Classification-F1 0.4666666666666667 on epoch=74
06/02/2022 13:11:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.192522 on epoch=76
06/02/2022 13:11:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.268977 on epoch=77
06/02/2022 13:11:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.185085 on epoch=78
06/02/2022 13:11:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.207221 on epoch=79
06/02/2022 13:11:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.143075 on epoch=81
06/02/2022 13:11:30 - INFO - __main__ - Global step 650 Train loss 0.199376 Classification-F1 0.2289156626506024 on epoch=81
06/02/2022 13:11:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.201339 on epoch=82
06/02/2022 13:11:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.159793 on epoch=83
06/02/2022 13:11:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.150194 on epoch=84
06/02/2022 13:11:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.192114 on epoch=86
06/02/2022 13:11:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.125320 on epoch=87
06/02/2022 13:11:56 - INFO - __main__ - Global step 700 Train loss 0.165752 Classification-F1 0.488 on epoch=87
06/02/2022 13:12:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.173457 on epoch=88
06/02/2022 13:12:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.217752 on epoch=89
06/02/2022 13:12:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.111463 on epoch=91
06/02/2022 13:12:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.102711 on epoch=92
06/02/2022 13:12:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.147596 on epoch=93
06/02/2022 13:12:22 - INFO - __main__ - Global step 750 Train loss 0.150596 Classification-F1 0.4796747967479675 on epoch=93
06/02/2022 13:12:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.193506 on epoch=94
06/02/2022 13:12:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.097129 on epoch=96
06/02/2022 13:12:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.062535 on epoch=97
06/02/2022 13:12:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.066608 on epoch=98
06/02/2022 13:12:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.107146 on epoch=99
06/02/2022 13:12:48 - INFO - __main__ - Global step 800 Train loss 0.105385 Classification-F1 0.49206349206349204 on epoch=99
06/02/2022 13:12:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.106093 on epoch=101
06/02/2022 13:12:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.104341 on epoch=102
06/02/2022 13:13:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.118780 on epoch=103
06/02/2022 13:13:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.085897 on epoch=104
06/02/2022 13:13:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.077743 on epoch=106
06/02/2022 13:13:14 - INFO - __main__ - Global step 850 Train loss 0.098571 Classification-F1 0.39622641509433965 on epoch=106
06/02/2022 13:13:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.128277 on epoch=107
06/02/2022 13:13:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.052964 on epoch=108
06/02/2022 13:13:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.064251 on epoch=109
06/02/2022 13:13:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.070408 on epoch=111
06/02/2022 13:13:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.126383 on epoch=112
06/02/2022 13:13:40 - INFO - __main__ - Global step 900 Train loss 0.088457 Classification-F1 0.488 on epoch=112
06/02/2022 13:13:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.096873 on epoch=113
06/02/2022 13:13:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.090155 on epoch=114
06/02/2022 13:13:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.079952 on epoch=116
06/02/2022 13:14:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.075953 on epoch=117
06/02/2022 13:14:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.061903 on epoch=118
06/02/2022 13:14:06 - INFO - __main__ - Global step 950 Train loss 0.080967 Classification-F1 0.46218487394957986 on epoch=118
06/02/2022 13:14:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.049487 on epoch=119
06/02/2022 13:14:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.081699 on epoch=121
06/02/2022 13:14:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.018554 on epoch=122
06/02/2022 13:14:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.046849 on epoch=123
06/02/2022 13:14:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.062418 on epoch=124
06/02/2022 13:14:32 - INFO - __main__ - Global step 1000 Train loss 0.051801 Classification-F1 0.488 on epoch=124
06/02/2022 13:14:32 - INFO - __main__ - save last model!
06/02/2022 13:14:33 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 13:14:33 - INFO - __main__ - Printing 3 examples
06/02/2022 13:14:33 - INFO - __main__ -  [ethos-religion] Women deserve to be abused, I guess.
06/02/2022 13:14:33 - INFO - __main__ - ['false']
06/02/2022 13:14:33 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
06/02/2022 13:14:33 - INFO - __main__ - ['false']
06/02/2022 13:14:33 - INFO - __main__ -  [ethos-religion] The only thing more repulsive than a nigger is a nigger faggot. Self-hating whites shower images of cross-dressed niggers with praise because the modern left fetishizes all forms of depravity.
06/02/2022 13:14:33 - INFO - __main__ - ['false']
06/02/2022 13:14:33 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:14:33 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:14:33 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 13:14:33 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:14:33 - INFO - __main__ - Printing 3 examples
06/02/2022 13:14:33 - INFO - __main__ -  [ethos-religion] Down syndrome? Problematic people must be dead
06/02/2022 13:14:33 - INFO - __main__ - ['false']
06/02/2022 13:14:33 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
06/02/2022 13:14:33 - INFO - __main__ - ['false']
06/02/2022 13:14:33 - INFO - __main__ -  [ethos-religion] Ugly long chin brainless problematic mother fucker he looks retarded
06/02/2022 13:14:33 - INFO - __main__ - ['false']
06/02/2022 13:14:33 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:14:33 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:14:33 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:14:40 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 13:14:40 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 13:14:40 - INFO - __main__ - Printing 3 examples
06/02/2022 13:14:40 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 13:14:40 - INFO - __main__ - ['false']
06/02/2022 13:14:40 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 13:14:40 - INFO - __main__ - ['false']
06/02/2022 13:14:40 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 13:14:40 - INFO - __main__ - ['true']
06/02/2022 13:14:40 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:14:40 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:14:40 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 13:14:42 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_42_0.0003_8_predictions.txt
06/02/2022 13:14:42 - INFO - __main__ - Classification-F1 on test data: 0.9142
06/02/2022 13:14:42 - INFO - __main__ - prefix=ethos-religion_64_42, lr=0.0003, bsz=8, dev_performance=0.49606299212598426, test_performance=0.9141842572499508
06/02/2022 13:14:42 - INFO - __main__ - Running ... prefix=ethos-religion_64_42, lr=0.0002, bsz=8 ...
06/02/2022 13:14:43 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 13:14:43 - INFO - __main__ - Printing 3 examples
06/02/2022 13:14:43 - INFO - __main__ -  [ethos-religion] Women deserve to be abused, I guess.
06/02/2022 13:14:43 - INFO - __main__ - ['false']
06/02/2022 13:14:43 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
06/02/2022 13:14:43 - INFO - __main__ - ['false']
06/02/2022 13:14:43 - INFO - __main__ -  [ethos-religion] The only thing more repulsive than a nigger is a nigger faggot. Self-hating whites shower images of cross-dressed niggers with praise because the modern left fetishizes all forms of depravity.
06/02/2022 13:14:43 - INFO - __main__ - ['false']
06/02/2022 13:14:43 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:14:43 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:14:43 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 13:14:43 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:14:43 - INFO - __main__ - Printing 3 examples
06/02/2022 13:14:43 - INFO - __main__ -  [ethos-religion] Down syndrome? Problematic people must be dead
06/02/2022 13:14:43 - INFO - __main__ - ['false']
06/02/2022 13:14:43 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
06/02/2022 13:14:43 - INFO - __main__ - ['false']
06/02/2022 13:14:43 - INFO - __main__ -  [ethos-religion] Ugly long chin brainless problematic mother fucker he looks retarded
06/02/2022 13:14:43 - INFO - __main__ - ['false']
06/02/2022 13:14:43 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:14:43 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:14:43 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:14:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 13:14:46 - INFO - __main__ - Starting training!
06/02/2022 13:14:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 13:14:56 - INFO - __main__ - Starting training!
06/02/2022 13:15:01 - INFO - __main__ - Step 10 Global step 10 Train loss 23.565531 on epoch=1
06/02/2022 13:15:06 - INFO - __main__ - Step 20 Global step 20 Train loss 19.860142 on epoch=2
06/02/2022 13:15:11 - INFO - __main__ - Step 30 Global step 30 Train loss 17.393234 on epoch=3
06/02/2022 13:15:16 - INFO - __main__ - Step 40 Global step 40 Train loss 16.728512 on epoch=4
06/02/2022 13:15:21 - INFO - __main__ - Step 50 Global step 50 Train loss 16.034786 on epoch=6
06/02/2022 13:15:35 - INFO - __main__ - Global step 50 Train loss 18.716440 Classification-F1 0.0 on epoch=6
06/02/2022 13:15:41 - INFO - __main__ - Step 60 Global step 60 Train loss 15.045380 on epoch=7
06/02/2022 13:15:46 - INFO - __main__ - Step 70 Global step 70 Train loss 14.369926 on epoch=8
06/02/2022 13:15:51 - INFO - __main__ - Step 80 Global step 80 Train loss 13.326594 on epoch=9
06/02/2022 13:15:56 - INFO - __main__ - Step 90 Global step 90 Train loss 13.521799 on epoch=11
06/02/2022 13:16:01 - INFO - __main__ - Step 100 Global step 100 Train loss 12.097852 on epoch=12
06/02/2022 13:16:02 - INFO - __main__ - Global step 100 Train loss 13.672310 Classification-F1 0.0 on epoch=12
06/02/2022 13:16:07 - INFO - __main__ - Step 110 Global step 110 Train loss 10.680143 on epoch=13
06/02/2022 13:16:12 - INFO - __main__ - Step 120 Global step 120 Train loss 3.024428 on epoch=14
06/02/2022 13:16:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.637175 on epoch=16
06/02/2022 13:16:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.524036 on epoch=17
06/02/2022 13:16:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.622837 on epoch=18
06/02/2022 13:16:29 - INFO - __main__ - Global step 150 Train loss 3.097724 Classification-F1 0.3786407766990291 on epoch=18
06/02/2022 13:16:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.415473 on epoch=19
06/02/2022 13:16:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.259207 on epoch=21
06/02/2022 13:16:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.833369 on epoch=22
06/02/2022 13:16:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.194464 on epoch=23
06/02/2022 13:16:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.673632 on epoch=24
06/02/2022 13:16:55 - INFO - __main__ - Global step 200 Train loss 0.475229 Classification-F1 0.49206349206349204 on epoch=24
06/02/2022 13:17:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.152841 on epoch=26
06/02/2022 13:17:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.151189 on epoch=27
06/02/2022 13:17:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.109264 on epoch=28
06/02/2022 13:17:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.197094 on epoch=29
06/02/2022 13:17:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.085439 on epoch=31
06/02/2022 13:17:23 - INFO - __main__ - Global step 250 Train loss 0.139165 Classification-F1 0.46218487394957986 on epoch=31
06/02/2022 13:17:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.124795 on epoch=32
06/02/2022 13:17:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.079595 on epoch=33
06/02/2022 13:17:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.088821 on epoch=34
06/02/2022 13:17:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.095922 on epoch=36
06/02/2022 13:17:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.092099 on epoch=37
06/02/2022 13:17:49 - INFO - __main__ - Global step 300 Train loss 0.096246 Classification-F1 0.4796747967479675 on epoch=37
06/02/2022 13:17:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.065511 on epoch=38
06/02/2022 13:17:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.043308 on epoch=39
06/02/2022 13:18:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.063070 on epoch=41
06/02/2022 13:18:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.032982 on epoch=42
06/02/2022 13:18:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.025486 on epoch=43
06/02/2022 13:18:15 - INFO - __main__ - Global step 350 Train loss 0.046071 Classification-F1 0.49206349206349204 on epoch=43
06/02/2022 13:18:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.042675 on epoch=44
06/02/2022 13:18:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.018577 on epoch=46
06/02/2022 13:18:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.019785 on epoch=47
06/02/2022 13:18:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.032987 on epoch=48
06/02/2022 13:18:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.040041 on epoch=49
06/02/2022 13:18:42 - INFO - __main__ - Global step 400 Train loss 0.030813 Classification-F1 0.49206349206349204 on epoch=49
06/02/2022 13:18:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.031616 on epoch=51
06/02/2022 13:18:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.036693 on epoch=52
06/02/2022 13:18:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.045584 on epoch=53
06/02/2022 13:19:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.006344 on epoch=54
06/02/2022 13:19:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.006485 on epoch=56
06/02/2022 13:19:09 - INFO - __main__ - Global step 450 Train loss 0.025344 Classification-F1 0.4838709677419355 on epoch=56
06/02/2022 13:19:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.004473 on epoch=57
06/02/2022 13:19:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.018550 on epoch=58
06/02/2022 13:19:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001668 on epoch=59
06/02/2022 13:19:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.003390 on epoch=61
06/02/2022 13:19:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.004475 on epoch=62
06/02/2022 13:19:35 - INFO - __main__ - Global step 500 Train loss 0.006511 Classification-F1 0.488 on epoch=62
06/02/2022 13:19:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.005809 on epoch=63
06/02/2022 13:19:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.004028 on epoch=64
06/02/2022 13:19:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.006814 on epoch=66
06/02/2022 13:19:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.011033 on epoch=67
06/02/2022 13:20:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.003664 on epoch=68
06/02/2022 13:20:01 - INFO - __main__ - Global step 550 Train loss 0.006269 Classification-F1 0.49206349206349204 on epoch=68
06/02/2022 13:20:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.027302 on epoch=69
06/02/2022 13:20:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.002495 on epoch=71
06/02/2022 13:20:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.005080 on epoch=72
06/02/2022 13:20:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.042220 on epoch=73
06/02/2022 13:20:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001639 on epoch=74
06/02/2022 13:20:27 - INFO - __main__ - Global step 600 Train loss 0.015747 Classification-F1 0.488 on epoch=74
06/02/2022 13:20:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.001304 on epoch=76
06/02/2022 13:20:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.006563 on epoch=77
06/02/2022 13:20:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.004674 on epoch=78
06/02/2022 13:20:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001724 on epoch=79
06/02/2022 13:20:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.005654 on epoch=81
06/02/2022 13:20:53 - INFO - __main__ - Global step 650 Train loss 0.003984 Classification-F1 0.488 on epoch=81
06/02/2022 13:20:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.001144 on epoch=82
06/02/2022 13:21:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.010661 on epoch=83
06/02/2022 13:21:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.006857 on epoch=84
06/02/2022 13:21:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.002044 on epoch=86
06/02/2022 13:21:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.008020 on epoch=87
06/02/2022 13:21:20 - INFO - __main__ - Global step 700 Train loss 0.005745 Classification-F1 0.49206349206349204 on epoch=87
06/02/2022 13:21:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.002181 on epoch=88
06/02/2022 13:21:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000682 on epoch=89
06/02/2022 13:21:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.014563 on epoch=91
06/02/2022 13:21:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000153 on epoch=92
06/02/2022 13:21:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.011634 on epoch=93
06/02/2022 13:21:46 - INFO - __main__ - Global step 750 Train loss 0.005842 Classification-F1 0.488 on epoch=93
06/02/2022 13:21:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.005485 on epoch=94
06/02/2022 13:21:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.008324 on epoch=96
06/02/2022 13:22:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000780 on epoch=97
06/02/2022 13:22:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000213 on epoch=98
06/02/2022 13:22:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000781 on epoch=99
06/02/2022 13:22:12 - INFO - __main__ - Global step 800 Train loss 0.003117 Classification-F1 0.49206349206349204 on epoch=99
06/02/2022 13:22:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000171 on epoch=101
06/02/2022 13:22:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.003008 on epoch=102
06/02/2022 13:22:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000035 on epoch=103
06/02/2022 13:22:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000138 on epoch=104
06/02/2022 13:22:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000071 on epoch=106
06/02/2022 13:22:38 - INFO - __main__ - Global step 850 Train loss 0.000685 Classification-F1 0.49206349206349204 on epoch=106
06/02/2022 13:22:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000033 on epoch=107
06/02/2022 13:22:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000658 on epoch=108
06/02/2022 13:22:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000323 on epoch=109
06/02/2022 13:22:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001477 on epoch=111
06/02/2022 13:23:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000239 on epoch=112
06/02/2022 13:23:05 - INFO - __main__ - Global step 900 Train loss 0.000546 Classification-F1 0.488 on epoch=112
06/02/2022 13:23:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000042 on epoch=113
06/02/2022 13:23:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000022 on epoch=114
06/02/2022 13:23:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.031096 on epoch=116
06/02/2022 13:23:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.001004 on epoch=117
06/02/2022 13:23:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000805 on epoch=118
06/02/2022 13:23:31 - INFO - __main__ - Global step 950 Train loss 0.006594 Classification-F1 0.488 on epoch=118
06/02/2022 13:23:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000300 on epoch=119
06/02/2022 13:23:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000089 on epoch=121
06/02/2022 13:23:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000218 on epoch=122
06/02/2022 13:23:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000054 on epoch=123
06/02/2022 13:23:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000731 on epoch=124
06/02/2022 13:23:57 - INFO - __main__ - Global step 1000 Train loss 0.000278 Classification-F1 0.488 on epoch=124
06/02/2022 13:23:57 - INFO - __main__ - save last model!
06/02/2022 13:23:58 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 13:23:58 - INFO - __main__ - Printing 3 examples
06/02/2022 13:23:58 - INFO - __main__ -  [ethos-religion] Women deserve to be abused, I guess.
06/02/2022 13:23:58 - INFO - __main__ - ['false']
06/02/2022 13:23:58 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
06/02/2022 13:23:58 - INFO - __main__ - ['false']
06/02/2022 13:23:58 - INFO - __main__ -  [ethos-religion] The only thing more repulsive than a nigger is a nigger faggot. Self-hating whites shower images of cross-dressed niggers with praise because the modern left fetishizes all forms of depravity.
06/02/2022 13:23:58 - INFO - __main__ - ['false']
06/02/2022 13:23:58 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:23:58 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:23:58 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 13:23:58 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:23:58 - INFO - __main__ - Printing 3 examples
06/02/2022 13:23:58 - INFO - __main__ -  [ethos-religion] Down syndrome? Problematic people must be dead
06/02/2022 13:23:58 - INFO - __main__ - ['false']
06/02/2022 13:23:58 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
06/02/2022 13:23:58 - INFO - __main__ - ['false']
06/02/2022 13:23:58 - INFO - __main__ -  [ethos-religion] Ugly long chin brainless problematic mother fucker he looks retarded
06/02/2022 13:23:58 - INFO - __main__ - ['false']
06/02/2022 13:23:58 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:23:58 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:23:58 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:24:04 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 13:24:05 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 13:24:05 - INFO - __main__ - Printing 3 examples
06/02/2022 13:24:05 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 13:24:05 - INFO - __main__ - ['false']
06/02/2022 13:24:05 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 13:24:05 - INFO - __main__ - ['false']
06/02/2022 13:24:05 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 13:24:05 - INFO - __main__ - ['true']
06/02/2022 13:24:05 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:24:05 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:24:05 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 13:24:07 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_42_0.0002_8_predictions.txt
06/02/2022 13:24:07 - INFO - __main__ - Classification-F1 on test data: 0.9351
06/02/2022 13:24:07 - INFO - __main__ - prefix=ethos-religion_64_42, lr=0.0002, bsz=8, dev_performance=0.49206349206349204, test_performance=0.9350746268656716
06/02/2022 13:24:07 - INFO - __main__ - Running ... prefix=ethos-religion_64_42, lr=0.0001, bsz=8 ...
06/02/2022 13:24:08 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 13:24:08 - INFO - __main__ - Printing 3 examples
06/02/2022 13:24:08 - INFO - __main__ -  [ethos-religion] Women deserve to be abused, I guess.
06/02/2022 13:24:08 - INFO - __main__ - ['false']
06/02/2022 13:24:08 - INFO - __main__ -  [ethos-religion] How can Europe allow millions of rapists animals to go there ?
06/02/2022 13:24:08 - INFO - __main__ - ['false']
06/02/2022 13:24:08 - INFO - __main__ -  [ethos-religion] The only thing more repulsive than a nigger is a nigger faggot. Self-hating whites shower images of cross-dressed niggers with praise because the modern left fetishizes all forms of depravity.
06/02/2022 13:24:08 - INFO - __main__ - ['false']
06/02/2022 13:24:08 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:24:08 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:24:08 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 13:24:08 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:24:08 - INFO - __main__ - Printing 3 examples
06/02/2022 13:24:08 - INFO - __main__ -  [ethos-religion] Down syndrome? Problematic people must be dead
06/02/2022 13:24:08 - INFO - __main__ - ['false']
06/02/2022 13:24:08 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
06/02/2022 13:24:08 - INFO - __main__ - ['false']
06/02/2022 13:24:08 - INFO - __main__ -  [ethos-religion] Ugly long chin brainless problematic mother fucker he looks retarded
06/02/2022 13:24:08 - INFO - __main__ - ['false']
06/02/2022 13:24:08 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:24:08 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:24:08 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:24:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 13:24:11 - INFO - __main__ - Starting training!
06/02/2022 13:24:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 13:24:19 - INFO - __main__ - Starting training!
06/02/2022 13:24:24 - INFO - __main__ - Step 10 Global step 10 Train loss 23.922138 on epoch=1
06/02/2022 13:24:29 - INFO - __main__ - Step 20 Global step 20 Train loss 23.628195 on epoch=2
06/02/2022 13:24:34 - INFO - __main__ - Step 30 Global step 30 Train loss 19.237539 on epoch=3
06/02/2022 13:24:39 - INFO - __main__ - Step 40 Global step 40 Train loss 18.891544 on epoch=4
06/02/2022 13:24:45 - INFO - __main__ - Step 50 Global step 50 Train loss 18.597635 on epoch=6
06/02/2022 13:25:05 - INFO - __main__ - Global step 50 Train loss 20.855413 Classification-F1 0.0 on epoch=6
06/02/2022 13:25:11 - INFO - __main__ - Step 60 Global step 60 Train loss 16.950081 on epoch=7
06/02/2022 13:25:16 - INFO - __main__ - Step 70 Global step 70 Train loss 16.784056 on epoch=8
06/02/2022 13:25:21 - INFO - __main__ - Step 80 Global step 80 Train loss 16.322880 on epoch=9
06/02/2022 13:25:27 - INFO - __main__ - Step 90 Global step 90 Train loss 15.606755 on epoch=11
06/02/2022 13:25:32 - INFO - __main__ - Step 100 Global step 100 Train loss 15.699804 on epoch=12
06/02/2022 13:25:51 - INFO - __main__ - Global step 100 Train loss 16.272715 Classification-F1 0.0 on epoch=12
06/02/2022 13:25:56 - INFO - __main__ - Step 110 Global step 110 Train loss 15.331573 on epoch=13
06/02/2022 13:26:01 - INFO - __main__ - Step 120 Global step 120 Train loss 15.485042 on epoch=14
06/02/2022 13:26:07 - INFO - __main__ - Step 130 Global step 130 Train loss 14.954241 on epoch=16
06/02/2022 13:26:12 - INFO - __main__ - Step 140 Global step 140 Train loss 14.262117 on epoch=17
06/02/2022 13:26:17 - INFO - __main__ - Step 150 Global step 150 Train loss 13.574984 on epoch=18
06/02/2022 13:26:36 - INFO - __main__ - Global step 150 Train loss 14.721592 Classification-F1 0.0 on epoch=18
06/02/2022 13:26:41 - INFO - __main__ - Step 160 Global step 160 Train loss 13.800575 on epoch=19
06/02/2022 13:26:47 - INFO - __main__ - Step 170 Global step 170 Train loss 12.774415 on epoch=21
06/02/2022 13:26:52 - INFO - __main__ - Step 180 Global step 180 Train loss 12.577347 on epoch=22
06/02/2022 13:26:57 - INFO - __main__ - Step 190 Global step 190 Train loss 11.673074 on epoch=23
06/02/2022 13:27:03 - INFO - __main__ - Step 200 Global step 200 Train loss 10.982637 on epoch=24
06/02/2022 13:27:21 - INFO - __main__ - Global step 200 Train loss 12.361610 Classification-F1 0.0 on epoch=24
06/02/2022 13:27:26 - INFO - __main__ - Step 210 Global step 210 Train loss 10.832663 on epoch=26
06/02/2022 13:27:32 - INFO - __main__ - Step 220 Global step 220 Train loss 8.735350 on epoch=27
06/02/2022 13:27:37 - INFO - __main__ - Step 230 Global step 230 Train loss 7.653944 on epoch=28
06/02/2022 13:27:43 - INFO - __main__ - Step 240 Global step 240 Train loss 5.380412 on epoch=29
06/02/2022 13:27:48 - INFO - __main__ - Step 250 Global step 250 Train loss 7.452082 on epoch=31
06/02/2022 13:27:48 - INFO - __main__ - Global step 250 Train loss 8.010889 Classification-F1 0.10526315789473684 on epoch=31
06/02/2022 13:27:54 - INFO - __main__ - Step 260 Global step 260 Train loss 2.417215 on epoch=32
06/02/2022 13:28:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.978119 on epoch=33
06/02/2022 13:28:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.777479 on epoch=34
06/02/2022 13:28:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.519713 on epoch=36
06/02/2022 13:28:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.565348 on epoch=37
06/02/2022 13:28:16 - INFO - __main__ - Global step 300 Train loss 1.051575 Classification-F1 0.4576271186440678 on epoch=37
06/02/2022 13:28:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.600982 on epoch=38
06/02/2022 13:28:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.524659 on epoch=39
06/02/2022 13:28:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.423710 on epoch=41
06/02/2022 13:28:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.624633 on epoch=42
06/02/2022 13:28:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.418059 on epoch=43
06/02/2022 13:28:44 - INFO - __main__ - Global step 350 Train loss 0.518409 Classification-F1 0.4576271186440678 on epoch=43
06/02/2022 13:28:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.443738 on epoch=44
06/02/2022 13:28:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.434388 on epoch=46
06/02/2022 13:29:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.343778 on epoch=47
06/02/2022 13:29:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.254066 on epoch=48
06/02/2022 13:29:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.273070 on epoch=49
06/02/2022 13:29:11 - INFO - __main__ - Global step 400 Train loss 0.349808 Classification-F1 0.47540983606557374 on epoch=49
06/02/2022 13:29:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.311987 on epoch=51
06/02/2022 13:29:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.341462 on epoch=52
06/02/2022 13:29:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.161405 on epoch=53
06/02/2022 13:29:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.233651 on epoch=54
06/02/2022 13:29:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.185239 on epoch=56
06/02/2022 13:29:39 - INFO - __main__ - Global step 450 Train loss 0.246749 Classification-F1 0.4434782608695652 on epoch=56
06/02/2022 13:29:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.189587 on epoch=57
06/02/2022 13:29:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.145270 on epoch=58
06/02/2022 13:29:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.205094 on epoch=59
06/02/2022 13:30:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.195510 on epoch=61
06/02/2022 13:30:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.168408 on epoch=62
06/02/2022 13:30:06 - INFO - __main__ - Global step 500 Train loss 0.180774 Classification-F1 0.488 on epoch=62
06/02/2022 13:30:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.325979 on epoch=63
06/02/2022 13:30:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.200292 on epoch=64
06/02/2022 13:30:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.135021 on epoch=66
06/02/2022 13:30:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.169586 on epoch=67
06/02/2022 13:30:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.126419 on epoch=68
06/02/2022 13:30:33 - INFO - __main__ - Global step 550 Train loss 0.191459 Classification-F1 0.4838709677419355 on epoch=68
06/02/2022 13:30:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.135277 on epoch=69
06/02/2022 13:30:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.092627 on epoch=71
06/02/2022 13:30:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.172937 on epoch=72
06/02/2022 13:30:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.121833 on epoch=73
06/02/2022 13:31:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.096493 on epoch=74
06/02/2022 13:31:00 - INFO - __main__ - Global step 600 Train loss 0.123833 Classification-F1 0.49606299212598426 on epoch=74
06/02/2022 13:31:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.107007 on epoch=76
06/02/2022 13:31:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.075362 on epoch=77
06/02/2022 13:31:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.103211 on epoch=78
06/02/2022 13:31:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.081353 on epoch=79
06/02/2022 13:31:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.046644 on epoch=81
06/02/2022 13:31:28 - INFO - __main__ - Global step 650 Train loss 0.082716 Classification-F1 0.452991452991453 on epoch=81
06/02/2022 13:31:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.036168 on epoch=82
06/02/2022 13:31:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.103533 on epoch=83
06/02/2022 13:31:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.155753 on epoch=84
06/02/2022 13:31:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.058784 on epoch=86
06/02/2022 13:31:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.097949 on epoch=87
06/02/2022 13:31:55 - INFO - __main__ - Global step 700 Train loss 0.090437 Classification-F1 0.49206349206349204 on epoch=87
06/02/2022 13:32:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.127250 on epoch=88
06/02/2022 13:32:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.053093 on epoch=89
06/02/2022 13:32:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.034765 on epoch=91
06/02/2022 13:32:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.049942 on epoch=92
06/02/2022 13:32:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.096497 on epoch=93
06/02/2022 13:32:21 - INFO - __main__ - Global step 750 Train loss 0.072309 Classification-F1 0.4576271186440678 on epoch=93
06/02/2022 13:32:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.044836 on epoch=94
06/02/2022 13:32:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.063773 on epoch=96
06/02/2022 13:32:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.079015 on epoch=97
06/02/2022 13:32:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.074794 on epoch=98
06/02/2022 13:32:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.031880 on epoch=99
06/02/2022 13:32:48 - INFO - __main__ - Global step 800 Train loss 0.058859 Classification-F1 0.49206349206349204 on epoch=99
06/02/2022 13:32:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.051994 on epoch=101
06/02/2022 13:32:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.027530 on epoch=102
06/02/2022 13:33:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.037501 on epoch=103
06/02/2022 13:33:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.035175 on epoch=104
06/02/2022 13:33:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.026846 on epoch=106
06/02/2022 13:33:15 - INFO - __main__ - Global step 850 Train loss 0.035809 Classification-F1 0.4838709677419355 on epoch=106
06/02/2022 13:33:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.048669 on epoch=107
06/02/2022 13:33:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.026795 on epoch=108
06/02/2022 13:33:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.053318 on epoch=109
06/02/2022 13:33:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.017623 on epoch=111
06/02/2022 13:33:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.073856 on epoch=112
06/02/2022 13:33:42 - INFO - __main__ - Global step 900 Train loss 0.044052 Classification-F1 0.488 on epoch=112
06/02/2022 13:33:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.021101 on epoch=113
06/02/2022 13:33:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.028187 on epoch=114
06/02/2022 13:33:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.007776 on epoch=116
06/02/2022 13:34:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.010005 on epoch=117
06/02/2022 13:34:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.019531 on epoch=118
06/02/2022 13:34:08 - INFO - __main__ - Global step 950 Train loss 0.017320 Classification-F1 0.49206349206349204 on epoch=118
06/02/2022 13:34:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.005871 on epoch=119
06/02/2022 13:34:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.030273 on epoch=121
06/02/2022 13:34:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.012036 on epoch=122
06/02/2022 13:34:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.012206 on epoch=123
06/02/2022 13:34:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.004477 on epoch=124
06/02/2022 13:34:35 - INFO - __main__ - Global step 1000 Train loss 0.012973 Classification-F1 0.488 on epoch=124
06/02/2022 13:34:35 - INFO - __main__ - save last model!
06/02/2022 13:34:36 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 13:34:36 - INFO - __main__ - Printing 3 examples
06/02/2022 13:34:36 - INFO - __main__ -  [ethos-religion] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
06/02/2022 13:34:36 - INFO - __main__ - ['false']
06/02/2022 13:34:36 - INFO - __main__ -  [ethos-religion] Honestly, go fuck yourself! bitch!
06/02/2022 13:34:36 - INFO - __main__ - ['false']
06/02/2022 13:34:36 - INFO - __main__ -  [ethos-religion] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
06/02/2022 13:34:36 - INFO - __main__ - ['false']
06/02/2022 13:34:36 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:34:36 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:34:36 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 13:34:36 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:34:36 - INFO - __main__ - Printing 3 examples
06/02/2022 13:34:36 - INFO - __main__ -  [ethos-religion] fuck bulgaria stupid country
06/02/2022 13:34:36 - INFO - __main__ - ['false']
06/02/2022 13:34:36 - INFO - __main__ -  [ethos-religion] Indian Ned Flanders. You look like you professionally harrass subpar women on Facebook
06/02/2022 13:34:36 - INFO - __main__ - ['false']
06/02/2022 13:34:36 - INFO - __main__ -  [ethos-religion] Trans people=gays and loosers
06/02/2022 13:34:36 - INFO - __main__ - ['false']
06/02/2022 13:34:36 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:34:36 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:34:36 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:34:42 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 13:34:43 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 13:34:43 - INFO - __main__ - Printing 3 examples
06/02/2022 13:34:43 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 13:34:43 - INFO - __main__ - ['false']
06/02/2022 13:34:43 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 13:34:43 - INFO - __main__ - ['false']
06/02/2022 13:34:43 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 13:34:43 - INFO - __main__ - ['true']
06/02/2022 13:34:43 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:34:43 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:34:43 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 13:34:45 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_42_0.0001_8_predictions.txt
06/02/2022 13:34:45 - INFO - __main__ - Classification-F1 on test data: 0.9663
06/02/2022 13:34:45 - INFO - __main__ - prefix=ethos-religion_64_42, lr=0.0001, bsz=8, dev_performance=0.49606299212598426, test_performance=0.9663312693498451
06/02/2022 13:34:45 - INFO - __main__ - Running ... prefix=ethos-religion_64_87, lr=0.0005, bsz=8 ...
06/02/2022 13:34:46 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 13:34:46 - INFO - __main__ - Printing 3 examples
06/02/2022 13:34:46 - INFO - __main__ -  [ethos-religion] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
06/02/2022 13:34:46 - INFO - __main__ - ['false']
06/02/2022 13:34:46 - INFO - __main__ -  [ethos-religion] Honestly, go fuck yourself! bitch!
06/02/2022 13:34:46 - INFO - __main__ - ['false']
06/02/2022 13:34:46 - INFO - __main__ -  [ethos-religion] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
06/02/2022 13:34:46 - INFO - __main__ - ['false']
06/02/2022 13:34:46 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:34:46 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:34:46 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 13:34:46 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:34:46 - INFO - __main__ - Printing 3 examples
06/02/2022 13:34:46 - INFO - __main__ -  [ethos-religion] fuck bulgaria stupid country
06/02/2022 13:34:46 - INFO - __main__ - ['false']
06/02/2022 13:34:46 - INFO - __main__ -  [ethos-religion] Indian Ned Flanders. You look like you professionally harrass subpar women on Facebook
06/02/2022 13:34:46 - INFO - __main__ - ['false']
06/02/2022 13:34:46 - INFO - __main__ -  [ethos-religion] Trans people=gays and loosers
06/02/2022 13:34:46 - INFO - __main__ - ['false']
06/02/2022 13:34:46 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:34:46 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:34:46 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:34:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 13:34:48 - INFO - __main__ - Starting training!
06/02/2022 13:34:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 13:34:59 - INFO - __main__ - Starting training!
06/02/2022 13:35:03 - INFO - __main__ - Step 10 Global step 10 Train loss 23.553844 on epoch=1
06/02/2022 13:35:09 - INFO - __main__ - Step 20 Global step 20 Train loss 18.702885 on epoch=2
06/02/2022 13:35:14 - INFO - __main__ - Step 30 Global step 30 Train loss 15.433928 on epoch=3
06/02/2022 13:35:19 - INFO - __main__ - Step 40 Global step 40 Train loss 12.995107 on epoch=4
06/02/2022 13:35:24 - INFO - __main__ - Step 50 Global step 50 Train loss 11.440063 on epoch=6
06/02/2022 13:35:31 - INFO - __main__ - Global step 50 Train loss 16.425165 Classification-F1 0.0 on epoch=6
06/02/2022 13:35:36 - INFO - __main__ - Step 60 Global step 60 Train loss 6.100035 on epoch=7
06/02/2022 13:35:41 - INFO - __main__ - Step 70 Global step 70 Train loss 1.996083 on epoch=8
06/02/2022 13:35:46 - INFO - __main__ - Step 80 Global step 80 Train loss 0.813670 on epoch=9
06/02/2022 13:35:51 - INFO - __main__ - Step 90 Global step 90 Train loss 0.616206 on epoch=11
06/02/2022 13:35:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.421803 on epoch=12
06/02/2022 13:35:57 - INFO - __main__ - Global step 100 Train loss 1.989559 Classification-F1 0.058823529411764705 on epoch=12
06/02/2022 13:36:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.368125 on epoch=13
06/02/2022 13:36:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.311062 on epoch=14
06/02/2022 13:36:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.267700 on epoch=16
06/02/2022 13:36:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.200977 on epoch=17
06/02/2022 13:36:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.179502 on epoch=18
06/02/2022 13:36:25 - INFO - __main__ - Global step 150 Train loss 0.265473 Classification-F1 0.4434782608695652 on epoch=18
06/02/2022 13:36:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.156765 on epoch=19
06/02/2022 13:36:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.141443 on epoch=21
06/02/2022 13:36:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.139033 on epoch=22
06/02/2022 13:36:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.125944 on epoch=23
06/02/2022 13:36:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.115552 on epoch=24
06/02/2022 13:36:52 - INFO - __main__ - Global step 200 Train loss 0.135748 Classification-F1 0.4666666666666667 on epoch=24
06/02/2022 13:36:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.151346 on epoch=26
06/02/2022 13:37:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.079034 on epoch=27
06/02/2022 13:37:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.084564 on epoch=28
06/02/2022 13:37:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.100210 on epoch=29
06/02/2022 13:37:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.981352 on epoch=31
06/02/2022 13:37:19 - INFO - __main__ - Global step 250 Train loss 0.279301 Classification-F1 0.18726591760299627 on epoch=31
06/02/2022 13:37:24 - INFO - __main__ - Step 260 Global step 260 Train loss 2.727276 on epoch=32
06/02/2022 13:37:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.956222 on epoch=33
06/02/2022 13:37:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.187100 on epoch=34
06/02/2022 13:37:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.336389 on epoch=36
06/02/2022 13:37:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.275223 on epoch=37
06/02/2022 13:37:46 - INFO - __main__ - Global step 300 Train loss 0.896442 Classification-F1 0.4482758620689655 on epoch=37
06/02/2022 13:37:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.256797 on epoch=38
06/02/2022 13:37:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.287429 on epoch=39
06/02/2022 13:38:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.214812 on epoch=41
06/02/2022 13:38:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.210577 on epoch=42
06/02/2022 13:38:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.146786 on epoch=43
06/02/2022 13:38:12 - INFO - __main__ - Global step 350 Train loss 0.223280 Classification-F1 0.47540983606557374 on epoch=43
06/02/2022 13:38:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.366333 on epoch=44
06/02/2022 13:38:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.169622 on epoch=46
06/02/2022 13:38:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.152786 on epoch=47
06/02/2022 13:38:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.258664 on epoch=48
06/02/2022 13:38:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.123441 on epoch=49
06/02/2022 13:38:40 - INFO - __main__ - Global step 400 Train loss 0.214169 Classification-F1 0.4576271186440678 on epoch=49
06/02/2022 13:38:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.088836 on epoch=51
06/02/2022 13:38:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.265335 on epoch=52
06/02/2022 13:38:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.069809 on epoch=53
06/02/2022 13:39:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.087460 on epoch=54
06/02/2022 13:39:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.148352 on epoch=56
06/02/2022 13:39:06 - INFO - __main__ - Global step 450 Train loss 0.131958 Classification-F1 0.38461538461538464 on epoch=56
06/02/2022 13:39:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.117893 on epoch=57
06/02/2022 13:39:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.097650 on epoch=58
06/02/2022 13:39:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.073053 on epoch=59
06/02/2022 13:39:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.087774 on epoch=61
06/02/2022 13:39:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.065410 on epoch=62
06/02/2022 13:39:33 - INFO - __main__ - Global step 500 Train loss 0.088356 Classification-F1 0.4796747967479675 on epoch=62
06/02/2022 13:39:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.071403 on epoch=63
06/02/2022 13:39:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.094473 on epoch=64
06/02/2022 13:39:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.095855 on epoch=66
06/02/2022 13:39:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.071689 on epoch=67
06/02/2022 13:39:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.093054 on epoch=68
06/02/2022 13:40:00 - INFO - __main__ - Global step 550 Train loss 0.085295 Classification-F1 0.4666666666666667 on epoch=68
06/02/2022 13:40:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.096241 on epoch=69
06/02/2022 13:40:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.058095 on epoch=71
06/02/2022 13:40:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.129074 on epoch=72
06/02/2022 13:40:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.121469 on epoch=73
06/02/2022 13:40:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.101555 on epoch=74
06/02/2022 13:40:27 - INFO - __main__ - Global step 600 Train loss 0.101287 Classification-F1 0.42857142857142855 on epoch=74
06/02/2022 13:40:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.074976 on epoch=76
06/02/2022 13:40:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.079826 on epoch=77
06/02/2022 13:40:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.025959 on epoch=78
06/02/2022 13:40:47 - INFO - __main__ - Step 640 Global step 640 Train loss 0.017302 on epoch=79
06/02/2022 13:40:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.135481 on epoch=81
06/02/2022 13:40:53 - INFO - __main__ - Global step 650 Train loss 0.066709 Classification-F1 0.47107438016528924 on epoch=81
06/02/2022 13:40:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.033361 on epoch=82
06/02/2022 13:41:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.022547 on epoch=83
06/02/2022 13:41:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.043179 on epoch=84
06/02/2022 13:41:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.006198 on epoch=86
06/02/2022 13:41:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.053918 on epoch=87
06/02/2022 13:41:20 - INFO - __main__ - Global step 700 Train loss 0.031841 Classification-F1 0.47107438016528924 on epoch=87
06/02/2022 13:41:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.003838 on epoch=88
06/02/2022 13:41:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.016029 on epoch=89
06/02/2022 13:41:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.018810 on epoch=91
06/02/2022 13:41:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.008716 on epoch=92
06/02/2022 13:41:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.007180 on epoch=93
06/02/2022 13:41:47 - INFO - __main__ - Global step 750 Train loss 0.010915 Classification-F1 0.47540983606557374 on epoch=93
06/02/2022 13:41:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.005058 on epoch=94
06/02/2022 13:41:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.011932 on epoch=96
06/02/2022 13:42:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000669 on epoch=97
06/02/2022 13:42:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.004930 on epoch=98
06/02/2022 13:42:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.004670 on epoch=99
06/02/2022 13:42:13 - INFO - __main__ - Global step 800 Train loss 0.005452 Classification-F1 0.47540983606557374 on epoch=99
06/02/2022 13:42:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000952 on epoch=101
06/02/2022 13:42:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000175 on epoch=102
06/02/2022 13:42:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000654 on epoch=103
06/02/2022 13:42:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.001839 on epoch=104
06/02/2022 13:42:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001145 on epoch=106
06/02/2022 13:42:40 - INFO - __main__ - Global step 850 Train loss 0.000953 Classification-F1 0.47107438016528924 on epoch=106
06/02/2022 13:42:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.002617 on epoch=107
06/02/2022 13:42:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000071 on epoch=108
06/02/2022 13:42:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.003096 on epoch=109
06/02/2022 13:43:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001127 on epoch=111
06/02/2022 13:43:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.007869 on epoch=112
06/02/2022 13:43:06 - INFO - __main__ - Global step 900 Train loss 0.002956 Classification-F1 0.47107438016528924 on epoch=112
06/02/2022 13:43:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.127308 on epoch=113
06/02/2022 13:43:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.005800 on epoch=114
06/02/2022 13:43:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.007628 on epoch=116
06/02/2022 13:43:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.023889 on epoch=117
06/02/2022 13:43:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.076803 on epoch=118
06/02/2022 13:43:32 - INFO - __main__ - Global step 950 Train loss 0.048285 Classification-F1 0.47540983606557374 on epoch=118
06/02/2022 13:43:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.023706 on epoch=119
06/02/2022 13:43:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000841 on epoch=121
06/02/2022 13:43:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.012020 on epoch=122
06/02/2022 13:43:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.225956 on epoch=123
06/02/2022 13:43:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.068580 on epoch=124
06/02/2022 13:43:58 - INFO - __main__ - Global step 1000 Train loss 0.066221 Classification-F1 0.47107438016528924 on epoch=124
06/02/2022 13:43:58 - INFO - __main__ - save last model!
06/02/2022 13:43:58 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 13:43:58 - INFO - __main__ - Printing 3 examples
06/02/2022 13:43:58 - INFO - __main__ -  [ethos-religion] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
06/02/2022 13:43:58 - INFO - __main__ - ['false']
06/02/2022 13:43:58 - INFO - __main__ -  [ethos-religion] Honestly, go fuck yourself! bitch!
06/02/2022 13:43:58 - INFO - __main__ - ['false']
06/02/2022 13:43:58 - INFO - __main__ -  [ethos-religion] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
06/02/2022 13:43:58 - INFO - __main__ - ['false']
06/02/2022 13:43:58 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:43:59 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:43:59 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 13:43:59 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:43:59 - INFO - __main__ - Printing 3 examples
06/02/2022 13:43:59 - INFO - __main__ -  [ethos-religion] fuck bulgaria stupid country
06/02/2022 13:43:59 - INFO - __main__ - ['false']
06/02/2022 13:43:59 - INFO - __main__ -  [ethos-religion] Indian Ned Flanders. You look like you professionally harrass subpar women on Facebook
06/02/2022 13:43:59 - INFO - __main__ - ['false']
06/02/2022 13:43:59 - INFO - __main__ -  [ethos-religion] Trans people=gays and loosers
06/02/2022 13:43:59 - INFO - __main__ - ['false']
06/02/2022 13:43:59 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:43:59 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:43:59 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:44:05 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 13:44:06 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 13:44:06 - INFO - __main__ - Printing 3 examples
06/02/2022 13:44:06 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 13:44:06 - INFO - __main__ - ['false']
06/02/2022 13:44:06 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 13:44:06 - INFO - __main__ - ['false']
06/02/2022 13:44:06 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 13:44:06 - INFO - __main__ - ['true']
06/02/2022 13:44:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:44:06 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:44:06 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 13:44:07 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_87_0.0005_8_predictions.txt
06/02/2022 13:44:07 - INFO - __main__ - Classification-F1 on test data: 0.9351
06/02/2022 13:44:08 - INFO - __main__ - prefix=ethos-religion_64_87, lr=0.0005, bsz=8, dev_performance=0.4796747967479675, test_performance=0.9350746268656716
06/02/2022 13:44:08 - INFO - __main__ - Running ... prefix=ethos-religion_64_87, lr=0.0003, bsz=8 ...
06/02/2022 13:44:09 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 13:44:09 - INFO - __main__ - Printing 3 examples
06/02/2022 13:44:09 - INFO - __main__ -  [ethos-religion] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
06/02/2022 13:44:09 - INFO - __main__ - ['false']
06/02/2022 13:44:09 - INFO - __main__ -  [ethos-religion] Honestly, go fuck yourself! bitch!
06/02/2022 13:44:09 - INFO - __main__ - ['false']
06/02/2022 13:44:09 - INFO - __main__ -  [ethos-religion] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
06/02/2022 13:44:09 - INFO - __main__ - ['false']
06/02/2022 13:44:09 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:44:09 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:44:09 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 13:44:09 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:44:09 - INFO - __main__ - Printing 3 examples
06/02/2022 13:44:09 - INFO - __main__ -  [ethos-religion] fuck bulgaria stupid country
06/02/2022 13:44:09 - INFO - __main__ - ['false']
06/02/2022 13:44:09 - INFO - __main__ -  [ethos-religion] Indian Ned Flanders. You look like you professionally harrass subpar women on Facebook
06/02/2022 13:44:09 - INFO - __main__ - ['false']
06/02/2022 13:44:09 - INFO - __main__ -  [ethos-religion] Trans people=gays and loosers
06/02/2022 13:44:09 - INFO - __main__ - ['false']
06/02/2022 13:44:09 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:44:09 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:44:09 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:44:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 13:44:10 - INFO - __main__ - Starting training!
06/02/2022 13:44:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 13:44:22 - INFO - __main__ - Starting training!
06/02/2022 13:44:26 - INFO - __main__ - Step 10 Global step 10 Train loss 24.214605 on epoch=1
06/02/2022 13:44:31 - INFO - __main__ - Step 20 Global step 20 Train loss 19.431469 on epoch=2
06/02/2022 13:44:36 - INFO - __main__ - Step 30 Global step 30 Train loss 17.754665 on epoch=3
06/02/2022 13:44:41 - INFO - __main__ - Step 40 Global step 40 Train loss 15.864054 on epoch=4
06/02/2022 13:44:46 - INFO - __main__ - Step 50 Global step 50 Train loss 14.465167 on epoch=6
06/02/2022 13:45:06 - INFO - __main__ - Global step 50 Train loss 18.345993 Classification-F1 0.0 on epoch=6
06/02/2022 13:45:11 - INFO - __main__ - Step 60 Global step 60 Train loss 13.881757 on epoch=7
06/02/2022 13:45:16 - INFO - __main__ - Step 70 Global step 70 Train loss 12.960116 on epoch=8
06/02/2022 13:45:21 - INFO - __main__ - Step 80 Global step 80 Train loss 9.479233 on epoch=9
06/02/2022 13:45:26 - INFO - __main__ - Step 90 Global step 90 Train loss 3.783683 on epoch=11
06/02/2022 13:45:31 - INFO - __main__ - Step 100 Global step 100 Train loss 2.155798 on epoch=12
06/02/2022 13:45:32 - INFO - __main__ - Global step 100 Train loss 8.452117 Classification-F1 0.07246376811594203 on epoch=12
06/02/2022 13:45:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.775942 on epoch=13
06/02/2022 13:45:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.455532 on epoch=14
06/02/2022 13:45:48 - INFO - __main__ - Step 130 Global step 130 Train loss 0.495731 on epoch=16
06/02/2022 13:45:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.591489 on epoch=17
06/02/2022 13:45:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.552794 on epoch=18
06/02/2022 13:45:59 - INFO - __main__ - Global step 150 Train loss 0.574298 Classification-F1 0.452991452991453 on epoch=18
06/02/2022 13:46:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.409623 on epoch=19
06/02/2022 13:46:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.388635 on epoch=21
06/02/2022 13:46:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.431416 on epoch=22
06/02/2022 13:46:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.358777 on epoch=23
06/02/2022 13:46:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.455723 on epoch=24
06/02/2022 13:46:26 - INFO - __main__ - Global step 200 Train loss 0.408835 Classification-F1 0.189873417721519 on epoch=24
06/02/2022 13:46:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.398141 on epoch=26
06/02/2022 13:46:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.470917 on epoch=27
06/02/2022 13:46:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.401598 on epoch=28
06/02/2022 13:46:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.350158 on epoch=29
06/02/2022 13:46:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.389386 on epoch=31
06/02/2022 13:46:52 - INFO - __main__ - Global step 250 Train loss 0.402040 Classification-F1 0.15789473684210525 on epoch=31
06/02/2022 13:46:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.333556 on epoch=32
06/02/2022 13:47:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.325341 on epoch=33
06/02/2022 13:47:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.337167 on epoch=34
06/02/2022 13:47:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.329403 on epoch=36
06/02/2022 13:47:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.332240 on epoch=37
06/02/2022 13:47:19 - INFO - __main__ - Global step 300 Train loss 0.331541 Classification-F1 0.42857142857142855 on epoch=37
06/02/2022 13:47:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.285093 on epoch=38
06/02/2022 13:47:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.269791 on epoch=39
06/02/2022 13:47:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.233621 on epoch=41
06/02/2022 13:47:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.287430 on epoch=42
06/02/2022 13:47:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.235613 on epoch=43
06/02/2022 13:47:45 - INFO - __main__ - Global step 350 Train loss 0.262310 Classification-F1 0.488 on epoch=43
06/02/2022 13:47:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.264501 on epoch=44
06/02/2022 13:47:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.204004 on epoch=46
06/02/2022 13:48:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.277926 on epoch=47
06/02/2022 13:48:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.176139 on epoch=48
06/02/2022 13:48:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.191522 on epoch=49
06/02/2022 13:48:12 - INFO - __main__ - Global step 400 Train loss 0.222818 Classification-F1 0.3904761904761905 on epoch=49
06/02/2022 13:48:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.223596 on epoch=51
06/02/2022 13:48:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.178726 on epoch=52
06/02/2022 13:48:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.157316 on epoch=53
06/02/2022 13:48:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.157184 on epoch=54
06/02/2022 13:48:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.175071 on epoch=56
06/02/2022 13:48:38 - INFO - __main__ - Global step 450 Train loss 0.178379 Classification-F1 0.4576271186440678 on epoch=56
06/02/2022 13:48:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.152006 on epoch=57
06/02/2022 13:48:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.101010 on epoch=58
06/02/2022 13:48:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.102051 on epoch=59
06/02/2022 13:48:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.058710 on epoch=61
06/02/2022 13:49:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.091964 on epoch=62
06/02/2022 13:49:04 - INFO - __main__ - Global step 500 Train loss 0.101148 Classification-F1 0.47540983606557374 on epoch=62
06/02/2022 13:49:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.184608 on epoch=63
06/02/2022 13:49:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.057793 on epoch=64
06/02/2022 13:49:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.116321 on epoch=66
06/02/2022 13:49:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.071264 on epoch=67
06/02/2022 13:49:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.021884 on epoch=68
06/02/2022 13:49:30 - INFO - __main__ - Global step 550 Train loss 0.090374 Classification-F1 0.4796747967479675 on epoch=68
06/02/2022 13:49:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.075511 on epoch=69
06/02/2022 13:49:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.029553 on epoch=71
06/02/2022 13:49:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.042505 on epoch=72
06/02/2022 13:49:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.064585 on epoch=73
06/02/2022 13:49:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.090270 on epoch=74
06/02/2022 13:49:57 - INFO - __main__ - Global step 600 Train loss 0.060485 Classification-F1 0.4796747967479675 on epoch=74
06/02/2022 13:50:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.053770 on epoch=76
06/02/2022 13:50:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.025782 on epoch=77
06/02/2022 13:50:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.024091 on epoch=78
06/02/2022 13:50:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.002405 on epoch=79
06/02/2022 13:50:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.062471 on epoch=81
06/02/2022 13:50:23 - INFO - __main__ - Global step 650 Train loss 0.033704 Classification-F1 0.43859649122807015 on epoch=81
06/02/2022 13:50:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.015476 on epoch=82
06/02/2022 13:50:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.035695 on epoch=83
06/02/2022 13:50:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.041907 on epoch=84
06/02/2022 13:50:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.009842 on epoch=86
06/02/2022 13:50:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.060925 on epoch=87
06/02/2022 13:50:49 - INFO - __main__ - Global step 700 Train loss 0.032769 Classification-F1 0.47540983606557374 on epoch=87
06/02/2022 13:50:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.031720 on epoch=88
06/02/2022 13:50:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.127762 on epoch=89
06/02/2022 13:51:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.006888 on epoch=91
06/02/2022 13:51:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.003792 on epoch=92
06/02/2022 13:51:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.003358 on epoch=93
06/02/2022 13:51:15 - INFO - __main__ - Global step 750 Train loss 0.034704 Classification-F1 0.47540983606557374 on epoch=93
06/02/2022 13:51:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.001671 on epoch=94
06/02/2022 13:51:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.006404 on epoch=96
06/02/2022 13:51:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000087 on epoch=97
06/02/2022 13:51:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.002750 on epoch=98
06/02/2022 13:51:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.002073 on epoch=99
06/02/2022 13:51:42 - INFO - __main__ - Global step 800 Train loss 0.002597 Classification-F1 0.4796747967479675 on epoch=99
06/02/2022 13:51:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.028690 on epoch=101
06/02/2022 13:51:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.112473 on epoch=102
06/02/2022 13:51:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.024320 on epoch=103
06/02/2022 13:52:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.029119 on epoch=104
06/02/2022 13:52:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.057885 on epoch=106
06/02/2022 13:52:08 - INFO - __main__ - Global step 850 Train loss 0.050497 Classification-F1 0.452991452991453 on epoch=106
06/02/2022 13:52:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.065700 on epoch=107
06/02/2022 13:52:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.004984 on epoch=108
06/02/2022 13:52:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.002170 on epoch=109
06/02/2022 13:52:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.002597 on epoch=111
06/02/2022 13:52:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.002674 on epoch=112
06/02/2022 13:52:34 - INFO - __main__ - Global step 900 Train loss 0.015625 Classification-F1 0.47540983606557374 on epoch=112
06/02/2022 13:52:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.004348 on epoch=113
06/02/2022 13:52:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.003400 on epoch=114
06/02/2022 13:52:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000339 on epoch=116
06/02/2022 13:52:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000312 on epoch=117
06/02/2022 13:53:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.002561 on epoch=118
06/02/2022 13:53:00 - INFO - __main__ - Global step 950 Train loss 0.002192 Classification-F1 0.452991452991453 on epoch=118
06/02/2022 13:53:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.008692 on epoch=119
06/02/2022 13:53:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.003204 on epoch=121
06/02/2022 13:53:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000257 on epoch=122
06/02/2022 13:53:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.015153 on epoch=123
06/02/2022 13:53:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.001156 on epoch=124
06/02/2022 13:53:26 - INFO - __main__ - Global step 1000 Train loss 0.005692 Classification-F1 0.4796747967479675 on epoch=124
06/02/2022 13:53:26 - INFO - __main__ - save last model!
06/02/2022 13:53:27 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 13:53:27 - INFO - __main__ - Printing 3 examples
06/02/2022 13:53:27 - INFO - __main__ -  [ethos-religion] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
06/02/2022 13:53:27 - INFO - __main__ - ['false']
06/02/2022 13:53:27 - INFO - __main__ -  [ethos-religion] Honestly, go fuck yourself! bitch!
06/02/2022 13:53:27 - INFO - __main__ - ['false']
06/02/2022 13:53:27 - INFO - __main__ -  [ethos-religion] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
06/02/2022 13:53:27 - INFO - __main__ - ['false']
06/02/2022 13:53:27 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:53:27 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:53:27 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 13:53:27 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:53:27 - INFO - __main__ - Printing 3 examples
06/02/2022 13:53:27 - INFO - __main__ -  [ethos-religion] fuck bulgaria stupid country
06/02/2022 13:53:27 - INFO - __main__ - ['false']
06/02/2022 13:53:27 - INFO - __main__ -  [ethos-religion] Indian Ned Flanders. You look like you professionally harrass subpar women on Facebook
06/02/2022 13:53:27 - INFO - __main__ - ['false']
06/02/2022 13:53:27 - INFO - __main__ -  [ethos-religion] Trans people=gays and loosers
06/02/2022 13:53:27 - INFO - __main__ - ['false']
06/02/2022 13:53:27 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:53:27 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:53:27 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:53:33 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 13:53:34 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 13:53:34 - INFO - __main__ - Printing 3 examples
06/02/2022 13:53:34 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 13:53:34 - INFO - __main__ - ['false']
06/02/2022 13:53:34 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 13:53:34 - INFO - __main__ - ['false']
06/02/2022 13:53:34 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 13:53:34 - INFO - __main__ - ['true']
06/02/2022 13:53:34 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:53:34 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:53:34 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 13:53:36 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_87_0.0003_8_predictions.txt
06/02/2022 13:53:36 - INFO - __main__ - Classification-F1 on test data: 0.8468
06/02/2022 13:53:36 - INFO - __main__ - prefix=ethos-religion_64_87, lr=0.0003, bsz=8, dev_performance=0.488, test_performance=0.846830985915493
06/02/2022 13:53:36 - INFO - __main__ - Running ... prefix=ethos-religion_64_87, lr=0.0002, bsz=8 ...
06/02/2022 13:53:37 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 13:53:37 - INFO - __main__ - Printing 3 examples
06/02/2022 13:53:37 - INFO - __main__ -  [ethos-religion] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
06/02/2022 13:53:37 - INFO - __main__ - ['false']
06/02/2022 13:53:37 - INFO - __main__ -  [ethos-religion] Honestly, go fuck yourself! bitch!
06/02/2022 13:53:37 - INFO - __main__ - ['false']
06/02/2022 13:53:37 - INFO - __main__ -  [ethos-religion] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
06/02/2022 13:53:37 - INFO - __main__ - ['false']
06/02/2022 13:53:37 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:53:37 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:53:37 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 13:53:37 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:53:37 - INFO - __main__ - Printing 3 examples
06/02/2022 13:53:37 - INFO - __main__ -  [ethos-religion] fuck bulgaria stupid country
06/02/2022 13:53:37 - INFO - __main__ - ['false']
06/02/2022 13:53:37 - INFO - __main__ -  [ethos-religion] Indian Ned Flanders. You look like you professionally harrass subpar women on Facebook
06/02/2022 13:53:37 - INFO - __main__ - ['false']
06/02/2022 13:53:37 - INFO - __main__ -  [ethos-religion] Trans people=gays and loosers
06/02/2022 13:53:37 - INFO - __main__ - ['false']
06/02/2022 13:53:37 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:53:37 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:53:37 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:53:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 13:53:38 - INFO - __main__ - Starting training!
06/02/2022 13:53:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 13:53:48 - INFO - __main__ - Starting training!
06/02/2022 13:53:53 - INFO - __main__ - Step 10 Global step 10 Train loss 23.757631 on epoch=1
06/02/2022 13:53:58 - INFO - __main__ - Step 20 Global step 20 Train loss 20.560524 on epoch=2
06/02/2022 13:54:03 - INFO - __main__ - Step 30 Global step 30 Train loss 17.916079 on epoch=3
06/02/2022 13:54:08 - INFO - __main__ - Step 40 Global step 40 Train loss 16.721584 on epoch=4
06/02/2022 13:54:13 - INFO - __main__ - Step 50 Global step 50 Train loss 16.284208 on epoch=6
06/02/2022 13:54:33 - INFO - __main__ - Global step 50 Train loss 19.048006 Classification-F1 0.0 on epoch=6
06/02/2022 13:54:39 - INFO - __main__ - Step 60 Global step 60 Train loss 14.732259 on epoch=7
06/02/2022 13:54:44 - INFO - __main__ - Step 70 Global step 70 Train loss 14.796135 on epoch=8
06/02/2022 13:54:49 - INFO - __main__ - Step 80 Global step 80 Train loss 14.093836 on epoch=9
06/02/2022 13:54:54 - INFO - __main__ - Step 90 Global step 90 Train loss 13.000173 on epoch=11
06/02/2022 13:55:00 - INFO - __main__ - Step 100 Global step 100 Train loss 11.853916 on epoch=12
06/02/2022 13:55:18 - INFO - __main__ - Global step 100 Train loss 13.695264 Classification-F1 0.0 on epoch=12
06/02/2022 13:55:23 - INFO - __main__ - Step 110 Global step 110 Train loss 11.078729 on epoch=13
06/02/2022 13:55:29 - INFO - __main__ - Step 120 Global step 120 Train loss 8.452135 on epoch=14
06/02/2022 13:55:34 - INFO - __main__ - Step 130 Global step 130 Train loss 5.076560 on epoch=16
06/02/2022 13:55:39 - INFO - __main__ - Step 140 Global step 140 Train loss 3.885795 on epoch=17
06/02/2022 13:55:44 - INFO - __main__ - Step 150 Global step 150 Train loss 2.835843 on epoch=18
06/02/2022 13:55:45 - INFO - __main__ - Global step 150 Train loss 6.265812 Classification-F1 0.4482758620689655 on epoch=18
06/02/2022 13:55:51 - INFO - __main__ - Step 160 Global step 160 Train loss 2.684261 on epoch=19
06/02/2022 13:55:57 - INFO - __main__ - Step 170 Global step 170 Train loss 2.278331 on epoch=21
06/02/2022 13:56:02 - INFO - __main__ - Step 180 Global step 180 Train loss 2.309021 on epoch=22
06/02/2022 13:56:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.966320 on epoch=23
06/02/2022 13:56:12 - INFO - __main__ - Step 200 Global step 200 Train loss 1.987472 on epoch=24
06/02/2022 13:56:12 - INFO - __main__ - Global step 200 Train loss 2.045081 Classification-F1 0.4576271186440678 on epoch=24
06/02/2022 13:56:19 - INFO - __main__ - Step 210 Global step 210 Train loss 1.047642 on epoch=26
06/02/2022 13:56:24 - INFO - __main__ - Step 220 Global step 220 Train loss 1.363194 on epoch=27
06/02/2022 13:56:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.821057 on epoch=28
06/02/2022 13:56:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.862403 on epoch=29
06/02/2022 13:56:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.479830 on epoch=31
06/02/2022 13:56:40 - INFO - __main__ - Global step 250 Train loss 0.914825 Classification-F1 0.47107438016528924 on epoch=31
06/02/2022 13:56:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.363371 on epoch=32
06/02/2022 13:56:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.324459 on epoch=33
06/02/2022 13:56:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.438723 on epoch=34
06/02/2022 13:57:02 - INFO - __main__ - Step 290 Global step 290 Train loss 0.279275 on epoch=36
06/02/2022 13:57:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.216761 on epoch=37
06/02/2022 13:57:08 - INFO - __main__ - Global step 300 Train loss 0.324518 Classification-F1 0.46218487394957986 on epoch=37
06/02/2022 13:57:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.286841 on epoch=38
06/02/2022 13:57:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.180635 on epoch=39
06/02/2022 13:57:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.172102 on epoch=41
06/02/2022 13:57:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.202312 on epoch=42
06/02/2022 13:57:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.132592 on epoch=43
06/02/2022 13:57:34 - INFO - __main__ - Global step 350 Train loss 0.194896 Classification-F1 0.4666666666666667 on epoch=43
06/02/2022 13:57:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.143183 on epoch=44
06/02/2022 13:57:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.139405 on epoch=46
06/02/2022 13:57:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.190333 on epoch=47
06/02/2022 13:57:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.145505 on epoch=48
06/02/2022 13:58:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.117292 on epoch=49
06/02/2022 13:58:01 - INFO - __main__ - Global step 400 Train loss 0.147144 Classification-F1 0.4482758620689655 on epoch=49
06/02/2022 13:58:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.080225 on epoch=51
06/02/2022 13:58:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.130978 on epoch=52
06/02/2022 13:58:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.082585 on epoch=53
06/02/2022 13:58:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.055667 on epoch=54
06/02/2022 13:58:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.063741 on epoch=56
06/02/2022 13:58:28 - INFO - __main__ - Global step 450 Train loss 0.082639 Classification-F1 0.4482758620689655 on epoch=56
06/02/2022 13:58:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.070623 on epoch=57
06/02/2022 13:58:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.033353 on epoch=58
06/02/2022 13:58:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.070596 on epoch=59
06/02/2022 13:58:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.080357 on epoch=61
06/02/2022 13:58:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.072840 on epoch=62
06/02/2022 13:58:54 - INFO - __main__ - Global step 500 Train loss 0.065554 Classification-F1 0.47540983606557374 on epoch=62
06/02/2022 13:59:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.038458 on epoch=63
06/02/2022 13:59:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.016493 on epoch=64
06/02/2022 13:59:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.052419 on epoch=66
06/02/2022 13:59:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.020221 on epoch=67
06/02/2022 13:59:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.031019 on epoch=68
06/02/2022 13:59:22 - INFO - __main__ - Global step 550 Train loss 0.031722 Classification-F1 0.47107438016528924 on epoch=68
06/02/2022 13:59:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.026054 on epoch=69
06/02/2022 13:59:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.043456 on epoch=71
06/02/2022 13:59:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.012056 on epoch=72
06/02/2022 13:59:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.058529 on epoch=73
06/02/2022 13:59:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.019085 on epoch=74
06/02/2022 13:59:48 - INFO - __main__ - Global step 600 Train loss 0.031836 Classification-F1 0.46218487394957986 on epoch=74
06/02/2022 13:59:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.034614 on epoch=76
06/02/2022 13:59:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.007878 on epoch=77
06/02/2022 14:00:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.033402 on epoch=78
06/02/2022 14:00:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.012069 on epoch=79
06/02/2022 14:00:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.014588 on epoch=81
06/02/2022 14:00:15 - INFO - __main__ - Global step 650 Train loss 0.020510 Classification-F1 0.4666666666666667 on epoch=81
06/02/2022 14:00:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.010464 on epoch=82
06/02/2022 14:00:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.010585 on epoch=83
06/02/2022 14:00:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.001260 on epoch=84
06/02/2022 14:00:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.005144 on epoch=86
06/02/2022 14:00:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.015876 on epoch=87
06/02/2022 14:00:41 - INFO - __main__ - Global step 700 Train loss 0.008666 Classification-F1 0.4576271186440678 on epoch=87
06/02/2022 14:00:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.047383 on epoch=88
06/02/2022 14:00:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.021138 on epoch=89
06/02/2022 14:00:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.002908 on epoch=91
06/02/2022 14:01:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.001179 on epoch=92
06/02/2022 14:01:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.003410 on epoch=93
06/02/2022 14:01:08 - INFO - __main__ - Global step 750 Train loss 0.015204 Classification-F1 0.4666666666666667 on epoch=93
06/02/2022 14:01:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.031996 on epoch=94
06/02/2022 14:01:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.008998 on epoch=96
06/02/2022 14:01:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.003777 on epoch=97
06/02/2022 14:01:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.014968 on epoch=98
06/02/2022 14:01:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000970 on epoch=99
06/02/2022 14:01:34 - INFO - __main__ - Global step 800 Train loss 0.012142 Classification-F1 0.46218487394957986 on epoch=99
06/02/2022 14:01:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000595 on epoch=101
06/02/2022 14:01:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001637 on epoch=102
06/02/2022 14:01:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.012131 on epoch=103
06/02/2022 14:01:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.001609 on epoch=104
06/02/2022 14:02:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.014753 on epoch=106
06/02/2022 14:02:01 - INFO - __main__ - Global step 850 Train loss 0.006145 Classification-F1 0.4666666666666667 on epoch=106
06/02/2022 14:02:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.002121 on epoch=107
06/02/2022 14:02:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.038683 on epoch=108
06/02/2022 14:02:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001848 on epoch=109
06/02/2022 14:02:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.002904 on epoch=111
06/02/2022 14:02:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.008078 on epoch=112
06/02/2022 14:02:27 - INFO - __main__ - Global step 900 Train loss 0.010727 Classification-F1 0.46218487394957986 on epoch=112
06/02/2022 14:02:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000670 on epoch=113
06/02/2022 14:02:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.008169 on epoch=114
06/02/2022 14:02:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.041332 on epoch=116
06/02/2022 14:02:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.068645 on epoch=117
06/02/2022 14:02:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.004333 on epoch=118
06/02/2022 14:02:54 - INFO - __main__ - Global step 950 Train loss 0.024630 Classification-F1 0.4666666666666667 on epoch=118
06/02/2022 14:02:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.010030 on epoch=119
06/02/2022 14:03:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000780 on epoch=121
06/02/2022 14:03:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.006162 on epoch=122
06/02/2022 14:03:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.001321 on epoch=123
06/02/2022 14:03:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.011289 on epoch=124
06/02/2022 14:03:20 - INFO - __main__ - Global step 1000 Train loss 0.005916 Classification-F1 0.47540983606557374 on epoch=124
06/02/2022 14:03:20 - INFO - __main__ - save last model!
06/02/2022 14:03:21 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 14:03:21 - INFO - __main__ - Printing 3 examples
06/02/2022 14:03:21 - INFO - __main__ -  [ethos-religion] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
06/02/2022 14:03:21 - INFO - __main__ - ['false']
06/02/2022 14:03:21 - INFO - __main__ -  [ethos-religion] Honestly, go fuck yourself! bitch!
06/02/2022 14:03:21 - INFO - __main__ - ['false']
06/02/2022 14:03:21 - INFO - __main__ -  [ethos-religion] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
06/02/2022 14:03:21 - INFO - __main__ - ['false']
06/02/2022 14:03:21 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:03:21 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:03:21 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 14:03:21 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:03:21 - INFO - __main__ - Printing 3 examples
06/02/2022 14:03:21 - INFO - __main__ -  [ethos-religion] fuck bulgaria stupid country
06/02/2022 14:03:21 - INFO - __main__ - ['false']
06/02/2022 14:03:21 - INFO - __main__ -  [ethos-religion] Indian Ned Flanders. You look like you professionally harrass subpar women on Facebook
06/02/2022 14:03:21 - INFO - __main__ - ['false']
06/02/2022 14:03:21 - INFO - __main__ -  [ethos-religion] Trans people=gays and loosers
06/02/2022 14:03:21 - INFO - __main__ - ['false']
06/02/2022 14:03:21 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:03:21 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:03:21 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 14:03:27 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 14:03:28 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 14:03:28 - INFO - __main__ - Printing 3 examples
06/02/2022 14:03:28 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 14:03:28 - INFO - __main__ - ['false']
06/02/2022 14:03:28 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 14:03:28 - INFO - __main__ - ['false']
06/02/2022 14:03:28 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 14:03:28 - INFO - __main__ - ['true']
06/02/2022 14:03:28 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:03:28 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:03:28 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 14:03:30 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_87_0.0002_8_predictions.txt
06/02/2022 14:03:30 - INFO - __main__ - Classification-F1 on test data: 0.9835
06/02/2022 14:03:30 - INFO - __main__ - prefix=ethos-religion_64_87, lr=0.0002, bsz=8, dev_performance=0.47540983606557374, test_performance=0.9834757834757835
06/02/2022 14:03:30 - INFO - __main__ - Running ... prefix=ethos-religion_64_87, lr=0.0001, bsz=8 ...
06/02/2022 14:03:31 - INFO - __main__ - Start tokenizing ... 126 instances
06/02/2022 14:03:31 - INFO - __main__ - Printing 3 examples
06/02/2022 14:03:31 - INFO - __main__ -  [ethos-religion] Shemales are paranormal creatures. We shouldn't let them be free. Neither be captured. Just dead and buried already
06/02/2022 14:03:31 - INFO - __main__ - ['false']
06/02/2022 14:03:31 - INFO - __main__ -  [ethos-religion] Honestly, go fuck yourself! bitch!
06/02/2022 14:03:31 - INFO - __main__ - ['false']
06/02/2022 14:03:31 - INFO - __main__ -  [ethos-religion] After traumatic brain injury turned gay. I don't know which is worst. Suicide is the way.
06/02/2022 14:03:31 - INFO - __main__ - ['false']
06/02/2022 14:03:31 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:03:31 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:03:31 - INFO - __main__ - Loaded 126 examples from train data
06/02/2022 14:03:31 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:03:31 - INFO - __main__ - Printing 3 examples
06/02/2022 14:03:31 - INFO - __main__ -  [ethos-religion] fuck bulgaria stupid country
06/02/2022 14:03:31 - INFO - __main__ - ['false']
06/02/2022 14:03:31 - INFO - __main__ -  [ethos-religion] Indian Ned Flanders. You look like you professionally harrass subpar women on Facebook
06/02/2022 14:03:31 - INFO - __main__ - ['false']
06/02/2022 14:03:31 - INFO - __main__ -  [ethos-religion] Trans people=gays and loosers
06/02/2022 14:03:31 - INFO - __main__ - ['false']
06/02/2022 14:03:31 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:03:31 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:03:31 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 14:03:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 14:03:34 - INFO - __main__ - Starting training!
06/02/2022 14:03:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 14:03:42 - INFO - __main__ - Starting training!
06/02/2022 14:03:46 - INFO - __main__ - Step 10 Global step 10 Train loss 23.707653 on epoch=1
06/02/2022 14:03:51 - INFO - __main__ - Step 20 Global step 20 Train loss 21.531723 on epoch=2
06/02/2022 14:03:57 - INFO - __main__ - Step 30 Global step 30 Train loss 18.360182 on epoch=3
06/02/2022 14:04:02 - INFO - __main__ - Step 40 Global step 40 Train loss 17.503998 on epoch=4
06/02/2022 14:04:07 - INFO - __main__ - Step 50 Global step 50 Train loss 17.591314 on epoch=6
06/02/2022 14:04:27 - INFO - __main__ - Global step 50 Train loss 19.738974 Classification-F1 0.0 on epoch=6
06/02/2022 14:04:33 - INFO - __main__ - Step 60 Global step 60 Train loss 17.025204 on epoch=7
06/02/2022 14:04:38 - INFO - __main__ - Step 70 Global step 70 Train loss 16.763336 on epoch=8
06/02/2022 14:04:43 - INFO - __main__ - Step 80 Global step 80 Train loss 15.825449 on epoch=9
06/02/2022 14:04:48 - INFO - __main__ - Step 90 Global step 90 Train loss 16.236897 on epoch=11
06/02/2022 14:04:53 - INFO - __main__ - Step 100 Global step 100 Train loss 15.490489 on epoch=12
06/02/2022 14:05:12 - INFO - __main__ - Global step 100 Train loss 16.268274 Classification-F1 0.0 on epoch=12
06/02/2022 14:05:17 - INFO - __main__ - Step 110 Global step 110 Train loss 14.671931 on epoch=13
06/02/2022 14:05:23 - INFO - __main__ - Step 120 Global step 120 Train loss 15.647207 on epoch=14
06/02/2022 14:05:28 - INFO - __main__ - Step 130 Global step 130 Train loss 14.083212 on epoch=16
06/02/2022 14:05:33 - INFO - __main__ - Step 140 Global step 140 Train loss 14.227678 on epoch=17
06/02/2022 14:05:38 - INFO - __main__ - Step 150 Global step 150 Train loss 13.177116 on epoch=18
06/02/2022 14:05:57 - INFO - __main__ - Global step 150 Train loss 14.361428 Classification-F1 0.0 on epoch=18
06/02/2022 14:06:02 - INFO - __main__ - Step 160 Global step 160 Train loss 13.241260 on epoch=19
06/02/2022 14:06:07 - INFO - __main__ - Step 170 Global step 170 Train loss 12.661734 on epoch=21
06/02/2022 14:06:13 - INFO - __main__ - Step 180 Global step 180 Train loss 11.906458 on epoch=22
06/02/2022 14:06:18 - INFO - __main__ - Step 190 Global step 190 Train loss 11.364164 on epoch=23
06/02/2022 14:06:23 - INFO - __main__ - Step 200 Global step 200 Train loss 10.763426 on epoch=24
06/02/2022 14:06:42 - INFO - __main__ - Global step 200 Train loss 11.987408 Classification-F1 0.0 on epoch=24
06/02/2022 14:06:47 - INFO - __main__ - Step 210 Global step 210 Train loss 9.062647 on epoch=26
06/02/2022 14:06:52 - INFO - __main__ - Step 220 Global step 220 Train loss 6.504654 on epoch=27
06/02/2022 14:06:57 - INFO - __main__ - Step 230 Global step 230 Train loss 5.943607 on epoch=28
06/02/2022 14:07:02 - INFO - __main__ - Step 240 Global step 240 Train loss 3.454559 on epoch=29
06/02/2022 14:07:07 - INFO - __main__ - Step 250 Global step 250 Train loss 2.210781 on epoch=31
06/02/2022 14:07:08 - INFO - __main__ - Global step 250 Train loss 5.435250 Classification-F1 1.0 on epoch=31
06/02/2022 14:07:14 - INFO - __main__ - Step 260 Global step 260 Train loss 2.484323 on epoch=32
06/02/2022 14:07:19 - INFO - __main__ - Step 270 Global step 270 Train loss 2.483439 on epoch=33
06/02/2022 14:07:24 - INFO - __main__ - Step 280 Global step 280 Train loss 2.045582 on epoch=34
06/02/2022 14:07:29 - INFO - __main__ - Step 290 Global step 290 Train loss 1.542993 on epoch=36
06/02/2022 14:07:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.795427 on epoch=37
06/02/2022 14:07:35 - INFO - __main__ - Global step 300 Train loss 1.870353 Classification-F1 0.49606299212598426 on epoch=37
06/02/2022 14:07:40 - INFO - __main__ - Step 310 Global step 310 Train loss 1.248083 on epoch=38
06/02/2022 14:07:45 - INFO - __main__ - Step 320 Global step 320 Train loss 1.435564 on epoch=39
06/02/2022 14:07:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.602915 on epoch=41
06/02/2022 14:07:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.577508 on epoch=42
06/02/2022 14:08:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.909722 on epoch=43
06/02/2022 14:08:01 - INFO - __main__ - Global step 350 Train loss 0.954758 Classification-F1 0.41818181818181815 on epoch=43
06/02/2022 14:08:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.644516 on epoch=44
06/02/2022 14:08:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.523213 on epoch=46
06/02/2022 14:08:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.432965 on epoch=47
06/02/2022 14:08:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.502373 on epoch=48
06/02/2022 14:08:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.550887 on epoch=49
06/02/2022 14:08:28 - INFO - __main__ - Global step 400 Train loss 0.530791 Classification-F1 0.328042328042328 on epoch=49
06/02/2022 14:08:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.608471 on epoch=51
06/02/2022 14:08:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.399392 on epoch=52
06/02/2022 14:08:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.513860 on epoch=53
06/02/2022 14:08:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.485484 on epoch=54
06/02/2022 14:08:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.555092 on epoch=56
06/02/2022 14:08:54 - INFO - __main__ - Global step 450 Train loss 0.512460 Classification-F1 0.3118279569892473 on epoch=56
06/02/2022 14:08:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.369485 on epoch=57
06/02/2022 14:09:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.358933 on epoch=58
06/02/2022 14:09:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.302495 on epoch=59
06/02/2022 14:09:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.287601 on epoch=61
06/02/2022 14:09:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.296553 on epoch=62
06/02/2022 14:09:21 - INFO - __main__ - Global step 500 Train loss 0.323013 Classification-F1 0.4666666666666667 on epoch=62
06/02/2022 14:09:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.400193 on epoch=63
06/02/2022 14:09:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.320776 on epoch=64
06/02/2022 14:09:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.258556 on epoch=66
06/02/2022 14:09:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.361023 on epoch=67
06/02/2022 14:09:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.297216 on epoch=68
06/02/2022 14:09:47 - INFO - __main__ - Global step 550 Train loss 0.327553 Classification-F1 0.4482758620689655 on epoch=68
06/02/2022 14:09:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.305814 on epoch=69
06/02/2022 14:09:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.206220 on epoch=71
06/02/2022 14:10:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.248506 on epoch=72
06/02/2022 14:10:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.296342 on epoch=73
06/02/2022 14:10:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.269762 on epoch=74
06/02/2022 14:10:14 - INFO - __main__ - Global step 600 Train loss 0.265329 Classification-F1 0.4576271186440678 on epoch=74
06/02/2022 14:10:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.229129 on epoch=76
06/02/2022 14:10:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.228905 on epoch=77
06/02/2022 14:10:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.263004 on epoch=78
06/02/2022 14:10:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.225698 on epoch=79
06/02/2022 14:10:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.261088 on epoch=81
06/02/2022 14:10:40 - INFO - __main__ - Global step 650 Train loss 0.241565 Classification-F1 0.43859649122807015 on epoch=81
06/02/2022 14:10:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.253873 on epoch=82
06/02/2022 14:10:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.257561 on epoch=83
06/02/2022 14:10:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.202671 on epoch=84
06/02/2022 14:11:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.181267 on epoch=86
06/02/2022 14:11:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.233545 on epoch=87
06/02/2022 14:11:07 - INFO - __main__ - Global step 700 Train loss 0.225783 Classification-F1 0.46218487394957986 on epoch=87
06/02/2022 14:11:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.231098 on epoch=88
06/02/2022 14:11:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.182528 on epoch=89
06/02/2022 14:11:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.257755 on epoch=91
06/02/2022 14:11:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.245576 on epoch=92
06/02/2022 14:11:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.202162 on epoch=93
06/02/2022 14:11:33 - INFO - __main__ - Global step 750 Train loss 0.223824 Classification-F1 0.42857142857142855 on epoch=93
06/02/2022 14:11:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.170614 on epoch=94
06/02/2022 14:11:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.194123 on epoch=96
06/02/2022 14:11:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.191975 on epoch=97
06/02/2022 14:11:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.211759 on epoch=98
06/02/2022 14:11:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.227307 on epoch=99
06/02/2022 14:12:00 - INFO - __main__ - Global step 800 Train loss 0.199156 Classification-F1 0.4576271186440678 on epoch=99
06/02/2022 14:12:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.110078 on epoch=101
06/02/2022 14:12:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.090553 on epoch=102
06/02/2022 14:12:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.162738 on epoch=103
06/02/2022 14:12:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.230719 on epoch=104
06/02/2022 14:12:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.128092 on epoch=106
06/02/2022 14:12:27 - INFO - __main__ - Global step 850 Train loss 0.144436 Classification-F1 0.43859649122807015 on epoch=106
06/02/2022 14:12:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.153157 on epoch=107
06/02/2022 14:12:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.169351 on epoch=108
06/02/2022 14:12:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.129685 on epoch=109
06/02/2022 14:12:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.156314 on epoch=111
06/02/2022 14:12:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.152725 on epoch=112
06/02/2022 14:12:53 - INFO - __main__ - Global step 900 Train loss 0.152246 Classification-F1 0.4796747967479675 on epoch=112
06/02/2022 14:12:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.210177 on epoch=113
06/02/2022 14:13:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.157638 on epoch=114
06/02/2022 14:13:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.134387 on epoch=116
06/02/2022 14:13:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.107471 on epoch=117
06/02/2022 14:13:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.148680 on epoch=118
06/02/2022 14:13:20 - INFO - __main__ - Global step 950 Train loss 0.151671 Classification-F1 0.4796747967479675 on epoch=118
06/02/2022 14:13:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.209237 on epoch=119
06/02/2022 14:13:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.131559 on epoch=121
06/02/2022 14:13:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.158304 on epoch=122
06/02/2022 14:13:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.146710 on epoch=123
06/02/2022 14:13:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.096764 on epoch=124
06/02/2022 14:13:47 - INFO - __main__ - Global step 1000 Train loss 0.148515 Classification-F1 0.47540983606557374 on epoch=124
06/02/2022 14:13:47 - INFO - __main__ - save last model!
06/02/2022 14:13:54 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 14:13:55 - INFO - __main__ - Start tokenizing ... 87 instances
06/02/2022 14:13:55 - INFO - __main__ - Printing 3 examples
06/02/2022 14:13:55 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
06/02/2022 14:13:55 - INFO - __main__ - ['false']
06/02/2022 14:13:55 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
06/02/2022 14:13:55 - INFO - __main__ - ['false']
06/02/2022 14:13:55 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
06/02/2022 14:13:55 - INFO - __main__ - ['true']
06/02/2022 14:13:55 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:13:55 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:13:55 - INFO - __main__ - Loaded 87 examples from test data
06/02/2022 14:13:57 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-ethos-religion/ethos-religion_64_87_0.0001_8_predictions.txt
06/02/2022 14:13:57 - INFO - __main__ - Classification-F1 on test data: 0.2851
06/02/2022 14:13:57 - INFO - __main__ - prefix=ethos-religion_64_87, lr=0.0001, bsz=8, dev_performance=1.0, test_performance=0.2850877192982456
