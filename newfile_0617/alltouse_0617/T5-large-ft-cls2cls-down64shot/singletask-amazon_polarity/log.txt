05/21/2022 21:27:31 - INFO - __main__ - Namespace(task_dir='data_64/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-ft-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:27:31 - INFO - __main__ - models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity
05/21/2022 21:27:31 - INFO - __main__ - Namespace(task_dir='data_64/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-ft-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:27:31 - INFO - __main__ - models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity
05/21/2022 21:27:33 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:27:33 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:27:33 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:27:33 - INFO - __main__ - Using 2 gpus
05/21/2022 21:27:33 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_64_100', 'amazon_polarity_64_13', 'amazon_polarity_64_21', 'amazon_polarity_64_42', 'amazon_polarity_64_87']
05/21/2022 21:27:33 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:27:33 - INFO - __main__ - Using 2 gpus
05/21/2022 21:27:33 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_64_100', 'amazon_polarity_64_13', 'amazon_polarity_64_21', 'amazon_polarity_64_42', 'amazon_polarity_64_87']
05/21/2022 21:27:38 - INFO - __main__ - Running ... prefix=amazon_polarity_64_100, lr=0.0005, bsz=8 ...
06/02/2022 14:14:02 - INFO - __main__ - Namespace(task_dir='data_64/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-ft-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
06/02/2022 14:14:02 - INFO - __main__ - models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity
06/02/2022 14:14:02 - INFO - __main__ - Namespace(task_dir='data_64/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-ft-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
06/02/2022 14:14:02 - INFO - __main__ - models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity
06/02/2022 14:14:04 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/02/2022 14:14:04 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/02/2022 14:14:04 - INFO - __main__ - args.device: cuda:0
06/02/2022 14:14:04 - INFO - __main__ - Using 2 gpus
06/02/2022 14:14:04 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_64_100', 'amazon_polarity_64_13', 'amazon_polarity_64_21', 'amazon_polarity_64_42', 'amazon_polarity_64_87']
06/02/2022 14:14:04 - INFO - __main__ - args.device: cuda:1
06/02/2022 14:14:04 - INFO - __main__ - Using 2 gpus
06/02/2022 14:14:04 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_64_100', 'amazon_polarity_64_13', 'amazon_polarity_64_21', 'amazon_polarity_64_42', 'amazon_polarity_64_87']
06/02/2022 14:14:09 - INFO - __main__ - Running ... prefix=amazon_polarity_64_100, lr=0.0005, bsz=8 ...
06/02/2022 14:14:09 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:14:09 - INFO - __main__ - Printing 3 examples
06/02/2022 14:14:09 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
06/02/2022 14:14:09 - INFO - __main__ - ['positive']
06/02/2022 14:14:09 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
06/02/2022 14:14:09 - INFO - __main__ - ['positive']
06/02/2022 14:14:09 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
06/02/2022 14:14:09 - INFO - __main__ - ['positive']
06/02/2022 14:14:09 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:14:09 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:14:09 - INFO - __main__ - Printing 3 examples
06/02/2022 14:14:09 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
06/02/2022 14:14:09 - INFO - __main__ - ['positive']
06/02/2022 14:14:09 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
06/02/2022 14:14:09 - INFO - __main__ - ['positive']
06/02/2022 14:14:09 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
06/02/2022 14:14:09 - INFO - __main__ - ['positive']
06/02/2022 14:14:09 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:14:10 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:14:10 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:14:10 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 14:14:10 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:14:10 - INFO - __main__ - Printing 3 examples
06/02/2022 14:14:10 - INFO - __main__ -  [amazon_polarity] title: A must buy [SEP] content: Having no idea what to expect before going to my first Rebecca's Statue show, I asked around. "Like Phish, but more bluesy.", some said. "Highly inspirational", a rollicking young hipster blurted out. The Statue was all these things, and then some. But could they make an album? Many spectacular live bands have trouble going from the live world to the studio world. Well simply put, they got it taken care of. This album ranks high among my favorites. The Statue is one of those bands that has so many influences it would be impossible to name a few bands or styles as their influence. All I can say is that I've played this CD for fans of jazz, blues, hard rock, pop, and bluegrass, and all of those people have bought "Drinking From the Water Clock", or gone to a Statue show. You will not be disappointed with this disc.
06/02/2022 14:14:10 - INFO - __main__ - ['positive']
06/02/2022 14:14:10 - INFO - __main__ -  [amazon_polarity] title: snow treasure [SEP] content: This was a gift for my 2nd grade Grandson. He is an advanced reader, and we constantly set the goals for him. I read this the first time at age 28, and it was the start of my reading journeys
06/02/2022 14:14:10 - INFO - __main__ - ['positive']
06/02/2022 14:14:10 - INFO - __main__ -  [amazon_polarity] title: This is the best music in the world [SEP] content: i loved this tape, i listen to it all the time and hope that bruce springstein lives a long and happylife, amen
06/02/2022 14:14:10 - INFO - __main__ - ['positive']
06/02/2022 14:14:10 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:14:10 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 14:14:10 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:14:10 - INFO - __main__ - Printing 3 examples
06/02/2022 14:14:10 - INFO - __main__ -  [amazon_polarity] title: A must buy [SEP] content: Having no idea what to expect before going to my first Rebecca's Statue show, I asked around. "Like Phish, but more bluesy.", some said. "Highly inspirational", a rollicking young hipster blurted out. The Statue was all these things, and then some. But could they make an album? Many spectacular live bands have trouble going from the live world to the studio world. Well simply put, they got it taken care of. This album ranks high among my favorites. The Statue is one of those bands that has so many influences it would be impossible to name a few bands or styles as their influence. All I can say is that I've played this CD for fans of jazz, blues, hard rock, pop, and bluegrass, and all of those people have bought "Drinking From the Water Clock", or gone to a Statue show. You will not be disappointed with this disc.
06/02/2022 14:14:10 - INFO - __main__ - ['positive']
06/02/2022 14:14:10 - INFO - __main__ -  [amazon_polarity] title: snow treasure [SEP] content: This was a gift for my 2nd grade Grandson. He is an advanced reader, and we constantly set the goals for him. I read this the first time at age 28, and it was the start of my reading journeys
06/02/2022 14:14:10 - INFO - __main__ - ['positive']
06/02/2022 14:14:10 - INFO - __main__ -  [amazon_polarity] title: This is the best music in the world [SEP] content: i loved this tape, i listen to it all the time and hope that bruce springstein lives a long and happylife, amen
06/02/2022 14:14:10 - INFO - __main__ - ['positive']
06/02/2022 14:14:10 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:14:10 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:14:10 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:14:10 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 14:14:10 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 14:14:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 14:14:23 - INFO - __main__ - Starting training!
06/02/2022 14:14:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 14:14:23 - INFO - __main__ - Starting training!
06/02/2022 14:14:28 - INFO - __main__ - Step 10 Global step 10 Train loss 23.689861 on epoch=1
06/02/2022 14:14:33 - INFO - __main__ - Step 20 Global step 20 Train loss 17.150211 on epoch=2
06/02/2022 14:14:38 - INFO - __main__ - Step 30 Global step 30 Train loss 15.125621 on epoch=3
06/02/2022 14:14:43 - INFO - __main__ - Step 40 Global step 40 Train loss 15.297117 on epoch=4
06/02/2022 14:14:48 - INFO - __main__ - Step 50 Global step 50 Train loss 19.523039 on epoch=6
06/02/2022 14:14:53 - INFO - __main__ - Global step 50 Train loss 18.157171 Classification-F1 0.0 on epoch=6
06/02/2022 14:14:59 - INFO - __main__ - Step 60 Global step 60 Train loss 10.766780 on epoch=7
06/02/2022 14:15:04 - INFO - __main__ - Step 70 Global step 70 Train loss 7.541105 on epoch=8
06/02/2022 14:15:09 - INFO - __main__ - Step 80 Global step 80 Train loss 1.479610 on epoch=9
06/02/2022 14:15:14 - INFO - __main__ - Step 90 Global step 90 Train loss 0.573765 on epoch=11
06/02/2022 14:15:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.740896 on epoch=12
06/02/2022 14:15:21 - INFO - __main__ - Global step 100 Train loss 4.220431 Classification-F1 0.4515405019383799 on epoch=12
06/02/2022 14:15:28 - INFO - __main__ - Step 110 Global step 110 Train loss 0.498440 on epoch=13
06/02/2022 14:15:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.488604 on epoch=14
06/02/2022 14:15:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.418124 on epoch=16
06/02/2022 14:15:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.421994 on epoch=17
06/02/2022 14:15:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.418868 on epoch=18
06/02/2022 14:15:50 - INFO - __main__ - Global step 150 Train loss 0.449206 Classification-F1 0.3333333333333333 on epoch=18
06/02/2022 14:15:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.511815 on epoch=19
06/02/2022 14:16:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.396075 on epoch=21
06/02/2022 14:16:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.367380 on epoch=22
06/02/2022 14:16:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.400861 on epoch=23
06/02/2022 14:16:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.395945 on epoch=24
06/02/2022 14:16:17 - INFO - __main__ - Global step 200 Train loss 0.414415 Classification-F1 0.3671451355661882 on epoch=24
06/02/2022 14:16:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.435533 on epoch=26
06/02/2022 14:16:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.388132 on epoch=27
06/02/2022 14:16:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.374090 on epoch=28
06/02/2022 14:16:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.365925 on epoch=29
06/02/2022 14:16:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.411361 on epoch=31
06/02/2022 14:16:44 - INFO - __main__ - Global step 250 Train loss 0.395008 Classification-F1 0.3333333333333333 on epoch=31
06/02/2022 14:16:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.343897 on epoch=32
06/02/2022 14:16:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.448661 on epoch=33
06/02/2022 14:17:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.387805 on epoch=34
06/02/2022 14:17:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.389305 on epoch=36
06/02/2022 14:17:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.351982 on epoch=37
06/02/2022 14:17:12 - INFO - __main__ - Global step 300 Train loss 0.384330 Classification-F1 0.4055576703464027 on epoch=37
06/02/2022 14:17:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.376537 on epoch=38
06/02/2022 14:17:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.360729 on epoch=39
06/02/2022 14:17:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.373296 on epoch=41
06/02/2022 14:17:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.398091 on epoch=42
06/02/2022 14:17:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.374357 on epoch=43
06/02/2022 14:17:39 - INFO - __main__ - Global step 350 Train loss 0.376602 Classification-F1 0.3333333333333333 on epoch=43
06/02/2022 14:17:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.339144 on epoch=44
06/02/2022 14:17:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.388831 on epoch=46
06/02/2022 14:17:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.377756 on epoch=47
06/02/2022 14:17:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.362478 on epoch=48
06/02/2022 14:18:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.343832 on epoch=49
06/02/2022 14:18:06 - INFO - __main__ - Global step 400 Train loss 0.362408 Classification-F1 0.43468822516655437 on epoch=49
06/02/2022 14:18:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.368336 on epoch=51
06/02/2022 14:18:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.346375 on epoch=52
06/02/2022 14:18:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.386693 on epoch=53
06/02/2022 14:18:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.348427 on epoch=54
06/02/2022 14:18:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.344417 on epoch=56
06/02/2022 14:18:33 - INFO - __main__ - Global step 450 Train loss 0.358850 Classification-F1 0.36318407960199 on epoch=56
06/02/2022 14:18:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.326614 on epoch=57
06/02/2022 14:18:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.357713 on epoch=58
06/02/2022 14:18:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.336193 on epoch=59
06/02/2022 14:18:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.411924 on epoch=61
06/02/2022 14:18:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.400957 on epoch=62
06/02/2022 14:19:00 - INFO - __main__ - Global step 500 Train loss 0.366680 Classification-F1 0.6251591545709192 on epoch=62
06/02/2022 14:19:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.349282 on epoch=63
06/02/2022 14:19:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.313266 on epoch=64
06/02/2022 14:19:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.349717 on epoch=66
06/02/2022 14:19:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.372620 on epoch=67
06/02/2022 14:19:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.293306 on epoch=68
06/02/2022 14:19:28 - INFO - __main__ - Global step 550 Train loss 0.335638 Classification-F1 0.46777546777546775 on epoch=68
06/02/2022 14:19:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.282463 on epoch=69
06/02/2022 14:19:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.244963 on epoch=71
06/02/2022 14:19:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.259374 on epoch=72
06/02/2022 14:19:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.163330 on epoch=73
06/02/2022 14:19:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.235782 on epoch=74
06/02/2022 14:19:56 - INFO - __main__ - Global step 600 Train loss 0.237182 Classification-F1 0.6948468732807629 on epoch=74
06/02/2022 14:20:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.114542 on epoch=76
06/02/2022 14:20:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.245170 on epoch=77
06/02/2022 14:20:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.084401 on epoch=78
06/02/2022 14:20:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.090361 on epoch=79
06/02/2022 14:20:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.107076 on epoch=81
06/02/2022 14:20:24 - INFO - __main__ - Global step 650 Train loss 0.128310 Classification-F1 0.6921369102682701 on epoch=81
06/02/2022 14:20:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.111215 on epoch=82
06/02/2022 14:20:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.060903 on epoch=83
06/02/2022 14:20:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.031141 on epoch=84
06/02/2022 14:20:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.033693 on epoch=86
06/02/2022 14:20:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.045329 on epoch=87
06/02/2022 14:20:51 - INFO - __main__ - Global step 700 Train loss 0.056456 Classification-F1 0.7184750733137829 on epoch=87
06/02/2022 14:20:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.029092 on epoch=88
06/02/2022 14:21:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.063770 on epoch=89
06/02/2022 14:21:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.019829 on epoch=91
06/02/2022 14:21:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.046691 on epoch=92
06/02/2022 14:21:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.029644 on epoch=93
06/02/2022 14:21:19 - INFO - __main__ - Global step 750 Train loss 0.037805 Classification-F1 0.7227551209852094 on epoch=93
06/02/2022 14:21:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.004411 on epoch=94
06/02/2022 14:21:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.010093 on epoch=96
06/02/2022 14:21:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.011204 on epoch=97
06/02/2022 14:21:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.003594 on epoch=98
06/02/2022 14:21:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.004920 on epoch=99
06/02/2022 14:21:47 - INFO - __main__ - Global step 800 Train loss 0.006844 Classification-F1 0.8043890213338224 on epoch=99
06/02/2022 14:21:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.013736 on epoch=101
06/02/2022 14:21:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.003270 on epoch=102
06/02/2022 14:22:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.001964 on epoch=103
06/02/2022 14:22:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.001227 on epoch=104
06/02/2022 14:22:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.119607 on epoch=106
06/02/2022 14:22:15 - INFO - __main__ - Global step 850 Train loss 0.027961 Classification-F1 0.765625 on epoch=106
06/02/2022 14:22:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.085836 on epoch=107
06/02/2022 14:22:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.008429 on epoch=108
06/02/2022 14:22:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.003298 on epoch=109
06/02/2022 14:22:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.004674 on epoch=111
06/02/2022 14:22:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.012150 on epoch=112
06/02/2022 14:22:42 - INFO - __main__ - Global step 900 Train loss 0.022877 Classification-F1 0.7884297520661158 on epoch=112
06/02/2022 14:22:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.005623 on epoch=113
06/02/2022 14:22:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.011648 on epoch=114
06/02/2022 14:22:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.009079 on epoch=116
06/02/2022 14:23:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.001088 on epoch=117
06/02/2022 14:23:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000330 on epoch=118
06/02/2022 14:23:10 - INFO - __main__ - Global step 950 Train loss 0.005554 Classification-F1 0.8120871054563249 on epoch=118
06/02/2022 14:23:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000186 on epoch=119
06/02/2022 14:23:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000321 on epoch=121
06/02/2022 14:23:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000160 on epoch=122
06/02/2022 14:23:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.001156 on epoch=123
06/02/2022 14:23:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000510 on epoch=124
06/02/2022 14:23:37 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:23:37 - INFO - __main__ - Printing 3 examples
06/02/2022 14:23:37 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
06/02/2022 14:23:37 - INFO - __main__ - ['positive']
06/02/2022 14:23:37 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
06/02/2022 14:23:37 - INFO - __main__ - ['positive']
06/02/2022 14:23:37 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
06/02/2022 14:23:37 - INFO - __main__ - ['positive']
06/02/2022 14:23:37 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:23:37 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:23:37 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 14:23:37 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:23:37 - INFO - __main__ - Printing 3 examples
06/02/2022 14:23:37 - INFO - __main__ -  [amazon_polarity] title: A must buy [SEP] content: Having no idea what to expect before going to my first Rebecca's Statue show, I asked around. "Like Phish, but more bluesy.", some said. "Highly inspirational", a rollicking young hipster blurted out. The Statue was all these things, and then some. But could they make an album? Many spectacular live bands have trouble going from the live world to the studio world. Well simply put, they got it taken care of. This album ranks high among my favorites. The Statue is one of those bands that has so many influences it would be impossible to name a few bands or styles as their influence. All I can say is that I've played this CD for fans of jazz, blues, hard rock, pop, and bluegrass, and all of those people have bought "Drinking From the Water Clock", or gone to a Statue show. You will not be disappointed with this disc.
06/02/2022 14:23:37 - INFO - __main__ - ['positive']
06/02/2022 14:23:37 - INFO - __main__ -  [amazon_polarity] title: snow treasure [SEP] content: This was a gift for my 2nd grade Grandson. He is an advanced reader, and we constantly set the goals for him. I read this the first time at age 28, and it was the start of my reading journeys
06/02/2022 14:23:37 - INFO - __main__ - ['positive']
06/02/2022 14:23:37 - INFO - __main__ -  [amazon_polarity] title: This is the best music in the world [SEP] content: i loved this tape, i listen to it all the time and hope that bruce springstein lives a long and happylife, amen
06/02/2022 14:23:37 - INFO - __main__ - ['positive']
06/02/2022 14:23:37 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:23:38 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:23:38 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 14:23:38 - INFO - __main__ - Global step 1000 Train loss 0.000467 Classification-F1 0.796078431372549 on epoch=124
06/02/2022 14:23:38 - INFO - __main__ - save last model!
06/02/2022 14:23:45 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 14:23:46 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 14:23:46 - INFO - __main__ - Printing 3 examples
06/02/2022 14:23:46 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 14:23:46 - INFO - __main__ - ['negative']
06/02/2022 14:23:46 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 14:23:46 - INFO - __main__ - ['negative']
06/02/2022 14:23:46 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 14:23:46 - INFO - __main__ - ['negative']
06/02/2022 14:23:46 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:23:46 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:23:47 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 14:23:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 14:23:48 - INFO - __main__ - Starting training!
06/02/2022 14:24:02 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_100_0.0005_8_predictions.txt
06/02/2022 14:24:02 - INFO - __main__ - Classification-F1 on test data: 0.8720
06/02/2022 14:24:02 - INFO - __main__ - prefix=amazon_polarity_64_100, lr=0.0005, bsz=8, dev_performance=0.8120871054563249, test_performance=0.8719749070817879
06/02/2022 14:24:02 - INFO - __main__ - Running ... prefix=amazon_polarity_64_100, lr=0.0003, bsz=8 ...
06/02/2022 14:24:03 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:24:03 - INFO - __main__ - Printing 3 examples
06/02/2022 14:24:03 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
06/02/2022 14:24:03 - INFO - __main__ - ['positive']
06/02/2022 14:24:03 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
06/02/2022 14:24:03 - INFO - __main__ - ['positive']
06/02/2022 14:24:03 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
06/02/2022 14:24:03 - INFO - __main__ - ['positive']
06/02/2022 14:24:03 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:24:03 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:24:04 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 14:24:04 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:24:04 - INFO - __main__ - Printing 3 examples
06/02/2022 14:24:04 - INFO - __main__ -  [amazon_polarity] title: A must buy [SEP] content: Having no idea what to expect before going to my first Rebecca's Statue show, I asked around. "Like Phish, but more bluesy.", some said. "Highly inspirational", a rollicking young hipster blurted out. The Statue was all these things, and then some. But could they make an album? Many spectacular live bands have trouble going from the live world to the studio world. Well simply put, they got it taken care of. This album ranks high among my favorites. The Statue is one of those bands that has so many influences it would be impossible to name a few bands or styles as their influence. All I can say is that I've played this CD for fans of jazz, blues, hard rock, pop, and bluegrass, and all of those people have bought "Drinking From the Water Clock", or gone to a Statue show. You will not be disappointed with this disc.
06/02/2022 14:24:04 - INFO - __main__ - ['positive']
06/02/2022 14:24:04 - INFO - __main__ -  [amazon_polarity] title: snow treasure [SEP] content: This was a gift for my 2nd grade Grandson. He is an advanced reader, and we constantly set the goals for him. I read this the first time at age 28, and it was the start of my reading journeys
06/02/2022 14:24:04 - INFO - __main__ - ['positive']
06/02/2022 14:24:04 - INFO - __main__ -  [amazon_polarity] title: This is the best music in the world [SEP] content: i loved this tape, i listen to it all the time and hope that bruce springstein lives a long and happylife, amen
06/02/2022 14:24:04 - INFO - __main__ - ['positive']
06/02/2022 14:24:04 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:24:04 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:24:04 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 14:24:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 14:24:15 - INFO - __main__ - Starting training!
06/02/2022 14:24:19 - INFO - __main__ - Step 10 Global step 10 Train loss 22.717304 on epoch=1
06/02/2022 14:24:24 - INFO - __main__ - Step 20 Global step 20 Train loss 17.163319 on epoch=2
06/02/2022 14:24:29 - INFO - __main__ - Step 30 Global step 30 Train loss 17.120022 on epoch=3
06/02/2022 14:24:34 - INFO - __main__ - Step 40 Global step 40 Train loss 15.398807 on epoch=4
06/02/2022 14:24:39 - INFO - __main__ - Step 50 Global step 50 Train loss 14.456268 on epoch=6
06/02/2022 14:25:07 - INFO - __main__ - Global step 50 Train loss 17.371143 Classification-F1 0.0 on epoch=6
06/02/2022 14:25:13 - INFO - __main__ - Step 60 Global step 60 Train loss 12.818799 on epoch=7
06/02/2022 14:25:18 - INFO - __main__ - Step 70 Global step 70 Train loss 12.025357 on epoch=8
06/02/2022 14:25:23 - INFO - __main__ - Step 80 Global step 80 Train loss 10.693471 on epoch=9
06/02/2022 14:25:28 - INFO - __main__ - Step 90 Global step 90 Train loss 6.732580 on epoch=11
06/02/2022 14:25:33 - INFO - __main__ - Step 100 Global step 100 Train loss 1.705110 on epoch=12
06/02/2022 14:25:40 - INFO - __main__ - Global step 100 Train loss 8.795063 Classification-F1 0.016657446677730652 on epoch=12
06/02/2022 14:25:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.756370 on epoch=13
06/02/2022 14:25:52 - INFO - __main__ - Step 120 Global step 120 Train loss 0.604070 on epoch=14
06/02/2022 14:25:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.438287 on epoch=16
06/02/2022 14:26:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.423719 on epoch=17
06/02/2022 14:26:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.385474 on epoch=18
06/02/2022 14:26:09 - INFO - __main__ - Global step 150 Train loss 0.521584 Classification-F1 0.17435897435897438 on epoch=18
06/02/2022 14:26:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.435378 on epoch=19
06/02/2022 14:26:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.376533 on epoch=21
06/02/2022 14:26:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.419511 on epoch=22
06/02/2022 14:26:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.441667 on epoch=23
06/02/2022 14:26:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.399435 on epoch=24
06/02/2022 14:26:37 - INFO - __main__ - Global step 200 Train loss 0.414505 Classification-F1 0.3006281134936105 on epoch=24
06/02/2022 14:26:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.409149 on epoch=26
06/02/2022 14:26:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.395843 on epoch=27
06/02/2022 14:26:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.382464 on epoch=28
06/02/2022 14:26:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.397055 on epoch=29
06/02/2022 14:27:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.386990 on epoch=31
06/02/2022 14:27:06 - INFO - __main__ - Global step 250 Train loss 0.394300 Classification-F1 0.25120772946859904 on epoch=31
06/02/2022 14:27:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.395066 on epoch=32
06/02/2022 14:27:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.386556 on epoch=33
06/02/2022 14:27:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.370149 on epoch=34
06/02/2022 14:27:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.374644 on epoch=36
06/02/2022 14:27:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.382449 on epoch=37
06/02/2022 14:27:33 - INFO - __main__ - Global step 300 Train loss 0.381773 Classification-F1 0.3333333333333333 on epoch=37
06/02/2022 14:27:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.364969 on epoch=38
06/02/2022 14:27:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.375917 on epoch=39
06/02/2022 14:27:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.390389 on epoch=41
06/02/2022 14:27:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.379310 on epoch=42
06/02/2022 14:27:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.361507 on epoch=43
06/02/2022 14:28:00 - INFO - __main__ - Global step 350 Train loss 0.374418 Classification-F1 0.3333333333333333 on epoch=43
06/02/2022 14:28:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.368391 on epoch=44
06/02/2022 14:28:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.381019 on epoch=46
06/02/2022 14:28:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.370164 on epoch=47
06/02/2022 14:28:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.370793 on epoch=48
06/02/2022 14:28:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.353874 on epoch=49
06/02/2022 14:28:27 - INFO - __main__ - Global step 400 Train loss 0.368848 Classification-F1 0.3333333333333333 on epoch=49
06/02/2022 14:28:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.365344 on epoch=51
06/02/2022 14:28:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.366642 on epoch=52
06/02/2022 14:28:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.367702 on epoch=53
06/02/2022 14:28:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.373775 on epoch=54
06/02/2022 14:28:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.371889 on epoch=56
06/02/2022 14:28:54 - INFO - __main__ - Global step 450 Train loss 0.369070 Classification-F1 0.3333333333333333 on epoch=56
06/02/2022 14:28:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.363166 on epoch=57
06/02/2022 14:29:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.369535 on epoch=58
06/02/2022 14:29:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.340692 on epoch=59
06/02/2022 14:29:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.473668 on epoch=61
06/02/2022 14:29:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.334978 on epoch=62
06/02/2022 14:29:21 - INFO - __main__ - Global step 500 Train loss 0.376408 Classification-F1 0.4589371980676329 on epoch=62
06/02/2022 14:29:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.363220 on epoch=63
06/02/2022 14:29:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.343136 on epoch=64
06/02/2022 14:29:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.304273 on epoch=66
06/02/2022 14:29:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.352372 on epoch=67
06/02/2022 14:29:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.318448 on epoch=68
06/02/2022 14:29:49 - INFO - __main__ - Global step 550 Train loss 0.336290 Classification-F1 0.533264533883729 on epoch=68
06/02/2022 14:29:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.329866 on epoch=69
06/02/2022 14:30:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.317599 on epoch=71
06/02/2022 14:30:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.331896 on epoch=72
06/02/2022 14:30:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.306431 on epoch=73
06/02/2022 14:30:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.254884 on epoch=74
06/02/2022 14:30:17 - INFO - __main__ - Global step 600 Train loss 0.308135 Classification-F1 0.5607885352889725 on epoch=74
06/02/2022 14:30:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.288723 on epoch=76
06/02/2022 14:30:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.288322 on epoch=77
06/02/2022 14:30:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.291440 on epoch=78
06/02/2022 14:30:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.276026 on epoch=79
06/02/2022 14:30:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.244779 on epoch=81
06/02/2022 14:30:45 - INFO - __main__ - Global step 650 Train loss 0.277858 Classification-F1 0.52754370081482 on epoch=81
06/02/2022 14:30:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.253198 on epoch=82
06/02/2022 14:30:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.257267 on epoch=83
06/02/2022 14:31:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.265744 on epoch=84
06/02/2022 14:31:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.253846 on epoch=86
06/02/2022 14:31:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.220256 on epoch=87
06/02/2022 14:31:12 - INFO - __main__ - Global step 700 Train loss 0.250062 Classification-F1 0.5758218451749735 on epoch=87
06/02/2022 14:31:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.269418 on epoch=88
06/02/2022 14:31:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.231408 on epoch=89
06/02/2022 14:31:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.306359 on epoch=91
06/02/2022 14:31:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.237419 on epoch=92
06/02/2022 14:31:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.229845 on epoch=93
06/02/2022 14:31:40 - INFO - __main__ - Global step 750 Train loss 0.254890 Classification-F1 0.6930455635491606 on epoch=93
06/02/2022 14:31:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.212351 on epoch=94
06/02/2022 14:31:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.181529 on epoch=96
06/02/2022 14:31:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.165184 on epoch=97
06/02/2022 14:32:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.176115 on epoch=98
06/02/2022 14:32:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.194398 on epoch=99
06/02/2022 14:32:09 - INFO - __main__ - Global step 800 Train loss 0.185915 Classification-F1 0.6752274274398169 on epoch=99
06/02/2022 14:32:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.154893 on epoch=101
06/02/2022 14:32:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.210037 on epoch=102
06/02/2022 14:32:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.176361 on epoch=103
06/02/2022 14:32:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.167096 on epoch=104
06/02/2022 14:32:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.180929 on epoch=106
06/02/2022 14:32:36 - INFO - __main__ - Global step 850 Train loss 0.177863 Classification-F1 0.6031400488158849 on epoch=106
06/02/2022 14:32:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.174518 on epoch=107
06/02/2022 14:32:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.207287 on epoch=108
06/02/2022 14:32:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.121256 on epoch=109
06/02/2022 14:32:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.249198 on epoch=111
06/02/2022 14:33:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.163221 on epoch=112
06/02/2022 14:33:04 - INFO - __main__ - Global step 900 Train loss 0.183096 Classification-F1 0.5734203903965177 on epoch=112
06/02/2022 14:33:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.189190 on epoch=113
06/02/2022 14:33:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.183998 on epoch=114
06/02/2022 14:33:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.194529 on epoch=116
06/02/2022 14:33:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.273406 on epoch=117
06/02/2022 14:33:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.224738 on epoch=118
06/02/2022 14:33:31 - INFO - __main__ - Global step 950 Train loss 0.213172 Classification-F1 0.6065985071818354 on epoch=118
06/02/2022 14:33:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.202283 on epoch=119
06/02/2022 14:33:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.133520 on epoch=121
06/02/2022 14:33:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.156722 on epoch=122
06/02/2022 14:33:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.163270 on epoch=123
06/02/2022 14:33:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.193775 on epoch=124
06/02/2022 14:33:57 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:33:57 - INFO - __main__ - Printing 3 examples
06/02/2022 14:33:57 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
06/02/2022 14:33:57 - INFO - __main__ - ['positive']
06/02/2022 14:33:57 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
06/02/2022 14:33:57 - INFO - __main__ - ['positive']
06/02/2022 14:33:57 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
06/02/2022 14:33:57 - INFO - __main__ - ['positive']
06/02/2022 14:33:57 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:33:57 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:33:58 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 14:33:58 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:33:58 - INFO - __main__ - Printing 3 examples
06/02/2022 14:33:58 - INFO - __main__ -  [amazon_polarity] title: A must buy [SEP] content: Having no idea what to expect before going to my first Rebecca's Statue show, I asked around. "Like Phish, but more bluesy.", some said. "Highly inspirational", a rollicking young hipster blurted out. The Statue was all these things, and then some. But could they make an album? Many spectacular live bands have trouble going from the live world to the studio world. Well simply put, they got it taken care of. This album ranks high among my favorites. The Statue is one of those bands that has so many influences it would be impossible to name a few bands or styles as their influence. All I can say is that I've played this CD for fans of jazz, blues, hard rock, pop, and bluegrass, and all of those people have bought "Drinking From the Water Clock", or gone to a Statue show. You will not be disappointed with this disc.
06/02/2022 14:33:58 - INFO - __main__ - ['positive']
06/02/2022 14:33:58 - INFO - __main__ -  [amazon_polarity] title: snow treasure [SEP] content: This was a gift for my 2nd grade Grandson. He is an advanced reader, and we constantly set the goals for him. I read this the first time at age 28, and it was the start of my reading journeys
06/02/2022 14:33:58 - INFO - __main__ - ['positive']
06/02/2022 14:33:58 - INFO - __main__ -  [amazon_polarity] title: This is the best music in the world [SEP] content: i loved this tape, i listen to it all the time and hope that bruce springstein lives a long and happylife, amen
06/02/2022 14:33:58 - INFO - __main__ - ['positive']
06/02/2022 14:33:58 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:33:58 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:33:58 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 14:33:58 - INFO - __main__ - Global step 1000 Train loss 0.169914 Classification-F1 0.6435422984095551 on epoch=124
06/02/2022 14:33:58 - INFO - __main__ - save last model!
06/02/2022 14:34:05 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 14:34:06 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 14:34:06 - INFO - __main__ - Printing 3 examples
06/02/2022 14:34:06 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 14:34:06 - INFO - __main__ - ['negative']
06/02/2022 14:34:06 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 14:34:06 - INFO - __main__ - ['negative']
06/02/2022 14:34:06 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 14:34:06 - INFO - __main__ - ['negative']
06/02/2022 14:34:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:34:06 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:34:07 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 14:34:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 14:34:11 - INFO - __main__ - Starting training!
06/02/2022 14:34:23 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_100_0.0003_8_predictions.txt
06/02/2022 14:34:23 - INFO - __main__ - Classification-F1 on test data: 0.7156
06/02/2022 14:34:23 - INFO - __main__ - prefix=amazon_polarity_64_100, lr=0.0003, bsz=8, dev_performance=0.6930455635491606, test_performance=0.7156314583700476
06/02/2022 14:34:23 - INFO - __main__ - Running ... prefix=amazon_polarity_64_100, lr=0.0002, bsz=8 ...
06/02/2022 14:34:24 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:34:24 - INFO - __main__ - Printing 3 examples
06/02/2022 14:34:24 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
06/02/2022 14:34:24 - INFO - __main__ - ['positive']
06/02/2022 14:34:24 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
06/02/2022 14:34:24 - INFO - __main__ - ['positive']
06/02/2022 14:34:24 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
06/02/2022 14:34:24 - INFO - __main__ - ['positive']
06/02/2022 14:34:24 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:34:24 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:34:24 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 14:34:24 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:34:24 - INFO - __main__ - Printing 3 examples
06/02/2022 14:34:24 - INFO - __main__ -  [amazon_polarity] title: A must buy [SEP] content: Having no idea what to expect before going to my first Rebecca's Statue show, I asked around. "Like Phish, but more bluesy.", some said. "Highly inspirational", a rollicking young hipster blurted out. The Statue was all these things, and then some. But could they make an album? Many spectacular live bands have trouble going from the live world to the studio world. Well simply put, they got it taken care of. This album ranks high among my favorites. The Statue is one of those bands that has so many influences it would be impossible to name a few bands or styles as their influence. All I can say is that I've played this CD for fans of jazz, blues, hard rock, pop, and bluegrass, and all of those people have bought "Drinking From the Water Clock", or gone to a Statue show. You will not be disappointed with this disc.
06/02/2022 14:34:24 - INFO - __main__ - ['positive']
06/02/2022 14:34:24 - INFO - __main__ -  [amazon_polarity] title: snow treasure [SEP] content: This was a gift for my 2nd grade Grandson. He is an advanced reader, and we constantly set the goals for him. I read this the first time at age 28, and it was the start of my reading journeys
06/02/2022 14:34:24 - INFO - __main__ - ['positive']
06/02/2022 14:34:24 - INFO - __main__ -  [amazon_polarity] title: This is the best music in the world [SEP] content: i loved this tape, i listen to it all the time and hope that bruce springstein lives a long and happylife, amen
06/02/2022 14:34:24 - INFO - __main__ - ['positive']
06/02/2022 14:34:24 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:34:24 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:34:24 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 14:34:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 14:34:35 - INFO - __main__ - Starting training!
06/02/2022 14:34:39 - INFO - __main__ - Step 10 Global step 10 Train loss 23.133678 on epoch=1
06/02/2022 14:34:45 - INFO - __main__ - Step 20 Global step 20 Train loss 17.534472 on epoch=2
06/02/2022 14:34:50 - INFO - __main__ - Step 30 Global step 30 Train loss 16.502911 on epoch=3
06/02/2022 14:34:55 - INFO - __main__ - Step 40 Global step 40 Train loss 15.692029 on epoch=4
06/02/2022 14:35:00 - INFO - __main__ - Step 50 Global step 50 Train loss 15.410321 on epoch=6
06/02/2022 14:35:29 - INFO - __main__ - Global step 50 Train loss 17.654682 Classification-F1 0.0 on epoch=6
06/02/2022 14:35:35 - INFO - __main__ - Step 60 Global step 60 Train loss 14.003990 on epoch=7
06/02/2022 14:35:40 - INFO - __main__ - Step 70 Global step 70 Train loss 13.409868 on epoch=8
06/02/2022 14:35:45 - INFO - __main__ - Step 80 Global step 80 Train loss 12.641613 on epoch=9
06/02/2022 14:35:50 - INFO - __main__ - Step 90 Global step 90 Train loss 11.636670 on epoch=11
06/02/2022 14:35:56 - INFO - __main__ - Step 100 Global step 100 Train loss 11.016726 on epoch=12
06/02/2022 14:36:03 - INFO - __main__ - Global step 100 Train loss 12.541775 Classification-F1 0.004395604395604396 on epoch=12
06/02/2022 14:36:09 - INFO - __main__ - Step 110 Global step 110 Train loss 9.757549 on epoch=13
06/02/2022 14:36:14 - INFO - __main__ - Step 120 Global step 120 Train loss 5.250600 on epoch=14
06/02/2022 14:36:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.779992 on epoch=16
06/02/2022 14:36:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.507830 on epoch=17
06/02/2022 14:36:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.169749 on epoch=18
06/02/2022 14:36:32 - INFO - __main__ - Global step 150 Train loss 3.293144 Classification-F1 0.8903841448495229 on epoch=18
06/02/2022 14:36:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.146185 on epoch=19
06/02/2022 14:36:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.058488 on epoch=21
06/02/2022 14:36:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.072461 on epoch=22
06/02/2022 14:36:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.101497 on epoch=23
06/02/2022 14:36:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.082797 on epoch=24
06/02/2022 14:37:00 - INFO - __main__ - Global step 200 Train loss 0.092286 Classification-F1 0.9140572544710981 on epoch=24
06/02/2022 14:37:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.074658 on epoch=26
06/02/2022 14:37:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.120032 on epoch=27
06/02/2022 14:37:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.204011 on epoch=28
06/02/2022 14:37:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.155914 on epoch=29
06/02/2022 14:37:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.044489 on epoch=31
06/02/2022 14:37:28 - INFO - __main__ - Global step 250 Train loss 0.119821 Classification-F1 0.8982822910935877 on epoch=31
06/02/2022 14:37:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.055855 on epoch=32
06/02/2022 14:37:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.031218 on epoch=33
06/02/2022 14:37:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.061034 on epoch=34
06/02/2022 14:37:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.048014 on epoch=36
06/02/2022 14:37:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.047763 on epoch=37
06/02/2022 14:37:55 - INFO - __main__ - Global step 300 Train loss 0.048777 Classification-F1 0.9218559218559218 on epoch=37
06/02/2022 14:38:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.010178 on epoch=38
06/02/2022 14:38:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.009107 on epoch=39
06/02/2022 14:38:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.015692 on epoch=41
06/02/2022 14:38:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.002240 on epoch=42
06/02/2022 14:38:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.009984 on epoch=43
06/02/2022 14:38:23 - INFO - __main__ - Global step 350 Train loss 0.009440 Classification-F1 0.921875 on epoch=43
06/02/2022 14:38:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.009213 on epoch=44
06/02/2022 14:38:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.023519 on epoch=46
06/02/2022 14:38:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.199345 on epoch=47
06/02/2022 14:38:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.286617 on epoch=48
06/02/2022 14:38:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.392956 on epoch=49
06/02/2022 14:38:52 - INFO - __main__ - Global step 400 Train loss 0.182330 Classification-F1 0.906158357771261 on epoch=49
06/02/2022 14:38:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.111331 on epoch=51
06/02/2022 14:39:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.023822 on epoch=52
06/02/2022 14:39:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.025267 on epoch=53
06/02/2022 14:39:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.074513 on epoch=54
06/02/2022 14:39:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.016216 on epoch=56
06/02/2022 14:39:19 - INFO - __main__ - Global step 450 Train loss 0.050230 Classification-F1 0.9218559218559218 on epoch=56
06/02/2022 14:39:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.054267 on epoch=57
06/02/2022 14:39:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.074789 on epoch=58
06/02/2022 14:39:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.068898 on epoch=59
06/02/2022 14:39:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.100376 on epoch=61
06/02/2022 14:39:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.136813 on epoch=62
06/02/2022 14:39:46 - INFO - __main__ - Global step 500 Train loss 0.087029 Classification-F1 0.9296832082036257 on epoch=62
06/02/2022 14:39:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.046451 on epoch=63
06/02/2022 14:39:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.037951 on epoch=64
06/02/2022 14:40:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.046781 on epoch=66
06/02/2022 14:40:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.190610 on epoch=67
06/02/2022 14:40:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.153853 on epoch=68
06/02/2022 14:40:15 - INFO - __main__ - Global step 550 Train loss 0.095129 Classification-F1 0.8745098039215686 on epoch=68
06/02/2022 14:40:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.036298 on epoch=69
06/02/2022 14:40:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.143029 on epoch=71
06/02/2022 14:40:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.008030 on epoch=72
06/02/2022 14:40:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.247028 on epoch=73
06/02/2022 14:40:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.166687 on epoch=74
06/02/2022 14:40:42 - INFO - __main__ - Global step 600 Train loss 0.120215 Classification-F1 0.8983816793893129 on epoch=74
06/02/2022 14:40:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.113491 on epoch=76
06/02/2022 14:40:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.050119 on epoch=77
06/02/2022 14:40:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.081115 on epoch=78
06/02/2022 14:41:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.038197 on epoch=79
06/02/2022 14:41:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.130444 on epoch=81
06/02/2022 14:41:10 - INFO - __main__ - Global step 650 Train loss 0.082673 Classification-F1 0.906158357771261 on epoch=81
06/02/2022 14:41:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.003786 on epoch=82
06/02/2022 14:41:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.012053 on epoch=83
06/02/2022 14:41:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.022646 on epoch=84
06/02/2022 14:41:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.030172 on epoch=86
06/02/2022 14:41:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.033815 on epoch=87
06/02/2022 14:41:37 - INFO - __main__ - Global step 700 Train loss 0.020495 Classification-F1 0.6717948717948719 on epoch=87
06/02/2022 14:41:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.215730 on epoch=88
06/02/2022 14:41:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.111229 on epoch=89
06/02/2022 14:41:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.130590 on epoch=91
06/02/2022 14:41:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.076501 on epoch=92
06/02/2022 14:42:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.041195 on epoch=93
06/02/2022 14:42:04 - INFO - __main__ - Global step 750 Train loss 0.115049 Classification-F1 0.875 on epoch=93
06/02/2022 14:42:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.023332 on epoch=94
06/02/2022 14:42:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.014799 on epoch=96
06/02/2022 14:42:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.004464 on epoch=97
06/02/2022 14:42:25 - INFO - __main__ - Step 790 Global step 790 Train loss 0.008777 on epoch=98
06/02/2022 14:42:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.007313 on epoch=99
06/02/2022 14:42:32 - INFO - __main__ - Global step 800 Train loss 0.011737 Classification-F1 0.9218559218559218 on epoch=99
06/02/2022 14:42:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.063355 on epoch=101
06/02/2022 14:42:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.030307 on epoch=102
06/02/2022 14:42:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.005322 on epoch=103
06/02/2022 14:42:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.004178 on epoch=104
06/02/2022 14:42:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.020488 on epoch=106
06/02/2022 14:43:00 - INFO - __main__ - Global step 850 Train loss 0.024730 Classification-F1 0.8984313007385705 on epoch=106
06/02/2022 14:43:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.031026 on epoch=107
06/02/2022 14:43:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.007415 on epoch=108
06/02/2022 14:43:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.003552 on epoch=109
06/02/2022 14:43:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.008276 on epoch=111
06/02/2022 14:43:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.010935 on epoch=112
06/02/2022 14:43:28 - INFO - __main__ - Global step 900 Train loss 0.012241 Classification-F1 0.8982822910935877 on epoch=112
06/02/2022 14:43:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.008443 on epoch=113
06/02/2022 14:43:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.004019 on epoch=114
06/02/2022 14:43:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000935 on epoch=116
06/02/2022 14:43:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000590 on epoch=117
06/02/2022 14:43:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.006003 on epoch=118
06/02/2022 14:43:55 - INFO - __main__ - Global step 950 Train loss 0.003998 Classification-F1 0.9062271062271061 on epoch=118
06/02/2022 14:44:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000366 on epoch=119
06/02/2022 14:44:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.001796 on epoch=121
06/02/2022 14:44:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000122 on epoch=122
06/02/2022 14:44:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.001324 on epoch=123
06/02/2022 14:44:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.010866 on epoch=124
06/02/2022 14:44:22 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:44:22 - INFO - __main__ - Printing 3 examples
06/02/2022 14:44:22 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
06/02/2022 14:44:22 - INFO - __main__ - ['positive']
06/02/2022 14:44:22 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
06/02/2022 14:44:22 - INFO - __main__ - ['positive']
06/02/2022 14:44:22 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
06/02/2022 14:44:22 - INFO - __main__ - ['positive']
06/02/2022 14:44:22 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:44:22 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:44:22 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 14:44:22 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:44:22 - INFO - __main__ - Printing 3 examples
06/02/2022 14:44:22 - INFO - __main__ -  [amazon_polarity] title: A must buy [SEP] content: Having no idea what to expect before going to my first Rebecca's Statue show, I asked around. "Like Phish, but more bluesy.", some said. "Highly inspirational", a rollicking young hipster blurted out. The Statue was all these things, and then some. But could they make an album? Many spectacular live bands have trouble going from the live world to the studio world. Well simply put, they got it taken care of. This album ranks high among my favorites. The Statue is one of those bands that has so many influences it would be impossible to name a few bands or styles as their influence. All I can say is that I've played this CD for fans of jazz, blues, hard rock, pop, and bluegrass, and all of those people have bought "Drinking From the Water Clock", or gone to a Statue show. You will not be disappointed with this disc.
06/02/2022 14:44:22 - INFO - __main__ - ['positive']
06/02/2022 14:44:22 - INFO - __main__ -  [amazon_polarity] title: snow treasure [SEP] content: This was a gift for my 2nd grade Grandson. He is an advanced reader, and we constantly set the goals for him. I read this the first time at age 28, and it was the start of my reading journeys
06/02/2022 14:44:22 - INFO - __main__ - ['positive']
06/02/2022 14:44:22 - INFO - __main__ -  [amazon_polarity] title: This is the best music in the world [SEP] content: i loved this tape, i listen to it all the time and hope that bruce springstein lives a long and happylife, amen
06/02/2022 14:44:22 - INFO - __main__ - ['positive']
06/02/2022 14:44:22 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:44:22 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:44:22 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 14:44:23 - INFO - __main__ - Global step 1000 Train loss 0.002895 Classification-F1 0.9140152671755726 on epoch=124
06/02/2022 14:44:23 - INFO - __main__ - save last model!
06/02/2022 14:44:30 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 14:44:30 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 14:44:30 - INFO - __main__ - Printing 3 examples
06/02/2022 14:44:30 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 14:44:30 - INFO - __main__ - ['negative']
06/02/2022 14:44:30 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 14:44:30 - INFO - __main__ - ['negative']
06/02/2022 14:44:30 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 14:44:30 - INFO - __main__ - ['negative']
06/02/2022 14:44:30 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:44:31 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:44:32 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 14:44:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 14:44:33 - INFO - __main__ - Starting training!
06/02/2022 14:44:46 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_100_0.0002_8_predictions.txt
06/02/2022 14:44:46 - INFO - __main__ - Classification-F1 on test data: 0.9560
06/02/2022 14:44:47 - INFO - __main__ - prefix=amazon_polarity_64_100, lr=0.0002, bsz=8, dev_performance=0.9296832082036257, test_performance=0.9559936630874846
06/02/2022 14:44:47 - INFO - __main__ - Running ... prefix=amazon_polarity_64_100, lr=0.0001, bsz=8 ...
06/02/2022 14:44:48 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:44:48 - INFO - __main__ - Printing 3 examples
06/02/2022 14:44:48 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
06/02/2022 14:44:48 - INFO - __main__ - ['positive']
06/02/2022 14:44:48 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
06/02/2022 14:44:48 - INFO - __main__ - ['positive']
06/02/2022 14:44:48 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
06/02/2022 14:44:48 - INFO - __main__ - ['positive']
06/02/2022 14:44:48 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:44:48 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:44:48 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 14:44:48 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:44:48 - INFO - __main__ - Printing 3 examples
06/02/2022 14:44:48 - INFO - __main__ -  [amazon_polarity] title: A must buy [SEP] content: Having no idea what to expect before going to my first Rebecca's Statue show, I asked around. "Like Phish, but more bluesy.", some said. "Highly inspirational", a rollicking young hipster blurted out. The Statue was all these things, and then some. But could they make an album? Many spectacular live bands have trouble going from the live world to the studio world. Well simply put, they got it taken care of. This album ranks high among my favorites. The Statue is one of those bands that has so many influences it would be impossible to name a few bands or styles as their influence. All I can say is that I've played this CD for fans of jazz, blues, hard rock, pop, and bluegrass, and all of those people have bought "Drinking From the Water Clock", or gone to a Statue show. You will not be disappointed with this disc.
06/02/2022 14:44:48 - INFO - __main__ - ['positive']
06/02/2022 14:44:48 - INFO - __main__ -  [amazon_polarity] title: snow treasure [SEP] content: This was a gift for my 2nd grade Grandson. He is an advanced reader, and we constantly set the goals for him. I read this the first time at age 28, and it was the start of my reading journeys
06/02/2022 14:44:48 - INFO - __main__ - ['positive']
06/02/2022 14:44:48 - INFO - __main__ -  [amazon_polarity] title: This is the best music in the world [SEP] content: i loved this tape, i listen to it all the time and hope that bruce springstein lives a long and happylife, amen
06/02/2022 14:44:48 - INFO - __main__ - ['positive']
06/02/2022 14:44:48 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:44:48 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:44:48 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 14:45:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 14:45:01 - INFO - __main__ - Starting training!
06/02/2022 14:45:05 - INFO - __main__ - Step 10 Global step 10 Train loss 22.682730 on epoch=1
06/02/2022 14:45:10 - INFO - __main__ - Step 20 Global step 20 Train loss 19.711084 on epoch=2
06/02/2022 14:45:15 - INFO - __main__ - Step 30 Global step 30 Train loss 18.333893 on epoch=3
06/02/2022 14:45:20 - INFO - __main__ - Step 40 Global step 40 Train loss 17.385258 on epoch=4
06/02/2022 14:45:25 - INFO - __main__ - Step 50 Global step 50 Train loss 16.665237 on epoch=6
06/02/2022 14:46:09 - INFO - __main__ - Global step 50 Train loss 18.955641 Classification-F1 0.0 on epoch=6
06/02/2022 14:46:14 - INFO - __main__ - Step 60 Global step 60 Train loss 16.148026 on epoch=7
06/02/2022 14:46:20 - INFO - __main__ - Step 70 Global step 70 Train loss 16.061054 on epoch=8
06/02/2022 14:46:25 - INFO - __main__ - Step 80 Global step 80 Train loss 15.010478 on epoch=9
06/02/2022 14:46:30 - INFO - __main__ - Step 90 Global step 90 Train loss 15.187958 on epoch=11
06/02/2022 14:46:35 - INFO - __main__ - Step 100 Global step 100 Train loss 15.015762 on epoch=12
06/02/2022 14:47:06 - INFO - __main__ - Global step 100 Train loss 15.484655 Classification-F1 0.0 on epoch=12
06/02/2022 14:47:11 - INFO - __main__ - Step 110 Global step 110 Train loss 14.309237 on epoch=13
06/02/2022 14:47:16 - INFO - __main__ - Step 120 Global step 120 Train loss 14.093562 on epoch=14
06/02/2022 14:47:22 - INFO - __main__ - Step 130 Global step 130 Train loss 12.964816 on epoch=16
06/02/2022 14:47:27 - INFO - __main__ - Step 140 Global step 140 Train loss 13.786984 on epoch=17
06/02/2022 14:47:32 - INFO - __main__ - Step 150 Global step 150 Train loss 13.261393 on epoch=18
06/02/2022 14:47:48 - INFO - __main__ - Global step 150 Train loss 13.683198 Classification-F1 0.0 on epoch=18
06/02/2022 14:47:53 - INFO - __main__ - Step 160 Global step 160 Train loss 13.206716 on epoch=19
06/02/2022 14:47:58 - INFO - __main__ - Step 170 Global step 170 Train loss 12.061855 on epoch=21
06/02/2022 14:48:03 - INFO - __main__ - Step 180 Global step 180 Train loss 12.175517 on epoch=22
06/02/2022 14:48:08 - INFO - __main__ - Step 190 Global step 190 Train loss 11.836049 on epoch=23
06/02/2022 14:48:13 - INFO - __main__ - Step 200 Global step 200 Train loss 10.610410 on epoch=24
06/02/2022 14:48:28 - INFO - __main__ - Global step 200 Train loss 11.978110 Classification-F1 0.0 on epoch=24
06/02/2022 14:48:33 - INFO - __main__ - Step 210 Global step 210 Train loss 10.397563 on epoch=26
06/02/2022 14:48:38 - INFO - __main__ - Step 220 Global step 220 Train loss 8.525496 on epoch=27
06/02/2022 14:48:43 - INFO - __main__ - Step 230 Global step 230 Train loss 7.392996 on epoch=28
06/02/2022 14:48:48 - INFO - __main__ - Step 240 Global step 240 Train loss 4.127591 on epoch=29
06/02/2022 14:48:53 - INFO - __main__ - Step 250 Global step 250 Train loss 3.846260 on epoch=31
06/02/2022 14:48:55 - INFO - __main__ - Global step 250 Train loss 6.857981 Classification-F1 0.48594377510040165 on epoch=31
06/02/2022 14:49:01 - INFO - __main__ - Step 260 Global step 260 Train loss 2.062219 on epoch=32
06/02/2022 14:49:06 - INFO - __main__ - Step 270 Global step 270 Train loss 2.391275 on epoch=33
06/02/2022 14:49:11 - INFO - __main__ - Step 280 Global step 280 Train loss 2.197966 on epoch=34
06/02/2022 14:49:16 - INFO - __main__ - Step 290 Global step 290 Train loss 2.201933 on epoch=36
06/02/2022 14:49:21 - INFO - __main__ - Step 300 Global step 300 Train loss 2.079152 on epoch=37
06/02/2022 14:49:23 - INFO - __main__ - Global step 300 Train loss 2.186509 Classification-F1 0.6457861635220126 on epoch=37
06/02/2022 14:49:29 - INFO - __main__ - Step 310 Global step 310 Train loss 2.002949 on epoch=38
06/02/2022 14:49:34 - INFO - __main__ - Step 320 Global step 320 Train loss 1.613783 on epoch=39
06/02/2022 14:49:39 - INFO - __main__ - Step 330 Global step 330 Train loss 1.409825 on epoch=41
06/02/2022 14:49:44 - INFO - __main__ - Step 340 Global step 340 Train loss 1.590154 on epoch=42
06/02/2022 14:49:49 - INFO - __main__ - Step 350 Global step 350 Train loss 1.457212 on epoch=43
06/02/2022 14:49:51 - INFO - __main__ - Global step 350 Train loss 1.614785 Classification-F1 0.3750290630086026 on epoch=43
06/02/2022 14:49:56 - INFO - __main__ - Step 360 Global step 360 Train loss 1.333189 on epoch=44
06/02/2022 14:50:01 - INFO - __main__ - Step 370 Global step 370 Train loss 1.043387 on epoch=46
06/02/2022 14:50:06 - INFO - __main__ - Step 380 Global step 380 Train loss 1.342102 on epoch=47
06/02/2022 14:50:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.869288 on epoch=48
06/02/2022 14:50:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.746112 on epoch=49
06/02/2022 14:50:18 - INFO - __main__ - Global step 400 Train loss 1.066816 Classification-F1 0.8592375366568914 on epoch=49
06/02/2022 14:50:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.737177 on epoch=51
06/02/2022 14:50:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.650231 on epoch=52
06/02/2022 14:50:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.462123 on epoch=53
06/02/2022 14:50:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.260578 on epoch=54
06/02/2022 14:50:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.340386 on epoch=56
06/02/2022 14:50:47 - INFO - __main__ - Global step 450 Train loss 0.490099 Classification-F1 0.906158357771261 on epoch=56
06/02/2022 14:50:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.169675 on epoch=57
06/02/2022 14:50:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.114259 on epoch=58
06/02/2022 14:51:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.152478 on epoch=59
06/02/2022 14:51:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.239912 on epoch=61
06/02/2022 14:51:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.225937 on epoch=62
06/02/2022 14:51:14 - INFO - __main__ - Global step 500 Train loss 0.180452 Classification-F1 0.906158357771261 on epoch=62
06/02/2022 14:51:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.277418 on epoch=63
06/02/2022 14:51:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.155268 on epoch=64
06/02/2022 14:51:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.122719 on epoch=66
06/02/2022 14:51:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.042197 on epoch=67
06/02/2022 14:51:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.061119 on epoch=68
06/02/2022 14:51:42 - INFO - __main__ - Global step 550 Train loss 0.131744 Classification-F1 0.921875 on epoch=68
06/02/2022 14:51:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.115089 on epoch=69
06/02/2022 14:51:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.050513 on epoch=71
06/02/2022 14:51:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.062727 on epoch=72
06/02/2022 14:52:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.070367 on epoch=73
06/02/2022 14:52:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.040741 on epoch=74
06/02/2022 14:52:09 - INFO - __main__ - Global step 600 Train loss 0.067888 Classification-F1 0.9374847374847375 on epoch=74
06/02/2022 14:52:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.071426 on epoch=76
06/02/2022 14:52:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.044091 on epoch=77
06/02/2022 14:52:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.060859 on epoch=78
06/02/2022 14:52:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.040573 on epoch=79
06/02/2022 14:52:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.032894 on epoch=81
06/02/2022 14:52:38 - INFO - __main__ - Global step 650 Train loss 0.049969 Classification-F1 0.9140152671755726 on epoch=81
06/02/2022 14:52:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.022330 on epoch=82
06/02/2022 14:52:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.033814 on epoch=83
06/02/2022 14:52:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.024981 on epoch=84
06/02/2022 14:52:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.034794 on epoch=86
06/02/2022 14:53:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.009641 on epoch=87
06/02/2022 14:53:05 - INFO - __main__ - Global step 700 Train loss 0.025112 Classification-F1 0.9062271062271061 on epoch=87
06/02/2022 14:53:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.007516 on epoch=88
06/02/2022 14:53:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.006071 on epoch=89
06/02/2022 14:53:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.014754 on epoch=91
06/02/2022 14:53:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.004831 on epoch=92
06/02/2022 14:53:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.006169 on epoch=93
06/02/2022 14:53:33 - INFO - __main__ - Global step 750 Train loss 0.007868 Classification-F1 0.9375 on epoch=93
06/02/2022 14:53:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.027455 on epoch=94
06/02/2022 14:53:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.006295 on epoch=96
06/02/2022 14:53:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.012388 on epoch=97
06/02/2022 14:53:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.017266 on epoch=98
06/02/2022 14:54:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.007317 on epoch=99
06/02/2022 14:54:02 - INFO - __main__ - Global step 800 Train loss 0.014144 Classification-F1 0.9140152671755726 on epoch=99
06/02/2022 14:54:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.061081 on epoch=101
06/02/2022 14:54:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.033684 on epoch=102
06/02/2022 14:54:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.017940 on epoch=103
06/02/2022 14:54:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.037722 on epoch=104
06/02/2022 14:54:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.035919 on epoch=106
06/02/2022 14:54:29 - INFO - __main__ - Global step 850 Train loss 0.037269 Classification-F1 0.9374847374847375 on epoch=106
06/02/2022 14:54:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.046935 on epoch=107
06/02/2022 14:54:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.024390 on epoch=108
06/02/2022 14:54:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.020913 on epoch=109
06/02/2022 14:54:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.076934 on epoch=111
06/02/2022 14:54:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.017641 on epoch=112
06/02/2022 14:54:57 - INFO - __main__ - Global step 900 Train loss 0.037363 Classification-F1 0.9296832082036257 on epoch=112
06/02/2022 14:55:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.117104 on epoch=113
06/02/2022 14:55:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.031105 on epoch=114
06/02/2022 14:55:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.154480 on epoch=116
06/02/2022 14:55:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.008013 on epoch=117
06/02/2022 14:55:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.071949 on epoch=118
06/02/2022 14:55:24 - INFO - __main__ - Global step 950 Train loss 0.076530 Classification-F1 0.929648854961832 on epoch=118
06/02/2022 14:55:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.015585 on epoch=119
06/02/2022 14:55:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.003048 on epoch=121
06/02/2022 14:55:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.010865 on epoch=122
06/02/2022 14:55:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.038280 on epoch=123
06/02/2022 14:55:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.004671 on epoch=124
06/02/2022 14:55:52 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:55:52 - INFO - __main__ - Printing 3 examples
06/02/2022 14:55:52 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
06/02/2022 14:55:52 - INFO - __main__ - ['negative']
06/02/2022 14:55:52 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
06/02/2022 14:55:52 - INFO - __main__ - ['negative']
06/02/2022 14:55:52 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
06/02/2022 14:55:52 - INFO - __main__ - ['negative']
06/02/2022 14:55:52 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:55:52 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:55:52 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 14:55:52 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:55:52 - INFO - __main__ - Printing 3 examples
06/02/2022 14:55:52 - INFO - __main__ -  [amazon_polarity] title: not made of driftwood [SEP] content: please read the description carefully,,,,this is not made out of wood but made out of resin in china....it is heavy enough but not hand sculpted or even scuplted at all.....for that reason i am dissappointed...I am not sure why they put driftwood in the name of this product but for that reason i feel deceived and dissappointed....this product is not what i was expecting at all....
06/02/2022 14:55:52 - INFO - __main__ - ['negative']
06/02/2022 14:55:52 - INFO - __main__ -  [amazon_polarity] title: Farfetched [SEP] content: The beginning of the book is a thriller, I can accept that, but when a 17 years old kid made a plan to find the person who want to kill her and everybody accept her as the bait is really farfetched. Everything they do to trap the killer is pathetic. The trip to Italy and changing the newspapers in the web is hilarious. The end, when everybody stays alive except the killer is just a way to end this fairy tale in the most pathetic way.
06/02/2022 14:55:52 - INFO - __main__ - ['negative']
06/02/2022 14:55:52 - INFO - __main__ -  [amazon_polarity] title: Not all the songs you thought you was purchasing came with the album [SEP] content: I only got 5 songs i did not get all the songs that's suppose to come with the entire album
06/02/2022 14:55:52 - INFO - __main__ - ['negative']
06/02/2022 14:55:52 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:55:52 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:55:52 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 14:55:52 - INFO - __main__ - Global step 1000 Train loss 0.014490 Classification-F1 0.9140572544710981 on epoch=124
06/02/2022 14:55:52 - INFO - __main__ - save last model!
06/02/2022 14:55:59 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 14:56:00 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 14:56:00 - INFO - __main__ - Printing 3 examples
06/02/2022 14:56:00 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 14:56:00 - INFO - __main__ - ['negative']
06/02/2022 14:56:00 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 14:56:00 - INFO - __main__ - ['negative']
06/02/2022 14:56:00 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 14:56:00 - INFO - __main__ - ['negative']
06/02/2022 14:56:00 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:56:00 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:56:01 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 14:56:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 14:56:02 - INFO - __main__ - Starting training!
06/02/2022 14:56:17 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_100_0.0001_8_predictions.txt
06/02/2022 14:56:17 - INFO - __main__ - Classification-F1 on test data: 0.9600
06/02/2022 14:56:17 - INFO - __main__ - prefix=amazon_polarity_64_100, lr=0.0001, bsz=8, dev_performance=0.9375, test_performance=0.9599959995999601
06/02/2022 14:56:17 - INFO - __main__ - Running ... prefix=amazon_polarity_64_13, lr=0.0005, bsz=8 ...
06/02/2022 14:56:18 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:56:18 - INFO - __main__ - Printing 3 examples
06/02/2022 14:56:18 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
06/02/2022 14:56:18 - INFO - __main__ - ['negative']
06/02/2022 14:56:18 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
06/02/2022 14:56:18 - INFO - __main__ - ['negative']
06/02/2022 14:56:18 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
06/02/2022 14:56:18 - INFO - __main__ - ['negative']
06/02/2022 14:56:18 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:56:18 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:56:18 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 14:56:18 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 14:56:18 - INFO - __main__ - Printing 3 examples
06/02/2022 14:56:18 - INFO - __main__ -  [amazon_polarity] title: not made of driftwood [SEP] content: please read the description carefully,,,,this is not made out of wood but made out of resin in china....it is heavy enough but not hand sculpted or even scuplted at all.....for that reason i am dissappointed...I am not sure why they put driftwood in the name of this product but for that reason i feel deceived and dissappointed....this product is not what i was expecting at all....
06/02/2022 14:56:18 - INFO - __main__ - ['negative']
06/02/2022 14:56:18 - INFO - __main__ -  [amazon_polarity] title: Farfetched [SEP] content: The beginning of the book is a thriller, I can accept that, but when a 17 years old kid made a plan to find the person who want to kill her and everybody accept her as the bait is really farfetched. Everything they do to trap the killer is pathetic. The trip to Italy and changing the newspapers in the web is hilarious. The end, when everybody stays alive except the killer is just a way to end this fairy tale in the most pathetic way.
06/02/2022 14:56:18 - INFO - __main__ - ['negative']
06/02/2022 14:56:18 - INFO - __main__ -  [amazon_polarity] title: Not all the songs you thought you was purchasing came with the album [SEP] content: I only got 5 songs i did not get all the songs that's suppose to come with the entire album
06/02/2022 14:56:18 - INFO - __main__ - ['negative']
06/02/2022 14:56:18 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:56:18 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:56:18 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 14:56:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 14:56:31 - INFO - __main__ - Starting training!
06/02/2022 14:56:36 - INFO - __main__ - Step 10 Global step 10 Train loss 22.516012 on epoch=1
06/02/2022 14:56:41 - INFO - __main__ - Step 20 Global step 20 Train loss 17.688776 on epoch=2
06/02/2022 14:56:46 - INFO - __main__ - Step 30 Global step 30 Train loss 14.710234 on epoch=3
06/02/2022 14:56:51 - INFO - __main__ - Step 40 Global step 40 Train loss 12.618975 on epoch=4
06/02/2022 14:56:57 - INFO - __main__ - Step 50 Global step 50 Train loss 9.355383 on epoch=6
06/02/2022 14:57:49 - INFO - __main__ - Global step 50 Train loss 15.377876 Classification-F1 0.0 on epoch=6
06/02/2022 14:57:55 - INFO - __main__ - Step 60 Global step 60 Train loss 2.327670 on epoch=7
06/02/2022 14:58:00 - INFO - __main__ - Step 70 Global step 70 Train loss 0.478780 on epoch=8
06/02/2022 14:58:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.247955 on epoch=9
06/02/2022 14:58:11 - INFO - __main__ - Step 90 Global step 90 Train loss 0.093666 on epoch=11
06/02/2022 14:58:17 - INFO - __main__ - Step 100 Global step 100 Train loss 0.064735 on epoch=12
06/02/2022 14:58:18 - INFO - __main__ - Global step 100 Train loss 0.642561 Classification-F1 0.9218559218559218 on epoch=12
06/02/2022 14:58:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.100348 on epoch=13
06/02/2022 14:58:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.129081 on epoch=14
06/02/2022 14:58:35 - INFO - __main__ - Step 130 Global step 130 Train loss 0.049570 on epoch=16
06/02/2022 14:58:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.033283 on epoch=17
06/02/2022 14:58:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.417532 on epoch=18
06/02/2022 14:58:47 - INFO - __main__ - Global step 150 Train loss 0.145963 Classification-F1 0.9217986314760509 on epoch=18
06/02/2022 14:58:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.019332 on epoch=19
06/02/2022 14:58:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.061371 on epoch=21
06/02/2022 14:59:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.005686 on epoch=22
06/02/2022 14:59:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.001697 on epoch=23
06/02/2022 14:59:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.004132 on epoch=24
06/02/2022 14:59:16 - INFO - __main__ - Global step 200 Train loss 0.018443 Classification-F1 0.9374389051808407 on epoch=24
06/02/2022 14:59:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.003972 on epoch=26
06/02/2022 14:59:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.073231 on epoch=27
06/02/2022 14:59:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.008382 on epoch=28
06/02/2022 14:59:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.001858 on epoch=29
06/02/2022 14:59:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.000832 on epoch=31
06/02/2022 14:59:45 - INFO - __main__ - Global step 250 Train loss 0.017655 Classification-F1 0.9296832082036257 on epoch=31
06/02/2022 14:59:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.000216 on epoch=32
06/02/2022 14:59:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.000169 on epoch=33
06/02/2022 15:00:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.000208 on epoch=34
06/02/2022 15:00:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.000378 on epoch=36
06/02/2022 15:00:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.000130 on epoch=37
06/02/2022 15:00:13 - INFO - __main__ - Global step 300 Train loss 0.000220 Classification-F1 0.9296832082036257 on epoch=37
06/02/2022 15:00:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000113 on epoch=38
06/02/2022 15:00:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.007936 on epoch=39
06/02/2022 15:00:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.015964 on epoch=41
06/02/2022 15:00:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000182 on epoch=42
06/02/2022 15:00:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000106 on epoch=43
06/02/2022 15:00:42 - INFO - __main__ - Global step 350 Train loss 0.004860 Classification-F1 0.9374389051808407 on epoch=43
06/02/2022 15:00:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000398 on epoch=44
06/02/2022 15:00:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.000427 on epoch=46
06/02/2022 15:00:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.000888 on epoch=47
06/02/2022 15:01:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000103 on epoch=48
06/02/2022 15:01:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.001184 on epoch=49
06/02/2022 15:01:10 - INFO - __main__ - Global step 400 Train loss 0.000600 Classification-F1 0.9218559218559218 on epoch=49
06/02/2022 15:01:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.001635 on epoch=51
06/02/2022 15:01:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.003475 on epoch=52
06/02/2022 15:01:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000257 on epoch=53
06/02/2022 15:01:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000094 on epoch=54
06/02/2022 15:01:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000092 on epoch=56
06/02/2022 15:01:39 - INFO - __main__ - Global step 450 Train loss 0.001111 Classification-F1 0.9373623684854416 on epoch=56
06/02/2022 15:01:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000093 on epoch=57
06/02/2022 15:01:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000035 on epoch=58
06/02/2022 15:01:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000040 on epoch=59
06/02/2022 15:02:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.003997 on epoch=61
06/02/2022 15:02:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.014915 on epoch=62
06/02/2022 15:02:07 - INFO - __main__ - Global step 500 Train loss 0.003816 Classification-F1 0.9374389051808407 on epoch=62
06/02/2022 15:02:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000290 on epoch=63
06/02/2022 15:02:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.018524 on epoch=64
06/02/2022 15:02:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.128960 on epoch=66
06/02/2022 15:02:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.022669 on epoch=67
06/02/2022 15:02:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.032560 on epoch=68
06/02/2022 15:02:35 - INFO - __main__ - Global step 550 Train loss 0.040600 Classification-F1 0.9140152671755726 on epoch=68
06/02/2022 15:02:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000158 on epoch=69
06/02/2022 15:02:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.141576 on epoch=71
06/02/2022 15:02:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.003274 on epoch=72
06/02/2022 15:02:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.002998 on epoch=73
06/02/2022 15:03:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.002161 on epoch=74
06/02/2022 15:03:04 - INFO - __main__ - Global step 600 Train loss 0.030033 Classification-F1 0.9140572544710981 on epoch=74
06/02/2022 15:03:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000303 on epoch=76
06/02/2022 15:03:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.001869 on epoch=77
06/02/2022 15:03:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000074 on epoch=78
06/02/2022 15:03:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000153 on epoch=79
06/02/2022 15:03:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.012030 on epoch=81
06/02/2022 15:03:32 - INFO - __main__ - Global step 650 Train loss 0.002886 Classification-F1 0.9375 on epoch=81
06/02/2022 15:03:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000367 on epoch=82
06/02/2022 15:03:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000370 on epoch=83
06/02/2022 15:03:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000049 on epoch=84
06/02/2022 15:03:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000023 on epoch=86
06/02/2022 15:03:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000017 on epoch=87
06/02/2022 15:04:01 - INFO - __main__ - Global step 700 Train loss 0.000165 Classification-F1 0.9453091619361533 on epoch=87
06/02/2022 15:04:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000035 on epoch=88
06/02/2022 15:04:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000035 on epoch=89
06/02/2022 15:04:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000031 on epoch=91
06/02/2022 15:04:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000104 on epoch=92
06/02/2022 15:04:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000020 on epoch=93
06/02/2022 15:04:30 - INFO - __main__ - Global step 750 Train loss 0.000045 Classification-F1 0.9453091619361533 on epoch=93
06/02/2022 15:04:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000015 on epoch=94
06/02/2022 15:04:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000014 on epoch=96
06/02/2022 15:04:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000025 on epoch=97
06/02/2022 15:04:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000779 on epoch=98
06/02/2022 15:04:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000029 on epoch=99
06/02/2022 15:04:59 - INFO - __main__ - Global step 800 Train loss 0.000172 Classification-F1 0.9296832082036257 on epoch=99
06/02/2022 15:05:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000058 on epoch=101
06/02/2022 15:05:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000607 on epoch=102
06/02/2022 15:05:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000147 on epoch=103
06/02/2022 15:05:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000058 on epoch=104
06/02/2022 15:05:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000081 on epoch=106
06/02/2022 15:05:27 - INFO - __main__ - Global step 850 Train loss 0.000190 Classification-F1 0.9218559218559218 on epoch=106
06/02/2022 15:05:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000056 on epoch=107
06/02/2022 15:05:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000039 on epoch=108
06/02/2022 15:05:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.038332 on epoch=109
06/02/2022 15:05:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001779 on epoch=111
06/02/2022 15:05:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000098 on epoch=112
06/02/2022 15:05:56 - INFO - __main__ - Global step 900 Train loss 0.008061 Classification-F1 0.9296832082036257 on epoch=112
06/02/2022 15:06:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000040 on epoch=113
06/02/2022 15:06:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000040 on epoch=114
06/02/2022 15:06:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000036 on epoch=116
06/02/2022 15:06:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000020 on epoch=117
06/02/2022 15:06:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000025 on epoch=118
06/02/2022 15:06:24 - INFO - __main__ - Global step 950 Train loss 0.000032 Classification-F1 0.9296832082036257 on epoch=118
06/02/2022 15:06:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000025 on epoch=119
06/02/2022 15:06:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000034 on epoch=121
06/02/2022 15:06:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000014 on epoch=122
06/02/2022 15:06:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000127 on epoch=123
06/02/2022 15:06:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000012 on epoch=124
06/02/2022 15:06:52 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:06:52 - INFO - __main__ - Printing 3 examples
06/02/2022 15:06:52 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
06/02/2022 15:06:52 - INFO - __main__ - ['negative']
06/02/2022 15:06:52 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
06/02/2022 15:06:52 - INFO - __main__ - ['negative']
06/02/2022 15:06:52 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
06/02/2022 15:06:52 - INFO - __main__ - ['negative']
06/02/2022 15:06:52 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:06:52 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:06:52 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 15:06:52 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:06:52 - INFO - __main__ - Printing 3 examples
06/02/2022 15:06:52 - INFO - __main__ -  [amazon_polarity] title: not made of driftwood [SEP] content: please read the description carefully,,,,this is not made out of wood but made out of resin in china....it is heavy enough but not hand sculpted or even scuplted at all.....for that reason i am dissappointed...I am not sure why they put driftwood in the name of this product but for that reason i feel deceived and dissappointed....this product is not what i was expecting at all....
06/02/2022 15:06:52 - INFO - __main__ - ['negative']
06/02/2022 15:06:52 - INFO - __main__ -  [amazon_polarity] title: Farfetched [SEP] content: The beginning of the book is a thriller, I can accept that, but when a 17 years old kid made a plan to find the person who want to kill her and everybody accept her as the bait is really farfetched. Everything they do to trap the killer is pathetic. The trip to Italy and changing the newspapers in the web is hilarious. The end, when everybody stays alive except the killer is just a way to end this fairy tale in the most pathetic way.
06/02/2022 15:06:52 - INFO - __main__ - ['negative']
06/02/2022 15:06:52 - INFO - __main__ -  [amazon_polarity] title: Not all the songs you thought you was purchasing came with the album [SEP] content: I only got 5 songs i did not get all the songs that's suppose to come with the entire album
06/02/2022 15:06:52 - INFO - __main__ - ['negative']
06/02/2022 15:06:52 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:06:52 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:06:52 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 15:06:52 - INFO - __main__ - Global step 1000 Train loss 0.000042 Classification-F1 0.9296832082036257 on epoch=124
06/02/2022 15:06:52 - INFO - __main__ - save last model!
06/02/2022 15:06:59 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 15:07:00 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 15:07:00 - INFO - __main__ - Printing 3 examples
06/02/2022 15:07:00 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 15:07:00 - INFO - __main__ - ['negative']
06/02/2022 15:07:00 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 15:07:00 - INFO - __main__ - ['negative']
06/02/2022 15:07:00 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 15:07:00 - INFO - __main__ - ['negative']
06/02/2022 15:07:00 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:07:00 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:07:01 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 15:07:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 15:07:02 - INFO - __main__ - Starting training!
06/02/2022 15:07:17 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_13_0.0005_8_predictions.txt
06/02/2022 15:07:17 - INFO - __main__ - Classification-F1 on test data: 0.9390
06/02/2022 15:07:17 - INFO - __main__ - prefix=amazon_polarity_64_13, lr=0.0005, bsz=8, dev_performance=0.9453091619361533, test_performance=0.9389618511569731
06/02/2022 15:07:17 - INFO - __main__ - Running ... prefix=amazon_polarity_64_13, lr=0.0003, bsz=8 ...
06/02/2022 15:07:18 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:07:18 - INFO - __main__ - Printing 3 examples
06/02/2022 15:07:18 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
06/02/2022 15:07:18 - INFO - __main__ - ['negative']
06/02/2022 15:07:18 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
06/02/2022 15:07:18 - INFO - __main__ - ['negative']
06/02/2022 15:07:18 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
06/02/2022 15:07:18 - INFO - __main__ - ['negative']
06/02/2022 15:07:18 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:07:18 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:07:18 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 15:07:18 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:07:18 - INFO - __main__ - Printing 3 examples
06/02/2022 15:07:18 - INFO - __main__ -  [amazon_polarity] title: not made of driftwood [SEP] content: please read the description carefully,,,,this is not made out of wood but made out of resin in china....it is heavy enough but not hand sculpted or even scuplted at all.....for that reason i am dissappointed...I am not sure why they put driftwood in the name of this product but for that reason i feel deceived and dissappointed....this product is not what i was expecting at all....
06/02/2022 15:07:18 - INFO - __main__ - ['negative']
06/02/2022 15:07:18 - INFO - __main__ -  [amazon_polarity] title: Farfetched [SEP] content: The beginning of the book is a thriller, I can accept that, but when a 17 years old kid made a plan to find the person who want to kill her and everybody accept her as the bait is really farfetched. Everything they do to trap the killer is pathetic. The trip to Italy and changing the newspapers in the web is hilarious. The end, when everybody stays alive except the killer is just a way to end this fairy tale in the most pathetic way.
06/02/2022 15:07:18 - INFO - __main__ - ['negative']
06/02/2022 15:07:18 - INFO - __main__ -  [amazon_polarity] title: Not all the songs you thought you was purchasing came with the album [SEP] content: I only got 5 songs i did not get all the songs that's suppose to come with the entire album
06/02/2022 15:07:18 - INFO - __main__ - ['negative']
06/02/2022 15:07:18 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:07:18 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:07:18 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 15:07:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 15:07:29 - INFO - __main__ - Starting training!
06/02/2022 15:07:34 - INFO - __main__ - Step 10 Global step 10 Train loss 22.841234 on epoch=1
06/02/2022 15:07:39 - INFO - __main__ - Step 20 Global step 20 Train loss 18.484577 on epoch=2
06/02/2022 15:07:44 - INFO - __main__ - Step 30 Global step 30 Train loss 17.323803 on epoch=3
06/02/2022 15:07:50 - INFO - __main__ - Step 40 Global step 40 Train loss 14.574310 on epoch=4
06/02/2022 15:07:55 - INFO - __main__ - Step 50 Global step 50 Train loss 13.447545 on epoch=6
06/02/2022 15:07:57 - INFO - __main__ - Global step 50 Train loss 17.334293 Classification-F1 0.0 on epoch=6
06/02/2022 15:08:03 - INFO - __main__ - Step 60 Global step 60 Train loss 12.776186 on epoch=7
06/02/2022 15:08:09 - INFO - __main__ - Step 70 Global step 70 Train loss 11.695093 on epoch=8
06/02/2022 15:08:14 - INFO - __main__ - Step 80 Global step 80 Train loss 9.446452 on epoch=9
06/02/2022 15:08:19 - INFO - __main__ - Step 90 Global step 90 Train loss 4.976411 on epoch=11
06/02/2022 15:08:24 - INFO - __main__ - Step 100 Global step 100 Train loss 1.525935 on epoch=12
06/02/2022 15:08:26 - INFO - __main__ - Global step 100 Train loss 8.084016 Classification-F1 0.3333333333333333 on epoch=12
06/02/2022 15:08:32 - INFO - __main__ - Step 110 Global step 110 Train loss 1.500717 on epoch=13
06/02/2022 15:08:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.567112 on epoch=14
06/02/2022 15:08:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.381543 on epoch=16
06/02/2022 15:08:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.491493 on epoch=17
06/02/2022 15:08:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.353285 on epoch=18
06/02/2022 15:08:56 - INFO - __main__ - Global step 150 Train loss 0.658830 Classification-F1 0.5272727272727273 on epoch=18
06/02/2022 15:09:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.428459 on epoch=19
06/02/2022 15:09:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.410244 on epoch=21
06/02/2022 15:09:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.459224 on epoch=22
06/02/2022 15:09:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.384576 on epoch=23
06/02/2022 15:09:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.395395 on epoch=24
06/02/2022 15:09:25 - INFO - __main__ - Global step 200 Train loss 0.415580 Classification-F1 0.47602339181286546 on epoch=24
06/02/2022 15:09:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.399424 on epoch=26
06/02/2022 15:09:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.352820 on epoch=27
06/02/2022 15:09:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.468913 on epoch=28
06/02/2022 15:09:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.347940 on epoch=29
06/02/2022 15:09:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.367881 on epoch=31
06/02/2022 15:09:54 - INFO - __main__ - Global step 250 Train loss 0.387396 Classification-F1 0.25207296849087896 on epoch=31
06/02/2022 15:09:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.410978 on epoch=32
06/02/2022 15:10:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.394134 on epoch=33
06/02/2022 15:10:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.419724 on epoch=34
06/02/2022 15:10:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.380796 on epoch=36
06/02/2022 15:10:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.367521 on epoch=37
06/02/2022 15:10:23 - INFO - __main__ - Global step 300 Train loss 0.394631 Classification-F1 0.42482199039576085 on epoch=37
06/02/2022 15:10:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.361150 on epoch=38
06/02/2022 15:10:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.340089 on epoch=39
06/02/2022 15:10:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.403360 on epoch=41
06/02/2022 15:10:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.337967 on epoch=42
06/02/2022 15:10:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.387860 on epoch=43
06/02/2022 15:10:51 - INFO - __main__ - Global step 350 Train loss 0.366085 Classification-F1 0.32631578947368417 on epoch=43
06/02/2022 15:10:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.344798 on epoch=44
06/02/2022 15:11:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.347614 on epoch=46
06/02/2022 15:11:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.358543 on epoch=47
06/02/2022 15:11:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.319675 on epoch=48
06/02/2022 15:11:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.326096 on epoch=49
06/02/2022 15:11:19 - INFO - __main__ - Global step 400 Train loss 0.339345 Classification-F1 0.32631578947368417 on epoch=49
06/02/2022 15:11:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.372066 on epoch=51
06/02/2022 15:11:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.330831 on epoch=52
06/02/2022 15:11:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.340446 on epoch=53
06/02/2022 15:11:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.367827 on epoch=54
06/02/2022 15:11:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.330745 on epoch=56
06/02/2022 15:11:48 - INFO - __main__ - Global step 450 Train loss 0.348383 Classification-F1 0.43840100285535205 on epoch=56
06/02/2022 15:11:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.345321 on epoch=57
06/02/2022 15:11:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.357095 on epoch=58
06/02/2022 15:12:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.297644 on epoch=59
06/02/2022 15:12:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.378486 on epoch=61
06/02/2022 15:12:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.350525 on epoch=62
06/02/2022 15:12:16 - INFO - __main__ - Global step 500 Train loss 0.345814 Classification-F1 0.2414057704112953 on epoch=62
06/02/2022 15:12:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.311353 on epoch=63
06/02/2022 15:12:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.392319 on epoch=64
06/02/2022 15:12:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.301974 on epoch=66
06/02/2022 15:12:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.299588 on epoch=67
06/02/2022 15:12:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.322422 on epoch=68
06/02/2022 15:12:44 - INFO - __main__ - Global step 550 Train loss 0.325531 Classification-F1 0.6401438378555083 on epoch=68
06/02/2022 15:12:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.287879 on epoch=69
06/02/2022 15:12:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.275037 on epoch=71
06/02/2022 15:13:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.302454 on epoch=72
06/02/2022 15:13:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.271843 on epoch=73
06/02/2022 15:13:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.293059 on epoch=74
06/02/2022 15:13:13 - INFO - __main__ - Global step 600 Train loss 0.286054 Classification-F1 0.8108374384236454 on epoch=74
06/02/2022 15:13:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.335762 on epoch=76
06/02/2022 15:13:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.260015 on epoch=77
06/02/2022 15:13:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.283708 on epoch=78
06/02/2022 15:13:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.303127 on epoch=79
06/02/2022 15:13:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.314429 on epoch=81
06/02/2022 15:13:43 - INFO - __main__ - Global step 650 Train loss 0.299408 Classification-F1 0.7437437437437437 on epoch=81
06/02/2022 15:13:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.228281 on epoch=82
06/02/2022 15:13:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.275065 on epoch=83
06/02/2022 15:13:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.245964 on epoch=84
06/02/2022 15:14:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.245256 on epoch=86
06/02/2022 15:14:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.274933 on epoch=87
06/02/2022 15:14:12 - INFO - __main__ - Global step 700 Train loss 0.253900 Classification-F1 0.7950738916256157 on epoch=87
06/02/2022 15:14:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.240675 on epoch=88
06/02/2022 15:14:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.280333 on epoch=89
06/02/2022 15:14:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.234710 on epoch=91
06/02/2022 15:14:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.186535 on epoch=92
06/02/2022 15:14:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.227606 on epoch=93
06/02/2022 15:14:40 - INFO - __main__ - Global step 750 Train loss 0.233972 Classification-F1 0.6693360888659261 on epoch=93
06/02/2022 15:14:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.194836 on epoch=94
06/02/2022 15:14:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.198164 on epoch=96
06/02/2022 15:14:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.234887 on epoch=97
06/02/2022 15:15:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.186094 on epoch=98
06/02/2022 15:15:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.145387 on epoch=99
06/02/2022 15:15:09 - INFO - __main__ - Global step 800 Train loss 0.191873 Classification-F1 0.664771638454168 on epoch=99
06/02/2022 15:15:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.162666 on epoch=101
06/02/2022 15:15:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.166656 on epoch=102
06/02/2022 15:15:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.381869 on epoch=103
06/02/2022 15:15:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.259517 on epoch=104
06/02/2022 15:15:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.221562 on epoch=106
06/02/2022 15:15:37 - INFO - __main__ - Global step 850 Train loss 0.238454 Classification-F1 0.7651088818204063 on epoch=106
06/02/2022 15:15:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.188486 on epoch=107
06/02/2022 15:15:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.087686 on epoch=108
06/02/2022 15:15:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.041060 on epoch=109
06/02/2022 15:15:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.022605 on epoch=111
06/02/2022 15:16:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.031339 on epoch=112
06/02/2022 15:16:06 - INFO - __main__ - Global step 900 Train loss 0.074235 Classification-F1 0.8827480916030535 on epoch=112
06/02/2022 15:16:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.056501 on epoch=113
06/02/2022 15:16:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.022825 on epoch=114
06/02/2022 15:16:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.002521 on epoch=116
06/02/2022 15:16:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.004911 on epoch=117
06/02/2022 15:16:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.008083 on epoch=118
06/02/2022 15:16:35 - INFO - __main__ - Global step 950 Train loss 0.018968 Classification-F1 0.8665276329509906 on epoch=118
06/02/2022 15:16:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.003568 on epoch=119
06/02/2022 15:16:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.001547 on epoch=121
06/02/2022 15:16:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.001979 on epoch=122
06/02/2022 15:16:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.003194 on epoch=123
06/02/2022 15:17:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.001261 on epoch=124
06/02/2022 15:17:03 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:17:03 - INFO - __main__ - Printing 3 examples
06/02/2022 15:17:03 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
06/02/2022 15:17:03 - INFO - __main__ - ['negative']
06/02/2022 15:17:03 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
06/02/2022 15:17:03 - INFO - __main__ - ['negative']
06/02/2022 15:17:03 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
06/02/2022 15:17:03 - INFO - __main__ - ['negative']
06/02/2022 15:17:03 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:17:03 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:17:03 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 15:17:03 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:17:03 - INFO - __main__ - Printing 3 examples
06/02/2022 15:17:03 - INFO - __main__ -  [amazon_polarity] title: not made of driftwood [SEP] content: please read the description carefully,,,,this is not made out of wood but made out of resin in china....it is heavy enough but not hand sculpted or even scuplted at all.....for that reason i am dissappointed...I am not sure why they put driftwood in the name of this product but for that reason i feel deceived and dissappointed....this product is not what i was expecting at all....
06/02/2022 15:17:03 - INFO - __main__ - ['negative']
06/02/2022 15:17:03 - INFO - __main__ -  [amazon_polarity] title: Farfetched [SEP] content: The beginning of the book is a thriller, I can accept that, but when a 17 years old kid made a plan to find the person who want to kill her and everybody accept her as the bait is really farfetched. Everything they do to trap the killer is pathetic. The trip to Italy and changing the newspapers in the web is hilarious. The end, when everybody stays alive except the killer is just a way to end this fairy tale in the most pathetic way.
06/02/2022 15:17:03 - INFO - __main__ - ['negative']
06/02/2022 15:17:03 - INFO - __main__ -  [amazon_polarity] title: Not all the songs you thought you was purchasing came with the album [SEP] content: I only got 5 songs i did not get all the songs that's suppose to come with the entire album
06/02/2022 15:17:03 - INFO - __main__ - ['negative']
06/02/2022 15:17:03 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:17:03 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:17:03 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 15:17:04 - INFO - __main__ - Global step 1000 Train loss 0.002310 Classification-F1 0.9140572544710981 on epoch=124
06/02/2022 15:17:05 - INFO - __main__ - save last model!
06/02/2022 15:17:12 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 15:17:12 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 15:17:12 - INFO - __main__ - Printing 3 examples
06/02/2022 15:17:12 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 15:17:12 - INFO - __main__ - ['negative']
06/02/2022 15:17:12 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 15:17:12 - INFO - __main__ - ['negative']
06/02/2022 15:17:12 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 15:17:12 - INFO - __main__ - ['negative']
06/02/2022 15:17:12 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:17:13 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:17:14 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 15:17:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 15:17:16 - INFO - __main__ - Starting training!
06/02/2022 15:17:29 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_13_0.0003_8_predictions.txt
06/02/2022 15:17:29 - INFO - __main__ - Classification-F1 on test data: 0.9230
06/02/2022 15:17:29 - INFO - __main__ - prefix=amazon_polarity_64_13, lr=0.0003, bsz=8, dev_performance=0.9140572544710981, test_performance=0.9229721929616591
06/02/2022 15:17:29 - INFO - __main__ - Running ... prefix=amazon_polarity_64_13, lr=0.0002, bsz=8 ...
06/02/2022 15:17:30 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:17:30 - INFO - __main__ - Printing 3 examples
06/02/2022 15:17:30 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
06/02/2022 15:17:30 - INFO - __main__ - ['negative']
06/02/2022 15:17:30 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
06/02/2022 15:17:30 - INFO - __main__ - ['negative']
06/02/2022 15:17:30 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
06/02/2022 15:17:30 - INFO - __main__ - ['negative']
06/02/2022 15:17:30 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:17:30 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:17:30 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 15:17:30 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:17:30 - INFO - __main__ - Printing 3 examples
06/02/2022 15:17:30 - INFO - __main__ -  [amazon_polarity] title: not made of driftwood [SEP] content: please read the description carefully,,,,this is not made out of wood but made out of resin in china....it is heavy enough but not hand sculpted or even scuplted at all.....for that reason i am dissappointed...I am not sure why they put driftwood in the name of this product but for that reason i feel deceived and dissappointed....this product is not what i was expecting at all....
06/02/2022 15:17:30 - INFO - __main__ - ['negative']
06/02/2022 15:17:30 - INFO - __main__ -  [amazon_polarity] title: Farfetched [SEP] content: The beginning of the book is a thriller, I can accept that, but when a 17 years old kid made a plan to find the person who want to kill her and everybody accept her as the bait is really farfetched. Everything they do to trap the killer is pathetic. The trip to Italy and changing the newspapers in the web is hilarious. The end, when everybody stays alive except the killer is just a way to end this fairy tale in the most pathetic way.
06/02/2022 15:17:30 - INFO - __main__ - ['negative']
06/02/2022 15:17:30 - INFO - __main__ -  [amazon_polarity] title: Not all the songs you thought you was purchasing came with the album [SEP] content: I only got 5 songs i did not get all the songs that's suppose to come with the entire album
06/02/2022 15:17:30 - INFO - __main__ - ['negative']
06/02/2022 15:17:30 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:17:31 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:17:31 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 15:17:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 15:17:44 - INFO - __main__ - Starting training!
06/02/2022 15:17:48 - INFO - __main__ - Step 10 Global step 10 Train loss 23.345692 on epoch=1
06/02/2022 15:17:54 - INFO - __main__ - Step 20 Global step 20 Train loss 19.249655 on epoch=2
06/02/2022 15:17:59 - INFO - __main__ - Step 30 Global step 30 Train loss 17.180700 on epoch=3
06/02/2022 15:18:05 - INFO - __main__ - Step 40 Global step 40 Train loss 15.324415 on epoch=4
06/02/2022 15:18:10 - INFO - __main__ - Step 50 Global step 50 Train loss 14.946727 on epoch=6
06/02/2022 15:18:35 - INFO - __main__ - Global step 50 Train loss 18.009438 Classification-F1 0.0 on epoch=6
06/02/2022 15:18:41 - INFO - __main__ - Step 60 Global step 60 Train loss 14.303744 on epoch=7
06/02/2022 15:18:46 - INFO - __main__ - Step 70 Global step 70 Train loss 13.050595 on epoch=8
06/02/2022 15:18:52 - INFO - __main__ - Step 80 Global step 80 Train loss 11.769276 on epoch=9
06/02/2022 15:18:57 - INFO - __main__ - Step 90 Global step 90 Train loss 12.117636 on epoch=11
06/02/2022 15:19:02 - INFO - __main__ - Step 100 Global step 100 Train loss 11.483091 on epoch=12
06/02/2022 15:19:06 - INFO - __main__ - Global step 100 Train loss 12.544869 Classification-F1 0.0 on epoch=12
06/02/2022 15:19:11 - INFO - __main__ - Step 110 Global step 110 Train loss 4.982453 on epoch=13
06/02/2022 15:19:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.846743 on epoch=14
06/02/2022 15:19:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.448891 on epoch=16
06/02/2022 15:19:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.369512 on epoch=17
06/02/2022 15:19:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.215978 on epoch=18
06/02/2022 15:19:35 - INFO - __main__ - Global step 150 Train loss 1.372715 Classification-F1 0.9138047138047138 on epoch=18
06/02/2022 15:19:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.117896 on epoch=19
06/02/2022 15:19:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.120188 on epoch=21
06/02/2022 15:19:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.084819 on epoch=22
06/02/2022 15:19:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.064158 on epoch=23
06/02/2022 15:20:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.078646 on epoch=24
06/02/2022 15:20:03 - INFO - __main__ - Global step 200 Train loss 0.093141 Classification-F1 0.9140152671755726 on epoch=24
06/02/2022 15:20:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.066111 on epoch=26
06/02/2022 15:20:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.051197 on epoch=27
06/02/2022 15:20:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.024124 on epoch=28
06/02/2022 15:20:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.044954 on epoch=29
06/02/2022 15:20:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.099722 on epoch=31
06/02/2022 15:20:33 - INFO - __main__ - Global step 250 Train loss 0.057222 Classification-F1 0.9375 on epoch=31
06/02/2022 15:20:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.015131 on epoch=32
06/02/2022 15:20:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.030664 on epoch=33
06/02/2022 15:20:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.016136 on epoch=34
06/02/2022 15:20:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.006127 on epoch=36
06/02/2022 15:21:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.004051 on epoch=37
06/02/2022 15:21:02 - INFO - __main__ - Global step 300 Train loss 0.014422 Classification-F1 0.9374847374847375 on epoch=37
06/02/2022 15:21:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.002442 on epoch=38
06/02/2022 15:21:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.025710 on epoch=39
06/02/2022 15:21:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.029258 on epoch=41
06/02/2022 15:21:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.004899 on epoch=42
06/02/2022 15:21:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.053194 on epoch=43
06/02/2022 15:21:30 - INFO - __main__ - Global step 350 Train loss 0.023101 Classification-F1 0.9215686274509803 on epoch=43
06/02/2022 15:21:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.055248 on epoch=44
06/02/2022 15:21:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001493 on epoch=46
06/02/2022 15:21:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.002125 on epoch=47
06/02/2022 15:21:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000764 on epoch=48
06/02/2022 15:21:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.001628 on epoch=49
06/02/2022 15:21:59 - INFO - __main__ - Global step 400 Train loss 0.012252 Classification-F1 0.9453091619361533 on epoch=49
06/02/2022 15:22:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000351 on epoch=51
06/02/2022 15:22:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000472 on epoch=52
06/02/2022 15:22:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000506 on epoch=53
06/02/2022 15:22:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000323 on epoch=54
06/02/2022 15:22:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000252 on epoch=56
06/02/2022 15:22:28 - INFO - __main__ - Global step 450 Train loss 0.000381 Classification-F1 0.9453091619361533 on epoch=56
06/02/2022 15:22:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000305 on epoch=57
06/02/2022 15:22:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000307 on epoch=58
06/02/2022 15:22:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000693 on epoch=59
06/02/2022 15:22:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000080 on epoch=61
06/02/2022 15:22:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000209 on epoch=62
06/02/2022 15:22:56 - INFO - __main__ - Global step 500 Train loss 0.000319 Classification-F1 0.9453091619361533 on epoch=62
06/02/2022 15:23:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000120 on epoch=63
06/02/2022 15:23:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000281 on epoch=64
06/02/2022 15:23:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000211 on epoch=66
06/02/2022 15:23:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.050774 on epoch=67
06/02/2022 15:23:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001963 on epoch=68
06/02/2022 15:23:25 - INFO - __main__ - Global step 550 Train loss 0.010670 Classification-F1 0.9374389051808407 on epoch=68
06/02/2022 15:23:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.002180 on epoch=69
06/02/2022 15:23:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.003578 on epoch=71
06/02/2022 15:23:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000407 on epoch=72
06/02/2022 15:23:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000353 on epoch=73
06/02/2022 15:23:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000309 on epoch=74
06/02/2022 15:23:53 - INFO - __main__ - Global step 600 Train loss 0.001366 Classification-F1 0.921875 on epoch=74
06/02/2022 15:23:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000433 on epoch=76
06/02/2022 15:24:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000351 on epoch=77
06/02/2022 15:24:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000157 on epoch=78
06/02/2022 15:24:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000678 on epoch=79
06/02/2022 15:24:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001428 on epoch=81
06/02/2022 15:24:21 - INFO - __main__ - Global step 650 Train loss 0.000610 Classification-F1 0.9374389051808407 on epoch=81
06/02/2022 15:24:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000211 on epoch=82
06/02/2022 15:24:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000037 on epoch=83
06/02/2022 15:24:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000092 on epoch=84
06/02/2022 15:24:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000042 on epoch=86
06/02/2022 15:24:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000076 on epoch=87
06/02/2022 15:24:50 - INFO - __main__ - Global step 700 Train loss 0.000092 Classification-F1 0.9452824427480916 on epoch=87
06/02/2022 15:24:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000026 on epoch=88
06/02/2022 15:25:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000033 on epoch=89
06/02/2022 15:25:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000038 on epoch=91
06/02/2022 15:25:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000053 on epoch=92
06/02/2022 15:25:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000027 on epoch=93
06/02/2022 15:25:18 - INFO - __main__ - Global step 750 Train loss 0.000036 Classification-F1 0.9452824427480916 on epoch=93
06/02/2022 15:25:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000032 on epoch=94
06/02/2022 15:25:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000118 on epoch=96
06/02/2022 15:25:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000036 on epoch=97
06/02/2022 15:25:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000290 on epoch=98
06/02/2022 15:25:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000074 on epoch=99
06/02/2022 15:25:47 - INFO - __main__ - Global step 800 Train loss 0.000110 Classification-F1 0.9452824427480916 on epoch=99
06/02/2022 15:25:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000402 on epoch=101
06/02/2022 15:25:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000020 on epoch=102
06/02/2022 15:26:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000051 on epoch=103
06/02/2022 15:26:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000017 on epoch=104
06/02/2022 15:26:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.001420 on epoch=106
06/02/2022 15:26:16 - INFO - __main__ - Global step 850 Train loss 0.000382 Classification-F1 0.9296832082036257 on epoch=106
06/02/2022 15:26:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000089 on epoch=107
06/02/2022 15:26:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000059 on epoch=108
06/02/2022 15:26:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000098 on epoch=109
06/02/2022 15:26:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000069 on epoch=111
06/02/2022 15:26:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000133 on epoch=112
06/02/2022 15:26:44 - INFO - __main__ - Global step 900 Train loss 0.000089 Classification-F1 0.9296832082036257 on epoch=112
06/02/2022 15:26:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000033 on epoch=113
06/02/2022 15:26:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000025 on epoch=114
06/02/2022 15:27:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000018 on epoch=116
06/02/2022 15:27:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000071 on epoch=117
06/02/2022 15:27:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000141 on epoch=118
06/02/2022 15:27:13 - INFO - __main__ - Global step 950 Train loss 0.000058 Classification-F1 0.921875 on epoch=118
06/02/2022 15:27:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000019 on epoch=119
06/02/2022 15:27:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000195 on epoch=121
06/02/2022 15:27:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000071 on epoch=122
06/02/2022 15:27:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000024 on epoch=123
06/02/2022 15:27:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000026 on epoch=124
06/02/2022 15:27:40 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:27:40 - INFO - __main__ - Printing 3 examples
06/02/2022 15:27:40 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
06/02/2022 15:27:40 - INFO - __main__ - ['negative']
06/02/2022 15:27:40 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
06/02/2022 15:27:40 - INFO - __main__ - ['negative']
06/02/2022 15:27:40 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
06/02/2022 15:27:40 - INFO - __main__ - ['negative']
06/02/2022 15:27:40 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:27:40 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:27:40 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 15:27:40 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:27:40 - INFO - __main__ - Printing 3 examples
06/02/2022 15:27:40 - INFO - __main__ -  [amazon_polarity] title: not made of driftwood [SEP] content: please read the description carefully,,,,this is not made out of wood but made out of resin in china....it is heavy enough but not hand sculpted or even scuplted at all.....for that reason i am dissappointed...I am not sure why they put driftwood in the name of this product but for that reason i feel deceived and dissappointed....this product is not what i was expecting at all....
06/02/2022 15:27:40 - INFO - __main__ - ['negative']
06/02/2022 15:27:40 - INFO - __main__ -  [amazon_polarity] title: Farfetched [SEP] content: The beginning of the book is a thriller, I can accept that, but when a 17 years old kid made a plan to find the person who want to kill her and everybody accept her as the bait is really farfetched. Everything they do to trap the killer is pathetic. The trip to Italy and changing the newspapers in the web is hilarious. The end, when everybody stays alive except the killer is just a way to end this fairy tale in the most pathetic way.
06/02/2022 15:27:40 - INFO - __main__ - ['negative']
06/02/2022 15:27:40 - INFO - __main__ -  [amazon_polarity] title: Not all the songs you thought you was purchasing came with the album [SEP] content: I only got 5 songs i did not get all the songs that's suppose to come with the entire album
06/02/2022 15:27:40 - INFO - __main__ - ['negative']
06/02/2022 15:27:40 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:27:40 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:27:40 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 15:27:41 - INFO - __main__ - Global step 1000 Train loss 0.000067 Classification-F1 0.9296832082036257 on epoch=124
06/02/2022 15:27:41 - INFO - __main__ - save last model!
06/02/2022 15:27:47 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 15:27:48 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 15:27:48 - INFO - __main__ - Printing 3 examples
06/02/2022 15:27:48 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 15:27:48 - INFO - __main__ - ['negative']
06/02/2022 15:27:48 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 15:27:48 - INFO - __main__ - ['negative']
06/02/2022 15:27:48 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 15:27:48 - INFO - __main__ - ['negative']
06/02/2022 15:27:48 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:27:49 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:27:50 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 15:27:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 15:27:53 - INFO - __main__ - Starting training!
06/02/2022 15:28:05 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_13_0.0002_8_predictions.txt
06/02/2022 15:28:05 - INFO - __main__ - Classification-F1 on test data: 0.9460
06/02/2022 15:28:05 - INFO - __main__ - prefix=amazon_polarity_64_13, lr=0.0002, bsz=8, dev_performance=0.9453091619361533, test_performance=0.9459738513440505
06/02/2022 15:28:05 - INFO - __main__ - Running ... prefix=amazon_polarity_64_13, lr=0.0001, bsz=8 ...
06/02/2022 15:28:06 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:28:06 - INFO - __main__ - Printing 3 examples
06/02/2022 15:28:06 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
06/02/2022 15:28:06 - INFO - __main__ - ['negative']
06/02/2022 15:28:06 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
06/02/2022 15:28:06 - INFO - __main__ - ['negative']
06/02/2022 15:28:06 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
06/02/2022 15:28:06 - INFO - __main__ - ['negative']
06/02/2022 15:28:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:28:06 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:28:06 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 15:28:06 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:28:06 - INFO - __main__ - Printing 3 examples
06/02/2022 15:28:06 - INFO - __main__ -  [amazon_polarity] title: not made of driftwood [SEP] content: please read the description carefully,,,,this is not made out of wood but made out of resin in china....it is heavy enough but not hand sculpted or even scuplted at all.....for that reason i am dissappointed...I am not sure why they put driftwood in the name of this product but for that reason i feel deceived and dissappointed....this product is not what i was expecting at all....
06/02/2022 15:28:06 - INFO - __main__ - ['negative']
06/02/2022 15:28:06 - INFO - __main__ -  [amazon_polarity] title: Farfetched [SEP] content: The beginning of the book is a thriller, I can accept that, but when a 17 years old kid made a plan to find the person who want to kill her and everybody accept her as the bait is really farfetched. Everything they do to trap the killer is pathetic. The trip to Italy and changing the newspapers in the web is hilarious. The end, when everybody stays alive except the killer is just a way to end this fairy tale in the most pathetic way.
06/02/2022 15:28:06 - INFO - __main__ - ['negative']
06/02/2022 15:28:06 - INFO - __main__ -  [amazon_polarity] title: Not all the songs you thought you was purchasing came with the album [SEP] content: I only got 5 songs i did not get all the songs that's suppose to come with the entire album
06/02/2022 15:28:06 - INFO - __main__ - ['negative']
06/02/2022 15:28:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:28:06 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:28:06 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 15:28:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 15:28:17 - INFO - __main__ - Starting training!
06/02/2022 15:28:22 - INFO - __main__ - Step 10 Global step 10 Train loss 23.316282 on epoch=1
06/02/2022 15:28:27 - INFO - __main__ - Step 20 Global step 20 Train loss 19.918051 on epoch=2
06/02/2022 15:28:32 - INFO - __main__ - Step 30 Global step 30 Train loss 17.848013 on epoch=3
06/02/2022 15:28:38 - INFO - __main__ - Step 40 Global step 40 Train loss 17.518648 on epoch=4
06/02/2022 15:28:43 - INFO - __main__ - Step 50 Global step 50 Train loss 17.055485 on epoch=6
06/02/2022 15:29:08 - INFO - __main__ - Global step 50 Train loss 19.131298 Classification-F1 0.0 on epoch=6
06/02/2022 15:29:14 - INFO - __main__ - Step 60 Global step 60 Train loss 16.030132 on epoch=7
06/02/2022 15:29:20 - INFO - __main__ - Step 70 Global step 70 Train loss 15.662703 on epoch=8
06/02/2022 15:29:25 - INFO - __main__ - Step 80 Global step 80 Train loss 15.262159 on epoch=9
06/02/2022 15:29:30 - INFO - __main__ - Step 90 Global step 90 Train loss 14.822072 on epoch=11
06/02/2022 15:29:36 - INFO - __main__ - Step 100 Global step 100 Train loss 14.188820 on epoch=12
06/02/2022 15:29:54 - INFO - __main__ - Global step 100 Train loss 15.193177 Classification-F1 0.0 on epoch=12
06/02/2022 15:29:59 - INFO - __main__ - Step 110 Global step 110 Train loss 14.069040 on epoch=13
06/02/2022 15:30:04 - INFO - __main__ - Step 120 Global step 120 Train loss 13.459045 on epoch=14
06/02/2022 15:30:10 - INFO - __main__ - Step 130 Global step 130 Train loss 13.136648 on epoch=16
06/02/2022 15:30:15 - INFO - __main__ - Step 140 Global step 140 Train loss 12.691168 on epoch=17
06/02/2022 15:30:20 - INFO - __main__ - Step 150 Global step 150 Train loss 12.882675 on epoch=18
06/02/2022 15:30:38 - INFO - __main__ - Global step 150 Train loss 13.247716 Classification-F1 0.0 on epoch=18
06/02/2022 15:30:43 - INFO - __main__ - Step 160 Global step 160 Train loss 11.846200 on epoch=19
06/02/2022 15:30:48 - INFO - __main__ - Step 170 Global step 170 Train loss 11.451741 on epoch=21
06/02/2022 15:30:54 - INFO - __main__ - Step 180 Global step 180 Train loss 11.072268 on epoch=22
06/02/2022 15:30:59 - INFO - __main__ - Step 190 Global step 190 Train loss 10.607236 on epoch=23
06/02/2022 15:31:04 - INFO - __main__ - Step 200 Global step 200 Train loss 5.933140 on epoch=24
06/02/2022 15:31:06 - INFO - __main__ - Global step 200 Train loss 10.182117 Classification-F1 0.22626230549380755 on epoch=24
06/02/2022 15:31:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.995600 on epoch=26
06/02/2022 15:31:18 - INFO - __main__ - Step 220 Global step 220 Train loss 1.112133 on epoch=27
06/02/2022 15:31:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.349310 on epoch=28
06/02/2022 15:31:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.291449 on epoch=29
06/02/2022 15:31:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.207205 on epoch=31
06/02/2022 15:31:35 - INFO - __main__ - Global step 250 Train loss 0.591139 Classification-F1 0.8984313007385705 on epoch=31
06/02/2022 15:31:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.200157 on epoch=32
06/02/2022 15:31:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.231330 on epoch=33
06/02/2022 15:31:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.195505 on epoch=34
06/02/2022 15:31:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.154883 on epoch=36
06/02/2022 15:32:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.122126 on epoch=37
06/02/2022 15:32:05 - INFO - __main__ - Global step 300 Train loss 0.180800 Classification-F1 0.8905180840664713 on epoch=37
06/02/2022 15:32:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.071361 on epoch=38
06/02/2022 15:32:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.082644 on epoch=39
06/02/2022 15:32:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.097443 on epoch=41
06/02/2022 15:32:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.112468 on epoch=42
06/02/2022 15:32:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.028587 on epoch=43
06/02/2022 15:32:33 - INFO - __main__ - Global step 350 Train loss 0.078501 Classification-F1 0.8905180840664713 on epoch=43
06/02/2022 15:32:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.079590 on epoch=44
06/02/2022 15:32:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.058699 on epoch=46
06/02/2022 15:32:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.059666 on epoch=47
06/02/2022 15:32:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.048611 on epoch=48
06/02/2022 15:33:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.032895 on epoch=49
06/02/2022 15:33:02 - INFO - __main__ - Global step 400 Train loss 0.055892 Classification-F1 0.8984313007385705 on epoch=49
06/02/2022 15:33:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.048737 on epoch=51
06/02/2022 15:33:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.036908 on epoch=52
06/02/2022 15:33:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.049868 on epoch=53
06/02/2022 15:33:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.135950 on epoch=54
06/02/2022 15:33:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.052260 on epoch=56
06/02/2022 15:33:30 - INFO - __main__ - Global step 450 Train loss 0.064744 Classification-F1 0.8667891031527395 on epoch=56
06/02/2022 15:33:35 - INFO - __main__ - Step 460 Global step 460 Train loss 4.238179 on epoch=57
06/02/2022 15:33:40 - INFO - __main__ - Step 470 Global step 470 Train loss 2.097077 on epoch=58
06/02/2022 15:33:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.418245 on epoch=59
06/02/2022 15:33:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.542237 on epoch=61
06/02/2022 15:33:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.368300 on epoch=62
06/02/2022 15:33:58 - INFO - __main__ - Global step 500 Train loss 1.532808 Classification-F1 0.4545454545454546 on epoch=62
06/02/2022 15:34:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.123289 on epoch=63
06/02/2022 15:34:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.084821 on epoch=64
06/02/2022 15:34:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.038620 on epoch=66
06/02/2022 15:34:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.032333 on epoch=67
06/02/2022 15:34:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.047576 on epoch=68
06/02/2022 15:34:26 - INFO - __main__ - Global step 550 Train loss 0.065328 Classification-F1 0.8983816793893129 on epoch=68
06/02/2022 15:34:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.048317 on epoch=69
06/02/2022 15:34:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.062012 on epoch=71
06/02/2022 15:34:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.017035 on epoch=72
06/02/2022 15:34:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.028276 on epoch=73
06/02/2022 15:34:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.039413 on epoch=74
06/02/2022 15:34:55 - INFO - __main__ - Global step 600 Train loss 0.039010 Classification-F1 0.9140152671755726 on epoch=74
06/02/2022 15:35:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.007160 on epoch=76
06/02/2022 15:35:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.006873 on epoch=77
06/02/2022 15:35:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.024059 on epoch=78
06/02/2022 15:35:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.011204 on epoch=79
06/02/2022 15:35:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.012194 on epoch=81
06/02/2022 15:35:24 - INFO - __main__ - Global step 650 Train loss 0.012298 Classification-F1 0.9140152671755726 on epoch=81
06/02/2022 15:35:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.013018 on epoch=82
06/02/2022 15:35:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.031402 on epoch=83
06/02/2022 15:35:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.021728 on epoch=84
06/02/2022 15:35:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.009264 on epoch=86
06/02/2022 15:35:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.014492 on epoch=87
06/02/2022 15:35:53 - INFO - __main__ - Global step 700 Train loss 0.017981 Classification-F1 0.9140152671755726 on epoch=87
06/02/2022 15:35:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.008955 on epoch=88
06/02/2022 15:36:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.027530 on epoch=89
06/02/2022 15:36:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.038351 on epoch=91
06/02/2022 15:36:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.026435 on epoch=92
06/02/2022 15:36:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.009720 on epoch=93
06/02/2022 15:36:21 - INFO - __main__ - Global step 750 Train loss 0.022198 Classification-F1 0.90625 on epoch=93
06/02/2022 15:36:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.015806 on epoch=94
06/02/2022 15:36:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.003371 on epoch=96
06/02/2022 15:36:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.004190 on epoch=97
06/02/2022 15:36:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.001453 on epoch=98
06/02/2022 15:36:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.013589 on epoch=99
06/02/2022 15:36:49 - INFO - __main__ - Global step 800 Train loss 0.007682 Classification-F1 0.90625 on epoch=99
06/02/2022 15:36:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.010513 on epoch=101
06/02/2022 15:37:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.007128 on epoch=102
06/02/2022 15:37:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.007616 on epoch=103
06/02/2022 15:37:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.026584 on epoch=104
06/02/2022 15:37:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.040366 on epoch=106
06/02/2022 15:37:18 - INFO - __main__ - Global step 850 Train loss 0.018441 Classification-F1 0.906158357771261 on epoch=106
06/02/2022 15:37:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.017008 on epoch=107
06/02/2022 15:37:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.013555 on epoch=108
06/02/2022 15:37:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.031757 on epoch=109
06/02/2022 15:37:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.046929 on epoch=111
06/02/2022 15:37:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.013913 on epoch=112
06/02/2022 15:37:47 - INFO - __main__ - Global step 900 Train loss 0.024632 Classification-F1 0.90625 on epoch=112
06/02/2022 15:37:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.006868 on epoch=113
06/02/2022 15:37:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.004836 on epoch=114
06/02/2022 15:38:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.013252 on epoch=116
06/02/2022 15:38:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.005467 on epoch=117
06/02/2022 15:38:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.008891 on epoch=118
06/02/2022 15:38:15 - INFO - __main__ - Global step 950 Train loss 0.007863 Classification-F1 0.8822302643685211 on epoch=118
06/02/2022 15:38:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.017796 on epoch=119
06/02/2022 15:38:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.001196 on epoch=121
06/02/2022 15:38:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.002685 on epoch=122
06/02/2022 15:38:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.003497 on epoch=123
06/02/2022 15:38:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.046200 on epoch=124
06/02/2022 15:38:43 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:38:43 - INFO - __main__ - Printing 3 examples
06/02/2022 15:38:43 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
06/02/2022 15:38:43 - INFO - __main__ - ['positive']
06/02/2022 15:38:43 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
06/02/2022 15:38:43 - INFO - __main__ - ['positive']
06/02/2022 15:38:43 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
06/02/2022 15:38:43 - INFO - __main__ - ['positive']
06/02/2022 15:38:43 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:38:43 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:38:43 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 15:38:43 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:38:43 - INFO - __main__ - Printing 3 examples
06/02/2022 15:38:43 - INFO - __main__ -  [amazon_polarity] title: If Your Looking For A Good Crue Album... [SEP] content: Greatest Hits is the most suitable title for this album. Released in 1998, Motley Crue's greatest hits is a great compilation album for any die-hard Crue fan. On the album you'll find hits such as: Kickstart My Heart, Dr. Feelgood, and Smokin' In The Boys Room. If your into the heavier Crue stuff, you'll find hits such as Too Fast For Love, Looks That Kill, and Shout At The Devil! If your looking for a good variety of Motley Crue songs on a single album, then Motley Crue: Greatest Hits is the album for you.
06/02/2022 15:38:43 - INFO - __main__ - ['positive']
06/02/2022 15:38:43 - INFO - __main__ -  [amazon_polarity] title: LOVE IT! [SEP] content: Are you pondering what I'm pondering? Sure, but horizontal stripes make me look hippy.A must for any fan. The best part (really!) is the extra material with Maurice LaMarche and Rob Paulsen -- they are as intelligent, witty and friendly with each other as you'd ever hope. Restores your faith in humanity.
06/02/2022 15:38:43 - INFO - __main__ - ['positive']
06/02/2022 15:38:43 - INFO - __main__ -  [amazon_polarity] title: TAILOR MADE ROLE FOR BLACK! TOUGH TO DISLIKE! [SEP] content: Black is his usual over the top self in this very funny crowd pleaser. The supporting cast of kids and adults in the film are great. Throw in some catchy rock tunes and a whole lot of zany situations and you've got one of the best comedies of the year. The original release on DVD has a very good transfer and some very cool extras including excellent commentaries which make for some interesting repeated viewings of the film.
06/02/2022 15:38:43 - INFO - __main__ - ['positive']
06/02/2022 15:38:43 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:38:43 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:38:44 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 15:38:44 - INFO - __main__ - Global step 1000 Train loss 0.014275 Classification-F1 0.90625 on epoch=124
06/02/2022 15:38:44 - INFO - __main__ - save last model!
06/02/2022 15:38:51 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 15:38:51 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 15:38:51 - INFO - __main__ - Printing 3 examples
06/02/2022 15:38:51 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 15:38:51 - INFO - __main__ - ['negative']
06/02/2022 15:38:51 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 15:38:51 - INFO - __main__ - ['negative']
06/02/2022 15:38:51 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 15:38:51 - INFO - __main__ - ['negative']
06/02/2022 15:38:51 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:38:52 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:38:53 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 15:38:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 15:38:54 - INFO - __main__ - Starting training!
06/02/2022 15:39:08 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_13_0.0001_8_predictions.txt
06/02/2022 15:39:08 - INFO - __main__ - Classification-F1 on test data: 0.9318
06/02/2022 15:39:08 - INFO - __main__ - prefix=amazon_polarity_64_13, lr=0.0001, bsz=8, dev_performance=0.9140152671755726, test_performance=0.9318429661941112
06/02/2022 15:39:08 - INFO - __main__ - Running ... prefix=amazon_polarity_64_21, lr=0.0005, bsz=8 ...
06/02/2022 15:39:09 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:39:09 - INFO - __main__ - Printing 3 examples
06/02/2022 15:39:09 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
06/02/2022 15:39:09 - INFO - __main__ - ['positive']
06/02/2022 15:39:09 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
06/02/2022 15:39:09 - INFO - __main__ - ['positive']
06/02/2022 15:39:09 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
06/02/2022 15:39:09 - INFO - __main__ - ['positive']
06/02/2022 15:39:09 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:39:09 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:39:09 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 15:39:09 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:39:09 - INFO - __main__ - Printing 3 examples
06/02/2022 15:39:09 - INFO - __main__ -  [amazon_polarity] title: If Your Looking For A Good Crue Album... [SEP] content: Greatest Hits is the most suitable title for this album. Released in 1998, Motley Crue's greatest hits is a great compilation album for any die-hard Crue fan. On the album you'll find hits such as: Kickstart My Heart, Dr. Feelgood, and Smokin' In The Boys Room. If your into the heavier Crue stuff, you'll find hits such as Too Fast For Love, Looks That Kill, and Shout At The Devil! If your looking for a good variety of Motley Crue songs on a single album, then Motley Crue: Greatest Hits is the album for you.
06/02/2022 15:39:09 - INFO - __main__ - ['positive']
06/02/2022 15:39:09 - INFO - __main__ -  [amazon_polarity] title: LOVE IT! [SEP] content: Are you pondering what I'm pondering? Sure, but horizontal stripes make me look hippy.A must for any fan. The best part (really!) is the extra material with Maurice LaMarche and Rob Paulsen -- they are as intelligent, witty and friendly with each other as you'd ever hope. Restores your faith in humanity.
06/02/2022 15:39:09 - INFO - __main__ - ['positive']
06/02/2022 15:39:09 - INFO - __main__ -  [amazon_polarity] title: TAILOR MADE ROLE FOR BLACK! TOUGH TO DISLIKE! [SEP] content: Black is his usual over the top self in this very funny crowd pleaser. The supporting cast of kids and adults in the film are great. Throw in some catchy rock tunes and a whole lot of zany situations and you've got one of the best comedies of the year. The original release on DVD has a very good transfer and some very cool extras including excellent commentaries which make for some interesting repeated viewings of the film.
06/02/2022 15:39:09 - INFO - __main__ - ['positive']
06/02/2022 15:39:09 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:39:09 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:39:10 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 15:39:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 15:39:20 - INFO - __main__ - Starting training!
06/02/2022 15:39:25 - INFO - __main__ - Step 10 Global step 10 Train loss 22.866148 on epoch=1
06/02/2022 15:39:30 - INFO - __main__ - Step 20 Global step 20 Train loss 17.534138 on epoch=2
06/02/2022 15:39:35 - INFO - __main__ - Step 30 Global step 30 Train loss 14.658010 on epoch=3
06/02/2022 15:39:40 - INFO - __main__ - Step 40 Global step 40 Train loss 12.715610 on epoch=4
06/02/2022 15:39:45 - INFO - __main__ - Step 50 Global step 50 Train loss 9.412758 on epoch=6
06/02/2022 15:39:47 - INFO - __main__ - Global step 50 Train loss 15.437334 Classification-F1 0.007692307692307693 on epoch=6
06/02/2022 15:39:53 - INFO - __main__ - Step 60 Global step 60 Train loss 3.351444 on epoch=7
06/02/2022 15:39:57 - INFO - __main__ - Step 70 Global step 70 Train loss 2.952930 on epoch=8
06/02/2022 15:40:02 - INFO - __main__ - Step 80 Global step 80 Train loss 0.862477 on epoch=9
06/02/2022 15:40:07 - INFO - __main__ - Step 90 Global step 90 Train loss 0.449858 on epoch=11
06/02/2022 15:40:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.513430 on epoch=12
06/02/2022 15:40:14 - INFO - __main__ - Global step 100 Train loss 1.626028 Classification-F1 0.4055576703464027 on epoch=12
06/02/2022 15:40:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.484864 on epoch=13
06/02/2022 15:40:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.348472 on epoch=14
06/02/2022 15:40:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.415673 on epoch=16
06/02/2022 15:40:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.378440 on epoch=17
06/02/2022 15:40:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.385628 on epoch=18
06/02/2022 15:40:43 - INFO - __main__ - Global step 150 Train loss 0.402615 Classification-F1 0.3333333333333333 on epoch=18
06/02/2022 15:40:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.399088 on epoch=19
06/02/2022 15:40:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.392560 on epoch=21
06/02/2022 15:40:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.352832 on epoch=22
06/02/2022 15:41:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.428557 on epoch=23
06/02/2022 15:41:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.366054 on epoch=24
06/02/2022 15:41:11 - INFO - __main__ - Global step 200 Train loss 0.387818 Classification-F1 0.3948694102146787 on epoch=24
06/02/2022 15:41:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.382065 on epoch=26
06/02/2022 15:41:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.375211 on epoch=27
06/02/2022 15:41:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.374255 on epoch=28
06/02/2022 15:41:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.417545 on epoch=29
06/02/2022 15:41:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.350942 on epoch=31
06/02/2022 15:41:39 - INFO - __main__ - Global step 250 Train loss 0.380004 Classification-F1 0.3671451355661882 on epoch=31
06/02/2022 15:41:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.404249 on epoch=32
06/02/2022 15:41:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.370728 on epoch=33
06/02/2022 15:41:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.364218 on epoch=34
06/02/2022 15:42:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.381005 on epoch=36
06/02/2022 15:42:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.395212 on epoch=37
06/02/2022 15:42:07 - INFO - __main__ - Global step 300 Train loss 0.383083 Classification-F1 0.543564556280697 on epoch=37
06/02/2022 15:42:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.378376 on epoch=38
06/02/2022 15:42:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.358879 on epoch=39
06/02/2022 15:42:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.315940 on epoch=41
06/02/2022 15:42:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.352621 on epoch=42
06/02/2022 15:42:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.883922 on epoch=43
06/02/2022 15:42:36 - INFO - __main__ - Global step 350 Train loss 0.457948 Classification-F1 0.3333333333333333 on epoch=43
06/02/2022 15:42:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.337030 on epoch=44
06/02/2022 15:42:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.286840 on epoch=46
06/02/2022 15:42:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.289470 on epoch=47
06/02/2022 15:42:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.327358 on epoch=48
06/02/2022 15:43:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.300567 on epoch=49
06/02/2022 15:43:04 - INFO - __main__ - Global step 400 Train loss 0.308253 Classification-F1 0.653803021307474 on epoch=49
06/02/2022 15:43:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.296448 on epoch=51
06/02/2022 15:43:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.306484 on epoch=52
06/02/2022 15:43:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.243747 on epoch=53
06/02/2022 15:43:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.242206 on epoch=54
06/02/2022 15:43:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.252204 on epoch=56
06/02/2022 15:43:34 - INFO - __main__ - Global step 450 Train loss 0.268218 Classification-F1 0.5331473932632482 on epoch=56
06/02/2022 15:43:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.250565 on epoch=57
06/02/2022 15:43:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.279664 on epoch=58
06/02/2022 15:43:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.327988 on epoch=59
06/02/2022 15:43:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.237276 on epoch=61
06/02/2022 15:44:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.225558 on epoch=62
06/02/2022 15:44:04 - INFO - __main__ - Global step 500 Train loss 0.264210 Classification-F1 0.5797537619699042 on epoch=62
06/02/2022 15:44:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.304605 on epoch=63
06/02/2022 15:44:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.240895 on epoch=64
06/02/2022 15:44:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.289857 on epoch=66
06/02/2022 15:44:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.509446 on epoch=67
06/02/2022 15:44:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.262568 on epoch=68
06/02/2022 15:44:31 - INFO - __main__ - Global step 550 Train loss 0.321474 Classification-F1 0.7421717634132943 on epoch=68
06/02/2022 15:44:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.224514 on epoch=69
06/02/2022 15:44:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.255867 on epoch=71
06/02/2022 15:44:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.337434 on epoch=72
06/02/2022 15:44:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.309587 on epoch=73
06/02/2022 15:44:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.281715 on epoch=74
06/02/2022 15:45:00 - INFO - __main__ - Global step 600 Train loss 0.281823 Classification-F1 0.6623560445216496 on epoch=74
06/02/2022 15:45:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.245229 on epoch=76
06/02/2022 15:45:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.295259 on epoch=77
06/02/2022 15:45:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.285941 on epoch=78
06/02/2022 15:45:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.228023 on epoch=79
06/02/2022 15:45:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.167535 on epoch=81
06/02/2022 15:45:28 - INFO - __main__ - Global step 650 Train loss 0.244397 Classification-F1 0.6952939022157114 on epoch=81
06/02/2022 15:45:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.218106 on epoch=82
06/02/2022 15:45:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.210972 on epoch=83
06/02/2022 15:45:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.132782 on epoch=84
06/02/2022 15:45:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.365354 on epoch=86
06/02/2022 15:45:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.259562 on epoch=87
06/02/2022 15:45:56 - INFO - __main__ - Global step 700 Train loss 0.237355 Classification-F1 0.6956956956956957 on epoch=87
06/02/2022 15:46:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.234272 on epoch=88
06/02/2022 15:46:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.234781 on epoch=89
06/02/2022 15:46:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.260926 on epoch=91
06/02/2022 15:46:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.237333 on epoch=92
06/02/2022 15:46:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.185411 on epoch=93
06/02/2022 15:46:24 - INFO - __main__ - Global step 750 Train loss 0.230545 Classification-F1 0.557530864197531 on epoch=93
06/02/2022 15:46:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.185056 on epoch=94
06/02/2022 15:46:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.137823 on epoch=96
06/02/2022 15:46:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.118454 on epoch=97
06/02/2022 15:46:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.110801 on epoch=98
06/02/2022 15:46:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.094086 on epoch=99
06/02/2022 15:46:52 - INFO - __main__ - Global step 800 Train loss 0.129244 Classification-F1 0.7335856196783349 on epoch=99
06/02/2022 15:46:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.152262 on epoch=101
06/02/2022 15:47:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.208328 on epoch=102
06/02/2022 15:47:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.206384 on epoch=103
06/02/2022 15:47:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.109957 on epoch=104
06/02/2022 15:47:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.107127 on epoch=106
06/02/2022 15:47:20 - INFO - __main__ - Global step 850 Train loss 0.156811 Classification-F1 0.6971357409713574 on epoch=106
06/02/2022 15:47:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.094243 on epoch=107
06/02/2022 15:47:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.126337 on epoch=108
06/02/2022 15:47:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.093277 on epoch=109
06/02/2022 15:47:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.098711 on epoch=111
06/02/2022 15:47:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.070093 on epoch=112
06/02/2022 15:47:48 - INFO - __main__ - Global step 900 Train loss 0.096532 Classification-F1 0.6796679484831838 on epoch=112
06/02/2022 15:47:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.087959 on epoch=113
06/02/2022 15:47:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.036967 on epoch=114
06/02/2022 15:48:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.142774 on epoch=116
06/02/2022 15:48:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.068934 on epoch=117
06/02/2022 15:48:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.063228 on epoch=118
06/02/2022 15:48:16 - INFO - __main__ - Global step 950 Train loss 0.079973 Classification-F1 0.6796679484831838 on epoch=118
06/02/2022 15:48:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.024890 on epoch=119
06/02/2022 15:48:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.088555 on epoch=121
06/02/2022 15:48:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.034642 on epoch=122
06/02/2022 15:48:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.038149 on epoch=123
06/02/2022 15:48:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.125544 on epoch=124
06/02/2022 15:48:43 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:48:43 - INFO - __main__ - Printing 3 examples
06/02/2022 15:48:43 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
06/02/2022 15:48:43 - INFO - __main__ - ['positive']
06/02/2022 15:48:43 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
06/02/2022 15:48:43 - INFO - __main__ - ['positive']
06/02/2022 15:48:43 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
06/02/2022 15:48:43 - INFO - __main__ - ['positive']
06/02/2022 15:48:43 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:48:44 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:48:44 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 15:48:44 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:48:44 - INFO - __main__ - Printing 3 examples
06/02/2022 15:48:44 - INFO - __main__ -  [amazon_polarity] title: If Your Looking For A Good Crue Album... [SEP] content: Greatest Hits is the most suitable title for this album. Released in 1998, Motley Crue's greatest hits is a great compilation album for any die-hard Crue fan. On the album you'll find hits such as: Kickstart My Heart, Dr. Feelgood, and Smokin' In The Boys Room. If your into the heavier Crue stuff, you'll find hits such as Too Fast For Love, Looks That Kill, and Shout At The Devil! If your looking for a good variety of Motley Crue songs on a single album, then Motley Crue: Greatest Hits is the album for you.
06/02/2022 15:48:44 - INFO - __main__ - ['positive']
06/02/2022 15:48:44 - INFO - __main__ -  [amazon_polarity] title: LOVE IT! [SEP] content: Are you pondering what I'm pondering? Sure, but horizontal stripes make me look hippy.A must for any fan. The best part (really!) is the extra material with Maurice LaMarche and Rob Paulsen -- they are as intelligent, witty and friendly with each other as you'd ever hope. Restores your faith in humanity.
06/02/2022 15:48:44 - INFO - __main__ - ['positive']
06/02/2022 15:48:44 - INFO - __main__ -  [amazon_polarity] title: TAILOR MADE ROLE FOR BLACK! TOUGH TO DISLIKE! [SEP] content: Black is his usual over the top self in this very funny crowd pleaser. The supporting cast of kids and adults in the film are great. Throw in some catchy rock tunes and a whole lot of zany situations and you've got one of the best comedies of the year. The original release on DVD has a very good transfer and some very cool extras including excellent commentaries which make for some interesting repeated viewings of the film.
06/02/2022 15:48:44 - INFO - __main__ - ['positive']
06/02/2022 15:48:44 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:48:44 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:48:44 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 15:48:44 - INFO - __main__ - Global step 1000 Train loss 0.062356 Classification-F1 0.6930455635491606 on epoch=124
06/02/2022 15:48:44 - INFO - __main__ - save last model!
06/02/2022 15:48:51 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 15:48:52 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 15:48:52 - INFO - __main__ - Printing 3 examples
06/02/2022 15:48:52 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 15:48:52 - INFO - __main__ - ['negative']
06/02/2022 15:48:52 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 15:48:52 - INFO - __main__ - ['negative']
06/02/2022 15:48:52 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 15:48:52 - INFO - __main__ - ['negative']
06/02/2022 15:48:52 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:48:52 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:48:53 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 15:48:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 15:48:54 - INFO - __main__ - Starting training!
06/02/2022 15:49:08 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_21_0.0005_8_predictions.txt
06/02/2022 15:49:08 - INFO - __main__ - Classification-F1 on test data: 0.7630
06/02/2022 15:49:09 - INFO - __main__ - prefix=amazon_polarity_64_21, lr=0.0005, bsz=8, dev_performance=0.7421717634132943, test_performance=0.7629599402298988
06/02/2022 15:49:09 - INFO - __main__ - Running ... prefix=amazon_polarity_64_21, lr=0.0003, bsz=8 ...
06/02/2022 15:49:10 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:49:10 - INFO - __main__ - Printing 3 examples
06/02/2022 15:49:10 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
06/02/2022 15:49:10 - INFO - __main__ - ['positive']
06/02/2022 15:49:10 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
06/02/2022 15:49:10 - INFO - __main__ - ['positive']
06/02/2022 15:49:10 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
06/02/2022 15:49:10 - INFO - __main__ - ['positive']
06/02/2022 15:49:10 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:49:10 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:49:10 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 15:49:10 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:49:10 - INFO - __main__ - Printing 3 examples
06/02/2022 15:49:10 - INFO - __main__ -  [amazon_polarity] title: If Your Looking For A Good Crue Album... [SEP] content: Greatest Hits is the most suitable title for this album. Released in 1998, Motley Crue's greatest hits is a great compilation album for any die-hard Crue fan. On the album you'll find hits such as: Kickstart My Heart, Dr. Feelgood, and Smokin' In The Boys Room. If your into the heavier Crue stuff, you'll find hits such as Too Fast For Love, Looks That Kill, and Shout At The Devil! If your looking for a good variety of Motley Crue songs on a single album, then Motley Crue: Greatest Hits is the album for you.
06/02/2022 15:49:10 - INFO - __main__ - ['positive']
06/02/2022 15:49:10 - INFO - __main__ -  [amazon_polarity] title: LOVE IT! [SEP] content: Are you pondering what I'm pondering? Sure, but horizontal stripes make me look hippy.A must for any fan. The best part (really!) is the extra material with Maurice LaMarche and Rob Paulsen -- they are as intelligent, witty and friendly with each other as you'd ever hope. Restores your faith in humanity.
06/02/2022 15:49:10 - INFO - __main__ - ['positive']
06/02/2022 15:49:10 - INFO - __main__ -  [amazon_polarity] title: TAILOR MADE ROLE FOR BLACK! TOUGH TO DISLIKE! [SEP] content: Black is his usual over the top self in this very funny crowd pleaser. The supporting cast of kids and adults in the film are great. Throw in some catchy rock tunes and a whole lot of zany situations and you've got one of the best comedies of the year. The original release on DVD has a very good transfer and some very cool extras including excellent commentaries which make for some interesting repeated viewings of the film.
06/02/2022 15:49:10 - INFO - __main__ - ['positive']
06/02/2022 15:49:10 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:49:10 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:49:10 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 15:49:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 15:49:23 - INFO - __main__ - Starting training!
06/02/2022 15:49:28 - INFO - __main__ - Step 10 Global step 10 Train loss 23.861431 on epoch=1
06/02/2022 15:49:33 - INFO - __main__ - Step 20 Global step 20 Train loss 18.159662 on epoch=2
06/02/2022 15:49:37 - INFO - __main__ - Step 30 Global step 30 Train loss 17.646351 on epoch=3
06/02/2022 15:49:43 - INFO - __main__ - Step 40 Global step 40 Train loss 15.269506 on epoch=4
06/02/2022 15:49:48 - INFO - __main__ - Step 50 Global step 50 Train loss 14.309186 on epoch=6
06/02/2022 15:50:16 - INFO - __main__ - Global step 50 Train loss 17.849228 Classification-F1 0.0 on epoch=6
06/02/2022 15:50:21 - INFO - __main__ - Step 60 Global step 60 Train loss 13.037683 on epoch=7
06/02/2022 15:50:27 - INFO - __main__ - Step 70 Global step 70 Train loss 11.803569 on epoch=8
06/02/2022 15:50:32 - INFO - __main__ - Step 80 Global step 80 Train loss 6.555377 on epoch=9
06/02/2022 15:50:37 - INFO - __main__ - Step 90 Global step 90 Train loss 1.071969 on epoch=11
06/02/2022 15:50:42 - INFO - __main__ - Step 100 Global step 100 Train loss 0.564093 on epoch=12
06/02/2022 15:50:55 - INFO - __main__ - Global step 100 Train loss 6.606537 Classification-F1 0.13977099847866123 on epoch=12
06/02/2022 15:51:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.564640 on epoch=13
06/02/2022 15:51:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.654782 on epoch=14
06/02/2022 15:51:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.459704 on epoch=16
06/02/2022 15:51:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.704683 on epoch=17
06/02/2022 15:51:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.422760 on epoch=18
06/02/2022 15:51:24 - INFO - __main__ - Global step 150 Train loss 0.561314 Classification-F1 0.1765806657111005 on epoch=18
06/02/2022 15:51:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.413434 on epoch=19
06/02/2022 15:51:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.418620 on epoch=21
06/02/2022 15:51:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.459949 on epoch=22
06/02/2022 15:51:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.407692 on epoch=23
06/02/2022 15:51:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.388442 on epoch=24
06/02/2022 15:51:53 - INFO - __main__ - Global step 200 Train loss 0.417627 Classification-F1 0.5927889713679746 on epoch=24
06/02/2022 15:51:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.357262 on epoch=26
06/02/2022 15:52:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.376839 on epoch=27
06/02/2022 15:52:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.359793 on epoch=28
06/02/2022 15:52:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.379002 on epoch=29
06/02/2022 15:52:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.345498 on epoch=31
06/02/2022 15:52:22 - INFO - __main__ - Global step 250 Train loss 0.363679 Classification-F1 0.6635491166941745 on epoch=31
06/02/2022 15:52:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.368014 on epoch=32
06/02/2022 15:52:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.395343 on epoch=33
06/02/2022 15:52:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.365637 on epoch=34
06/02/2022 15:52:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.379577 on epoch=36
06/02/2022 15:52:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.364936 on epoch=37
06/02/2022 15:52:50 - INFO - __main__ - Global step 300 Train loss 0.374701 Classification-F1 0.2198952879581152 on epoch=37
06/02/2022 15:52:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.402464 on epoch=38
06/02/2022 15:53:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.341749 on epoch=39
06/02/2022 15:53:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.342824 on epoch=41
06/02/2022 15:53:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.336296 on epoch=42
06/02/2022 15:53:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.357650 on epoch=43
06/02/2022 15:53:18 - INFO - __main__ - Global step 350 Train loss 0.356197 Classification-F1 0.41013824884792627 on epoch=43
06/02/2022 15:53:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.373103 on epoch=44
06/02/2022 15:53:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.327381 on epoch=46
06/02/2022 15:53:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.362460 on epoch=47
06/02/2022 15:53:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.369764 on epoch=48
06/02/2022 15:53:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.362484 on epoch=49
06/02/2022 15:53:46 - INFO - __main__ - Global step 400 Train loss 0.359039 Classification-F1 0.6190476190476191 on epoch=49
06/02/2022 15:53:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.319825 on epoch=51
06/02/2022 15:53:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.388367 on epoch=52
06/02/2022 15:54:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.339641 on epoch=53
06/02/2022 15:54:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.324645 on epoch=54
06/02/2022 15:54:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.357526 on epoch=56
06/02/2022 15:54:14 - INFO - __main__ - Global step 450 Train loss 0.346001 Classification-F1 0.5569230769230769 on epoch=56
06/02/2022 15:54:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.332453 on epoch=57
06/02/2022 15:54:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.400040 on epoch=58
06/02/2022 15:54:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.335019 on epoch=59
06/02/2022 15:54:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.341638 on epoch=61
06/02/2022 15:54:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.349093 on epoch=62
06/02/2022 15:54:42 - INFO - __main__ - Global step 500 Train loss 0.351649 Classification-F1 0.4573099415204678 on epoch=62
06/02/2022 15:54:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.336750 on epoch=63
06/02/2022 15:54:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.324685 on epoch=64
06/02/2022 15:54:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.326326 on epoch=66
06/02/2022 15:55:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.351157 on epoch=67
06/02/2022 15:55:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.337493 on epoch=68
06/02/2022 15:55:11 - INFO - __main__ - Global step 550 Train loss 0.335282 Classification-F1 0.3948694102146787 on epoch=68
06/02/2022 15:55:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.311213 on epoch=69
06/02/2022 15:55:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.337182 on epoch=71
06/02/2022 15:55:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.352882 on epoch=72
06/02/2022 15:55:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.293256 on epoch=73
06/02/2022 15:55:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.374770 on epoch=74
06/02/2022 15:55:39 - INFO - __main__ - Global step 600 Train loss 0.333861 Classification-F1 0.7012929675181331 on epoch=74
06/02/2022 15:55:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.334615 on epoch=76
06/02/2022 15:55:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.347839 on epoch=77
06/02/2022 15:55:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.313731 on epoch=78
06/02/2022 15:56:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.306077 on epoch=79
06/02/2022 15:56:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.337250 on epoch=81
06/02/2022 15:56:07 - INFO - __main__ - Global step 650 Train loss 0.327902 Classification-F1 0.3834004580273237 on epoch=81
06/02/2022 15:56:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.374712 on epoch=82
06/02/2022 15:56:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.307839 on epoch=83
06/02/2022 15:56:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.283285 on epoch=84
06/02/2022 15:56:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.354325 on epoch=86
06/02/2022 15:56:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.317165 on epoch=87
06/02/2022 15:56:35 - INFO - __main__ - Global step 700 Train loss 0.327465 Classification-F1 0.7264122137404581 on epoch=87
06/02/2022 15:56:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.293086 on epoch=88
06/02/2022 15:56:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.318003 on epoch=89
06/02/2022 15:56:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.322632 on epoch=91
06/02/2022 15:56:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.278067 on epoch=92
06/02/2022 15:57:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.290356 on epoch=93
06/02/2022 15:57:04 - INFO - __main__ - Global step 750 Train loss 0.300429 Classification-F1 0.703052503052503 on epoch=93
06/02/2022 15:57:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.311964 on epoch=94
06/02/2022 15:57:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.288858 on epoch=96
06/02/2022 15:57:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.270941 on epoch=97
06/02/2022 15:57:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.304584 on epoch=98
06/02/2022 15:57:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.318011 on epoch=99
06/02/2022 15:57:32 - INFO - __main__ - Global step 800 Train loss 0.298872 Classification-F1 0.7484647506755098 on epoch=99
06/02/2022 15:57:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.323997 on epoch=101
06/02/2022 15:57:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.312370 on epoch=102
06/02/2022 15:57:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.306036 on epoch=103
06/02/2022 15:57:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.339519 on epoch=104
06/02/2022 15:57:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.311963 on epoch=106
06/02/2022 15:58:00 - INFO - __main__ - Global step 850 Train loss 0.318777 Classification-F1 0.5033509700176367 on epoch=106
06/02/2022 15:58:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.290139 on epoch=107
06/02/2022 15:58:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.279275 on epoch=108
06/02/2022 15:58:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.287533 on epoch=109
06/02/2022 15:58:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.274118 on epoch=111
06/02/2022 15:58:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.330780 on epoch=112
06/02/2022 15:58:28 - INFO - __main__ - Global step 900 Train loss 0.292369 Classification-F1 0.7311588831233012 on epoch=112
06/02/2022 15:58:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.298410 on epoch=113
06/02/2022 15:58:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.280116 on epoch=114
06/02/2022 15:58:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.273807 on epoch=116
06/02/2022 15:58:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.274639 on epoch=117
06/02/2022 15:58:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.245072 on epoch=118
06/02/2022 15:58:56 - INFO - __main__ - Global step 950 Train loss 0.274409 Classification-F1 0.6623560445216496 on epoch=118
06/02/2022 15:59:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.256294 on epoch=119
06/02/2022 15:59:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.242754 on epoch=121
06/02/2022 15:59:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.293828 on epoch=122
06/02/2022 15:59:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.248036 on epoch=123
06/02/2022 15:59:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.215334 on epoch=124
06/02/2022 15:59:23 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:59:23 - INFO - __main__ - Printing 3 examples
06/02/2022 15:59:23 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
06/02/2022 15:59:23 - INFO - __main__ - ['positive']
06/02/2022 15:59:23 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
06/02/2022 15:59:23 - INFO - __main__ - ['positive']
06/02/2022 15:59:23 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
06/02/2022 15:59:23 - INFO - __main__ - ['positive']
06/02/2022 15:59:23 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:59:23 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:59:23 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 15:59:23 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:59:23 - INFO - __main__ - Printing 3 examples
06/02/2022 15:59:23 - INFO - __main__ -  [amazon_polarity] title: If Your Looking For A Good Crue Album... [SEP] content: Greatest Hits is the most suitable title for this album. Released in 1998, Motley Crue's greatest hits is a great compilation album for any die-hard Crue fan. On the album you'll find hits such as: Kickstart My Heart, Dr. Feelgood, and Smokin' In The Boys Room. If your into the heavier Crue stuff, you'll find hits such as Too Fast For Love, Looks That Kill, and Shout At The Devil! If your looking for a good variety of Motley Crue songs on a single album, then Motley Crue: Greatest Hits is the album for you.
06/02/2022 15:59:23 - INFO - __main__ - ['positive']
06/02/2022 15:59:23 - INFO - __main__ -  [amazon_polarity] title: LOVE IT! [SEP] content: Are you pondering what I'm pondering? Sure, but horizontal stripes make me look hippy.A must for any fan. The best part (really!) is the extra material with Maurice LaMarche and Rob Paulsen -- they are as intelligent, witty and friendly with each other as you'd ever hope. Restores your faith in humanity.
06/02/2022 15:59:23 - INFO - __main__ - ['positive']
06/02/2022 15:59:23 - INFO - __main__ -  [amazon_polarity] title: TAILOR MADE ROLE FOR BLACK! TOUGH TO DISLIKE! [SEP] content: Black is his usual over the top self in this very funny crowd pleaser. The supporting cast of kids and adults in the film are great. Throw in some catchy rock tunes and a whole lot of zany situations and you've got one of the best comedies of the year. The original release on DVD has a very good transfer and some very cool extras including excellent commentaries which make for some interesting repeated viewings of the film.
06/02/2022 15:59:23 - INFO - __main__ - ['positive']
06/02/2022 15:59:23 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:59:23 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:59:24 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 15:59:24 - INFO - __main__ - Global step 1000 Train loss 0.251249 Classification-F1 0.7057471264367816 on epoch=124
06/02/2022 15:59:24 - INFO - __main__ - save last model!
06/02/2022 15:59:31 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 15:59:32 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 15:59:32 - INFO - __main__ - Printing 3 examples
06/02/2022 15:59:32 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 15:59:32 - INFO - __main__ - ['negative']
06/02/2022 15:59:32 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 15:59:32 - INFO - __main__ - ['negative']
06/02/2022 15:59:32 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 15:59:32 - INFO - __main__ - ['negative']
06/02/2022 15:59:32 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:59:32 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:59:33 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 15:59:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 15:59:34 - INFO - __main__ - Starting training!
06/02/2022 15:59:48 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_21_0.0003_8_predictions.txt
06/02/2022 15:59:48 - INFO - __main__ - Classification-F1 on test data: 0.7545
06/02/2022 15:59:49 - INFO - __main__ - prefix=amazon_polarity_64_21, lr=0.0003, bsz=8, dev_performance=0.7484647506755098, test_performance=0.7545028683083244
06/02/2022 15:59:49 - INFO - __main__ - Running ... prefix=amazon_polarity_64_21, lr=0.0002, bsz=8 ...
06/02/2022 15:59:50 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:59:50 - INFO - __main__ - Printing 3 examples
06/02/2022 15:59:50 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
06/02/2022 15:59:50 - INFO - __main__ - ['positive']
06/02/2022 15:59:50 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
06/02/2022 15:59:50 - INFO - __main__ - ['positive']
06/02/2022 15:59:50 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
06/02/2022 15:59:50 - INFO - __main__ - ['positive']
06/02/2022 15:59:50 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:59:50 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:59:50 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 15:59:50 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 15:59:50 - INFO - __main__ - Printing 3 examples
06/02/2022 15:59:50 - INFO - __main__ -  [amazon_polarity] title: If Your Looking For A Good Crue Album... [SEP] content: Greatest Hits is the most suitable title for this album. Released in 1998, Motley Crue's greatest hits is a great compilation album for any die-hard Crue fan. On the album you'll find hits such as: Kickstart My Heart, Dr. Feelgood, and Smokin' In The Boys Room. If your into the heavier Crue stuff, you'll find hits such as Too Fast For Love, Looks That Kill, and Shout At The Devil! If your looking for a good variety of Motley Crue songs on a single album, then Motley Crue: Greatest Hits is the album for you.
06/02/2022 15:59:50 - INFO - __main__ - ['positive']
06/02/2022 15:59:50 - INFO - __main__ -  [amazon_polarity] title: LOVE IT! [SEP] content: Are you pondering what I'm pondering? Sure, but horizontal stripes make me look hippy.A must for any fan. The best part (really!) is the extra material with Maurice LaMarche and Rob Paulsen -- they are as intelligent, witty and friendly with each other as you'd ever hope. Restores your faith in humanity.
06/02/2022 15:59:50 - INFO - __main__ - ['positive']
06/02/2022 15:59:50 - INFO - __main__ -  [amazon_polarity] title: TAILOR MADE ROLE FOR BLACK! TOUGH TO DISLIKE! [SEP] content: Black is his usual over the top self in this very funny crowd pleaser. The supporting cast of kids and adults in the film are great. Throw in some catchy rock tunes and a whole lot of zany situations and you've got one of the best comedies of the year. The original release on DVD has a very good transfer and some very cool extras including excellent commentaries which make for some interesting repeated viewings of the film.
06/02/2022 15:59:50 - INFO - __main__ - ['positive']
06/02/2022 15:59:50 - INFO - __main__ - Tokenizing Input ...
06/02/2022 15:59:50 - INFO - __main__ - Tokenizing Output ...
06/02/2022 15:59:50 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 16:00:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 16:00:01 - INFO - __main__ - Starting training!
06/02/2022 16:00:06 - INFO - __main__ - Step 10 Global step 10 Train loss 22.571667 on epoch=1
06/02/2022 16:00:11 - INFO - __main__ - Step 20 Global step 20 Train loss 18.887365 on epoch=2
06/02/2022 16:00:16 - INFO - __main__ - Step 30 Global step 30 Train loss 17.088190 on epoch=3
06/02/2022 16:00:21 - INFO - __main__ - Step 40 Global step 40 Train loss 16.022060 on epoch=4
06/02/2022 16:00:27 - INFO - __main__ - Step 50 Global step 50 Train loss 15.101191 on epoch=6
06/02/2022 16:01:09 - INFO - __main__ - Global step 50 Train loss 17.934095 Classification-F1 0.0 on epoch=6
06/02/2022 16:01:15 - INFO - __main__ - Step 60 Global step 60 Train loss 14.208811 on epoch=7
06/02/2022 16:01:20 - INFO - __main__ - Step 70 Global step 70 Train loss 13.225986 on epoch=8
06/02/2022 16:01:25 - INFO - __main__ - Step 80 Global step 80 Train loss 12.916258 on epoch=9
06/02/2022 16:01:30 - INFO - __main__ - Step 90 Global step 90 Train loss 11.879798 on epoch=11
06/02/2022 16:01:36 - INFO - __main__ - Step 100 Global step 100 Train loss 11.259730 on epoch=12
06/02/2022 16:02:14 - INFO - __main__ - Global step 100 Train loss 12.698116 Classification-F1 0.0 on epoch=12
06/02/2022 16:02:19 - INFO - __main__ - Step 110 Global step 110 Train loss 9.431071 on epoch=13
06/02/2022 16:02:24 - INFO - __main__ - Step 120 Global step 120 Train loss 8.133752 on epoch=14
06/02/2022 16:02:29 - INFO - __main__ - Step 130 Global step 130 Train loss 2.189656 on epoch=16
06/02/2022 16:02:34 - INFO - __main__ - Step 140 Global step 140 Train loss 2.039582 on epoch=17
06/02/2022 16:02:38 - INFO - __main__ - Step 150 Global step 150 Train loss 2.480995 on epoch=18
06/02/2022 16:02:40 - INFO - __main__ - Global step 150 Train loss 4.855011 Classification-F1 0.5588547189819725 on epoch=18
06/02/2022 16:02:47 - INFO - __main__ - Step 160 Global step 160 Train loss 1.143514 on epoch=19
06/02/2022 16:02:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.841108 on epoch=21
06/02/2022 16:02:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.461210 on epoch=22
06/02/2022 16:03:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.402823 on epoch=23
06/02/2022 16:03:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.582646 on epoch=24
06/02/2022 16:03:09 - INFO - __main__ - Global step 200 Train loss 0.686260 Classification-F1 0.6421248835041938 on epoch=24
06/02/2022 16:03:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.535875 on epoch=26
06/02/2022 16:03:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.361866 on epoch=27
06/02/2022 16:03:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.387758 on epoch=28
06/02/2022 16:03:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.388171 on epoch=29
06/02/2022 16:03:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.374579 on epoch=31
06/02/2022 16:03:38 - INFO - __main__ - Global step 250 Train loss 0.409650 Classification-F1 0.3948694102146787 on epoch=31
06/02/2022 16:03:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.390645 on epoch=32
06/02/2022 16:03:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.388039 on epoch=33
06/02/2022 16:03:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.374315 on epoch=34
06/02/2022 16:03:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.399120 on epoch=36
06/02/2022 16:04:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.381647 on epoch=37
06/02/2022 16:04:06 - INFO - __main__ - Global step 300 Train loss 0.386753 Classification-F1 0.3671451355661882 on epoch=37
06/02/2022 16:04:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.387116 on epoch=38
06/02/2022 16:04:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.358946 on epoch=39
06/02/2022 16:04:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.399341 on epoch=41
06/02/2022 16:04:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.360642 on epoch=42
06/02/2022 16:04:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.400607 on epoch=43
06/02/2022 16:04:34 - INFO - __main__ - Global step 350 Train loss 0.381330 Classification-F1 0.6226557109703318 on epoch=43
06/02/2022 16:04:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.397114 on epoch=44
06/02/2022 16:04:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.386921 on epoch=46
06/02/2022 16:04:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.375565 on epoch=47
06/02/2022 16:04:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.369018 on epoch=48
06/02/2022 16:05:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.397622 on epoch=49
06/02/2022 16:05:02 - INFO - __main__ - Global step 400 Train loss 0.385248 Classification-F1 0.5331473932632482 on epoch=49
06/02/2022 16:05:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.365924 on epoch=51
06/02/2022 16:05:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.378801 on epoch=52
06/02/2022 16:05:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.411717 on epoch=53
06/02/2022 16:05:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.352200 on epoch=54
06/02/2022 16:05:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.366310 on epoch=56
06/02/2022 16:05:30 - INFO - __main__ - Global step 450 Train loss 0.374990 Classification-F1 0.7465346534653464 on epoch=56
06/02/2022 16:05:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.345355 on epoch=57
06/02/2022 16:05:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.374518 on epoch=58
06/02/2022 16:05:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.349285 on epoch=59
06/02/2022 16:05:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.336580 on epoch=61
06/02/2022 16:05:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.374130 on epoch=62
06/02/2022 16:05:59 - INFO - __main__ - Global step 500 Train loss 0.355974 Classification-F1 0.5335015419119707 on epoch=62
06/02/2022 16:06:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.357749 on epoch=63
06/02/2022 16:06:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.371232 on epoch=64
06/02/2022 16:06:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.367600 on epoch=66
06/02/2022 16:06:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.370742 on epoch=67
06/02/2022 16:06:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.395168 on epoch=68
06/02/2022 16:06:26 - INFO - __main__ - Global step 550 Train loss 0.372498 Classification-F1 0.6316316316316316 on epoch=68
06/02/2022 16:06:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.363126 on epoch=69
06/02/2022 16:06:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.373314 on epoch=71
06/02/2022 16:06:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.418883 on epoch=72
06/02/2022 16:06:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.371467 on epoch=73
06/02/2022 16:06:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.345264 on epoch=74
06/02/2022 16:06:54 - INFO - __main__ - Global step 600 Train loss 0.374411 Classification-F1 0.3333333333333333 on epoch=74
06/02/2022 16:07:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.344072 on epoch=76
06/02/2022 16:07:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.351296 on epoch=77
06/02/2022 16:07:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.369801 on epoch=78
06/02/2022 16:07:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.345174 on epoch=79
06/02/2022 16:07:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.341099 on epoch=81
06/02/2022 16:07:22 - INFO - __main__ - Global step 650 Train loss 0.350288 Classification-F1 0.7104957515740571 on epoch=81
06/02/2022 16:07:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.368496 on epoch=82
06/02/2022 16:07:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.357566 on epoch=83
06/02/2022 16:07:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.341581 on epoch=84
06/02/2022 16:07:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.334841 on epoch=86
06/02/2022 16:07:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.344661 on epoch=87
06/02/2022 16:07:50 - INFO - __main__ - Global step 700 Train loss 0.349429 Classification-F1 0.3948694102146787 on epoch=87
06/02/2022 16:07:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.369289 on epoch=88
06/02/2022 16:08:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.336564 on epoch=89
06/02/2022 16:08:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.362633 on epoch=91
06/02/2022 16:08:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.328338 on epoch=92
06/02/2022 16:08:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.391022 on epoch=93
06/02/2022 16:08:18 - INFO - __main__ - Global step 750 Train loss 0.357569 Classification-F1 0.6003673094582185 on epoch=93
06/02/2022 16:08:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.380577 on epoch=94
06/02/2022 16:08:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.349219 on epoch=96
06/02/2022 16:08:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.376894 on epoch=97
06/02/2022 16:08:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.366992 on epoch=98
06/02/2022 16:08:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.354488 on epoch=99
06/02/2022 16:08:46 - INFO - __main__ - Global step 800 Train loss 0.365634 Classification-F1 0.37922403003754696 on epoch=99
06/02/2022 16:08:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.351665 on epoch=101
06/02/2022 16:08:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.351884 on epoch=102
06/02/2022 16:09:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.359649 on epoch=103
06/02/2022 16:09:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.329086 on epoch=104
06/02/2022 16:09:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.329246 on epoch=106
06/02/2022 16:09:14 - INFO - __main__ - Global step 850 Train loss 0.344306 Classification-F1 0.3298429319371728 on epoch=106
06/02/2022 16:09:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.357631 on epoch=107
06/02/2022 16:09:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.360717 on epoch=108
06/02/2022 16:09:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.332837 on epoch=109
06/02/2022 16:09:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.331696 on epoch=111
06/02/2022 16:09:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.333933 on epoch=112
06/02/2022 16:09:42 - INFO - __main__ - Global step 900 Train loss 0.343363 Classification-F1 0.34673046251993617 on epoch=112
06/02/2022 16:09:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.339033 on epoch=113
06/02/2022 16:09:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.349922 on epoch=114
06/02/2022 16:09:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.318836 on epoch=116
06/02/2022 16:10:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.335492 on epoch=117
06/02/2022 16:10:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.351217 on epoch=118
06/02/2022 16:10:10 - INFO - __main__ - Global step 950 Train loss 0.338900 Classification-F1 0.6940880503144655 on epoch=118
06/02/2022 16:10:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.320578 on epoch=119
06/02/2022 16:10:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.321204 on epoch=121
06/02/2022 16:10:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.303822 on epoch=122
06/02/2022 16:10:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.340037 on epoch=123
06/02/2022 16:10:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.334694 on epoch=124
06/02/2022 16:10:37 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:10:37 - INFO - __main__ - Printing 3 examples
06/02/2022 16:10:37 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
06/02/2022 16:10:37 - INFO - __main__ - ['positive']
06/02/2022 16:10:37 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
06/02/2022 16:10:37 - INFO - __main__ - ['positive']
06/02/2022 16:10:37 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
06/02/2022 16:10:37 - INFO - __main__ - ['positive']
06/02/2022 16:10:37 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:10:37 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:10:37 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 16:10:37 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:10:37 - INFO - __main__ - Printing 3 examples
06/02/2022 16:10:37 - INFO - __main__ -  [amazon_polarity] title: If Your Looking For A Good Crue Album... [SEP] content: Greatest Hits is the most suitable title for this album. Released in 1998, Motley Crue's greatest hits is a great compilation album for any die-hard Crue fan. On the album you'll find hits such as: Kickstart My Heart, Dr. Feelgood, and Smokin' In The Boys Room. If your into the heavier Crue stuff, you'll find hits such as Too Fast For Love, Looks That Kill, and Shout At The Devil! If your looking for a good variety of Motley Crue songs on a single album, then Motley Crue: Greatest Hits is the album for you.
06/02/2022 16:10:37 - INFO - __main__ - ['positive']
06/02/2022 16:10:37 - INFO - __main__ -  [amazon_polarity] title: LOVE IT! [SEP] content: Are you pondering what I'm pondering? Sure, but horizontal stripes make me look hippy.A must for any fan. The best part (really!) is the extra material with Maurice LaMarche and Rob Paulsen -- they are as intelligent, witty and friendly with each other as you'd ever hope. Restores your faith in humanity.
06/02/2022 16:10:37 - INFO - __main__ - ['positive']
06/02/2022 16:10:37 - INFO - __main__ -  [amazon_polarity] title: TAILOR MADE ROLE FOR BLACK! TOUGH TO DISLIKE! [SEP] content: Black is his usual over the top self in this very funny crowd pleaser. The supporting cast of kids and adults in the film are great. Throw in some catchy rock tunes and a whole lot of zany situations and you've got one of the best comedies of the year. The original release on DVD has a very good transfer and some very cool extras including excellent commentaries which make for some interesting repeated viewings of the film.
06/02/2022 16:10:37 - INFO - __main__ - ['positive']
06/02/2022 16:10:37 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:10:37 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:10:37 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 16:10:38 - INFO - __main__ - Global step 1000 Train loss 0.324067 Classification-F1 0.5097127222982216 on epoch=124
06/02/2022 16:10:38 - INFO - __main__ - save last model!
06/02/2022 16:10:45 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 16:10:45 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 16:10:45 - INFO - __main__ - Printing 3 examples
06/02/2022 16:10:45 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 16:10:45 - INFO - __main__ - ['negative']
06/02/2022 16:10:45 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 16:10:45 - INFO - __main__ - ['negative']
06/02/2022 16:10:45 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 16:10:45 - INFO - __main__ - ['negative']
06/02/2022 16:10:45 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:10:46 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:10:47 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 16:10:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 16:10:50 - INFO - __main__ - Starting training!
06/02/2022 16:11:02 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_21_0.0002_8_predictions.txt
06/02/2022 16:11:02 - INFO - __main__ - Classification-F1 on test data: 0.6572
06/02/2022 16:11:02 - INFO - __main__ - prefix=amazon_polarity_64_21, lr=0.0002, bsz=8, dev_performance=0.7465346534653464, test_performance=0.6571521863350658
06/02/2022 16:11:02 - INFO - __main__ - Running ... prefix=amazon_polarity_64_21, lr=0.0001, bsz=8 ...
06/02/2022 16:11:03 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:11:03 - INFO - __main__ - Printing 3 examples
06/02/2022 16:11:03 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
06/02/2022 16:11:03 - INFO - __main__ - ['positive']
06/02/2022 16:11:03 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
06/02/2022 16:11:03 - INFO - __main__ - ['positive']
06/02/2022 16:11:03 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
06/02/2022 16:11:03 - INFO - __main__ - ['positive']
06/02/2022 16:11:03 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:11:03 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:11:04 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 16:11:04 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:11:04 - INFO - __main__ - Printing 3 examples
06/02/2022 16:11:04 - INFO - __main__ -  [amazon_polarity] title: If Your Looking For A Good Crue Album... [SEP] content: Greatest Hits is the most suitable title for this album. Released in 1998, Motley Crue's greatest hits is a great compilation album for any die-hard Crue fan. On the album you'll find hits such as: Kickstart My Heart, Dr. Feelgood, and Smokin' In The Boys Room. If your into the heavier Crue stuff, you'll find hits such as Too Fast For Love, Looks That Kill, and Shout At The Devil! If your looking for a good variety of Motley Crue songs on a single album, then Motley Crue: Greatest Hits is the album for you.
06/02/2022 16:11:04 - INFO - __main__ - ['positive']
06/02/2022 16:11:04 - INFO - __main__ -  [amazon_polarity] title: LOVE IT! [SEP] content: Are you pondering what I'm pondering? Sure, but horizontal stripes make me look hippy.A must for any fan. The best part (really!) is the extra material with Maurice LaMarche and Rob Paulsen -- they are as intelligent, witty and friendly with each other as you'd ever hope. Restores your faith in humanity.
06/02/2022 16:11:04 - INFO - __main__ - ['positive']
06/02/2022 16:11:04 - INFO - __main__ -  [amazon_polarity] title: TAILOR MADE ROLE FOR BLACK! TOUGH TO DISLIKE! [SEP] content: Black is his usual over the top self in this very funny crowd pleaser. The supporting cast of kids and adults in the film are great. Throw in some catchy rock tunes and a whole lot of zany situations and you've got one of the best comedies of the year. The original release on DVD has a very good transfer and some very cool extras including excellent commentaries which make for some interesting repeated viewings of the film.
06/02/2022 16:11:04 - INFO - __main__ - ['positive']
06/02/2022 16:11:04 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:11:04 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:11:04 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 16:11:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 16:11:17 - INFO - __main__ - Starting training!
06/02/2022 16:11:21 - INFO - __main__ - Step 10 Global step 10 Train loss 23.642673 on epoch=1
06/02/2022 16:11:26 - INFO - __main__ - Step 20 Global step 20 Train loss 18.684986 on epoch=2
06/02/2022 16:11:32 - INFO - __main__ - Step 30 Global step 30 Train loss 17.803131 on epoch=3
06/02/2022 16:11:37 - INFO - __main__ - Step 40 Global step 40 Train loss 17.511827 on epoch=4
06/02/2022 16:11:42 - INFO - __main__ - Step 50 Global step 50 Train loss 16.482548 on epoch=6
06/02/2022 16:12:26 - INFO - __main__ - Global step 50 Train loss 18.825033 Classification-F1 0.0 on epoch=6
06/02/2022 16:12:31 - INFO - __main__ - Step 60 Global step 60 Train loss 16.592762 on epoch=7
06/02/2022 16:12:37 - INFO - __main__ - Step 70 Global step 70 Train loss 15.460146 on epoch=8
06/02/2022 16:12:42 - INFO - __main__ - Step 80 Global step 80 Train loss 15.071817 on epoch=9
06/02/2022 16:12:47 - INFO - __main__ - Step 90 Global step 90 Train loss 15.195600 on epoch=11
06/02/2022 16:12:52 - INFO - __main__ - Step 100 Global step 100 Train loss 14.093924 on epoch=12
06/02/2022 16:13:31 - INFO - __main__ - Global step 100 Train loss 15.282850 Classification-F1 0.0 on epoch=12
06/02/2022 16:13:36 - INFO - __main__ - Step 110 Global step 110 Train loss 13.728127 on epoch=13
06/02/2022 16:13:42 - INFO - __main__ - Step 120 Global step 120 Train loss 13.122223 on epoch=14
06/02/2022 16:13:47 - INFO - __main__ - Step 130 Global step 130 Train loss 13.086301 on epoch=16
06/02/2022 16:13:52 - INFO - __main__ - Step 140 Global step 140 Train loss 12.113671 on epoch=17
06/02/2022 16:13:57 - INFO - __main__ - Step 150 Global step 150 Train loss 12.313667 on epoch=18
06/02/2022 16:14:36 - INFO - __main__ - Global step 150 Train loss 12.872797 Classification-F1 0.0 on epoch=18
06/02/2022 16:14:41 - INFO - __main__ - Step 160 Global step 160 Train loss 11.769178 on epoch=19
06/02/2022 16:14:46 - INFO - __main__ - Step 170 Global step 170 Train loss 11.536380 on epoch=21
06/02/2022 16:14:51 - INFO - __main__ - Step 180 Global step 180 Train loss 10.931796 on epoch=22
06/02/2022 16:14:57 - INFO - __main__ - Step 190 Global step 190 Train loss 9.265855 on epoch=23
06/02/2022 16:15:02 - INFO - __main__ - Step 200 Global step 200 Train loss 7.721217 on epoch=24
06/02/2022 16:15:04 - INFO - __main__ - Global step 200 Train loss 10.244885 Classification-F1 0.37919506236337924 on epoch=24
06/02/2022 16:15:10 - INFO - __main__ - Step 210 Global step 210 Train loss 1.180055 on epoch=26
06/02/2022 16:15:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.841395 on epoch=27
06/02/2022 16:15:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.671556 on epoch=28
06/02/2022 16:15:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.199363 on epoch=29
06/02/2022 16:15:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.311875 on epoch=31
06/02/2022 16:15:33 - INFO - __main__ - Global step 250 Train loss 0.640849 Classification-F1 0.9687423687423687 on epoch=31
06/02/2022 16:15:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.267188 on epoch=32
06/02/2022 16:15:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.092358 on epoch=33
06/02/2022 16:15:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.100487 on epoch=34
06/02/2022 16:15:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.106254 on epoch=36
06/02/2022 16:15:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.110712 on epoch=37
06/02/2022 16:16:01 - INFO - __main__ - Global step 300 Train loss 0.135400 Classification-F1 0.960935115668681 on epoch=37
06/02/2022 16:16:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.027444 on epoch=38
06/02/2022 16:16:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.044439 on epoch=39
06/02/2022 16:16:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.059907 on epoch=41
06/02/2022 16:16:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.076515 on epoch=42
06/02/2022 16:16:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.037643 on epoch=43
06/02/2022 16:16:29 - INFO - __main__ - Global step 350 Train loss 0.049190 Classification-F1 0.96875 on epoch=43
06/02/2022 16:16:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.049075 on epoch=44
06/02/2022 16:16:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.016849 on epoch=46
06/02/2022 16:16:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.047156 on epoch=47
06/02/2022 16:16:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.029866 on epoch=48
06/02/2022 16:16:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.015747 on epoch=49
06/02/2022 16:16:57 - INFO - __main__ - Global step 400 Train loss 0.031739 Classification-F1 0.960935115668681 on epoch=49
06/02/2022 16:17:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.002556 on epoch=51
06/02/2022 16:17:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.100511 on epoch=52
06/02/2022 16:17:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.022982 on epoch=53
06/02/2022 16:17:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.012377 on epoch=54
06/02/2022 16:17:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.030756 on epoch=56
06/02/2022 16:17:25 - INFO - __main__ - Global step 450 Train loss 0.033836 Classification-F1 0.9687423687423687 on epoch=56
06/02/2022 16:17:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.013318 on epoch=57
06/02/2022 16:17:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.022657 on epoch=58
06/02/2022 16:17:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.002578 on epoch=59
06/02/2022 16:17:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.003398 on epoch=61
06/02/2022 16:17:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.016365 on epoch=62
06/02/2022 16:17:53 - INFO - __main__ - Global step 500 Train loss 0.011663 Classification-F1 0.96875 on epoch=62
06/02/2022 16:17:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.001714 on epoch=63
06/02/2022 16:18:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.014398 on epoch=64
06/02/2022 16:18:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.023066 on epoch=66
06/02/2022 16:18:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000994 on epoch=67
06/02/2022 16:18:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001045 on epoch=68
06/02/2022 16:18:21 - INFO - __main__ - Global step 550 Train loss 0.008243 Classification-F1 0.96875 on epoch=68
06/02/2022 16:18:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001311 on epoch=69
06/02/2022 16:18:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.004573 on epoch=71
06/02/2022 16:18:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000528 on epoch=72
06/02/2022 16:18:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.001484 on epoch=73
06/02/2022 16:18:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000361 on epoch=74
06/02/2022 16:18:48 - INFO - __main__ - Global step 600 Train loss 0.001651 Classification-F1 0.96875 on epoch=74
06/02/2022 16:18:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.011555 on epoch=76
06/02/2022 16:18:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000274 on epoch=77
06/02/2022 16:19:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000142 on epoch=78
06/02/2022 16:19:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.006864 on epoch=79
06/02/2022 16:19:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000341 on epoch=81
06/02/2022 16:19:16 - INFO - __main__ - Global step 650 Train loss 0.003835 Classification-F1 0.96875 on epoch=81
06/02/2022 16:19:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000805 on epoch=82
06/02/2022 16:19:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000174 on epoch=83
06/02/2022 16:19:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.003277 on epoch=84
06/02/2022 16:19:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.001666 on epoch=86
06/02/2022 16:19:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000806 on epoch=87
06/02/2022 16:19:44 - INFO - __main__ - Global step 700 Train loss 0.001346 Classification-F1 0.96875 on epoch=87
06/02/2022 16:19:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.021847 on epoch=88
06/02/2022 16:19:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000081 on epoch=89
06/02/2022 16:19:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000282 on epoch=91
06/02/2022 16:20:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000308 on epoch=92
06/02/2022 16:20:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000086 on epoch=93
06/02/2022 16:20:11 - INFO - __main__ - Global step 750 Train loss 0.004521 Classification-F1 0.96875 on epoch=93
06/02/2022 16:20:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000192 on epoch=94
06/02/2022 16:20:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000431 on epoch=96
06/02/2022 16:20:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.001024 on epoch=97
06/02/2022 16:20:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000043 on epoch=98
06/02/2022 16:20:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000379 on epoch=99
06/02/2022 16:20:39 - INFO - __main__ - Global step 800 Train loss 0.000414 Classification-F1 0.96875 on epoch=99
06/02/2022 16:20:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.001046 on epoch=101
06/02/2022 16:20:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000134 on epoch=102
06/02/2022 16:20:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000039 on epoch=103
06/02/2022 16:20:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000031 on epoch=104
06/02/2022 16:21:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000205 on epoch=106
06/02/2022 16:21:06 - INFO - __main__ - Global step 850 Train loss 0.000291 Classification-F1 0.96875 on epoch=106
06/02/2022 16:21:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000025 on epoch=107
06/02/2022 16:21:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000138 on epoch=108
06/02/2022 16:21:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000056 on epoch=109
06/02/2022 16:21:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000226 on epoch=111
06/02/2022 16:21:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.005936 on epoch=112
06/02/2022 16:21:33 - INFO - __main__ - Global step 900 Train loss 0.001276 Classification-F1 0.9687423687423687 on epoch=112
06/02/2022 16:21:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000043 on epoch=113
06/02/2022 16:21:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000063 on epoch=114
06/02/2022 16:21:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.015981 on epoch=116
06/02/2022 16:21:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.001094 on epoch=117
06/02/2022 16:21:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000831 on epoch=118
06/02/2022 16:22:01 - INFO - __main__ - Global step 950 Train loss 0.003602 Classification-F1 0.9687423687423687 on epoch=118
06/02/2022 16:22:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000042 on epoch=119
06/02/2022 16:22:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.004831 on epoch=121
06/02/2022 16:22:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000186 on epoch=122
06/02/2022 16:22:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000049 on epoch=123
06/02/2022 16:22:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000186 on epoch=124
06/02/2022 16:22:28 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:22:28 - INFO - __main__ - Printing 3 examples
06/02/2022 16:22:28 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
06/02/2022 16:22:28 - INFO - __main__ - ['negative']
06/02/2022 16:22:28 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
06/02/2022 16:22:28 - INFO - __main__ - ['negative']
06/02/2022 16:22:28 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
06/02/2022 16:22:28 - INFO - __main__ - ['negative']
06/02/2022 16:22:28 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:22:28 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:22:28 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 16:22:28 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:22:28 - INFO - __main__ - Printing 3 examples
06/02/2022 16:22:28 - INFO - __main__ -  [amazon_polarity] title: Amazon rip off [SEP] content: Don't buy this product from Amazon. I bought this product because I thought I was going to receive ten blank vhs tapes instead I received one. The picture that they have is very deceptive. It clearly showed 10.
06/02/2022 16:22:28 - INFO - __main__ - ['negative']
06/02/2022 16:22:28 - INFO - __main__ -  [amazon_polarity] title: Full of inaccuracies. [SEP] content: There are some interesting technical details in this book, which might make it worth reading for a Titanic enthusiast.There are numerous contradictions in the book to known established facts. Stokers assaulting people with shovels for a place in the boats? Attempted stabbing of the radio operator for a life belt? Male passengers being shot? The bridge phone not being answered for minutes? Collapsible boats disintegrating immediately?I'm a good portion through this book, and I'm doubtful it's worth more of my time. It's an old book written in a different time, the price is right, but this is fiction.
06/02/2022 16:22:28 - INFO - __main__ - ['negative']
06/02/2022 16:22:28 - INFO - __main__ -  [amazon_polarity] title: A real stupid book [SEP] content: One of the most boring and stupid books I`ve read in a long time. Full of ignorant remarks. From Mr Clancys observation that asian girls like westerners because they are better "equipped" than the asian males, via people saying Comrade Doctor etc to each other in Russia, ten years after the Soviet Unions fall, to the "fact" that american girls are slimmer and healthier than the rest of the world(when we know that they are sicker due to overweight than the rest of the world). Well, a writer can write about his white supremacy but when he writes as boring as Tom Clancy does in this book it gets completely uninteresting. Just pure typewriting and after 350 pages I had to give up.  Yes, and I spell terrible...
06/02/2022 16:22:28 - INFO - __main__ - ['negative']
06/02/2022 16:22:28 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:22:28 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:22:28 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 16:22:29 - INFO - __main__ - Global step 1000 Train loss 0.001059 Classification-F1 0.96875 on epoch=124
06/02/2022 16:22:29 - INFO - __main__ - save last model!
06/02/2022 16:22:36 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 16:22:37 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 16:22:37 - INFO - __main__ - Printing 3 examples
06/02/2022 16:22:37 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 16:22:37 - INFO - __main__ - ['negative']
06/02/2022 16:22:37 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 16:22:37 - INFO - __main__ - ['negative']
06/02/2022 16:22:37 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 16:22:37 - INFO - __main__ - ['negative']
06/02/2022 16:22:37 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:22:37 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:22:38 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 16:22:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 16:22:41 - INFO - __main__ - Starting training!
06/02/2022 16:22:53 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_21_0.0001_8_predictions.txt
06/02/2022 16:22:53 - INFO - __main__ - Classification-F1 on test data: 0.9650
06/02/2022 16:22:53 - INFO - __main__ - prefix=amazon_polarity_64_21, lr=0.0001, bsz=8, dev_performance=0.96875, test_performance=0.9649996849971649
06/02/2022 16:22:53 - INFO - __main__ - Running ... prefix=amazon_polarity_64_42, lr=0.0005, bsz=8 ...
06/02/2022 16:22:54 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:22:54 - INFO - __main__ - Printing 3 examples
06/02/2022 16:22:54 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
06/02/2022 16:22:54 - INFO - __main__ - ['negative']
06/02/2022 16:22:54 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
06/02/2022 16:22:54 - INFO - __main__ - ['negative']
06/02/2022 16:22:54 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
06/02/2022 16:22:54 - INFO - __main__ - ['negative']
06/02/2022 16:22:54 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:22:54 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:22:55 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 16:22:55 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:22:55 - INFO - __main__ - Printing 3 examples
06/02/2022 16:22:55 - INFO - __main__ -  [amazon_polarity] title: Amazon rip off [SEP] content: Don't buy this product from Amazon. I bought this product because I thought I was going to receive ten blank vhs tapes instead I received one. The picture that they have is very deceptive. It clearly showed 10.
06/02/2022 16:22:55 - INFO - __main__ - ['negative']
06/02/2022 16:22:55 - INFO - __main__ -  [amazon_polarity] title: Full of inaccuracies. [SEP] content: There are some interesting technical details in this book, which might make it worth reading for a Titanic enthusiast.There are numerous contradictions in the book to known established facts. Stokers assaulting people with shovels for a place in the boats? Attempted stabbing of the radio operator for a life belt? Male passengers being shot? The bridge phone not being answered for minutes? Collapsible boats disintegrating immediately?I'm a good portion through this book, and I'm doubtful it's worth more of my time. It's an old book written in a different time, the price is right, but this is fiction.
06/02/2022 16:22:55 - INFO - __main__ - ['negative']
06/02/2022 16:22:55 - INFO - __main__ -  [amazon_polarity] title: A real stupid book [SEP] content: One of the most boring and stupid books I`ve read in a long time. Full of ignorant remarks. From Mr Clancys observation that asian girls like westerners because they are better "equipped" than the asian males, via people saying Comrade Doctor etc to each other in Russia, ten years after the Soviet Unions fall, to the "fact" that american girls are slimmer and healthier than the rest of the world(when we know that they are sicker due to overweight than the rest of the world). Well, a writer can write about his white supremacy but when he writes as boring as Tom Clancy does in this book it gets completely uninteresting. Just pure typewriting and after 350 pages I had to give up.  Yes, and I spell terrible...
06/02/2022 16:22:55 - INFO - __main__ - ['negative']
06/02/2022 16:22:55 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:22:55 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:22:55 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 16:23:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 16:23:08 - INFO - __main__ - Starting training!
06/02/2022 16:23:12 - INFO - __main__ - Step 10 Global step 10 Train loss 22.215624 on epoch=1
06/02/2022 16:23:17 - INFO - __main__ - Step 20 Global step 20 Train loss 17.330402 on epoch=2
06/02/2022 16:23:23 - INFO - __main__ - Step 30 Global step 30 Train loss 14.471728 on epoch=3
06/02/2022 16:23:28 - INFO - __main__ - Step 40 Global step 40 Train loss 12.391832 on epoch=4
06/02/2022 16:23:33 - INFO - __main__ - Step 50 Global step 50 Train loss 6.575456 on epoch=6
06/02/2022 16:23:35 - INFO - __main__ - Global step 50 Train loss 14.597007 Classification-F1 0.22105263157894736 on epoch=6
06/02/2022 16:23:40 - INFO - __main__ - Step 60 Global step 60 Train loss 0.459142 on epoch=7
06/02/2022 16:23:46 - INFO - __main__ - Step 70 Global step 70 Train loss 0.481379 on epoch=8
06/02/2022 16:23:51 - INFO - __main__ - Step 80 Global step 80 Train loss 0.323676 on epoch=9
06/02/2022 16:23:56 - INFO - __main__ - Step 90 Global step 90 Train loss 0.259411 on epoch=11
06/02/2022 16:24:01 - INFO - __main__ - Step 100 Global step 100 Train loss 0.237385 on epoch=12
06/02/2022 16:24:03 - INFO - __main__ - Global step 100 Train loss 0.352199 Classification-F1 0.9372549019607843 on epoch=12
06/02/2022 16:24:09 - INFO - __main__ - Step 110 Global step 110 Train loss 0.160724 on epoch=13
06/02/2022 16:24:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.120650 on epoch=14
06/02/2022 16:24:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.207132 on epoch=16
06/02/2022 16:24:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.170162 on epoch=17
06/02/2022 16:24:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.041535 on epoch=18
06/02/2022 16:24:32 - INFO - __main__ - Global step 150 Train loss 0.140041 Classification-F1 0.9530791788856304 on epoch=18
06/02/2022 16:24:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.090718 on epoch=19
06/02/2022 16:24:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.028471 on epoch=21
06/02/2022 16:24:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.030521 on epoch=22
06/02/2022 16:24:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.005099 on epoch=23
06/02/2022 16:24:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.005621 on epoch=24
06/02/2022 16:25:00 - INFO - __main__ - Global step 200 Train loss 0.032086 Classification-F1 0.9609160305343512 on epoch=24
06/02/2022 16:25:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.001623 on epoch=26
06/02/2022 16:25:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.000983 on epoch=27
06/02/2022 16:25:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.003232 on epoch=28
06/02/2022 16:25:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.000641 on epoch=29
06/02/2022 16:25:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.003609 on epoch=31
06/02/2022 16:25:29 - INFO - __main__ - Global step 250 Train loss 0.002018 Classification-F1 0.9530791788856304 on epoch=31
06/02/2022 16:25:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.003093 on epoch=32
06/02/2022 16:25:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.051972 on epoch=33
06/02/2022 16:25:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.055077 on epoch=34
06/02/2022 16:25:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.018538 on epoch=36
06/02/2022 16:25:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.005174 on epoch=37
06/02/2022 16:25:57 - INFO - __main__ - Global step 300 Train loss 0.026771 Classification-F1 0.9687194525904204 on epoch=37
06/02/2022 16:26:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.000236 on epoch=38
06/02/2022 16:26:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.000299 on epoch=39
06/02/2022 16:26:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000346 on epoch=41
06/02/2022 16:26:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.000118 on epoch=42
06/02/2022 16:26:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.000117 on epoch=43
06/02/2022 16:26:26 - INFO - __main__ - Global step 350 Train loss 0.000223 Classification-F1 0.9530791788856304 on epoch=43
06/02/2022 16:26:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.000058 on epoch=44
06/02/2022 16:26:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.061725 on epoch=46
06/02/2022 16:26:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.038350 on epoch=47
06/02/2022 16:26:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.006112 on epoch=48
06/02/2022 16:26:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.002491 on epoch=49
06/02/2022 16:26:54 - INFO - __main__ - Global step 400 Train loss 0.021747 Classification-F1 0.9374847374847375 on epoch=49
06/02/2022 16:26:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.000845 on epoch=51
06/02/2022 16:27:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.000411 on epoch=52
06/02/2022 16:27:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000358 on epoch=53
06/02/2022 16:27:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.000602 on epoch=54
06/02/2022 16:27:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000363 on epoch=56
06/02/2022 16:27:22 - INFO - __main__ - Global step 450 Train loss 0.000516 Classification-F1 0.9375 on epoch=56
06/02/2022 16:27:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.000145 on epoch=57
06/02/2022 16:27:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000120 on epoch=58
06/02/2022 16:27:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.004183 on epoch=59
06/02/2022 16:27:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000063 on epoch=61
06/02/2022 16:27:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000423 on epoch=62
06/02/2022 16:27:50 - INFO - __main__ - Global step 500 Train loss 0.000987 Classification-F1 0.9452289259734703 on epoch=62
06/02/2022 16:27:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.000099 on epoch=63
06/02/2022 16:28:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000048 on epoch=64
06/02/2022 16:28:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000101 on epoch=66
06/02/2022 16:28:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000211 on epoch=67
06/02/2022 16:28:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000033 on epoch=68
06/02/2022 16:28:17 - INFO - __main__ - Global step 550 Train loss 0.000098 Classification-F1 0.9452289259734703 on epoch=68
06/02/2022 16:28:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000053 on epoch=69
06/02/2022 16:28:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000015 on epoch=71
06/02/2022 16:28:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000146 on epoch=72
06/02/2022 16:28:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000026 on epoch=73
06/02/2022 16:28:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000022 on epoch=74
06/02/2022 16:28:45 - INFO - __main__ - Global step 600 Train loss 0.000052 Classification-F1 0.9452824427480916 on epoch=74
06/02/2022 16:28:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000031 on epoch=76
06/02/2022 16:28:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000204 on epoch=77
06/02/2022 16:29:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000010 on epoch=78
06/02/2022 16:29:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000018 on epoch=79
06/02/2022 16:29:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000028 on epoch=81
06/02/2022 16:29:14 - INFO - __main__ - Global step 650 Train loss 0.000058 Classification-F1 0.9530217763640813 on epoch=81
06/02/2022 16:29:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000014 on epoch=82
06/02/2022 16:29:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000010 on epoch=83
06/02/2022 16:29:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000008 on epoch=84
06/02/2022 16:29:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000009 on epoch=86
06/02/2022 16:29:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000042 on epoch=87
06/02/2022 16:29:42 - INFO - __main__ - Global step 700 Train loss 0.000017 Classification-F1 0.9452289259734703 on epoch=87
06/02/2022 16:29:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000010 on epoch=88
06/02/2022 16:29:52 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000014 on epoch=89
06/02/2022 16:29:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000009 on epoch=91
06/02/2022 16:30:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.118528 on epoch=92
06/02/2022 16:30:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000023 on epoch=93
06/02/2022 16:30:09 - INFO - __main__ - Global step 750 Train loss 0.023717 Classification-F1 0.9452289259734703 on epoch=93
06/02/2022 16:30:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000021 on epoch=94
06/02/2022 16:30:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.033369 on epoch=96
06/02/2022 16:30:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000101 on epoch=97
06/02/2022 16:30:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000034 on epoch=98
06/02/2022 16:30:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.001158 on epoch=99
06/02/2022 16:30:37 - INFO - __main__ - Global step 800 Train loss 0.006937 Classification-F1 0.9687194525904204 on epoch=99
06/02/2022 16:30:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000019 on epoch=101
06/02/2022 16:30:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000078 on epoch=102
06/02/2022 16:30:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000101 on epoch=103
06/02/2022 16:30:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000034 on epoch=104
06/02/2022 16:31:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000013 on epoch=106
06/02/2022 16:31:05 - INFO - __main__ - Global step 850 Train loss 0.000049 Classification-F1 0.9687194525904204 on epoch=106
06/02/2022 16:31:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000017 on epoch=107
06/02/2022 16:31:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000008 on epoch=108
06/02/2022 16:31:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.011367 on epoch=109
06/02/2022 16:31:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.074076 on epoch=111
06/02/2022 16:31:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001079 on epoch=112
06/02/2022 16:31:33 - INFO - __main__ - Global step 900 Train loss 0.017309 Classification-F1 0.9217986314760509 on epoch=112
06/02/2022 16:31:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000094 on epoch=113
06/02/2022 16:31:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000079 on epoch=114
06/02/2022 16:31:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000029 on epoch=116
06/02/2022 16:31:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000030 on epoch=117
06/02/2022 16:31:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000027 on epoch=118
06/02/2022 16:32:01 - INFO - __main__ - Global step 950 Train loss 0.000052 Classification-F1 0.9217986314760509 on epoch=118
06/02/2022 16:32:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000061 on epoch=119
06/02/2022 16:32:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000031 on epoch=121
06/02/2022 16:32:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000031 on epoch=122
06/02/2022 16:32:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000168 on epoch=123
06/02/2022 16:32:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000027 on epoch=124
06/02/2022 16:32:29 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:32:29 - INFO - __main__ - Printing 3 examples
06/02/2022 16:32:29 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
06/02/2022 16:32:29 - INFO - __main__ - ['negative']
06/02/2022 16:32:29 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
06/02/2022 16:32:29 - INFO - __main__ - ['negative']
06/02/2022 16:32:29 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
06/02/2022 16:32:29 - INFO - __main__ - ['negative']
06/02/2022 16:32:29 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:32:29 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:32:29 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 16:32:29 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:32:29 - INFO - __main__ - Printing 3 examples
06/02/2022 16:32:29 - INFO - __main__ -  [amazon_polarity] title: Amazon rip off [SEP] content: Don't buy this product from Amazon. I bought this product because I thought I was going to receive ten blank vhs tapes instead I received one. The picture that they have is very deceptive. It clearly showed 10.
06/02/2022 16:32:29 - INFO - __main__ - ['negative']
06/02/2022 16:32:29 - INFO - __main__ -  [amazon_polarity] title: Full of inaccuracies. [SEP] content: There are some interesting technical details in this book, which might make it worth reading for a Titanic enthusiast.There are numerous contradictions in the book to known established facts. Stokers assaulting people with shovels for a place in the boats? Attempted stabbing of the radio operator for a life belt? Male passengers being shot? The bridge phone not being answered for minutes? Collapsible boats disintegrating immediately?I'm a good portion through this book, and I'm doubtful it's worth more of my time. It's an old book written in a different time, the price is right, but this is fiction.
06/02/2022 16:32:29 - INFO - __main__ - ['negative']
06/02/2022 16:32:29 - INFO - __main__ -  [amazon_polarity] title: A real stupid book [SEP] content: One of the most boring and stupid books I`ve read in a long time. Full of ignorant remarks. From Mr Clancys observation that asian girls like westerners because they are better "equipped" than the asian males, via people saying Comrade Doctor etc to each other in Russia, ten years after the Soviet Unions fall, to the "fact" that american girls are slimmer and healthier than the rest of the world(when we know that they are sicker due to overweight than the rest of the world). Well, a writer can write about his white supremacy but when he writes as boring as Tom Clancy does in this book it gets completely uninteresting. Just pure typewriting and after 350 pages I had to give up.  Yes, and I spell terrible...
06/02/2022 16:32:29 - INFO - __main__ - ['negative']
06/02/2022 16:32:29 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:32:29 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:32:29 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 16:32:29 - INFO - __main__ - Global step 1000 Train loss 0.000064 Classification-F1 0.9217986314760509 on epoch=124
06/02/2022 16:32:29 - INFO - __main__ - save last model!
06/02/2022 16:32:36 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 16:32:37 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 16:32:37 - INFO - __main__ - Printing 3 examples
06/02/2022 16:32:37 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 16:32:37 - INFO - __main__ - ['negative']
06/02/2022 16:32:37 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 16:32:37 - INFO - __main__ - ['negative']
06/02/2022 16:32:37 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 16:32:37 - INFO - __main__ - ['negative']
06/02/2022 16:32:37 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:32:38 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:32:38 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 16:32:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 16:32:40 - INFO - __main__ - Starting training!
06/02/2022 16:32:53 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_42_0.0005_8_predictions.txt
06/02/2022 16:32:53 - INFO - __main__ - Classification-F1 on test data: 0.9260
06/02/2022 16:32:54 - INFO - __main__ - prefix=amazon_polarity_64_42, lr=0.0005, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9259810511490942
06/02/2022 16:32:54 - INFO - __main__ - Running ... prefix=amazon_polarity_64_42, lr=0.0003, bsz=8 ...
06/02/2022 16:32:55 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:32:55 - INFO - __main__ - Printing 3 examples
06/02/2022 16:32:55 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
06/02/2022 16:32:55 - INFO - __main__ - ['negative']
06/02/2022 16:32:55 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
06/02/2022 16:32:55 - INFO - __main__ - ['negative']
06/02/2022 16:32:55 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
06/02/2022 16:32:55 - INFO - __main__ - ['negative']
06/02/2022 16:32:55 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:32:55 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:32:55 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 16:32:55 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:32:55 - INFO - __main__ - Printing 3 examples
06/02/2022 16:32:55 - INFO - __main__ -  [amazon_polarity] title: Amazon rip off [SEP] content: Don't buy this product from Amazon. I bought this product because I thought I was going to receive ten blank vhs tapes instead I received one. The picture that they have is very deceptive. It clearly showed 10.
06/02/2022 16:32:55 - INFO - __main__ - ['negative']
06/02/2022 16:32:55 - INFO - __main__ -  [amazon_polarity] title: Full of inaccuracies. [SEP] content: There are some interesting technical details in this book, which might make it worth reading for a Titanic enthusiast.There are numerous contradictions in the book to known established facts. Stokers assaulting people with shovels for a place in the boats? Attempted stabbing of the radio operator for a life belt? Male passengers being shot? The bridge phone not being answered for minutes? Collapsible boats disintegrating immediately?I'm a good portion through this book, and I'm doubtful it's worth more of my time. It's an old book written in a different time, the price is right, but this is fiction.
06/02/2022 16:32:55 - INFO - __main__ - ['negative']
06/02/2022 16:32:55 - INFO - __main__ -  [amazon_polarity] title: A real stupid book [SEP] content: One of the most boring and stupid books I`ve read in a long time. Full of ignorant remarks. From Mr Clancys observation that asian girls like westerners because they are better "equipped" than the asian males, via people saying Comrade Doctor etc to each other in Russia, ten years after the Soviet Unions fall, to the "fact" that american girls are slimmer and healthier than the rest of the world(when we know that they are sicker due to overweight than the rest of the world). Well, a writer can write about his white supremacy but when he writes as boring as Tom Clancy does in this book it gets completely uninteresting. Just pure typewriting and after 350 pages I had to give up.  Yes, and I spell terrible...
06/02/2022 16:32:55 - INFO - __main__ - ['negative']
06/02/2022 16:32:55 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:32:55 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:32:55 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 16:33:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 16:33:06 - INFO - __main__ - Starting training!
06/02/2022 16:33:10 - INFO - __main__ - Step 10 Global step 10 Train loss 22.940363 on epoch=1
06/02/2022 16:33:15 - INFO - __main__ - Step 20 Global step 20 Train loss 17.522091 on epoch=2
06/02/2022 16:33:20 - INFO - __main__ - Step 30 Global step 30 Train loss 16.141972 on epoch=3
06/02/2022 16:33:25 - INFO - __main__ - Step 40 Global step 40 Train loss 14.852404 on epoch=4
06/02/2022 16:33:30 - INFO - __main__ - Step 50 Global step 50 Train loss 14.651072 on epoch=6
06/02/2022 16:34:04 - INFO - __main__ - Global step 50 Train loss 17.221581 Classification-F1 0.0 on epoch=6
06/02/2022 16:34:10 - INFO - __main__ - Step 60 Global step 60 Train loss 12.758271 on epoch=7
06/02/2022 16:34:15 - INFO - __main__ - Step 70 Global step 70 Train loss 11.689194 on epoch=8
06/02/2022 16:34:20 - INFO - __main__ - Step 80 Global step 80 Train loss 9.313845 on epoch=9
06/02/2022 16:34:25 - INFO - __main__ - Step 90 Global step 90 Train loss 5.235885 on epoch=11
06/02/2022 16:34:30 - INFO - __main__ - Step 100 Global step 100 Train loss 2.490721 on epoch=12
06/02/2022 16:34:32 - INFO - __main__ - Global step 100 Train loss 8.297583 Classification-F1 0.21304347826086956 on epoch=12
06/02/2022 16:34:38 - INFO - __main__ - Step 110 Global step 110 Train loss 1.609124 on epoch=13
06/02/2022 16:34:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.488455 on epoch=14
06/02/2022 16:34:48 - INFO - __main__ - Step 130 Global step 130 Train loss 0.429830 on epoch=16
06/02/2022 16:34:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.460388 on epoch=17
06/02/2022 16:34:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.404946 on epoch=18
06/02/2022 16:35:00 - INFO - __main__ - Global step 150 Train loss 0.678549 Classification-F1 0.3834004580273237 on epoch=18
06/02/2022 16:35:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.390597 on epoch=19
06/02/2022 16:35:11 - INFO - __main__ - Step 170 Global step 170 Train loss 0.318298 on epoch=21
06/02/2022 16:35:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.293703 on epoch=22
06/02/2022 16:35:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.370631 on epoch=23
06/02/2022 16:35:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.349765 on epoch=24
06/02/2022 16:35:28 - INFO - __main__ - Global step 200 Train loss 0.344599 Classification-F1 0.49090909090909085 on epoch=24
06/02/2022 16:35:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.368247 on epoch=26
06/02/2022 16:35:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.306102 on epoch=27
06/02/2022 16:35:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.322206 on epoch=28
06/02/2022 16:35:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.334654 on epoch=29
06/02/2022 16:35:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.228556 on epoch=31
06/02/2022 16:35:56 - INFO - __main__ - Global step 250 Train loss 0.311953 Classification-F1 0.864802733768251 on epoch=31
06/02/2022 16:36:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.115548 on epoch=32
06/02/2022 16:36:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.126511 on epoch=33
06/02/2022 16:36:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.086879 on epoch=34
06/02/2022 16:36:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.061856 on epoch=36
06/02/2022 16:36:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.065009 on epoch=37
06/02/2022 16:36:25 - INFO - __main__ - Global step 300 Train loss 0.091161 Classification-F1 0.9843711843711844 on epoch=37
06/02/2022 16:36:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.047590 on epoch=38
06/02/2022 16:36:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.026256 on epoch=39
06/02/2022 16:36:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.044948 on epoch=41
06/02/2022 16:36:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.013668 on epoch=42
06/02/2022 16:36:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.022360 on epoch=43
06/02/2022 16:36:53 - INFO - __main__ - Global step 350 Train loss 0.030965 Classification-F1 0.9687423687423687 on epoch=43
06/02/2022 16:36:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.017452 on epoch=44
06/02/2022 16:37:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.004412 on epoch=46
06/02/2022 16:37:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.025889 on epoch=47
06/02/2022 16:37:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.005251 on epoch=48
06/02/2022 16:37:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.002161 on epoch=49
06/02/2022 16:37:20 - INFO - __main__ - Global step 400 Train loss 0.011033 Classification-F1 0.9765496183206106 on epoch=49
06/02/2022 16:37:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.003444 on epoch=51
06/02/2022 16:37:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001283 on epoch=52
06/02/2022 16:37:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000961 on epoch=53
06/02/2022 16:37:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.023427 on epoch=54
06/02/2022 16:37:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.011434 on epoch=56
06/02/2022 16:37:48 - INFO - __main__ - Global step 450 Train loss 0.008110 Classification-F1 0.9687423687423687 on epoch=56
06/02/2022 16:37:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.015630 on epoch=57
06/02/2022 16:37:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001225 on epoch=58
06/02/2022 16:38:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.001215 on epoch=59
06/02/2022 16:38:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.001628 on epoch=61
06/02/2022 16:38:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.003491 on epoch=62
06/02/2022 16:38:15 - INFO - __main__ - Global step 500 Train loss 0.004638 Classification-F1 0.9372549019607843 on epoch=62
06/02/2022 16:38:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.010031 on epoch=63
06/02/2022 16:38:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.022093 on epoch=64
06/02/2022 16:38:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.002288 on epoch=66
06/02/2022 16:38:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.002719 on epoch=67
06/02/2022 16:38:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001929 on epoch=68
06/02/2022 16:38:42 - INFO - __main__ - Global step 550 Train loss 0.007812 Classification-F1 0.9843711843711844 on epoch=68
06/02/2022 16:38:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000310 on epoch=69
06/02/2022 16:38:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000554 on epoch=71
06/02/2022 16:38:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001416 on epoch=72
06/02/2022 16:39:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000836 on epoch=73
06/02/2022 16:39:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000210 on epoch=74
06/02/2022 16:39:09 - INFO - __main__ - Global step 600 Train loss 0.000665 Classification-F1 0.9843711843711844 on epoch=74
06/02/2022 16:39:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.002015 on epoch=76
06/02/2022 16:39:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.009781 on epoch=77
06/02/2022 16:39:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.002632 on epoch=78
06/02/2022 16:39:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000693 on epoch=79
06/02/2022 16:39:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.011571 on epoch=81
06/02/2022 16:39:37 - INFO - __main__ - Global step 650 Train loss 0.005338 Classification-F1 0.9374389051808407 on epoch=81
06/02/2022 16:39:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.004429 on epoch=82
06/02/2022 16:39:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000259 on epoch=83
06/02/2022 16:39:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000245 on epoch=84
06/02/2022 16:39:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000272 on epoch=86
06/02/2022 16:40:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000084 on epoch=87
06/02/2022 16:40:04 - INFO - __main__ - Global step 700 Train loss 0.001058 Classification-F1 0.9374847374847375 on epoch=87
06/02/2022 16:40:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000085 on epoch=88
06/02/2022 16:40:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.002478 on epoch=89
06/02/2022 16:40:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.002176 on epoch=91
06/02/2022 16:40:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000161 on epoch=92
06/02/2022 16:40:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001093 on epoch=93
06/02/2022 16:40:31 - INFO - __main__ - Global step 750 Train loss 0.001199 Classification-F1 0.953125 on epoch=93
06/02/2022 16:40:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000032 on epoch=94
06/02/2022 16:40:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000048 on epoch=96
06/02/2022 16:40:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000034 on epoch=97
06/02/2022 16:40:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000095 on epoch=98
06/02/2022 16:40:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000626 on epoch=99
06/02/2022 16:40:59 - INFO - __main__ - Global step 800 Train loss 0.000167 Classification-F1 0.9531135531135531 on epoch=99
06/02/2022 16:41:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000022 on epoch=101
06/02/2022 16:41:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000340 on epoch=102
06/02/2022 16:41:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.098629 on epoch=103
06/02/2022 16:41:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000431 on epoch=104
06/02/2022 16:41:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000375 on epoch=106
06/02/2022 16:41:26 - INFO - __main__ - Global step 850 Train loss 0.019959 Classification-F1 0.960935115668681 on epoch=106
06/02/2022 16:41:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.001201 on epoch=107
06/02/2022 16:41:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000862 on epoch=108
06/02/2022 16:41:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000088 on epoch=109
06/02/2022 16:41:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000139 on epoch=111
06/02/2022 16:41:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000132 on epoch=112
06/02/2022 16:41:53 - INFO - __main__ - Global step 900 Train loss 0.000484 Classification-F1 0.9609160305343512 on epoch=112
06/02/2022 16:41:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000058 on epoch=113
06/02/2022 16:42:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000075 on epoch=114
06/02/2022 16:42:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.065612 on epoch=116
06/02/2022 16:42:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000041 on epoch=117
06/02/2022 16:42:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.010286 on epoch=118
06/02/2022 16:42:21 - INFO - __main__ - Global step 950 Train loss 0.015215 Classification-F1 0.9530791788856304 on epoch=118
06/02/2022 16:42:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.003324 on epoch=119
06/02/2022 16:42:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000039 on epoch=121
06/02/2022 16:42:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000049 on epoch=122
06/02/2022 16:42:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000026 on epoch=123
06/02/2022 16:42:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000035 on epoch=124
06/02/2022 16:42:48 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:42:48 - INFO - __main__ - Printing 3 examples
06/02/2022 16:42:48 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
06/02/2022 16:42:48 - INFO - __main__ - ['negative']
06/02/2022 16:42:48 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
06/02/2022 16:42:48 - INFO - __main__ - ['negative']
06/02/2022 16:42:48 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
06/02/2022 16:42:48 - INFO - __main__ - ['negative']
06/02/2022 16:42:48 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:42:48 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:42:48 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 16:42:48 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:42:48 - INFO - __main__ - Printing 3 examples
06/02/2022 16:42:48 - INFO - __main__ -  [amazon_polarity] title: Amazon rip off [SEP] content: Don't buy this product from Amazon. I bought this product because I thought I was going to receive ten blank vhs tapes instead I received one. The picture that they have is very deceptive. It clearly showed 10.
06/02/2022 16:42:48 - INFO - __main__ - ['negative']
06/02/2022 16:42:48 - INFO - __main__ -  [amazon_polarity] title: Full of inaccuracies. [SEP] content: There are some interesting technical details in this book, which might make it worth reading for a Titanic enthusiast.There are numerous contradictions in the book to known established facts. Stokers assaulting people with shovels for a place in the boats? Attempted stabbing of the radio operator for a life belt? Male passengers being shot? The bridge phone not being answered for minutes? Collapsible boats disintegrating immediately?I'm a good portion through this book, and I'm doubtful it's worth more of my time. It's an old book written in a different time, the price is right, but this is fiction.
06/02/2022 16:42:48 - INFO - __main__ - ['negative']
06/02/2022 16:42:48 - INFO - __main__ -  [amazon_polarity] title: A real stupid book [SEP] content: One of the most boring and stupid books I`ve read in a long time. Full of ignorant remarks. From Mr Clancys observation that asian girls like westerners because they are better "equipped" than the asian males, via people saying Comrade Doctor etc to each other in Russia, ten years after the Soviet Unions fall, to the "fact" that american girls are slimmer and healthier than the rest of the world(when we know that they are sicker due to overweight than the rest of the world). Well, a writer can write about his white supremacy but when he writes as boring as Tom Clancy does in this book it gets completely uninteresting. Just pure typewriting and after 350 pages I had to give up.  Yes, and I spell terrible...
06/02/2022 16:42:48 - INFO - __main__ - ['negative']
06/02/2022 16:42:48 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:42:48 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:42:48 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 16:42:48 - INFO - __main__ - Global step 1000 Train loss 0.000695 Classification-F1 0.9608778042667645 on epoch=124
06/02/2022 16:42:48 - INFO - __main__ - save last model!
06/02/2022 16:42:56 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 16:42:56 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 16:42:56 - INFO - __main__ - Printing 3 examples
06/02/2022 16:42:56 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 16:42:56 - INFO - __main__ - ['negative']
06/02/2022 16:42:56 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 16:42:56 - INFO - __main__ - ['negative']
06/02/2022 16:42:56 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 16:42:56 - INFO - __main__ - ['negative']
06/02/2022 16:42:56 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:42:57 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:42:58 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 16:43:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 16:43:01 - INFO - __main__ - Starting training!
06/02/2022 16:43:13 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_42_0.0003_8_predictions.txt
06/02/2022 16:43:13 - INFO - __main__ - Classification-F1 on test data: 0.9500
06/02/2022 16:43:13 - INFO - __main__ - prefix=amazon_polarity_64_42, lr=0.0003, bsz=8, dev_performance=0.9843711843711844, test_performance=0.9499711834016393
06/02/2022 16:43:13 - INFO - __main__ - Running ... prefix=amazon_polarity_64_42, lr=0.0002, bsz=8 ...
06/02/2022 16:43:14 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:43:14 - INFO - __main__ - Printing 3 examples
06/02/2022 16:43:14 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
06/02/2022 16:43:14 - INFO - __main__ - ['negative']
06/02/2022 16:43:14 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
06/02/2022 16:43:14 - INFO - __main__ - ['negative']
06/02/2022 16:43:14 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
06/02/2022 16:43:14 - INFO - __main__ - ['negative']
06/02/2022 16:43:14 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:43:14 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:43:14 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 16:43:14 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:43:14 - INFO - __main__ - Printing 3 examples
06/02/2022 16:43:14 - INFO - __main__ -  [amazon_polarity] title: Amazon rip off [SEP] content: Don't buy this product from Amazon. I bought this product because I thought I was going to receive ten blank vhs tapes instead I received one. The picture that they have is very deceptive. It clearly showed 10.
06/02/2022 16:43:14 - INFO - __main__ - ['negative']
06/02/2022 16:43:14 - INFO - __main__ -  [amazon_polarity] title: Full of inaccuracies. [SEP] content: There are some interesting technical details in this book, which might make it worth reading for a Titanic enthusiast.There are numerous contradictions in the book to known established facts. Stokers assaulting people with shovels for a place in the boats? Attempted stabbing of the radio operator for a life belt? Male passengers being shot? The bridge phone not being answered for minutes? Collapsible boats disintegrating immediately?I'm a good portion through this book, and I'm doubtful it's worth more of my time. It's an old book written in a different time, the price is right, but this is fiction.
06/02/2022 16:43:14 - INFO - __main__ - ['negative']
06/02/2022 16:43:14 - INFO - __main__ -  [amazon_polarity] title: A real stupid book [SEP] content: One of the most boring and stupid books I`ve read in a long time. Full of ignorant remarks. From Mr Clancys observation that asian girls like westerners because they are better "equipped" than the asian males, via people saying Comrade Doctor etc to each other in Russia, ten years after the Soviet Unions fall, to the "fact" that american girls are slimmer and healthier than the rest of the world(when we know that they are sicker due to overweight than the rest of the world). Well, a writer can write about his white supremacy but when he writes as boring as Tom Clancy does in this book it gets completely uninteresting. Just pure typewriting and after 350 pages I had to give up.  Yes, and I spell terrible...
06/02/2022 16:43:14 - INFO - __main__ - ['negative']
06/02/2022 16:43:14 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:43:14 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:43:14 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 16:43:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 16:43:25 - INFO - __main__ - Starting training!
06/02/2022 16:43:30 - INFO - __main__ - Step 10 Global step 10 Train loss 24.139109 on epoch=1
06/02/2022 16:43:35 - INFO - __main__ - Step 20 Global step 20 Train loss 18.690123 on epoch=2
06/02/2022 16:43:39 - INFO - __main__ - Step 30 Global step 30 Train loss 17.001324 on epoch=3
06/02/2022 16:43:44 - INFO - __main__ - Step 40 Global step 40 Train loss 16.555828 on epoch=4
06/02/2022 16:43:50 - INFO - __main__ - Step 50 Global step 50 Train loss 15.941920 on epoch=6
06/02/2022 16:44:13 - INFO - __main__ - Global step 50 Train loss 18.465662 Classification-F1 0.0 on epoch=6
06/02/2022 16:44:19 - INFO - __main__ - Step 60 Global step 60 Train loss 14.357570 on epoch=7
06/02/2022 16:44:25 - INFO - __main__ - Step 70 Global step 70 Train loss 14.031194 on epoch=8
06/02/2022 16:44:30 - INFO - __main__ - Step 80 Global step 80 Train loss 13.502238 on epoch=9
06/02/2022 16:44:35 - INFO - __main__ - Step 90 Global step 90 Train loss 12.171373 on epoch=11
06/02/2022 16:44:40 - INFO - __main__ - Step 100 Global step 100 Train loss 12.113829 on epoch=12
06/02/2022 16:45:08 - INFO - __main__ - Global step 100 Train loss 13.235241 Classification-F1 0.004879275653923541 on epoch=12
06/02/2022 16:45:15 - INFO - __main__ - Step 110 Global step 110 Train loss 11.042870 on epoch=13
06/02/2022 16:45:20 - INFO - __main__ - Step 120 Global step 120 Train loss 9.297301 on epoch=14
06/02/2022 16:45:25 - INFO - __main__ - Step 130 Global step 130 Train loss 5.417120 on epoch=16
06/02/2022 16:45:30 - INFO - __main__ - Step 140 Global step 140 Train loss 2.451925 on epoch=17
06/02/2022 16:45:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.743175 on epoch=18
06/02/2022 16:45:37 - INFO - __main__ - Global step 150 Train loss 5.790478 Classification-F1 0.10283060592658118 on epoch=18
06/02/2022 16:45:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.423454 on epoch=19
06/02/2022 16:45:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.554699 on epoch=21
06/02/2022 16:45:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.454477 on epoch=22
06/02/2022 16:45:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.420784 on epoch=23
06/02/2022 16:46:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.406019 on epoch=24
06/02/2022 16:46:06 - INFO - __main__ - Global step 200 Train loss 0.451887 Classification-F1 0.8041016222834405 on epoch=24
06/02/2022 16:46:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.390056 on epoch=26
06/02/2022 16:46:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.334761 on epoch=27
06/02/2022 16:46:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.349763 on epoch=28
06/02/2022 16:46:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.225178 on epoch=29
06/02/2022 16:46:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.199048 on epoch=31
06/02/2022 16:46:35 - INFO - __main__ - Global step 250 Train loss 0.299761 Classification-F1 0.9374847374847375 on epoch=31
06/02/2022 16:46:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.193198 on epoch=32
06/02/2022 16:46:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.171089 on epoch=33
06/02/2022 16:46:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.144276 on epoch=34
06/02/2022 16:46:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.106419 on epoch=36
06/02/2022 16:47:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.069421 on epoch=37
06/02/2022 16:47:03 - INFO - __main__ - Global step 300 Train loss 0.136881 Classification-F1 0.9608778042667645 on epoch=37
06/02/2022 16:47:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.094993 on epoch=38
06/02/2022 16:47:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.033883 on epoch=39
06/02/2022 16:47:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.047686 on epoch=41
06/02/2022 16:47:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.027097 on epoch=42
06/02/2022 16:47:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.025288 on epoch=43
06/02/2022 16:47:32 - INFO - __main__ - Global step 350 Train loss 0.045789 Classification-F1 0.9530217763640813 on epoch=43
06/02/2022 16:47:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.042262 on epoch=44
06/02/2022 16:47:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.012688 on epoch=46
06/02/2022 16:47:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.044311 on epoch=47
06/02/2022 16:47:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.010232 on epoch=48
06/02/2022 16:47:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.008003 on epoch=49
06/02/2022 16:48:00 - INFO - __main__ - Global step 400 Train loss 0.023499 Classification-F1 0.9530791788856304 on epoch=49
06/02/2022 16:48:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.047473 on epoch=51
06/02/2022 16:48:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.042056 on epoch=52
06/02/2022 16:48:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.039790 on epoch=53
06/02/2022 16:48:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.020339 on epoch=54
06/02/2022 16:48:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.037072 on epoch=56
06/02/2022 16:48:28 - INFO - __main__ - Global step 450 Train loss 0.037346 Classification-F1 0.9451484542393633 on epoch=56
06/02/2022 16:48:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.020191 on epoch=57
06/02/2022 16:48:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.054072 on epoch=58
06/02/2022 16:48:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.184052 on epoch=59
06/02/2022 16:48:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.318266 on epoch=61
06/02/2022 16:48:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.258117 on epoch=62
06/02/2022 16:48:56 - INFO - __main__ - Global step 500 Train loss 0.166939 Classification-F1 0.711767355079457 on epoch=62
06/02/2022 16:49:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.257951 on epoch=63
06/02/2022 16:49:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.188689 on epoch=64
06/02/2022 16:49:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.166766 on epoch=66
06/02/2022 16:49:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.115535 on epoch=67
06/02/2022 16:49:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.093856 on epoch=68
06/02/2022 16:49:24 - INFO - __main__ - Global step 550 Train loss 0.164559 Classification-F1 0.9530217763640813 on epoch=68
06/02/2022 16:49:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.082234 on epoch=69
06/02/2022 16:49:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.167796 on epoch=71
06/02/2022 16:49:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.129698 on epoch=72
06/02/2022 16:49:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.154520 on epoch=73
06/02/2022 16:49:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.207495 on epoch=74
06/02/2022 16:49:52 - INFO - __main__ - Global step 600 Train loss 0.148349 Classification-F1 0.8437118437118436 on epoch=74
06/02/2022 16:49:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.217093 on epoch=76
06/02/2022 16:50:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.302826 on epoch=77
06/02/2022 16:50:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.194496 on epoch=78
06/02/2022 16:50:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.196650 on epoch=79
06/02/2022 16:50:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.146119 on epoch=81
06/02/2022 16:50:20 - INFO - __main__ - Global step 650 Train loss 0.211437 Classification-F1 0.9687194525904204 on epoch=81
06/02/2022 16:50:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.092541 on epoch=82
06/02/2022 16:50:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.128827 on epoch=83
06/02/2022 16:50:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.081814 on epoch=84
06/02/2022 16:50:42 - INFO - __main__ - Step 690 Global step 690 Train loss 0.051435 on epoch=86
06/02/2022 16:50:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.037925 on epoch=87
06/02/2022 16:50:49 - INFO - __main__ - Global step 700 Train loss 0.078508 Classification-F1 0.9687423687423687 on epoch=87
06/02/2022 16:50:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.074037 on epoch=88
06/02/2022 16:51:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.031495 on epoch=89
06/02/2022 16:51:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.049979 on epoch=91
06/02/2022 16:51:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.022125 on epoch=92
06/02/2022 16:51:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.025045 on epoch=93
06/02/2022 16:51:18 - INFO - __main__ - Global step 750 Train loss 0.040536 Classification-F1 0.9453091619361533 on epoch=93
06/02/2022 16:51:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.015127 on epoch=94
06/02/2022 16:51:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.013887 on epoch=96
06/02/2022 16:51:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.028807 on epoch=97
06/02/2022 16:51:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.007917 on epoch=98
06/02/2022 16:51:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.052062 on epoch=99
06/02/2022 16:51:46 - INFO - __main__ - Global step 800 Train loss 0.023560 Classification-F1 0.9765496183206106 on epoch=99
06/02/2022 16:51:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.003028 on epoch=101
06/02/2022 16:51:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.004151 on epoch=102
06/02/2022 16:52:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.027607 on epoch=103
06/02/2022 16:52:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.016993 on epoch=104
06/02/2022 16:52:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.012465 on epoch=106
06/02/2022 16:52:15 - INFO - __main__ - Global step 850 Train loss 0.012849 Classification-F1 0.9765496183206106 on epoch=106
06/02/2022 16:52:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.013458 on epoch=107
06/02/2022 16:52:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.006987 on epoch=108
06/02/2022 16:52:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.038801 on epoch=109
06/02/2022 16:52:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.002745 on epoch=111
06/02/2022 16:52:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001243 on epoch=112
06/02/2022 16:52:43 - INFO - __main__ - Global step 900 Train loss 0.012647 Classification-F1 0.9765496183206106 on epoch=112
06/02/2022 16:52:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000874 on epoch=113
06/02/2022 16:52:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.004593 on epoch=114
06/02/2022 16:52:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.008528 on epoch=116
06/02/2022 16:53:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.006211 on epoch=117
06/02/2022 16:53:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.003602 on epoch=118
06/02/2022 16:53:11 - INFO - __main__ - Global step 950 Train loss 0.004762 Classification-F1 0.9765496183206106 on epoch=118
06/02/2022 16:53:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.010371 on epoch=119
06/02/2022 16:53:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000443 on epoch=121
06/02/2022 16:53:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.001429 on epoch=122
06/02/2022 16:53:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000432 on epoch=123
06/02/2022 16:53:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000567 on epoch=124
06/02/2022 16:53:38 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:53:38 - INFO - __main__ - Printing 3 examples
06/02/2022 16:53:38 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
06/02/2022 16:53:38 - INFO - __main__ - ['negative']
06/02/2022 16:53:38 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
06/02/2022 16:53:38 - INFO - __main__ - ['negative']
06/02/2022 16:53:38 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
06/02/2022 16:53:38 - INFO - __main__ - ['negative']
06/02/2022 16:53:38 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:53:38 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:53:38 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 16:53:38 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:53:38 - INFO - __main__ - Printing 3 examples
06/02/2022 16:53:38 - INFO - __main__ -  [amazon_polarity] title: Amazon rip off [SEP] content: Don't buy this product from Amazon. I bought this product because I thought I was going to receive ten blank vhs tapes instead I received one. The picture that they have is very deceptive. It clearly showed 10.
06/02/2022 16:53:38 - INFO - __main__ - ['negative']
06/02/2022 16:53:38 - INFO - __main__ -  [amazon_polarity] title: Full of inaccuracies. [SEP] content: There are some interesting technical details in this book, which might make it worth reading for a Titanic enthusiast.There are numerous contradictions in the book to known established facts. Stokers assaulting people with shovels for a place in the boats? Attempted stabbing of the radio operator for a life belt? Male passengers being shot? The bridge phone not being answered for minutes? Collapsible boats disintegrating immediately?I'm a good portion through this book, and I'm doubtful it's worth more of my time. It's an old book written in a different time, the price is right, but this is fiction.
06/02/2022 16:53:38 - INFO - __main__ - ['negative']
06/02/2022 16:53:38 - INFO - __main__ -  [amazon_polarity] title: A real stupid book [SEP] content: One of the most boring and stupid books I`ve read in a long time. Full of ignorant remarks. From Mr Clancys observation that asian girls like westerners because they are better "equipped" than the asian males, via people saying Comrade Doctor etc to each other in Russia, ten years after the Soviet Unions fall, to the "fact" that american girls are slimmer and healthier than the rest of the world(when we know that they are sicker due to overweight than the rest of the world). Well, a writer can write about his white supremacy but when he writes as boring as Tom Clancy does in this book it gets completely uninteresting. Just pure typewriting and after 350 pages I had to give up.  Yes, and I spell terrible...
06/02/2022 16:53:38 - INFO - __main__ - ['negative']
06/02/2022 16:53:38 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:53:38 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:53:38 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 16:53:39 - INFO - __main__ - Global step 1000 Train loss 0.002648 Classification-F1 0.9765496183206106 on epoch=124
06/02/2022 16:53:39 - INFO - __main__ - save last model!
06/02/2022 16:53:46 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 16:53:47 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 16:53:47 - INFO - __main__ - Printing 3 examples
06/02/2022 16:53:47 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 16:53:47 - INFO - __main__ - ['negative']
06/02/2022 16:53:47 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 16:53:47 - INFO - __main__ - ['negative']
06/02/2022 16:53:47 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 16:53:47 - INFO - __main__ - ['negative']
06/02/2022 16:53:47 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:53:47 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:53:48 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 16:53:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 16:53:51 - INFO - __main__ - Starting training!
06/02/2022 16:54:03 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_42_0.0002_8_predictions.txt
06/02/2022 16:54:03 - INFO - __main__ - Classification-F1 on test data: 0.9490
06/02/2022 16:54:04 - INFO - __main__ - prefix=amazon_polarity_64_42, lr=0.0002, bsz=8, dev_performance=0.9765496183206106, test_performance=0.9489995409958689
06/02/2022 16:54:04 - INFO - __main__ - Running ... prefix=amazon_polarity_64_42, lr=0.0001, bsz=8 ...
06/02/2022 16:54:04 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:54:04 - INFO - __main__ - Printing 3 examples
06/02/2022 16:54:04 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
06/02/2022 16:54:04 - INFO - __main__ - ['negative']
06/02/2022 16:54:04 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
06/02/2022 16:54:04 - INFO - __main__ - ['negative']
06/02/2022 16:54:04 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
06/02/2022 16:54:04 - INFO - __main__ - ['negative']
06/02/2022 16:54:04 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:54:04 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:54:05 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 16:54:05 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 16:54:05 - INFO - __main__ - Printing 3 examples
06/02/2022 16:54:05 - INFO - __main__ -  [amazon_polarity] title: Amazon rip off [SEP] content: Don't buy this product from Amazon. I bought this product because I thought I was going to receive ten blank vhs tapes instead I received one. The picture that they have is very deceptive. It clearly showed 10.
06/02/2022 16:54:05 - INFO - __main__ - ['negative']
06/02/2022 16:54:05 - INFO - __main__ -  [amazon_polarity] title: Full of inaccuracies. [SEP] content: There are some interesting technical details in this book, which might make it worth reading for a Titanic enthusiast.There are numerous contradictions in the book to known established facts. Stokers assaulting people with shovels for a place in the boats? Attempted stabbing of the radio operator for a life belt? Male passengers being shot? The bridge phone not being answered for minutes? Collapsible boats disintegrating immediately?I'm a good portion through this book, and I'm doubtful it's worth more of my time. It's an old book written in a different time, the price is right, but this is fiction.
06/02/2022 16:54:05 - INFO - __main__ - ['negative']
06/02/2022 16:54:05 - INFO - __main__ -  [amazon_polarity] title: A real stupid book [SEP] content: One of the most boring and stupid books I`ve read in a long time. Full of ignorant remarks. From Mr Clancys observation that asian girls like westerners because they are better "equipped" than the asian males, via people saying Comrade Doctor etc to each other in Russia, ten years after the Soviet Unions fall, to the "fact" that american girls are slimmer and healthier than the rest of the world(when we know that they are sicker due to overweight than the rest of the world). Well, a writer can write about his white supremacy but when he writes as boring as Tom Clancy does in this book it gets completely uninteresting. Just pure typewriting and after 350 pages I had to give up.  Yes, and I spell terrible...
06/02/2022 16:54:05 - INFO - __main__ - ['negative']
06/02/2022 16:54:05 - INFO - __main__ - Tokenizing Input ...
06/02/2022 16:54:05 - INFO - __main__ - Tokenizing Output ...
06/02/2022 16:54:05 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 16:54:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 16:54:18 - INFO - __main__ - Starting training!
06/02/2022 16:54:22 - INFO - __main__ - Step 10 Global step 10 Train loss 23.401571 on epoch=1
06/02/2022 16:54:27 - INFO - __main__ - Step 20 Global step 20 Train loss 19.948065 on epoch=2
06/02/2022 16:54:33 - INFO - __main__ - Step 30 Global step 30 Train loss 18.005749 on epoch=3
06/02/2022 16:54:38 - INFO - __main__ - Step 40 Global step 40 Train loss 17.195667 on epoch=4
06/02/2022 16:54:43 - INFO - __main__ - Step 50 Global step 50 Train loss 16.968918 on epoch=6
06/02/2022 16:55:22 - INFO - __main__ - Global step 50 Train loss 19.103994 Classification-F1 0.0 on epoch=6
06/02/2022 16:55:27 - INFO - __main__ - Step 60 Global step 60 Train loss 16.292261 on epoch=7
06/02/2022 16:55:32 - INFO - __main__ - Step 70 Global step 70 Train loss 16.180515 on epoch=8
06/02/2022 16:55:37 - INFO - __main__ - Step 80 Global step 80 Train loss 15.232617 on epoch=9
06/02/2022 16:55:42 - INFO - __main__ - Step 90 Global step 90 Train loss 15.213880 on epoch=11
06/02/2022 16:55:48 - INFO - __main__ - Step 100 Global step 100 Train loss 14.952151 on epoch=12
06/02/2022 16:56:21 - INFO - __main__ - Global step 100 Train loss 15.574286 Classification-F1 0.0 on epoch=12
06/02/2022 16:56:27 - INFO - __main__ - Step 110 Global step 110 Train loss 14.380710 on epoch=13
06/02/2022 16:56:32 - INFO - __main__ - Step 120 Global step 120 Train loss 13.156802 on epoch=14
06/02/2022 16:56:37 - INFO - __main__ - Step 130 Global step 130 Train loss 13.382976 on epoch=16
06/02/2022 16:56:42 - INFO - __main__ - Step 140 Global step 140 Train loss 12.617708 on epoch=17
06/02/2022 16:56:47 - INFO - __main__ - Step 150 Global step 150 Train loss 12.719217 on epoch=18
06/02/2022 16:57:17 - INFO - __main__ - Global step 150 Train loss 13.251482 Classification-F1 0.0 on epoch=18
06/02/2022 16:57:22 - INFO - __main__ - Step 160 Global step 160 Train loss 12.708510 on epoch=19
06/02/2022 16:57:27 - INFO - __main__ - Step 170 Global step 170 Train loss 12.051373 on epoch=21
06/02/2022 16:57:33 - INFO - __main__ - Step 180 Global step 180 Train loss 11.877012 on epoch=22
06/02/2022 16:57:38 - INFO - __main__ - Step 190 Global step 190 Train loss 11.278028 on epoch=23
06/02/2022 16:57:43 - INFO - __main__ - Step 200 Global step 200 Train loss 10.049990 on epoch=24
06/02/2022 16:57:56 - INFO - __main__ - Global step 200 Train loss 11.592982 Classification-F1 0.02936220831205526 on epoch=24
06/02/2022 16:58:02 - INFO - __main__ - Step 210 Global step 210 Train loss 9.747373 on epoch=26
06/02/2022 16:58:07 - INFO - __main__ - Step 220 Global step 220 Train loss 7.698233 on epoch=27
06/02/2022 16:58:12 - INFO - __main__ - Step 230 Global step 230 Train loss 6.010598 on epoch=28
06/02/2022 16:58:17 - INFO - __main__ - Step 240 Global step 240 Train loss 5.610641 on epoch=29
06/02/2022 16:58:22 - INFO - __main__ - Step 250 Global step 250 Train loss 4.941638 on epoch=31
06/02/2022 16:58:28 - INFO - __main__ - Global step 250 Train loss 6.801697 Classification-F1 0.0884290475083314 on epoch=31
06/02/2022 16:58:33 - INFO - __main__ - Step 260 Global step 260 Train loss 4.331901 on epoch=32
06/02/2022 16:58:39 - INFO - __main__ - Step 270 Global step 270 Train loss 3.233325 on epoch=33
06/02/2022 16:58:44 - INFO - __main__ - Step 280 Global step 280 Train loss 2.998686 on epoch=34
06/02/2022 16:58:49 - INFO - __main__ - Step 290 Global step 290 Train loss 2.389772 on epoch=36
06/02/2022 16:58:54 - INFO - __main__ - Step 300 Global step 300 Train loss 2.278893 on epoch=37
06/02/2022 16:58:56 - INFO - __main__ - Global step 300 Train loss 3.046515 Classification-F1 0.5137254901960784 on epoch=37
06/02/2022 16:59:01 - INFO - __main__ - Step 310 Global step 310 Train loss 1.782923 on epoch=38
06/02/2022 16:59:07 - INFO - __main__ - Step 320 Global step 320 Train loss 1.551112 on epoch=39
06/02/2022 16:59:12 - INFO - __main__ - Step 330 Global step 330 Train loss 1.727712 on epoch=41
06/02/2022 16:59:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.753995 on epoch=42
06/02/2022 16:59:22 - INFO - __main__ - Step 350 Global step 350 Train loss 1.037614 on epoch=43
06/02/2022 16:59:24 - INFO - __main__ - Global step 350 Train loss 1.370671 Classification-F1 0.2626358695652174 on epoch=43
06/02/2022 16:59:29 - INFO - __main__ - Step 360 Global step 360 Train loss 1.105895 on epoch=44
06/02/2022 16:59:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.589237 on epoch=46
06/02/2022 16:59:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.497991 on epoch=47
06/02/2022 16:59:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.516346 on epoch=48
06/02/2022 16:59:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.547033 on epoch=49
06/02/2022 16:59:51 - INFO - __main__ - Global step 400 Train loss 0.651300 Classification-F1 0.4341290893015031 on epoch=49
06/02/2022 16:59:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.590726 on epoch=51
06/02/2022 17:00:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.364921 on epoch=52
06/02/2022 17:00:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.500795 on epoch=53
06/02/2022 17:00:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.460804 on epoch=54
06/02/2022 17:00:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.405067 on epoch=56
06/02/2022 17:00:28 - INFO - __main__ - Global step 450 Train loss 0.464463 Classification-F1 0.4055576703464027 on epoch=56
06/02/2022 17:00:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.477649 on epoch=57
06/02/2022 17:00:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.422657 on epoch=58
06/02/2022 17:00:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.420025 on epoch=59
06/02/2022 17:00:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.387668 on epoch=61
06/02/2022 17:00:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.476979 on epoch=62
06/02/2022 17:00:56 - INFO - __main__ - Global step 500 Train loss 0.436996 Classification-F1 0.4562295424472456 on epoch=62
06/02/2022 17:01:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.416254 on epoch=63
06/02/2022 17:01:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.434536 on epoch=64
06/02/2022 17:01:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.391652 on epoch=66
06/02/2022 17:01:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.373923 on epoch=67
06/02/2022 17:01:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.396392 on epoch=68
06/02/2022 17:01:24 - INFO - __main__ - Global step 550 Train loss 0.402551 Classification-F1 0.5307917888563051 on epoch=68
06/02/2022 17:01:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.525066 on epoch=69
06/02/2022 17:01:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.400059 on epoch=71
06/02/2022 17:01:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.391666 on epoch=72
06/02/2022 17:01:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.397238 on epoch=73
06/02/2022 17:01:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.360543 on epoch=74
06/02/2022 17:01:52 - INFO - __main__ - Global step 600 Train loss 0.414914 Classification-F1 0.5390343648904352 on epoch=74
06/02/2022 17:01:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.348153 on epoch=76
06/02/2022 17:02:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.357820 on epoch=77
06/02/2022 17:02:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.431014 on epoch=78
06/02/2022 17:02:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.400014 on epoch=79
06/02/2022 17:02:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.398584 on epoch=81
06/02/2022 17:02:20 - INFO - __main__ - Global step 650 Train loss 0.387117 Classification-F1 0.5388091603053435 on epoch=81
06/02/2022 17:02:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.403871 on epoch=82
06/02/2022 17:02:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.367552 on epoch=83
06/02/2022 17:02:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.338278 on epoch=84
06/02/2022 17:02:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.369518 on epoch=86
06/02/2022 17:02:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.415558 on epoch=87
06/02/2022 17:02:48 - INFO - __main__ - Global step 700 Train loss 0.378955 Classification-F1 0.5198917788845847 on epoch=87
06/02/2022 17:02:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.373018 on epoch=88
06/02/2022 17:02:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.368349 on epoch=89
06/02/2022 17:03:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.393996 on epoch=91
06/02/2022 17:03:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.381185 on epoch=92
06/02/2022 17:03:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.432429 on epoch=93
06/02/2022 17:03:15 - INFO - __main__ - Global step 750 Train loss 0.389795 Classification-F1 0.5500462534690101 on epoch=93
06/02/2022 17:03:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.392308 on epoch=94
06/02/2022 17:03:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.356830 on epoch=96
06/02/2022 17:03:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.354576 on epoch=97
06/02/2022 17:03:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.362959 on epoch=98
06/02/2022 17:03:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.359871 on epoch=99
06/02/2022 17:03:44 - INFO - __main__ - Global step 800 Train loss 0.365309 Classification-F1 0.5333333333333333 on epoch=99
06/02/2022 17:03:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.377628 on epoch=101
06/02/2022 17:03:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.392613 on epoch=102
06/02/2022 17:03:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.362274 on epoch=103
06/02/2022 17:04:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.386228 on epoch=104
06/02/2022 17:04:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.331466 on epoch=106
06/02/2022 17:04:11 - INFO - __main__ - Global step 850 Train loss 0.370042 Classification-F1 0.5210697417653193 on epoch=106
06/02/2022 17:04:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.389644 on epoch=107
06/02/2022 17:04:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.367549 on epoch=108
06/02/2022 17:04:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.327923 on epoch=109
06/02/2022 17:04:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.394627 on epoch=111
06/02/2022 17:04:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.329086 on epoch=112
06/02/2022 17:04:39 - INFO - __main__ - Global step 900 Train loss 0.361766 Classification-F1 0.6362737830491723 on epoch=112
06/02/2022 17:04:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.399096 on epoch=113
06/02/2022 17:04:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.376058 on epoch=114
06/02/2022 17:04:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.368729 on epoch=116
06/02/2022 17:05:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.342619 on epoch=117
06/02/2022 17:05:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.388204 on epoch=118
06/02/2022 17:05:07 - INFO - __main__ - Global step 950 Train loss 0.374941 Classification-F1 0.5398297067171239 on epoch=118
06/02/2022 17:05:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.396225 on epoch=119
06/02/2022 17:05:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.379470 on epoch=121
06/02/2022 17:05:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.370083 on epoch=122
06/02/2022 17:05:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.367863 on epoch=123
06/02/2022 17:05:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.363979 on epoch=124
06/02/2022 17:05:34 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 17:05:34 - INFO - __main__ - Printing 3 examples
06/02/2022 17:05:34 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
06/02/2022 17:05:34 - INFO - __main__ - ['negative']
06/02/2022 17:05:34 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
06/02/2022 17:05:34 - INFO - __main__ - ['negative']
06/02/2022 17:05:34 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
06/02/2022 17:05:34 - INFO - __main__ - ['negative']
06/02/2022 17:05:34 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:05:34 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:05:34 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 17:05:34 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 17:05:34 - INFO - __main__ - Printing 3 examples
06/02/2022 17:05:34 - INFO - __main__ -  [amazon_polarity] title: I was an extra in this movie [SEP] content: I got paid $35.00 for 12 hours work. Brad Davis pushed me in the critical student protest scene. I'm the one in the blue sweater.
06/02/2022 17:05:34 - INFO - __main__ - ['negative']
06/02/2022 17:05:34 - INFO - __main__ -  [amazon_polarity] title: A book for specialists [SEP] content: More than a book it is a collection of papers, not always easy to understand and written in academic language. It could be used as a reference by one who knows already the subject. Not recommended for beginners
06/02/2022 17:05:34 - INFO - __main__ - ['negative']
06/02/2022 17:05:34 - INFO - __main__ -  [amazon_polarity] title: Didn't fulfill its promise, but I'm glad I read it. [SEP] content: One of the other reviewers perhaps said it best: I really wanted to like this book. As a mid-life motorcyclist (Harley) and pilot, with a penchant for old airplanes and motorcycles, I have long harbored the dream of restoring something really classic; a Harley J, a '32 Ford, a Stearman biplane, etc. So I grabbed the book without a second thought, anticipating a real restoration chronicle. What I got was part soap opera and part Pirsig, with a too-large serving of jobbing out a restoration to various subcontractors, and not enough nuts and bolts. Wish it had been edited bt Tom and Ray Magliozzi (The Tappet Brothers).
06/02/2022 17:05:34 - INFO - __main__ - ['negative']
06/02/2022 17:05:34 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:05:34 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:05:35 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 17:05:35 - INFO - __main__ - Global step 1000 Train loss 0.375524 Classification-F1 0.6638778625954198 on epoch=124
06/02/2022 17:05:35 - INFO - __main__ - save last model!
06/02/2022 17:05:42 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 17:05:43 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 17:05:43 - INFO - __main__ - Printing 3 examples
06/02/2022 17:05:43 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 17:05:43 - INFO - __main__ - ['negative']
06/02/2022 17:05:43 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 17:05:43 - INFO - __main__ - ['negative']
06/02/2022 17:05:43 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 17:05:43 - INFO - __main__ - ['negative']
06/02/2022 17:05:43 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:05:43 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:05:45 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 17:05:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 17:05:45 - INFO - __main__ - Starting training!
06/02/2022 17:05:59 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_42_0.0001_8_predictions.txt
06/02/2022 17:05:59 - INFO - __main__ - Classification-F1 on test data: 0.6082
06/02/2022 17:06:00 - INFO - __main__ - prefix=amazon_polarity_64_42, lr=0.0001, bsz=8, dev_performance=0.6638778625954198, test_performance=0.6081690211780111
06/02/2022 17:06:00 - INFO - __main__ - Running ... prefix=amazon_polarity_64_87, lr=0.0005, bsz=8 ...
06/02/2022 17:06:01 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 17:06:01 - INFO - __main__ - Printing 3 examples
06/02/2022 17:06:01 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
06/02/2022 17:06:01 - INFO - __main__ - ['negative']
06/02/2022 17:06:01 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
06/02/2022 17:06:01 - INFO - __main__ - ['negative']
06/02/2022 17:06:01 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
06/02/2022 17:06:01 - INFO - __main__ - ['negative']
06/02/2022 17:06:01 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:06:01 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:06:01 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 17:06:01 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 17:06:01 - INFO - __main__ - Printing 3 examples
06/02/2022 17:06:01 - INFO - __main__ -  [amazon_polarity] title: I was an extra in this movie [SEP] content: I got paid $35.00 for 12 hours work. Brad Davis pushed me in the critical student protest scene. I'm the one in the blue sweater.
06/02/2022 17:06:01 - INFO - __main__ - ['negative']
06/02/2022 17:06:01 - INFO - __main__ -  [amazon_polarity] title: A book for specialists [SEP] content: More than a book it is a collection of papers, not always easy to understand and written in academic language. It could be used as a reference by one who knows already the subject. Not recommended for beginners
06/02/2022 17:06:01 - INFO - __main__ - ['negative']
06/02/2022 17:06:01 - INFO - __main__ -  [amazon_polarity] title: Didn't fulfill its promise, but I'm glad I read it. [SEP] content: One of the other reviewers perhaps said it best: I really wanted to like this book. As a mid-life motorcyclist (Harley) and pilot, with a penchant for old airplanes and motorcycles, I have long harbored the dream of restoring something really classic; a Harley J, a '32 Ford, a Stearman biplane, etc. So I grabbed the book without a second thought, anticipating a real restoration chronicle. What I got was part soap opera and part Pirsig, with a too-large serving of jobbing out a restoration to various subcontractors, and not enough nuts and bolts. Wish it had been edited bt Tom and Ray Magliozzi (The Tappet Brothers).
06/02/2022 17:06:01 - INFO - __main__ - ['negative']
06/02/2022 17:06:01 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:06:01 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:06:01 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 17:06:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 17:06:12 - INFO - __main__ - Starting training!
06/02/2022 17:06:17 - INFO - __main__ - Step 10 Global step 10 Train loss 23.018986 on epoch=1
06/02/2022 17:06:22 - INFO - __main__ - Step 20 Global step 20 Train loss 19.551670 on epoch=2
06/02/2022 17:06:27 - INFO - __main__ - Step 30 Global step 30 Train loss 14.950895 on epoch=3
06/02/2022 17:06:32 - INFO - __main__ - Step 40 Global step 40 Train loss 12.973219 on epoch=4
06/02/2022 17:06:37 - INFO - __main__ - Step 50 Global step 50 Train loss 10.489541 on epoch=6
06/02/2022 17:06:40 - INFO - __main__ - Global step 50 Train loss 16.196863 Classification-F1 0.1814373897707231 on epoch=6
06/02/2022 17:06:46 - INFO - __main__ - Step 60 Global step 60 Train loss 5.126630 on epoch=7
06/02/2022 17:06:51 - INFO - __main__ - Step 70 Global step 70 Train loss 0.612052 on epoch=8
06/02/2022 17:06:56 - INFO - __main__ - Step 80 Global step 80 Train loss 0.362853 on epoch=9
06/02/2022 17:07:02 - INFO - __main__ - Step 90 Global step 90 Train loss 0.411830 on epoch=11
06/02/2022 17:07:07 - INFO - __main__ - Step 100 Global step 100 Train loss 0.174025 on epoch=12
06/02/2022 17:07:08 - INFO - __main__ - Global step 100 Train loss 1.337478 Classification-F1 0.9373623684854416 on epoch=12
06/02/2022 17:07:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.093549 on epoch=13
06/02/2022 17:07:20 - INFO - __main__ - Step 120 Global step 120 Train loss 0.032298 on epoch=14
06/02/2022 17:07:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.028379 on epoch=16
06/02/2022 17:07:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.315667 on epoch=17
06/02/2022 17:07:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.020109 on epoch=18
06/02/2022 17:07:37 - INFO - __main__ - Global step 150 Train loss 0.098000 Classification-F1 0.9531135531135531 on epoch=18
06/02/2022 17:07:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.029696 on epoch=19
06/02/2022 17:07:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.005585 on epoch=21
06/02/2022 17:07:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.042125 on epoch=22
06/02/2022 17:07:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.006983 on epoch=23
06/02/2022 17:08:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.028664 on epoch=24
06/02/2022 17:08:06 - INFO - __main__ - Global step 200 Train loss 0.022611 Classification-F1 0.9609160305343512 on epoch=24
06/02/2022 17:08:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.081909 on epoch=26
06/02/2022 17:08:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.023419 on epoch=27
06/02/2022 17:08:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.014195 on epoch=28
06/02/2022 17:08:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.003097 on epoch=29
06/02/2022 17:08:33 - INFO - __main__ - Step 250 Global step 250 Train loss 2.771382 on epoch=31
06/02/2022 17:08:34 - INFO - __main__ - Global step 250 Train loss 0.578800 Classification-F1 0.37922403003754696 on epoch=31
06/02/2022 17:08:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.419340 on epoch=32
06/02/2022 17:08:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.123598 on epoch=33
06/02/2022 17:08:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.016679 on epoch=34
06/02/2022 17:08:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.007221 on epoch=36
06/02/2022 17:09:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.006343 on epoch=37
06/02/2022 17:09:02 - INFO - __main__ - Global step 300 Train loss 0.114636 Classification-F1 0.960935115668681 on epoch=37
06/02/2022 17:09:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.002439 on epoch=38
06/02/2022 17:09:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.001699 on epoch=39
06/02/2022 17:09:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.000857 on epoch=41
06/02/2022 17:09:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.002080 on epoch=42
06/02/2022 17:09:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.001061 on epoch=43
06/02/2022 17:09:31 - INFO - __main__ - Global step 350 Train loss 0.001627 Classification-F1 0.953125 on epoch=43
06/02/2022 17:09:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.003201 on epoch=44
06/02/2022 17:09:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.102578 on epoch=46
06/02/2022 17:09:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.002956 on epoch=47
06/02/2022 17:09:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.000643 on epoch=48
06/02/2022 17:09:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.010925 on epoch=49
06/02/2022 17:09:59 - INFO - __main__ - Global step 400 Train loss 0.024061 Classification-F1 0.9453091619361533 on epoch=49
06/02/2022 17:10:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.032733 on epoch=51
06/02/2022 17:10:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.003637 on epoch=52
06/02/2022 17:10:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000581 on epoch=53
06/02/2022 17:10:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.048159 on epoch=54
06/02/2022 17:10:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.001138 on epoch=56
06/02/2022 17:10:27 - INFO - __main__ - Global step 450 Train loss 0.017250 Classification-F1 0.9296832082036257 on epoch=56
06/02/2022 17:10:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001581 on epoch=57
06/02/2022 17:10:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000657 on epoch=58
06/02/2022 17:10:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.000158 on epoch=59
06/02/2022 17:10:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000109 on epoch=61
06/02/2022 17:10:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000098 on epoch=62
06/02/2022 17:10:54 - INFO - __main__ - Global step 500 Train loss 0.000520 Classification-F1 0.9374847374847375 on epoch=62
06/02/2022 17:11:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.003123 on epoch=63
06/02/2022 17:11:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.000394 on epoch=64
06/02/2022 17:11:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000283 on epoch=66
06/02/2022 17:11:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000366 on epoch=67
06/02/2022 17:11:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000475 on epoch=68
06/02/2022 17:11:22 - INFO - __main__ - Global step 550 Train loss 0.000928 Classification-F1 0.9375 on epoch=68
06/02/2022 17:11:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000056 on epoch=69
06/02/2022 17:11:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000998 on epoch=71
06/02/2022 17:11:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000261 on epoch=72
06/02/2022 17:11:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.000911 on epoch=73
06/02/2022 17:11:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.000478 on epoch=74
06/02/2022 17:11:50 - INFO - __main__ - Global step 600 Train loss 0.000541 Classification-F1 0.9296832082036257 on epoch=74
06/02/2022 17:11:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000116 on epoch=76
06/02/2022 17:12:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000314 on epoch=77
06/02/2022 17:12:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000034 on epoch=78
06/02/2022 17:12:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000061 on epoch=79
06/02/2022 17:12:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000155 on epoch=81
06/02/2022 17:12:17 - INFO - __main__ - Global step 650 Train loss 0.000136 Classification-F1 0.9375 on epoch=81
06/02/2022 17:12:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000066 on epoch=82
06/02/2022 17:12:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000079 on epoch=83
06/02/2022 17:12:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000019 on epoch=84
06/02/2022 17:12:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000095 on epoch=86
06/02/2022 17:12:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000016 on epoch=87
06/02/2022 17:12:45 - INFO - __main__ - Global step 700 Train loss 0.000055 Classification-F1 0.9375 on epoch=87
06/02/2022 17:12:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000041 on epoch=88
06/02/2022 17:12:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000027 on epoch=89
06/02/2022 17:13:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000151 on epoch=91
06/02/2022 17:13:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.001777 on epoch=92
06/02/2022 17:13:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000016 on epoch=93
06/02/2022 17:13:12 - INFO - __main__ - Global step 750 Train loss 0.000403 Classification-F1 0.9374847374847375 on epoch=93
06/02/2022 17:13:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.061142 on epoch=94
06/02/2022 17:13:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.005930 on epoch=96
06/02/2022 17:13:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000131 on epoch=97
06/02/2022 17:13:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000061 on epoch=98
06/02/2022 17:13:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000029 on epoch=99
06/02/2022 17:13:40 - INFO - __main__ - Global step 800 Train loss 0.013459 Classification-F1 0.9375 on epoch=99
06/02/2022 17:13:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000057 on epoch=101
06/02/2022 17:13:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000892 on epoch=102
06/02/2022 17:13:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.003356 on epoch=103
06/02/2022 17:14:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000088 on epoch=104
06/02/2022 17:14:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000588 on epoch=106
06/02/2022 17:14:08 - INFO - __main__ - Global step 850 Train loss 0.000996 Classification-F1 0.953125 on epoch=106
06/02/2022 17:14:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.008933 on epoch=107
06/02/2022 17:14:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.011187 on epoch=108
06/02/2022 17:14:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.055100 on epoch=109
06/02/2022 17:14:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000074 on epoch=111
06/02/2022 17:14:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.014055 on epoch=112
06/02/2022 17:14:36 - INFO - __main__ - Global step 900 Train loss 0.017870 Classification-F1 0.9609160305343512 on epoch=112
06/02/2022 17:14:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000012 on epoch=113
06/02/2022 17:14:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.029818 on epoch=114
06/02/2022 17:14:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.001265 on epoch=116
06/02/2022 17:14:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000050 on epoch=117
06/02/2022 17:15:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000175 on epoch=118
06/02/2022 17:15:04 - INFO - __main__ - Global step 950 Train loss 0.006264 Classification-F1 0.9375 on epoch=118
06/02/2022 17:15:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000047 on epoch=119
06/02/2022 17:15:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000102 on epoch=121
06/02/2022 17:15:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000027 on epoch=122
06/02/2022 17:15:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000060 on epoch=123
06/02/2022 17:15:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000028 on epoch=124
06/02/2022 17:15:31 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 17:15:31 - INFO - __main__ - Printing 3 examples
06/02/2022 17:15:31 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
06/02/2022 17:15:31 - INFO - __main__ - ['negative']
06/02/2022 17:15:31 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
06/02/2022 17:15:31 - INFO - __main__ - ['negative']
06/02/2022 17:15:31 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
06/02/2022 17:15:31 - INFO - __main__ - ['negative']
06/02/2022 17:15:31 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:15:31 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:15:32 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 17:15:32 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 17:15:32 - INFO - __main__ - Printing 3 examples
06/02/2022 17:15:32 - INFO - __main__ -  [amazon_polarity] title: I was an extra in this movie [SEP] content: I got paid $35.00 for 12 hours work. Brad Davis pushed me in the critical student protest scene. I'm the one in the blue sweater.
06/02/2022 17:15:32 - INFO - __main__ - ['negative']
06/02/2022 17:15:32 - INFO - __main__ -  [amazon_polarity] title: A book for specialists [SEP] content: More than a book it is a collection of papers, not always easy to understand and written in academic language. It could be used as a reference by one who knows already the subject. Not recommended for beginners
06/02/2022 17:15:32 - INFO - __main__ - ['negative']
06/02/2022 17:15:32 - INFO - __main__ -  [amazon_polarity] title: Didn't fulfill its promise, but I'm glad I read it. [SEP] content: One of the other reviewers perhaps said it best: I really wanted to like this book. As a mid-life motorcyclist (Harley) and pilot, with a penchant for old airplanes and motorcycles, I have long harbored the dream of restoring something really classic; a Harley J, a '32 Ford, a Stearman biplane, etc. So I grabbed the book without a second thought, anticipating a real restoration chronicle. What I got was part soap opera and part Pirsig, with a too-large serving of jobbing out a restoration to various subcontractors, and not enough nuts and bolts. Wish it had been edited bt Tom and Ray Magliozzi (The Tappet Brothers).
06/02/2022 17:15:32 - INFO - __main__ - ['negative']
06/02/2022 17:15:32 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:15:32 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:15:32 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 17:15:32 - INFO - __main__ - Global step 1000 Train loss 0.000053 Classification-F1 0.9375 on epoch=124
06/02/2022 17:15:32 - INFO - __main__ - save last model!
06/02/2022 17:15:38 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 17:15:39 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 17:15:39 - INFO - __main__ - Printing 3 examples
06/02/2022 17:15:39 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 17:15:39 - INFO - __main__ - ['negative']
06/02/2022 17:15:39 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 17:15:39 - INFO - __main__ - ['negative']
06/02/2022 17:15:39 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 17:15:39 - INFO - __main__ - ['negative']
06/02/2022 17:15:39 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:15:40 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:15:41 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 17:15:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 17:15:42 - INFO - __main__ - Starting training!
06/02/2022 17:15:56 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_87_0.0005_8_predictions.txt
06/02/2022 17:15:56 - INFO - __main__ - Classification-F1 on test data: 0.9500
06/02/2022 17:15:56 - INFO - __main__ - prefix=amazon_polarity_64_87, lr=0.0005, bsz=8, dev_performance=0.960935115668681, test_performance=0.9499927989630507
06/02/2022 17:15:56 - INFO - __main__ - Running ... prefix=amazon_polarity_64_87, lr=0.0003, bsz=8 ...
06/02/2022 17:15:57 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 17:15:57 - INFO - __main__ - Printing 3 examples
06/02/2022 17:15:57 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
06/02/2022 17:15:57 - INFO - __main__ - ['negative']
06/02/2022 17:15:57 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
06/02/2022 17:15:57 - INFO - __main__ - ['negative']
06/02/2022 17:15:57 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
06/02/2022 17:15:57 - INFO - __main__ - ['negative']
06/02/2022 17:15:57 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:15:57 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:15:58 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 17:15:58 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 17:15:58 - INFO - __main__ - Printing 3 examples
06/02/2022 17:15:58 - INFO - __main__ -  [amazon_polarity] title: I was an extra in this movie [SEP] content: I got paid $35.00 for 12 hours work. Brad Davis pushed me in the critical student protest scene. I'm the one in the blue sweater.
06/02/2022 17:15:58 - INFO - __main__ - ['negative']
06/02/2022 17:15:58 - INFO - __main__ -  [amazon_polarity] title: A book for specialists [SEP] content: More than a book it is a collection of papers, not always easy to understand and written in academic language. It could be used as a reference by one who knows already the subject. Not recommended for beginners
06/02/2022 17:15:58 - INFO - __main__ - ['negative']
06/02/2022 17:15:58 - INFO - __main__ -  [amazon_polarity] title: Didn't fulfill its promise, but I'm glad I read it. [SEP] content: One of the other reviewers perhaps said it best: I really wanted to like this book. As a mid-life motorcyclist (Harley) and pilot, with a penchant for old airplanes and motorcycles, I have long harbored the dream of restoring something really classic; a Harley J, a '32 Ford, a Stearman biplane, etc. So I grabbed the book without a second thought, anticipating a real restoration chronicle. What I got was part soap opera and part Pirsig, with a too-large serving of jobbing out a restoration to various subcontractors, and not enough nuts and bolts. Wish it had been edited bt Tom and Ray Magliozzi (The Tappet Brothers).
06/02/2022 17:15:58 - INFO - __main__ - ['negative']
06/02/2022 17:15:58 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:15:58 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:15:58 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 17:16:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 17:16:08 - INFO - __main__ - Starting training!
06/02/2022 17:16:13 - INFO - __main__ - Step 10 Global step 10 Train loss 21.939878 on epoch=1
06/02/2022 17:16:18 - INFO - __main__ - Step 20 Global step 20 Train loss 18.545597 on epoch=2
06/02/2022 17:16:23 - INFO - __main__ - Step 30 Global step 30 Train loss 16.471182 on epoch=3
06/02/2022 17:16:28 - INFO - __main__ - Step 40 Global step 40 Train loss 15.500685 on epoch=4
06/02/2022 17:16:33 - INFO - __main__ - Step 50 Global step 50 Train loss 14.898529 on epoch=6
06/02/2022 17:16:37 - INFO - __main__ - Global step 50 Train loss 17.471172 Classification-F1 0.0 on epoch=6
06/02/2022 17:16:43 - INFO - __main__ - Step 60 Global step 60 Train loss 12.779428 on epoch=7
06/02/2022 17:16:48 - INFO - __main__ - Step 70 Global step 70 Train loss 11.925348 on epoch=8
06/02/2022 17:16:53 - INFO - __main__ - Step 80 Global step 80 Train loss 10.458520 on epoch=9
06/02/2022 17:16:58 - INFO - __main__ - Step 90 Global step 90 Train loss 7.049119 on epoch=11
06/02/2022 17:17:03 - INFO - __main__ - Step 100 Global step 100 Train loss 3.192014 on epoch=12
06/02/2022 17:17:05 - INFO - __main__ - Global step 100 Train loss 9.080886 Classification-F1 0.5330817610062892 on epoch=12
06/02/2022 17:17:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.756709 on epoch=13
06/02/2022 17:17:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.506877 on epoch=14
06/02/2022 17:17:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.473206 on epoch=16
06/02/2022 17:17:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.469358 on epoch=17
06/02/2022 17:17:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.402338 on epoch=18
06/02/2022 17:17:34 - INFO - __main__ - Global step 150 Train loss 0.521698 Classification-F1 0.350463149416029 on epoch=18
06/02/2022 17:17:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.394667 on epoch=19
06/02/2022 17:17:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.396143 on epoch=21
06/02/2022 17:17:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.430588 on epoch=22
06/02/2022 17:17:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.364079 on epoch=23
06/02/2022 17:18:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.380707 on epoch=24
06/02/2022 17:18:12 - INFO - __main__ - Global step 200 Train loss 0.393236 Classification-F1 0.3917433917433918 on epoch=24
06/02/2022 17:18:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.407766 on epoch=26
06/02/2022 17:18:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.417070 on epoch=27
06/02/2022 17:18:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.420196 on epoch=28
06/02/2022 17:18:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.369922 on epoch=29
06/02/2022 17:18:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.349586 on epoch=31
06/02/2022 17:18:50 - INFO - __main__ - Global step 250 Train loss 0.392908 Classification-F1 0.29068002428658163 on epoch=31
06/02/2022 17:18:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.358296 on epoch=32
06/02/2022 17:19:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.417739 on epoch=33
06/02/2022 17:19:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.371038 on epoch=34
06/02/2022 17:19:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.388938 on epoch=36
06/02/2022 17:19:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.375884 on epoch=37
06/02/2022 17:19:28 - INFO - __main__ - Global step 300 Train loss 0.382379 Classification-F1 0.2638146167557932 on epoch=37
06/02/2022 17:19:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.330561 on epoch=38
06/02/2022 17:19:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.356791 on epoch=39
06/02/2022 17:19:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.336342 on epoch=41
06/02/2022 17:19:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.334594 on epoch=42
06/02/2022 17:19:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.302076 on epoch=43
06/02/2022 17:19:57 - INFO - __main__ - Global step 350 Train loss 0.332073 Classification-F1 0.8019679435608639 on epoch=43
06/02/2022 17:20:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.490970 on epoch=44
06/02/2022 17:20:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.314157 on epoch=46
06/02/2022 17:20:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.320879 on epoch=47
06/02/2022 17:20:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.300040 on epoch=48
06/02/2022 17:20:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.346937 on epoch=49
06/02/2022 17:20:36 - INFO - __main__ - Global step 400 Train loss 0.354597 Classification-F1 0.3323192814718238 on epoch=49
06/02/2022 17:20:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.329073 on epoch=51
06/02/2022 17:20:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.342447 on epoch=52
06/02/2022 17:20:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.365765 on epoch=53
06/02/2022 17:20:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.327704 on epoch=54
06/02/2022 17:21:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.275815 on epoch=56
06/02/2022 17:21:14 - INFO - __main__ - Global step 450 Train loss 0.328161 Classification-F1 0.40575786291232346 on epoch=56
06/02/2022 17:21:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.392429 on epoch=57
06/02/2022 17:21:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.351893 on epoch=58
06/02/2022 17:21:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.615003 on epoch=59
06/02/2022 17:21:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.255956 on epoch=61
06/02/2022 17:21:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.364304 on epoch=62
06/02/2022 17:21:53 - INFO - __main__ - Global step 500 Train loss 0.395917 Classification-F1 0.4914482588901194 on epoch=62
06/02/2022 17:21:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.286665 on epoch=63
06/02/2022 17:22:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.282633 on epoch=64
06/02/2022 17:22:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.249646 on epoch=66
06/02/2022 17:22:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.327684 on epoch=67
06/02/2022 17:22:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.259022 on epoch=68
06/02/2022 17:22:31 - INFO - __main__ - Global step 550 Train loss 0.281130 Classification-F1 0.5068982508006897 on epoch=68
06/02/2022 17:22:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.241382 on epoch=69
06/02/2022 17:22:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.256554 on epoch=71
06/02/2022 17:22:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.286422 on epoch=72
06/02/2022 17:22:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.202939 on epoch=73
06/02/2022 17:22:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.305319 on epoch=74
06/02/2022 17:22:59 - INFO - __main__ - Global step 600 Train loss 0.258523 Classification-F1 0.7968253968253969 on epoch=74
06/02/2022 17:23:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.297144 on epoch=76
06/02/2022 17:23:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.269066 on epoch=77
06/02/2022 17:23:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.222418 on epoch=78
06/02/2022 17:23:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.198866 on epoch=79
06/02/2022 17:23:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.284607 on epoch=81
06/02/2022 17:23:28 - INFO - __main__ - Global step 650 Train loss 0.254420 Classification-F1 0.7964276975776854 on epoch=81
06/02/2022 17:23:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.216148 on epoch=82
06/02/2022 17:23:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.229142 on epoch=83
06/02/2022 17:23:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.151076 on epoch=84
06/02/2022 17:23:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.205530 on epoch=86
06/02/2022 17:23:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.253933 on epoch=87
06/02/2022 17:23:56 - INFO - __main__ - Global step 700 Train loss 0.211166 Classification-F1 0.7811965811965812 on epoch=87
06/02/2022 17:24:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.185963 on epoch=88
06/02/2022 17:24:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.172810 on epoch=89
06/02/2022 17:24:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.243231 on epoch=91
06/02/2022 17:24:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.177475 on epoch=92
06/02/2022 17:24:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.157430 on epoch=93
06/02/2022 17:24:35 - INFO - __main__ - Global step 750 Train loss 0.187382 Classification-F1 0.5170110107633304 on epoch=93
06/02/2022 17:24:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.189937 on epoch=94
06/02/2022 17:24:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.166596 on epoch=96
06/02/2022 17:24:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.129923 on epoch=97
06/02/2022 17:24:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.145572 on epoch=98
06/02/2022 17:25:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.136176 on epoch=99
06/02/2022 17:25:03 - INFO - __main__ - Global step 800 Train loss 0.153641 Classification-F1 0.7494494739417665 on epoch=99
06/02/2022 17:25:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.136252 on epoch=101
06/02/2022 17:25:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.155042 on epoch=102
06/02/2022 17:25:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.184123 on epoch=103
06/02/2022 17:25:24 - INFO - __main__ - Step 840 Global step 840 Train loss 0.114548 on epoch=104
06/02/2022 17:25:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.110984 on epoch=106
06/02/2022 17:25:31 - INFO - __main__ - Global step 850 Train loss 0.140190 Classification-F1 0.7104957515740571 on epoch=106
06/02/2022 17:25:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.113274 on epoch=107
06/02/2022 17:25:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.113796 on epoch=108
06/02/2022 17:25:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.058152 on epoch=109
06/02/2022 17:25:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.069197 on epoch=111
06/02/2022 17:25:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.108655 on epoch=112
06/02/2022 17:25:59 - INFO - __main__ - Global step 900 Train loss 0.092615 Classification-F1 0.6230254350736278 on epoch=112
06/02/2022 17:26:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.095957 on epoch=113
06/02/2022 17:26:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.067689 on epoch=114
06/02/2022 17:26:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.084689 on epoch=116
06/02/2022 17:26:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.107193 on epoch=117
06/02/2022 17:26:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.040897 on epoch=118
06/02/2022 17:26:27 - INFO - __main__ - Global step 950 Train loss 0.079285 Classification-F1 0.7635467980295567 on epoch=118
06/02/2022 17:26:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.071648 on epoch=119
06/02/2022 17:26:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.054199 on epoch=121
06/02/2022 17:26:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.030179 on epoch=122
06/02/2022 17:26:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.034767 on epoch=123
06/02/2022 17:26:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.033335 on epoch=124
06/02/2022 17:26:55 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 17:26:55 - INFO - __main__ - Printing 3 examples
06/02/2022 17:26:55 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
06/02/2022 17:26:55 - INFO - __main__ - ['negative']
06/02/2022 17:26:55 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
06/02/2022 17:26:55 - INFO - __main__ - ['negative']
06/02/2022 17:26:55 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
06/02/2022 17:26:55 - INFO - __main__ - ['negative']
06/02/2022 17:26:55 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:26:55 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:26:55 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 17:26:55 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 17:26:55 - INFO - __main__ - Printing 3 examples
06/02/2022 17:26:55 - INFO - __main__ -  [amazon_polarity] title: I was an extra in this movie [SEP] content: I got paid $35.00 for 12 hours work. Brad Davis pushed me in the critical student protest scene. I'm the one in the blue sweater.
06/02/2022 17:26:55 - INFO - __main__ - ['negative']
06/02/2022 17:26:55 - INFO - __main__ -  [amazon_polarity] title: A book for specialists [SEP] content: More than a book it is a collection of papers, not always easy to understand and written in academic language. It could be used as a reference by one who knows already the subject. Not recommended for beginners
06/02/2022 17:26:55 - INFO - __main__ - ['negative']
06/02/2022 17:26:55 - INFO - __main__ -  [amazon_polarity] title: Didn't fulfill its promise, but I'm glad I read it. [SEP] content: One of the other reviewers perhaps said it best: I really wanted to like this book. As a mid-life motorcyclist (Harley) and pilot, with a penchant for old airplanes and motorcycles, I have long harbored the dream of restoring something really classic; a Harley J, a '32 Ford, a Stearman biplane, etc. So I grabbed the book without a second thought, anticipating a real restoration chronicle. What I got was part soap opera and part Pirsig, with a too-large serving of jobbing out a restoration to various subcontractors, and not enough nuts and bolts. Wish it had been edited bt Tom and Ray Magliozzi (The Tappet Brothers).
06/02/2022 17:26:55 - INFO - __main__ - ['negative']
06/02/2022 17:26:55 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:26:55 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:26:55 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 17:26:55 - INFO - __main__ - Global step 1000 Train loss 0.044826 Classification-F1 0.7570860116314662 on epoch=124
06/02/2022 17:26:55 - INFO - __main__ - save last model!
06/02/2022 17:27:02 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 17:27:03 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 17:27:03 - INFO - __main__ - Printing 3 examples
06/02/2022 17:27:03 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 17:27:03 - INFO - __main__ - ['negative']
06/02/2022 17:27:03 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 17:27:03 - INFO - __main__ - ['negative']
06/02/2022 17:27:03 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 17:27:03 - INFO - __main__ - ['negative']
06/02/2022 17:27:03 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:27:03 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:27:04 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 17:27:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 17:27:06 - INFO - __main__ - Starting training!
06/02/2022 17:27:39 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_87_0.0003_8_predictions.txt
06/02/2022 17:27:39 - INFO - __main__ - Classification-F1 on test data: 0.5397
06/02/2022 17:27:40 - INFO - __main__ - prefix=amazon_polarity_64_87, lr=0.0003, bsz=8, dev_performance=0.8019679435608639, test_performance=0.5396617808315645
06/02/2022 17:27:40 - INFO - __main__ - Running ... prefix=amazon_polarity_64_87, lr=0.0002, bsz=8 ...
06/02/2022 17:27:41 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 17:27:41 - INFO - __main__ - Printing 3 examples
06/02/2022 17:27:41 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
06/02/2022 17:27:41 - INFO - __main__ - ['negative']
06/02/2022 17:27:41 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
06/02/2022 17:27:41 - INFO - __main__ - ['negative']
06/02/2022 17:27:41 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
06/02/2022 17:27:41 - INFO - __main__ - ['negative']
06/02/2022 17:27:41 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:27:41 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:27:41 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 17:27:41 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 17:27:41 - INFO - __main__ - Printing 3 examples
06/02/2022 17:27:41 - INFO - __main__ -  [amazon_polarity] title: I was an extra in this movie [SEP] content: I got paid $35.00 for 12 hours work. Brad Davis pushed me in the critical student protest scene. I'm the one in the blue sweater.
06/02/2022 17:27:41 - INFO - __main__ - ['negative']
06/02/2022 17:27:41 - INFO - __main__ -  [amazon_polarity] title: A book for specialists [SEP] content: More than a book it is a collection of papers, not always easy to understand and written in academic language. It could be used as a reference by one who knows already the subject. Not recommended for beginners
06/02/2022 17:27:41 - INFO - __main__ - ['negative']
06/02/2022 17:27:41 - INFO - __main__ -  [amazon_polarity] title: Didn't fulfill its promise, but I'm glad I read it. [SEP] content: One of the other reviewers perhaps said it best: I really wanted to like this book. As a mid-life motorcyclist (Harley) and pilot, with a penchant for old airplanes and motorcycles, I have long harbored the dream of restoring something really classic; a Harley J, a '32 Ford, a Stearman biplane, etc. So I grabbed the book without a second thought, anticipating a real restoration chronicle. What I got was part soap opera and part Pirsig, with a too-large serving of jobbing out a restoration to various subcontractors, and not enough nuts and bolts. Wish it had been edited bt Tom and Ray Magliozzi (The Tappet Brothers).
06/02/2022 17:27:41 - INFO - __main__ - ['negative']
06/02/2022 17:27:41 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:27:41 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:27:41 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 17:27:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 17:27:52 - INFO - __main__ - Starting training!
06/02/2022 17:27:56 - INFO - __main__ - Step 10 Global step 10 Train loss 22.371700 on epoch=1
06/02/2022 17:28:01 - INFO - __main__ - Step 20 Global step 20 Train loss 18.335558 on epoch=2
06/02/2022 17:28:07 - INFO - __main__ - Step 30 Global step 30 Train loss 17.056171 on epoch=3
06/02/2022 17:28:11 - INFO - __main__ - Step 40 Global step 40 Train loss 17.113514 on epoch=4
06/02/2022 17:28:17 - INFO - __main__ - Step 50 Global step 50 Train loss 16.327618 on epoch=6
06/02/2022 17:28:51 - INFO - __main__ - Global step 50 Train loss 18.240911 Classification-F1 0.0 on epoch=6
06/02/2022 17:28:58 - INFO - __main__ - Step 60 Global step 60 Train loss 15.297995 on epoch=7
06/02/2022 17:29:03 - INFO - __main__ - Step 70 Global step 70 Train loss 13.954666 on epoch=8
06/02/2022 17:29:08 - INFO - __main__ - Step 80 Global step 80 Train loss 13.353350 on epoch=9
06/02/2022 17:29:14 - INFO - __main__ - Step 90 Global step 90 Train loss 12.475484 on epoch=11
06/02/2022 17:29:19 - INFO - __main__ - Step 100 Global step 100 Train loss 12.296648 on epoch=12
06/02/2022 17:29:45 - INFO - __main__ - Global step 100 Train loss 13.475629 Classification-F1 0.0 on epoch=12
06/02/2022 17:29:50 - INFO - __main__ - Step 110 Global step 110 Train loss 11.073343 on epoch=13
06/02/2022 17:29:55 - INFO - __main__ - Step 120 Global step 120 Train loss 9.738314 on epoch=14
06/02/2022 17:30:01 - INFO - __main__ - Step 130 Global step 130 Train loss 6.831426 on epoch=16
06/02/2022 17:30:06 - INFO - __main__ - Step 140 Global step 140 Train loss 3.874184 on epoch=17
06/02/2022 17:30:11 - INFO - __main__ - Step 150 Global step 150 Train loss 2.692099 on epoch=18
06/02/2022 17:30:24 - INFO - __main__ - Global step 150 Train loss 6.841873 Classification-F1 0.09300680635259576 on epoch=18
06/02/2022 17:30:30 - INFO - __main__ - Step 160 Global step 160 Train loss 2.617880 on epoch=19
06/02/2022 17:30:35 - INFO - __main__ - Step 170 Global step 170 Train loss 2.093458 on epoch=21
06/02/2022 17:30:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.907231 on epoch=22
06/02/2022 17:30:46 - INFO - __main__ - Step 190 Global step 190 Train loss 1.049507 on epoch=23
06/02/2022 17:30:51 - INFO - __main__ - Step 200 Global step 200 Train loss 1.375606 on epoch=24
06/02/2022 17:30:55 - INFO - __main__ - Global step 200 Train loss 1.608736 Classification-F1 0.3353483685964504 on epoch=24
06/02/2022 17:31:01 - INFO - __main__ - Step 210 Global step 210 Train loss 1.062111 on epoch=26
06/02/2022 17:31:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.868712 on epoch=27
06/02/2022 17:31:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.592608 on epoch=28
06/02/2022 17:31:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.746211 on epoch=29
06/02/2022 17:31:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.686375 on epoch=31
06/02/2022 17:31:24 - INFO - __main__ - Global step 250 Train loss 0.791203 Classification-F1 0.3588712522045855 on epoch=31
06/02/2022 17:31:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.440565 on epoch=32
06/02/2022 17:31:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.458864 on epoch=33
06/02/2022 17:31:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.510260 on epoch=34
06/02/2022 17:31:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.405241 on epoch=36
06/02/2022 17:31:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.384039 on epoch=37
06/02/2022 17:31:53 - INFO - __main__ - Global step 300 Train loss 0.439794 Classification-F1 0.5037747398490104 on epoch=37
06/02/2022 17:31:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.370600 on epoch=38
06/02/2022 17:32:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.382726 on epoch=39
06/02/2022 17:32:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.384964 on epoch=41
06/02/2022 17:32:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.436587 on epoch=42
06/02/2022 17:32:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.375087 on epoch=43
06/02/2022 17:32:22 - INFO - __main__ - Global step 350 Train loss 0.389993 Classification-F1 0.3727353727353727 on epoch=43
06/02/2022 17:32:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.511300 on epoch=44
06/02/2022 17:32:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.375071 on epoch=46
06/02/2022 17:32:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.326484 on epoch=47
06/02/2022 17:32:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.362668 on epoch=48
06/02/2022 17:32:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.371016 on epoch=49
06/02/2022 17:32:52 - INFO - __main__ - Global step 400 Train loss 0.389308 Classification-F1 0.3298429319371728 on epoch=49
06/02/2022 17:32:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.373633 on epoch=51
06/02/2022 17:33:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.410711 on epoch=52
06/02/2022 17:33:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.409630 on epoch=53
06/02/2022 17:33:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.384703 on epoch=54
06/02/2022 17:33:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.372976 on epoch=56
06/02/2022 17:33:20 - INFO - __main__ - Global step 450 Train loss 0.390330 Classification-F1 0.45744466123931915 on epoch=56
06/02/2022 17:33:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.455804 on epoch=57
06/02/2022 17:33:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.374152 on epoch=58
06/02/2022 17:33:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.372167 on epoch=59
06/02/2022 17:33:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.342026 on epoch=61
06/02/2022 17:33:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.379801 on epoch=62
06/02/2022 17:33:48 - INFO - __main__ - Global step 500 Train loss 0.384790 Classification-F1 0.5993612264452252 on epoch=62
06/02/2022 17:33:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.375866 on epoch=63
06/02/2022 17:33:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.361162 on epoch=64
06/02/2022 17:34:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.347576 on epoch=66
06/02/2022 17:34:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.360894 on epoch=67
06/02/2022 17:34:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.353684 on epoch=68
06/02/2022 17:34:17 - INFO - __main__ - Global step 550 Train loss 0.359836 Classification-F1 0.5454545454545454 on epoch=68
06/02/2022 17:34:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.363426 on epoch=69
06/02/2022 17:34:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.348634 on epoch=71
06/02/2022 17:34:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.346286 on epoch=72
06/02/2022 17:34:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.388982 on epoch=73
06/02/2022 17:34:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.368717 on epoch=74
06/02/2022 17:34:45 - INFO - __main__ - Global step 600 Train loss 0.363209 Classification-F1 0.5331195263352658 on epoch=74
06/02/2022 17:34:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.337977 on epoch=76
06/02/2022 17:34:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.367836 on epoch=77
06/02/2022 17:35:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.350094 on epoch=78
06/02/2022 17:35:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.346931 on epoch=79
06/02/2022 17:35:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.347071 on epoch=81
06/02/2022 17:35:13 - INFO - __main__ - Global step 650 Train loss 0.349982 Classification-F1 0.6903563255966001 on epoch=81
06/02/2022 17:35:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.344316 on epoch=82
06/02/2022 17:35:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.371442 on epoch=83
06/02/2022 17:35:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.370161 on epoch=84
06/02/2022 17:35:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.337798 on epoch=86
06/02/2022 17:35:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.339218 on epoch=87
06/02/2022 17:35:42 - INFO - __main__ - Global step 700 Train loss 0.352587 Classification-F1 0.7576793893129771 on epoch=87
06/02/2022 17:35:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.364800 on epoch=88
06/02/2022 17:35:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.357374 on epoch=89
06/02/2022 17:35:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.344980 on epoch=91
06/02/2022 17:36:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.372453 on epoch=92
06/02/2022 17:36:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.335942 on epoch=93
06/02/2022 17:36:11 - INFO - __main__ - Global step 750 Train loss 0.355110 Classification-F1 0.7702828145306021 on epoch=93
06/02/2022 17:36:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.343964 on epoch=94
06/02/2022 17:36:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.358134 on epoch=96
06/02/2022 17:36:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.337691 on epoch=97
06/02/2022 17:36:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.332822 on epoch=98
06/02/2022 17:36:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.336197 on epoch=99
06/02/2022 17:36:40 - INFO - __main__ - Global step 800 Train loss 0.341762 Classification-F1 0.7534638086362224 on epoch=99
06/02/2022 17:36:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.359620 on epoch=101
06/02/2022 17:36:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.348269 on epoch=102
06/02/2022 17:36:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.316703 on epoch=103
06/02/2022 17:37:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.342949 on epoch=104
06/02/2022 17:37:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.332049 on epoch=106
06/02/2022 17:37:08 - INFO - __main__ - Global step 850 Train loss 0.339918 Classification-F1 0.7843100542969481 on epoch=106
06/02/2022 17:37:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.330157 on epoch=107
06/02/2022 17:37:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.344419 on epoch=108
06/02/2022 17:37:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.309500 on epoch=109
06/02/2022 17:37:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.304751 on epoch=111
06/02/2022 17:37:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.345038 on epoch=112
06/02/2022 17:37:37 - INFO - __main__ - Global step 900 Train loss 0.326773 Classification-F1 0.7350561374898075 on epoch=112
06/02/2022 17:37:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.333268 on epoch=113
06/02/2022 17:37:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.358036 on epoch=114
06/02/2022 17:37:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.315135 on epoch=116
06/02/2022 17:37:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.344379 on epoch=117
06/02/2022 17:38:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.316310 on epoch=118
06/02/2022 17:38:05 - INFO - __main__ - Global step 950 Train loss 0.333426 Classification-F1 0.8078078078078079 on epoch=118
06/02/2022 17:38:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.318802 on epoch=119
06/02/2022 17:38:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.313004 on epoch=121
06/02/2022 17:38:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.328680 on epoch=122
06/02/2022 17:38:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.310241 on epoch=123
06/02/2022 17:38:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.294768 on epoch=124
06/02/2022 17:38:33 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 17:38:33 - INFO - __main__ - Printing 3 examples
06/02/2022 17:38:33 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
06/02/2022 17:38:33 - INFO - __main__ - ['negative']
06/02/2022 17:38:33 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
06/02/2022 17:38:33 - INFO - __main__ - ['negative']
06/02/2022 17:38:33 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
06/02/2022 17:38:33 - INFO - __main__ - ['negative']
06/02/2022 17:38:33 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:38:33 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:38:33 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 17:38:33 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 17:38:33 - INFO - __main__ - Printing 3 examples
06/02/2022 17:38:33 - INFO - __main__ -  [amazon_polarity] title: I was an extra in this movie [SEP] content: I got paid $35.00 for 12 hours work. Brad Davis pushed me in the critical student protest scene. I'm the one in the blue sweater.
06/02/2022 17:38:33 - INFO - __main__ - ['negative']
06/02/2022 17:38:33 - INFO - __main__ -  [amazon_polarity] title: A book for specialists [SEP] content: More than a book it is a collection of papers, not always easy to understand and written in academic language. It could be used as a reference by one who knows already the subject. Not recommended for beginners
06/02/2022 17:38:33 - INFO - __main__ - ['negative']
06/02/2022 17:38:33 - INFO - __main__ -  [amazon_polarity] title: Didn't fulfill its promise, but I'm glad I read it. [SEP] content: One of the other reviewers perhaps said it best: I really wanted to like this book. As a mid-life motorcyclist (Harley) and pilot, with a penchant for old airplanes and motorcycles, I have long harbored the dream of restoring something really classic; a Harley J, a '32 Ford, a Stearman biplane, etc. So I grabbed the book without a second thought, anticipating a real restoration chronicle. What I got was part soap opera and part Pirsig, with a too-large serving of jobbing out a restoration to various subcontractors, and not enough nuts and bolts. Wish it had been edited bt Tom and Ray Magliozzi (The Tappet Brothers).
06/02/2022 17:38:33 - INFO - __main__ - ['negative']
06/02/2022 17:38:33 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:38:33 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:38:34 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 17:38:34 - INFO - __main__ - Global step 1000 Train loss 0.313099 Classification-F1 0.8270695160894128 on epoch=124
06/02/2022 17:38:35 - INFO - __main__ - save last model!
06/02/2022 17:38:42 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 17:38:42 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 17:38:42 - INFO - __main__ - Printing 3 examples
06/02/2022 17:38:42 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 17:38:42 - INFO - __main__ - ['negative']
06/02/2022 17:38:42 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 17:38:42 - INFO - __main__ - ['negative']
06/02/2022 17:38:42 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 17:38:42 - INFO - __main__ - ['negative']
06/02/2022 17:38:42 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:38:43 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:38:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 17:38:44 - INFO - __main__ - Starting training!
06/02/2022 17:38:44 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 17:38:59 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_87_0.0002_8_predictions.txt
06/02/2022 17:38:59 - INFO - __main__ - Classification-F1 on test data: 0.8166
06/02/2022 17:39:00 - INFO - __main__ - prefix=amazon_polarity_64_87, lr=0.0002, bsz=8, dev_performance=0.8270695160894128, test_performance=0.8165741937118391
06/02/2022 17:39:00 - INFO - __main__ - Running ... prefix=amazon_polarity_64_87, lr=0.0001, bsz=8 ...
06/02/2022 17:39:01 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 17:39:01 - INFO - __main__ - Printing 3 examples
06/02/2022 17:39:01 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
06/02/2022 17:39:01 - INFO - __main__ - ['negative']
06/02/2022 17:39:01 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
06/02/2022 17:39:01 - INFO - __main__ - ['negative']
06/02/2022 17:39:01 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
06/02/2022 17:39:01 - INFO - __main__ - ['negative']
06/02/2022 17:39:01 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:39:01 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:39:01 - INFO - __main__ - Loaded 128 examples from train data
06/02/2022 17:39:01 - INFO - __main__ - Start tokenizing ... 128 instances
06/02/2022 17:39:01 - INFO - __main__ - Printing 3 examples
06/02/2022 17:39:01 - INFO - __main__ -  [amazon_polarity] title: I was an extra in this movie [SEP] content: I got paid $35.00 for 12 hours work. Brad Davis pushed me in the critical student protest scene. I'm the one in the blue sweater.
06/02/2022 17:39:01 - INFO - __main__ - ['negative']
06/02/2022 17:39:01 - INFO - __main__ -  [amazon_polarity] title: A book for specialists [SEP] content: More than a book it is a collection of papers, not always easy to understand and written in academic language. It could be used as a reference by one who knows already the subject. Not recommended for beginners
06/02/2022 17:39:01 - INFO - __main__ - ['negative']
06/02/2022 17:39:01 - INFO - __main__ -  [amazon_polarity] title: Didn't fulfill its promise, but I'm glad I read it. [SEP] content: One of the other reviewers perhaps said it best: I really wanted to like this book. As a mid-life motorcyclist (Harley) and pilot, with a penchant for old airplanes and motorcycles, I have long harbored the dream of restoring something really classic; a Harley J, a '32 Ford, a Stearman biplane, etc. So I grabbed the book without a second thought, anticipating a real restoration chronicle. What I got was part soap opera and part Pirsig, with a too-large serving of jobbing out a restoration to various subcontractors, and not enough nuts and bolts. Wish it had been edited bt Tom and Ray Magliozzi (The Tappet Brothers).
06/02/2022 17:39:01 - INFO - __main__ - ['negative']
06/02/2022 17:39:01 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:39:01 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:39:01 - INFO - __main__ - Loaded 128 examples from dev data
06/02/2022 17:39:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 17:39:12 - INFO - __main__ - Starting training!
06/02/2022 17:39:17 - INFO - __main__ - Step 10 Global step 10 Train loss 24.269476 on epoch=1
06/02/2022 17:39:22 - INFO - __main__ - Step 20 Global step 20 Train loss 18.774546 on epoch=2
06/02/2022 17:39:27 - INFO - __main__ - Step 30 Global step 30 Train loss 18.166407 on epoch=3
06/02/2022 17:39:32 - INFO - __main__ - Step 40 Global step 40 Train loss 17.021526 on epoch=4
06/02/2022 17:39:37 - INFO - __main__ - Step 50 Global step 50 Train loss 16.597448 on epoch=6
06/02/2022 17:40:00 - INFO - __main__ - Global step 50 Train loss 18.965879 Classification-F1 0.0 on epoch=6
06/02/2022 17:40:06 - INFO - __main__ - Step 60 Global step 60 Train loss 15.440905 on epoch=7
06/02/2022 17:40:11 - INFO - __main__ - Step 70 Global step 70 Train loss 15.697557 on epoch=8
06/02/2022 17:40:16 - INFO - __main__ - Step 80 Global step 80 Train loss 15.426890 on epoch=9
06/02/2022 17:40:22 - INFO - __main__ - Step 90 Global step 90 Train loss 14.849299 on epoch=11
06/02/2022 17:40:27 - INFO - __main__ - Step 100 Global step 100 Train loss 14.609065 on epoch=12
06/02/2022 17:40:33 - INFO - __main__ - Global step 100 Train loss 15.204743 Classification-F1 0.0 on epoch=12
06/02/2022 17:40:38 - INFO - __main__ - Step 110 Global step 110 Train loss 14.161128 on epoch=13
06/02/2022 17:40:43 - INFO - __main__ - Step 120 Global step 120 Train loss 13.723021 on epoch=14
06/02/2022 17:40:49 - INFO - __main__ - Step 130 Global step 130 Train loss 13.305737 on epoch=16
06/02/2022 17:40:54 - INFO - __main__ - Step 140 Global step 140 Train loss 12.779078 on epoch=17
06/02/2022 17:40:59 - INFO - __main__ - Step 150 Global step 150 Train loss 12.705664 on epoch=18
06/02/2022 17:41:04 - INFO - __main__ - Global step 150 Train loss 13.334927 Classification-F1 0.0 on epoch=18
06/02/2022 17:41:09 - INFO - __main__ - Step 160 Global step 160 Train loss 11.806025 on epoch=19
06/02/2022 17:41:14 - INFO - __main__ - Step 170 Global step 170 Train loss 11.315874 on epoch=21
06/02/2022 17:41:19 - INFO - __main__ - Step 180 Global step 180 Train loss 10.949333 on epoch=22
06/02/2022 17:41:25 - INFO - __main__ - Step 190 Global step 190 Train loss 10.363359 on epoch=23
06/02/2022 17:41:30 - INFO - __main__ - Step 200 Global step 200 Train loss 9.106357 on epoch=24
06/02/2022 17:41:32 - INFO - __main__ - Global step 200 Train loss 10.708188 Classification-F1 0.0 on epoch=24
06/02/2022 17:41:37 - INFO - __main__ - Step 210 Global step 210 Train loss 6.487995 on epoch=26
06/02/2022 17:41:42 - INFO - __main__ - Step 220 Global step 220 Train loss 4.943290 on epoch=27
06/02/2022 17:41:47 - INFO - __main__ - Step 230 Global step 230 Train loss 4.134147 on epoch=28
06/02/2022 17:41:53 - INFO - __main__ - Step 240 Global step 240 Train loss 3.207982 on epoch=29
06/02/2022 17:41:58 - INFO - __main__ - Step 250 Global step 250 Train loss 1.813941 on epoch=31
06/02/2022 17:41:59 - INFO - __main__ - Global step 250 Train loss 4.117471 Classification-F1 0.32418799212598426 on epoch=31
06/02/2022 17:42:06 - INFO - __main__ - Step 260 Global step 260 Train loss 1.478923 on epoch=32
06/02/2022 17:42:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.823606 on epoch=33
06/02/2022 17:42:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.402998 on epoch=34
06/02/2022 17:42:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.541651 on epoch=36
06/02/2022 17:42:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.136675 on epoch=37
06/02/2022 17:42:28 - INFO - __main__ - Global step 300 Train loss 0.676771 Classification-F1 0.9530791788856304 on epoch=37
06/02/2022 17:42:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.319360 on epoch=38
06/02/2022 17:42:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.084698 on epoch=39
06/02/2022 17:42:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.214291 on epoch=41
06/02/2022 17:42:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.133977 on epoch=42
06/02/2022 17:42:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.019987 on epoch=43
06/02/2022 17:42:57 - INFO - __main__ - Global step 350 Train loss 0.154463 Classification-F1 0.953125 on epoch=43
06/02/2022 17:43:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.090145 on epoch=44
06/02/2022 17:43:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.040600 on epoch=46
06/02/2022 17:43:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.114316 on epoch=47
06/02/2022 17:43:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.084938 on epoch=48
06/02/2022 17:43:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.025970 on epoch=49
06/02/2022 17:43:26 - INFO - __main__ - Global step 400 Train loss 0.071194 Classification-F1 0.9374847374847375 on epoch=49
06/02/2022 17:43:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.014528 on epoch=51
06/02/2022 17:43:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.041177 on epoch=52
06/02/2022 17:43:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.007934 on epoch=53
06/02/2022 17:43:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.008128 on epoch=54
06/02/2022 17:43:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.030949 on epoch=56
06/02/2022 17:43:54 - INFO - __main__ - Global step 450 Train loss 0.020543 Classification-F1 0.9453091619361533 on epoch=56
06/02/2022 17:43:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.006292 on epoch=57
06/02/2022 17:44:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.022124 on epoch=58
06/02/2022 17:44:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.061441 on epoch=59
06/02/2022 17:44:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.009154 on epoch=61
06/02/2022 17:44:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.005220 on epoch=62
06/02/2022 17:44:22 - INFO - __main__ - Global step 500 Train loss 0.020846 Classification-F1 0.9453091619361533 on epoch=62
06/02/2022 17:44:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.019960 on epoch=63
06/02/2022 17:44:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.029340 on epoch=64
06/02/2022 17:44:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.027983 on epoch=66
06/02/2022 17:44:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.004499 on epoch=67
06/02/2022 17:44:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001995 on epoch=68
06/02/2022 17:44:49 - INFO - __main__ - Global step 550 Train loss 0.016755 Classification-F1 0.9531135531135531 on epoch=68
06/02/2022 17:44:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.002907 on epoch=69
06/02/2022 17:45:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.004526 on epoch=71
06/02/2022 17:45:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000957 on epoch=72
06/02/2022 17:45:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.068764 on epoch=73
06/02/2022 17:45:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001965 on epoch=74
06/02/2022 17:45:17 - INFO - __main__ - Global step 600 Train loss 0.015824 Classification-F1 0.9531135531135531 on epoch=74
06/02/2022 17:45:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.068378 on epoch=76
06/02/2022 17:45:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000322 on epoch=77
06/02/2022 17:45:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.073544 on epoch=78
06/02/2022 17:45:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000561 on epoch=79
06/02/2022 17:45:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000825 on epoch=81
06/02/2022 17:45:45 - INFO - __main__ - Global step 650 Train loss 0.028726 Classification-F1 0.9453091619361533 on epoch=81
06/02/2022 17:45:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000527 on epoch=82
06/02/2022 17:45:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.004733 on epoch=83
06/02/2022 17:46:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.001166 on epoch=84
06/02/2022 17:46:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.008180 on epoch=86
06/02/2022 17:46:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000629 on epoch=87
06/02/2022 17:46:12 - INFO - __main__ - Global step 700 Train loss 0.003047 Classification-F1 0.9452824427480916 on epoch=87
06/02/2022 17:46:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.002172 on epoch=88
06/02/2022 17:46:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000439 on epoch=89
06/02/2022 17:46:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000401 on epoch=91
06/02/2022 17:46:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.001675 on epoch=92
06/02/2022 17:46:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000186 on epoch=93
06/02/2022 17:46:40 - INFO - __main__ - Global step 750 Train loss 0.000974 Classification-F1 0.9453091619361533 on epoch=93
06/02/2022 17:46:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000181 on epoch=94
06/02/2022 17:46:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.002345 on epoch=96
06/02/2022 17:46:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000417 on epoch=97
06/02/2022 17:47:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000086 on epoch=98
06/02/2022 17:47:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000369 on epoch=99
06/02/2022 17:47:08 - INFO - __main__ - Global step 800 Train loss 0.000679 Classification-F1 0.9453091619361533 on epoch=99
06/02/2022 17:47:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000086 on epoch=101
06/02/2022 17:47:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.032678 on epoch=102
06/02/2022 17:47:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000094 on epoch=103
06/02/2022 17:47:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.029353 on epoch=104
06/02/2022 17:47:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000194 on epoch=106
06/02/2022 17:47:36 - INFO - __main__ - Global step 850 Train loss 0.012481 Classification-F1 0.9375 on epoch=106
06/02/2022 17:47:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000319 on epoch=107
06/02/2022 17:47:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000115 on epoch=108
06/02/2022 17:47:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000611 on epoch=109
06/02/2022 17:47:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000606 on epoch=111
06/02/2022 17:48:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000108 on epoch=112
06/02/2022 17:48:03 - INFO - __main__ - Global step 900 Train loss 0.000352 Classification-F1 0.9609160305343512 on epoch=112
06/02/2022 17:48:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000064 on epoch=113
06/02/2022 17:48:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000164 on epoch=114
06/02/2022 17:48:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.038958 on epoch=116
06/02/2022 17:48:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000350 on epoch=117
06/02/2022 17:48:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.006853 on epoch=118
06/02/2022 17:48:32 - INFO - __main__ - Global step 950 Train loss 0.009278 Classification-F1 0.9452289259734703 on epoch=118
06/02/2022 17:48:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000982 on epoch=119
06/02/2022 17:48:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000303 on epoch=121
06/02/2022 17:48:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.008182 on epoch=122
06/02/2022 17:48:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000342 on epoch=123
06/02/2022 17:48:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.002209 on epoch=124
06/02/2022 17:49:00 - INFO - __main__ - Global step 1000 Train loss 0.002403 Classification-F1 0.9453091619361533 on epoch=124
06/02/2022 17:49:00 - INFO - __main__ - save last model!
06/02/2022 17:49:06 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 17:49:07 - INFO - __main__ - Start tokenizing ... 1000 instances
06/02/2022 17:49:07 - INFO - __main__ - Printing 3 examples
06/02/2022 17:49:07 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
06/02/2022 17:49:07 - INFO - __main__ - ['negative']
06/02/2022 17:49:07 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
06/02/2022 17:49:07 - INFO - __main__ - ['negative']
06/02/2022 17:49:07 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
06/02/2022 17:49:07 - INFO - __main__ - ['negative']
06/02/2022 17:49:07 - INFO - __main__ - Tokenizing Input ...
06/02/2022 17:49:08 - INFO - __main__ - Tokenizing Output ...
06/02/2022 17:49:09 - INFO - __main__ - Loaded 1000 examples from test data
06/02/2022 17:49:24 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-amazon_polarity/amazon_polarity_64_87_0.0001_8_predictions.txt
06/02/2022 17:49:24 - INFO - __main__ - Classification-F1 on test data: 0.9470
06/02/2022 17:49:24 - INFO - __main__ - prefix=amazon_polarity_64_87, lr=0.0001, bsz=8, dev_performance=0.9609160305343512, test_performance=0.9469613348130788
