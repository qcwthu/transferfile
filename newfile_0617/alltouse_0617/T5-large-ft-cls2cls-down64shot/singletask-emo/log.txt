05/21/2022 21:25:54 - INFO - __main__ - Namespace(task_dir='data_64/emo/', task_name='emo', identifier='T5-large-ft-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down64shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:25:54 - INFO - __main__ - models/T5-large-ft-cls2cls-down64shot/singletask-emo
05/21/2022 21:25:54 - INFO - __main__ - Namespace(task_dir='data_64/emo/', task_name='emo', identifier='T5-large-ft-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down64shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:25:54 - INFO - __main__ - models/T5-large-ft-cls2cls-down64shot/singletask-emo
05/21/2022 21:25:55 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:25:55 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:25:55 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:25:55 - INFO - __main__ - Using 2 gpus
05/21/2022 21:25:55 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:25:55 - INFO - __main__ - Using 2 gpus
05/21/2022 21:25:55 - INFO - __main__ - Fine-tuning the following samples: ['emo_64_100', 'emo_64_13', 'emo_64_21', 'emo_64_42', 'emo_64_87']
05/21/2022 21:25:55 - INFO - __main__ - Fine-tuning the following samples: ['emo_64_100', 'emo_64_13', 'emo_64_21', 'emo_64_42', 'emo_64_87']
05/21/2022 21:26:01 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.0005, bsz=8 ...
06/02/2022 02:01:09 - INFO - __main__ - Namespace(task_dir='data_64/emo/', task_name='emo', identifier='T5-large-ft-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down64shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
06/02/2022 02:01:09 - INFO - __main__ - models/T5-large-ft-cls2cls-down64shot/singletask-emo
06/02/2022 02:01:09 - INFO - __main__ - Namespace(task_dir='data_64/emo/', task_name='emo', identifier='T5-large-ft-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down64shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
06/02/2022 02:01:09 - INFO - __main__ - models/T5-large-ft-cls2cls-down64shot/singletask-emo
06/02/2022 02:01:11 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/02/2022 02:01:11 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/02/2022 02:01:11 - INFO - __main__ - args.device: cuda:0
06/02/2022 02:01:11 - INFO - __main__ - Using 2 gpus
06/02/2022 02:01:11 - INFO - __main__ - args.device: cuda:1
06/02/2022 02:01:11 - INFO - __main__ - Using 2 gpus
06/02/2022 02:01:11 - INFO - __main__ - Fine-tuning the following samples: ['emo_64_100', 'emo_64_13', 'emo_64_21', 'emo_64_42', 'emo_64_87']
06/02/2022 02:01:11 - INFO - __main__ - Fine-tuning the following samples: ['emo_64_100', 'emo_64_13', 'emo_64_21', 'emo_64_42', 'emo_64_87']
06/02/2022 02:01:15 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.0005, bsz=8 ...
06/02/2022 02:01:16 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:01:16 - INFO - __main__ - Printing 3 examples
06/02/2022 02:01:16 - INFO - __main__ -  [emo] how cause yes am listening
06/02/2022 02:01:16 - INFO - __main__ - ['others']
06/02/2022 02:01:16 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/02/2022 02:01:16 - INFO - __main__ - ['others']
06/02/2022 02:01:16 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/02/2022 02:01:16 - INFO - __main__ - ['others']
06/02/2022 02:01:16 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:01:16 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:01:16 - INFO - __main__ - Printing 3 examples
06/02/2022 02:01:16 - INFO - __main__ -  [emo] how cause yes am listening
06/02/2022 02:01:16 - INFO - __main__ - ['others']
06/02/2022 02:01:16 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/02/2022 02:01:16 - INFO - __main__ - ['others']
06/02/2022 02:01:16 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/02/2022 02:01:16 - INFO - __main__ - ['others']
06/02/2022 02:01:16 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:01:16 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:01:16 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:01:17 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 02:01:17 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:01:17 - INFO - __main__ - Printing 3 examples
06/02/2022 02:01:17 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
06/02/2022 02:01:17 - INFO - __main__ - ['others']
06/02/2022 02:01:17 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
06/02/2022 02:01:17 - INFO - __main__ - ['others']
06/02/2022 02:01:17 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
06/02/2022 02:01:17 - INFO - __main__ - ['others']
06/02/2022 02:01:17 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:01:17 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 02:01:17 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:01:17 - INFO - __main__ - Printing 3 examples
06/02/2022 02:01:17 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
06/02/2022 02:01:17 - INFO - __main__ - ['others']
06/02/2022 02:01:17 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
06/02/2022 02:01:17 - INFO - __main__ - ['others']
06/02/2022 02:01:17 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
06/02/2022 02:01:17 - INFO - __main__ - ['others']
06/02/2022 02:01:17 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:01:17 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:01:17 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:01:17 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 02:01:17 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 02:01:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 02:01:30 - INFO - __main__ - Starting training!
06/02/2022 02:01:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 02:01:31 - INFO - __main__ - Starting training!
06/02/2022 02:01:36 - INFO - __main__ - Step 10 Global step 10 Train loss 23.868593 on epoch=0
06/02/2022 02:01:41 - INFO - __main__ - Step 20 Global step 20 Train loss 19.698687 on epoch=1
06/02/2022 02:01:46 - INFO - __main__ - Step 30 Global step 30 Train loss 16.618139 on epoch=1
06/02/2022 02:01:51 - INFO - __main__ - Step 40 Global step 40 Train loss 14.884361 on epoch=2
06/02/2022 02:01:56 - INFO - __main__ - Step 50 Global step 50 Train loss 13.544708 on epoch=3
06/02/2022 02:01:58 - INFO - __main__ - Global step 50 Train loss 17.722898 Classification-F1 0.0 on epoch=3
06/02/2022 02:02:05 - INFO - __main__ - Step 60 Global step 60 Train loss 12.877686 on epoch=3
06/02/2022 02:02:10 - INFO - __main__ - Step 70 Global step 70 Train loss 10.024112 on epoch=4
06/02/2022 02:02:15 - INFO - __main__ - Step 80 Global step 80 Train loss 7.515594 on epoch=4
06/02/2022 02:02:21 - INFO - __main__ - Step 90 Global step 90 Train loss 5.125386 on epoch=5
06/02/2022 02:02:26 - INFO - __main__ - Step 100 Global step 100 Train loss 2.499734 on epoch=6
06/02/2022 02:02:28 - INFO - __main__ - Global step 100 Train loss 7.608503 Classification-F1 0.3432599906701912 on epoch=6
06/02/2022 02:02:35 - INFO - __main__ - Step 110 Global step 110 Train loss 1.530959 on epoch=6
06/02/2022 02:02:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.920533 on epoch=7
06/02/2022 02:02:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.881692 on epoch=8
06/02/2022 02:02:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.667999 on epoch=8
06/02/2022 02:02:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.647320 on epoch=9
06/02/2022 02:02:59 - INFO - __main__ - Global step 150 Train loss 0.929701 Classification-F1 0.5850087412587412 on epoch=9
06/02/2022 02:03:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.588941 on epoch=9
06/02/2022 02:03:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.642262 on epoch=10
06/02/2022 02:03:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.669279 on epoch=11
06/02/2022 02:03:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.568675 on epoch=11
06/02/2022 02:03:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.526124 on epoch=12
06/02/2022 02:03:27 - INFO - __main__ - Global step 200 Train loss 0.599056 Classification-F1 0.6426257108220871 on epoch=12
06/02/2022 02:03:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.584773 on epoch=13
06/02/2022 02:03:39 - INFO - __main__ - Step 220 Global step 220 Train loss 1.374226 on epoch=13
06/02/2022 02:03:44 - INFO - __main__ - Step 230 Global step 230 Train loss 3.591169 on epoch=14
06/02/2022 02:03:49 - INFO - __main__ - Step 240 Global step 240 Train loss 3.342550 on epoch=14
06/02/2022 02:03:55 - INFO - __main__ - Step 250 Global step 250 Train loss 2.712648 on epoch=15
06/02/2022 02:03:57 - INFO - __main__ - Global step 250 Train loss 2.321074 Classification-F1 0.1 on epoch=15
06/02/2022 02:04:02 - INFO - __main__ - Step 260 Global step 260 Train loss 2.635079 on epoch=16
06/02/2022 02:04:07 - INFO - __main__ - Step 270 Global step 270 Train loss 2.384150 on epoch=16
06/02/2022 02:04:12 - INFO - __main__ - Step 280 Global step 280 Train loss 1.983606 on epoch=17
06/02/2022 02:04:17 - INFO - __main__ - Step 290 Global step 290 Train loss 1.860861 on epoch=18
06/02/2022 02:04:23 - INFO - __main__ - Step 300 Global step 300 Train loss 1.603475 on epoch=18
06/02/2022 02:04:25 - INFO - __main__ - Global step 300 Train loss 2.093434 Classification-F1 0.1 on epoch=18
06/02/2022 02:04:30 - INFO - __main__ - Step 310 Global step 310 Train loss 1.618452 on epoch=19
06/02/2022 02:04:35 - INFO - __main__ - Step 320 Global step 320 Train loss 1.547417 on epoch=19
06/02/2022 02:04:40 - INFO - __main__ - Step 330 Global step 330 Train loss 1.369294 on epoch=20
06/02/2022 02:04:45 - INFO - __main__ - Step 340 Global step 340 Train loss 1.325899 on epoch=21
06/02/2022 02:04:51 - INFO - __main__ - Step 350 Global step 350 Train loss 1.055720 on epoch=21
06/02/2022 02:04:53 - INFO - __main__ - Global step 350 Train loss 1.383356 Classification-F1 0.1 on epoch=21
06/02/2022 02:04:58 - INFO - __main__ - Step 360 Global step 360 Train loss 1.110727 on epoch=22
06/02/2022 02:05:03 - INFO - __main__ - Step 370 Global step 370 Train loss 1.193990 on epoch=23
06/02/2022 02:05:08 - INFO - __main__ - Step 380 Global step 380 Train loss 1.082926 on epoch=23
06/02/2022 02:05:13 - INFO - __main__ - Step 390 Global step 390 Train loss 1.112199 on epoch=24
06/02/2022 02:05:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.916564 on epoch=24
06/02/2022 02:05:21 - INFO - __main__ - Global step 400 Train loss 1.083281 Classification-F1 0.1 on epoch=24
06/02/2022 02:05:26 - INFO - __main__ - Step 410 Global step 410 Train loss 1.061895 on epoch=25
06/02/2022 02:05:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.893248 on epoch=26
06/02/2022 02:05:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.993047 on epoch=26
06/02/2022 02:05:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.957185 on epoch=27
06/02/2022 02:05:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.900874 on epoch=28
06/02/2022 02:05:49 - INFO - __main__ - Global step 450 Train loss 0.961250 Classification-F1 0.11578044596912522 on epoch=28
06/02/2022 02:05:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.846786 on epoch=28
06/02/2022 02:05:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.777324 on epoch=29
06/02/2022 02:06:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.838371 on epoch=29
06/02/2022 02:06:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.894284 on epoch=30
06/02/2022 02:06:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.845471 on epoch=31
06/02/2022 02:06:17 - INFO - __main__ - Global step 500 Train loss 0.840447 Classification-F1 0.1 on epoch=31
06/02/2022 02:06:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.781558 on epoch=31
06/02/2022 02:06:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.831066 on epoch=32
06/02/2022 02:06:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.747108 on epoch=33
06/02/2022 02:06:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.804879 on epoch=33
06/02/2022 02:06:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.841445 on epoch=34
06/02/2022 02:06:45 - INFO - __main__ - Global step 550 Train loss 0.801211 Classification-F1 0.1 on epoch=34
06/02/2022 02:06:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.786824 on epoch=34
06/02/2022 02:06:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.811274 on epoch=35
06/02/2022 02:07:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.802921 on epoch=36
06/02/2022 02:07:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.818877 on epoch=36
06/02/2022 02:07:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.791003 on epoch=37
06/02/2022 02:07:13 - INFO - __main__ - Global step 600 Train loss 0.802180 Classification-F1 0.14108100059559261 on epoch=37
06/02/2022 02:07:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.782072 on epoch=38
06/02/2022 02:07:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.817557 on epoch=38
06/02/2022 02:07:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.754300 on epoch=39
06/02/2022 02:07:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.800481 on epoch=39
06/02/2022 02:07:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.720022 on epoch=40
06/02/2022 02:07:41 - INFO - __main__ - Global step 650 Train loss 0.774887 Classification-F1 0.16087483944626801 on epoch=40
06/02/2022 02:07:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.802277 on epoch=41
06/02/2022 02:07:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.763583 on epoch=41
06/02/2022 02:07:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.752668 on epoch=42
06/02/2022 02:08:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.772999 on epoch=43
06/02/2022 02:08:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.710991 on epoch=43
06/02/2022 02:08:09 - INFO - __main__ - Global step 700 Train loss 0.760504 Classification-F1 0.17176690400444378 on epoch=43
06/02/2022 02:08:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.751267 on epoch=44
06/02/2022 02:08:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.745145 on epoch=44
06/02/2022 02:08:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.776872 on epoch=45
06/02/2022 02:08:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.728334 on epoch=46
06/02/2022 02:08:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.742244 on epoch=46
06/02/2022 02:08:38 - INFO - __main__ - Global step 750 Train loss 0.748773 Classification-F1 0.12888989990900818 on epoch=46
06/02/2022 02:08:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.707993 on epoch=47
06/02/2022 02:08:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.707692 on epoch=48
06/02/2022 02:08:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.755512 on epoch=48
06/02/2022 02:08:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.723754 on epoch=49
06/02/2022 02:09:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.737602 on epoch=49
06/02/2022 02:09:06 - INFO - __main__ - Global step 800 Train loss 0.726511 Classification-F1 0.2629154447702835 on epoch=49
06/02/2022 02:09:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.755850 on epoch=50
06/02/2022 02:09:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.739136 on epoch=51
06/02/2022 02:09:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.696473 on epoch=51
06/02/2022 02:09:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.664891 on epoch=52
06/02/2022 02:09:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.751777 on epoch=53
06/02/2022 02:09:34 - INFO - __main__ - Global step 850 Train loss 0.721626 Classification-F1 0.14923137911289258 on epoch=53
06/02/2022 02:09:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.734716 on epoch=53
06/02/2022 02:09:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.775689 on epoch=54
06/02/2022 02:09:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.520776 on epoch=54
06/02/2022 02:09:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.512335 on epoch=55
06/02/2022 02:10:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.691188 on epoch=56
06/02/2022 02:10:02 - INFO - __main__ - Global step 900 Train loss 0.646941 Classification-F1 0.483931549008585 on epoch=56
06/02/2022 02:10:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.643727 on epoch=56
06/02/2022 02:10:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.655509 on epoch=57
06/02/2022 02:10:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.671831 on epoch=58
06/02/2022 02:10:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.584608 on epoch=58
06/02/2022 02:10:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.641296 on epoch=59
06/02/2022 02:10:30 - INFO - __main__ - Global step 950 Train loss 0.639394 Classification-F1 0.5986912713803471 on epoch=59
06/02/2022 02:10:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.534062 on epoch=59
06/02/2022 02:10:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.539464 on epoch=60
06/02/2022 02:10:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.444259 on epoch=61
06/02/2022 02:10:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.397846 on epoch=61
06/02/2022 02:10:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.522458 on epoch=62
06/02/2022 02:10:57 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:10:57 - INFO - __main__ - Printing 3 examples
06/02/2022 02:10:57 - INFO - __main__ -  [emo] how cause yes am listening
06/02/2022 02:10:57 - INFO - __main__ - ['others']
06/02/2022 02:10:57 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/02/2022 02:10:57 - INFO - __main__ - ['others']
06/02/2022 02:10:57 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/02/2022 02:10:57 - INFO - __main__ - ['others']
06/02/2022 02:10:57 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:10:58 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:10:58 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 02:10:58 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:10:58 - INFO - __main__ - Printing 3 examples
06/02/2022 02:10:58 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
06/02/2022 02:10:58 - INFO - __main__ - ['others']
06/02/2022 02:10:58 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
06/02/2022 02:10:58 - INFO - __main__ - ['others']
06/02/2022 02:10:58 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
06/02/2022 02:10:58 - INFO - __main__ - ['others']
06/02/2022 02:10:58 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:10:58 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:10:58 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 02:10:58 - INFO - __main__ - Global step 1000 Train loss 0.487618 Classification-F1 0.5809472084244438 on epoch=62
06/02/2022 02:10:58 - INFO - __main__ - save last model!
06/02/2022 02:11:06 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 02:11:06 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 02:11:06 - INFO - __main__ - Printing 3 examples
06/02/2022 02:11:06 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 02:11:06 - INFO - __main__ - ['others']
06/02/2022 02:11:06 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 02:11:06 - INFO - __main__ - ['others']
06/02/2022 02:11:06 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 02:11:06 - INFO - __main__ - ['others']
06/02/2022 02:11:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:11:09 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:11:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 02:11:12 - INFO - __main__ - Starting training!
06/02/2022 02:11:14 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 02:11:55 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_100_0.0005_8_predictions.txt
06/02/2022 02:11:55 - INFO - __main__ - Classification-F1 on test data: 0.4502
06/02/2022 02:11:56 - INFO - __main__ - prefix=emo_64_100, lr=0.0005, bsz=8, dev_performance=0.6426257108220871, test_performance=0.4501535774246407
06/02/2022 02:11:56 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.0003, bsz=8 ...
06/02/2022 02:11:57 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:11:57 - INFO - __main__ - Printing 3 examples
06/02/2022 02:11:57 - INFO - __main__ -  [emo] how cause yes am listening
06/02/2022 02:11:57 - INFO - __main__ - ['others']
06/02/2022 02:11:57 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/02/2022 02:11:57 - INFO - __main__ - ['others']
06/02/2022 02:11:57 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/02/2022 02:11:57 - INFO - __main__ - ['others']
06/02/2022 02:11:57 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:11:57 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:11:57 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 02:11:57 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:11:57 - INFO - __main__ - Printing 3 examples
06/02/2022 02:11:57 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
06/02/2022 02:11:57 - INFO - __main__ - ['others']
06/02/2022 02:11:57 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
06/02/2022 02:11:57 - INFO - __main__ - ['others']
06/02/2022 02:11:57 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
06/02/2022 02:11:57 - INFO - __main__ - ['others']
06/02/2022 02:11:57 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:11:57 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:11:57 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 02:12:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 02:12:10 - INFO - __main__ - Starting training!
06/02/2022 02:12:14 - INFO - __main__ - Step 10 Global step 10 Train loss 24.644106 on epoch=0
06/02/2022 02:12:19 - INFO - __main__ - Step 20 Global step 20 Train loss 20.366182 on epoch=1
06/02/2022 02:12:24 - INFO - __main__ - Step 30 Global step 30 Train loss 18.871220 on epoch=1
06/02/2022 02:12:29 - INFO - __main__ - Step 40 Global step 40 Train loss 17.890001 on epoch=2
06/02/2022 02:12:35 - INFO - __main__ - Step 50 Global step 50 Train loss 15.571042 on epoch=3
06/02/2022 02:12:38 - INFO - __main__ - Global step 50 Train loss 19.468508 Classification-F1 0.0 on epoch=3
06/02/2022 02:12:44 - INFO - __main__ - Step 60 Global step 60 Train loss 15.517407 on epoch=3
06/02/2022 02:12:49 - INFO - __main__ - Step 70 Global step 70 Train loss 13.994699 on epoch=4
06/02/2022 02:12:54 - INFO - __main__ - Step 80 Global step 80 Train loss 13.093409 on epoch=4
06/02/2022 02:12:59 - INFO - __main__ - Step 90 Global step 90 Train loss 12.858892 on epoch=5
06/02/2022 02:13:05 - INFO - __main__ - Step 100 Global step 100 Train loss 11.673024 on epoch=6
06/02/2022 02:13:06 - INFO - __main__ - Global step 100 Train loss 13.427485 Classification-F1 0.0 on epoch=6
06/02/2022 02:13:12 - INFO - __main__ - Step 110 Global step 110 Train loss 10.723216 on epoch=6
06/02/2022 02:13:17 - INFO - __main__ - Step 120 Global step 120 Train loss 8.857610 on epoch=7
06/02/2022 02:13:22 - INFO - __main__ - Step 130 Global step 130 Train loss 6.123327 on epoch=8
06/02/2022 02:13:27 - INFO - __main__ - Step 140 Global step 140 Train loss 4.484473 on epoch=8
06/02/2022 02:13:32 - INFO - __main__ - Step 150 Global step 150 Train loss 5.006882 on epoch=9
06/02/2022 02:13:34 - INFO - __main__ - Global step 150 Train loss 7.039101 Classification-F1 0.1 on epoch=9
06/02/2022 02:13:40 - INFO - __main__ - Step 160 Global step 160 Train loss 3.593717 on epoch=9
06/02/2022 02:13:45 - INFO - __main__ - Step 170 Global step 170 Train loss 4.402640 on epoch=10
06/02/2022 02:13:50 - INFO - __main__ - Step 180 Global step 180 Train loss 2.855516 on epoch=11
06/02/2022 02:13:55 - INFO - __main__ - Step 190 Global step 190 Train loss 3.263700 on epoch=11
06/02/2022 02:14:00 - INFO - __main__ - Step 200 Global step 200 Train loss 3.089113 on epoch=12
06/02/2022 02:14:02 - INFO - __main__ - Global step 200 Train loss 3.440937 Classification-F1 0.1 on epoch=12
06/02/2022 02:14:08 - INFO - __main__ - Step 210 Global step 210 Train loss 2.789961 on epoch=13
06/02/2022 02:14:13 - INFO - __main__ - Step 220 Global step 220 Train loss 2.910730 on epoch=13
06/02/2022 02:14:18 - INFO - __main__ - Step 230 Global step 230 Train loss 2.324038 on epoch=14
06/02/2022 02:14:23 - INFO - __main__ - Step 240 Global step 240 Train loss 2.874357 on epoch=14
06/02/2022 02:14:28 - INFO - __main__ - Step 250 Global step 250 Train loss 2.039647 on epoch=15
06/02/2022 02:14:30 - INFO - __main__ - Global step 250 Train loss 2.587747 Classification-F1 0.1 on epoch=15
06/02/2022 02:14:36 - INFO - __main__ - Step 260 Global step 260 Train loss 2.705074 on epoch=16
06/02/2022 02:14:41 - INFO - __main__ - Step 270 Global step 270 Train loss 2.240443 on epoch=16
06/02/2022 02:14:46 - INFO - __main__ - Step 280 Global step 280 Train loss 2.135529 on epoch=17
06/02/2022 02:14:51 - INFO - __main__ - Step 290 Global step 290 Train loss 1.864541 on epoch=18
06/02/2022 02:14:56 - INFO - __main__ - Step 300 Global step 300 Train loss 1.549434 on epoch=18
06/02/2022 02:14:58 - INFO - __main__ - Global step 300 Train loss 2.099004 Classification-F1 0.1 on epoch=18
06/02/2022 02:15:03 - INFO - __main__ - Step 310 Global step 310 Train loss 1.369257 on epoch=19
06/02/2022 02:15:09 - INFO - __main__ - Step 320 Global step 320 Train loss 1.383225 on epoch=19
06/02/2022 02:15:14 - INFO - __main__ - Step 330 Global step 330 Train loss 1.079077 on epoch=20
06/02/2022 02:15:19 - INFO - __main__ - Step 340 Global step 340 Train loss 1.147350 on epoch=21
06/02/2022 02:15:24 - INFO - __main__ - Step 350 Global step 350 Train loss 1.073125 on epoch=21
06/02/2022 02:15:26 - INFO - __main__ - Global step 350 Train loss 1.210407 Classification-F1 0.2426243225920574 on epoch=21
06/02/2022 02:15:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.988786 on epoch=22
06/02/2022 02:15:37 - INFO - __main__ - Step 370 Global step 370 Train loss 1.485935 on epoch=23
06/02/2022 02:15:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.972713 on epoch=23
06/02/2022 02:15:47 - INFO - __main__ - Step 390 Global step 390 Train loss 1.003835 on epoch=24
06/02/2022 02:15:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.828943 on epoch=24
06/02/2022 02:15:55 - INFO - __main__ - Global step 400 Train loss 1.056042 Classification-F1 0.5185235631341613 on epoch=24
06/02/2022 02:16:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.778438 on epoch=25
06/02/2022 02:16:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.770517 on epoch=26
06/02/2022 02:16:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.714435 on epoch=26
06/02/2022 02:16:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.783260 on epoch=27
06/02/2022 02:16:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.591226 on epoch=28
06/02/2022 02:16:23 - INFO - __main__ - Global step 450 Train loss 0.727575 Classification-F1 0.6923496206500389 on epoch=28
06/02/2022 02:16:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.489925 on epoch=28
06/02/2022 02:16:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.496954 on epoch=29
06/02/2022 02:16:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.649589 on epoch=29
06/02/2022 02:16:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.394602 on epoch=30
06/02/2022 02:16:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.460422 on epoch=31
06/02/2022 02:16:51 - INFO - __main__ - Global step 500 Train loss 0.498299 Classification-F1 0.7209137126243125 on epoch=31
06/02/2022 02:16:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.344082 on epoch=31
06/02/2022 02:17:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.388467 on epoch=32
06/02/2022 02:17:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.368443 on epoch=33
06/02/2022 02:17:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.261331 on epoch=33
06/02/2022 02:17:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.286463 on epoch=34
06/02/2022 02:17:20 - INFO - __main__ - Global step 550 Train loss 0.329757 Classification-F1 0.693702943000337 on epoch=34
06/02/2022 02:17:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.371121 on epoch=34
06/02/2022 02:17:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.417453 on epoch=35
06/02/2022 02:17:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.366487 on epoch=36
06/02/2022 02:17:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.295810 on epoch=36
06/02/2022 02:17:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.234855 on epoch=37
06/02/2022 02:17:47 - INFO - __main__ - Global step 600 Train loss 0.337145 Classification-F1 0.7667408581712594 on epoch=37
06/02/2022 02:17:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.279758 on epoch=38
06/02/2022 02:17:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.171240 on epoch=38
06/02/2022 02:18:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.176801 on epoch=39
06/02/2022 02:18:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.165392 on epoch=39
06/02/2022 02:18:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.164036 on epoch=40
06/02/2022 02:18:16 - INFO - __main__ - Global step 650 Train loss 0.191445 Classification-F1 0.7111435548033912 on epoch=40
06/02/2022 02:18:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.161602 on epoch=41
06/02/2022 02:18:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.110146 on epoch=41
06/02/2022 02:18:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.129698 on epoch=42
06/02/2022 02:18:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.130210 on epoch=43
06/02/2022 02:18:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.138023 on epoch=43
06/02/2022 02:18:43 - INFO - __main__ - Global step 700 Train loss 0.133936 Classification-F1 0.7579585363355226 on epoch=43
06/02/2022 02:18:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.096316 on epoch=44
06/02/2022 02:18:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.075830 on epoch=44
06/02/2022 02:18:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.081464 on epoch=45
06/02/2022 02:19:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.109320 on epoch=46
06/02/2022 02:19:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.127189 on epoch=46
06/02/2022 02:19:11 - INFO - __main__ - Global step 750 Train loss 0.098024 Classification-F1 0.7681271076902145 on epoch=46
06/02/2022 02:19:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.044611 on epoch=47
06/02/2022 02:19:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.063599 on epoch=48
06/02/2022 02:19:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.044970 on epoch=48
06/02/2022 02:19:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.031841 on epoch=49
06/02/2022 02:19:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.080733 on epoch=49
06/02/2022 02:19:40 - INFO - __main__ - Global step 800 Train loss 0.053151 Classification-F1 0.7839480264673764 on epoch=49
06/02/2022 02:19:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.026915 on epoch=50
06/02/2022 02:19:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.019605 on epoch=51
06/02/2022 02:19:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.078924 on epoch=51
06/02/2022 02:20:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.043903 on epoch=52
06/02/2022 02:20:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.072951 on epoch=53
06/02/2022 02:20:08 - INFO - __main__ - Global step 850 Train loss 0.048460 Classification-F1 0.7469108620766427 on epoch=53
06/02/2022 02:20:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.016609 on epoch=53
06/02/2022 02:20:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.005833 on epoch=54
06/02/2022 02:20:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.042238 on epoch=54
06/02/2022 02:20:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.022344 on epoch=55
06/02/2022 02:20:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.031318 on epoch=56
06/02/2022 02:20:36 - INFO - __main__ - Global step 900 Train loss 0.023668 Classification-F1 0.7874692013060997 on epoch=56
06/02/2022 02:20:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.013116 on epoch=56
06/02/2022 02:20:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.027608 on epoch=57
06/02/2022 02:20:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.021880 on epoch=58
06/02/2022 02:20:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.030561 on epoch=58
06/02/2022 02:21:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.020632 on epoch=59
06/02/2022 02:21:04 - INFO - __main__ - Global step 950 Train loss 0.022759 Classification-F1 0.750181183099793 on epoch=59
06/02/2022 02:21:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.009480 on epoch=59
06/02/2022 02:21:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.020782 on epoch=60
06/02/2022 02:21:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.019585 on epoch=61
06/02/2022 02:21:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.023198 on epoch=61
06/02/2022 02:21:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.017007 on epoch=62
06/02/2022 02:21:32 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:21:32 - INFO - __main__ - Printing 3 examples
06/02/2022 02:21:32 - INFO - __main__ -  [emo] how cause yes am listening
06/02/2022 02:21:32 - INFO - __main__ - ['others']
06/02/2022 02:21:32 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/02/2022 02:21:32 - INFO - __main__ - ['others']
06/02/2022 02:21:32 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/02/2022 02:21:32 - INFO - __main__ - ['others']
06/02/2022 02:21:32 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:21:32 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:21:32 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 02:21:32 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:21:32 - INFO - __main__ - Printing 3 examples
06/02/2022 02:21:32 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
06/02/2022 02:21:32 - INFO - __main__ - ['others']
06/02/2022 02:21:32 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
06/02/2022 02:21:32 - INFO - __main__ - ['others']
06/02/2022 02:21:32 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
06/02/2022 02:21:32 - INFO - __main__ - ['others']
06/02/2022 02:21:32 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:21:32 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:21:32 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 02:21:32 - INFO - __main__ - Global step 1000 Train loss 0.018010 Classification-F1 0.7853470377417981 on epoch=62
06/02/2022 02:21:32 - INFO - __main__ - save last model!
06/02/2022 02:21:39 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 02:21:40 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 02:21:40 - INFO - __main__ - Printing 3 examples
06/02/2022 02:21:40 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 02:21:40 - INFO - __main__ - ['others']
06/02/2022 02:21:40 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 02:21:40 - INFO - __main__ - ['others']
06/02/2022 02:21:40 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 02:21:40 - INFO - __main__ - ['others']
06/02/2022 02:21:40 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:21:42 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:21:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 02:21:45 - INFO - __main__ - Starting training!
06/02/2022 02:21:47 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 02:22:29 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_100_0.0003_8_predictions.txt
06/02/2022 02:22:29 - INFO - __main__ - Classification-F1 on test data: 0.5018
06/02/2022 02:22:29 - INFO - __main__ - prefix=emo_64_100, lr=0.0003, bsz=8, dev_performance=0.7874692013060997, test_performance=0.5017799747792555
06/02/2022 02:22:29 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.0002, bsz=8 ...
06/02/2022 02:22:30 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:22:30 - INFO - __main__ - Printing 3 examples
06/02/2022 02:22:30 - INFO - __main__ -  [emo] how cause yes am listening
06/02/2022 02:22:30 - INFO - __main__ - ['others']
06/02/2022 02:22:30 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/02/2022 02:22:30 - INFO - __main__ - ['others']
06/02/2022 02:22:30 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/02/2022 02:22:30 - INFO - __main__ - ['others']
06/02/2022 02:22:30 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:22:30 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:22:30 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 02:22:30 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:22:30 - INFO - __main__ - Printing 3 examples
06/02/2022 02:22:30 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
06/02/2022 02:22:30 - INFO - __main__ - ['others']
06/02/2022 02:22:30 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
06/02/2022 02:22:30 - INFO - __main__ - ['others']
06/02/2022 02:22:30 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
06/02/2022 02:22:30 - INFO - __main__ - ['others']
06/02/2022 02:22:30 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:22:30 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:22:31 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 02:22:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 02:22:43 - INFO - __main__ - Starting training!
06/02/2022 02:22:48 - INFO - __main__ - Step 10 Global step 10 Train loss 24.259380 on epoch=0
06/02/2022 02:22:53 - INFO - __main__ - Step 20 Global step 20 Train loss 20.262678 on epoch=1
06/02/2022 02:22:58 - INFO - __main__ - Step 30 Global step 30 Train loss 18.146648 on epoch=1
06/02/2022 02:23:03 - INFO - __main__ - Step 40 Global step 40 Train loss 17.376810 on epoch=2
06/02/2022 02:23:08 - INFO - __main__ - Step 50 Global step 50 Train loss 15.770082 on epoch=3
06/02/2022 02:23:35 - INFO - __main__ - Global step 50 Train loss 19.163120 Classification-F1 0.0 on epoch=3
06/02/2022 02:23:41 - INFO - __main__ - Step 60 Global step 60 Train loss 15.261464 on epoch=3
06/02/2022 02:23:46 - INFO - __main__ - Step 70 Global step 70 Train loss 15.219088 on epoch=4
06/02/2022 02:23:51 - INFO - __main__ - Step 80 Global step 80 Train loss 14.201574 on epoch=4
06/02/2022 02:23:56 - INFO - __main__ - Step 90 Global step 90 Train loss 14.593809 on epoch=5
06/02/2022 02:24:02 - INFO - __main__ - Step 100 Global step 100 Train loss 13.132225 on epoch=6
06/02/2022 02:24:07 - INFO - __main__ - Global step 100 Train loss 14.481632 Classification-F1 0.0 on epoch=6
06/02/2022 02:24:13 - INFO - __main__ - Step 110 Global step 110 Train loss 12.041920 on epoch=6
06/02/2022 02:24:18 - INFO - __main__ - Step 120 Global step 120 Train loss 12.171981 on epoch=7
06/02/2022 02:24:23 - INFO - __main__ - Step 130 Global step 130 Train loss 10.971233 on epoch=8
06/02/2022 02:24:28 - INFO - __main__ - Step 140 Global step 140 Train loss 10.983569 on epoch=8
06/02/2022 02:24:34 - INFO - __main__ - Step 150 Global step 150 Train loss 8.753207 on epoch=9
06/02/2022 02:24:38 - INFO - __main__ - Global step 150 Train loss 10.984383 Classification-F1 0.0 on epoch=9
06/02/2022 02:24:44 - INFO - __main__ - Step 160 Global step 160 Train loss 7.499645 on epoch=9
06/02/2022 02:24:49 - INFO - __main__ - Step 170 Global step 170 Train loss 6.132354 on epoch=10
06/02/2022 02:24:54 - INFO - __main__ - Step 180 Global step 180 Train loss 5.183580 on epoch=11
06/02/2022 02:24:59 - INFO - __main__ - Step 190 Global step 190 Train loss 4.110593 on epoch=11
06/02/2022 02:25:04 - INFO - __main__ - Step 200 Global step 200 Train loss 3.793547 on epoch=12
06/02/2022 02:25:06 - INFO - __main__ - Global step 200 Train loss 5.343945 Classification-F1 0.1542373403583368 on epoch=12
06/02/2022 02:25:13 - INFO - __main__ - Step 210 Global step 210 Train loss 3.814668 on epoch=13
06/02/2022 02:25:18 - INFO - __main__ - Step 220 Global step 220 Train loss 5.200219 on epoch=13
06/02/2022 02:25:23 - INFO - __main__ - Step 230 Global step 230 Train loss 3.943564 on epoch=14
06/02/2022 02:25:28 - INFO - __main__ - Step 240 Global step 240 Train loss 3.608069 on epoch=14
06/02/2022 02:25:34 - INFO - __main__ - Step 250 Global step 250 Train loss 3.106805 on epoch=15
06/02/2022 02:25:36 - INFO - __main__ - Global step 250 Train loss 3.934665 Classification-F1 0.189527351918532 on epoch=15
06/02/2022 02:25:42 - INFO - __main__ - Step 260 Global step 260 Train loss 2.563434 on epoch=16
06/02/2022 02:25:48 - INFO - __main__ - Step 270 Global step 270 Train loss 1.561210 on epoch=16
06/02/2022 02:25:53 - INFO - __main__ - Step 280 Global step 280 Train loss 1.088850 on epoch=17
06/02/2022 02:25:58 - INFO - __main__ - Step 290 Global step 290 Train loss 1.054593 on epoch=18
06/02/2022 02:26:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.822249 on epoch=18
06/02/2022 02:26:05 - INFO - __main__ - Global step 300 Train loss 1.418067 Classification-F1 0.21260149019391059 on epoch=18
06/02/2022 02:26:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.861197 on epoch=19
06/02/2022 02:26:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.885349 on epoch=19
06/02/2022 02:26:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.825816 on epoch=20
06/02/2022 02:26:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.764232 on epoch=21
06/02/2022 02:26:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.856474 on epoch=21
06/02/2022 02:26:34 - INFO - __main__ - Global step 350 Train loss 0.838614 Classification-F1 0.251912752558488 on epoch=21
06/02/2022 02:26:41 - INFO - __main__ - Step 360 Global step 360 Train loss 1.060875 on epoch=22
06/02/2022 02:26:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.915288 on epoch=23
06/02/2022 02:26:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.766306 on epoch=23
06/02/2022 02:26:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.773620 on epoch=24
06/02/2022 02:27:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.753749 on epoch=24
06/02/2022 02:27:03 - INFO - __main__ - Global step 400 Train loss 0.853968 Classification-F1 0.26592090309697664 on epoch=24
06/02/2022 02:27:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.717594 on epoch=25
06/02/2022 02:27:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.736470 on epoch=26
06/02/2022 02:27:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.692610 on epoch=26
06/02/2022 02:27:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.718993 on epoch=27
06/02/2022 02:27:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.754094 on epoch=28
06/02/2022 02:27:32 - INFO - __main__ - Global step 450 Train loss 0.723952 Classification-F1 0.37133455683660604 on epoch=28
06/02/2022 02:27:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.758085 on epoch=28
06/02/2022 02:27:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.685448 on epoch=29
06/02/2022 02:27:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.687988 on epoch=29
06/02/2022 02:27:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.628004 on epoch=30
06/02/2022 02:28:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.654441 on epoch=31
06/02/2022 02:28:02 - INFO - __main__ - Global step 500 Train loss 0.682793 Classification-F1 0.45359600320230237 on epoch=31
06/02/2022 02:28:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.608209 on epoch=31
06/02/2022 02:28:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.699797 on epoch=32
06/02/2022 02:28:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.581946 on epoch=33
06/02/2022 02:28:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.584051 on epoch=33
06/02/2022 02:28:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.752567 on epoch=34
06/02/2022 02:28:31 - INFO - __main__ - Global step 550 Train loss 0.645314 Classification-F1 0.4443292888439501 on epoch=34
06/02/2022 02:28:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.638807 on epoch=34
06/02/2022 02:28:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.626138 on epoch=35
06/02/2022 02:28:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.609977 on epoch=36
06/02/2022 02:28:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.525688 on epoch=36
06/02/2022 02:28:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.608478 on epoch=37
06/02/2022 02:28:59 - INFO - __main__ - Global step 600 Train loss 0.601817 Classification-F1 0.45884867808177277 on epoch=37
06/02/2022 02:29:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.544724 on epoch=38
06/02/2022 02:29:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.570411 on epoch=38
06/02/2022 02:29:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.589833 on epoch=39
06/02/2022 02:29:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.548670 on epoch=39
06/02/2022 02:29:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.486399 on epoch=40
06/02/2022 02:29:28 - INFO - __main__ - Global step 650 Train loss 0.548007 Classification-F1 0.5059149623890387 on epoch=40
06/02/2022 02:29:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.550476 on epoch=41
06/02/2022 02:29:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.453042 on epoch=41
06/02/2022 02:29:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.593327 on epoch=42
06/02/2022 02:29:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.528361 on epoch=43
06/02/2022 02:29:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.503809 on epoch=43
06/02/2022 02:29:57 - INFO - __main__ - Global step 700 Train loss 0.525803 Classification-F1 0.44459872274407963 on epoch=43
06/02/2022 02:30:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.466827 on epoch=44
06/02/2022 02:30:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.624481 on epoch=44
06/02/2022 02:30:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.481953 on epoch=45
06/02/2022 02:30:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.482149 on epoch=46
06/02/2022 02:30:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.477833 on epoch=46
06/02/2022 02:30:25 - INFO - __main__ - Global step 750 Train loss 0.506649 Classification-F1 0.4666121249883362 on epoch=46
06/02/2022 02:30:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.555598 on epoch=47
06/02/2022 02:30:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.472939 on epoch=48
06/02/2022 02:30:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.499009 on epoch=48
06/02/2022 02:30:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.574535 on epoch=49
06/02/2022 02:30:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.512117 on epoch=49
06/02/2022 02:30:53 - INFO - __main__ - Global step 800 Train loss 0.522839 Classification-F1 0.5164298787462823 on epoch=49
06/02/2022 02:30:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.429772 on epoch=50
06/02/2022 02:31:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.421449 on epoch=51
06/02/2022 02:31:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.434698 on epoch=51
06/02/2022 02:31:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.440742 on epoch=52
06/02/2022 02:31:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.516279 on epoch=53
06/02/2022 02:31:22 - INFO - __main__ - Global step 850 Train loss 0.448588 Classification-F1 0.47586478418360095 on epoch=53
06/02/2022 02:31:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.374226 on epoch=53
06/02/2022 02:31:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.423778 on epoch=54
06/02/2022 02:31:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.404925 on epoch=54
06/02/2022 02:31:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.278975 on epoch=55
06/02/2022 02:31:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.283788 on epoch=56
06/02/2022 02:31:51 - INFO - __main__ - Global step 900 Train loss 0.353138 Classification-F1 0.6580973760158754 on epoch=56
06/02/2022 02:31:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.401820 on epoch=56
06/02/2022 02:32:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.383463 on epoch=57
06/02/2022 02:32:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.336309 on epoch=58
06/02/2022 02:32:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.357407 on epoch=58
06/02/2022 02:32:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.312432 on epoch=59
06/02/2022 02:32:20 - INFO - __main__ - Global step 950 Train loss 0.358286 Classification-F1 0.689155198223205 on epoch=59
06/02/2022 02:32:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.320844 on epoch=59
06/02/2022 02:32:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.298619 on epoch=60
06/02/2022 02:32:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.217630 on epoch=61
06/02/2022 02:32:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.198927 on epoch=61
06/02/2022 02:32:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.220765 on epoch=62
06/02/2022 02:32:47 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:32:47 - INFO - __main__ - Printing 3 examples
06/02/2022 02:32:47 - INFO - __main__ -  [emo] how cause yes am listening
06/02/2022 02:32:47 - INFO - __main__ - ['others']
06/02/2022 02:32:47 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/02/2022 02:32:47 - INFO - __main__ - ['others']
06/02/2022 02:32:47 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/02/2022 02:32:47 - INFO - __main__ - ['others']
06/02/2022 02:32:47 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:32:48 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:32:48 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 02:32:48 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:32:48 - INFO - __main__ - Printing 3 examples
06/02/2022 02:32:48 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
06/02/2022 02:32:48 - INFO - __main__ - ['others']
06/02/2022 02:32:48 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
06/02/2022 02:32:48 - INFO - __main__ - ['others']
06/02/2022 02:32:48 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
06/02/2022 02:32:48 - INFO - __main__ - ['others']
06/02/2022 02:32:48 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:32:48 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:32:48 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 02:32:49 - INFO - __main__ - Global step 1000 Train loss 0.251357 Classification-F1 0.704123925007546 on epoch=62
06/02/2022 02:32:50 - INFO - __main__ - save last model!
06/02/2022 02:32:57 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 02:32:58 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 02:32:58 - INFO - __main__ - Printing 3 examples
06/02/2022 02:32:58 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 02:32:58 - INFO - __main__ - ['others']
06/02/2022 02:32:58 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 02:32:58 - INFO - __main__ - ['others']
06/02/2022 02:32:58 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 02:32:58 - INFO - __main__ - ['others']
06/02/2022 02:32:58 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:32:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 02:32:59 - INFO - __main__ - Starting training!
06/02/2022 02:33:00 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:33:05 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 02:33:59 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_100_0.0002_8_predictions.txt
06/02/2022 02:33:59 - INFO - __main__ - Classification-F1 on test data: 0.0741
06/02/2022 02:34:00 - INFO - __main__ - prefix=emo_64_100, lr=0.0002, bsz=8, dev_performance=0.704123925007546, test_performance=0.0740882571150952
06/02/2022 02:34:00 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.0001, bsz=8 ...
06/02/2022 02:34:01 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:34:01 - INFO - __main__ - Printing 3 examples
06/02/2022 02:34:01 - INFO - __main__ -  [emo] how cause yes am listening
06/02/2022 02:34:01 - INFO - __main__ - ['others']
06/02/2022 02:34:01 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/02/2022 02:34:01 - INFO - __main__ - ['others']
06/02/2022 02:34:01 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/02/2022 02:34:01 - INFO - __main__ - ['others']
06/02/2022 02:34:01 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:34:01 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:34:01 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 02:34:01 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:34:01 - INFO - __main__ - Printing 3 examples
06/02/2022 02:34:01 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
06/02/2022 02:34:01 - INFO - __main__ - ['others']
06/02/2022 02:34:01 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
06/02/2022 02:34:01 - INFO - __main__ - ['others']
06/02/2022 02:34:01 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
06/02/2022 02:34:01 - INFO - __main__ - ['others']
06/02/2022 02:34:01 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:34:01 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:34:02 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 02:34:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 02:34:13 - INFO - __main__ - Starting training!
06/02/2022 02:34:17 - INFO - __main__ - Step 10 Global step 10 Train loss 26.016850 on epoch=0
06/02/2022 02:34:22 - INFO - __main__ - Step 20 Global step 20 Train loss 23.329931 on epoch=1
06/02/2022 02:34:27 - INFO - __main__ - Step 30 Global step 30 Train loss 18.909649 on epoch=1
06/02/2022 02:34:33 - INFO - __main__ - Step 40 Global step 40 Train loss 20.123283 on epoch=2
06/02/2022 02:34:38 - INFO - __main__ - Step 50 Global step 50 Train loss 18.237965 on epoch=3
06/02/2022 02:35:58 - INFO - __main__ - Global step 50 Train loss 21.323536 Classification-F1 0.0 on epoch=3
06/02/2022 02:36:04 - INFO - __main__ - Step 60 Global step 60 Train loss 17.626389 on epoch=3
06/02/2022 02:36:09 - INFO - __main__ - Step 70 Global step 70 Train loss 16.749477 on epoch=4
06/02/2022 02:36:14 - INFO - __main__ - Step 80 Global step 80 Train loss 17.769260 on epoch=4
06/02/2022 02:36:20 - INFO - __main__ - Step 90 Global step 90 Train loss 15.923800 on epoch=5
06/02/2022 02:36:25 - INFO - __main__ - Step 100 Global step 100 Train loss 17.160311 on epoch=6
06/02/2022 02:37:29 - INFO - __main__ - Global step 100 Train loss 17.045847 Classification-F1 0.0 on epoch=6
06/02/2022 02:37:34 - INFO - __main__ - Step 110 Global step 110 Train loss 16.255478 on epoch=6
06/02/2022 02:37:40 - INFO - __main__ - Step 120 Global step 120 Train loss 14.871508 on epoch=7
06/02/2022 02:37:45 - INFO - __main__ - Step 130 Global step 130 Train loss 14.897444 on epoch=8
06/02/2022 02:37:50 - INFO - __main__ - Step 140 Global step 140 Train loss 15.206505 on epoch=8
06/02/2022 02:37:55 - INFO - __main__ - Step 150 Global step 150 Train loss 14.946211 on epoch=9
06/02/2022 02:38:27 - INFO - __main__ - Global step 150 Train loss 15.235429 Classification-F1 0.0003046458492003047 on epoch=9
06/02/2022 02:38:34 - INFO - __main__ - Step 160 Global step 160 Train loss 14.529986 on epoch=9
06/02/2022 02:38:39 - INFO - __main__ - Step 170 Global step 170 Train loss 13.526570 on epoch=10
06/02/2022 02:38:44 - INFO - __main__ - Step 180 Global step 180 Train loss 13.457861 on epoch=11
06/02/2022 02:38:50 - INFO - __main__ - Step 190 Global step 190 Train loss 13.318426 on epoch=11
06/02/2022 02:38:55 - INFO - __main__ - Step 200 Global step 200 Train loss 12.887962 on epoch=12
06/02/2022 02:39:14 - INFO - __main__ - Global step 200 Train loss 13.544160 Classification-F1 0.0 on epoch=12
06/02/2022 02:39:19 - INFO - __main__ - Step 210 Global step 210 Train loss 12.766795 on epoch=13
06/02/2022 02:39:24 - INFO - __main__ - Step 220 Global step 220 Train loss 12.124357 on epoch=13
06/02/2022 02:39:30 - INFO - __main__ - Step 230 Global step 230 Train loss 12.122049 on epoch=14
06/02/2022 02:39:35 - INFO - __main__ - Step 240 Global step 240 Train loss 12.334086 on epoch=14
06/02/2022 02:39:40 - INFO - __main__ - Step 250 Global step 250 Train loss 11.306513 on epoch=15
06/02/2022 02:39:58 - INFO - __main__ - Global step 250 Train loss 12.130758 Classification-F1 0.000591715976331361 on epoch=15
06/02/2022 02:40:04 - INFO - __main__ - Step 260 Global step 260 Train loss 10.703829 on epoch=16
06/02/2022 02:40:09 - INFO - __main__ - Step 270 Global step 270 Train loss 10.767299 on epoch=16
06/02/2022 02:40:15 - INFO - __main__ - Step 280 Global step 280 Train loss 10.295036 on epoch=17
06/02/2022 02:40:20 - INFO - __main__ - Step 290 Global step 290 Train loss 10.056129 on epoch=18
06/02/2022 02:40:25 - INFO - __main__ - Step 300 Global step 300 Train loss 9.665442 on epoch=18
06/02/2022 02:40:36 - INFO - __main__ - Global step 300 Train loss 10.297547 Classification-F1 0.0008316008316008316 on epoch=18
06/02/2022 02:40:42 - INFO - __main__ - Step 310 Global step 310 Train loss 9.099463 on epoch=19
06/02/2022 02:40:47 - INFO - __main__ - Step 320 Global step 320 Train loss 8.261552 on epoch=19
06/02/2022 02:40:53 - INFO - __main__ - Step 330 Global step 330 Train loss 7.974423 on epoch=20
06/02/2022 02:40:58 - INFO - __main__ - Step 340 Global step 340 Train loss 6.316567 on epoch=21
06/02/2022 02:41:03 - INFO - __main__ - Step 350 Global step 350 Train loss 5.126303 on epoch=21
06/02/2022 02:41:06 - INFO - __main__ - Global step 350 Train loss 7.355660 Classification-F1 0.0018099547511312218 on epoch=21
06/02/2022 02:41:12 - INFO - __main__ - Step 360 Global step 360 Train loss 5.409546 on epoch=22
06/02/2022 02:41:18 - INFO - __main__ - Step 370 Global step 370 Train loss 4.304748 on epoch=23
06/02/2022 02:41:23 - INFO - __main__ - Step 380 Global step 380 Train loss 4.830681 on epoch=23
06/02/2022 02:41:28 - INFO - __main__ - Step 390 Global step 390 Train loss 4.499796 on epoch=24
06/02/2022 02:41:34 - INFO - __main__ - Step 400 Global step 400 Train loss 4.560407 on epoch=24
06/02/2022 02:41:36 - INFO - __main__ - Global step 400 Train loss 4.721036 Classification-F1 0.14944695827048768 on epoch=24
06/02/2022 02:41:42 - INFO - __main__ - Step 410 Global step 410 Train loss 3.734077 on epoch=25
06/02/2022 02:41:48 - INFO - __main__ - Step 420 Global step 420 Train loss 3.691965 on epoch=26
06/02/2022 02:41:53 - INFO - __main__ - Step 430 Global step 430 Train loss 4.869233 on epoch=26
06/02/2022 02:41:58 - INFO - __main__ - Step 440 Global step 440 Train loss 3.624187 on epoch=27
06/02/2022 02:42:04 - INFO - __main__ - Step 450 Global step 450 Train loss 2.735876 on epoch=28
06/02/2022 02:42:06 - INFO - __main__ - Global step 450 Train loss 3.731068 Classification-F1 0.27338413839109166 on epoch=28
06/02/2022 02:42:12 - INFO - __main__ - Step 460 Global step 460 Train loss 2.711960 on epoch=28
06/02/2022 02:42:17 - INFO - __main__ - Step 470 Global step 470 Train loss 2.005733 on epoch=29
06/02/2022 02:42:23 - INFO - __main__ - Step 480 Global step 480 Train loss 1.415705 on epoch=29
06/02/2022 02:42:28 - INFO - __main__ - Step 490 Global step 490 Train loss 2.280619 on epoch=30
06/02/2022 02:42:33 - INFO - __main__ - Step 500 Global step 500 Train loss 2.107267 on epoch=31
06/02/2022 02:42:35 - INFO - __main__ - Global step 500 Train loss 2.104257 Classification-F1 0.48838141025641024 on epoch=31
06/02/2022 02:42:41 - INFO - __main__ - Step 510 Global step 510 Train loss 1.502867 on epoch=31
06/02/2022 02:42:46 - INFO - __main__ - Step 520 Global step 520 Train loss 1.397333 on epoch=32
06/02/2022 02:42:52 - INFO - __main__ - Step 530 Global step 530 Train loss 1.327893 on epoch=33
06/02/2022 02:42:57 - INFO - __main__ - Step 540 Global step 540 Train loss 1.017830 on epoch=33
06/02/2022 02:43:02 - INFO - __main__ - Step 550 Global step 550 Train loss 1.436459 on epoch=34
06/02/2022 02:43:04 - INFO - __main__ - Global step 550 Train loss 1.336477 Classification-F1 0.6677014911958776 on epoch=34
06/02/2022 02:43:10 - INFO - __main__ - Step 560 Global step 560 Train loss 1.048288 on epoch=34
06/02/2022 02:43:16 - INFO - __main__ - Step 570 Global step 570 Train loss 1.191547 on epoch=35
06/02/2022 02:43:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.935196 on epoch=36
06/02/2022 02:43:26 - INFO - __main__ - Step 590 Global step 590 Train loss 1.106464 on epoch=36
06/02/2022 02:43:31 - INFO - __main__ - Step 600 Global step 600 Train loss 1.061521 on epoch=37
06/02/2022 02:43:34 - INFO - __main__ - Global step 600 Train loss 1.068603 Classification-F1 0.6095615703092339 on epoch=37
06/02/2022 02:43:39 - INFO - __main__ - Step 610 Global step 610 Train loss 1.055291 on epoch=38
06/02/2022 02:43:44 - INFO - __main__ - Step 620 Global step 620 Train loss 1.110305 on epoch=38
06/02/2022 02:43:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.862396 on epoch=39
06/02/2022 02:43:55 - INFO - __main__ - Step 640 Global step 640 Train loss 1.277313 on epoch=39
06/02/2022 02:44:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.880314 on epoch=40
06/02/2022 02:44:02 - INFO - __main__ - Global step 650 Train loss 1.037124 Classification-F1 0.6883335203647704 on epoch=40
06/02/2022 02:44:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.641718 on epoch=41
06/02/2022 02:44:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.891408 on epoch=41
06/02/2022 02:44:19 - INFO - __main__ - Step 680 Global step 680 Train loss 1.048847 on epoch=42
06/02/2022 02:44:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.988196 on epoch=43
06/02/2022 02:44:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.872799 on epoch=43
06/02/2022 02:44:32 - INFO - __main__ - Global step 700 Train loss 0.888594 Classification-F1 0.6116239779548096 on epoch=43
06/02/2022 02:44:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.944417 on epoch=44
06/02/2022 02:44:42 - INFO - __main__ - Step 720 Global step 720 Train loss 1.022389 on epoch=44
06/02/2022 02:44:48 - INFO - __main__ - Step 730 Global step 730 Train loss 1.023816 on epoch=45
06/02/2022 02:44:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.916609 on epoch=46
06/02/2022 02:44:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.795582 on epoch=46
06/02/2022 02:45:00 - INFO - __main__ - Global step 750 Train loss 0.940562 Classification-F1 0.4994074521504984 on epoch=46
06/02/2022 02:45:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.728548 on epoch=47
06/02/2022 02:45:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.503873 on epoch=48
06/02/2022 02:45:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.724063 on epoch=48
06/02/2022 02:45:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.559482 on epoch=49
06/02/2022 02:45:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.751034 on epoch=49
06/02/2022 02:45:29 - INFO - __main__ - Global step 800 Train loss 0.653400 Classification-F1 0.6607047510079547 on epoch=49
06/02/2022 02:45:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.598775 on epoch=50
06/02/2022 02:45:39 - INFO - __main__ - Step 820 Global step 820 Train loss 1.017510 on epoch=51
06/02/2022 02:45:45 - INFO - __main__ - Step 830 Global step 830 Train loss 1.064674 on epoch=51
06/02/2022 02:45:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.715877 on epoch=52
06/02/2022 02:45:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.679916 on epoch=53
06/02/2022 02:45:57 - INFO - __main__ - Global step 850 Train loss 0.815350 Classification-F1 0.669515388360373 on epoch=53
06/02/2022 02:46:02 - INFO - __main__ - Step 860 Global step 860 Train loss 1.017561 on epoch=53
06/02/2022 02:46:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.552504 on epoch=54
06/02/2022 02:46:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.741926 on epoch=54
06/02/2022 02:46:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.624593 on epoch=55
06/02/2022 02:46:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.811808 on epoch=56
06/02/2022 02:46:26 - INFO - __main__ - Global step 900 Train loss 0.749679 Classification-F1 0.6693954155653585 on epoch=56
06/02/2022 02:46:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.461051 on epoch=56
06/02/2022 02:46:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.509479 on epoch=57
06/02/2022 02:46:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.410550 on epoch=58
06/02/2022 02:46:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.455555 on epoch=58
06/02/2022 02:46:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.374486 on epoch=59
06/02/2022 02:46:54 - INFO - __main__ - Global step 950 Train loss 0.442224 Classification-F1 0.7094757229793727 on epoch=59
06/02/2022 02:47:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.339406 on epoch=59
06/02/2022 02:47:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.306540 on epoch=60
06/02/2022 02:47:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.323218 on epoch=61
06/02/2022 02:47:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.410529 on epoch=61
06/02/2022 02:47:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.392628 on epoch=62
06/02/2022 02:47:22 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:47:22 - INFO - __main__ - Printing 3 examples
06/02/2022 02:47:22 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/02/2022 02:47:22 - INFO - __main__ - ['others']
06/02/2022 02:47:22 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/02/2022 02:47:22 - INFO - __main__ - ['others']
06/02/2022 02:47:22 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/02/2022 02:47:22 - INFO - __main__ - ['others']
06/02/2022 02:47:22 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:47:23 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:47:23 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 02:47:23 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:47:23 - INFO - __main__ - Printing 3 examples
06/02/2022 02:47:23 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
06/02/2022 02:47:23 - INFO - __main__ - ['others']
06/02/2022 02:47:23 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
06/02/2022 02:47:23 - INFO - __main__ - ['others']
06/02/2022 02:47:23 - INFO - __main__ -  [emo] why because you don't want to i want to
06/02/2022 02:47:23 - INFO - __main__ - ['others']
06/02/2022 02:47:23 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:47:23 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:47:23 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 02:47:23 - INFO - __main__ - Global step 1000 Train loss 0.354464 Classification-F1 0.7239037765353555 on epoch=62
06/02/2022 02:47:24 - INFO - __main__ - save last model!
06/02/2022 02:47:32 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 02:47:32 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 02:47:32 - INFO - __main__ - Printing 3 examples
06/02/2022 02:47:32 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 02:47:32 - INFO - __main__ - ['others']
06/02/2022 02:47:32 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 02:47:32 - INFO - __main__ - ['others']
06/02/2022 02:47:32 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 02:47:32 - INFO - __main__ - ['others']
06/02/2022 02:47:32 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:47:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 02:47:34 - INFO - __main__ - Starting training!
06/02/2022 02:47:34 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:47:39 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 02:48:25 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_100_0.0001_8_predictions.txt
06/02/2022 02:48:25 - INFO - __main__ - Classification-F1 on test data: 0.1406
06/02/2022 02:48:26 - INFO - __main__ - prefix=emo_64_100, lr=0.0001, bsz=8, dev_performance=0.7239037765353555, test_performance=0.14064919380513677
06/02/2022 02:48:26 - INFO - __main__ - Running ... prefix=emo_64_13, lr=0.0005, bsz=8 ...
06/02/2022 02:48:27 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:48:27 - INFO - __main__ - Printing 3 examples
06/02/2022 02:48:27 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/02/2022 02:48:27 - INFO - __main__ - ['others']
06/02/2022 02:48:27 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/02/2022 02:48:27 - INFO - __main__ - ['others']
06/02/2022 02:48:27 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/02/2022 02:48:27 - INFO - __main__ - ['others']
06/02/2022 02:48:27 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:48:27 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:48:27 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 02:48:27 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:48:27 - INFO - __main__ - Printing 3 examples
06/02/2022 02:48:27 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
06/02/2022 02:48:27 - INFO - __main__ - ['others']
06/02/2022 02:48:27 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
06/02/2022 02:48:27 - INFO - __main__ - ['others']
06/02/2022 02:48:27 - INFO - __main__ -  [emo] why because you don't want to i want to
06/02/2022 02:48:27 - INFO - __main__ - ['others']
06/02/2022 02:48:27 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:48:27 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:48:28 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 02:48:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 02:48:38 - INFO - __main__ - Starting training!
06/02/2022 02:48:43 - INFO - __main__ - Step 10 Global step 10 Train loss 23.121954 on epoch=0
06/02/2022 02:48:48 - INFO - __main__ - Step 20 Global step 20 Train loss 19.149492 on epoch=1
06/02/2022 02:48:53 - INFO - __main__ - Step 30 Global step 30 Train loss 15.089144 on epoch=1
06/02/2022 02:48:58 - INFO - __main__ - Step 40 Global step 40 Train loss 14.568077 on epoch=2
06/02/2022 02:49:04 - INFO - __main__ - Step 50 Global step 50 Train loss 11.792636 on epoch=3
06/02/2022 02:49:09 - INFO - __main__ - Global step 50 Train loss 16.744261 Classification-F1 0.0 on epoch=3
06/02/2022 02:49:15 - INFO - __main__ - Step 60 Global step 60 Train loss 11.217100 on epoch=3
06/02/2022 02:49:20 - INFO - __main__ - Step 70 Global step 70 Train loss 8.914217 on epoch=4
06/02/2022 02:49:25 - INFO - __main__ - Step 80 Global step 80 Train loss 5.454024 on epoch=4
06/02/2022 02:49:31 - INFO - __main__ - Step 90 Global step 90 Train loss 4.079241 on epoch=5
06/02/2022 02:49:36 - INFO - __main__ - Step 100 Global step 100 Train loss 3.437428 on epoch=6
06/02/2022 02:49:38 - INFO - __main__ - Global step 100 Train loss 6.620401 Classification-F1 0.17025233878355495 on epoch=6
06/02/2022 02:49:44 - INFO - __main__ - Step 110 Global step 110 Train loss 3.222337 on epoch=6
06/02/2022 02:49:50 - INFO - __main__ - Step 120 Global step 120 Train loss 2.272999 on epoch=7
06/02/2022 02:49:55 - INFO - __main__ - Step 130 Global step 130 Train loss 2.025043 on epoch=8
06/02/2022 02:50:00 - INFO - __main__ - Step 140 Global step 140 Train loss 1.975649 on epoch=8
06/02/2022 02:50:05 - INFO - __main__ - Step 150 Global step 150 Train loss 2.032290 on epoch=9
06/02/2022 02:50:08 - INFO - __main__ - Global step 150 Train loss 2.305663 Classification-F1 0.18525769745649265 on epoch=9
06/02/2022 02:50:14 - INFO - __main__ - Step 160 Global step 160 Train loss 1.106453 on epoch=9
06/02/2022 02:50:19 - INFO - __main__ - Step 170 Global step 170 Train loss 1.043831 on epoch=10
06/02/2022 02:50:24 - INFO - __main__ - Step 180 Global step 180 Train loss 0.992598 on epoch=11
06/02/2022 02:50:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.874873 on epoch=11
06/02/2022 02:50:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.998443 on epoch=12
06/02/2022 02:50:37 - INFO - __main__ - Global step 200 Train loss 1.003240 Classification-F1 0.1 on epoch=12
06/02/2022 02:50:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.986824 on epoch=13
06/02/2022 02:50:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.930781 on epoch=13
06/02/2022 02:50:53 - INFO - __main__ - Step 230 Global step 230 Train loss 1.058663 on epoch=14
06/02/2022 02:50:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.801513 on epoch=14
06/02/2022 02:51:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.925859 on epoch=15
06/02/2022 02:51:05 - INFO - __main__ - Global step 250 Train loss 0.940728 Classification-F1 0.1 on epoch=15
06/02/2022 02:51:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.938216 on epoch=16
06/02/2022 02:51:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.874075 on epoch=16
06/02/2022 02:51:21 - INFO - __main__ - Step 280 Global step 280 Train loss 2.600598 on epoch=17
06/02/2022 02:51:26 - INFO - __main__ - Step 290 Global step 290 Train loss 1.160557 on epoch=18
06/02/2022 02:51:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.830023 on epoch=18
06/02/2022 02:51:33 - INFO - __main__ - Global step 300 Train loss 1.280694 Classification-F1 0.20857641090678503 on epoch=18
06/02/2022 02:51:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.831884 on epoch=19
06/02/2022 02:51:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.861708 on epoch=19
06/02/2022 02:51:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.817661 on epoch=20
06/02/2022 02:51:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.831686 on epoch=21
06/02/2022 02:52:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.803965 on epoch=21
06/02/2022 02:52:02 - INFO - __main__ - Global step 350 Train loss 0.829381 Classification-F1 0.10800578731613214 on epoch=21
06/02/2022 02:52:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.827636 on epoch=22
06/02/2022 02:52:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.809050 on epoch=23
06/02/2022 02:52:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.778146 on epoch=23
06/02/2022 02:52:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.766746 on epoch=24
06/02/2022 02:52:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.762165 on epoch=24
06/02/2022 02:52:31 - INFO - __main__ - Global step 400 Train loss 0.788749 Classification-F1 0.24254953449510624 on epoch=24
06/02/2022 02:52:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.769451 on epoch=25
06/02/2022 02:52:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.767918 on epoch=26
06/02/2022 02:52:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.673432 on epoch=26
06/02/2022 02:52:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.627165 on epoch=27
06/02/2022 02:52:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.710256 on epoch=28
06/02/2022 02:53:00 - INFO - __main__ - Global step 450 Train loss 0.709645 Classification-F1 0.489208542548936 on epoch=28
06/02/2022 02:53:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.656838 on epoch=28
06/02/2022 02:53:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.725672 on epoch=29
06/02/2022 02:53:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.557229 on epoch=29
06/02/2022 02:53:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.571700 on epoch=30
06/02/2022 02:53:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.545349 on epoch=31
06/02/2022 02:53:29 - INFO - __main__ - Global step 500 Train loss 0.611358 Classification-F1 0.5891791641139021 on epoch=31
06/02/2022 02:53:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.491566 on epoch=31
06/02/2022 02:53:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.604907 on epoch=32
06/02/2022 02:53:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.611418 on epoch=33
06/02/2022 02:53:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.450195 on epoch=33
06/02/2022 02:53:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.542682 on epoch=34
06/02/2022 02:53:58 - INFO - __main__ - Global step 550 Train loss 0.540154 Classification-F1 0.6288125814630314 on epoch=34
06/02/2022 02:54:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.381127 on epoch=34
06/02/2022 02:54:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.383184 on epoch=35
06/02/2022 02:54:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.406299 on epoch=36
06/02/2022 02:54:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.376864 on epoch=36
06/02/2022 02:54:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.501579 on epoch=37
06/02/2022 02:54:27 - INFO - __main__ - Global step 600 Train loss 0.409811 Classification-F1 0.6554162741129419 on epoch=37
06/02/2022 02:54:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.299461 on epoch=38
06/02/2022 02:54:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.360616 on epoch=38
06/02/2022 02:54:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.462273 on epoch=39
06/02/2022 02:54:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.717279 on epoch=39
06/02/2022 02:54:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.718214 on epoch=40
06/02/2022 02:54:56 - INFO - __main__ - Global step 650 Train loss 0.511568 Classification-F1 0.1 on epoch=40
06/02/2022 02:55:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.734629 on epoch=41
06/02/2022 02:55:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.766603 on epoch=41
06/02/2022 02:55:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.789177 on epoch=42
06/02/2022 02:55:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.707033 on epoch=43
06/02/2022 02:55:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.786364 on epoch=43
06/02/2022 02:55:24 - INFO - __main__ - Global step 700 Train loss 0.756761 Classification-F1 0.1 on epoch=43
06/02/2022 02:55:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.715620 on epoch=44
06/02/2022 02:55:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.709067 on epoch=44
06/02/2022 02:55:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.707004 on epoch=45
06/02/2022 02:55:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.748741 on epoch=46
06/02/2022 02:55:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.684232 on epoch=46
06/02/2022 02:55:52 - INFO - __main__ - Global step 750 Train loss 0.712933 Classification-F1 0.202959321380374 on epoch=46
06/02/2022 02:55:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.770392 on epoch=47
06/02/2022 02:56:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.725621 on epoch=48
06/02/2022 02:56:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.675228 on epoch=48
06/02/2022 02:56:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.697974 on epoch=49
06/02/2022 02:56:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.715560 on epoch=49
06/02/2022 02:56:20 - INFO - __main__ - Global step 800 Train loss 0.716955 Classification-F1 0.18132905039148767 on epoch=49
06/02/2022 02:56:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.615825 on epoch=50
06/02/2022 02:56:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.741489 on epoch=51
06/02/2022 02:56:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.697056 on epoch=51
06/02/2022 02:56:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.723211 on epoch=52
06/02/2022 02:56:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.735220 on epoch=53
06/02/2022 02:56:48 - INFO - __main__ - Global step 850 Train loss 0.702560 Classification-F1 0.1 on epoch=53
06/02/2022 02:56:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.718548 on epoch=53
06/02/2022 02:56:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.735306 on epoch=54
06/02/2022 02:57:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.722129 on epoch=54
06/02/2022 02:57:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.723692 on epoch=55
06/02/2022 02:57:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.728311 on epoch=56
06/02/2022 02:57:17 - INFO - __main__ - Global step 900 Train loss 0.725597 Classification-F1 0.1204701049748973 on epoch=56
06/02/2022 02:57:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.745174 on epoch=56
06/02/2022 02:57:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.755294 on epoch=57
06/02/2022 02:57:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.742298 on epoch=58
06/02/2022 02:57:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.715742 on epoch=58
06/02/2022 02:57:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.758765 on epoch=59
06/02/2022 02:57:45 - INFO - __main__ - Global step 950 Train loss 0.743455 Classification-F1 0.1569620253164557 on epoch=59
06/02/2022 02:57:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.657241 on epoch=59
06/02/2022 02:57:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.701752 on epoch=60
06/02/2022 02:58:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.750682 on epoch=61
06/02/2022 02:58:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.666149 on epoch=61
06/02/2022 02:58:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.714988 on epoch=62
06/02/2022 02:58:12 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:58:12 - INFO - __main__ - Printing 3 examples
06/02/2022 02:58:12 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/02/2022 02:58:12 - INFO - __main__ - ['others']
06/02/2022 02:58:12 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/02/2022 02:58:12 - INFO - __main__ - ['others']
06/02/2022 02:58:12 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/02/2022 02:58:12 - INFO - __main__ - ['others']
06/02/2022 02:58:12 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:58:12 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:58:13 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 02:58:13 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:58:13 - INFO - __main__ - Printing 3 examples
06/02/2022 02:58:13 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
06/02/2022 02:58:13 - INFO - __main__ - ['others']
06/02/2022 02:58:13 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
06/02/2022 02:58:13 - INFO - __main__ - ['others']
06/02/2022 02:58:13 - INFO - __main__ -  [emo] why because you don't want to i want to
06/02/2022 02:58:13 - INFO - __main__ - ['others']
06/02/2022 02:58:13 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:58:13 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:58:13 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 02:58:13 - INFO - __main__ - Global step 1000 Train loss 0.698163 Classification-F1 0.2029003253682259 on epoch=62
06/02/2022 02:58:13 - INFO - __main__ - save last model!
06/02/2022 02:58:21 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 02:58:21 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 02:58:22 - INFO - __main__ - Printing 3 examples
06/02/2022 02:58:22 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 02:58:22 - INFO - __main__ - ['others']
06/02/2022 02:58:22 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 02:58:22 - INFO - __main__ - ['others']
06/02/2022 02:58:22 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 02:58:22 - INFO - __main__ - ['others']
06/02/2022 02:58:22 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:58:24 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:58:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 02:58:26 - INFO - __main__ - Starting training!
06/02/2022 02:58:29 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 02:59:11 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_13_0.0005_8_predictions.txt
06/02/2022 02:59:11 - INFO - __main__ - Classification-F1 on test data: 0.4750
06/02/2022 02:59:11 - INFO - __main__ - prefix=emo_64_13, lr=0.0005, bsz=8, dev_performance=0.6554162741129419, test_performance=0.4749924090442152
06/02/2022 02:59:11 - INFO - __main__ - Running ... prefix=emo_64_13, lr=0.0003, bsz=8 ...
06/02/2022 02:59:12 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:59:12 - INFO - __main__ - Printing 3 examples
06/02/2022 02:59:12 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/02/2022 02:59:12 - INFO - __main__ - ['others']
06/02/2022 02:59:12 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/02/2022 02:59:12 - INFO - __main__ - ['others']
06/02/2022 02:59:12 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/02/2022 02:59:12 - INFO - __main__ - ['others']
06/02/2022 02:59:12 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:59:12 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:59:12 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 02:59:12 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 02:59:12 - INFO - __main__ - Printing 3 examples
06/02/2022 02:59:12 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
06/02/2022 02:59:12 - INFO - __main__ - ['others']
06/02/2022 02:59:12 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
06/02/2022 02:59:12 - INFO - __main__ - ['others']
06/02/2022 02:59:12 - INFO - __main__ -  [emo] why because you don't want to i want to
06/02/2022 02:59:12 - INFO - __main__ - ['others']
06/02/2022 02:59:12 - INFO - __main__ - Tokenizing Input ...
06/02/2022 02:59:12 - INFO - __main__ - Tokenizing Output ...
06/02/2022 02:59:13 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 02:59:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 02:59:25 - INFO - __main__ - Starting training!
06/02/2022 02:59:30 - INFO - __main__ - Step 10 Global step 10 Train loss 24.644348 on epoch=0
06/02/2022 02:59:35 - INFO - __main__ - Step 20 Global step 20 Train loss 18.944794 on epoch=1
06/02/2022 02:59:40 - INFO - __main__ - Step 30 Global step 30 Train loss 18.831791 on epoch=1
06/02/2022 02:59:45 - INFO - __main__ - Step 40 Global step 40 Train loss 16.613781 on epoch=2
06/02/2022 02:59:50 - INFO - __main__ - Step 50 Global step 50 Train loss 15.756540 on epoch=3
06/02/2022 02:59:53 - INFO - __main__ - Global step 50 Train loss 18.958252 Classification-F1 0.0 on epoch=3
06/02/2022 02:59:58 - INFO - __main__ - Step 60 Global step 60 Train loss 15.127573 on epoch=3
06/02/2022 03:00:04 - INFO - __main__ - Step 70 Global step 70 Train loss 13.876139 on epoch=4
06/02/2022 03:00:09 - INFO - __main__ - Step 80 Global step 80 Train loss 12.555402 on epoch=4
06/02/2022 03:00:14 - INFO - __main__ - Step 90 Global step 90 Train loss 12.377215 on epoch=5
06/02/2022 03:00:19 - INFO - __main__ - Step 100 Global step 100 Train loss 11.639135 on epoch=6
06/02/2022 03:00:22 - INFO - __main__ - Global step 100 Train loss 13.115093 Classification-F1 0.0 on epoch=6
06/02/2022 03:00:27 - INFO - __main__ - Step 110 Global step 110 Train loss 10.448495 on epoch=6
06/02/2022 03:00:32 - INFO - __main__ - Step 120 Global step 120 Train loss 8.248584 on epoch=7
06/02/2022 03:00:38 - INFO - __main__ - Step 130 Global step 130 Train loss 5.451242 on epoch=8
06/02/2022 03:00:43 - INFO - __main__ - Step 140 Global step 140 Train loss 4.968081 on epoch=8
06/02/2022 03:00:48 - INFO - __main__ - Step 150 Global step 150 Train loss 4.354984 on epoch=9
06/02/2022 03:00:50 - INFO - __main__ - Global step 150 Train loss 6.694277 Classification-F1 0.18491612065555726 on epoch=9
06/02/2022 03:00:56 - INFO - __main__ - Step 160 Global step 160 Train loss 2.769688 on epoch=9
06/02/2022 03:01:01 - INFO - __main__ - Step 170 Global step 170 Train loss 1.148418 on epoch=10
06/02/2022 03:01:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.870132 on epoch=11
06/02/2022 03:01:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.757795 on epoch=11
06/02/2022 03:01:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.685147 on epoch=12
06/02/2022 03:01:18 - INFO - __main__ - Global step 200 Train loss 1.246236 Classification-F1 0.5910548521582799 on epoch=12
06/02/2022 03:01:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.656347 on epoch=13
06/02/2022 03:01:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.543386 on epoch=13
06/02/2022 03:01:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.557127 on epoch=14
06/02/2022 03:01:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.577416 on epoch=14
06/02/2022 03:01:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.528919 on epoch=15
06/02/2022 03:01:47 - INFO - __main__ - Global step 250 Train loss 0.572639 Classification-F1 0.6641991608769081 on epoch=15
06/02/2022 03:01:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.647267 on epoch=16
06/02/2022 03:01:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.376129 on epoch=16
06/02/2022 03:02:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.516669 on epoch=17
06/02/2022 03:02:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.380590 on epoch=18
06/02/2022 03:02:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.420298 on epoch=18
06/02/2022 03:02:15 - INFO - __main__ - Global step 300 Train loss 0.468190 Classification-F1 0.6821420187793428 on epoch=18
06/02/2022 03:02:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.342507 on epoch=19
06/02/2022 03:02:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.284501 on epoch=19
06/02/2022 03:02:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.333887 on epoch=20
06/02/2022 03:02:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.315338 on epoch=21
06/02/2022 03:02:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.211774 on epoch=21
06/02/2022 03:02:44 - INFO - __main__ - Global step 350 Train loss 0.297601 Classification-F1 0.5645090473216076 on epoch=21
06/02/2022 03:02:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.422515 on epoch=22
06/02/2022 03:02:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.237860 on epoch=23
06/02/2022 03:02:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.325969 on epoch=23
06/02/2022 03:03:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.228593 on epoch=24
06/02/2022 03:03:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.251757 on epoch=24
06/02/2022 03:03:12 - INFO - __main__ - Global step 400 Train loss 0.293339 Classification-F1 0.5699766081871346 on epoch=24
06/02/2022 03:03:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.362283 on epoch=25
06/02/2022 03:03:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.298047 on epoch=26
06/02/2022 03:03:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.165019 on epoch=26
06/02/2022 03:03:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.197640 on epoch=27
06/02/2022 03:03:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.133234 on epoch=28
06/02/2022 03:03:40 - INFO - __main__ - Global step 450 Train loss 0.231245 Classification-F1 0.47265353683096384 on epoch=28
06/02/2022 03:03:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.135385 on epoch=28
06/02/2022 03:03:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.154443 on epoch=29
06/02/2022 03:03:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.107183 on epoch=29
06/02/2022 03:04:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.070540 on epoch=30
06/02/2022 03:04:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.169155 on epoch=31
06/02/2022 03:04:08 - INFO - __main__ - Global step 500 Train loss 0.127341 Classification-F1 0.506163676374664 on epoch=31
06/02/2022 03:04:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.075732 on epoch=31
06/02/2022 03:04:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.176943 on epoch=32
06/02/2022 03:04:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.073519 on epoch=33
06/02/2022 03:04:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.086785 on epoch=33
06/02/2022 03:04:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.043505 on epoch=34
06/02/2022 03:04:36 - INFO - __main__ - Global step 550 Train loss 0.091297 Classification-F1 0.5034042166685334 on epoch=34
06/02/2022 03:04:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.087659 on epoch=34
06/02/2022 03:04:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.077725 on epoch=35
06/02/2022 03:04:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.114340 on epoch=36
06/02/2022 03:04:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.080533 on epoch=36
06/02/2022 03:05:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.057409 on epoch=37
06/02/2022 03:05:05 - INFO - __main__ - Global step 600 Train loss 0.083533 Classification-F1 0.5170973266659032 on epoch=37
06/02/2022 03:05:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.054705 on epoch=38
06/02/2022 03:05:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.039466 on epoch=38
06/02/2022 03:05:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.056830 on epoch=39
06/02/2022 03:05:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.019035 on epoch=39
06/02/2022 03:05:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.086280 on epoch=40
06/02/2022 03:05:33 - INFO - __main__ - Global step 650 Train loss 0.051263 Classification-F1 0.5084515484944315 on epoch=40
06/02/2022 03:05:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.061327 on epoch=41
06/02/2022 03:05:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.068854 on epoch=41
06/02/2022 03:05:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.072981 on epoch=42
06/02/2022 03:05:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.078644 on epoch=43
06/02/2022 03:05:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.044431 on epoch=43
06/02/2022 03:06:01 - INFO - __main__ - Global step 700 Train loss 0.065248 Classification-F1 0.42901325429385934 on epoch=43
06/02/2022 03:06:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.019522 on epoch=44
06/02/2022 03:06:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.092537 on epoch=44
06/02/2022 03:06:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.019071 on epoch=45
06/02/2022 03:06:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.023764 on epoch=46
06/02/2022 03:06:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.009427 on epoch=46
06/02/2022 03:06:29 - INFO - __main__ - Global step 750 Train loss 0.032864 Classification-F1 0.49811489957169525 on epoch=46
06/02/2022 03:06:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.060937 on epoch=47
06/02/2022 03:06:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.020892 on epoch=48
06/02/2022 03:06:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.070336 on epoch=48
06/02/2022 03:06:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.010188 on epoch=49
06/02/2022 03:06:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.017785 on epoch=49
06/02/2022 03:06:58 - INFO - __main__ - Global step 800 Train loss 0.036028 Classification-F1 0.4240714392311835 on epoch=49
06/02/2022 03:07:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.027243 on epoch=50
06/02/2022 03:07:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.051036 on epoch=51
06/02/2022 03:07:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.005972 on epoch=51
06/02/2022 03:07:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.015996 on epoch=52
06/02/2022 03:07:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.008341 on epoch=53
06/02/2022 03:07:26 - INFO - __main__ - Global step 850 Train loss 0.021717 Classification-F1 0.3872386481178015 on epoch=53
06/02/2022 03:07:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.042867 on epoch=53
06/02/2022 03:07:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.007163 on epoch=54
06/02/2022 03:07:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.008582 on epoch=54
06/02/2022 03:07:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.011406 on epoch=55
06/02/2022 03:07:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.024761 on epoch=56
06/02/2022 03:07:53 - INFO - __main__ - Global step 900 Train loss 0.018956 Classification-F1 0.37478859882510995 on epoch=56
06/02/2022 03:07:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.032244 on epoch=56
06/02/2022 03:08:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.008566 on epoch=57
06/02/2022 03:08:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.015820 on epoch=58
06/02/2022 03:08:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.007630 on epoch=58
06/02/2022 03:08:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.021544 on epoch=59
06/02/2022 03:08:21 - INFO - __main__ - Global step 950 Train loss 0.017161 Classification-F1 0.25466261622019704 on epoch=59
06/02/2022 03:08:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.009517 on epoch=59
06/02/2022 03:08:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.004638 on epoch=60
06/02/2022 03:08:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.003449 on epoch=61
06/02/2022 03:08:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000798 on epoch=61
06/02/2022 03:08:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.026310 on epoch=62
06/02/2022 03:08:48 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:08:48 - INFO - __main__ - Printing 3 examples
06/02/2022 03:08:48 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/02/2022 03:08:48 - INFO - __main__ - ['others']
06/02/2022 03:08:48 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/02/2022 03:08:48 - INFO - __main__ - ['others']
06/02/2022 03:08:48 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/02/2022 03:08:48 - INFO - __main__ - ['others']
06/02/2022 03:08:48 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:08:48 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:08:49 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 03:08:49 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:08:49 - INFO - __main__ - Printing 3 examples
06/02/2022 03:08:49 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
06/02/2022 03:08:49 - INFO - __main__ - ['others']
06/02/2022 03:08:49 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
06/02/2022 03:08:49 - INFO - __main__ - ['others']
06/02/2022 03:08:49 - INFO - __main__ -  [emo] why because you don't want to i want to
06/02/2022 03:08:49 - INFO - __main__ - ['others']
06/02/2022 03:08:49 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:08:49 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:08:49 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 03:08:49 - INFO - __main__ - Global step 1000 Train loss 0.008942 Classification-F1 0.44011735900033777 on epoch=62
06/02/2022 03:08:49 - INFO - __main__ - save last model!
06/02/2022 03:08:56 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 03:08:57 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 03:08:57 - INFO - __main__ - Printing 3 examples
06/02/2022 03:08:57 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 03:08:57 - INFO - __main__ - ['others']
06/02/2022 03:08:57 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 03:08:57 - INFO - __main__ - ['others']
06/02/2022 03:08:57 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 03:08:57 - INFO - __main__ - ['others']
06/02/2022 03:08:57 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:08:59 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:09:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 03:09:02 - INFO - __main__ - Starting training!
06/02/2022 03:09:04 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 03:09:48 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_13_0.0003_8_predictions.txt
06/02/2022 03:09:48 - INFO - __main__ - Classification-F1 on test data: 0.3446
06/02/2022 03:09:48 - INFO - __main__ - prefix=emo_64_13, lr=0.0003, bsz=8, dev_performance=0.6821420187793428, test_performance=0.34460610265791053
06/02/2022 03:09:48 - INFO - __main__ - Running ... prefix=emo_64_13, lr=0.0002, bsz=8 ...
06/02/2022 03:09:49 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:09:49 - INFO - __main__ - Printing 3 examples
06/02/2022 03:09:49 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/02/2022 03:09:49 - INFO - __main__ - ['others']
06/02/2022 03:09:49 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/02/2022 03:09:49 - INFO - __main__ - ['others']
06/02/2022 03:09:49 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/02/2022 03:09:49 - INFO - __main__ - ['others']
06/02/2022 03:09:49 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:09:49 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:09:50 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 03:09:50 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:09:50 - INFO - __main__ - Printing 3 examples
06/02/2022 03:09:50 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
06/02/2022 03:09:50 - INFO - __main__ - ['others']
06/02/2022 03:09:50 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
06/02/2022 03:09:50 - INFO - __main__ - ['others']
06/02/2022 03:09:50 - INFO - __main__ -  [emo] why because you don't want to i want to
06/02/2022 03:09:50 - INFO - __main__ - ['others']
06/02/2022 03:09:50 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:09:50 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:09:50 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 03:10:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 03:10:01 - INFO - __main__ - Starting training!
06/02/2022 03:10:06 - INFO - __main__ - Step 10 Global step 10 Train loss 25.091030 on epoch=0
06/02/2022 03:10:11 - INFO - __main__ - Step 20 Global step 20 Train loss 20.915010 on epoch=1
06/02/2022 03:10:16 - INFO - __main__ - Step 30 Global step 30 Train loss 17.817959 on epoch=1
06/02/2022 03:10:21 - INFO - __main__ - Step 40 Global step 40 Train loss 17.206518 on epoch=2
06/02/2022 03:10:26 - INFO - __main__ - Step 50 Global step 50 Train loss 16.606670 on epoch=3
06/02/2022 03:11:26 - INFO - __main__ - Global step 50 Train loss 19.527437 Classification-F1 0.0 on epoch=3
06/02/2022 03:11:32 - INFO - __main__ - Step 60 Global step 60 Train loss 15.569826 on epoch=3
06/02/2022 03:11:38 - INFO - __main__ - Step 70 Global step 70 Train loss 15.098216 on epoch=4
06/02/2022 03:11:43 - INFO - __main__ - Step 80 Global step 80 Train loss 14.740854 on epoch=4
06/02/2022 03:11:48 - INFO - __main__ - Step 90 Global step 90 Train loss 14.739092 on epoch=5
06/02/2022 03:11:53 - INFO - __main__ - Step 100 Global step 100 Train loss 13.037236 on epoch=6
06/02/2022 03:12:38 - INFO - __main__ - Global step 100 Train loss 14.637046 Classification-F1 0.00034311202607651396 on epoch=6
06/02/2022 03:12:44 - INFO - __main__ - Step 110 Global step 110 Train loss 12.989805 on epoch=6
06/02/2022 03:12:49 - INFO - __main__ - Step 120 Global step 120 Train loss 12.659601 on epoch=7
06/02/2022 03:12:54 - INFO - __main__ - Step 130 Global step 130 Train loss 11.313350 on epoch=8
06/02/2022 03:12:59 - INFO - __main__ - Step 140 Global step 140 Train loss 10.305594 on epoch=8
06/02/2022 03:13:04 - INFO - __main__ - Step 150 Global step 150 Train loss 9.546522 on epoch=9
06/02/2022 03:13:42 - INFO - __main__ - Global step 150 Train loss 11.362976 Classification-F1 0.0007274667183976333 on epoch=9
06/02/2022 03:13:47 - INFO - __main__ - Step 160 Global step 160 Train loss 7.639807 on epoch=9
06/02/2022 03:13:53 - INFO - __main__ - Step 170 Global step 170 Train loss 4.973144 on epoch=10
06/02/2022 03:13:58 - INFO - __main__ - Step 180 Global step 180 Train loss 4.703546 on epoch=11
06/02/2022 03:14:03 - INFO - __main__ - Step 190 Global step 190 Train loss 3.508163 on epoch=11
06/02/2022 03:14:08 - INFO - __main__ - Step 200 Global step 200 Train loss 5.254540 on epoch=12
06/02/2022 03:14:10 - INFO - __main__ - Global step 200 Train loss 5.215840 Classification-F1 0.2967427977553511 on epoch=12
06/02/2022 03:14:16 - INFO - __main__ - Step 210 Global step 210 Train loss 3.641773 on epoch=13
06/02/2022 03:14:21 - INFO - __main__ - Step 220 Global step 220 Train loss 3.707366 on epoch=13
06/02/2022 03:14:26 - INFO - __main__ - Step 230 Global step 230 Train loss 3.685731 on epoch=14
06/02/2022 03:14:31 - INFO - __main__ - Step 240 Global step 240 Train loss 3.433044 on epoch=14
06/02/2022 03:14:36 - INFO - __main__ - Step 250 Global step 250 Train loss 3.613612 on epoch=15
06/02/2022 03:14:38 - INFO - __main__ - Global step 250 Train loss 3.616306 Classification-F1 0.27843476420852376 on epoch=15
06/02/2022 03:14:43 - INFO - __main__ - Step 260 Global step 260 Train loss 2.786003 on epoch=16
06/02/2022 03:14:48 - INFO - __main__ - Step 270 Global step 270 Train loss 3.496876 on epoch=16
06/02/2022 03:14:53 - INFO - __main__ - Step 280 Global step 280 Train loss 3.893682 on epoch=17
06/02/2022 03:14:58 - INFO - __main__ - Step 290 Global step 290 Train loss 3.103826 on epoch=18
06/02/2022 03:15:03 - INFO - __main__ - Step 300 Global step 300 Train loss 2.499199 on epoch=18
06/02/2022 03:15:05 - INFO - __main__ - Global step 300 Train loss 3.155917 Classification-F1 0.1 on epoch=18
06/02/2022 03:15:10 - INFO - __main__ - Step 310 Global step 310 Train loss 3.232854 on epoch=19
06/02/2022 03:15:15 - INFO - __main__ - Step 320 Global step 320 Train loss 2.898976 on epoch=19
06/02/2022 03:15:20 - INFO - __main__ - Step 330 Global step 330 Train loss 3.138129 on epoch=20
06/02/2022 03:15:25 - INFO - __main__ - Step 340 Global step 340 Train loss 3.163635 on epoch=21
06/02/2022 03:15:30 - INFO - __main__ - Step 350 Global step 350 Train loss 2.608302 on epoch=21
06/02/2022 03:15:32 - INFO - __main__ - Global step 350 Train loss 3.008379 Classification-F1 0.1 on epoch=21
06/02/2022 03:15:37 - INFO - __main__ - Step 360 Global step 360 Train loss 2.247762 on epoch=22
06/02/2022 03:15:42 - INFO - __main__ - Step 370 Global step 370 Train loss 2.810197 on epoch=23
06/02/2022 03:15:47 - INFO - __main__ - Step 380 Global step 380 Train loss 2.249492 on epoch=23
06/02/2022 03:15:52 - INFO - __main__ - Step 390 Global step 390 Train loss 2.631771 on epoch=24
06/02/2022 03:15:57 - INFO - __main__ - Step 400 Global step 400 Train loss 2.281006 on epoch=24
06/02/2022 03:15:59 - INFO - __main__ - Global step 400 Train loss 2.444045 Classification-F1 0.37679793476572354 on epoch=24
06/02/2022 03:16:05 - INFO - __main__ - Step 410 Global step 410 Train loss 1.890188 on epoch=25
06/02/2022 03:16:10 - INFO - __main__ - Step 420 Global step 420 Train loss 2.004039 on epoch=26
06/02/2022 03:16:15 - INFO - __main__ - Step 430 Global step 430 Train loss 2.085712 on epoch=26
06/02/2022 03:16:20 - INFO - __main__ - Step 440 Global step 440 Train loss 1.683581 on epoch=27
06/02/2022 03:16:25 - INFO - __main__ - Step 450 Global step 450 Train loss 1.621939 on epoch=28
06/02/2022 03:16:27 - INFO - __main__ - Global step 450 Train loss 1.857092 Classification-F1 0.447289100410601 on epoch=28
06/02/2022 03:16:33 - INFO - __main__ - Step 460 Global step 460 Train loss 1.682526 on epoch=28
06/02/2022 03:16:38 - INFO - __main__ - Step 470 Global step 470 Train loss 1.169981 on epoch=29
06/02/2022 03:16:43 - INFO - __main__ - Step 480 Global step 480 Train loss 1.222316 on epoch=29
06/02/2022 03:16:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.913634 on epoch=30
06/02/2022 03:16:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.668161 on epoch=31
06/02/2022 03:16:55 - INFO - __main__ - Global step 500 Train loss 1.131324 Classification-F1 0.728266509086986 on epoch=31
06/02/2022 03:17:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.732860 on epoch=31
06/02/2022 03:17:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.448230 on epoch=32
06/02/2022 03:17:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.416400 on epoch=33
06/02/2022 03:17:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.395684 on epoch=33
06/02/2022 03:17:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.576684 on epoch=34
06/02/2022 03:17:23 - INFO - __main__ - Global step 550 Train loss 0.513972 Classification-F1 0.7248245379284249 on epoch=34
06/02/2022 03:17:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.397195 on epoch=34
06/02/2022 03:17:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.365642 on epoch=35
06/02/2022 03:17:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.233745 on epoch=36
06/02/2022 03:17:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.358284 on epoch=36
06/02/2022 03:17:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.274781 on epoch=37
06/02/2022 03:17:50 - INFO - __main__ - Global step 600 Train loss 0.325929 Classification-F1 0.7602058823529412 on epoch=37
06/02/2022 03:17:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.203292 on epoch=38
06/02/2022 03:18:01 - INFO - __main__ - Step 620 Global step 620 Train loss 0.334589 on epoch=38
06/02/2022 03:18:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.183138 on epoch=39
06/02/2022 03:18:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.188493 on epoch=39
06/02/2022 03:18:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.235639 on epoch=40
06/02/2022 03:18:18 - INFO - __main__ - Global step 650 Train loss 0.229030 Classification-F1 0.7888863397954204 on epoch=40
06/02/2022 03:18:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.241728 on epoch=41
06/02/2022 03:18:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.159920 on epoch=41
06/02/2022 03:18:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.196904 on epoch=42
06/02/2022 03:18:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.190636 on epoch=43
06/02/2022 03:18:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.117191 on epoch=43
06/02/2022 03:18:46 - INFO - __main__ - Global step 700 Train loss 0.181276 Classification-F1 0.8053869472198687 on epoch=43
06/02/2022 03:18:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.211562 on epoch=44
06/02/2022 03:18:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.064672 on epoch=44
06/02/2022 03:19:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.136634 on epoch=45
06/02/2022 03:19:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.092139 on epoch=46
06/02/2022 03:19:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.092311 on epoch=46
06/02/2022 03:19:14 - INFO - __main__ - Global step 750 Train loss 0.119463 Classification-F1 0.8329184039360993 on epoch=46
06/02/2022 03:19:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.094597 on epoch=47
06/02/2022 03:19:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.098290 on epoch=48
06/02/2022 03:19:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.074029 on epoch=48
06/02/2022 03:19:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.132735 on epoch=49
06/02/2022 03:19:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.037137 on epoch=49
06/02/2022 03:19:42 - INFO - __main__ - Global step 800 Train loss 0.087358 Classification-F1 0.8298614907964279 on epoch=49
06/02/2022 03:19:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.103356 on epoch=50
06/02/2022 03:19:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.077821 on epoch=51
06/02/2022 03:19:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.047947 on epoch=51
06/02/2022 03:20:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.085010 on epoch=52
06/02/2022 03:20:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.055073 on epoch=53
06/02/2022 03:20:10 - INFO - __main__ - Global step 850 Train loss 0.073841 Classification-F1 0.8058207362059929 on epoch=53
06/02/2022 03:20:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.075862 on epoch=53
06/02/2022 03:20:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.078378 on epoch=54
06/02/2022 03:20:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.030014 on epoch=54
06/02/2022 03:20:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.055596 on epoch=55
06/02/2022 03:20:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.054191 on epoch=56
06/02/2022 03:20:37 - INFO - __main__ - Global step 900 Train loss 0.058808 Classification-F1 0.8074082696663971 on epoch=56
06/02/2022 03:20:43 - INFO - __main__ - Step 910 Global step 910 Train loss 0.032470 on epoch=56
06/02/2022 03:20:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.035861 on epoch=57
06/02/2022 03:20:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.041073 on epoch=58
06/02/2022 03:20:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.027610 on epoch=58
06/02/2022 03:21:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.022746 on epoch=59
06/02/2022 03:21:05 - INFO - __main__ - Global step 950 Train loss 0.031952 Classification-F1 0.8318683414901066 on epoch=59
06/02/2022 03:21:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.050006 on epoch=59
06/02/2022 03:21:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.074042 on epoch=60
06/02/2022 03:21:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.038532 on epoch=61
06/02/2022 03:21:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.023276 on epoch=61
06/02/2022 03:21:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.021989 on epoch=62
06/02/2022 03:21:31 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:21:31 - INFO - __main__ - Printing 3 examples
06/02/2022 03:21:31 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/02/2022 03:21:31 - INFO - __main__ - ['others']
06/02/2022 03:21:31 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/02/2022 03:21:31 - INFO - __main__ - ['others']
06/02/2022 03:21:31 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/02/2022 03:21:31 - INFO - __main__ - ['others']
06/02/2022 03:21:31 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:21:31 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:21:32 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 03:21:32 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:21:32 - INFO - __main__ - Printing 3 examples
06/02/2022 03:21:32 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
06/02/2022 03:21:32 - INFO - __main__ - ['others']
06/02/2022 03:21:32 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
06/02/2022 03:21:32 - INFO - __main__ - ['others']
06/02/2022 03:21:32 - INFO - __main__ -  [emo] why because you don't want to i want to
06/02/2022 03:21:32 - INFO - __main__ - ['others']
06/02/2022 03:21:32 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:21:32 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:21:32 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 03:21:32 - INFO - __main__ - Global step 1000 Train loss 0.041569 Classification-F1 0.8047666910331384 on epoch=62
06/02/2022 03:21:32 - INFO - __main__ - save last model!
06/02/2022 03:21:39 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 03:21:40 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 03:21:40 - INFO - __main__ - Printing 3 examples
06/02/2022 03:21:40 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 03:21:40 - INFO - __main__ - ['others']
06/02/2022 03:21:40 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 03:21:40 - INFO - __main__ - ['others']
06/02/2022 03:21:40 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 03:21:40 - INFO - __main__ - ['others']
06/02/2022 03:21:40 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:21:42 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:21:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 03:21:45 - INFO - __main__ - Starting training!
06/02/2022 03:21:47 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 03:22:29 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_13_0.0002_8_predictions.txt
06/02/2022 03:22:29 - INFO - __main__ - Classification-F1 on test data: 0.3935
06/02/2022 03:22:29 - INFO - __main__ - prefix=emo_64_13, lr=0.0002, bsz=8, dev_performance=0.8329184039360993, test_performance=0.39346392563963817
06/02/2022 03:22:29 - INFO - __main__ - Running ... prefix=emo_64_13, lr=0.0001, bsz=8 ...
06/02/2022 03:22:30 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:22:30 - INFO - __main__ - Printing 3 examples
06/02/2022 03:22:30 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/02/2022 03:22:30 - INFO - __main__ - ['others']
06/02/2022 03:22:30 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/02/2022 03:22:30 - INFO - __main__ - ['others']
06/02/2022 03:22:30 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/02/2022 03:22:30 - INFO - __main__ - ['others']
06/02/2022 03:22:30 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:22:30 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:22:31 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 03:22:31 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:22:31 - INFO - __main__ - Printing 3 examples
06/02/2022 03:22:31 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
06/02/2022 03:22:31 - INFO - __main__ - ['others']
06/02/2022 03:22:31 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
06/02/2022 03:22:31 - INFO - __main__ - ['others']
06/02/2022 03:22:31 - INFO - __main__ -  [emo] why because you don't want to i want to
06/02/2022 03:22:31 - INFO - __main__ - ['others']
06/02/2022 03:22:31 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:22:31 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:22:31 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 03:22:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 03:22:44 - INFO - __main__ - Starting training!
06/02/2022 03:22:48 - INFO - __main__ - Step 10 Global step 10 Train loss 25.728863 on epoch=0
06/02/2022 03:22:53 - INFO - __main__ - Step 20 Global step 20 Train loss 20.837523 on epoch=1
06/02/2022 03:22:58 - INFO - __main__ - Step 30 Global step 30 Train loss 19.419355 on epoch=1
06/02/2022 03:23:03 - INFO - __main__ - Step 40 Global step 40 Train loss 18.242472 on epoch=2
06/02/2022 03:23:08 - INFO - __main__ - Step 50 Global step 50 Train loss 17.450842 on epoch=3
06/02/2022 03:23:17 - INFO - __main__ - Global step 50 Train loss 20.335812 Classification-F1 0.0 on epoch=3
06/02/2022 03:23:23 - INFO - __main__ - Step 60 Global step 60 Train loss 17.431902 on epoch=3
06/02/2022 03:23:28 - INFO - __main__ - Step 70 Global step 70 Train loss 17.131493 on epoch=4
06/02/2022 03:23:33 - INFO - __main__ - Step 80 Global step 80 Train loss 16.820913 on epoch=4
06/02/2022 03:23:38 - INFO - __main__ - Step 90 Global step 90 Train loss 15.933046 on epoch=5
06/02/2022 03:23:43 - INFO - __main__ - Step 100 Global step 100 Train loss 15.757947 on epoch=6
06/02/2022 03:23:46 - INFO - __main__ - Global step 100 Train loss 16.615059 Classification-F1 0.0 on epoch=6
06/02/2022 03:23:51 - INFO - __main__ - Step 110 Global step 110 Train loss 16.187004 on epoch=6
06/02/2022 03:23:56 - INFO - __main__ - Step 120 Global step 120 Train loss 15.571159 on epoch=7
06/02/2022 03:24:01 - INFO - __main__ - Step 130 Global step 130 Train loss 14.877675 on epoch=8
06/02/2022 03:24:06 - INFO - __main__ - Step 140 Global step 140 Train loss 15.400441 on epoch=8
06/02/2022 03:24:11 - INFO - __main__ - Step 150 Global step 150 Train loss 14.493421 on epoch=9
06/02/2022 03:24:14 - INFO - __main__ - Global step 150 Train loss 15.305940 Classification-F1 0.0 on epoch=9
06/02/2022 03:24:19 - INFO - __main__ - Step 160 Global step 160 Train loss 14.485176 on epoch=9
06/02/2022 03:24:24 - INFO - __main__ - Step 170 Global step 170 Train loss 14.407394 on epoch=10
06/02/2022 03:24:29 - INFO - __main__ - Step 180 Global step 180 Train loss 12.897525 on epoch=11
06/02/2022 03:24:34 - INFO - __main__ - Step 190 Global step 190 Train loss 13.085363 on epoch=11
06/02/2022 03:24:39 - INFO - __main__ - Step 200 Global step 200 Train loss 13.069365 on epoch=12
06/02/2022 03:24:42 - INFO - __main__ - Global step 200 Train loss 13.588964 Classification-F1 0.000803582382529751 on epoch=12
06/02/2022 03:24:48 - INFO - __main__ - Step 210 Global step 210 Train loss 12.957550 on epoch=13
06/02/2022 03:24:53 - INFO - __main__ - Step 220 Global step 220 Train loss 11.756009 on epoch=13
06/02/2022 03:24:58 - INFO - __main__ - Step 230 Global step 230 Train loss 11.901954 on epoch=14
06/02/2022 03:25:03 - INFO - __main__ - Step 240 Global step 240 Train loss 11.589209 on epoch=14
06/02/2022 03:25:08 - INFO - __main__ - Step 250 Global step 250 Train loss 10.933227 on epoch=15
06/02/2022 03:25:10 - INFO - __main__ - Global step 250 Train loss 11.827589 Classification-F1 0.0009925558312655087 on epoch=15
06/02/2022 03:25:16 - INFO - __main__ - Step 260 Global step 260 Train loss 10.983109 on epoch=16
06/02/2022 03:25:21 - INFO - __main__ - Step 270 Global step 270 Train loss 10.558218 on epoch=16
06/02/2022 03:25:26 - INFO - __main__ - Step 280 Global step 280 Train loss 9.817938 on epoch=17
06/02/2022 03:25:31 - INFO - __main__ - Step 290 Global step 290 Train loss 8.725148 on epoch=18
06/02/2022 03:25:36 - INFO - __main__ - Step 300 Global step 300 Train loss 8.054831 on epoch=18
06/02/2022 03:25:38 - INFO - __main__ - Global step 300 Train loss 9.627850 Classification-F1 0.002197802197802198 on epoch=18
06/02/2022 03:25:44 - INFO - __main__ - Step 310 Global step 310 Train loss 6.559724 on epoch=19
06/02/2022 03:25:49 - INFO - __main__ - Step 320 Global step 320 Train loss 5.960537 on epoch=19
06/02/2022 03:25:54 - INFO - __main__ - Step 330 Global step 330 Train loss 2.317131 on epoch=20
06/02/2022 03:25:59 - INFO - __main__ - Step 340 Global step 340 Train loss 1.415033 on epoch=21
06/02/2022 03:26:04 - INFO - __main__ - Step 350 Global step 350 Train loss 1.319619 on epoch=21
06/02/2022 03:26:06 - INFO - __main__ - Global step 350 Train loss 3.514409 Classification-F1 0.4406609713852704 on epoch=21
06/02/2022 03:26:12 - INFO - __main__ - Step 360 Global step 360 Train loss 1.294093 on epoch=22
06/02/2022 03:26:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.985012 on epoch=23
06/02/2022 03:26:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.719179 on epoch=23
06/02/2022 03:26:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.714833 on epoch=24
06/02/2022 03:26:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.926822 on epoch=24
06/02/2022 03:26:33 - INFO - __main__ - Global step 400 Train loss 0.927988 Classification-F1 0.6392115625559335 on epoch=24
06/02/2022 03:26:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.779149 on epoch=25
06/02/2022 03:26:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.679817 on epoch=26
06/02/2022 03:26:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.713638 on epoch=26
06/02/2022 03:26:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.578203 on epoch=27
06/02/2022 03:27:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.583445 on epoch=28
06/02/2022 03:27:02 - INFO - __main__ - Global step 450 Train loss 0.666850 Classification-F1 0.6485316975791918 on epoch=28
06/02/2022 03:27:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.623430 on epoch=28
06/02/2022 03:27:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.582845 on epoch=29
06/02/2022 03:27:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.538425 on epoch=29
06/02/2022 03:27:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.546784 on epoch=30
06/02/2022 03:27:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.472154 on epoch=31
06/02/2022 03:27:31 - INFO - __main__ - Global step 500 Train loss 0.552728 Classification-F1 0.7038768745141205 on epoch=31
06/02/2022 03:27:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.484692 on epoch=31
06/02/2022 03:27:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.451937 on epoch=32
06/02/2022 03:27:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.486654 on epoch=33
06/02/2022 03:27:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.509782 on epoch=33
06/02/2022 03:27:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.426881 on epoch=34
06/02/2022 03:27:59 - INFO - __main__ - Global step 550 Train loss 0.471989 Classification-F1 0.7122834773369124 on epoch=34
06/02/2022 03:28:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.340294 on epoch=34
06/02/2022 03:28:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.459941 on epoch=35
06/02/2022 03:28:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.359431 on epoch=36
06/02/2022 03:28:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.333937 on epoch=36
06/02/2022 03:28:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.354375 on epoch=37
06/02/2022 03:28:29 - INFO - __main__ - Global step 600 Train loss 0.369596 Classification-F1 0.7622285136501517 on epoch=37
06/02/2022 03:28:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.319550 on epoch=38
06/02/2022 03:28:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.415082 on epoch=38
06/02/2022 03:28:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.244566 on epoch=39
06/02/2022 03:28:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.355142 on epoch=39
06/02/2022 03:28:56 - INFO - __main__ - Step 650 Global step 650 Train loss 0.429165 on epoch=40
06/02/2022 03:28:58 - INFO - __main__ - Global step 650 Train loss 0.352701 Classification-F1 0.7567293374739736 on epoch=40
06/02/2022 03:29:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.233748 on epoch=41
06/02/2022 03:29:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.329041 on epoch=41
06/02/2022 03:29:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.260382 on epoch=42
06/02/2022 03:29:19 - INFO - __main__ - Step 690 Global step 690 Train loss 0.214359 on epoch=43
06/02/2022 03:29:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.295621 on epoch=43
06/02/2022 03:29:26 - INFO - __main__ - Global step 700 Train loss 0.266630 Classification-F1 0.750099611440892 on epoch=43
06/02/2022 03:29:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.268957 on epoch=44
06/02/2022 03:29:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.231150 on epoch=44
06/02/2022 03:29:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.265058 on epoch=45
06/02/2022 03:29:47 - INFO - __main__ - Step 740 Global step 740 Train loss 0.252022 on epoch=46
06/02/2022 03:29:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.172898 on epoch=46
06/02/2022 03:29:54 - INFO - __main__ - Global step 750 Train loss 0.238017 Classification-F1 0.7401209225108808 on epoch=46
06/02/2022 03:29:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.181281 on epoch=47
06/02/2022 03:30:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.272447 on epoch=48
06/02/2022 03:30:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.158087 on epoch=48
06/02/2022 03:30:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.221876 on epoch=49
06/02/2022 03:30:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.156601 on epoch=49
06/02/2022 03:30:22 - INFO - __main__ - Global step 800 Train loss 0.198058 Classification-F1 0.6266656809351306 on epoch=49
06/02/2022 03:30:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.213898 on epoch=50
06/02/2022 03:30:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.175670 on epoch=51
06/02/2022 03:30:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.211770 on epoch=51
06/02/2022 03:30:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.234739 on epoch=52
06/02/2022 03:30:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.117424 on epoch=53
06/02/2022 03:30:50 - INFO - __main__ - Global step 850 Train loss 0.190700 Classification-F1 0.7367573277641323 on epoch=53
06/02/2022 03:30:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.144331 on epoch=53
06/02/2022 03:31:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.148906 on epoch=54
06/02/2022 03:31:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.126758 on epoch=54
06/02/2022 03:31:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.175912 on epoch=55
06/02/2022 03:31:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.240396 on epoch=56
06/02/2022 03:31:19 - INFO - __main__ - Global step 900 Train loss 0.167261 Classification-F1 0.7918150519685648 on epoch=56
06/02/2022 03:31:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.121763 on epoch=56
06/02/2022 03:31:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.100292 on epoch=57
06/02/2022 03:31:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.240716 on epoch=58
06/02/2022 03:31:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.085196 on epoch=58
06/02/2022 03:31:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.123431 on epoch=59
06/02/2022 03:31:48 - INFO - __main__ - Global step 950 Train loss 0.134280 Classification-F1 0.6053275044793722 on epoch=59
06/02/2022 03:31:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.071854 on epoch=59
06/02/2022 03:31:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.150812 on epoch=60
06/02/2022 03:32:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.053157 on epoch=61
06/02/2022 03:32:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.139924 on epoch=61
06/02/2022 03:32:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.106694 on epoch=62
06/02/2022 03:32:15 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:32:15 - INFO - __main__ - Printing 3 examples
06/02/2022 03:32:15 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/02/2022 03:32:15 - INFO - __main__ - ['sad']
06/02/2022 03:32:15 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/02/2022 03:32:15 - INFO - __main__ - ['sad']
06/02/2022 03:32:15 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/02/2022 03:32:15 - INFO - __main__ - ['sad']
06/02/2022 03:32:15 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:32:15 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:32:15 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 03:32:15 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:32:15 - INFO - __main__ - Printing 3 examples
06/02/2022 03:32:15 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
06/02/2022 03:32:15 - INFO - __main__ - ['sad']
06/02/2022 03:32:15 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
06/02/2022 03:32:15 - INFO - __main__ - ['sad']
06/02/2022 03:32:15 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
06/02/2022 03:32:15 - INFO - __main__ - ['sad']
06/02/2022 03:32:15 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:32:15 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:32:15 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 03:32:16 - INFO - __main__ - Global step 1000 Train loss 0.104488 Classification-F1 0.6282662865158681 on epoch=62
06/02/2022 03:32:16 - INFO - __main__ - save last model!
06/02/2022 03:32:22 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 03:32:23 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 03:32:23 - INFO - __main__ - Printing 3 examples
06/02/2022 03:32:23 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 03:32:23 - INFO - __main__ - ['others']
06/02/2022 03:32:23 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 03:32:23 - INFO - __main__ - ['others']
06/02/2022 03:32:23 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 03:32:23 - INFO - __main__ - ['others']
06/02/2022 03:32:23 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:32:25 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:32:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 03:32:26 - INFO - __main__ - Starting training!
06/02/2022 03:32:31 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 03:33:15 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_13_0.0001_8_predictions.txt
06/02/2022 03:33:15 - INFO - __main__ - Classification-F1 on test data: 0.1117
06/02/2022 03:33:16 - INFO - __main__ - prefix=emo_64_13, lr=0.0001, bsz=8, dev_performance=0.7918150519685648, test_performance=0.11171398390967985
06/02/2022 03:33:16 - INFO - __main__ - Running ... prefix=emo_64_21, lr=0.0005, bsz=8 ...
06/02/2022 03:33:17 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:33:17 - INFO - __main__ - Printing 3 examples
06/02/2022 03:33:17 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/02/2022 03:33:17 - INFO - __main__ - ['sad']
06/02/2022 03:33:17 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/02/2022 03:33:17 - INFO - __main__ - ['sad']
06/02/2022 03:33:17 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/02/2022 03:33:17 - INFO - __main__ - ['sad']
06/02/2022 03:33:17 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:33:17 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:33:17 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 03:33:17 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:33:17 - INFO - __main__ - Printing 3 examples
06/02/2022 03:33:17 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
06/02/2022 03:33:17 - INFO - __main__ - ['sad']
06/02/2022 03:33:17 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
06/02/2022 03:33:17 - INFO - __main__ - ['sad']
06/02/2022 03:33:17 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
06/02/2022 03:33:17 - INFO - __main__ - ['sad']
06/02/2022 03:33:17 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:33:17 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:33:17 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 03:33:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 03:33:29 - INFO - __main__ - Starting training!
06/02/2022 03:33:33 - INFO - __main__ - Step 10 Global step 10 Train loss 24.754011 on epoch=0
06/02/2022 03:33:39 - INFO - __main__ - Step 20 Global step 20 Train loss 17.370670 on epoch=1
06/02/2022 03:33:44 - INFO - __main__ - Step 30 Global step 30 Train loss 15.523397 on epoch=1
06/02/2022 03:33:49 - INFO - __main__ - Step 40 Global step 40 Train loss 14.581080 on epoch=2
06/02/2022 03:33:54 - INFO - __main__ - Step 50 Global step 50 Train loss 12.593340 on epoch=3
06/02/2022 03:33:57 - INFO - __main__ - Global step 50 Train loss 16.964500 Classification-F1 0.0 on epoch=3
06/02/2022 03:34:03 - INFO - __main__ - Step 60 Global step 60 Train loss 11.488794 on epoch=3
06/02/2022 03:34:08 - INFO - __main__ - Step 70 Global step 70 Train loss 9.161893 on epoch=4
06/02/2022 03:34:13 - INFO - __main__ - Step 80 Global step 80 Train loss 5.310212 on epoch=4
06/02/2022 03:34:19 - INFO - __main__ - Step 90 Global step 90 Train loss 2.917701 on epoch=5
06/02/2022 03:34:24 - INFO - __main__ - Step 100 Global step 100 Train loss 1.545134 on epoch=6
06/02/2022 03:34:26 - INFO - __main__ - Global step 100 Train loss 6.084747 Classification-F1 0.1886974195828915 on epoch=6
06/02/2022 03:34:32 - INFO - __main__ - Step 110 Global step 110 Train loss 1.128681 on epoch=6
06/02/2022 03:34:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.890054 on epoch=7
06/02/2022 03:34:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.866914 on epoch=8
06/02/2022 03:34:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.799094 on epoch=8
06/02/2022 03:34:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.798262 on epoch=9
06/02/2022 03:34:55 - INFO - __main__ - Global step 150 Train loss 0.896601 Classification-F1 0.21416401635620175 on epoch=9
06/02/2022 03:35:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.798093 on epoch=9
06/02/2022 03:35:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.727002 on epoch=10
06/02/2022 03:35:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.727533 on epoch=11
06/02/2022 03:35:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.676573 on epoch=11
06/02/2022 03:35:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.786925 on epoch=12
06/02/2022 03:35:25 - INFO - __main__ - Global step 200 Train loss 0.743225 Classification-F1 0.28626125302383304 on epoch=12
06/02/2022 03:35:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.747584 on epoch=13
06/02/2022 03:35:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.620535 on epoch=13
06/02/2022 03:35:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.738725 on epoch=14
06/02/2022 03:35:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.711028 on epoch=14
06/02/2022 03:35:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.605969 on epoch=15
06/02/2022 03:35:54 - INFO - __main__ - Global step 250 Train loss 0.684768 Classification-F1 0.3728374091862305 on epoch=15
06/02/2022 03:36:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.707746 on epoch=16
06/02/2022 03:36:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.612873 on epoch=16
06/02/2022 03:36:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.591319 on epoch=17
06/02/2022 03:36:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.703459 on epoch=18
06/02/2022 03:36:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.699477 on epoch=18
06/02/2022 03:36:24 - INFO - __main__ - Global step 300 Train loss 0.662975 Classification-F1 0.36038223646464507 on epoch=18
06/02/2022 03:36:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.565128 on epoch=19
06/02/2022 03:36:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.583114 on epoch=19
06/02/2022 03:36:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.563293 on epoch=20
06/02/2022 03:36:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.552546 on epoch=21
06/02/2022 03:36:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.490151 on epoch=21
06/02/2022 03:36:52 - INFO - __main__ - Global step 350 Train loss 0.550847 Classification-F1 0.41620302675627796 on epoch=21
06/02/2022 03:36:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.591111 on epoch=22
06/02/2022 03:37:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.512598 on epoch=23
06/02/2022 03:37:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.448390 on epoch=23
06/02/2022 03:37:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.504260 on epoch=24
06/02/2022 03:37:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.556028 on epoch=24
06/02/2022 03:37:21 - INFO - __main__ - Global step 400 Train loss 0.522477 Classification-F1 0.45029659337850725 on epoch=24
06/02/2022 03:37:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.529671 on epoch=25
06/02/2022 03:37:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.488178 on epoch=26
06/02/2022 03:37:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.528283 on epoch=26
06/02/2022 03:37:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.480024 on epoch=27
06/02/2022 03:37:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.751964 on epoch=28
06/02/2022 03:37:51 - INFO - __main__ - Global step 450 Train loss 0.555624 Classification-F1 0.6018088588909456 on epoch=28
06/02/2022 03:37:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.573423 on epoch=28
06/02/2022 03:38:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.471922 on epoch=29
06/02/2022 03:38:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.505029 on epoch=29
06/02/2022 03:38:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.493171 on epoch=30
06/02/2022 03:38:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.464619 on epoch=31
06/02/2022 03:38:20 - INFO - __main__ - Global step 500 Train loss 0.501633 Classification-F1 0.630841002603153 on epoch=31
06/02/2022 03:38:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.465018 on epoch=31
06/02/2022 03:38:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.398613 on epoch=32
06/02/2022 03:38:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.420270 on epoch=33
06/02/2022 03:38:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.433630 on epoch=33
06/02/2022 03:38:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.432874 on epoch=34
06/02/2022 03:38:49 - INFO - __main__ - Global step 550 Train loss 0.430081 Classification-F1 0.5016197554426476 on epoch=34
06/02/2022 03:38:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.490757 on epoch=34
06/02/2022 03:39:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.458190 on epoch=35
06/02/2022 03:39:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.410308 on epoch=36
06/02/2022 03:39:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.410496 on epoch=36
06/02/2022 03:39:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.423303 on epoch=37
06/02/2022 03:39:18 - INFO - __main__ - Global step 600 Train loss 0.438611 Classification-F1 0.6301699747695654 on epoch=37
06/02/2022 03:39:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.411838 on epoch=38
06/02/2022 03:39:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.297737 on epoch=38
06/02/2022 03:39:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.409218 on epoch=39
06/02/2022 03:39:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.326976 on epoch=39
06/02/2022 03:39:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.453043 on epoch=40
06/02/2022 03:39:46 - INFO - __main__ - Global step 650 Train loss 0.379762 Classification-F1 0.5918509882300689 on epoch=40
06/02/2022 03:39:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.324337 on epoch=41
06/02/2022 03:39:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.344035 on epoch=41
06/02/2022 03:40:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.437138 on epoch=42
06/02/2022 03:40:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.395350 on epoch=43
06/02/2022 03:40:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.437035 on epoch=43
06/02/2022 03:40:15 - INFO - __main__ - Global step 700 Train loss 0.387579 Classification-F1 0.563609376204796 on epoch=43
06/02/2022 03:40:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.406989 on epoch=44
06/02/2022 03:40:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.396889 on epoch=44
06/02/2022 03:40:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.344779 on epoch=45
06/02/2022 03:40:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.294433 on epoch=46
06/02/2022 03:40:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.342664 on epoch=46
06/02/2022 03:40:43 - INFO - __main__ - Global step 750 Train loss 0.357151 Classification-F1 0.5745521386608938 on epoch=46
06/02/2022 03:40:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.271601 on epoch=47
06/02/2022 03:40:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.343387 on epoch=48
06/02/2022 03:40:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.277029 on epoch=48
06/02/2022 03:41:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.453604 on epoch=49
06/02/2022 03:41:10 - INFO - __main__ - Step 800 Global step 800 Train loss 0.384202 on epoch=49
06/02/2022 03:41:12 - INFO - __main__ - Global step 800 Train loss 0.345965 Classification-F1 0.5580501305028572 on epoch=49
06/02/2022 03:41:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.391907 on epoch=50
06/02/2022 03:41:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.327605 on epoch=51
06/02/2022 03:41:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.412168 on epoch=51
06/02/2022 03:41:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.226142 on epoch=52
06/02/2022 03:41:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.306677 on epoch=53
06/02/2022 03:41:41 - INFO - __main__ - Global step 850 Train loss 0.332900 Classification-F1 0.5908653218435828 on epoch=53
06/02/2022 03:41:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.347726 on epoch=53
06/02/2022 03:41:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.357582 on epoch=54
06/02/2022 03:41:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.306411 on epoch=54
06/02/2022 03:42:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.391959 on epoch=55
06/02/2022 03:42:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.174251 on epoch=56
06/02/2022 03:42:09 - INFO - __main__ - Global step 900 Train loss 0.315586 Classification-F1 0.6446551521328776 on epoch=56
06/02/2022 03:42:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.224855 on epoch=56
06/02/2022 03:42:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.264172 on epoch=57
06/02/2022 03:42:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.254585 on epoch=58
06/02/2022 03:42:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.257629 on epoch=58
06/02/2022 03:42:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.295335 on epoch=59
06/02/2022 03:42:38 - INFO - __main__ - Global step 950 Train loss 0.259315 Classification-F1 0.6279316268823568 on epoch=59
06/02/2022 03:42:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.289066 on epoch=59
06/02/2022 03:42:49 - INFO - __main__ - Step 970 Global step 970 Train loss 0.222039 on epoch=60
06/02/2022 03:42:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.216940 on epoch=61
06/02/2022 03:43:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.231305 on epoch=61
06/02/2022 03:43:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.181327 on epoch=62
06/02/2022 03:43:06 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:43:06 - INFO - __main__ - Printing 3 examples
06/02/2022 03:43:06 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/02/2022 03:43:06 - INFO - __main__ - ['sad']
06/02/2022 03:43:06 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/02/2022 03:43:06 - INFO - __main__ - ['sad']
06/02/2022 03:43:06 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/02/2022 03:43:06 - INFO - __main__ - ['sad']
06/02/2022 03:43:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:43:06 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:43:06 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 03:43:06 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:43:06 - INFO - __main__ - Printing 3 examples
06/02/2022 03:43:06 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
06/02/2022 03:43:06 - INFO - __main__ - ['sad']
06/02/2022 03:43:06 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
06/02/2022 03:43:06 - INFO - __main__ - ['sad']
06/02/2022 03:43:06 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
06/02/2022 03:43:06 - INFO - __main__ - ['sad']
06/02/2022 03:43:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:43:06 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:43:07 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 03:43:07 - INFO - __main__ - Global step 1000 Train loss 0.228135 Classification-F1 0.6246146932338847 on epoch=62
06/02/2022 03:43:07 - INFO - __main__ - save last model!
06/02/2022 03:43:14 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 03:43:15 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 03:43:15 - INFO - __main__ - Printing 3 examples
06/02/2022 03:43:15 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 03:43:15 - INFO - __main__ - ['others']
06/02/2022 03:43:15 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 03:43:15 - INFO - __main__ - ['others']
06/02/2022 03:43:15 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 03:43:15 - INFO - __main__ - ['others']
06/02/2022 03:43:15 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:43:17 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:43:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 03:43:20 - INFO - __main__ - Starting training!
06/02/2022 03:43:22 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 03:44:07 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_21_0.0005_8_predictions.txt
06/02/2022 03:44:07 - INFO - __main__ - Classification-F1 on test data: 0.3606
06/02/2022 03:44:07 - INFO - __main__ - prefix=emo_64_21, lr=0.0005, bsz=8, dev_performance=0.6446551521328776, test_performance=0.36060731767550797
06/02/2022 03:44:07 - INFO - __main__ - Running ... prefix=emo_64_21, lr=0.0003, bsz=8 ...
06/02/2022 03:44:08 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:44:08 - INFO - __main__ - Printing 3 examples
06/02/2022 03:44:08 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/02/2022 03:44:08 - INFO - __main__ - ['sad']
06/02/2022 03:44:08 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/02/2022 03:44:08 - INFO - __main__ - ['sad']
06/02/2022 03:44:08 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/02/2022 03:44:08 - INFO - __main__ - ['sad']
06/02/2022 03:44:08 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:44:08 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:44:09 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 03:44:09 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:44:09 - INFO - __main__ - Printing 3 examples
06/02/2022 03:44:09 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
06/02/2022 03:44:09 - INFO - __main__ - ['sad']
06/02/2022 03:44:09 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
06/02/2022 03:44:09 - INFO - __main__ - ['sad']
06/02/2022 03:44:09 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
06/02/2022 03:44:09 - INFO - __main__ - ['sad']
06/02/2022 03:44:09 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:44:09 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:44:09 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 03:44:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 03:44:20 - INFO - __main__ - Starting training!
06/02/2022 03:44:24 - INFO - __main__ - Step 10 Global step 10 Train loss 24.934839 on epoch=0
06/02/2022 03:44:29 - INFO - __main__ - Step 20 Global step 20 Train loss 19.350836 on epoch=1
06/02/2022 03:44:35 - INFO - __main__ - Step 30 Global step 30 Train loss 18.024027 on epoch=1
06/02/2022 03:44:40 - INFO - __main__ - Step 40 Global step 40 Train loss 15.688700 on epoch=2
06/02/2022 03:44:45 - INFO - __main__ - Step 50 Global step 50 Train loss 15.325623 on epoch=3
06/02/2022 03:44:49 - INFO - __main__ - Global step 50 Train loss 18.664803 Classification-F1 0.0 on epoch=3
06/02/2022 03:44:55 - INFO - __main__ - Step 60 Global step 60 Train loss 14.373652 on epoch=3
06/02/2022 03:45:00 - INFO - __main__ - Step 70 Global step 70 Train loss 13.849156 on epoch=4
06/02/2022 03:45:05 - INFO - __main__ - Step 80 Global step 80 Train loss 12.289375 on epoch=4
06/02/2022 03:45:11 - INFO - __main__ - Step 90 Global step 90 Train loss 11.945393 on epoch=5
06/02/2022 03:45:16 - INFO - __main__ - Step 100 Global step 100 Train loss 11.230044 on epoch=6
06/02/2022 03:45:19 - INFO - __main__ - Global step 100 Train loss 12.737524 Classification-F1 0.0 on epoch=6
06/02/2022 03:45:24 - INFO - __main__ - Step 110 Global step 110 Train loss 10.641821 on epoch=6
06/02/2022 03:45:30 - INFO - __main__ - Step 120 Global step 120 Train loss 7.161855 on epoch=7
06/02/2022 03:45:35 - INFO - __main__ - Step 130 Global step 130 Train loss 4.624396 on epoch=8
06/02/2022 03:45:40 - INFO - __main__ - Step 140 Global step 140 Train loss 4.856085 on epoch=8
06/02/2022 03:45:45 - INFO - __main__ - Step 150 Global step 150 Train loss 3.953669 on epoch=9
06/02/2022 03:45:47 - INFO - __main__ - Global step 150 Train loss 6.247564 Classification-F1 0.029166666666666667 on epoch=9
06/02/2022 03:45:53 - INFO - __main__ - Step 160 Global step 160 Train loss 3.528504 on epoch=9
06/02/2022 03:45:59 - INFO - __main__ - Step 170 Global step 170 Train loss 3.128344 on epoch=10
06/02/2022 03:46:04 - INFO - __main__ - Step 180 Global step 180 Train loss 3.593797 on epoch=11
06/02/2022 03:46:09 - INFO - __main__ - Step 190 Global step 190 Train loss 3.625394 on epoch=11
06/02/2022 03:46:14 - INFO - __main__ - Step 200 Global step 200 Train loss 2.858378 on epoch=12
06/02/2022 03:46:17 - INFO - __main__ - Global step 200 Train loss 3.346884 Classification-F1 0.16854395604395606 on epoch=12
06/02/2022 03:46:23 - INFO - __main__ - Step 210 Global step 210 Train loss 2.975729 on epoch=13
06/02/2022 03:46:28 - INFO - __main__ - Step 220 Global step 220 Train loss 2.409420 on epoch=13
06/02/2022 03:46:33 - INFO - __main__ - Step 230 Global step 230 Train loss 1.694691 on epoch=14
06/02/2022 03:46:39 - INFO - __main__ - Step 240 Global step 240 Train loss 2.400349 on epoch=14
06/02/2022 03:46:44 - INFO - __main__ - Step 250 Global step 250 Train loss 2.179626 on epoch=15
06/02/2022 03:46:46 - INFO - __main__ - Global step 250 Train loss 2.331963 Classification-F1 0.10800578731613214 on epoch=15
06/02/2022 03:46:51 - INFO - __main__ - Step 260 Global step 260 Train loss 1.581967 on epoch=16
06/02/2022 03:46:56 - INFO - __main__ - Step 270 Global step 270 Train loss 1.735255 on epoch=16
06/02/2022 03:47:02 - INFO - __main__ - Step 280 Global step 280 Train loss 1.809140 on epoch=17
06/02/2022 03:47:07 - INFO - __main__ - Step 290 Global step 290 Train loss 1.498795 on epoch=18
06/02/2022 03:47:12 - INFO - __main__ - Step 300 Global step 300 Train loss 1.634605 on epoch=18
06/02/2022 03:47:14 - INFO - __main__ - Global step 300 Train loss 1.651953 Classification-F1 0.1527908685517381 on epoch=18
06/02/2022 03:47:19 - INFO - __main__ - Step 310 Global step 310 Train loss 1.464938 on epoch=19
06/02/2022 03:47:24 - INFO - __main__ - Step 320 Global step 320 Train loss 1.459471 on epoch=19
06/02/2022 03:47:30 - INFO - __main__ - Step 330 Global step 330 Train loss 1.165188 on epoch=20
06/02/2022 03:47:35 - INFO - __main__ - Step 340 Global step 340 Train loss 1.305885 on epoch=21
06/02/2022 03:47:40 - INFO - __main__ - Step 350 Global step 350 Train loss 1.287538 on epoch=21
06/02/2022 03:47:42 - INFO - __main__ - Global step 350 Train loss 1.336604 Classification-F1 0.2835201620384687 on epoch=21
06/02/2022 03:47:49 - INFO - __main__ - Step 360 Global step 360 Train loss 1.090411 on epoch=22
06/02/2022 03:47:54 - INFO - __main__ - Step 370 Global step 370 Train loss 1.028474 on epoch=23
06/02/2022 03:47:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.986400 on epoch=23
06/02/2022 03:48:04 - INFO - __main__ - Step 390 Global step 390 Train loss 1.042202 on epoch=24
06/02/2022 03:48:10 - INFO - __main__ - Step 400 Global step 400 Train loss 1.111439 on epoch=24
06/02/2022 03:48:12 - INFO - __main__ - Global step 400 Train loss 1.051785 Classification-F1 0.20822698846640786 on epoch=24
06/02/2022 03:48:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.916512 on epoch=25
06/02/2022 03:48:22 - INFO - __main__ - Step 420 Global step 420 Train loss 1.117227 on epoch=26
06/02/2022 03:48:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.847452 on epoch=26
06/02/2022 03:48:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.900590 on epoch=27
06/02/2022 03:48:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.916640 on epoch=28
06/02/2022 03:48:40 - INFO - __main__ - Global step 450 Train loss 0.939684 Classification-F1 0.41567297006452186 on epoch=28
06/02/2022 03:48:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.819610 on epoch=28
06/02/2022 03:48:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.836322 on epoch=29
06/02/2022 03:48:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.959295 on epoch=29
06/02/2022 03:49:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.971830 on epoch=30
06/02/2022 03:49:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.891377 on epoch=31
06/02/2022 03:49:10 - INFO - __main__ - Global step 500 Train loss 0.895687 Classification-F1 0.2139989805021027 on epoch=31
06/02/2022 03:49:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.825883 on epoch=31
06/02/2022 03:49:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.849035 on epoch=32
06/02/2022 03:49:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.880627 on epoch=33
06/02/2022 03:49:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.840391 on epoch=33
06/02/2022 03:49:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.785054 on epoch=34
06/02/2022 03:49:38 - INFO - __main__ - Global step 550 Train loss 0.836198 Classification-F1 0.39476753730349706 on epoch=34
06/02/2022 03:49:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.887252 on epoch=34
06/02/2022 03:49:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.776769 on epoch=35
06/02/2022 03:49:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.832396 on epoch=36
06/02/2022 03:49:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.880542 on epoch=36
06/02/2022 03:50:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.856190 on epoch=37
06/02/2022 03:50:06 - INFO - __main__ - Global step 600 Train loss 0.846630 Classification-F1 0.26908733097392545 on epoch=37
06/02/2022 03:50:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.896281 on epoch=38
06/02/2022 03:50:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.774830 on epoch=38
06/02/2022 03:50:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.781669 on epoch=39
06/02/2022 03:50:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.813620 on epoch=39
06/02/2022 03:50:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.766113 on epoch=40
06/02/2022 03:50:35 - INFO - __main__ - Global step 650 Train loss 0.806503 Classification-F1 0.46995750245178164 on epoch=40
06/02/2022 03:50:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.763892 on epoch=41
06/02/2022 03:50:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.713285 on epoch=41
06/02/2022 03:50:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.695450 on epoch=42
06/02/2022 03:50:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.746131 on epoch=43
06/02/2022 03:51:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.629465 on epoch=43
06/02/2022 03:51:04 - INFO - __main__ - Global step 700 Train loss 0.709644 Classification-F1 0.43345863386191325 on epoch=43
06/02/2022 03:51:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.775235 on epoch=44
06/02/2022 03:51:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.764364 on epoch=44
06/02/2022 03:51:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.814636 on epoch=45
06/02/2022 03:51:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.852121 on epoch=46
06/02/2022 03:51:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.701043 on epoch=46
06/02/2022 03:51:32 - INFO - __main__ - Global step 750 Train loss 0.781480 Classification-F1 0.4586574250306645 on epoch=46
06/02/2022 03:51:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.702071 on epoch=47
06/02/2022 03:51:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.696784 on epoch=48
06/02/2022 03:51:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.730170 on epoch=48
06/02/2022 03:51:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.676652 on epoch=49
06/02/2022 03:51:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.694112 on epoch=49
06/02/2022 03:52:00 - INFO - __main__ - Global step 800 Train loss 0.699958 Classification-F1 0.33842993099484786 on epoch=49
06/02/2022 03:52:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.786657 on epoch=50
06/02/2022 03:52:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.673372 on epoch=51
06/02/2022 03:52:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.676154 on epoch=51
06/02/2022 03:52:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.704321 on epoch=52
06/02/2022 03:52:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.653554 on epoch=53
06/02/2022 03:52:29 - INFO - __main__ - Global step 850 Train loss 0.698812 Classification-F1 0.5543152477199584 on epoch=53
06/02/2022 03:52:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.652023 on epoch=53
06/02/2022 03:52:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.644011 on epoch=54
06/02/2022 03:52:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.656232 on epoch=54
06/02/2022 03:52:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.689467 on epoch=55
06/02/2022 03:52:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.542557 on epoch=56
06/02/2022 03:52:58 - INFO - __main__ - Global step 900 Train loss 0.636858 Classification-F1 0.5403368102655945 on epoch=56
06/02/2022 03:53:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.606884 on epoch=56
06/02/2022 03:53:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.491786 on epoch=57
06/02/2022 03:53:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.511636 on epoch=58
06/02/2022 03:53:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.575934 on epoch=58
06/02/2022 03:53:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.576833 on epoch=59
06/02/2022 03:53:25 - INFO - __main__ - Global step 950 Train loss 0.552615 Classification-F1 0.4072508631719158 on epoch=59
06/02/2022 03:53:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.546406 on epoch=59
06/02/2022 03:53:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.476046 on epoch=60
06/02/2022 03:53:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.515385 on epoch=61
06/02/2022 03:53:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.602581 on epoch=61
06/02/2022 03:53:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.548932 on epoch=62
06/02/2022 03:53:53 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:53:53 - INFO - __main__ - Printing 3 examples
06/02/2022 03:53:53 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/02/2022 03:53:53 - INFO - __main__ - ['sad']
06/02/2022 03:53:53 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/02/2022 03:53:53 - INFO - __main__ - ['sad']
06/02/2022 03:53:53 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/02/2022 03:53:53 - INFO - __main__ - ['sad']
06/02/2022 03:53:53 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:53:53 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:53:53 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 03:53:53 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:53:53 - INFO - __main__ - Printing 3 examples
06/02/2022 03:53:53 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
06/02/2022 03:53:53 - INFO - __main__ - ['sad']
06/02/2022 03:53:53 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
06/02/2022 03:53:53 - INFO - __main__ - ['sad']
06/02/2022 03:53:53 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
06/02/2022 03:53:53 - INFO - __main__ - ['sad']
06/02/2022 03:53:53 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:53:53 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:53:54 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 03:53:54 - INFO - __main__ - Global step 1000 Train loss 0.537870 Classification-F1 0.4743276382233838 on epoch=62
06/02/2022 03:53:54 - INFO - __main__ - save last model!
06/02/2022 03:54:01 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 03:54:02 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 03:54:02 - INFO - __main__ - Printing 3 examples
06/02/2022 03:54:02 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 03:54:02 - INFO - __main__ - ['others']
06/02/2022 03:54:02 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 03:54:02 - INFO - __main__ - ['others']
06/02/2022 03:54:02 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 03:54:02 - INFO - __main__ - ['others']
06/02/2022 03:54:02 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:54:04 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:54:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 03:54:06 - INFO - __main__ - Starting training!
06/02/2022 03:54:09 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 03:54:51 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_21_0.0003_8_predictions.txt
06/02/2022 03:54:51 - INFO - __main__ - Classification-F1 on test data: 0.3935
06/02/2022 03:54:52 - INFO - __main__ - prefix=emo_64_21, lr=0.0003, bsz=8, dev_performance=0.5543152477199584, test_performance=0.3935351669088296
06/02/2022 03:54:52 - INFO - __main__ - Running ... prefix=emo_64_21, lr=0.0002, bsz=8 ...
06/02/2022 03:54:53 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:54:53 - INFO - __main__ - Printing 3 examples
06/02/2022 03:54:53 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/02/2022 03:54:53 - INFO - __main__ - ['sad']
06/02/2022 03:54:53 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/02/2022 03:54:53 - INFO - __main__ - ['sad']
06/02/2022 03:54:53 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/02/2022 03:54:53 - INFO - __main__ - ['sad']
06/02/2022 03:54:53 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:54:53 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:54:53 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 03:54:53 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 03:54:53 - INFO - __main__ - Printing 3 examples
06/02/2022 03:54:53 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
06/02/2022 03:54:53 - INFO - __main__ - ['sad']
06/02/2022 03:54:53 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
06/02/2022 03:54:53 - INFO - __main__ - ['sad']
06/02/2022 03:54:53 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
06/02/2022 03:54:53 - INFO - __main__ - ['sad']
06/02/2022 03:54:53 - INFO - __main__ - Tokenizing Input ...
06/02/2022 03:54:53 - INFO - __main__ - Tokenizing Output ...
06/02/2022 03:54:53 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 03:55:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 03:55:04 - INFO - __main__ - Starting training!
06/02/2022 03:55:09 - INFO - __main__ - Step 10 Global step 10 Train loss 25.672390 on epoch=0
06/02/2022 03:55:15 - INFO - __main__ - Step 20 Global step 20 Train loss 20.250662 on epoch=1
06/02/2022 03:55:21 - INFO - __main__ - Step 30 Global step 30 Train loss 18.659336 on epoch=1
06/02/2022 03:55:26 - INFO - __main__ - Step 40 Global step 40 Train loss 17.349270 on epoch=2
06/02/2022 03:55:31 - INFO - __main__ - Step 50 Global step 50 Train loss 16.712925 on epoch=3
06/02/2022 03:56:21 - INFO - __main__ - Global step 50 Train loss 19.728918 Classification-F1 0.00018876828692779613 on epoch=3
06/02/2022 03:56:27 - INFO - __main__ - Step 60 Global step 60 Train loss 16.086519 on epoch=3
06/02/2022 03:56:32 - INFO - __main__ - Step 70 Global step 70 Train loss 16.033110 on epoch=4
06/02/2022 03:56:38 - INFO - __main__ - Step 80 Global step 80 Train loss 15.626444 on epoch=4
06/02/2022 03:56:43 - INFO - __main__ - Step 90 Global step 90 Train loss 14.643311 on epoch=5
06/02/2022 03:56:48 - INFO - __main__ - Step 100 Global step 100 Train loss 13.764521 on epoch=6
06/02/2022 03:57:03 - INFO - __main__ - Global step 100 Train loss 15.230782 Classification-F1 0.0005494505494505495 on epoch=6
06/02/2022 03:57:09 - INFO - __main__ - Step 110 Global step 110 Train loss 13.806506 on epoch=6
06/02/2022 03:57:14 - INFO - __main__ - Step 120 Global step 120 Train loss 13.044991 on epoch=7
06/02/2022 03:57:20 - INFO - __main__ - Step 130 Global step 130 Train loss 12.833705 on epoch=8
06/02/2022 03:57:25 - INFO - __main__ - Step 140 Global step 140 Train loss 12.365737 on epoch=8
06/02/2022 03:57:30 - INFO - __main__ - Step 150 Global step 150 Train loss 11.018064 on epoch=9
06/02/2022 03:57:38 - INFO - __main__ - Global step 150 Train loss 12.613800 Classification-F1 0.0006993006993006993 on epoch=9
06/02/2022 03:57:44 - INFO - __main__ - Step 160 Global step 160 Train loss 10.226623 on epoch=9
06/02/2022 03:57:50 - INFO - __main__ - Step 170 Global step 170 Train loss 10.303706 on epoch=10
06/02/2022 03:57:55 - INFO - __main__ - Step 180 Global step 180 Train loss 7.673660 on epoch=11
06/02/2022 03:58:00 - INFO - __main__ - Step 190 Global step 190 Train loss 6.953462 on epoch=11
06/02/2022 03:58:05 - INFO - __main__ - Step 200 Global step 200 Train loss 5.084543 on epoch=12
06/02/2022 03:58:07 - INFO - __main__ - Global step 200 Train loss 8.048399 Classification-F1 0.07999999999999999 on epoch=12
06/02/2022 03:58:13 - INFO - __main__ - Step 210 Global step 210 Train loss 3.453458 on epoch=13
06/02/2022 03:58:18 - INFO - __main__ - Step 220 Global step 220 Train loss 3.488078 on epoch=13
06/02/2022 03:58:23 - INFO - __main__ - Step 230 Global step 230 Train loss 3.492141 on epoch=14
06/02/2022 03:58:29 - INFO - __main__ - Step 240 Global step 240 Train loss 3.618165 on epoch=14
06/02/2022 03:58:34 - INFO - __main__ - Step 250 Global step 250 Train loss 3.125651 on epoch=15
06/02/2022 03:58:36 - INFO - __main__ - Global step 250 Train loss 3.435499 Classification-F1 0.1 on epoch=15
06/02/2022 03:58:42 - INFO - __main__ - Step 260 Global step 260 Train loss 3.228168 on epoch=16
06/02/2022 03:58:47 - INFO - __main__ - Step 270 Global step 270 Train loss 2.690932 on epoch=16
06/02/2022 03:58:53 - INFO - __main__ - Step 280 Global step 280 Train loss 3.152914 on epoch=17
06/02/2022 03:58:58 - INFO - __main__ - Step 290 Global step 290 Train loss 3.161850 on epoch=18
06/02/2022 03:59:03 - INFO - __main__ - Step 300 Global step 300 Train loss 3.127346 on epoch=18
06/02/2022 03:59:05 - INFO - __main__ - Global step 300 Train loss 3.072242 Classification-F1 0.28232648381691505 on epoch=18
06/02/2022 03:59:11 - INFO - __main__ - Step 310 Global step 310 Train loss 2.536064 on epoch=19
06/02/2022 03:59:16 - INFO - __main__ - Step 320 Global step 320 Train loss 3.058443 on epoch=19
06/02/2022 03:59:21 - INFO - __main__ - Step 330 Global step 330 Train loss 2.807134 on epoch=20
06/02/2022 03:59:26 - INFO - __main__ - Step 340 Global step 340 Train loss 2.464838 on epoch=21
06/02/2022 03:59:32 - INFO - __main__ - Step 350 Global step 350 Train loss 2.376431 on epoch=21
06/02/2022 03:59:34 - INFO - __main__ - Global step 350 Train loss 2.648582 Classification-F1 0.2846724613101547 on epoch=21
06/02/2022 03:59:40 - INFO - __main__ - Step 360 Global step 360 Train loss 2.427930 on epoch=22
06/02/2022 03:59:45 - INFO - __main__ - Step 370 Global step 370 Train loss 3.071543 on epoch=23
06/02/2022 03:59:50 - INFO - __main__ - Step 380 Global step 380 Train loss 1.949136 on epoch=23
06/02/2022 03:59:56 - INFO - __main__ - Step 390 Global step 390 Train loss 2.564374 on epoch=24
06/02/2022 04:00:01 - INFO - __main__ - Step 400 Global step 400 Train loss 2.268897 on epoch=24
06/02/2022 04:00:03 - INFO - __main__ - Global step 400 Train loss 2.456376 Classification-F1 0.30876314162028445 on epoch=24
06/02/2022 04:00:09 - INFO - __main__ - Step 410 Global step 410 Train loss 2.252849 on epoch=25
06/02/2022 04:00:14 - INFO - __main__ - Step 420 Global step 420 Train loss 1.803952 on epoch=26
06/02/2022 04:00:19 - INFO - __main__ - Step 430 Global step 430 Train loss 2.061501 on epoch=26
06/02/2022 04:00:24 - INFO - __main__ - Step 440 Global step 440 Train loss 2.343963 on epoch=27
06/02/2022 04:00:29 - INFO - __main__ - Step 450 Global step 450 Train loss 1.920441 on epoch=28
06/02/2022 04:00:31 - INFO - __main__ - Global step 450 Train loss 2.076542 Classification-F1 0.46692002940273114 on epoch=28
06/02/2022 04:00:38 - INFO - __main__ - Step 460 Global step 460 Train loss 2.067878 on epoch=28
06/02/2022 04:00:43 - INFO - __main__ - Step 470 Global step 470 Train loss 1.939092 on epoch=29
06/02/2022 04:00:48 - INFO - __main__ - Step 480 Global step 480 Train loss 1.567731 on epoch=29
06/02/2022 04:00:53 - INFO - __main__ - Step 490 Global step 490 Train loss 1.575309 on epoch=30
06/02/2022 04:00:58 - INFO - __main__ - Step 500 Global step 500 Train loss 1.437642 on epoch=31
06/02/2022 04:01:00 - INFO - __main__ - Global step 500 Train loss 1.717530 Classification-F1 0.4174239416652712 on epoch=31
06/02/2022 04:01:05 - INFO - __main__ - Step 510 Global step 510 Train loss 1.254434 on epoch=31
06/02/2022 04:01:11 - INFO - __main__ - Step 520 Global step 520 Train loss 1.216475 on epoch=32
06/02/2022 04:01:16 - INFO - __main__ - Step 530 Global step 530 Train loss 1.042649 on epoch=33
06/02/2022 04:01:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.652351 on epoch=33
06/02/2022 04:01:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.685934 on epoch=34
06/02/2022 04:01:28 - INFO - __main__ - Global step 550 Train loss 0.970369 Classification-F1 0.5757343486088039 on epoch=34
06/02/2022 04:01:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.606305 on epoch=34
06/02/2022 04:01:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.518735 on epoch=35
06/02/2022 04:01:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.416679 on epoch=36
06/02/2022 04:01:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.487043 on epoch=36
06/02/2022 04:01:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.306374 on epoch=37
06/02/2022 04:01:57 - INFO - __main__ - Global step 600 Train loss 0.467027 Classification-F1 0.6888735120533782 on epoch=37
06/02/2022 04:02:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.419611 on epoch=38
06/02/2022 04:02:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.400371 on epoch=38
06/02/2022 04:02:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.296651 on epoch=39
06/02/2022 04:02:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.396421 on epoch=39
06/02/2022 04:02:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.386546 on epoch=40
06/02/2022 04:02:26 - INFO - __main__ - Global step 650 Train loss 0.379920 Classification-F1 0.7218777142980455 on epoch=40
06/02/2022 04:02:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.210572 on epoch=41
06/02/2022 04:02:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.354208 on epoch=41
06/02/2022 04:02:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.280011 on epoch=42
06/02/2022 04:02:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.246162 on epoch=43
06/02/2022 04:02:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.224033 on epoch=43
06/02/2022 04:02:54 - INFO - __main__ - Global step 700 Train loss 0.262997 Classification-F1 0.7301892323777666 on epoch=43
06/02/2022 04:03:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.215519 on epoch=44
06/02/2022 04:03:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.160123 on epoch=44
06/02/2022 04:03:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.226996 on epoch=45
06/02/2022 04:03:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.211707 on epoch=46
06/02/2022 04:03:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.300686 on epoch=46
06/02/2022 04:03:24 - INFO - __main__ - Global step 750 Train loss 0.223006 Classification-F1 0.6695715930411053 on epoch=46
06/02/2022 04:03:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.162031 on epoch=47
06/02/2022 04:03:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.119969 on epoch=48
06/02/2022 04:03:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.124982 on epoch=48
06/02/2022 04:03:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.133036 on epoch=49
06/02/2022 04:03:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.098450 on epoch=49
06/02/2022 04:03:52 - INFO - __main__ - Global step 800 Train loss 0.127694 Classification-F1 0.7653987803241535 on epoch=49
06/02/2022 04:03:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.168721 on epoch=50
06/02/2022 04:04:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.097718 on epoch=51
06/02/2022 04:04:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.130528 on epoch=51
06/02/2022 04:04:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.094936 on epoch=52
06/02/2022 04:04:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.077540 on epoch=53
06/02/2022 04:04:21 - INFO - __main__ - Global step 850 Train loss 0.113889 Classification-F1 0.7605271660370647 on epoch=53
06/02/2022 04:04:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.125814 on epoch=53
06/02/2022 04:04:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.088888 on epoch=54
06/02/2022 04:04:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.038040 on epoch=54
06/02/2022 04:04:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.064309 on epoch=55
06/02/2022 04:04:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.076944 on epoch=56
06/02/2022 04:04:50 - INFO - __main__ - Global step 900 Train loss 0.078799 Classification-F1 0.7775010491606715 on epoch=56
06/02/2022 04:04:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.055229 on epoch=56
06/02/2022 04:05:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.044900 on epoch=57
06/02/2022 04:05:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.034810 on epoch=58
06/02/2022 04:05:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.071433 on epoch=58
06/02/2022 04:05:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.089334 on epoch=59
06/02/2022 04:05:19 - INFO - __main__ - Global step 950 Train loss 0.059141 Classification-F1 0.7827750429873026 on epoch=59
06/02/2022 04:05:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.034740 on epoch=59
06/02/2022 04:05:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.053074 on epoch=60
06/02/2022 04:05:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.054766 on epoch=61
06/02/2022 04:05:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.045551 on epoch=61
06/02/2022 04:05:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.052028 on epoch=62
06/02/2022 04:05:47 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:05:47 - INFO - __main__ - Printing 3 examples
06/02/2022 04:05:47 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/02/2022 04:05:47 - INFO - __main__ - ['sad']
06/02/2022 04:05:47 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/02/2022 04:05:47 - INFO - __main__ - ['sad']
06/02/2022 04:05:47 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/02/2022 04:05:47 - INFO - __main__ - ['sad']
06/02/2022 04:05:47 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:05:47 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:05:47 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 04:05:47 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:05:47 - INFO - __main__ - Printing 3 examples
06/02/2022 04:05:47 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
06/02/2022 04:05:47 - INFO - __main__ - ['sad']
06/02/2022 04:05:47 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
06/02/2022 04:05:47 - INFO - __main__ - ['sad']
06/02/2022 04:05:47 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
06/02/2022 04:05:47 - INFO - __main__ - ['sad']
06/02/2022 04:05:47 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:05:47 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:05:47 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 04:05:48 - INFO - __main__ - Global step 1000 Train loss 0.048032 Classification-F1 0.7714023425108723 on epoch=62
06/02/2022 04:05:48 - INFO - __main__ - save last model!
06/02/2022 04:05:55 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 04:05:56 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 04:05:56 - INFO - __main__ - Printing 3 examples
06/02/2022 04:05:56 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 04:05:56 - INFO - __main__ - ['others']
06/02/2022 04:05:56 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 04:05:56 - INFO - __main__ - ['others']
06/02/2022 04:05:56 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 04:05:56 - INFO - __main__ - ['others']
06/02/2022 04:05:56 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:05:58 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:05:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 04:05:58 - INFO - __main__ - Starting training!
06/02/2022 04:06:03 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 04:07:00 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_21_0.0002_8_predictions.txt
06/02/2022 04:07:00 - INFO - __main__ - Classification-F1 on test data: 0.1655
06/02/2022 04:07:00 - INFO - __main__ - prefix=emo_64_21, lr=0.0002, bsz=8, dev_performance=0.7827750429873026, test_performance=0.16551073054442525
06/02/2022 04:07:00 - INFO - __main__ - Running ... prefix=emo_64_21, lr=0.0001, bsz=8 ...
06/02/2022 04:07:01 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:07:01 - INFO - __main__ - Printing 3 examples
06/02/2022 04:07:01 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/02/2022 04:07:01 - INFO - __main__ - ['sad']
06/02/2022 04:07:01 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/02/2022 04:07:01 - INFO - __main__ - ['sad']
06/02/2022 04:07:01 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/02/2022 04:07:01 - INFO - __main__ - ['sad']
06/02/2022 04:07:01 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:07:01 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:07:02 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 04:07:02 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:07:02 - INFO - __main__ - Printing 3 examples
06/02/2022 04:07:02 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
06/02/2022 04:07:02 - INFO - __main__ - ['sad']
06/02/2022 04:07:02 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
06/02/2022 04:07:02 - INFO - __main__ - ['sad']
06/02/2022 04:07:02 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
06/02/2022 04:07:02 - INFO - __main__ - ['sad']
06/02/2022 04:07:02 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:07:02 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:07:02 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 04:07:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 04:07:15 - INFO - __main__ - Starting training!
06/02/2022 04:07:19 - INFO - __main__ - Step 10 Global step 10 Train loss 25.340366 on epoch=0
06/02/2022 04:07:24 - INFO - __main__ - Step 20 Global step 20 Train loss 22.205713 on epoch=1
06/02/2022 04:07:29 - INFO - __main__ - Step 30 Global step 30 Train loss 20.210381 on epoch=1
06/02/2022 04:07:34 - INFO - __main__ - Step 40 Global step 40 Train loss 18.564310 on epoch=2
06/02/2022 04:07:39 - INFO - __main__ - Step 50 Global step 50 Train loss 19.125744 on epoch=3
06/02/2022 04:07:42 - INFO - __main__ - Global step 50 Train loss 21.089304 Classification-F1 0.0 on epoch=3
06/02/2022 04:07:48 - INFO - __main__ - Step 60 Global step 60 Train loss 17.273273 on epoch=3
06/02/2022 04:07:53 - INFO - __main__ - Step 70 Global step 70 Train loss 16.917507 on epoch=4
06/02/2022 04:07:58 - INFO - __main__ - Step 80 Global step 80 Train loss 17.569807 on epoch=4
06/02/2022 04:08:03 - INFO - __main__ - Step 90 Global step 90 Train loss 16.203094 on epoch=5
06/02/2022 04:08:08 - INFO - __main__ - Step 100 Global step 100 Train loss 16.236279 on epoch=6
06/02/2022 04:08:11 - INFO - __main__ - Global step 100 Train loss 16.839993 Classification-F1 0.00039447731755424067 on epoch=6
06/02/2022 04:08:17 - INFO - __main__ - Step 110 Global step 110 Train loss 16.227467 on epoch=6
06/02/2022 04:08:22 - INFO - __main__ - Step 120 Global step 120 Train loss 15.708939 on epoch=7
06/02/2022 04:08:27 - INFO - __main__ - Step 130 Global step 130 Train loss 14.523259 on epoch=8
06/02/2022 04:08:32 - INFO - __main__ - Step 140 Global step 140 Train loss 15.960002 on epoch=8
06/02/2022 04:08:37 - INFO - __main__ - Step 150 Global step 150 Train loss 13.803805 on epoch=9
06/02/2022 04:08:40 - INFO - __main__ - Global step 150 Train loss 15.244694 Classification-F1 0.00042735042735042735 on epoch=9
06/02/2022 04:08:45 - INFO - __main__ - Step 160 Global step 160 Train loss 13.794907 on epoch=9
06/02/2022 04:08:50 - INFO - __main__ - Step 170 Global step 170 Train loss 14.222189 on epoch=10
06/02/2022 04:08:55 - INFO - __main__ - Step 180 Global step 180 Train loss 13.047012 on epoch=11
06/02/2022 04:09:01 - INFO - __main__ - Step 190 Global step 190 Train loss 13.438812 on epoch=11
06/02/2022 04:09:06 - INFO - __main__ - Step 200 Global step 200 Train loss 13.597303 on epoch=12
06/02/2022 04:09:08 - INFO - __main__ - Global step 200 Train loss 13.620045 Classification-F1 0.0 on epoch=12
06/02/2022 04:09:13 - INFO - __main__ - Step 210 Global step 210 Train loss 12.428440 on epoch=13
06/02/2022 04:09:19 - INFO - __main__ - Step 220 Global step 220 Train loss 12.229696 on epoch=13
06/02/2022 04:09:24 - INFO - __main__ - Step 230 Global step 230 Train loss 11.624511 on epoch=14
06/02/2022 04:09:29 - INFO - __main__ - Step 240 Global step 240 Train loss 11.992417 on epoch=14
06/02/2022 04:09:34 - INFO - __main__ - Step 250 Global step 250 Train loss 11.977510 on epoch=15
06/02/2022 04:09:37 - INFO - __main__ - Global step 250 Train loss 12.050515 Classification-F1 0.0008547008547008547 on epoch=15
06/02/2022 04:09:42 - INFO - __main__ - Step 260 Global step 260 Train loss 10.800461 on epoch=16
06/02/2022 04:09:47 - INFO - __main__ - Step 270 Global step 270 Train loss 11.246344 on epoch=16
06/02/2022 04:09:52 - INFO - __main__ - Step 280 Global step 280 Train loss 10.471255 on epoch=17
06/02/2022 04:09:57 - INFO - __main__ - Step 290 Global step 290 Train loss 9.944394 on epoch=18
06/02/2022 04:10:03 - INFO - __main__ - Step 300 Global step 300 Train loss 10.210993 on epoch=18
06/02/2022 04:10:05 - INFO - __main__ - Global step 300 Train loss 10.534689 Classification-F1 0.0023399677245831093 on epoch=18
06/02/2022 04:10:11 - INFO - __main__ - Step 310 Global step 310 Train loss 9.336210 on epoch=19
06/02/2022 04:10:16 - INFO - __main__ - Step 320 Global step 320 Train loss 8.504837 on epoch=19
06/02/2022 04:10:21 - INFO - __main__ - Step 330 Global step 330 Train loss 7.310661 on epoch=20
06/02/2022 04:10:26 - INFO - __main__ - Step 340 Global step 340 Train loss 5.848910 on epoch=21
06/02/2022 04:10:32 - INFO - __main__ - Step 350 Global step 350 Train loss 5.745547 on epoch=21
06/02/2022 04:10:34 - INFO - __main__ - Global step 350 Train loss 7.349234 Classification-F1 0.0020512820512820513 on epoch=21
06/02/2022 04:10:39 - INFO - __main__ - Step 360 Global step 360 Train loss 4.629676 on epoch=22
06/02/2022 04:10:44 - INFO - __main__ - Step 370 Global step 370 Train loss 3.725893 on epoch=23
06/02/2022 04:10:50 - INFO - __main__ - Step 380 Global step 380 Train loss 4.391828 on epoch=23
06/02/2022 04:10:55 - INFO - __main__ - Step 390 Global step 390 Train loss 3.998185 on epoch=24
06/02/2022 04:11:00 - INFO - __main__ - Step 400 Global step 400 Train loss 4.433521 on epoch=24
06/02/2022 04:11:18 - INFO - __main__ - Global step 400 Train loss 4.235821 Classification-F1 0.03501963501963502 on epoch=24
06/02/2022 04:11:24 - INFO - __main__ - Step 410 Global step 410 Train loss 4.542668 on epoch=25
06/02/2022 04:11:29 - INFO - __main__ - Step 420 Global step 420 Train loss 2.396007 on epoch=26
06/02/2022 04:11:34 - INFO - __main__ - Step 430 Global step 430 Train loss 1.240252 on epoch=26
06/02/2022 04:11:40 - INFO - __main__ - Step 440 Global step 440 Train loss 1.557104 on epoch=27
06/02/2022 04:11:46 - INFO - __main__ - Step 450 Global step 450 Train loss 1.469723 on epoch=28
06/02/2022 04:11:48 - INFO - __main__ - Global step 450 Train loss 2.241151 Classification-F1 0.4884007157224767 on epoch=28
06/02/2022 04:11:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.971956 on epoch=28
06/02/2022 04:11:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.861012 on epoch=29
06/02/2022 04:12:05 - INFO - __main__ - Step 480 Global step 480 Train loss 1.054396 on epoch=29
06/02/2022 04:12:11 - INFO - __main__ - Step 490 Global step 490 Train loss 1.091003 on epoch=30
06/02/2022 04:12:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.924456 on epoch=31
06/02/2022 04:12:18 - INFO - __main__ - Global step 500 Train loss 0.980565 Classification-F1 0.6900166064228564 on epoch=31
06/02/2022 04:12:24 - INFO - __main__ - Step 510 Global step 510 Train loss 1.051356 on epoch=31
06/02/2022 04:12:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.909872 on epoch=32
06/02/2022 04:12:34 - INFO - __main__ - Step 530 Global step 530 Train loss 1.282738 on epoch=33
06/02/2022 04:12:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.835071 on epoch=33
06/02/2022 04:12:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.874425 on epoch=34
06/02/2022 04:12:46 - INFO - __main__ - Global step 550 Train loss 0.990692 Classification-F1 0.5881102490572989 on epoch=34
06/02/2022 04:12:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.832093 on epoch=34
06/02/2022 04:12:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.943581 on epoch=35
06/02/2022 04:13:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.912829 on epoch=36
06/02/2022 04:13:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.467011 on epoch=36
06/02/2022 04:13:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.752105 on epoch=37
06/02/2022 04:13:13 - INFO - __main__ - Global step 600 Train loss 0.781524 Classification-F1 0.7153793388700509 on epoch=37
06/02/2022 04:13:19 - INFO - __main__ - Step 610 Global step 610 Train loss 1.126481 on epoch=38
06/02/2022 04:13:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.892822 on epoch=38
06/02/2022 04:13:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.796839 on epoch=39
06/02/2022 04:13:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.840117 on epoch=39
06/02/2022 04:13:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.718854 on epoch=40
06/02/2022 04:13:42 - INFO - __main__ - Global step 650 Train loss 0.875022 Classification-F1 0.7258150793553694 on epoch=40
06/02/2022 04:13:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.696449 on epoch=41
06/02/2022 04:13:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.617521 on epoch=41
06/02/2022 04:13:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.702553 on epoch=42
06/02/2022 04:14:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.793379 on epoch=43
06/02/2022 04:14:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.638608 on epoch=43
06/02/2022 04:14:10 - INFO - __main__ - Global step 700 Train loss 0.689702 Classification-F1 0.7339281370116284 on epoch=43
06/02/2022 04:14:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.597031 on epoch=44
06/02/2022 04:14:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.966379 on epoch=44
06/02/2022 04:14:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.640605 on epoch=45
06/02/2022 04:14:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.570566 on epoch=46
06/02/2022 04:14:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.547002 on epoch=46
06/02/2022 04:14:38 - INFO - __main__ - Global step 750 Train loss 0.664316 Classification-F1 0.7154342986809883 on epoch=46
06/02/2022 04:14:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.756737 on epoch=47
06/02/2022 04:14:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.874014 on epoch=48
06/02/2022 04:14:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.306266 on epoch=48
06/02/2022 04:14:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.378602 on epoch=49
06/02/2022 04:15:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.500727 on epoch=49
06/02/2022 04:15:05 - INFO - __main__ - Global step 800 Train loss 0.563269 Classification-F1 0.6261136909094271 on epoch=49
06/02/2022 04:15:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.535626 on epoch=50
06/02/2022 04:15:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.638767 on epoch=51
06/02/2022 04:15:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.449888 on epoch=51
06/02/2022 04:15:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.484236 on epoch=52
06/02/2022 04:15:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.603354 on epoch=53
06/02/2022 04:15:33 - INFO - __main__ - Global step 850 Train loss 0.542374 Classification-F1 0.6924969459689769 on epoch=53
06/02/2022 04:15:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.583017 on epoch=53
06/02/2022 04:15:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.316529 on epoch=54
06/02/2022 04:15:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.787678 on epoch=54
06/02/2022 04:15:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.601043 on epoch=55
06/02/2022 04:15:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.659696 on epoch=56
06/02/2022 04:16:00 - INFO - __main__ - Global step 900 Train loss 0.589592 Classification-F1 0.719988821657596 on epoch=56
06/02/2022 04:16:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.348868 on epoch=56
06/02/2022 04:16:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.401903 on epoch=57
06/02/2022 04:16:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.459236 on epoch=58
06/02/2022 04:16:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.356932 on epoch=58
06/02/2022 04:16:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.422791 on epoch=59
06/02/2022 04:16:28 - INFO - __main__ - Global step 950 Train loss 0.397946 Classification-F1 0.7080947088425744 on epoch=59
06/02/2022 04:16:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.355007 on epoch=59
06/02/2022 04:16:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.382560 on epoch=60
06/02/2022 04:16:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.457289 on epoch=61
06/02/2022 04:16:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.503887 on epoch=61
06/02/2022 04:16:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.392787 on epoch=62
06/02/2022 04:16:55 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:16:55 - INFO - __main__ - Printing 3 examples
06/02/2022 04:16:55 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/02/2022 04:16:55 - INFO - __main__ - ['happy']
06/02/2022 04:16:55 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/02/2022 04:16:55 - INFO - __main__ - ['happy']
06/02/2022 04:16:55 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/02/2022 04:16:55 - INFO - __main__ - ['happy']
06/02/2022 04:16:55 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:16:55 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:16:55 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 04:16:55 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:16:55 - INFO - __main__ - Printing 3 examples
06/02/2022 04:16:55 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
06/02/2022 04:16:55 - INFO - __main__ - ['happy']
06/02/2022 04:16:55 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
06/02/2022 04:16:55 - INFO - __main__ - ['happy']
06/02/2022 04:16:55 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
06/02/2022 04:16:55 - INFO - __main__ - ['happy']
06/02/2022 04:16:55 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:16:55 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:16:56 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 04:16:56 - INFO - __main__ - Global step 1000 Train loss 0.418306 Classification-F1 0.7417435468787745 on epoch=62
06/02/2022 04:16:56 - INFO - __main__ - save last model!
06/02/2022 04:17:03 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 04:17:04 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 04:17:04 - INFO - __main__ - Printing 3 examples
06/02/2022 04:17:04 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 04:17:04 - INFO - __main__ - ['others']
06/02/2022 04:17:04 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 04:17:04 - INFO - __main__ - ['others']
06/02/2022 04:17:04 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 04:17:04 - INFO - __main__ - ['others']
06/02/2022 04:17:04 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:17:06 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:17:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 04:17:09 - INFO - __main__ - Starting training!
06/02/2022 04:17:12 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 04:17:56 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_21_0.0001_8_predictions.txt
06/02/2022 04:17:56 - INFO - __main__ - Classification-F1 on test data: 0.0994
06/02/2022 04:17:57 - INFO - __main__ - prefix=emo_64_21, lr=0.0001, bsz=8, dev_performance=0.7417435468787745, test_performance=0.09943986144781676
06/02/2022 04:17:57 - INFO - __main__ - Running ... prefix=emo_64_42, lr=0.0005, bsz=8 ...
06/02/2022 04:17:58 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:17:58 - INFO - __main__ - Printing 3 examples
06/02/2022 04:17:58 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/02/2022 04:17:58 - INFO - __main__ - ['happy']
06/02/2022 04:17:58 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/02/2022 04:17:58 - INFO - __main__ - ['happy']
06/02/2022 04:17:58 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/02/2022 04:17:58 - INFO - __main__ - ['happy']
06/02/2022 04:17:58 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:17:58 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:17:58 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 04:17:58 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:17:58 - INFO - __main__ - Printing 3 examples
06/02/2022 04:17:58 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
06/02/2022 04:17:58 - INFO - __main__ - ['happy']
06/02/2022 04:17:58 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
06/02/2022 04:17:58 - INFO - __main__ - ['happy']
06/02/2022 04:17:58 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
06/02/2022 04:17:58 - INFO - __main__ - ['happy']
06/02/2022 04:17:58 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:17:58 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:17:58 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 04:18:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 04:18:11 - INFO - __main__ - Starting training!
06/02/2022 04:18:15 - INFO - __main__ - Step 10 Global step 10 Train loss 25.453016 on epoch=0
06/02/2022 04:18:20 - INFO - __main__ - Step 20 Global step 20 Train loss 18.462833 on epoch=1
06/02/2022 04:18:25 - INFO - __main__ - Step 30 Global step 30 Train loss 16.085064 on epoch=1
06/02/2022 04:18:30 - INFO - __main__ - Step 40 Global step 40 Train loss 14.750826 on epoch=2
06/02/2022 04:18:35 - INFO - __main__ - Step 50 Global step 50 Train loss 12.605670 on epoch=3
06/02/2022 04:18:38 - INFO - __main__ - Global step 50 Train loss 17.471481 Classification-F1 0.0 on epoch=3
06/02/2022 04:18:43 - INFO - __main__ - Step 60 Global step 60 Train loss 11.536756 on epoch=3
06/02/2022 04:18:48 - INFO - __main__ - Step 70 Global step 70 Train loss 9.928544 on epoch=4
06/02/2022 04:18:53 - INFO - __main__ - Step 80 Global step 80 Train loss 6.100262 on epoch=4
06/02/2022 04:18:58 - INFO - __main__ - Step 90 Global step 90 Train loss 3.533866 on epoch=5
06/02/2022 04:19:03 - INFO - __main__ - Step 100 Global step 100 Train loss 3.255232 on epoch=6
06/02/2022 04:19:05 - INFO - __main__ - Global step 100 Train loss 6.870932 Classification-F1 0.21606265356265358 on epoch=6
06/02/2022 04:19:11 - INFO - __main__ - Step 110 Global step 110 Train loss 3.434725 on epoch=6
06/02/2022 04:19:16 - INFO - __main__ - Step 120 Global step 120 Train loss 3.500519 on epoch=7
06/02/2022 04:19:21 - INFO - __main__ - Step 130 Global step 130 Train loss 2.841211 on epoch=8
06/02/2022 04:19:25 - INFO - __main__ - Step 140 Global step 140 Train loss 2.593301 on epoch=8
06/02/2022 04:19:30 - INFO - __main__ - Step 150 Global step 150 Train loss 2.283652 on epoch=9
06/02/2022 04:19:32 - INFO - __main__ - Global step 150 Train loss 2.930681 Classification-F1 0.1 on epoch=9
06/02/2022 04:19:37 - INFO - __main__ - Step 160 Global step 160 Train loss 2.134608 on epoch=9
06/02/2022 04:19:42 - INFO - __main__ - Step 170 Global step 170 Train loss 1.549008 on epoch=10
06/02/2022 04:19:47 - INFO - __main__ - Step 180 Global step 180 Train loss 1.503292 on epoch=11
06/02/2022 04:19:52 - INFO - __main__ - Step 190 Global step 190 Train loss 1.261528 on epoch=11
06/02/2022 04:19:57 - INFO - __main__ - Step 200 Global step 200 Train loss 1.123152 on epoch=12
06/02/2022 04:19:59 - INFO - __main__ - Global step 200 Train loss 1.514318 Classification-F1 0.3105800694724124 on epoch=12
06/02/2022 04:20:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.931407 on epoch=13
06/02/2022 04:20:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.878844 on epoch=13
06/02/2022 04:20:14 - INFO - __main__ - Step 230 Global step 230 Train loss 1.023427 on epoch=14
06/02/2022 04:20:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.791014 on epoch=14
06/02/2022 04:20:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.809203 on epoch=15
06/02/2022 04:20:26 - INFO - __main__ - Global step 250 Train loss 0.886779 Classification-F1 0.5679022043777215 on epoch=15
06/02/2022 04:20:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.717928 on epoch=16
06/02/2022 04:20:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.635713 on epoch=16
06/02/2022 04:20:42 - INFO - __main__ - Step 280 Global step 280 Train loss 2.085158 on epoch=17
06/02/2022 04:20:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.808654 on epoch=18
06/02/2022 04:20:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.838988 on epoch=18
06/02/2022 04:20:54 - INFO - __main__ - Global step 300 Train loss 1.017288 Classification-F1 0.5967533102344447 on epoch=18
06/02/2022 04:20:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.636270 on epoch=19
06/02/2022 04:21:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.559581 on epoch=19
06/02/2022 04:21:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.697454 on epoch=20
06/02/2022 04:21:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.610780 on epoch=21
06/02/2022 04:21:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.622215 on epoch=21
06/02/2022 04:21:22 - INFO - __main__ - Global step 350 Train loss 0.625260 Classification-F1 0.39570230384600114 on epoch=21
06/02/2022 04:21:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.634097 on epoch=22
06/02/2022 04:21:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.617606 on epoch=23
06/02/2022 04:21:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.483473 on epoch=23
06/02/2022 04:21:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.598669 on epoch=24
06/02/2022 04:21:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.598764 on epoch=24
06/02/2022 04:21:49 - INFO - __main__ - Global step 400 Train loss 0.586522 Classification-F1 0.5586829836829836 on epoch=24
06/02/2022 04:21:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.636580 on epoch=25
06/02/2022 04:21:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.517516 on epoch=26
06/02/2022 04:22:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.394820 on epoch=26
06/02/2022 04:22:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.480237 on epoch=27
06/02/2022 04:22:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.438706 on epoch=28
06/02/2022 04:22:16 - INFO - __main__ - Global step 450 Train loss 0.493572 Classification-F1 0.6741146599905558 on epoch=28
06/02/2022 04:22:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.345205 on epoch=28
06/02/2022 04:22:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.327885 on epoch=29
06/02/2022 04:22:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.282311 on epoch=29
06/02/2022 04:22:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.368399 on epoch=30
06/02/2022 04:22:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.314467 on epoch=31
06/02/2022 04:22:44 - INFO - __main__ - Global step 500 Train loss 0.327654 Classification-F1 0.7267914287908656 on epoch=31
06/02/2022 04:22:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.230995 on epoch=31
06/02/2022 04:22:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.151028 on epoch=32
06/02/2022 04:22:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.159007 on epoch=33
06/02/2022 04:23:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.115981 on epoch=33
06/02/2022 04:23:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.104866 on epoch=34
06/02/2022 04:23:11 - INFO - __main__ - Global step 550 Train loss 0.152375 Classification-F1 0.7531822199942864 on epoch=34
06/02/2022 04:23:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.088153 on epoch=34
06/02/2022 04:23:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.118171 on epoch=35
06/02/2022 04:23:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.116236 on epoch=36
06/02/2022 04:23:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.084044 on epoch=36
06/02/2022 04:23:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.076435 on epoch=37
06/02/2022 04:23:39 - INFO - __main__ - Global step 600 Train loss 0.096608 Classification-F1 0.7868813306403263 on epoch=37
06/02/2022 04:23:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.056209 on epoch=38
06/02/2022 04:23:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.059524 on epoch=38
06/02/2022 04:23:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.029203 on epoch=39
06/02/2022 04:24:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.042040 on epoch=39
06/02/2022 04:24:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.057400 on epoch=40
06/02/2022 04:24:07 - INFO - __main__ - Global step 650 Train loss 0.048875 Classification-F1 0.6237169941183832 on epoch=40
06/02/2022 04:24:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.060371 on epoch=41
06/02/2022 04:24:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.050822 on epoch=41
06/02/2022 04:24:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.016736 on epoch=42
06/02/2022 04:24:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.088930 on epoch=43
06/02/2022 04:24:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.031791 on epoch=43
06/02/2022 04:24:35 - INFO - __main__ - Global step 700 Train loss 0.049730 Classification-F1 0.6295209732411031 on epoch=43
06/02/2022 04:24:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.024270 on epoch=44
06/02/2022 04:24:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.014049 on epoch=44
06/02/2022 04:24:50 - INFO - __main__ - Step 730 Global step 730 Train loss 0.010559 on epoch=45
06/02/2022 04:24:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.004148 on epoch=46
06/02/2022 04:25:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.035681 on epoch=46
06/02/2022 04:25:02 - INFO - __main__ - Global step 750 Train loss 0.017741 Classification-F1 0.775897477825976 on epoch=46
06/02/2022 04:25:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.002691 on epoch=47
06/02/2022 04:25:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.033222 on epoch=48
06/02/2022 04:25:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.011736 on epoch=48
06/02/2022 04:25:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.006703 on epoch=49
06/02/2022 04:25:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.006362 on epoch=49
06/02/2022 04:25:29 - INFO - __main__ - Global step 800 Train loss 0.012143 Classification-F1 0.792263482096077 on epoch=49
06/02/2022 04:25:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.010588 on epoch=50
06/02/2022 04:25:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.003623 on epoch=51
06/02/2022 04:25:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000988 on epoch=51
06/02/2022 04:25:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.033062 on epoch=52
06/02/2022 04:25:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.006322 on epoch=53
06/02/2022 04:25:57 - INFO - __main__ - Global step 850 Train loss 0.010917 Classification-F1 0.6506404385210363 on epoch=53
06/02/2022 04:26:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.014268 on epoch=53
06/02/2022 04:26:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.032780 on epoch=54
06/02/2022 04:26:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.023550 on epoch=54
06/02/2022 04:26:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.028985 on epoch=55
06/02/2022 04:26:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.020922 on epoch=56
06/02/2022 04:26:25 - INFO - __main__ - Global step 900 Train loss 0.024101 Classification-F1 0.7760689792095574 on epoch=56
06/02/2022 04:26:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.001896 on epoch=56
06/02/2022 04:26:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.005226 on epoch=57
06/02/2022 04:26:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.001944 on epoch=58
06/02/2022 04:26:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.023653 on epoch=58
06/02/2022 04:26:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.001351 on epoch=59
06/02/2022 04:26:52 - INFO - __main__ - Global step 950 Train loss 0.006814 Classification-F1 0.7687318702290076 on epoch=59
06/02/2022 04:26:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.003575 on epoch=59
06/02/2022 04:27:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.001063 on epoch=60
06/02/2022 04:27:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.001297 on epoch=61
06/02/2022 04:27:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.005171 on epoch=61
06/02/2022 04:27:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000536 on epoch=62
06/02/2022 04:27:19 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:27:19 - INFO - __main__ - Printing 3 examples
06/02/2022 04:27:19 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/02/2022 04:27:19 - INFO - __main__ - ['happy']
06/02/2022 04:27:19 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/02/2022 04:27:19 - INFO - __main__ - ['happy']
06/02/2022 04:27:19 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/02/2022 04:27:19 - INFO - __main__ - ['happy']
06/02/2022 04:27:19 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:27:19 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:27:19 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 04:27:19 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:27:19 - INFO - __main__ - Printing 3 examples
06/02/2022 04:27:19 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
06/02/2022 04:27:19 - INFO - __main__ - ['happy']
06/02/2022 04:27:19 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
06/02/2022 04:27:19 - INFO - __main__ - ['happy']
06/02/2022 04:27:19 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
06/02/2022 04:27:19 - INFO - __main__ - ['happy']
06/02/2022 04:27:19 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:27:19 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:27:19 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 04:27:20 - INFO - __main__ - Global step 1000 Train loss 0.002328 Classification-F1 0.792113203658256 on epoch=62
06/02/2022 04:27:20 - INFO - __main__ - save last model!
06/02/2022 04:27:27 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 04:27:27 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 04:27:27 - INFO - __main__ - Printing 3 examples
06/02/2022 04:27:27 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 04:27:27 - INFO - __main__ - ['others']
06/02/2022 04:27:27 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 04:27:27 - INFO - __main__ - ['others']
06/02/2022 04:27:27 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 04:27:27 - INFO - __main__ - ['others']
06/02/2022 04:27:27 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:27:29 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:27:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 04:27:31 - INFO - __main__ - Starting training!
06/02/2022 04:27:34 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 04:28:27 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_42_0.0005_8_predictions.txt
06/02/2022 04:28:27 - INFO - __main__ - Classification-F1 on test data: 0.4322
06/02/2022 04:28:28 - INFO - __main__ - prefix=emo_64_42, lr=0.0005, bsz=8, dev_performance=0.792263482096077, test_performance=0.4322098457382115
06/02/2022 04:28:28 - INFO - __main__ - Running ... prefix=emo_64_42, lr=0.0003, bsz=8 ...
06/02/2022 04:28:29 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:28:29 - INFO - __main__ - Printing 3 examples
06/02/2022 04:28:29 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/02/2022 04:28:29 - INFO - __main__ - ['happy']
06/02/2022 04:28:29 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/02/2022 04:28:29 - INFO - __main__ - ['happy']
06/02/2022 04:28:29 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/02/2022 04:28:29 - INFO - __main__ - ['happy']
06/02/2022 04:28:29 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:28:29 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:28:29 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 04:28:29 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:28:29 - INFO - __main__ - Printing 3 examples
06/02/2022 04:28:29 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
06/02/2022 04:28:29 - INFO - __main__ - ['happy']
06/02/2022 04:28:29 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
06/02/2022 04:28:29 - INFO - __main__ - ['happy']
06/02/2022 04:28:29 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
06/02/2022 04:28:29 - INFO - __main__ - ['happy']
06/02/2022 04:28:29 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:28:29 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:28:29 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 04:28:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 04:28:40 - INFO - __main__ - Starting training!
06/02/2022 04:28:44 - INFO - __main__ - Step 10 Global step 10 Train loss 24.713596 on epoch=0
06/02/2022 04:28:50 - INFO - __main__ - Step 20 Global step 20 Train loss 19.096642 on epoch=1
06/02/2022 04:28:55 - INFO - __main__ - Step 30 Global step 30 Train loss 17.874813 on epoch=1
06/02/2022 04:29:00 - INFO - __main__ - Step 40 Global step 40 Train loss 16.598469 on epoch=2
06/02/2022 04:29:05 - INFO - __main__ - Step 50 Global step 50 Train loss 15.054875 on epoch=3
06/02/2022 04:29:09 - INFO - __main__ - Global step 50 Train loss 18.667679 Classification-F1 0.0005805515239477504 on epoch=3
06/02/2022 04:29:14 - INFO - __main__ - Step 60 Global step 60 Train loss 14.224398 on epoch=3
06/02/2022 04:29:20 - INFO - __main__ - Step 70 Global step 70 Train loss 13.400767 on epoch=4
06/02/2022 04:29:25 - INFO - __main__ - Step 80 Global step 80 Train loss 12.408220 on epoch=4
06/02/2022 04:29:30 - INFO - __main__ - Step 90 Global step 90 Train loss 11.749101 on epoch=5
06/02/2022 04:29:35 - INFO - __main__ - Step 100 Global step 100 Train loss 9.401714 on epoch=6
06/02/2022 04:29:37 - INFO - __main__ - Global step 100 Train loss 12.236839 Classification-F1 0.0 on epoch=6
06/02/2022 04:29:42 - INFO - __main__ - Step 110 Global step 110 Train loss 6.221229 on epoch=6
06/02/2022 04:29:47 - INFO - __main__ - Step 120 Global step 120 Train loss 4.372143 on epoch=7
06/02/2022 04:29:52 - INFO - __main__ - Step 130 Global step 130 Train loss 6.301181 on epoch=8
06/02/2022 04:29:57 - INFO - __main__ - Step 140 Global step 140 Train loss 4.148047 on epoch=8
06/02/2022 04:30:02 - INFO - __main__ - Step 150 Global step 150 Train loss 2.225744 on epoch=9
06/02/2022 04:30:04 - INFO - __main__ - Global step 150 Train loss 4.653669 Classification-F1 0.382187131756654 on epoch=9
06/02/2022 04:30:10 - INFO - __main__ - Step 160 Global step 160 Train loss 1.077373 on epoch=9
06/02/2022 04:30:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.917353 on epoch=10
06/02/2022 04:30:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.692512 on epoch=11
06/02/2022 04:30:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.673612 on epoch=11
06/02/2022 04:30:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.678956 on epoch=12
06/02/2022 04:30:32 - INFO - __main__ - Global step 200 Train loss 0.807961 Classification-F1 0.4846323273300961 on epoch=12
06/02/2022 04:30:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.558957 on epoch=13
06/02/2022 04:30:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.572722 on epoch=13
06/02/2022 04:30:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.552584 on epoch=14
06/02/2022 04:30:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.489897 on epoch=14
06/02/2022 04:30:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.420199 on epoch=15
06/02/2022 04:31:01 - INFO - __main__ - Global step 250 Train loss 0.518872 Classification-F1 0.6845981747297537 on epoch=15
06/02/2022 04:31:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.503226 on epoch=16
06/02/2022 04:31:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.451779 on epoch=16
06/02/2022 04:31:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.400533 on epoch=17
06/02/2022 04:31:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.424500 on epoch=18
06/02/2022 04:31:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.416641 on epoch=18
06/02/2022 04:31:29 - INFO - __main__ - Global step 300 Train loss 0.439336 Classification-F1 0.5545090685176891 on epoch=18
06/02/2022 04:31:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.336666 on epoch=19
06/02/2022 04:31:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.382194 on epoch=19
06/02/2022 04:31:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.414518 on epoch=20
06/02/2022 04:31:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.316993 on epoch=21
06/02/2022 04:31:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.371805 on epoch=21
06/02/2022 04:31:57 - INFO - __main__ - Global step 350 Train loss 0.364435 Classification-F1 0.7375858123569794 on epoch=21
06/02/2022 04:32:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.346906 on epoch=22
06/02/2022 04:32:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.226272 on epoch=23
06/02/2022 04:32:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.283271 on epoch=23
06/02/2022 04:32:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.238238 on epoch=24
06/02/2022 04:32:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.179564 on epoch=24
06/02/2022 04:32:26 - INFO - __main__ - Global step 400 Train loss 0.254850 Classification-F1 0.7241776100742798 on epoch=24
06/02/2022 04:32:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.229275 on epoch=25
06/02/2022 04:32:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.136779 on epoch=26
06/02/2022 04:32:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.187302 on epoch=26
06/02/2022 04:32:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.187299 on epoch=27
06/02/2022 04:32:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.146382 on epoch=28
06/02/2022 04:32:53 - INFO - __main__ - Global step 450 Train loss 0.177407 Classification-F1 0.765602152424402 on epoch=28
06/02/2022 04:32:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.119923 on epoch=28
06/02/2022 04:33:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.210597 on epoch=29
06/02/2022 04:33:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.160559 on epoch=29
06/02/2022 04:33:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.122559 on epoch=30
06/02/2022 04:33:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.159009 on epoch=31
06/02/2022 04:33:22 - INFO - __main__ - Global step 500 Train loss 0.154529 Classification-F1 0.6016003810431055 on epoch=31
06/02/2022 04:33:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.147170 on epoch=31
06/02/2022 04:33:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.184586 on epoch=32
06/02/2022 04:33:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.064941 on epoch=33
06/02/2022 04:33:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.191727 on epoch=33
06/02/2022 04:33:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.075385 on epoch=34
06/02/2022 04:33:49 - INFO - __main__ - Global step 550 Train loss 0.132762 Classification-F1 0.5939344826419519 on epoch=34
06/02/2022 04:33:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.137032 on epoch=34
06/02/2022 04:34:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.104871 on epoch=35
06/02/2022 04:34:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.133041 on epoch=36
06/02/2022 04:34:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.158038 on epoch=36
06/02/2022 04:34:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.089476 on epoch=37
06/02/2022 04:34:17 - INFO - __main__ - Global step 600 Train loss 0.124492 Classification-F1 0.616198347107438 on epoch=37
06/02/2022 04:34:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.054428 on epoch=38
06/02/2022 04:34:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.135637 on epoch=38
06/02/2022 04:34:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.098090 on epoch=39
06/02/2022 04:34:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.100331 on epoch=39
06/02/2022 04:34:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.082405 on epoch=40
06/02/2022 04:34:44 - INFO - __main__ - Global step 650 Train loss 0.094178 Classification-F1 0.6196991045896458 on epoch=40
06/02/2022 04:34:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.071992 on epoch=41
06/02/2022 04:34:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.064202 on epoch=41
06/02/2022 04:35:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.102489 on epoch=42
06/02/2022 04:35:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.061734 on epoch=43
06/02/2022 04:35:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.151468 on epoch=43
06/02/2022 04:35:12 - INFO - __main__ - Global step 700 Train loss 0.090377 Classification-F1 0.5544879439979258 on epoch=43
06/02/2022 04:35:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.437637 on epoch=44
06/02/2022 04:35:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.301329 on epoch=44
06/02/2022 04:35:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.310554 on epoch=45
06/02/2022 04:35:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.336545 on epoch=46
06/02/2022 04:35:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.143901 on epoch=46
06/02/2022 04:35:40 - INFO - __main__ - Global step 750 Train loss 0.305993 Classification-F1 0.643941417784335 on epoch=46
06/02/2022 04:35:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.287979 on epoch=47
06/02/2022 04:35:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.275033 on epoch=48
06/02/2022 04:35:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.222253 on epoch=48
06/02/2022 04:36:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.324911 on epoch=49
06/02/2022 04:36:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.256539 on epoch=49
06/02/2022 04:36:08 - INFO - __main__ - Global step 800 Train loss 0.273343 Classification-F1 0.4587970967934571 on epoch=49
06/02/2022 04:36:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.394841 on epoch=50
06/02/2022 04:36:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.334271 on epoch=51
06/02/2022 04:36:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.273320 on epoch=51
06/02/2022 04:36:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.408280 on epoch=52
06/02/2022 04:36:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.245220 on epoch=53
06/02/2022 04:36:35 - INFO - __main__ - Global step 850 Train loss 0.331186 Classification-F1 0.5219891688940053 on epoch=53
06/02/2022 04:36:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.268941 on epoch=53
06/02/2022 04:36:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.244120 on epoch=54
06/02/2022 04:36:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.211569 on epoch=54
06/02/2022 04:36:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.229580 on epoch=55
06/02/2022 04:37:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.368079 on epoch=56
06/02/2022 04:37:03 - INFO - __main__ - Global step 900 Train loss 0.264458 Classification-F1 0.6390687095873847 on epoch=56
06/02/2022 04:37:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.295088 on epoch=56
06/02/2022 04:37:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.242248 on epoch=57
06/02/2022 04:37:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.223147 on epoch=58
06/02/2022 04:37:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.216777 on epoch=58
06/02/2022 04:37:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.197280 on epoch=59
06/02/2022 04:37:31 - INFO - __main__ - Global step 950 Train loss 0.234908 Classification-F1 0.6297137089207367 on epoch=59
06/02/2022 04:37:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.193275 on epoch=59
06/02/2022 04:37:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.194648 on epoch=60
06/02/2022 04:37:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.112069 on epoch=61
06/02/2022 04:37:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.151116 on epoch=61
06/02/2022 04:37:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.153308 on epoch=62
06/02/2022 04:37:57 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:37:57 - INFO - __main__ - Printing 3 examples
06/02/2022 04:37:57 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/02/2022 04:37:57 - INFO - __main__ - ['happy']
06/02/2022 04:37:57 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/02/2022 04:37:57 - INFO - __main__ - ['happy']
06/02/2022 04:37:57 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/02/2022 04:37:57 - INFO - __main__ - ['happy']
06/02/2022 04:37:57 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:37:57 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:37:58 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 04:37:58 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:37:58 - INFO - __main__ - Printing 3 examples
06/02/2022 04:37:58 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
06/02/2022 04:37:58 - INFO - __main__ - ['happy']
06/02/2022 04:37:58 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
06/02/2022 04:37:58 - INFO - __main__ - ['happy']
06/02/2022 04:37:58 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
06/02/2022 04:37:58 - INFO - __main__ - ['happy']
06/02/2022 04:37:58 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:37:58 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:37:58 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 04:37:58 - INFO - __main__ - Global step 1000 Train loss 0.160883 Classification-F1 0.654483342778208 on epoch=62
06/02/2022 04:37:58 - INFO - __main__ - save last model!
06/02/2022 04:38:05 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 04:38:06 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 04:38:06 - INFO - __main__ - Printing 3 examples
06/02/2022 04:38:06 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 04:38:06 - INFO - __main__ - ['others']
06/02/2022 04:38:06 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 04:38:06 - INFO - __main__ - ['others']
06/02/2022 04:38:06 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 04:38:06 - INFO - __main__ - ['others']
06/02/2022 04:38:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:38:08 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:38:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 04:38:09 - INFO - __main__ - Starting training!
06/02/2022 04:38:13 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 04:38:57 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_42_0.0003_8_predictions.txt
06/02/2022 04:38:57 - INFO - __main__ - Classification-F1 on test data: 0.1649
06/02/2022 04:38:57 - INFO - __main__ - prefix=emo_64_42, lr=0.0003, bsz=8, dev_performance=0.765602152424402, test_performance=0.16490394137284498
06/02/2022 04:38:57 - INFO - __main__ - Running ... prefix=emo_64_42, lr=0.0002, bsz=8 ...
06/02/2022 04:38:58 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:38:58 - INFO - __main__ - Printing 3 examples
06/02/2022 04:38:58 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/02/2022 04:38:58 - INFO - __main__ - ['happy']
06/02/2022 04:38:58 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/02/2022 04:38:58 - INFO - __main__ - ['happy']
06/02/2022 04:38:58 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/02/2022 04:38:58 - INFO - __main__ - ['happy']
06/02/2022 04:38:58 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:38:58 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:38:59 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 04:38:59 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:38:59 - INFO - __main__ - Printing 3 examples
06/02/2022 04:38:59 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
06/02/2022 04:38:59 - INFO - __main__ - ['happy']
06/02/2022 04:38:59 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
06/02/2022 04:38:59 - INFO - __main__ - ['happy']
06/02/2022 04:38:59 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
06/02/2022 04:38:59 - INFO - __main__ - ['happy']
06/02/2022 04:38:59 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:38:59 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:38:59 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 04:39:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 04:39:10 - INFO - __main__ - Starting training!
06/02/2022 04:39:14 - INFO - __main__ - Step 10 Global step 10 Train loss 24.073158 on epoch=0
06/02/2022 04:39:19 - INFO - __main__ - Step 20 Global step 20 Train loss 19.667713 on epoch=1
06/02/2022 04:39:24 - INFO - __main__ - Step 30 Global step 30 Train loss 17.889515 on epoch=1
06/02/2022 04:39:29 - INFO - __main__ - Step 40 Global step 40 Train loss 17.470650 on epoch=2
06/02/2022 04:39:35 - INFO - __main__ - Step 50 Global step 50 Train loss 16.471109 on epoch=3
06/02/2022 04:39:47 - INFO - __main__ - Global step 50 Train loss 19.114429 Classification-F1 0.0007597340930674265 on epoch=3
06/02/2022 04:39:52 - INFO - __main__ - Step 60 Global step 60 Train loss 15.903603 on epoch=3
06/02/2022 04:39:58 - INFO - __main__ - Step 70 Global step 70 Train loss 14.868612 on epoch=4
06/02/2022 04:40:03 - INFO - __main__ - Step 80 Global step 80 Train loss 14.419128 on epoch=4
06/02/2022 04:40:08 - INFO - __main__ - Step 90 Global step 90 Train loss 14.921786 on epoch=5
06/02/2022 04:40:13 - INFO - __main__ - Step 100 Global step 100 Train loss 12.761716 on epoch=6
06/02/2022 04:40:16 - INFO - __main__ - Global step 100 Train loss 14.574968 Classification-F1 0.0008205128205128206 on epoch=6
06/02/2022 04:40:23 - INFO - __main__ - Step 110 Global step 110 Train loss 12.852427 on epoch=6
06/02/2022 04:40:28 - INFO - __main__ - Step 120 Global step 120 Train loss 11.669714 on epoch=7
06/02/2022 04:40:33 - INFO - __main__ - Step 130 Global step 130 Train loss 10.919393 on epoch=8
06/02/2022 04:40:38 - INFO - __main__ - Step 140 Global step 140 Train loss 10.118648 on epoch=8
06/02/2022 04:40:43 - INFO - __main__ - Step 150 Global step 150 Train loss 8.812764 on epoch=9
06/02/2022 04:42:11 - INFO - __main__ - Global step 150 Train loss 10.874589 Classification-F1 0.0007941234862021044 on epoch=9
06/02/2022 04:42:17 - INFO - __main__ - Step 160 Global step 160 Train loss 6.982830 on epoch=9
06/02/2022 04:42:22 - INFO - __main__ - Step 170 Global step 170 Train loss 2.840802 on epoch=10
06/02/2022 04:42:26 - INFO - __main__ - Step 180 Global step 180 Train loss 1.739935 on epoch=11
06/02/2022 04:42:31 - INFO - __main__ - Step 190 Global step 190 Train loss 3.201676 on epoch=11
06/02/2022 04:42:36 - INFO - __main__ - Step 200 Global step 200 Train loss 1.296443 on epoch=12
06/02/2022 04:42:38 - INFO - __main__ - Global step 200 Train loss 3.212337 Classification-F1 0.25198181901447153 on epoch=12
06/02/2022 04:42:44 - INFO - __main__ - Step 210 Global step 210 Train loss 1.028420 on epoch=13
06/02/2022 04:42:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.885047 on epoch=13
06/02/2022 04:42:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.851697 on epoch=14
06/02/2022 04:42:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.740779 on epoch=14
06/02/2022 04:43:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.809831 on epoch=15
06/02/2022 04:43:07 - INFO - __main__ - Global step 250 Train loss 0.863155 Classification-F1 0.47951950634959406 on epoch=15
06/02/2022 04:43:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.733225 on epoch=16
06/02/2022 04:43:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.722552 on epoch=16
06/02/2022 04:43:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.672819 on epoch=17
06/02/2022 04:43:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.688744 on epoch=18
06/02/2022 04:43:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.502154 on epoch=18
06/02/2022 04:43:35 - INFO - __main__ - Global step 300 Train loss 0.663899 Classification-F1 0.5007106887357476 on epoch=18
06/02/2022 04:43:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.709153 on epoch=19
06/02/2022 04:43:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.683376 on epoch=19
06/02/2022 04:43:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.638406 on epoch=20
06/02/2022 04:43:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.774828 on epoch=21
06/02/2022 04:44:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.749337 on epoch=21
06/02/2022 04:44:04 - INFO - __main__ - Global step 350 Train loss 0.711020 Classification-F1 0.2832320801576465 on epoch=21
06/02/2022 04:44:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.734315 on epoch=22
06/02/2022 04:44:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.735519 on epoch=23
06/02/2022 04:44:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.671260 on epoch=23
06/02/2022 04:44:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.804245 on epoch=24
06/02/2022 04:44:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.729688 on epoch=24
06/02/2022 04:44:32 - INFO - __main__ - Global step 400 Train loss 0.735005 Classification-F1 0.3019937808670203 on epoch=24
06/02/2022 04:44:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.662721 on epoch=25
06/02/2022 04:44:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.751819 on epoch=26
06/02/2022 04:44:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.591480 on epoch=26
06/02/2022 04:44:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.613358 on epoch=27
06/02/2022 04:44:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.648109 on epoch=28
06/02/2022 04:45:00 - INFO - __main__ - Global step 450 Train loss 0.653497 Classification-F1 0.39020735067964374 on epoch=28
06/02/2022 04:45:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.603346 on epoch=28
06/02/2022 04:45:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.647548 on epoch=29
06/02/2022 04:45:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.553962 on epoch=29
06/02/2022 04:45:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.486663 on epoch=30
06/02/2022 04:45:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.459171 on epoch=31
06/02/2022 04:45:27 - INFO - __main__ - Global step 500 Train loss 0.550138 Classification-F1 0.7044100824853164 on epoch=31
06/02/2022 04:45:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.452378 on epoch=31
06/02/2022 04:45:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.535987 on epoch=32
06/02/2022 04:45:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.444844 on epoch=33
06/02/2022 04:45:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.470881 on epoch=33
06/02/2022 04:45:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.363422 on epoch=34
06/02/2022 04:45:56 - INFO - __main__ - Global step 550 Train loss 0.453503 Classification-F1 0.72643358533013 on epoch=34
06/02/2022 04:46:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.371191 on epoch=34
06/02/2022 04:46:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.337344 on epoch=35
06/02/2022 04:46:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.314060 on epoch=36
06/02/2022 04:46:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.263476 on epoch=36
06/02/2022 04:46:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.256555 on epoch=37
06/02/2022 04:46:24 - INFO - __main__ - Global step 600 Train loss 0.308525 Classification-F1 0.589067811002904 on epoch=37
06/02/2022 04:46:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.270156 on epoch=38
06/02/2022 04:46:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.216695 on epoch=38
06/02/2022 04:46:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.230528 on epoch=39
06/02/2022 04:46:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.134154 on epoch=39
06/02/2022 04:46:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.169895 on epoch=40
06/02/2022 04:46:52 - INFO - __main__ - Global step 650 Train loss 0.204286 Classification-F1 0.753222247803231 on epoch=40
06/02/2022 04:46:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.186262 on epoch=41
06/02/2022 04:47:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.165805 on epoch=41
06/02/2022 04:47:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.155127 on epoch=42
06/02/2022 04:47:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.133550 on epoch=43
06/02/2022 04:47:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.144482 on epoch=43
06/02/2022 04:47:21 - INFO - __main__ - Global step 700 Train loss 0.157045 Classification-F1 0.5519285539146344 on epoch=43
06/02/2022 04:47:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.124271 on epoch=44
06/02/2022 04:47:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.139775 on epoch=44
06/02/2022 04:47:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.095423 on epoch=45
06/02/2022 04:47:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.093958 on epoch=46
06/02/2022 04:47:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.154798 on epoch=46
06/02/2022 04:47:49 - INFO - __main__ - Global step 750 Train loss 0.121645 Classification-F1 0.5788544615969505 on epoch=46
06/02/2022 04:47:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.069279 on epoch=47
06/02/2022 04:47:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.103469 on epoch=48
06/02/2022 04:48:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.076200 on epoch=48
06/02/2022 04:48:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.062618 on epoch=49
06/02/2022 04:48:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.130299 on epoch=49
06/02/2022 04:48:16 - INFO - __main__ - Global step 800 Train loss 0.088373 Classification-F1 0.5917930014430015 on epoch=49
06/02/2022 04:48:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.192089 on epoch=50
06/02/2022 04:48:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.148247 on epoch=51
06/02/2022 04:48:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.152511 on epoch=51
06/02/2022 04:48:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.177575 on epoch=52
06/02/2022 04:48:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.077740 on epoch=53
06/02/2022 04:48:44 - INFO - __main__ - Global step 850 Train loss 0.149633 Classification-F1 0.5731344411827699 on epoch=53
06/02/2022 04:48:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.124561 on epoch=53
06/02/2022 04:48:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.164167 on epoch=54
06/02/2022 04:49:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.175550 on epoch=54
06/02/2022 04:49:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.322716 on epoch=55
06/02/2022 04:49:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.320677 on epoch=56
06/02/2022 04:49:12 - INFO - __main__ - Global step 900 Train loss 0.221534 Classification-F1 0.6641796506643008 on epoch=56
06/02/2022 04:49:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.349110 on epoch=56
06/02/2022 04:49:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.332759 on epoch=57
06/02/2022 04:49:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.158123 on epoch=58
06/02/2022 04:49:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.191197 on epoch=58
06/02/2022 04:49:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.144090 on epoch=59
06/02/2022 04:49:40 - INFO - __main__ - Global step 950 Train loss 0.235056 Classification-F1 0.4337089166375212 on epoch=59
06/02/2022 04:49:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.055747 on epoch=59
06/02/2022 04:49:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.104630 on epoch=60
06/02/2022 04:49:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.033917 on epoch=61
06/02/2022 04:50:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.058095 on epoch=61
06/02/2022 04:50:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.062020 on epoch=62
06/02/2022 04:50:07 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:50:07 - INFO - __main__ - Printing 3 examples
06/02/2022 04:50:07 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/02/2022 04:50:07 - INFO - __main__ - ['happy']
06/02/2022 04:50:07 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/02/2022 04:50:07 - INFO - __main__ - ['happy']
06/02/2022 04:50:07 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/02/2022 04:50:07 - INFO - __main__ - ['happy']
06/02/2022 04:50:07 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:50:07 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:50:07 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 04:50:07 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:50:07 - INFO - __main__ - Printing 3 examples
06/02/2022 04:50:07 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
06/02/2022 04:50:07 - INFO - __main__ - ['happy']
06/02/2022 04:50:07 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
06/02/2022 04:50:07 - INFO - __main__ - ['happy']
06/02/2022 04:50:07 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
06/02/2022 04:50:07 - INFO - __main__ - ['happy']
06/02/2022 04:50:07 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:50:07 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:50:07 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 04:50:08 - INFO - __main__ - Global step 1000 Train loss 0.062882 Classification-F1 0.6112583189175715 on epoch=62
06/02/2022 04:50:08 - INFO - __main__ - save last model!
06/02/2022 04:50:15 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 04:50:15 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 04:50:15 - INFO - __main__ - Printing 3 examples
06/02/2022 04:50:15 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 04:50:15 - INFO - __main__ - ['others']
06/02/2022 04:50:15 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 04:50:15 - INFO - __main__ - ['others']
06/02/2022 04:50:15 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 04:50:15 - INFO - __main__ - ['others']
06/02/2022 04:50:15 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:50:18 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:50:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 04:50:20 - INFO - __main__ - Starting training!
06/02/2022 04:50:23 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 04:51:04 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_42_0.0002_8_predictions.txt
06/02/2022 04:51:05 - INFO - __main__ - Classification-F1 on test data: 0.2007
06/02/2022 04:51:05 - INFO - __main__ - prefix=emo_64_42, lr=0.0002, bsz=8, dev_performance=0.753222247803231, test_performance=0.20069213667723237
06/02/2022 04:51:05 - INFO - __main__ - Running ... prefix=emo_64_42, lr=0.0001, bsz=8 ...
06/02/2022 04:51:06 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:51:06 - INFO - __main__ - Printing 3 examples
06/02/2022 04:51:06 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/02/2022 04:51:06 - INFO - __main__ - ['happy']
06/02/2022 04:51:06 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/02/2022 04:51:06 - INFO - __main__ - ['happy']
06/02/2022 04:51:06 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/02/2022 04:51:06 - INFO - __main__ - ['happy']
06/02/2022 04:51:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:51:06 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:51:06 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 04:51:06 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 04:51:06 - INFO - __main__ - Printing 3 examples
06/02/2022 04:51:06 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
06/02/2022 04:51:06 - INFO - __main__ - ['happy']
06/02/2022 04:51:06 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
06/02/2022 04:51:06 - INFO - __main__ - ['happy']
06/02/2022 04:51:06 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
06/02/2022 04:51:06 - INFO - __main__ - ['happy']
06/02/2022 04:51:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 04:51:06 - INFO - __main__ - Tokenizing Output ...
06/02/2022 04:51:07 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 04:51:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 04:51:19 - INFO - __main__ - Starting training!
06/02/2022 04:51:24 - INFO - __main__ - Step 10 Global step 10 Train loss 24.104549 on epoch=0
06/02/2022 04:51:28 - INFO - __main__ - Step 20 Global step 20 Train loss 21.038366 on epoch=1
06/02/2022 04:51:33 - INFO - __main__ - Step 30 Global step 30 Train loss 18.830912 on epoch=1
06/02/2022 04:51:38 - INFO - __main__ - Step 40 Global step 40 Train loss 18.280758 on epoch=2
06/02/2022 04:51:43 - INFO - __main__ - Step 50 Global step 50 Train loss 17.116821 on epoch=3
06/02/2022 04:52:57 - INFO - __main__ - Global step 50 Train loss 19.874283 Classification-F1 0.00015942606616181745 on epoch=3
06/02/2022 04:53:02 - INFO - __main__ - Step 60 Global step 60 Train loss 17.848362 on epoch=3
06/02/2022 04:53:07 - INFO - __main__ - Step 70 Global step 70 Train loss 16.361324 on epoch=4
06/02/2022 04:53:12 - INFO - __main__ - Step 80 Global step 80 Train loss 16.360003 on epoch=4
06/02/2022 04:53:17 - INFO - __main__ - Step 90 Global step 90 Train loss 16.924551 on epoch=5
06/02/2022 04:53:23 - INFO - __main__ - Step 100 Global step 100 Train loss 15.919731 on epoch=6
06/02/2022 04:53:56 - INFO - __main__ - Global step 100 Train loss 16.682795 Classification-F1 0.000338123415046492 on epoch=6
06/02/2022 04:54:02 - INFO - __main__ - Step 110 Global step 110 Train loss 15.398932 on epoch=6
06/02/2022 04:54:07 - INFO - __main__ - Step 120 Global step 120 Train loss 15.266663 on epoch=7
06/02/2022 04:54:12 - INFO - __main__ - Step 130 Global step 130 Train loss 15.219856 on epoch=8
06/02/2022 04:54:17 - INFO - __main__ - Step 140 Global step 140 Train loss 14.408432 on epoch=8
06/02/2022 04:54:22 - INFO - __main__ - Step 150 Global step 150 Train loss 14.558080 on epoch=9
06/02/2022 04:54:28 - INFO - __main__ - Global step 150 Train loss 14.970392 Classification-F1 0.0004102564102564103 on epoch=9
06/02/2022 04:54:34 - INFO - __main__ - Step 160 Global step 160 Train loss 14.203893 on epoch=9
06/02/2022 04:54:39 - INFO - __main__ - Step 170 Global step 170 Train loss 13.813924 on epoch=10
06/02/2022 04:54:44 - INFO - __main__ - Step 180 Global step 180 Train loss 14.087015 on epoch=11
06/02/2022 04:54:49 - INFO - __main__ - Step 190 Global step 190 Train loss 13.000168 on epoch=11
06/02/2022 04:54:54 - INFO - __main__ - Step 200 Global step 200 Train loss 13.077161 on epoch=12
06/02/2022 04:54:58 - INFO - __main__ - Global step 200 Train loss 13.636432 Classification-F1 0.0011188811188811189 on epoch=12
06/02/2022 04:55:04 - INFO - __main__ - Step 210 Global step 210 Train loss 12.799765 on epoch=13
06/02/2022 04:55:09 - INFO - __main__ - Step 220 Global step 220 Train loss 12.112887 on epoch=13
06/02/2022 04:55:14 - INFO - __main__ - Step 230 Global step 230 Train loss 11.856100 on epoch=14
06/02/2022 04:55:19 - INFO - __main__ - Step 240 Global step 240 Train loss 12.013207 on epoch=14
06/02/2022 04:55:24 - INFO - __main__ - Step 250 Global step 250 Train loss 11.579348 on epoch=15
06/02/2022 04:55:32 - INFO - __main__ - Global step 250 Train loss 12.072262 Classification-F1 0.0012558869701726845 on epoch=15
06/02/2022 04:55:38 - INFO - __main__ - Step 260 Global step 260 Train loss 10.761912 on epoch=16
06/02/2022 04:55:43 - INFO - __main__ - Step 270 Global step 270 Train loss 10.686905 on epoch=16
06/02/2022 04:55:48 - INFO - __main__ - Step 280 Global step 280 Train loss 9.979585 on epoch=17
06/02/2022 04:55:53 - INFO - __main__ - Step 290 Global step 290 Train loss 9.708078 on epoch=18
06/02/2022 04:55:58 - INFO - __main__ - Step 300 Global step 300 Train loss 9.324096 on epoch=18
06/02/2022 04:56:04 - INFO - __main__ - Global step 300 Train loss 10.092114 Classification-F1 0.0014311270125223615 on epoch=18
06/02/2022 04:56:09 - INFO - __main__ - Step 310 Global step 310 Train loss 8.186012 on epoch=19
06/02/2022 04:56:14 - INFO - __main__ - Step 320 Global step 320 Train loss 6.565161 on epoch=19
06/02/2022 04:56:19 - INFO - __main__ - Step 330 Global step 330 Train loss 5.287053 on epoch=20
06/02/2022 04:56:24 - INFO - __main__ - Step 340 Global step 340 Train loss 3.179293 on epoch=21
06/02/2022 04:56:29 - INFO - __main__ - Step 350 Global step 350 Train loss 2.007067 on epoch=21
06/02/2022 04:56:31 - INFO - __main__ - Global step 350 Train loss 5.044918 Classification-F1 0.28903326042914146 on epoch=21
06/02/2022 04:56:37 - INFO - __main__ - Step 360 Global step 360 Train loss 1.393330 on epoch=22
06/02/2022 04:56:42 - INFO - __main__ - Step 370 Global step 370 Train loss 1.062050 on epoch=23
06/02/2022 04:56:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.726937 on epoch=23
06/02/2022 04:56:52 - INFO - __main__ - Step 390 Global step 390 Train loss 1.075525 on epoch=24
06/02/2022 04:56:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.783798 on epoch=24
06/02/2022 04:56:59 - INFO - __main__ - Global step 400 Train loss 1.008328 Classification-F1 0.6089909441133716 on epoch=24
06/02/2022 04:57:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.881523 on epoch=25
06/02/2022 04:57:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.641164 on epoch=26
06/02/2022 04:57:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.602878 on epoch=26
06/02/2022 04:57:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.787400 on epoch=27
06/02/2022 04:57:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.574755 on epoch=28
06/02/2022 04:57:27 - INFO - __main__ - Global step 450 Train loss 0.697544 Classification-F1 0.6424119739909214 on epoch=28
06/02/2022 04:57:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.615681 on epoch=28
06/02/2022 04:57:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.656802 on epoch=29
06/02/2022 04:57:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.509189 on epoch=29
06/02/2022 04:57:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.581920 on epoch=30
06/02/2022 04:57:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.613327 on epoch=31
06/02/2022 04:57:54 - INFO - __main__ - Global step 500 Train loss 0.595384 Classification-F1 0.7108831110313361 on epoch=31
06/02/2022 04:58:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.495814 on epoch=31
06/02/2022 04:58:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.493433 on epoch=32
06/02/2022 04:58:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.541751 on epoch=33
06/02/2022 04:58:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.658844 on epoch=33
06/02/2022 04:58:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.482217 on epoch=34
06/02/2022 04:58:22 - INFO - __main__ - Global step 550 Train loss 0.534412 Classification-F1 0.6872553438057385 on epoch=34
06/02/2022 04:58:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.401057 on epoch=34
06/02/2022 04:58:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.488364 on epoch=35
06/02/2022 04:58:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.516321 on epoch=36
06/02/2022 04:58:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.580968 on epoch=36
06/02/2022 04:58:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.588593 on epoch=37
06/02/2022 04:58:49 - INFO - __main__ - Global step 600 Train loss 0.515061 Classification-F1 0.7293814645913883 on epoch=37
06/02/2022 04:58:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.496356 on epoch=38
06/02/2022 04:58:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.477240 on epoch=38
06/02/2022 04:59:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.385516 on epoch=39
06/02/2022 04:59:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.342465 on epoch=39
06/02/2022 04:59:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.465611 on epoch=40
06/02/2022 04:59:16 - INFO - __main__ - Global step 650 Train loss 0.433438 Classification-F1 0.7584682796249467 on epoch=40
06/02/2022 04:59:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.439033 on epoch=41
06/02/2022 04:59:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.388002 on epoch=41
06/02/2022 04:59:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.404005 on epoch=42
06/02/2022 04:59:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.345308 on epoch=43
06/02/2022 04:59:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.284194 on epoch=43
06/02/2022 04:59:44 - INFO - __main__ - Global step 700 Train loss 0.372108 Classification-F1 0.7147286309520351 on epoch=43
06/02/2022 04:59:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.346273 on epoch=44
06/02/2022 04:59:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.253144 on epoch=44
06/02/2022 04:59:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.248367 on epoch=45
06/02/2022 05:00:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.351086 on epoch=46
06/02/2022 05:00:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.238893 on epoch=46
06/02/2022 05:00:11 - INFO - __main__ - Global step 750 Train loss 0.287553 Classification-F1 0.7448110653602599 on epoch=46
06/02/2022 05:00:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.236631 on epoch=47
06/02/2022 05:00:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.226182 on epoch=48
06/02/2022 05:00:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.282706 on epoch=48
06/02/2022 05:00:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.181583 on epoch=49
06/02/2022 05:00:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.143858 on epoch=49
06/02/2022 05:00:38 - INFO - __main__ - Global step 800 Train loss 0.214192 Classification-F1 0.7466894135942884 on epoch=49
06/02/2022 05:00:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.308575 on epoch=50
06/02/2022 05:00:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.168888 on epoch=51
06/02/2022 05:00:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.154834 on epoch=51
06/02/2022 05:00:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.117384 on epoch=52
06/02/2022 05:01:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.204513 on epoch=53
06/02/2022 05:01:05 - INFO - __main__ - Global step 850 Train loss 0.190839 Classification-F1 0.7496650368845428 on epoch=53
06/02/2022 05:01:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.137230 on epoch=53
06/02/2022 05:01:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.183416 on epoch=54
06/02/2022 05:01:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.103334 on epoch=54
06/02/2022 05:01:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.206784 on epoch=55
06/02/2022 05:01:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.091078 on epoch=56
06/02/2022 05:01:32 - INFO - __main__ - Global step 900 Train loss 0.144368 Classification-F1 0.7308489151580486 on epoch=56
06/02/2022 05:01:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.116454 on epoch=56
06/02/2022 05:01:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.129448 on epoch=57
06/02/2022 05:01:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.051889 on epoch=58
06/02/2022 05:01:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.128392 on epoch=58
06/02/2022 05:01:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.140117 on epoch=59
06/02/2022 05:01:59 - INFO - __main__ - Global step 950 Train loss 0.113260 Classification-F1 0.7435374822075356 on epoch=59
06/02/2022 05:02:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.088010 on epoch=59
06/02/2022 05:02:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.095802 on epoch=60
06/02/2022 05:02:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.074615 on epoch=61
06/02/2022 05:02:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.084924 on epoch=61
06/02/2022 05:02:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.134351 on epoch=62
06/02/2022 05:02:26 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 05:02:26 - INFO - __main__ - Printing 3 examples
06/02/2022 05:02:26 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/02/2022 05:02:26 - INFO - __main__ - ['others']
06/02/2022 05:02:26 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/02/2022 05:02:26 - INFO - __main__ - ['others']
06/02/2022 05:02:26 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/02/2022 05:02:26 - INFO - __main__ - ['others']
06/02/2022 05:02:26 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:02:26 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:02:26 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 05:02:26 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 05:02:26 - INFO - __main__ - Printing 3 examples
06/02/2022 05:02:26 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
06/02/2022 05:02:26 - INFO - __main__ - ['others']
06/02/2022 05:02:26 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
06/02/2022 05:02:26 - INFO - __main__ - ['others']
06/02/2022 05:02:26 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
06/02/2022 05:02:26 - INFO - __main__ - ['others']
06/02/2022 05:02:26 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:02:26 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:02:27 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 05:02:27 - INFO - __main__ - Global step 1000 Train loss 0.095540 Classification-F1 0.7909393716922879 on epoch=62
06/02/2022 05:02:27 - INFO - __main__ - save last model!
06/02/2022 05:02:34 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 05:02:35 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 05:02:35 - INFO - __main__ - Printing 3 examples
06/02/2022 05:02:35 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 05:02:35 - INFO - __main__ - ['others']
06/02/2022 05:02:35 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 05:02:35 - INFO - __main__ - ['others']
06/02/2022 05:02:35 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 05:02:35 - INFO - __main__ - ['others']
06/02/2022 05:02:35 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:02:37 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:02:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 05:02:39 - INFO - __main__ - Starting training!
06/02/2022 05:02:43 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 05:03:25 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_42_0.0001_8_predictions.txt
06/02/2022 05:03:25 - INFO - __main__ - Classification-F1 on test data: 0.3534
06/02/2022 05:03:25 - INFO - __main__ - prefix=emo_64_42, lr=0.0001, bsz=8, dev_performance=0.7909393716922879, test_performance=0.353399530429823
06/02/2022 05:03:25 - INFO - __main__ - Running ... prefix=emo_64_87, lr=0.0005, bsz=8 ...
06/02/2022 05:03:26 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 05:03:26 - INFO - __main__ - Printing 3 examples
06/02/2022 05:03:26 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/02/2022 05:03:26 - INFO - __main__ - ['others']
06/02/2022 05:03:26 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/02/2022 05:03:26 - INFO - __main__ - ['others']
06/02/2022 05:03:26 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/02/2022 05:03:26 - INFO - __main__ - ['others']
06/02/2022 05:03:26 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:03:26 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:03:27 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 05:03:27 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 05:03:27 - INFO - __main__ - Printing 3 examples
06/02/2022 05:03:27 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
06/02/2022 05:03:27 - INFO - __main__ - ['others']
06/02/2022 05:03:27 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
06/02/2022 05:03:27 - INFO - __main__ - ['others']
06/02/2022 05:03:27 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
06/02/2022 05:03:27 - INFO - __main__ - ['others']
06/02/2022 05:03:27 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:03:27 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:03:27 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 05:03:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 05:03:40 - INFO - __main__ - Starting training!
06/02/2022 05:03:44 - INFO - __main__ - Step 10 Global step 10 Train loss 25.854767 on epoch=0
06/02/2022 05:03:49 - INFO - __main__ - Step 20 Global step 20 Train loss 20.057598 on epoch=1
06/02/2022 05:03:54 - INFO - __main__ - Step 30 Global step 30 Train loss 17.886543 on epoch=1
06/02/2022 05:03:59 - INFO - __main__ - Step 40 Global step 40 Train loss 15.464002 on epoch=2
06/02/2022 05:04:04 - INFO - __main__ - Step 50 Global step 50 Train loss 13.007650 on epoch=3
06/02/2022 05:04:06 - INFO - __main__ - Global step 50 Train loss 18.454111 Classification-F1 0.0 on epoch=3
06/02/2022 05:04:12 - INFO - __main__ - Step 60 Global step 60 Train loss 11.737260 on epoch=3
06/02/2022 05:04:17 - INFO - __main__ - Step 70 Global step 70 Train loss 7.768844 on epoch=4
06/02/2022 05:04:21 - INFO - __main__ - Step 80 Global step 80 Train loss 5.380698 on epoch=4
06/02/2022 05:04:26 - INFO - __main__ - Step 90 Global step 90 Train loss 3.022863 on epoch=5
06/02/2022 05:04:31 - INFO - __main__ - Step 100 Global step 100 Train loss 1.977296 on epoch=6
06/02/2022 05:04:33 - INFO - __main__ - Global step 100 Train loss 5.977393 Classification-F1 0.19481490963545042 on epoch=6
06/02/2022 05:04:39 - INFO - __main__ - Step 110 Global step 110 Train loss 1.382518 on epoch=6
06/02/2022 05:04:44 - INFO - __main__ - Step 120 Global step 120 Train loss 1.204175 on epoch=7
06/02/2022 05:04:49 - INFO - __main__ - Step 130 Global step 130 Train loss 1.169352 on epoch=8
06/02/2022 05:04:54 - INFO - __main__ - Step 140 Global step 140 Train loss 1.203308 on epoch=8
06/02/2022 05:04:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.886153 on epoch=9
06/02/2022 05:05:01 - INFO - __main__ - Global step 150 Train loss 1.169101 Classification-F1 0.183204775937831 on epoch=9
06/02/2022 05:05:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.866921 on epoch=9
06/02/2022 05:05:11 - INFO - __main__ - Step 170 Global step 170 Train loss 0.800515 on epoch=10
06/02/2022 05:05:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.879832 on epoch=11
06/02/2022 05:05:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.805698 on epoch=11
06/02/2022 05:05:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.839459 on epoch=12
06/02/2022 05:05:28 - INFO - __main__ - Global step 200 Train loss 0.838485 Classification-F1 0.30497157335310837 on epoch=12
06/02/2022 05:05:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.786746 on epoch=13
06/02/2022 05:05:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.714145 on epoch=13
06/02/2022 05:05:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.759145 on epoch=14
06/02/2022 05:05:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.715097 on epoch=14
06/02/2022 05:05:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.797326 on epoch=15
06/02/2022 05:05:56 - INFO - __main__ - Global step 250 Train loss 0.754492 Classification-F1 0.3879922970936093 on epoch=15
06/02/2022 05:06:01 - INFO - __main__ - Step 260 Global step 260 Train loss 1.226921 on epoch=16
06/02/2022 05:06:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.684671 on epoch=16
06/02/2022 05:06:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.795096 on epoch=17
06/02/2022 05:06:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.835815 on epoch=18
06/02/2022 05:06:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.630001 on epoch=18
06/02/2022 05:06:23 - INFO - __main__ - Global step 300 Train loss 0.834501 Classification-F1 0.36838597688747965 on epoch=18
06/02/2022 05:06:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.673089 on epoch=19
06/02/2022 05:06:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.644381 on epoch=19
06/02/2022 05:06:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.582729 on epoch=20
06/02/2022 05:06:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.529320 on epoch=21
06/02/2022 05:06:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.484976 on epoch=21
06/02/2022 05:06:50 - INFO - __main__ - Global step 350 Train loss 0.582899 Classification-F1 0.5593719652503678 on epoch=21
06/02/2022 05:06:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.492234 on epoch=22
06/02/2022 05:07:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.472704 on epoch=23
06/02/2022 05:07:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.375324 on epoch=23
06/02/2022 05:07:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.416405 on epoch=24
06/02/2022 05:07:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.458059 on epoch=24
06/02/2022 05:07:17 - INFO - __main__ - Global step 400 Train loss 0.442945 Classification-F1 0.4867788694800895 on epoch=24
06/02/2022 05:07:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.432430 on epoch=25
06/02/2022 05:07:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.459775 on epoch=26
06/02/2022 05:07:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.399709 on epoch=26
06/02/2022 05:07:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.468576 on epoch=27
06/02/2022 05:07:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.354388 on epoch=28
06/02/2022 05:07:44 - INFO - __main__ - Global step 450 Train loss 0.422976 Classification-F1 0.6252359139370024 on epoch=28
06/02/2022 05:07:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.333793 on epoch=28
06/02/2022 05:07:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.369300 on epoch=29
06/02/2022 05:08:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.411435 on epoch=29
06/02/2022 05:08:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.423427 on epoch=30
06/02/2022 05:08:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.365950 on epoch=31
06/02/2022 05:08:12 - INFO - __main__ - Global step 500 Train loss 0.380781 Classification-F1 0.5981127943342867 on epoch=31
06/02/2022 05:08:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.288982 on epoch=31
06/02/2022 05:08:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.281203 on epoch=32
06/02/2022 05:08:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.328048 on epoch=33
06/02/2022 05:08:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.414847 on epoch=33
06/02/2022 05:08:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.373607 on epoch=34
06/02/2022 05:08:39 - INFO - __main__ - Global step 550 Train loss 0.337337 Classification-F1 0.5918229066255383 on epoch=34
06/02/2022 05:08:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.532799 on epoch=34
06/02/2022 05:08:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.427310 on epoch=35
06/02/2022 05:08:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.360029 on epoch=36
06/02/2022 05:08:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.437162 on epoch=36
06/02/2022 05:09:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.301547 on epoch=37
06/02/2022 05:09:06 - INFO - __main__ - Global step 600 Train loss 0.411769 Classification-F1 0.6134386416644482 on epoch=37
06/02/2022 05:09:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.261450 on epoch=38
06/02/2022 05:09:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.210578 on epoch=38
06/02/2022 05:09:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.247768 on epoch=39
06/02/2022 05:09:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.186159 on epoch=39
06/02/2022 05:09:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.230945 on epoch=40
06/02/2022 05:09:33 - INFO - __main__ - Global step 650 Train loss 0.227380 Classification-F1 0.68913572701808 on epoch=40
06/02/2022 05:09:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.176167 on epoch=41
06/02/2022 05:09:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.119616 on epoch=41
06/02/2022 05:09:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.179665 on epoch=42
06/02/2022 05:09:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.123558 on epoch=43
06/02/2022 05:09:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.109182 on epoch=43
06/02/2022 05:10:00 - INFO - __main__ - Global step 700 Train loss 0.141638 Classification-F1 0.72569587257923 on epoch=43
06/02/2022 05:10:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.092038 on epoch=44
06/02/2022 05:10:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.104625 on epoch=44
06/02/2022 05:10:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.085855 on epoch=45
06/02/2022 05:10:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.102806 on epoch=46
06/02/2022 05:10:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.078760 on epoch=46
06/02/2022 05:10:28 - INFO - __main__ - Global step 750 Train loss 0.092817 Classification-F1 0.69672207285544 on epoch=46
06/02/2022 05:10:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.088953 on epoch=47
06/02/2022 05:10:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.164911 on epoch=48
06/02/2022 05:10:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.144954 on epoch=48
06/02/2022 05:10:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.132306 on epoch=49
06/02/2022 05:10:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.064345 on epoch=49
06/02/2022 05:10:55 - INFO - __main__ - Global step 800 Train loss 0.119094 Classification-F1 0.7075845372370122 on epoch=49
06/02/2022 05:11:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.050777 on epoch=50
06/02/2022 05:11:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.087059 on epoch=51
06/02/2022 05:11:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.043903 on epoch=51
06/02/2022 05:11:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.099815 on epoch=52
06/02/2022 05:11:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.082105 on epoch=53
06/02/2022 05:11:22 - INFO - __main__ - Global step 850 Train loss 0.072732 Classification-F1 0.7224747082916619 on epoch=53
06/02/2022 05:11:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.006711 on epoch=53
06/02/2022 05:11:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.102267 on epoch=54
06/02/2022 05:11:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.033967 on epoch=54
06/02/2022 05:11:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.067118 on epoch=55
06/02/2022 05:11:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.043142 on epoch=56
06/02/2022 05:11:49 - INFO - __main__ - Global step 900 Train loss 0.050641 Classification-F1 0.6515363721246075 on epoch=56
06/02/2022 05:11:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.021152 on epoch=56
06/02/2022 05:11:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.015472 on epoch=57
06/02/2022 05:12:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.057754 on epoch=58
06/02/2022 05:12:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.013674 on epoch=58
06/02/2022 05:12:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.018600 on epoch=59
06/02/2022 05:12:16 - INFO - __main__ - Global step 950 Train loss 0.025330 Classification-F1 0.6917813118840946 on epoch=59
06/02/2022 05:12:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.038603 on epoch=59
06/02/2022 05:12:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.032856 on epoch=60
06/02/2022 05:12:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.030556 on epoch=61
06/02/2022 05:12:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.049747 on epoch=61
06/02/2022 05:12:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.022794 on epoch=62
06/02/2022 05:12:42 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 05:12:42 - INFO - __main__ - Printing 3 examples
06/02/2022 05:12:42 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/02/2022 05:12:42 - INFO - __main__ - ['others']
06/02/2022 05:12:42 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/02/2022 05:12:42 - INFO - __main__ - ['others']
06/02/2022 05:12:42 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/02/2022 05:12:42 - INFO - __main__ - ['others']
06/02/2022 05:12:42 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:12:42 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:12:42 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 05:12:42 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 05:12:42 - INFO - __main__ - Printing 3 examples
06/02/2022 05:12:42 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
06/02/2022 05:12:42 - INFO - __main__ - ['others']
06/02/2022 05:12:42 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
06/02/2022 05:12:42 - INFO - __main__ - ['others']
06/02/2022 05:12:42 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
06/02/2022 05:12:42 - INFO - __main__ - ['others']
06/02/2022 05:12:42 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:12:42 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:12:42 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 05:12:43 - INFO - __main__ - Global step 1000 Train loss 0.034911 Classification-F1 0.7210990806579043 on epoch=62
06/02/2022 05:12:43 - INFO - __main__ - save last model!
06/02/2022 05:12:49 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 05:12:50 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 05:12:50 - INFO - __main__ - Printing 3 examples
06/02/2022 05:12:50 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 05:12:50 - INFO - __main__ - ['others']
06/02/2022 05:12:50 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 05:12:50 - INFO - __main__ - ['others']
06/02/2022 05:12:50 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 05:12:50 - INFO - __main__ - ['others']
06/02/2022 05:12:50 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:12:52 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:12:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 05:12:54 - INFO - __main__ - Starting training!
06/02/2022 05:12:57 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 05:13:44 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_87_0.0005_8_predictions.txt
06/02/2022 05:13:44 - INFO - __main__ - Classification-F1 on test data: 0.2317
06/02/2022 05:13:44 - INFO - __main__ - prefix=emo_64_87, lr=0.0005, bsz=8, dev_performance=0.72569587257923, test_performance=0.23165703048635103
06/02/2022 05:13:44 - INFO - __main__ - Running ... prefix=emo_64_87, lr=0.0003, bsz=8 ...
06/02/2022 05:13:45 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 05:13:45 - INFO - __main__ - Printing 3 examples
06/02/2022 05:13:45 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/02/2022 05:13:45 - INFO - __main__ - ['others']
06/02/2022 05:13:45 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/02/2022 05:13:45 - INFO - __main__ - ['others']
06/02/2022 05:13:45 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/02/2022 05:13:45 - INFO - __main__ - ['others']
06/02/2022 05:13:45 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:13:45 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:13:45 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 05:13:45 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 05:13:45 - INFO - __main__ - Printing 3 examples
06/02/2022 05:13:45 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
06/02/2022 05:13:45 - INFO - __main__ - ['others']
06/02/2022 05:13:45 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
06/02/2022 05:13:45 - INFO - __main__ - ['others']
06/02/2022 05:13:45 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
06/02/2022 05:13:45 - INFO - __main__ - ['others']
06/02/2022 05:13:45 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:13:45 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:13:46 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 05:13:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 05:13:57 - INFO - __main__ - Starting training!
06/02/2022 05:14:01 - INFO - __main__ - Step 10 Global step 10 Train loss 25.161236 on epoch=0
06/02/2022 05:14:06 - INFO - __main__ - Step 20 Global step 20 Train loss 20.448444 on epoch=1
06/02/2022 05:14:11 - INFO - __main__ - Step 30 Global step 30 Train loss 18.296026 on epoch=1
06/02/2022 05:14:16 - INFO - __main__ - Step 40 Global step 40 Train loss 17.196140 on epoch=2
06/02/2022 05:14:21 - INFO - __main__ - Step 50 Global step 50 Train loss 16.210682 on epoch=3
06/02/2022 05:14:24 - INFO - __main__ - Global step 50 Train loss 19.462505 Classification-F1 0.0 on epoch=3
06/02/2022 05:14:30 - INFO - __main__ - Step 60 Global step 60 Train loss 15.013713 on epoch=3
06/02/2022 05:14:35 - INFO - __main__ - Step 70 Global step 70 Train loss 13.527557 on epoch=4
06/02/2022 05:14:40 - INFO - __main__ - Step 80 Global step 80 Train loss 13.888741 on epoch=4
06/02/2022 05:14:45 - INFO - __main__ - Step 90 Global step 90 Train loss 13.480095 on epoch=5
06/02/2022 05:14:50 - INFO - __main__ - Step 100 Global step 100 Train loss 11.363686 on epoch=6
06/02/2022 05:14:53 - INFO - __main__ - Global step 100 Train loss 13.454757 Classification-F1 0.0 on epoch=6
06/02/2022 05:14:58 - INFO - __main__ - Step 110 Global step 110 Train loss 11.447153 on epoch=6
06/02/2022 05:15:03 - INFO - __main__ - Step 120 Global step 120 Train loss 10.215784 on epoch=7
06/02/2022 05:15:09 - INFO - __main__ - Step 130 Global step 130 Train loss 8.343319 on epoch=8
06/02/2022 05:15:14 - INFO - __main__ - Step 140 Global step 140 Train loss 6.520371 on epoch=8
06/02/2022 05:15:19 - INFO - __main__ - Step 150 Global step 150 Train loss 4.097375 on epoch=9
06/02/2022 05:15:22 - INFO - __main__ - Global step 150 Train loss 8.124801 Classification-F1 0.02997364953886693 on epoch=9
06/02/2022 05:15:28 - INFO - __main__ - Step 160 Global step 160 Train loss 4.192899 on epoch=9
06/02/2022 05:15:33 - INFO - __main__ - Step 170 Global step 170 Train loss 3.959676 on epoch=10
06/02/2022 05:15:38 - INFO - __main__ - Step 180 Global step 180 Train loss 4.260541 on epoch=11
06/02/2022 05:15:43 - INFO - __main__ - Step 190 Global step 190 Train loss 3.458726 on epoch=11
06/02/2022 05:15:49 - INFO - __main__ - Step 200 Global step 200 Train loss 3.450645 on epoch=12
06/02/2022 05:15:51 - INFO - __main__ - Global step 200 Train loss 3.864497 Classification-F1 0.1 on epoch=12
06/02/2022 05:15:57 - INFO - __main__ - Step 210 Global step 210 Train loss 3.398478 on epoch=13
06/02/2022 05:16:02 - INFO - __main__ - Step 220 Global step 220 Train loss 3.532802 on epoch=13
06/02/2022 05:16:07 - INFO - __main__ - Step 230 Global step 230 Train loss 3.520091 on epoch=14
06/02/2022 05:16:12 - INFO - __main__ - Step 240 Global step 240 Train loss 2.077825 on epoch=14
06/02/2022 05:16:17 - INFO - __main__ - Step 250 Global step 250 Train loss 2.473971 on epoch=15
06/02/2022 05:16:19 - INFO - __main__ - Global step 250 Train loss 3.000633 Classification-F1 0.1 on epoch=15
06/02/2022 05:16:25 - INFO - __main__ - Step 260 Global step 260 Train loss 3.023350 on epoch=16
06/02/2022 05:16:30 - INFO - __main__ - Step 270 Global step 270 Train loss 2.981835 on epoch=16
06/02/2022 05:16:35 - INFO - __main__ - Step 280 Global step 280 Train loss 3.090809 on epoch=17
06/02/2022 05:16:40 - INFO - __main__ - Step 290 Global step 290 Train loss 2.655588 on epoch=18
06/02/2022 05:16:45 - INFO - __main__ - Step 300 Global step 300 Train loss 2.398645 on epoch=18
06/02/2022 05:16:47 - INFO - __main__ - Global step 300 Train loss 2.830046 Classification-F1 0.10800578731613214 on epoch=18
06/02/2022 05:16:53 - INFO - __main__ - Step 310 Global step 310 Train loss 2.577461 on epoch=19
06/02/2022 05:16:59 - INFO - __main__ - Step 320 Global step 320 Train loss 1.745917 on epoch=19
06/02/2022 05:17:04 - INFO - __main__ - Step 330 Global step 330 Train loss 1.667569 on epoch=20
06/02/2022 05:17:09 - INFO - __main__ - Step 340 Global step 340 Train loss 1.752610 on epoch=21
06/02/2022 05:17:14 - INFO - __main__ - Step 350 Global step 350 Train loss 1.724194 on epoch=21
06/02/2022 05:17:16 - INFO - __main__ - Global step 350 Train loss 1.893550 Classification-F1 0.1 on epoch=21
06/02/2022 05:17:21 - INFO - __main__ - Step 360 Global step 360 Train loss 1.822040 on epoch=22
06/02/2022 05:17:26 - INFO - __main__ - Step 370 Global step 370 Train loss 1.497715 on epoch=23
06/02/2022 05:17:32 - INFO - __main__ - Step 380 Global step 380 Train loss 1.353935 on epoch=23
06/02/2022 05:17:37 - INFO - __main__ - Step 390 Global step 390 Train loss 1.409846 on epoch=24
06/02/2022 05:17:42 - INFO - __main__ - Step 400 Global step 400 Train loss 1.495361 on epoch=24
06/02/2022 05:17:44 - INFO - __main__ - Global step 400 Train loss 1.515779 Classification-F1 0.1 on epoch=24
06/02/2022 05:17:49 - INFO - __main__ - Step 410 Global step 410 Train loss 1.234935 on epoch=25
06/02/2022 05:17:55 - INFO - __main__ - Step 420 Global step 420 Train loss 1.058188 on epoch=26
06/02/2022 05:18:00 - INFO - __main__ - Step 430 Global step 430 Train loss 1.104644 on epoch=26
06/02/2022 05:18:05 - INFO - __main__ - Step 440 Global step 440 Train loss 1.034652 on epoch=27
06/02/2022 05:18:10 - INFO - __main__ - Step 450 Global step 450 Train loss 1.118714 on epoch=28
06/02/2022 05:18:12 - INFO - __main__ - Global step 450 Train loss 1.110227 Classification-F1 0.18221632382216324 on epoch=28
06/02/2022 05:18:18 - INFO - __main__ - Step 460 Global step 460 Train loss 1.085007 on epoch=28
06/02/2022 05:18:23 - INFO - __main__ - Step 470 Global step 470 Train loss 1.115166 on epoch=29
06/02/2022 05:18:28 - INFO - __main__ - Step 480 Global step 480 Train loss 1.019709 on epoch=29
06/02/2022 05:18:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.959072 on epoch=30
06/02/2022 05:18:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.940400 on epoch=31
06/02/2022 05:18:41 - INFO - __main__ - Global step 500 Train loss 1.023871 Classification-F1 0.1 on epoch=31
06/02/2022 05:18:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.920581 on epoch=31
06/02/2022 05:18:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.811463 on epoch=32
06/02/2022 05:18:56 - INFO - __main__ - Step 530 Global step 530 Train loss 1.055938 on epoch=33
06/02/2022 05:19:01 - INFO - __main__ - Step 540 Global step 540 Train loss 1.007418 on epoch=33
06/02/2022 05:19:07 - INFO - __main__ - Step 550 Global step 550 Train loss 1.050408 on epoch=34
06/02/2022 05:19:09 - INFO - __main__ - Global step 550 Train loss 0.969162 Classification-F1 0.10062893081761007 on epoch=34
06/02/2022 05:19:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.835689 on epoch=34
06/02/2022 05:19:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.944363 on epoch=35
06/02/2022 05:19:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.924797 on epoch=36
06/02/2022 05:19:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.953623 on epoch=36
06/02/2022 05:19:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.959445 on epoch=37
06/02/2022 05:19:37 - INFO - __main__ - Global step 600 Train loss 0.923583 Classification-F1 0.16183098591549294 on epoch=37
06/02/2022 05:19:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.875625 on epoch=38
06/02/2022 05:19:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.796706 on epoch=38
06/02/2022 05:19:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.879156 on epoch=39
06/02/2022 05:19:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.907286 on epoch=39
06/02/2022 05:20:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.766703 on epoch=40
06/02/2022 05:20:05 - INFO - __main__ - Global step 650 Train loss 0.845095 Classification-F1 0.19787259558314146 on epoch=40
06/02/2022 05:20:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.821099 on epoch=41
06/02/2022 05:20:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.801204 on epoch=41
06/02/2022 05:20:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.821768 on epoch=42
06/02/2022 05:20:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.842884 on epoch=43
06/02/2022 05:20:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.785698 on epoch=43
06/02/2022 05:20:35 - INFO - __main__ - Global step 700 Train loss 0.814531 Classification-F1 0.36172837630758703 on epoch=43
06/02/2022 05:20:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.814589 on epoch=44
06/02/2022 05:20:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.752263 on epoch=44
06/02/2022 05:20:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.720840 on epoch=45
06/02/2022 05:20:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.801059 on epoch=46
06/02/2022 05:21:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.700830 on epoch=46
06/02/2022 05:21:03 - INFO - __main__ - Global step 750 Train loss 0.757916 Classification-F1 0.22267320308557423 on epoch=46
06/02/2022 05:21:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.844013 on epoch=47
06/02/2022 05:21:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.826893 on epoch=48
06/02/2022 05:21:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.756925 on epoch=48
06/02/2022 05:21:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.735020 on epoch=49
06/02/2022 05:21:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.826141 on epoch=49
06/02/2022 05:21:31 - INFO - __main__ - Global step 800 Train loss 0.797799 Classification-F1 0.28432199994425633 on epoch=49
06/02/2022 05:21:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.747773 on epoch=50
06/02/2022 05:21:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.802775 on epoch=51
06/02/2022 05:21:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.772053 on epoch=51
06/02/2022 05:21:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.748367 on epoch=52
06/02/2022 05:21:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.823341 on epoch=53
06/02/2022 05:21:59 - INFO - __main__ - Global step 850 Train loss 0.778862 Classification-F1 0.2688627255781353 on epoch=53
06/02/2022 05:22:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.678013 on epoch=53
06/02/2022 05:22:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.734464 on epoch=54
06/02/2022 05:22:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.764945 on epoch=54
06/02/2022 05:22:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.757481 on epoch=55
06/02/2022 05:22:25 - INFO - __main__ - Step 900 Global step 900 Train loss 0.782752 on epoch=56
06/02/2022 05:22:27 - INFO - __main__ - Global step 900 Train loss 0.743531 Classification-F1 0.4812028058148748 on epoch=56
06/02/2022 05:22:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.686003 on epoch=56
06/02/2022 05:22:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.685833 on epoch=57
06/02/2022 05:22:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.714805 on epoch=58
06/02/2022 05:22:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.664094 on epoch=58
06/02/2022 05:22:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.773042 on epoch=59
06/02/2022 05:22:56 - INFO - __main__ - Global step 950 Train loss 0.704755 Classification-F1 0.3142015646388438 on epoch=59
06/02/2022 05:23:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.693484 on epoch=59
06/02/2022 05:23:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.750938 on epoch=60
06/02/2022 05:23:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.705025 on epoch=61
06/02/2022 05:23:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.644577 on epoch=61
06/02/2022 05:23:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.714944 on epoch=62
06/02/2022 05:23:23 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 05:23:23 - INFO - __main__ - Printing 3 examples
06/02/2022 05:23:23 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/02/2022 05:23:23 - INFO - __main__ - ['others']
06/02/2022 05:23:23 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/02/2022 05:23:23 - INFO - __main__ - ['others']
06/02/2022 05:23:23 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/02/2022 05:23:23 - INFO - __main__ - ['others']
06/02/2022 05:23:23 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:23:23 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:23:23 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 05:23:23 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 05:23:23 - INFO - __main__ - Printing 3 examples
06/02/2022 05:23:23 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
06/02/2022 05:23:23 - INFO - __main__ - ['others']
06/02/2022 05:23:23 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
06/02/2022 05:23:23 - INFO - __main__ - ['others']
06/02/2022 05:23:23 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
06/02/2022 05:23:23 - INFO - __main__ - ['others']
06/02/2022 05:23:23 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:23:23 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:23:24 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 05:23:24 - INFO - __main__ - Global step 1000 Train loss 0.701793 Classification-F1 0.41140561375929324 on epoch=62
06/02/2022 05:23:24 - INFO - __main__ - save last model!
06/02/2022 05:23:31 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 05:23:31 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 05:23:31 - INFO - __main__ - Printing 3 examples
06/02/2022 05:23:31 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 05:23:31 - INFO - __main__ - ['others']
06/02/2022 05:23:31 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 05:23:31 - INFO - __main__ - ['others']
06/02/2022 05:23:31 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 05:23:31 - INFO - __main__ - ['others']
06/02/2022 05:23:31 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:23:33 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:23:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 05:23:34 - INFO - __main__ - Starting training!
06/02/2022 05:23:39 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 05:24:22 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_87_0.0003_8_predictions.txt
06/02/2022 05:24:22 - INFO - __main__ - Classification-F1 on test data: 0.2905
06/02/2022 05:24:23 - INFO - __main__ - prefix=emo_64_87, lr=0.0003, bsz=8, dev_performance=0.4812028058148748, test_performance=0.2904958278466012
06/02/2022 05:24:23 - INFO - __main__ - Running ... prefix=emo_64_87, lr=0.0002, bsz=8 ...
06/02/2022 05:24:24 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 05:24:24 - INFO - __main__ - Printing 3 examples
06/02/2022 05:24:24 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/02/2022 05:24:24 - INFO - __main__ - ['others']
06/02/2022 05:24:24 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/02/2022 05:24:24 - INFO - __main__ - ['others']
06/02/2022 05:24:24 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/02/2022 05:24:24 - INFO - __main__ - ['others']
06/02/2022 05:24:24 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:24:24 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:24:24 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 05:24:24 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 05:24:24 - INFO - __main__ - Printing 3 examples
06/02/2022 05:24:24 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
06/02/2022 05:24:24 - INFO - __main__ - ['others']
06/02/2022 05:24:24 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
06/02/2022 05:24:24 - INFO - __main__ - ['others']
06/02/2022 05:24:24 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
06/02/2022 05:24:24 - INFO - __main__ - ['others']
06/02/2022 05:24:24 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:24:24 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:24:24 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 05:24:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 05:24:37 - INFO - __main__ - Starting training!
06/02/2022 05:24:41 - INFO - __main__ - Step 10 Global step 10 Train loss 26.308487 on epoch=0
06/02/2022 05:24:46 - INFO - __main__ - Step 20 Global step 20 Train loss 21.464649 on epoch=1
06/02/2022 05:24:51 - INFO - __main__ - Step 30 Global step 30 Train loss 18.059383 on epoch=1
06/02/2022 05:24:56 - INFO - __main__ - Step 40 Global step 40 Train loss 17.771412 on epoch=2
06/02/2022 05:25:01 - INFO - __main__ - Step 50 Global step 50 Train loss 16.963425 on epoch=3
06/02/2022 05:25:20 - INFO - __main__ - Global step 50 Train loss 20.113470 Classification-F1 0.0 on epoch=3
06/02/2022 05:25:26 - INFO - __main__ - Step 60 Global step 60 Train loss 16.064945 on epoch=3
06/02/2022 05:25:31 - INFO - __main__ - Step 70 Global step 70 Train loss 15.708133 on epoch=4
06/02/2022 05:25:36 - INFO - __main__ - Step 80 Global step 80 Train loss 14.082959 on epoch=4
06/02/2022 05:25:41 - INFO - __main__ - Step 90 Global step 90 Train loss 14.347020 on epoch=5
06/02/2022 05:25:47 - INFO - __main__ - Step 100 Global step 100 Train loss 13.682541 on epoch=6
06/02/2022 05:25:59 - INFO - __main__ - Global step 100 Train loss 14.777121 Classification-F1 0.0 on epoch=6
06/02/2022 05:26:04 - INFO - __main__ - Step 110 Global step 110 Train loss 12.842142 on epoch=6
06/02/2022 05:26:09 - INFO - __main__ - Step 120 Global step 120 Train loss 12.684592 on epoch=7
06/02/2022 05:26:14 - INFO - __main__ - Step 130 Global step 130 Train loss 11.761792 on epoch=8
06/02/2022 05:26:19 - INFO - __main__ - Step 140 Global step 140 Train loss 11.711801 on epoch=8
06/02/2022 05:26:25 - INFO - __main__ - Step 150 Global step 150 Train loss 11.066494 on epoch=9
06/02/2022 05:26:31 - INFO - __main__ - Global step 150 Train loss 12.013363 Classification-F1 0.0 on epoch=9
06/02/2022 05:26:36 - INFO - __main__ - Step 160 Global step 160 Train loss 10.179215 on epoch=9
06/02/2022 05:26:41 - INFO - __main__ - Step 170 Global step 170 Train loss 9.632968 on epoch=10
06/02/2022 05:26:46 - INFO - __main__ - Step 180 Global step 180 Train loss 8.152231 on epoch=11
06/02/2022 05:26:51 - INFO - __main__ - Step 190 Global step 190 Train loss 7.230384 on epoch=11
06/02/2022 05:26:56 - INFO - __main__ - Step 200 Global step 200 Train loss 6.272162 on epoch=12
06/02/2022 05:26:59 - INFO - __main__ - Global step 200 Train loss 8.293392 Classification-F1 0.0 on epoch=12
06/02/2022 05:27:04 - INFO - __main__ - Step 210 Global step 210 Train loss 3.972069 on epoch=13
06/02/2022 05:27:10 - INFO - __main__ - Step 220 Global step 220 Train loss 4.480377 on epoch=13
06/02/2022 05:27:15 - INFO - __main__ - Step 230 Global step 230 Train loss 3.683566 on epoch=14
06/02/2022 05:27:20 - INFO - __main__ - Step 240 Global step 240 Train loss 3.670635 on epoch=14
06/02/2022 05:27:25 - INFO - __main__ - Step 250 Global step 250 Train loss 3.536540 on epoch=15
06/02/2022 05:27:27 - INFO - __main__ - Global step 250 Train loss 3.868637 Classification-F1 0.1 on epoch=15
06/02/2022 05:27:33 - INFO - __main__ - Step 260 Global step 260 Train loss 3.294169 on epoch=16
06/02/2022 05:27:38 - INFO - __main__ - Step 270 Global step 270 Train loss 3.860470 on epoch=16
06/02/2022 05:27:43 - INFO - __main__ - Step 280 Global step 280 Train loss 1.855989 on epoch=17
06/02/2022 05:27:48 - INFO - __main__ - Step 290 Global step 290 Train loss 1.303221 on epoch=18
06/02/2022 05:27:53 - INFO - __main__ - Step 300 Global step 300 Train loss 1.207920 on epoch=18
06/02/2022 05:27:55 - INFO - __main__ - Global step 300 Train loss 2.304354 Classification-F1 0.4322934466848941 on epoch=18
06/02/2022 05:28:01 - INFO - __main__ - Step 310 Global step 310 Train loss 1.086075 on epoch=19
06/02/2022 05:28:06 - INFO - __main__ - Step 320 Global step 320 Train loss 1.007877 on epoch=19
06/02/2022 05:28:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.971097 on epoch=20
06/02/2022 05:28:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.747513 on epoch=21
06/02/2022 05:28:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.924788 on epoch=21
06/02/2022 05:28:24 - INFO - __main__ - Global step 350 Train loss 0.947470 Classification-F1 0.6602305305448055 on epoch=21
06/02/2022 05:28:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.849966 on epoch=22
06/02/2022 05:28:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.904935 on epoch=23
06/02/2022 05:28:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.559627 on epoch=23
06/02/2022 05:28:45 - INFO - __main__ - Step 390 Global step 390 Train loss 1.099911 on epoch=24
06/02/2022 05:28:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.518759 on epoch=24
06/02/2022 05:28:52 - INFO - __main__ - Global step 400 Train loss 0.786640 Classification-F1 0.6745820008610562 on epoch=24
06/02/2022 05:28:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.610254 on epoch=25
06/02/2022 05:29:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.587731 on epoch=26
06/02/2022 05:29:08 - INFO - __main__ - Step 430 Global step 430 Train loss 1.014382 on epoch=26
06/02/2022 05:29:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.636991 on epoch=27
06/02/2022 05:29:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.525361 on epoch=28
06/02/2022 05:29:21 - INFO - __main__ - Global step 450 Train loss 0.674944 Classification-F1 0.6772169290620439 on epoch=28
06/02/2022 05:29:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.508076 on epoch=28
06/02/2022 05:29:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.389166 on epoch=29
06/02/2022 05:29:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.658191 on epoch=29
06/02/2022 05:29:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.375556 on epoch=30
06/02/2022 05:29:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.472198 on epoch=31
06/02/2022 05:29:49 - INFO - __main__ - Global step 500 Train loss 0.480638 Classification-F1 0.6894308146987609 on epoch=31
06/02/2022 05:29:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.537985 on epoch=31
06/02/2022 05:30:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.345509 on epoch=32
06/02/2022 05:30:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.537693 on epoch=33
06/02/2022 05:30:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.404117 on epoch=33
06/02/2022 05:30:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.369325 on epoch=34
06/02/2022 05:30:18 - INFO - __main__ - Global step 550 Train loss 0.438926 Classification-F1 0.6960317460317461 on epoch=34
06/02/2022 05:30:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.284591 on epoch=34
06/02/2022 05:30:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.386474 on epoch=35
06/02/2022 05:30:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.337066 on epoch=36
06/02/2022 05:30:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.484167 on epoch=36
06/02/2022 05:30:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.576714 on epoch=37
06/02/2022 05:30:46 - INFO - __main__ - Global step 600 Train loss 0.413802 Classification-F1 0.6947935524824411 on epoch=37
06/02/2022 05:30:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.519926 on epoch=38
06/02/2022 05:30:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.383597 on epoch=38
06/02/2022 05:31:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.521843 on epoch=39
06/02/2022 05:31:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.397626 on epoch=39
06/02/2022 05:31:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.300445 on epoch=40
06/02/2022 05:31:14 - INFO - __main__ - Global step 650 Train loss 0.424687 Classification-F1 0.7119529914529915 on epoch=40
06/02/2022 05:31:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.298497 on epoch=41
06/02/2022 05:31:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.332799 on epoch=41
06/02/2022 05:31:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.176425 on epoch=42
06/02/2022 05:31:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.240672 on epoch=43
06/02/2022 05:31:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.268068 on epoch=43
06/02/2022 05:31:42 - INFO - __main__ - Global step 700 Train loss 0.263292 Classification-F1 0.7176652892561983 on epoch=43
06/02/2022 05:31:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.328276 on epoch=44
06/02/2022 05:31:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.263165 on epoch=44
06/02/2022 05:31:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.141602 on epoch=45
06/02/2022 05:32:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.379063 on epoch=46
06/02/2022 05:32:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.362758 on epoch=46
06/02/2022 05:32:11 - INFO - __main__ - Global step 750 Train loss 0.294973 Classification-F1 0.6880813694314838 on epoch=46
06/02/2022 05:32:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.149317 on epoch=47
06/02/2022 05:32:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.310531 on epoch=48
06/02/2022 05:32:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.272732 on epoch=48
06/02/2022 05:32:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.305243 on epoch=49
06/02/2022 05:32:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.284789 on epoch=49
06/02/2022 05:32:39 - INFO - __main__ - Global step 800 Train loss 0.264523 Classification-F1 0.694802230279828 on epoch=49
06/02/2022 05:32:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.163777 on epoch=50
06/02/2022 05:32:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.239540 on epoch=51
06/02/2022 05:32:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.383696 on epoch=51
06/02/2022 05:32:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.149946 on epoch=52
06/02/2022 05:33:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.180695 on epoch=53
06/02/2022 05:33:06 - INFO - __main__ - Global step 850 Train loss 0.223531 Classification-F1 0.6950840641344181 on epoch=53
06/02/2022 05:33:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.169997 on epoch=53
06/02/2022 05:33:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.271764 on epoch=54
06/02/2022 05:33:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.166412 on epoch=54
06/02/2022 05:33:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.167288 on epoch=55
06/02/2022 05:33:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.176189 on epoch=56
06/02/2022 05:33:34 - INFO - __main__ - Global step 900 Train loss 0.190330 Classification-F1 0.7186416872607 on epoch=56
06/02/2022 05:33:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.350798 on epoch=56
06/02/2022 05:33:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.192915 on epoch=57
06/02/2022 05:33:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.245262 on epoch=58
06/02/2022 05:33:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.154454 on epoch=58
06/02/2022 05:34:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.320097 on epoch=59
06/02/2022 05:34:03 - INFO - __main__ - Global step 950 Train loss 0.252705 Classification-F1 0.7121982755073246 on epoch=59
06/02/2022 05:34:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.367148 on epoch=59
06/02/2022 05:34:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.236259 on epoch=60
06/02/2022 05:34:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.218202 on epoch=61
06/02/2022 05:34:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.334017 on epoch=61
06/02/2022 05:34:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.231637 on epoch=62
06/02/2022 05:34:30 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 05:34:30 - INFO - __main__ - Printing 3 examples
06/02/2022 05:34:30 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/02/2022 05:34:30 - INFO - __main__ - ['others']
06/02/2022 05:34:30 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/02/2022 05:34:30 - INFO - __main__ - ['others']
06/02/2022 05:34:30 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/02/2022 05:34:30 - INFO - __main__ - ['others']
06/02/2022 05:34:30 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:34:30 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:34:30 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 05:34:30 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 05:34:30 - INFO - __main__ - Printing 3 examples
06/02/2022 05:34:30 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
06/02/2022 05:34:30 - INFO - __main__ - ['others']
06/02/2022 05:34:30 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
06/02/2022 05:34:30 - INFO - __main__ - ['others']
06/02/2022 05:34:30 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
06/02/2022 05:34:30 - INFO - __main__ - ['others']
06/02/2022 05:34:30 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:34:30 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:34:30 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 05:34:31 - INFO - __main__ - Global step 1000 Train loss 0.277452 Classification-F1 0.7100725088031457 on epoch=62
06/02/2022 05:34:31 - INFO - __main__ - save last model!
06/02/2022 05:34:38 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 05:34:39 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 05:34:39 - INFO - __main__ - Printing 3 examples
06/02/2022 05:34:39 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 05:34:39 - INFO - __main__ - ['others']
06/02/2022 05:34:39 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 05:34:39 - INFO - __main__ - ['others']
06/02/2022 05:34:39 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 05:34:39 - INFO - __main__ - ['others']
06/02/2022 05:34:39 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:34:41 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:34:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 05:34:43 - INFO - __main__ - Starting training!
06/02/2022 05:34:46 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 05:35:42 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_87_0.0002_8_predictions.txt
06/02/2022 05:35:42 - INFO - __main__ - Classification-F1 on test data: 0.0614
06/02/2022 05:35:43 - INFO - __main__ - prefix=emo_64_87, lr=0.0002, bsz=8, dev_performance=0.7186416872607, test_performance=0.061417272721178834
06/02/2022 05:35:43 - INFO - __main__ - Running ... prefix=emo_64_87, lr=0.0001, bsz=8 ...
06/02/2022 05:35:44 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 05:35:44 - INFO - __main__ - Printing 3 examples
06/02/2022 05:35:44 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/02/2022 05:35:44 - INFO - __main__ - ['others']
06/02/2022 05:35:44 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/02/2022 05:35:44 - INFO - __main__ - ['others']
06/02/2022 05:35:44 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/02/2022 05:35:44 - INFO - __main__ - ['others']
06/02/2022 05:35:44 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:35:44 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:35:44 - INFO - __main__ - Loaded 256 examples from train data
06/02/2022 05:35:44 - INFO - __main__ - Start tokenizing ... 256 instances
06/02/2022 05:35:44 - INFO - __main__ - Printing 3 examples
06/02/2022 05:35:44 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
06/02/2022 05:35:44 - INFO - __main__ - ['others']
06/02/2022 05:35:44 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
06/02/2022 05:35:44 - INFO - __main__ - ['others']
06/02/2022 05:35:44 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
06/02/2022 05:35:44 - INFO - __main__ - ['others']
06/02/2022 05:35:44 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:35:44 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:35:44 - INFO - __main__ - Loaded 256 examples from dev data
06/02/2022 05:35:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/02/2022 05:35:57 - INFO - __main__ - Starting training!
06/02/2022 05:36:01 - INFO - __main__ - Step 10 Global step 10 Train loss 25.666342 on epoch=0
06/02/2022 05:36:06 - INFO - __main__ - Step 20 Global step 20 Train loss 22.916376 on epoch=1
06/02/2022 05:36:11 - INFO - __main__ - Step 30 Global step 30 Train loss 20.183060 on epoch=1
06/02/2022 05:36:16 - INFO - __main__ - Step 40 Global step 40 Train loss 18.308826 on epoch=2
06/02/2022 05:36:21 - INFO - __main__ - Step 50 Global step 50 Train loss 17.973227 on epoch=3
06/02/2022 05:37:37 - INFO - __main__ - Global step 50 Train loss 21.009565 Classification-F1 0.0 on epoch=3
06/02/2022 05:37:43 - INFO - __main__ - Step 60 Global step 60 Train loss 16.995388 on epoch=3
06/02/2022 05:37:48 - INFO - __main__ - Step 70 Global step 70 Train loss 17.131876 on epoch=4
06/02/2022 05:37:53 - INFO - __main__ - Step 80 Global step 80 Train loss 16.884981 on epoch=4
06/02/2022 05:37:58 - INFO - __main__ - Step 90 Global step 90 Train loss 16.107775 on epoch=5
06/02/2022 05:38:03 - INFO - __main__ - Step 100 Global step 100 Train loss 15.702181 on epoch=6
06/02/2022 05:39:04 - INFO - __main__ - Global step 100 Train loss 16.564438 Classification-F1 0.00016999575010624734 on epoch=6
06/02/2022 05:39:09 - INFO - __main__ - Step 110 Global step 110 Train loss 16.175259 on epoch=6
06/02/2022 05:39:14 - INFO - __main__ - Step 120 Global step 120 Train loss 15.893919 on epoch=7
06/02/2022 05:39:19 - INFO - __main__ - Step 130 Global step 130 Train loss 14.160967 on epoch=8
06/02/2022 05:39:25 - INFO - __main__ - Step 140 Global step 140 Train loss 14.510310 on epoch=8
06/02/2022 05:39:30 - INFO - __main__ - Step 150 Global step 150 Train loss 14.218229 on epoch=9
06/02/2022 05:40:16 - INFO - __main__ - Global step 150 Train loss 14.991738 Classification-F1 0.0002229654403567447 on epoch=9
06/02/2022 05:40:21 - INFO - __main__ - Step 160 Global step 160 Train loss 13.926809 on epoch=9
06/02/2022 05:40:26 - INFO - __main__ - Step 170 Global step 170 Train loss 14.068068 on epoch=10
06/02/2022 05:40:31 - INFO - __main__ - Step 180 Global step 180 Train loss 13.330050 on epoch=11
06/02/2022 05:40:36 - INFO - __main__ - Step 190 Global step 190 Train loss 13.161105 on epoch=11
06/02/2022 05:40:41 - INFO - __main__ - Step 200 Global step 200 Train loss 12.593929 on epoch=12
06/02/2022 05:41:11 - INFO - __main__ - Global step 200 Train loss 13.415994 Classification-F1 0.0 on epoch=12
06/02/2022 05:41:16 - INFO - __main__ - Step 210 Global step 210 Train loss 12.610375 on epoch=13
06/02/2022 05:41:21 - INFO - __main__ - Step 220 Global step 220 Train loss 12.383937 on epoch=13
06/02/2022 05:41:26 - INFO - __main__ - Step 230 Global step 230 Train loss 12.632089 on epoch=14
06/02/2022 05:41:31 - INFO - __main__ - Step 240 Global step 240 Train loss 11.668365 on epoch=14
06/02/2022 05:41:36 - INFO - __main__ - Step 250 Global step 250 Train loss 11.204103 on epoch=15
06/02/2022 05:41:57 - INFO - __main__ - Global step 250 Train loss 12.099773 Classification-F1 0.0 on epoch=15
06/02/2022 05:42:02 - INFO - __main__ - Step 260 Global step 260 Train loss 10.574720 on epoch=16
06/02/2022 05:42:07 - INFO - __main__ - Step 270 Global step 270 Train loss 10.954865 on epoch=16
06/02/2022 05:42:12 - INFO - __main__ - Step 280 Global step 280 Train loss 10.101451 on epoch=17
06/02/2022 05:42:17 - INFO - __main__ - Step 290 Global step 290 Train loss 9.619964 on epoch=18
06/02/2022 05:42:22 - INFO - __main__ - Step 300 Global step 300 Train loss 8.270472 on epoch=18
06/02/2022 05:42:40 - INFO - __main__ - Global step 300 Train loss 9.904294 Classification-F1 0.0019242228197452079 on epoch=18
06/02/2022 05:42:45 - INFO - __main__ - Step 310 Global step 310 Train loss 7.637800 on epoch=19
06/02/2022 05:42:50 - INFO - __main__ - Step 320 Global step 320 Train loss 7.096377 on epoch=19
06/02/2022 05:42:55 - INFO - __main__ - Step 330 Global step 330 Train loss 4.827824 on epoch=20
06/02/2022 05:43:00 - INFO - __main__ - Step 340 Global step 340 Train loss 5.314963 on epoch=21
06/02/2022 05:43:05 - INFO - __main__ - Step 350 Global step 350 Train loss 5.066707 on epoch=21
06/02/2022 05:43:07 - INFO - __main__ - Global step 350 Train loss 5.988735 Classification-F1 0.1 on epoch=21
06/02/2022 05:43:13 - INFO - __main__ - Step 360 Global step 360 Train loss 4.729159 on epoch=22
06/02/2022 05:43:18 - INFO - __main__ - Step 370 Global step 370 Train loss 4.675486 on epoch=23
06/02/2022 05:43:23 - INFO - __main__ - Step 380 Global step 380 Train loss 4.488574 on epoch=23
06/02/2022 05:43:28 - INFO - __main__ - Step 390 Global step 390 Train loss 4.407738 on epoch=24
06/02/2022 05:43:33 - INFO - __main__ - Step 400 Global step 400 Train loss 3.711437 on epoch=24
06/02/2022 05:43:34 - INFO - __main__ - Global step 400 Train loss 4.402479 Classification-F1 0.1 on epoch=24
06/02/2022 05:43:39 - INFO - __main__ - Step 410 Global step 410 Train loss 4.384604 on epoch=25
06/02/2022 05:43:44 - INFO - __main__ - Step 420 Global step 420 Train loss 3.645495 on epoch=26
06/02/2022 05:43:49 - INFO - __main__ - Step 430 Global step 430 Train loss 4.188607 on epoch=26
06/02/2022 05:43:54 - INFO - __main__ - Step 440 Global step 440 Train loss 3.621738 on epoch=27
06/02/2022 05:43:59 - INFO - __main__ - Step 450 Global step 450 Train loss 3.380159 on epoch=28
06/02/2022 05:44:01 - INFO - __main__ - Global step 450 Train loss 3.844121 Classification-F1 0.17210884353741496 on epoch=28
06/02/2022 05:44:07 - INFO - __main__ - Step 460 Global step 460 Train loss 3.724678 on epoch=28
06/02/2022 05:44:12 - INFO - __main__ - Step 470 Global step 470 Train loss 3.296699 on epoch=29
06/02/2022 05:44:17 - INFO - __main__ - Step 480 Global step 480 Train loss 4.227035 on epoch=29
06/02/2022 05:44:22 - INFO - __main__ - Step 490 Global step 490 Train loss 3.578325 on epoch=30
06/02/2022 05:44:27 - INFO - __main__ - Step 500 Global step 500 Train loss 2.950428 on epoch=31
06/02/2022 05:44:29 - INFO - __main__ - Global step 500 Train loss 3.555433 Classification-F1 0.19161176316593218 on epoch=31
06/02/2022 05:44:34 - INFO - __main__ - Step 510 Global step 510 Train loss 3.565257 on epoch=31
06/02/2022 05:44:39 - INFO - __main__ - Step 520 Global step 520 Train loss 2.677754 on epoch=32
06/02/2022 05:44:44 - INFO - __main__ - Step 530 Global step 530 Train loss 3.163576 on epoch=33
06/02/2022 05:44:49 - INFO - __main__ - Step 540 Global step 540 Train loss 2.568528 on epoch=33
06/02/2022 05:44:54 - INFO - __main__ - Step 550 Global step 550 Train loss 3.048179 on epoch=34
06/02/2022 05:44:56 - INFO - __main__ - Global step 550 Train loss 3.004658 Classification-F1 0.17671290458175704 on epoch=34
06/02/2022 05:45:01 - INFO - __main__ - Step 560 Global step 560 Train loss 2.905545 on epoch=34
06/02/2022 05:45:06 - INFO - __main__ - Step 570 Global step 570 Train loss 2.945249 on epoch=35
06/02/2022 05:45:11 - INFO - __main__ - Step 580 Global step 580 Train loss 3.096112 on epoch=36
06/02/2022 05:45:16 - INFO - __main__ - Step 590 Global step 590 Train loss 3.468621 on epoch=36
06/02/2022 05:45:21 - INFO - __main__ - Step 600 Global step 600 Train loss 3.048555 on epoch=37
06/02/2022 05:45:23 - INFO - __main__ - Global step 600 Train loss 3.092816 Classification-F1 0.1 on epoch=37
06/02/2022 05:45:28 - INFO - __main__ - Step 610 Global step 610 Train loss 3.223742 on epoch=38
06/02/2022 05:45:33 - INFO - __main__ - Step 620 Global step 620 Train loss 2.568341 on epoch=38
06/02/2022 05:45:38 - INFO - __main__ - Step 630 Global step 630 Train loss 2.236046 on epoch=39
06/02/2022 05:45:43 - INFO - __main__ - Step 640 Global step 640 Train loss 2.794514 on epoch=39
06/02/2022 05:45:48 - INFO - __main__ - Step 650 Global step 650 Train loss 2.690830 on epoch=40
06/02/2022 05:45:50 - INFO - __main__ - Global step 650 Train loss 2.702694 Classification-F1 0.1378191856452726 on epoch=40
06/02/2022 05:45:55 - INFO - __main__ - Step 660 Global step 660 Train loss 2.568748 on epoch=41
06/02/2022 05:46:00 - INFO - __main__ - Step 670 Global step 670 Train loss 2.380986 on epoch=41
06/02/2022 05:46:05 - INFO - __main__ - Step 680 Global step 680 Train loss 2.670362 on epoch=42
06/02/2022 05:46:10 - INFO - __main__ - Step 690 Global step 690 Train loss 2.288105 on epoch=43
06/02/2022 05:46:15 - INFO - __main__ - Step 700 Global step 700 Train loss 1.869684 on epoch=43
06/02/2022 05:46:17 - INFO - __main__ - Global step 700 Train loss 2.355577 Classification-F1 0.2594917419925773 on epoch=43
06/02/2022 05:46:23 - INFO - __main__ - Step 710 Global step 710 Train loss 2.819801 on epoch=44
06/02/2022 05:46:28 - INFO - __main__ - Step 720 Global step 720 Train loss 1.970325 on epoch=44
06/02/2022 05:46:33 - INFO - __main__ - Step 730 Global step 730 Train loss 2.131220 on epoch=45
06/02/2022 05:46:38 - INFO - __main__ - Step 740 Global step 740 Train loss 1.965849 on epoch=46
06/02/2022 05:46:43 - INFO - __main__ - Step 750 Global step 750 Train loss 1.295447 on epoch=46
06/02/2022 05:46:45 - INFO - __main__ - Global step 750 Train loss 2.036528 Classification-F1 0.427929768189747 on epoch=46
06/02/2022 05:46:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.815062 on epoch=47
06/02/2022 05:46:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.708500 on epoch=48
06/02/2022 05:47:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.547423 on epoch=48
06/02/2022 05:47:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.731981 on epoch=49
06/02/2022 05:47:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.607016 on epoch=49
06/02/2022 05:47:12 - INFO - __main__ - Global step 800 Train loss 0.681997 Classification-F1 0.6944785179902706 on epoch=49
06/02/2022 05:47:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.765368 on epoch=50
06/02/2022 05:47:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.573581 on epoch=51
06/02/2022 05:47:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.540011 on epoch=51
06/02/2022 05:47:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.745141 on epoch=52
06/02/2022 05:47:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.600293 on epoch=53
06/02/2022 05:47:40 - INFO - __main__ - Global step 850 Train loss 0.644879 Classification-F1 0.5597854456978545 on epoch=53
06/02/2022 05:47:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.593720 on epoch=53
06/02/2022 05:47:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.544111 on epoch=54
06/02/2022 05:47:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.601246 on epoch=54
06/02/2022 05:48:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.484997 on epoch=55
06/02/2022 05:48:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.745015 on epoch=56
06/02/2022 05:48:07 - INFO - __main__ - Global step 900 Train loss 0.593818 Classification-F1 0.7213893946600176 on epoch=56
06/02/2022 05:48:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.547893 on epoch=56
06/02/2022 05:48:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.678635 on epoch=57
06/02/2022 05:48:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.816513 on epoch=58
06/02/2022 05:48:27 - INFO - __main__ - Step 940 Global step 940 Train loss 1.142912 on epoch=58
06/02/2022 05:48:32 - INFO - __main__ - Step 950 Global step 950 Train loss 1.105733 on epoch=59
06/02/2022 05:48:34 - INFO - __main__ - Global step 950 Train loss 0.858337 Classification-F1 0.7265710638970282 on epoch=59
06/02/2022 05:48:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.627539 on epoch=59
06/02/2022 05:48:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.826329 on epoch=60
06/02/2022 05:48:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.456552 on epoch=61
06/02/2022 05:48:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.454835 on epoch=61
06/02/2022 05:49:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.558747 on epoch=62
06/02/2022 05:49:02 - INFO - __main__ - Global step 1000 Train loss 0.584800 Classification-F1 0.7661093797961616 on epoch=62
06/02/2022 05:49:03 - INFO - __main__ - save last model!
06/02/2022 05:49:10 - INFO - __main__ - Loading checkpoint on the fly
06/02/2022 05:49:10 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 05:49:10 - INFO - __main__ - Printing 3 examples
06/02/2022 05:49:10 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 05:49:10 - INFO - __main__ - ['others']
06/02/2022 05:49:10 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 05:49:10 - INFO - __main__ - ['others']
06/02/2022 05:49:10 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 05:49:10 - INFO - __main__ - ['others']
06/02/2022 05:49:10 - INFO - __main__ - Tokenizing Input ...
06/02/2022 05:49:12 - INFO - __main__ - Tokenizing Output ...
06/02/2022 05:49:17 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 05:50:01 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-emo/emo_64_87_0.0001_8_predictions.txt
06/02/2022 05:50:01 - INFO - __main__ - Classification-F1 on test data: 0.4322
06/02/2022 05:50:02 - INFO - __main__ - prefix=emo_64_87, lr=0.0001, bsz=8, dev_performance=0.7661093797961616, test_performance=0.4322086160721037
