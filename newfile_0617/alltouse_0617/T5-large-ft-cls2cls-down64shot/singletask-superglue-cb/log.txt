05/21/2022 21:24:22 - INFO - __main__ - Namespace(task_dir='data_64/superglue-cb/', task_name='superglue-cb', identifier='T5-large-ft-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:24:22 - INFO - __main__ - models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb
05/21/2022 21:24:22 - INFO - __main__ - Namespace(task_dir='data_64/superglue-cb/', task_name='superglue-cb', identifier='T5-large-ft-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:24:22 - INFO - __main__ - models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb
05/21/2022 21:24:23 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:24:23 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:24:23 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:24:23 - INFO - __main__ - Using 2 gpus
05/21/2022 21:24:23 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:24:23 - INFO - __main__ - Using 2 gpus
05/21/2022 21:24:23 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_64_100', 'superglue-cb_64_13', 'superglue-cb_64_21', 'superglue-cb_64_42', 'superglue-cb_64_87']
05/21/2022 21:24:23 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_64_100', 'superglue-cb_64_13', 'superglue-cb_64_21', 'superglue-cb_64_42', 'superglue-cb_64_87']
05/21/2022 21:24:28 - INFO - __main__ - Running ... prefix=superglue-cb_64_100, lr=0.0005, bsz=8 ...
06/01/2022 13:36:12 - INFO - __main__ - Namespace(task_dir='data_64/superglue-cb/', task_name='superglue-cb', identifier='T5-large-ft-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
06/01/2022 13:36:12 - INFO - __main__ - models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb
06/01/2022 13:36:12 - INFO - __main__ - Namespace(task_dir='data_64/superglue-cb/', task_name='superglue-cb', identifier='T5-large-ft-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
06/01/2022 13:36:12 - INFO - __main__ - models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb
06/01/2022 13:36:13 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/01/2022 13:36:13 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/01/2022 13:36:13 - INFO - __main__ - args.device: cuda:0
06/01/2022 13:36:13 - INFO - __main__ - Using 2 gpus
06/01/2022 13:36:13 - INFO - __main__ - args.device: cuda:1
06/01/2022 13:36:13 - INFO - __main__ - Using 2 gpus
06/01/2022 13:36:13 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_64_100', 'superglue-cb_64_13', 'superglue-cb_64_21', 'superglue-cb_64_42', 'superglue-cb_64_87']
06/01/2022 13:36:13 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_64_100', 'superglue-cb_64_13', 'superglue-cb_64_21', 'superglue-cb_64_42', 'superglue-cb_64_87']
06/01/2022 13:36:18 - INFO - __main__ - Running ... prefix=superglue-cb_64_100, lr=0.0005, bsz=8 ...
06/01/2022 13:36:19 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 13:36:19 - INFO - __main__ - Printing 3 examples
06/01/2022 13:36:19 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/01/2022 13:36:19 - INFO - __main__ - ['contradiction']
06/01/2022 13:36:19 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/01/2022 13:36:19 - INFO - __main__ - ['contradiction']
06/01/2022 13:36:19 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/01/2022 13:36:19 - INFO - __main__ - ['contradiction']
06/01/2022 13:36:19 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:36:19 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 13:36:19 - INFO - __main__ - Printing 3 examples
06/01/2022 13:36:19 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/01/2022 13:36:19 - INFO - __main__ - ['contradiction']
06/01/2022 13:36:19 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/01/2022 13:36:19 - INFO - __main__ - ['contradiction']
06/01/2022 13:36:19 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/01/2022 13:36:19 - INFO - __main__ - ['contradiction']
06/01/2022 13:36:19 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:36:19 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:36:19 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:36:19 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 13:36:19 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 13:36:19 - INFO - __main__ - Printing 3 examples
06/01/2022 13:36:19 - INFO - __main__ -  [superglue-cb] premise: Robert Erwin, president of Biosource, called Plant Genetic's approach ``interesting'' and ``novel,'' and ``complementary rather than competitive.'' ``There is a large market out there hungry for hybrid seeds,'' he said. Mr. Robinson of Delta & Pine, the seed producer in Scott, Miss., said Plant Genetic's success in creating genetically engineered male steriles doesn't automatically mean it would be simple to create hybrids in all crops. [SEP] hypothesis: it would be simple to create hybrids in all crops
06/01/2022 13:36:19 - INFO - __main__ - ['contradiction']
06/01/2022 13:36:19 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/01/2022 13:36:19 - INFO - __main__ - ['contradiction']
06/01/2022 13:36:19 - INFO - __main__ -  [superglue-cb] premise: B: You know, back around, you know, in the twenties and thirties when they were growing up, uh, you know, they were all located together, in one small community. A: Right, right.  Right. B: And I mean when time went on the family grew and moved away and so forth. And now when they come together it's generally, you know, like say the kids of those people who are not, you know, anywhere near one another and I do not think they feel the closeness that they used to be there. Which is a shame [SEP] hypothesis: they feel the closeness that they used to be there
06/01/2022 13:36:19 - INFO - __main__ - ['contradiction']
06/01/2022 13:36:19 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:36:19 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 13:36:19 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 13:36:19 - INFO - __main__ - Printing 3 examples
06/01/2022 13:36:19 - INFO - __main__ -  [superglue-cb] premise: Robert Erwin, president of Biosource, called Plant Genetic's approach ``interesting'' and ``novel,'' and ``complementary rather than competitive.'' ``There is a large market out there hungry for hybrid seeds,'' he said. Mr. Robinson of Delta & Pine, the seed producer in Scott, Miss., said Plant Genetic's success in creating genetically engineered male steriles doesn't automatically mean it would be simple to create hybrids in all crops. [SEP] hypothesis: it would be simple to create hybrids in all crops
06/01/2022 13:36:19 - INFO - __main__ - ['contradiction']
06/01/2022 13:36:19 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/01/2022 13:36:19 - INFO - __main__ - ['contradiction']
06/01/2022 13:36:19 - INFO - __main__ -  [superglue-cb] premise: B: You know, back around, you know, in the twenties and thirties when they were growing up, uh, you know, they were all located together, in one small community. A: Right, right.  Right. B: And I mean when time went on the family grew and moved away and so forth. And now when they come together it's generally, you know, like say the kids of those people who are not, you know, anywhere near one another and I do not think they feel the closeness that they used to be there. Which is a shame [SEP] hypothesis: they feel the closeness that they used to be there
06/01/2022 13:36:19 - INFO - __main__ - ['contradiction']
06/01/2022 13:36:19 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:36:19 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:36:19 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:36:19 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 13:36:19 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 13:36:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 13:36:33 - INFO - __main__ - Starting training!
06/01/2022 13:36:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 13:36:33 - INFO - __main__ - Starting training!
06/01/2022 13:36:38 - INFO - __main__ - Step 10 Global step 10 Train loss 25.422840 on epoch=1
06/01/2022 13:36:43 - INFO - __main__ - Step 20 Global step 20 Train loss 17.850195 on epoch=2
06/01/2022 13:36:47 - INFO - __main__ - Step 30 Global step 30 Train loss 13.289230 on epoch=3
06/01/2022 13:36:53 - INFO - __main__ - Step 40 Global step 40 Train loss 11.013343 on epoch=4
06/01/2022 13:36:58 - INFO - __main__ - Step 50 Global step 50 Train loss 9.488722 on epoch=5
06/01/2022 13:37:00 - INFO - __main__ - Global step 50 Train loss 15.412865 ACC 0.0 on epoch=5
06/01/2022 13:37:06 - INFO - __main__ - Step 60 Global step 60 Train loss 8.751465 on epoch=6
06/01/2022 13:37:11 - INFO - __main__ - Step 70 Global step 70 Train loss 7.243627 on epoch=7
06/01/2022 13:37:16 - INFO - __main__ - Step 80 Global step 80 Train loss 5.641031 on epoch=8
06/01/2022 13:37:21 - INFO - __main__ - Step 90 Global step 90 Train loss 3.887330 on epoch=9
06/01/2022 13:37:27 - INFO - __main__ - Step 100 Global step 100 Train loss 2.437081 on epoch=11
06/01/2022 13:37:29 - INFO - __main__ - Global step 100 Train loss 5.592106 ACC 0.5188679245283019 on epoch=11
06/01/2022 13:37:36 - INFO - __main__ - Step 110 Global step 110 Train loss 2.273098 on epoch=12
06/01/2022 13:37:41 - INFO - __main__ - Step 120 Global step 120 Train loss 2.315467 on epoch=13
06/01/2022 13:37:46 - INFO - __main__ - Step 130 Global step 130 Train loss 2.146385 on epoch=14
06/01/2022 13:37:51 - INFO - __main__ - Step 140 Global step 140 Train loss 2.190294 on epoch=15
06/01/2022 13:37:56 - INFO - __main__ - Step 150 Global step 150 Train loss 2.301056 on epoch=16
06/01/2022 13:37:58 - INFO - __main__ - Global step 150 Train loss 2.245260 ACC 0.5188679245283019 on epoch=16
06/01/2022 13:38:04 - INFO - __main__ - Step 160 Global step 160 Train loss 1.388190 on epoch=17
06/01/2022 13:38:09 - INFO - __main__ - Step 170 Global step 170 Train loss 1.710314 on epoch=18
06/01/2022 13:38:14 - INFO - __main__ - Step 180 Global step 180 Train loss 1.564401 on epoch=19
06/01/2022 13:38:19 - INFO - __main__ - Step 190 Global step 190 Train loss 1.355539 on epoch=21
06/01/2022 13:38:24 - INFO - __main__ - Step 200 Global step 200 Train loss 1.361313 on epoch=22
06/01/2022 13:38:27 - INFO - __main__ - Global step 200 Train loss 1.475952 ACC 0.5188679245283019 on epoch=22
06/01/2022 13:38:32 - INFO - __main__ - Step 210 Global step 210 Train loss 1.367259 on epoch=23
06/01/2022 13:38:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.936467 on epoch=24
06/01/2022 13:38:42 - INFO - __main__ - Step 230 Global step 230 Train loss 1.170484 on epoch=25
06/01/2022 13:38:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.620625 on epoch=26
06/01/2022 13:38:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.840307 on epoch=27
06/01/2022 13:38:55 - INFO - __main__ - Global step 250 Train loss 0.987028 ACC 0.5188679245283019 on epoch=27
06/01/2022 13:39:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.720375 on epoch=28
06/01/2022 13:39:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.802383 on epoch=29
06/01/2022 13:39:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.513600 on epoch=31
06/01/2022 13:39:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.706377 on epoch=32
06/01/2022 13:39:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.575248 on epoch=33
06/01/2022 13:39:23 - INFO - __main__ - Global step 300 Train loss 0.663597 ACC 0.4811320754716981 on epoch=33
06/01/2022 13:39:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.441702 on epoch=34
06/01/2022 13:39:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.569505 on epoch=35
06/01/2022 13:39:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.601064 on epoch=36
06/01/2022 13:39:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.538381 on epoch=37
06/01/2022 13:39:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.574740 on epoch=38
06/01/2022 13:39:51 - INFO - __main__ - Global step 350 Train loss 0.545079 ACC 0.5188679245283019 on epoch=38
06/01/2022 13:39:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.530144 on epoch=39
06/01/2022 13:40:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.519351 on epoch=41
06/01/2022 13:40:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.512815 on epoch=42
06/01/2022 13:40:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.366349 on epoch=43
06/01/2022 13:40:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.433981 on epoch=44
06/01/2022 13:40:20 - INFO - __main__ - Global step 400 Train loss 0.472528 ACC 0.4811320754716981 on epoch=44
06/01/2022 13:40:25 - INFO - __main__ - Step 410 Global step 410 Train loss 1.168966 on epoch=45
06/01/2022 13:40:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.542163 on epoch=46
06/01/2022 13:40:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.559936 on epoch=47
06/01/2022 13:40:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.379296 on epoch=48
06/01/2022 13:40:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.449953 on epoch=49
06/01/2022 13:40:48 - INFO - __main__ - Global step 450 Train loss 0.620063 ACC 0.4811320754716981 on epoch=49
06/01/2022 13:40:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.436786 on epoch=51
06/01/2022 13:40:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.403791 on epoch=52
06/01/2022 13:41:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.380969 on epoch=53
06/01/2022 13:41:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.421771 on epoch=54
06/01/2022 13:41:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.391651 on epoch=55
06/01/2022 13:41:17 - INFO - __main__ - Global step 500 Train loss 0.406994 ACC 0.5188679245283019 on epoch=55
06/01/2022 13:41:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.377316 on epoch=56
06/01/2022 13:41:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.465441 on epoch=57
06/01/2022 13:41:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.438926 on epoch=58
06/01/2022 13:41:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.348121 on epoch=59
06/01/2022 13:41:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.443126 on epoch=61
06/01/2022 13:41:45 - INFO - __main__ - Global step 550 Train loss 0.414586 ACC 0.4811320754716981 on epoch=61
06/01/2022 13:41:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.431775 on epoch=62
06/01/2022 13:41:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.397509 on epoch=63
06/01/2022 13:42:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.351604 on epoch=64
06/01/2022 13:42:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.432023 on epoch=65
06/01/2022 13:42:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.341705 on epoch=66
06/01/2022 13:42:14 - INFO - __main__ - Global step 600 Train loss 0.390923 ACC 0.4528301886792453 on epoch=66
06/01/2022 13:42:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.373822 on epoch=67
06/01/2022 13:42:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.429600 on epoch=68
06/01/2022 13:42:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.380174 on epoch=69
06/01/2022 13:42:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.355215 on epoch=71
06/01/2022 13:42:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.375585 on epoch=72
06/01/2022 13:42:42 - INFO - __main__ - Global step 650 Train loss 0.382879 ACC 0.4811320754716981 on epoch=72
06/01/2022 13:42:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.392243 on epoch=73
06/01/2022 13:42:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.395745 on epoch=74
06/01/2022 13:42:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.400343 on epoch=75
06/01/2022 13:43:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.349670 on epoch=76
06/01/2022 13:43:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.351189 on epoch=77
06/01/2022 13:43:10 - INFO - __main__ - Global step 700 Train loss 0.377838 ACC 0.4528301886792453 on epoch=77
06/01/2022 13:43:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.309905 on epoch=78
06/01/2022 13:43:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.363104 on epoch=79
06/01/2022 13:43:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.344290 on epoch=81
06/01/2022 13:43:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.373878 on epoch=82
06/01/2022 13:43:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.407435 on epoch=83
06/01/2022 13:43:39 - INFO - __main__ - Global step 750 Train loss 0.359722 ACC 0.4811320754716981 on epoch=83
06/01/2022 13:43:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.340218 on epoch=84
06/01/2022 13:43:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.400667 on epoch=85
06/01/2022 13:43:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.377359 on epoch=86
06/01/2022 13:44:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.367007 on epoch=87
06/01/2022 13:44:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.352602 on epoch=88
06/01/2022 13:44:07 - INFO - __main__ - Global step 800 Train loss 0.367571 ACC 0.4811320754716981 on epoch=88
06/01/2022 13:44:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.302268 on epoch=89
06/01/2022 13:44:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.315839 on epoch=91
06/01/2022 13:44:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.339323 on epoch=92
06/01/2022 13:44:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.314464 on epoch=93
06/01/2022 13:44:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.299678 on epoch=94
06/01/2022 13:44:35 - INFO - __main__ - Global step 850 Train loss 0.314314 ACC 0.5283018867924528 on epoch=94
06/01/2022 13:44:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.318939 on epoch=95
06/01/2022 13:44:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.349005 on epoch=96
06/01/2022 13:44:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.307516 on epoch=97
06/01/2022 13:44:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.357424 on epoch=98
06/01/2022 13:45:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.339187 on epoch=99
06/01/2022 13:45:05 - INFO - __main__ - Global step 900 Train loss 0.334414 ACC 0.5471698113207547 on epoch=99
06/01/2022 13:45:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.322938 on epoch=101
06/01/2022 13:45:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.322104 on epoch=102
06/01/2022 13:45:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.323684 on epoch=103
06/01/2022 13:45:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.334031 on epoch=104
06/01/2022 13:45:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.357036 on epoch=105
06/01/2022 13:45:35 - INFO - __main__ - Global step 950 Train loss 0.331959 ACC 0.5188679245283019 on epoch=105
06/01/2022 13:45:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.348727 on epoch=106
06/01/2022 13:45:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.332817 on epoch=107
06/01/2022 13:45:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.328986 on epoch=108
06/01/2022 13:45:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.304410 on epoch=109
06/01/2022 13:46:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.315540 on epoch=111
06/01/2022 13:46:03 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 13:46:03 - INFO - __main__ - Printing 3 examples
06/01/2022 13:46:03 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/01/2022 13:46:03 - INFO - __main__ - ['contradiction']
06/01/2022 13:46:03 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/01/2022 13:46:03 - INFO - __main__ - ['contradiction']
06/01/2022 13:46:03 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/01/2022 13:46:03 - INFO - __main__ - ['contradiction']
06/01/2022 13:46:03 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:46:03 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:46:03 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 13:46:03 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 13:46:03 - INFO - __main__ - Printing 3 examples
06/01/2022 13:46:03 - INFO - __main__ -  [superglue-cb] premise: Robert Erwin, president of Biosource, called Plant Genetic's approach ``interesting'' and ``novel,'' and ``complementary rather than competitive.'' ``There is a large market out there hungry for hybrid seeds,'' he said. Mr. Robinson of Delta & Pine, the seed producer in Scott, Miss., said Plant Genetic's success in creating genetically engineered male steriles doesn't automatically mean it would be simple to create hybrids in all crops. [SEP] hypothesis: it would be simple to create hybrids in all crops
06/01/2022 13:46:03 - INFO - __main__ - ['contradiction']
06/01/2022 13:46:03 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/01/2022 13:46:03 - INFO - __main__ - ['contradiction']
06/01/2022 13:46:03 - INFO - __main__ -  [superglue-cb] premise: B: You know, back around, you know, in the twenties and thirties when they were growing up, uh, you know, they were all located together, in one small community. A: Right, right.  Right. B: And I mean when time went on the family grew and moved away and so forth. And now when they come together it's generally, you know, like say the kids of those people who are not, you know, anywhere near one another and I do not think they feel the closeness that they used to be there. Which is a shame [SEP] hypothesis: they feel the closeness that they used to be there
06/01/2022 13:46:03 - INFO - __main__ - ['contradiction']
06/01/2022 13:46:03 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:46:03 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:46:03 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 13:46:04 - INFO - __main__ - Global step 1000 Train loss 0.326096 ACC 0.4811320754716981 on epoch=111
06/01/2022 13:46:04 - INFO - __main__ - save last model!
06/01/2022 13:46:11 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 13:46:11 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 13:46:11 - INFO - __main__ - Printing 3 examples
06/01/2022 13:46:11 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 13:46:11 - INFO - __main__ - ['contradiction']
06/01/2022 13:46:11 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 13:46:11 - INFO - __main__ - ['neutral']
06/01/2022 13:46:11 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 13:46:11 - INFO - __main__ - ['entailment']
06/01/2022 13:46:11 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:46:11 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:46:12 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 13:46:13 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_100_0.0005_8_predictions.txt
06/01/2022 13:46:13 - INFO - __main__ - ACC on test data: 0.5357
06/01/2022 13:46:13 - INFO - __main__ - prefix=superglue-cb_64_100, lr=0.0005, bsz=8, dev_performance=0.5471698113207547, test_performance=0.5357142857142857
06/01/2022 13:46:13 - INFO - __main__ - Running ... prefix=superglue-cb_64_100, lr=0.0003, bsz=8 ...
06/01/2022 13:46:14 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 13:46:14 - INFO - __main__ - Printing 3 examples
06/01/2022 13:46:14 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/01/2022 13:46:14 - INFO - __main__ - ['contradiction']
06/01/2022 13:46:14 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/01/2022 13:46:14 - INFO - __main__ - ['contradiction']
06/01/2022 13:46:14 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/01/2022 13:46:14 - INFO - __main__ - ['contradiction']
06/01/2022 13:46:14 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:46:14 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:46:14 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 13:46:14 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 13:46:14 - INFO - __main__ - Printing 3 examples
06/01/2022 13:46:14 - INFO - __main__ -  [superglue-cb] premise: Robert Erwin, president of Biosource, called Plant Genetic's approach ``interesting'' and ``novel,'' and ``complementary rather than competitive.'' ``There is a large market out there hungry for hybrid seeds,'' he said. Mr. Robinson of Delta & Pine, the seed producer in Scott, Miss., said Plant Genetic's success in creating genetically engineered male steriles doesn't automatically mean it would be simple to create hybrids in all crops. [SEP] hypothesis: it would be simple to create hybrids in all crops
06/01/2022 13:46:14 - INFO - __main__ - ['contradiction']
06/01/2022 13:46:14 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/01/2022 13:46:14 - INFO - __main__ - ['contradiction']
06/01/2022 13:46:14 - INFO - __main__ -  [superglue-cb] premise: B: You know, back around, you know, in the twenties and thirties when they were growing up, uh, you know, they were all located together, in one small community. A: Right, right.  Right. B: And I mean when time went on the family grew and moved away and so forth. And now when they come together it's generally, you know, like say the kids of those people who are not, you know, anywhere near one another and I do not think they feel the closeness that they used to be there. Which is a shame [SEP] hypothesis: they feel the closeness that they used to be there
06/01/2022 13:46:14 - INFO - __main__ - ['contradiction']
06/01/2022 13:46:14 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:46:15 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:46:15 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 13:46:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 13:46:16 - INFO - __main__ - Starting training!
06/01/2022 13:46:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 13:46:25 - INFO - __main__ - Starting training!
06/01/2022 13:46:30 - INFO - __main__ - Step 10 Global step 10 Train loss 24.574863 on epoch=1
06/01/2022 13:46:35 - INFO - __main__ - Step 20 Global step 20 Train loss 20.193398 on epoch=2
06/01/2022 13:46:40 - INFO - __main__ - Step 30 Global step 30 Train loss 12.887929 on epoch=3
06/01/2022 13:46:45 - INFO - __main__ - Step 40 Global step 40 Train loss 11.719436 on epoch=4
06/01/2022 13:46:51 - INFO - __main__ - Step 50 Global step 50 Train loss 11.306767 on epoch=5
06/01/2022 13:46:55 - INFO - __main__ - Global step 50 Train loss 16.136480 ACC 0.03773584905660377 on epoch=5
06/01/2022 13:47:01 - INFO - __main__ - Step 60 Global step 60 Train loss 10.821802 on epoch=6
06/01/2022 13:47:06 - INFO - __main__ - Step 70 Global step 70 Train loss 9.849698 on epoch=7
06/01/2022 13:47:11 - INFO - __main__ - Step 80 Global step 80 Train loss 8.682264 on epoch=8
06/01/2022 13:47:17 - INFO - __main__ - Step 90 Global step 90 Train loss 7.581539 on epoch=9
06/01/2022 13:47:22 - INFO - __main__ - Step 100 Global step 100 Train loss 7.653442 on epoch=11
06/01/2022 13:47:25 - INFO - __main__ - Global step 100 Train loss 8.917749 ACC 0.03773584905660377 on epoch=11
06/01/2022 13:47:30 - INFO - __main__ - Step 110 Global step 110 Train loss 6.219352 on epoch=12
06/01/2022 13:47:36 - INFO - __main__ - Step 120 Global step 120 Train loss 5.566420 on epoch=13
06/01/2022 13:47:41 - INFO - __main__ - Step 130 Global step 130 Train loss 4.577714 on epoch=14
06/01/2022 13:47:46 - INFO - __main__ - Step 140 Global step 140 Train loss 4.043071 on epoch=15
06/01/2022 13:47:51 - INFO - __main__ - Step 150 Global step 150 Train loss 3.376892 on epoch=16
06/01/2022 13:47:54 - INFO - __main__ - Global step 150 Train loss 4.756690 ACC 0.20754716981132076 on epoch=16
06/01/2022 13:48:01 - INFO - __main__ - Step 160 Global step 160 Train loss 2.514832 on epoch=17
06/01/2022 13:48:07 - INFO - __main__ - Step 170 Global step 170 Train loss 2.666023 on epoch=18
06/01/2022 13:48:12 - INFO - __main__ - Step 180 Global step 180 Train loss 2.307005 on epoch=19
06/01/2022 13:48:17 - INFO - __main__ - Step 190 Global step 190 Train loss 2.573666 on epoch=21
06/01/2022 13:48:22 - INFO - __main__ - Step 200 Global step 200 Train loss 2.408584 on epoch=22
06/01/2022 13:48:25 - INFO - __main__ - Global step 200 Train loss 2.494022 ACC 0.5 on epoch=22
06/01/2022 13:48:31 - INFO - __main__ - Step 210 Global step 210 Train loss 2.066040 on epoch=23
06/01/2022 13:48:37 - INFO - __main__ - Step 220 Global step 220 Train loss 1.528648 on epoch=24
06/01/2022 13:48:42 - INFO - __main__ - Step 230 Global step 230 Train loss 1.820296 on epoch=25
06/01/2022 13:48:47 - INFO - __main__ - Step 240 Global step 240 Train loss 1.885932 on epoch=26
06/01/2022 13:48:52 - INFO - __main__ - Step 250 Global step 250 Train loss 1.631947 on epoch=27
06/01/2022 13:48:54 - INFO - __main__ - Global step 250 Train loss 1.786573 ACC 0.5188679245283019 on epoch=27
06/01/2022 13:49:01 - INFO - __main__ - Step 260 Global step 260 Train loss 2.218071 on epoch=28
06/01/2022 13:49:06 - INFO - __main__ - Step 270 Global step 270 Train loss 1.423756 on epoch=29
06/01/2022 13:49:12 - INFO - __main__ - Step 280 Global step 280 Train loss 1.627314 on epoch=31
06/01/2022 13:49:17 - INFO - __main__ - Step 290 Global step 290 Train loss 1.162180 on epoch=32
06/01/2022 13:49:22 - INFO - __main__ - Step 300 Global step 300 Train loss 1.318909 on epoch=33
06/01/2022 13:49:24 - INFO - __main__ - Global step 300 Train loss 1.550046 ACC 0.5188679245283019 on epoch=33
06/01/2022 13:49:30 - INFO - __main__ - Step 310 Global step 310 Train loss 1.328621 on epoch=34
06/01/2022 13:49:35 - INFO - __main__ - Step 320 Global step 320 Train loss 1.351040 on epoch=35
06/01/2022 13:49:40 - INFO - __main__ - Step 330 Global step 330 Train loss 1.385412 on epoch=36
06/01/2022 13:49:45 - INFO - __main__ - Step 340 Global step 340 Train loss 1.372651 on epoch=37
06/01/2022 13:49:51 - INFO - __main__ - Step 350 Global step 350 Train loss 1.265409 on epoch=38
06/01/2022 13:49:53 - INFO - __main__ - Global step 350 Train loss 1.340626 ACC 0.5188679245283019 on epoch=38
06/01/2022 13:49:58 - INFO - __main__ - Step 360 Global step 360 Train loss 1.097526 on epoch=39
06/01/2022 13:50:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.964407 on epoch=41
06/01/2022 13:50:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.854463 on epoch=42
06/01/2022 13:50:14 - INFO - __main__ - Step 390 Global step 390 Train loss 1.358284 on epoch=43
06/01/2022 13:50:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.967473 on epoch=44
06/01/2022 13:50:21 - INFO - __main__ - Global step 400 Train loss 1.048431 ACC 0.5188679245283019 on epoch=44
06/01/2022 13:50:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.876754 on epoch=45
06/01/2022 13:50:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.840471 on epoch=46
06/01/2022 13:50:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.838226 on epoch=47
06/01/2022 13:50:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.942499 on epoch=48
06/01/2022 13:50:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.876719 on epoch=49
06/01/2022 13:50:50 - INFO - __main__ - Global step 450 Train loss 0.874934 ACC 0.5188679245283019 on epoch=49
06/01/2022 13:50:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.709615 on epoch=51
06/01/2022 13:51:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.667871 on epoch=52
06/01/2022 13:51:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.786988 on epoch=53
06/01/2022 13:51:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.667423 on epoch=54
06/01/2022 13:51:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.678254 on epoch=55
06/01/2022 13:51:18 - INFO - __main__ - Global step 500 Train loss 0.702030 ACC 0.5188679245283019 on epoch=55
06/01/2022 13:51:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.682688 on epoch=56
06/01/2022 13:51:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.777672 on epoch=57
06/01/2022 13:51:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.578846 on epoch=58
06/01/2022 13:51:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.765818 on epoch=59
06/01/2022 13:51:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.630738 on epoch=61
06/01/2022 13:51:47 - INFO - __main__ - Global step 550 Train loss 0.687152 ACC 0.5566037735849056 on epoch=61
06/01/2022 13:51:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.640911 on epoch=62
06/01/2022 13:51:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.519279 on epoch=63
06/01/2022 13:52:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.550447 on epoch=64
06/01/2022 13:52:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.513155 on epoch=65
06/01/2022 13:52:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.486177 on epoch=66
06/01/2022 13:52:16 - INFO - __main__ - Global step 600 Train loss 0.541994 ACC 0.5943396226415094 on epoch=66
06/01/2022 13:52:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.482141 on epoch=67
06/01/2022 13:52:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.645830 on epoch=68
06/01/2022 13:52:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.517806 on epoch=69
06/01/2022 13:52:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.509687 on epoch=71
06/01/2022 13:52:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.470707 on epoch=72
06/01/2022 13:52:46 - INFO - __main__ - Global step 650 Train loss 0.525234 ACC 0.4811320754716981 on epoch=72
06/01/2022 13:52:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.534245 on epoch=73
06/01/2022 13:52:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.409679 on epoch=74
06/01/2022 13:53:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.501902 on epoch=75
06/01/2022 13:53:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.625034 on epoch=76
06/01/2022 13:53:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.497830 on epoch=77
06/01/2022 13:53:15 - INFO - __main__ - Global step 700 Train loss 0.513738 ACC 0.6320754716981132 on epoch=77
06/01/2022 13:53:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.507084 on epoch=78
06/01/2022 13:53:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.425591 on epoch=79
06/01/2022 13:53:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.383938 on epoch=81
06/01/2022 13:53:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.598027 on epoch=82
06/01/2022 13:53:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.401090 on epoch=83
06/01/2022 13:53:44 - INFO - __main__ - Global step 750 Train loss 0.463146 ACC 0.49056603773584906 on epoch=83
06/01/2022 13:53:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.468820 on epoch=84
06/01/2022 13:53:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.528639 on epoch=85
06/01/2022 13:53:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.529560 on epoch=86
06/01/2022 13:54:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.474279 on epoch=87
06/01/2022 13:54:10 - INFO - __main__ - Step 800 Global step 800 Train loss 0.371671 on epoch=88
06/01/2022 13:54:12 - INFO - __main__ - Global step 800 Train loss 0.474594 ACC 0.4811320754716981 on epoch=88
06/01/2022 13:54:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.443996 on epoch=89
06/01/2022 13:54:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.411113 on epoch=91
06/01/2022 13:54:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.422892 on epoch=92
06/01/2022 13:54:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.395950 on epoch=93
06/01/2022 13:54:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.425388 on epoch=94
06/01/2022 13:54:41 - INFO - __main__ - Global step 850 Train loss 0.419868 ACC 0.4811320754716981 on epoch=94
06/01/2022 13:54:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.369883 on epoch=95
06/01/2022 13:54:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.438450 on epoch=96
06/01/2022 13:54:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.400311 on epoch=97
06/01/2022 13:55:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.461601 on epoch=98
06/01/2022 13:55:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.368422 on epoch=99
06/01/2022 13:55:10 - INFO - __main__ - Global step 900 Train loss 0.407733 ACC 0.5 on epoch=99
06/01/2022 13:55:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.439583 on epoch=101
06/01/2022 13:55:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.479893 on epoch=102
06/01/2022 13:55:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.356776 on epoch=103
06/01/2022 13:55:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.443796 on epoch=104
06/01/2022 13:55:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.423172 on epoch=105
06/01/2022 13:55:38 - INFO - __main__ - Global step 950 Train loss 0.428644 ACC 0.49056603773584906 on epoch=105
06/01/2022 13:55:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.433966 on epoch=106
06/01/2022 13:55:49 - INFO - __main__ - Step 970 Global step 970 Train loss 0.479203 on epoch=107
06/01/2022 13:55:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.387656 on epoch=108
06/01/2022 13:55:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.415899 on epoch=109
06/01/2022 13:56:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.399336 on epoch=111
06/01/2022 13:56:05 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 13:56:05 - INFO - __main__ - Printing 3 examples
06/01/2022 13:56:05 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/01/2022 13:56:05 - INFO - __main__ - ['contradiction']
06/01/2022 13:56:05 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/01/2022 13:56:05 - INFO - __main__ - ['contradiction']
06/01/2022 13:56:05 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/01/2022 13:56:05 - INFO - __main__ - ['contradiction']
06/01/2022 13:56:05 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:56:06 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:56:06 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 13:56:06 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 13:56:06 - INFO - __main__ - Printing 3 examples
06/01/2022 13:56:06 - INFO - __main__ -  [superglue-cb] premise: Robert Erwin, president of Biosource, called Plant Genetic's approach ``interesting'' and ``novel,'' and ``complementary rather than competitive.'' ``There is a large market out there hungry for hybrid seeds,'' he said. Mr. Robinson of Delta & Pine, the seed producer in Scott, Miss., said Plant Genetic's success in creating genetically engineered male steriles doesn't automatically mean it would be simple to create hybrids in all crops. [SEP] hypothesis: it would be simple to create hybrids in all crops
06/01/2022 13:56:06 - INFO - __main__ - ['contradiction']
06/01/2022 13:56:06 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/01/2022 13:56:06 - INFO - __main__ - ['contradiction']
06/01/2022 13:56:06 - INFO - __main__ -  [superglue-cb] premise: B: You know, back around, you know, in the twenties and thirties when they were growing up, uh, you know, they were all located together, in one small community. A: Right, right.  Right. B: And I mean when time went on the family grew and moved away and so forth. And now when they come together it's generally, you know, like say the kids of those people who are not, you know, anywhere near one another and I do not think they feel the closeness that they used to be there. Which is a shame [SEP] hypothesis: they feel the closeness that they used to be there
06/01/2022 13:56:06 - INFO - __main__ - ['contradiction']
06/01/2022 13:56:06 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:56:06 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:56:06 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 13:56:07 - INFO - __main__ - Global step 1000 Train loss 0.423212 ACC 0.4811320754716981 on epoch=111
06/01/2022 13:56:07 - INFO - __main__ - save last model!
06/01/2022 13:56:13 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 13:56:14 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 13:56:14 - INFO - __main__ - Printing 3 examples
06/01/2022 13:56:14 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 13:56:14 - INFO - __main__ - ['contradiction']
06/01/2022 13:56:14 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 13:56:14 - INFO - __main__ - ['neutral']
06/01/2022 13:56:14 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 13:56:14 - INFO - __main__ - ['entailment']
06/01/2022 13:56:14 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:56:14 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:56:14 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 13:56:16 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_100_0.0003_8_predictions.txt
06/01/2022 13:56:16 - INFO - __main__ - ACC on test data: 0.5357
06/01/2022 13:56:16 - INFO - __main__ - prefix=superglue-cb_64_100, lr=0.0003, bsz=8, dev_performance=0.6320754716981132, test_performance=0.5357142857142857
06/01/2022 13:56:16 - INFO - __main__ - Running ... prefix=superglue-cb_64_100, lr=0.0002, bsz=8 ...
06/01/2022 13:56:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 13:56:17 - INFO - __main__ - Starting training!
06/01/2022 13:56:17 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 13:56:17 - INFO - __main__ - Printing 3 examples
06/01/2022 13:56:17 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/01/2022 13:56:17 - INFO - __main__ - ['contradiction']
06/01/2022 13:56:17 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/01/2022 13:56:17 - INFO - __main__ - ['contradiction']
06/01/2022 13:56:17 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/01/2022 13:56:17 - INFO - __main__ - ['contradiction']
06/01/2022 13:56:17 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:56:17 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:56:17 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 13:56:17 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 13:56:17 - INFO - __main__ - Printing 3 examples
06/01/2022 13:56:17 - INFO - __main__ -  [superglue-cb] premise: Robert Erwin, president of Biosource, called Plant Genetic's approach ``interesting'' and ``novel,'' and ``complementary rather than competitive.'' ``There is a large market out there hungry for hybrid seeds,'' he said. Mr. Robinson of Delta & Pine, the seed producer in Scott, Miss., said Plant Genetic's success in creating genetically engineered male steriles doesn't automatically mean it would be simple to create hybrids in all crops. [SEP] hypothesis: it would be simple to create hybrids in all crops
06/01/2022 13:56:17 - INFO - __main__ - ['contradiction']
06/01/2022 13:56:17 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/01/2022 13:56:17 - INFO - __main__ - ['contradiction']
06/01/2022 13:56:17 - INFO - __main__ -  [superglue-cb] premise: B: You know, back around, you know, in the twenties and thirties when they were growing up, uh, you know, they were all located together, in one small community. A: Right, right.  Right. B: And I mean when time went on the family grew and moved away and so forth. And now when they come together it's generally, you know, like say the kids of those people who are not, you know, anywhere near one another and I do not think they feel the closeness that they used to be there. Which is a shame [SEP] hypothesis: they feel the closeness that they used to be there
06/01/2022 13:56:17 - INFO - __main__ - ['contradiction']
06/01/2022 13:56:17 - INFO - __main__ - Tokenizing Input ...
06/01/2022 13:56:17 - INFO - __main__ - Tokenizing Output ...
06/01/2022 13:56:17 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 13:56:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 13:56:28 - INFO - __main__ - Starting training!
06/01/2022 13:56:32 - INFO - __main__ - Step 10 Global step 10 Train loss 24.908239 on epoch=1
06/01/2022 13:56:37 - INFO - __main__ - Step 20 Global step 20 Train loss 20.076149 on epoch=2
06/01/2022 13:56:42 - INFO - __main__ - Step 30 Global step 30 Train loss 13.933882 on epoch=3
06/01/2022 13:56:47 - INFO - __main__ - Step 40 Global step 40 Train loss 11.732121 on epoch=4
06/01/2022 13:56:53 - INFO - __main__ - Step 50 Global step 50 Train loss 10.849423 on epoch=5
06/01/2022 13:56:55 - INFO - __main__ - Global step 50 Train loss 16.299965 ACC 0.009433962264150943 on epoch=5
06/01/2022 13:57:01 - INFO - __main__ - Step 60 Global step 60 Train loss 10.636662 on epoch=6
06/01/2022 13:57:07 - INFO - __main__ - Step 70 Global step 70 Train loss 10.409466 on epoch=7
06/01/2022 13:57:12 - INFO - __main__ - Step 80 Global step 80 Train loss 9.788554 on epoch=8
06/01/2022 13:57:17 - INFO - __main__ - Step 90 Global step 90 Train loss 8.823797 on epoch=9
06/01/2022 13:57:22 - INFO - __main__ - Step 100 Global step 100 Train loss 8.587243 on epoch=11
06/01/2022 13:57:25 - INFO - __main__ - Global step 100 Train loss 9.649145 ACC 0.2358490566037736 on epoch=11
06/01/2022 13:57:31 - INFO - __main__ - Step 110 Global step 110 Train loss 8.350627 on epoch=12
06/01/2022 13:57:36 - INFO - __main__ - Step 120 Global step 120 Train loss 8.001629 on epoch=13
06/01/2022 13:57:41 - INFO - __main__ - Step 130 Global step 130 Train loss 7.512231 on epoch=14
06/01/2022 13:57:46 - INFO - __main__ - Step 140 Global step 140 Train loss 7.019289 on epoch=15
06/01/2022 13:57:51 - INFO - __main__ - Step 150 Global step 150 Train loss 6.945673 on epoch=16
06/01/2022 13:57:54 - INFO - __main__ - Global step 150 Train loss 7.565890 ACC 0.11320754716981132 on epoch=16
06/01/2022 13:57:59 - INFO - __main__ - Step 160 Global step 160 Train loss 6.192736 on epoch=17
06/01/2022 13:58:04 - INFO - __main__ - Step 170 Global step 170 Train loss 5.814995 on epoch=18
06/01/2022 13:58:09 - INFO - __main__ - Step 180 Global step 180 Train loss 5.000687 on epoch=19
06/01/2022 13:58:15 - INFO - __main__ - Step 190 Global step 190 Train loss 3.818959 on epoch=21
06/01/2022 13:58:20 - INFO - __main__ - Step 200 Global step 200 Train loss 4.010905 on epoch=22
06/01/2022 13:58:22 - INFO - __main__ - Global step 200 Train loss 4.967657 ACC 0.16981132075471697 on epoch=22
06/01/2022 13:58:27 - INFO - __main__ - Step 210 Global step 210 Train loss 2.815414 on epoch=23
06/01/2022 13:58:32 - INFO - __main__ - Step 220 Global step 220 Train loss 2.560117 on epoch=24
06/01/2022 13:58:37 - INFO - __main__ - Step 230 Global step 230 Train loss 2.372334 on epoch=25
06/01/2022 13:58:43 - INFO - __main__ - Step 240 Global step 240 Train loss 2.790273 on epoch=26
06/01/2022 13:58:48 - INFO - __main__ - Step 250 Global step 250 Train loss 2.158360 on epoch=27
06/01/2022 13:58:49 - INFO - __main__ - Global step 250 Train loss 2.539300 ACC 0.5188679245283019 on epoch=27
06/01/2022 13:58:56 - INFO - __main__ - Step 260 Global step 260 Train loss 2.244305 on epoch=28
06/01/2022 13:59:01 - INFO - __main__ - Step 270 Global step 270 Train loss 3.041878 on epoch=29
06/01/2022 13:59:06 - INFO - __main__ - Step 280 Global step 280 Train loss 2.410092 on epoch=31
06/01/2022 13:59:12 - INFO - __main__ - Step 290 Global step 290 Train loss 1.544777 on epoch=32
06/01/2022 13:59:17 - INFO - __main__ - Step 300 Global step 300 Train loss 2.188067 on epoch=33
06/01/2022 13:59:19 - INFO - __main__ - Global step 300 Train loss 2.285824 ACC 0.5188679245283019 on epoch=33
06/01/2022 13:59:24 - INFO - __main__ - Step 310 Global step 310 Train loss 1.774579 on epoch=34
06/01/2022 13:59:29 - INFO - __main__ - Step 320 Global step 320 Train loss 1.923066 on epoch=35
06/01/2022 13:59:34 - INFO - __main__ - Step 330 Global step 330 Train loss 2.336689 on epoch=36
06/01/2022 13:59:40 - INFO - __main__ - Step 340 Global step 340 Train loss 1.612411 on epoch=37
06/01/2022 13:59:45 - INFO - __main__ - Step 350 Global step 350 Train loss 1.569142 on epoch=38
06/01/2022 13:59:47 - INFO - __main__ - Global step 350 Train loss 1.843177 ACC 0.5188679245283019 on epoch=38
06/01/2022 13:59:52 - INFO - __main__ - Step 360 Global step 360 Train loss 1.613493 on epoch=39
06/01/2022 13:59:57 - INFO - __main__ - Step 370 Global step 370 Train loss 2.054571 on epoch=41
06/01/2022 14:00:02 - INFO - __main__ - Step 380 Global step 380 Train loss 1.779689 on epoch=42
06/01/2022 14:00:07 - INFO - __main__ - Step 390 Global step 390 Train loss 1.707564 on epoch=43
06/01/2022 14:00:13 - INFO - __main__ - Step 400 Global step 400 Train loss 1.807500 on epoch=44
06/01/2022 14:00:14 - INFO - __main__ - Global step 400 Train loss 1.792563 ACC 0.5188679245283019 on epoch=44
06/01/2022 14:00:20 - INFO - __main__ - Step 410 Global step 410 Train loss 1.923146 on epoch=45
06/01/2022 14:00:25 - INFO - __main__ - Step 420 Global step 420 Train loss 1.823619 on epoch=46
06/01/2022 14:00:30 - INFO - __main__ - Step 430 Global step 430 Train loss 1.880357 on epoch=47
06/01/2022 14:00:35 - INFO - __main__ - Step 440 Global step 440 Train loss 1.338393 on epoch=48
06/01/2022 14:00:40 - INFO - __main__ - Step 450 Global step 450 Train loss 1.428719 on epoch=49
06/01/2022 14:00:42 - INFO - __main__ - Global step 450 Train loss 1.678847 ACC 0.5188679245283019 on epoch=49
06/01/2022 14:00:47 - INFO - __main__ - Step 460 Global step 460 Train loss 1.171036 on epoch=51
06/01/2022 14:00:52 - INFO - __main__ - Step 470 Global step 470 Train loss 1.552698 on epoch=52
06/01/2022 14:00:57 - INFO - __main__ - Step 480 Global step 480 Train loss 1.443526 on epoch=53
06/01/2022 14:01:03 - INFO - __main__ - Step 490 Global step 490 Train loss 1.225767 on epoch=54
06/01/2022 14:01:08 - INFO - __main__ - Step 500 Global step 500 Train loss 1.000900 on epoch=55
06/01/2022 14:01:10 - INFO - __main__ - Global step 500 Train loss 1.278785 ACC 0.5188679245283019 on epoch=55
06/01/2022 14:01:15 - INFO - __main__ - Step 510 Global step 510 Train loss 1.224032 on epoch=56
06/01/2022 14:01:20 - INFO - __main__ - Step 520 Global step 520 Train loss 1.334141 on epoch=57
06/01/2022 14:01:25 - INFO - __main__ - Step 530 Global step 530 Train loss 1.053455 on epoch=58
06/01/2022 14:01:30 - INFO - __main__ - Step 540 Global step 540 Train loss 1.179994 on epoch=59
06/01/2022 14:01:36 - INFO - __main__ - Step 550 Global step 550 Train loss 1.256664 on epoch=61
06/01/2022 14:01:38 - INFO - __main__ - Global step 550 Train loss 1.209657 ACC 0.5188679245283019 on epoch=61
06/01/2022 14:01:43 - INFO - __main__ - Step 560 Global step 560 Train loss 1.011760 on epoch=62
06/01/2022 14:01:48 - INFO - __main__ - Step 570 Global step 570 Train loss 1.229392 on epoch=63
06/01/2022 14:01:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.905001 on epoch=64
06/01/2022 14:01:58 - INFO - __main__ - Step 590 Global step 590 Train loss 1.071552 on epoch=65
06/01/2022 14:02:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.782971 on epoch=66
06/01/2022 14:02:06 - INFO - __main__ - Global step 600 Train loss 1.000135 ACC 0.5188679245283019 on epoch=66
06/01/2022 14:02:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.919925 on epoch=67
06/01/2022 14:02:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.910821 on epoch=68
06/01/2022 14:02:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.812467 on epoch=69
06/01/2022 14:02:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.713155 on epoch=71
06/01/2022 14:02:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.741483 on epoch=72
06/01/2022 14:02:34 - INFO - __main__ - Global step 650 Train loss 0.819570 ACC 0.5188679245283019 on epoch=72
06/01/2022 14:02:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.674954 on epoch=73
06/01/2022 14:02:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.776445 on epoch=74
06/01/2022 14:02:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.661748 on epoch=75
06/01/2022 14:02:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.621906 on epoch=76
06/01/2022 14:03:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.780485 on epoch=77
06/01/2022 14:03:02 - INFO - __main__ - Global step 700 Train loss 0.703108 ACC 0.7264150943396226 on epoch=77
06/01/2022 14:03:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.609991 on epoch=78
06/01/2022 14:03:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.663281 on epoch=79
06/01/2022 14:03:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.521807 on epoch=81
06/01/2022 14:03:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.558437 on epoch=82
06/01/2022 14:03:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.606686 on epoch=83
06/01/2022 14:03:31 - INFO - __main__ - Global step 750 Train loss 0.592040 ACC 0.6981132075471698 on epoch=83
06/01/2022 14:03:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.588410 on epoch=84
06/01/2022 14:03:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.729653 on epoch=85
06/01/2022 14:03:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.591402 on epoch=86
06/01/2022 14:03:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.697563 on epoch=87
06/01/2022 14:03:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.595369 on epoch=88
06/01/2022 14:04:00 - INFO - __main__ - Global step 800 Train loss 0.640480 ACC 0.7735849056603774 on epoch=88
06/01/2022 14:04:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.482128 on epoch=89
06/01/2022 14:04:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.624596 on epoch=91
06/01/2022 14:04:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.700937 on epoch=92
06/01/2022 14:04:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.458100 on epoch=93
06/01/2022 14:04:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.429446 on epoch=94
06/01/2022 14:04:29 - INFO - __main__ - Global step 850 Train loss 0.539041 ACC 0.5377358490566038 on epoch=94
06/01/2022 14:04:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.621337 on epoch=95
06/01/2022 14:04:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.577788 on epoch=96
06/01/2022 14:04:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.481425 on epoch=97
06/01/2022 14:04:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.526832 on epoch=98
06/01/2022 14:04:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.448166 on epoch=99
06/01/2022 14:04:56 - INFO - __main__ - Global step 900 Train loss 0.531110 ACC 0.5283018867924528 on epoch=99
06/01/2022 14:05:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.440519 on epoch=101
06/01/2022 14:05:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.465130 on epoch=102
06/01/2022 14:05:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.475580 on epoch=103
06/01/2022 14:05:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.408884 on epoch=104
06/01/2022 14:05:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.404206 on epoch=105
06/01/2022 14:05:25 - INFO - __main__ - Global step 950 Train loss 0.438864 ACC 0.6132075471698113 on epoch=105
06/01/2022 14:05:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.439147 on epoch=106
06/01/2022 14:05:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.490930 on epoch=107
06/01/2022 14:05:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.422702 on epoch=108
06/01/2022 14:05:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.444805 on epoch=109
06/01/2022 14:05:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.438537 on epoch=111
06/01/2022 14:05:52 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 14:05:52 - INFO - __main__ - Printing 3 examples
06/01/2022 14:05:52 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/01/2022 14:05:52 - INFO - __main__ - ['contradiction']
06/01/2022 14:05:52 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/01/2022 14:05:52 - INFO - __main__ - ['contradiction']
06/01/2022 14:05:52 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/01/2022 14:05:52 - INFO - __main__ - ['contradiction']
06/01/2022 14:05:52 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:05:52 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:05:53 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 14:05:53 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 14:05:53 - INFO - __main__ - Printing 3 examples
06/01/2022 14:05:53 - INFO - __main__ -  [superglue-cb] premise: Robert Erwin, president of Biosource, called Plant Genetic's approach ``interesting'' and ``novel,'' and ``complementary rather than competitive.'' ``There is a large market out there hungry for hybrid seeds,'' he said. Mr. Robinson of Delta & Pine, the seed producer in Scott, Miss., said Plant Genetic's success in creating genetically engineered male steriles doesn't automatically mean it would be simple to create hybrids in all crops. [SEP] hypothesis: it would be simple to create hybrids in all crops
06/01/2022 14:05:53 - INFO - __main__ - ['contradiction']
06/01/2022 14:05:53 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/01/2022 14:05:53 - INFO - __main__ - ['contradiction']
06/01/2022 14:05:53 - INFO - __main__ -  [superglue-cb] premise: B: You know, back around, you know, in the twenties and thirties when they were growing up, uh, you know, they were all located together, in one small community. A: Right, right.  Right. B: And I mean when time went on the family grew and moved away and so forth. And now when they come together it's generally, you know, like say the kids of those people who are not, you know, anywhere near one another and I do not think they feel the closeness that they used to be there. Which is a shame [SEP] hypothesis: they feel the closeness that they used to be there
06/01/2022 14:05:53 - INFO - __main__ - ['contradiction']
06/01/2022 14:05:53 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:05:53 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:05:53 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 14:05:53 - INFO - __main__ - Global step 1000 Train loss 0.447224 ACC 0.660377358490566 on epoch=111
06/01/2022 14:05:53 - INFO - __main__ - save last model!
06/01/2022 14:06:00 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 14:06:01 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 14:06:01 - INFO - __main__ - Printing 3 examples
06/01/2022 14:06:01 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 14:06:01 - INFO - __main__ - ['contradiction']
06/01/2022 14:06:01 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 14:06:01 - INFO - __main__ - ['neutral']
06/01/2022 14:06:01 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 14:06:01 - INFO - __main__ - ['entailment']
06/01/2022 14:06:01 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:06:01 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:06:01 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 14:06:03 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_100_0.0002_8_predictions.txt
06/01/2022 14:06:03 - INFO - __main__ - ACC on test data: 0.6607
06/01/2022 14:06:03 - INFO - __main__ - prefix=superglue-cb_64_100, lr=0.0002, bsz=8, dev_performance=0.7735849056603774, test_performance=0.6607142857142857
06/01/2022 14:06:03 - INFO - __main__ - Running ... prefix=superglue-cb_64_100, lr=0.0001, bsz=8 ...
06/01/2022 14:06:04 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 14:06:04 - INFO - __main__ - Printing 3 examples
06/01/2022 14:06:04 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/01/2022 14:06:04 - INFO - __main__ - ['contradiction']
06/01/2022 14:06:04 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/01/2022 14:06:04 - INFO - __main__ - ['contradiction']
06/01/2022 14:06:04 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/01/2022 14:06:04 - INFO - __main__ - ['contradiction']
06/01/2022 14:06:04 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:06:04 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:06:04 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 14:06:04 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 14:06:04 - INFO - __main__ - Printing 3 examples
06/01/2022 14:06:04 - INFO - __main__ -  [superglue-cb] premise: Robert Erwin, president of Biosource, called Plant Genetic's approach ``interesting'' and ``novel,'' and ``complementary rather than competitive.'' ``There is a large market out there hungry for hybrid seeds,'' he said. Mr. Robinson of Delta & Pine, the seed producer in Scott, Miss., said Plant Genetic's success in creating genetically engineered male steriles doesn't automatically mean it would be simple to create hybrids in all crops. [SEP] hypothesis: it would be simple to create hybrids in all crops
06/01/2022 14:06:04 - INFO - __main__ - ['contradiction']
06/01/2022 14:06:04 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/01/2022 14:06:04 - INFO - __main__ - ['contradiction']
06/01/2022 14:06:04 - INFO - __main__ -  [superglue-cb] premise: B: You know, back around, you know, in the twenties and thirties when they were growing up, uh, you know, they were all located together, in one small community. A: Right, right.  Right. B: And I mean when time went on the family grew and moved away and so forth. And now when they come together it's generally, you know, like say the kids of those people who are not, you know, anywhere near one another and I do not think they feel the closeness that they used to be there. Which is a shame [SEP] hypothesis: they feel the closeness that they used to be there
06/01/2022 14:06:04 - INFO - __main__ - ['contradiction']
06/01/2022 14:06:04 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:06:04 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:06:04 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 14:06:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 14:06:06 - INFO - __main__ - Starting training!
06/01/2022 14:06:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 14:06:15 - INFO - __main__ - Starting training!
06/01/2022 14:06:19 - INFO - __main__ - Step 10 Global step 10 Train loss 24.852566 on epoch=1
06/01/2022 14:06:24 - INFO - __main__ - Step 20 Global step 20 Train loss 19.269287 on epoch=2
06/01/2022 14:06:30 - INFO - __main__ - Step 30 Global step 30 Train loss 13.443967 on epoch=3
06/01/2022 14:06:35 - INFO - __main__ - Step 40 Global step 40 Train loss 12.343916 on epoch=4
06/01/2022 14:06:40 - INFO - __main__ - Step 50 Global step 50 Train loss 11.629273 on epoch=5
06/01/2022 14:06:46 - INFO - __main__ - Global step 50 Train loss 16.307802 ACC 0.03773584905660377 on epoch=5
06/01/2022 14:06:52 - INFO - __main__ - Step 60 Global step 60 Train loss 11.394584 on epoch=6
06/01/2022 14:06:57 - INFO - __main__ - Step 70 Global step 70 Train loss 10.691201 on epoch=7
06/01/2022 14:07:02 - INFO - __main__ - Step 80 Global step 80 Train loss 10.983107 on epoch=8
06/01/2022 14:07:07 - INFO - __main__ - Step 90 Global step 90 Train loss 10.565728 on epoch=9
06/01/2022 14:07:13 - INFO - __main__ - Step 100 Global step 100 Train loss 10.272367 on epoch=11
06/01/2022 14:07:16 - INFO - __main__ - Global step 100 Train loss 10.781398 ACC 0.009433962264150943 on epoch=11
06/01/2022 14:07:21 - INFO - __main__ - Step 110 Global step 110 Train loss 9.821960 on epoch=12
06/01/2022 14:07:27 - INFO - __main__ - Step 120 Global step 120 Train loss 9.425953 on epoch=13
06/01/2022 14:07:32 - INFO - __main__ - Step 130 Global step 130 Train loss 9.800320 on epoch=14
06/01/2022 14:07:37 - INFO - __main__ - Step 140 Global step 140 Train loss 9.309976 on epoch=15
06/01/2022 14:07:42 - INFO - __main__ - Step 150 Global step 150 Train loss 9.724513 on epoch=16
06/01/2022 14:07:45 - INFO - __main__ - Global step 150 Train loss 9.616544 ACC 0.0 on epoch=16
06/01/2022 14:07:50 - INFO - __main__ - Step 160 Global step 160 Train loss 9.212301 on epoch=17
06/01/2022 14:07:55 - INFO - __main__ - Step 170 Global step 170 Train loss 9.094344 on epoch=18
06/01/2022 14:08:01 - INFO - __main__ - Step 180 Global step 180 Train loss 8.896967 on epoch=19
06/01/2022 14:08:06 - INFO - __main__ - Step 190 Global step 190 Train loss 8.399254 on epoch=21
06/01/2022 14:08:11 - INFO - __main__ - Step 200 Global step 200 Train loss 7.852726 on epoch=22
06/01/2022 14:08:14 - INFO - __main__ - Global step 200 Train loss 8.691118 ACC 0.0 on epoch=22
06/01/2022 14:08:19 - INFO - __main__ - Step 210 Global step 210 Train loss 7.627535 on epoch=23
06/01/2022 14:08:24 - INFO - __main__ - Step 220 Global step 220 Train loss 7.533795 on epoch=24
06/01/2022 14:08:29 - INFO - __main__ - Step 230 Global step 230 Train loss 7.428990 on epoch=25
06/01/2022 14:08:35 - INFO - __main__ - Step 240 Global step 240 Train loss 7.305643 on epoch=26
06/01/2022 14:08:40 - INFO - __main__ - Step 250 Global step 250 Train loss 6.768411 on epoch=27
06/01/2022 14:08:42 - INFO - __main__ - Global step 250 Train loss 7.332875 ACC 0.0 on epoch=27
06/01/2022 14:08:47 - INFO - __main__ - Step 260 Global step 260 Train loss 6.443908 on epoch=28
06/01/2022 14:08:53 - INFO - __main__ - Step 270 Global step 270 Train loss 6.085194 on epoch=29
06/01/2022 14:08:58 - INFO - __main__ - Step 280 Global step 280 Train loss 6.083136 on epoch=31
06/01/2022 14:09:03 - INFO - __main__ - Step 290 Global step 290 Train loss 4.969533 on epoch=32
06/01/2022 14:09:08 - INFO - __main__ - Step 300 Global step 300 Train loss 4.741424 on epoch=33
06/01/2022 14:09:11 - INFO - __main__ - Global step 300 Train loss 5.664639 ACC 0.0 on epoch=33
06/01/2022 14:09:16 - INFO - __main__ - Step 310 Global step 310 Train loss 4.376847 on epoch=34
06/01/2022 14:09:21 - INFO - __main__ - Step 320 Global step 320 Train loss 4.439357 on epoch=35
06/01/2022 14:09:26 - INFO - __main__ - Step 330 Global step 330 Train loss 4.961952 on epoch=36
06/01/2022 14:09:31 - INFO - __main__ - Step 340 Global step 340 Train loss 4.498167 on epoch=37
06/01/2022 14:09:36 - INFO - __main__ - Step 350 Global step 350 Train loss 3.979872 on epoch=38
06/01/2022 14:09:39 - INFO - __main__ - Global step 350 Train loss 4.451239 ACC 0.42452830188679247 on epoch=38
06/01/2022 14:09:44 - INFO - __main__ - Step 360 Global step 360 Train loss 3.630400 on epoch=39
06/01/2022 14:09:50 - INFO - __main__ - Step 370 Global step 370 Train loss 3.721025 on epoch=41
06/01/2022 14:09:55 - INFO - __main__ - Step 380 Global step 380 Train loss 2.899758 on epoch=42
06/01/2022 14:10:00 - INFO - __main__ - Step 390 Global step 390 Train loss 3.014766 on epoch=43
06/01/2022 14:10:05 - INFO - __main__ - Step 400 Global step 400 Train loss 2.936290 on epoch=44
06/01/2022 14:10:08 - INFO - __main__ - Global step 400 Train loss 3.240448 ACC 0.5188679245283019 on epoch=44
06/01/2022 14:10:14 - INFO - __main__ - Step 410 Global step 410 Train loss 3.199924 on epoch=45
06/01/2022 14:10:19 - INFO - __main__ - Step 420 Global step 420 Train loss 2.693444 on epoch=46
06/01/2022 14:10:24 - INFO - __main__ - Step 430 Global step 430 Train loss 3.005869 on epoch=47
06/01/2022 14:10:29 - INFO - __main__ - Step 440 Global step 440 Train loss 2.290381 on epoch=48
06/01/2022 14:10:34 - INFO - __main__ - Step 450 Global step 450 Train loss 2.493759 on epoch=49
06/01/2022 14:10:37 - INFO - __main__ - Global step 450 Train loss 2.736676 ACC 0.5188679245283019 on epoch=49
06/01/2022 14:10:42 - INFO - __main__ - Step 460 Global step 460 Train loss 2.660381 on epoch=51
06/01/2022 14:10:47 - INFO - __main__ - Step 470 Global step 470 Train loss 2.455370 on epoch=52
06/01/2022 14:10:52 - INFO - __main__ - Step 480 Global step 480 Train loss 2.659594 on epoch=53
06/01/2022 14:10:57 - INFO - __main__ - Step 490 Global step 490 Train loss 2.605334 on epoch=54
06/01/2022 14:11:02 - INFO - __main__ - Step 500 Global step 500 Train loss 2.706504 on epoch=55
06/01/2022 14:11:05 - INFO - __main__ - Global step 500 Train loss 2.617436 ACC 0.5188679245283019 on epoch=55
06/01/2022 14:11:10 - INFO - __main__ - Step 510 Global step 510 Train loss 2.373095 on epoch=56
06/01/2022 14:11:15 - INFO - __main__ - Step 520 Global step 520 Train loss 2.044896 on epoch=57
06/01/2022 14:11:20 - INFO - __main__ - Step 530 Global step 530 Train loss 2.327497 on epoch=58
06/01/2022 14:11:25 - INFO - __main__ - Step 540 Global step 540 Train loss 2.014466 on epoch=59
06/01/2022 14:11:31 - INFO - __main__ - Step 550 Global step 550 Train loss 1.689168 on epoch=61
06/01/2022 14:11:33 - INFO - __main__ - Global step 550 Train loss 2.089824 ACC 0.5188679245283019 on epoch=61
06/01/2022 14:11:38 - INFO - __main__ - Step 560 Global step 560 Train loss 1.867203 on epoch=62
06/01/2022 14:11:44 - INFO - __main__ - Step 570 Global step 570 Train loss 1.709696 on epoch=63
06/01/2022 14:11:49 - INFO - __main__ - Step 580 Global step 580 Train loss 2.044116 on epoch=64
06/01/2022 14:11:54 - INFO - __main__ - Step 590 Global step 590 Train loss 1.895255 on epoch=65
06/01/2022 14:11:59 - INFO - __main__ - Step 600 Global step 600 Train loss 2.153788 on epoch=66
06/01/2022 14:12:01 - INFO - __main__ - Global step 600 Train loss 1.934012 ACC 0.5188679245283019 on epoch=66
06/01/2022 14:12:07 - INFO - __main__ - Step 610 Global step 610 Train loss 2.116244 on epoch=67
06/01/2022 14:12:12 - INFO - __main__ - Step 620 Global step 620 Train loss 2.212042 on epoch=68
06/01/2022 14:12:17 - INFO - __main__ - Step 630 Global step 630 Train loss 1.933335 on epoch=69
06/01/2022 14:12:22 - INFO - __main__ - Step 640 Global step 640 Train loss 1.905145 on epoch=71
06/01/2022 14:12:27 - INFO - __main__ - Step 650 Global step 650 Train loss 1.798254 on epoch=72
06/01/2022 14:12:30 - INFO - __main__ - Global step 650 Train loss 1.993004 ACC 0.5188679245283019 on epoch=72
06/01/2022 14:12:35 - INFO - __main__ - Step 660 Global step 660 Train loss 1.672881 on epoch=73
06/01/2022 14:12:40 - INFO - __main__ - Step 670 Global step 670 Train loss 1.473786 on epoch=74
06/01/2022 14:12:45 - INFO - __main__ - Step 680 Global step 680 Train loss 1.900429 on epoch=75
06/01/2022 14:12:50 - INFO - __main__ - Step 690 Global step 690 Train loss 1.675594 on epoch=76
06/01/2022 14:12:55 - INFO - __main__ - Step 700 Global step 700 Train loss 1.469692 on epoch=77
06/01/2022 14:12:58 - INFO - __main__ - Global step 700 Train loss 1.638477 ACC 0.5188679245283019 on epoch=77
06/01/2022 14:13:03 - INFO - __main__ - Step 710 Global step 710 Train loss 1.988151 on epoch=78
06/01/2022 14:13:08 - INFO - __main__ - Step 720 Global step 720 Train loss 1.724502 on epoch=79
06/01/2022 14:13:13 - INFO - __main__ - Step 730 Global step 730 Train loss 1.401343 on epoch=81
06/01/2022 14:13:18 - INFO - __main__ - Step 740 Global step 740 Train loss 1.679690 on epoch=82
06/01/2022 14:13:24 - INFO - __main__ - Step 750 Global step 750 Train loss 1.598997 on epoch=83
06/01/2022 14:13:26 - INFO - __main__ - Global step 750 Train loss 1.678537 ACC 0.5188679245283019 on epoch=83
06/01/2022 14:13:31 - INFO - __main__ - Step 760 Global step 760 Train loss 1.192457 on epoch=84
06/01/2022 14:13:36 - INFO - __main__ - Step 770 Global step 770 Train loss 1.202376 on epoch=85
06/01/2022 14:13:41 - INFO - __main__ - Step 780 Global step 780 Train loss 1.474032 on epoch=86
06/01/2022 14:13:46 - INFO - __main__ - Step 790 Global step 790 Train loss 1.117813 on epoch=87
06/01/2022 14:13:51 - INFO - __main__ - Step 800 Global step 800 Train loss 1.091602 on epoch=88
06/01/2022 14:13:54 - INFO - __main__ - Global step 800 Train loss 1.215656 ACC 0.5188679245283019 on epoch=88
06/01/2022 14:13:59 - INFO - __main__ - Step 810 Global step 810 Train loss 1.760018 on epoch=89
06/01/2022 14:14:04 - INFO - __main__ - Step 820 Global step 820 Train loss 1.026339 on epoch=91
06/01/2022 14:14:09 - INFO - __main__ - Step 830 Global step 830 Train loss 1.483806 on epoch=92
06/01/2022 14:14:14 - INFO - __main__ - Step 840 Global step 840 Train loss 1.380966 on epoch=93
06/01/2022 14:14:20 - INFO - __main__ - Step 850 Global step 850 Train loss 1.391663 on epoch=94
06/01/2022 14:14:22 - INFO - __main__ - Global step 850 Train loss 1.408558 ACC 0.5188679245283019 on epoch=94
06/01/2022 14:14:27 - INFO - __main__ - Step 860 Global step 860 Train loss 1.100346 on epoch=95
06/01/2022 14:14:32 - INFO - __main__ - Step 870 Global step 870 Train loss 1.049730 on epoch=96
06/01/2022 14:14:37 - INFO - __main__ - Step 880 Global step 880 Train loss 1.244431 on epoch=97
06/01/2022 14:14:43 - INFO - __main__ - Step 890 Global step 890 Train loss 1.354580 on epoch=98
06/01/2022 14:14:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.993334 on epoch=99
06/01/2022 14:14:50 - INFO - __main__ - Global step 900 Train loss 1.148484 ACC 0.5188679245283019 on epoch=99
06/01/2022 14:14:55 - INFO - __main__ - Step 910 Global step 910 Train loss 1.151450 on epoch=101
06/01/2022 14:15:00 - INFO - __main__ - Step 920 Global step 920 Train loss 1.117440 on epoch=102
06/01/2022 14:15:05 - INFO - __main__ - Step 930 Global step 930 Train loss 1.137071 on epoch=103
06/01/2022 14:15:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.988253 on epoch=104
06/01/2022 14:15:16 - INFO - __main__ - Step 950 Global step 950 Train loss 1.023562 on epoch=105
06/01/2022 14:15:18 - INFO - __main__ - Global step 950 Train loss 1.083555 ACC 0.5188679245283019 on epoch=105
06/01/2022 14:15:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.765699 on epoch=106
06/01/2022 14:15:28 - INFO - __main__ - Step 970 Global step 970 Train loss 1.015164 on epoch=107
06/01/2022 14:15:33 - INFO - __main__ - Step 980 Global step 980 Train loss 1.045276 on epoch=108
06/01/2022 14:15:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.678321 on epoch=109
06/01/2022 14:15:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.453709 on epoch=111
06/01/2022 14:15:45 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 14:15:45 - INFO - __main__ - Printing 3 examples
06/01/2022 14:15:45 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/01/2022 14:15:45 - INFO - __main__ - ['contradiction']
06/01/2022 14:15:45 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/01/2022 14:15:45 - INFO - __main__ - ['contradiction']
06/01/2022 14:15:45 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/01/2022 14:15:45 - INFO - __main__ - ['contradiction']
06/01/2022 14:15:45 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:15:45 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:15:45 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 14:15:45 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 14:15:45 - INFO - __main__ - Printing 3 examples
06/01/2022 14:15:45 - INFO - __main__ -  [superglue-cb] premise: A: Yeah, that's crazy. B: and then you come here in the Dallas area, um, I don't believe that people should be allowed to carry guns in their vehicles. [SEP] hypothesis: people should be allowed to carry guns in their vehicles
06/01/2022 14:15:45 - INFO - __main__ - ['contradiction']
06/01/2022 14:15:45 - INFO - __main__ -  [superglue-cb] premise: Jed wondered. He 'd scarcely set eyes on him since the night they 'd had dinner together at the house in Westwood. Nobody had mentioned him either and Jed didn't feel he should ask. [SEP] hypothesis: Jed should ask
06/01/2022 14:15:45 - INFO - __main__ - ['contradiction']
06/01/2022 14:15:45 - INFO - __main__ -  [superglue-cb] premise: A: They have to for international trade. B: Yeah. A: But, I guess it's easier to switch back and forth than it used to be, uh, because uh, of computers coming into everything. B: Uh-huh. Yeah, I don't think switching back and forth is that big a deal. [SEP] hypothesis: switching back and forth is that big a deal
06/01/2022 14:15:45 - INFO - __main__ - ['contradiction']
06/01/2022 14:15:45 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:15:45 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:15:46 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 14:15:46 - INFO - __main__ - Global step 1000 Train loss 0.791634 ACC 0.5094339622641509 on epoch=111
06/01/2022 14:15:46 - INFO - __main__ - save last model!
06/01/2022 14:15:53 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 14:15:54 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 14:15:54 - INFO - __main__ - Printing 3 examples
06/01/2022 14:15:54 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 14:15:54 - INFO - __main__ - ['contradiction']
06/01/2022 14:15:54 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 14:15:54 - INFO - __main__ - ['neutral']
06/01/2022 14:15:54 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 14:15:54 - INFO - __main__ - ['entailment']
06/01/2022 14:15:54 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:15:54 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:15:54 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 14:15:55 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_100_0.0001_8_predictions.txt
06/01/2022 14:15:55 - INFO - __main__ - ACC on test data: 0.5000
06/01/2022 14:15:56 - INFO - __main__ - prefix=superglue-cb_64_100, lr=0.0001, bsz=8, dev_performance=0.5188679245283019, test_performance=0.5
06/01/2022 14:15:56 - INFO - __main__ - Running ... prefix=superglue-cb_64_13, lr=0.0005, bsz=8 ...
06/01/2022 14:15:57 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 14:15:57 - INFO - __main__ - Printing 3 examples
06/01/2022 14:15:57 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/01/2022 14:15:57 - INFO - __main__ - ['contradiction']
06/01/2022 14:15:57 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/01/2022 14:15:57 - INFO - __main__ - ['contradiction']
06/01/2022 14:15:57 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/01/2022 14:15:57 - INFO - __main__ - ['contradiction']
06/01/2022 14:15:57 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:15:57 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:15:57 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 14:15:57 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 14:15:57 - INFO - __main__ - Printing 3 examples
06/01/2022 14:15:57 - INFO - __main__ -  [superglue-cb] premise: A: Yeah, that's crazy. B: and then you come here in the Dallas area, um, I don't believe that people should be allowed to carry guns in their vehicles. [SEP] hypothesis: people should be allowed to carry guns in their vehicles
06/01/2022 14:15:57 - INFO - __main__ - ['contradiction']
06/01/2022 14:15:57 - INFO - __main__ -  [superglue-cb] premise: Jed wondered. He 'd scarcely set eyes on him since the night they 'd had dinner together at the house in Westwood. Nobody had mentioned him either and Jed didn't feel he should ask. [SEP] hypothesis: Jed should ask
06/01/2022 14:15:57 - INFO - __main__ - ['contradiction']
06/01/2022 14:15:57 - INFO - __main__ -  [superglue-cb] premise: A: They have to for international trade. B: Yeah. A: But, I guess it's easier to switch back and forth than it used to be, uh, because uh, of computers coming into everything. B: Uh-huh. Yeah, I don't think switching back and forth is that big a deal. [SEP] hypothesis: switching back and forth is that big a deal
06/01/2022 14:15:57 - INFO - __main__ - ['contradiction']
06/01/2022 14:15:57 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:15:57 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:15:57 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 14:15:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 14:15:59 - INFO - __main__ - Starting training!
06/01/2022 14:16:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 14:16:10 - INFO - __main__ - Starting training!
06/01/2022 14:16:14 - INFO - __main__ - Step 10 Global step 10 Train loss 25.175470 on epoch=1
06/01/2022 14:16:19 - INFO - __main__ - Step 20 Global step 20 Train loss 15.607623 on epoch=2
06/01/2022 14:16:24 - INFO - __main__ - Step 30 Global step 30 Train loss 12.807485 on epoch=3
06/01/2022 14:16:30 - INFO - __main__ - Step 40 Global step 40 Train loss 10.606418 on epoch=4
06/01/2022 14:16:35 - INFO - __main__ - Step 50 Global step 50 Train loss 9.026947 on epoch=5
06/01/2022 14:16:39 - INFO - __main__ - Global step 50 Train loss 14.644789 ACC 0.18867924528301888 on epoch=5
06/01/2022 14:16:44 - INFO - __main__ - Step 60 Global step 60 Train loss 7.671891 on epoch=6
06/01/2022 14:16:49 - INFO - __main__ - Step 70 Global step 70 Train loss 7.397727 on epoch=7
06/01/2022 14:16:55 - INFO - __main__ - Step 80 Global step 80 Train loss 5.630965 on epoch=8
06/01/2022 14:17:00 - INFO - __main__ - Step 90 Global step 90 Train loss 3.972634 on epoch=9
06/01/2022 14:17:05 - INFO - __main__ - Step 100 Global step 100 Train loss 2.944269 on epoch=11
06/01/2022 14:17:07 - INFO - __main__ - Global step 100 Train loss 5.523497 ACC 0.5188679245283019 on epoch=11
06/01/2022 14:17:13 - INFO - __main__ - Step 110 Global step 110 Train loss 2.576349 on epoch=12
06/01/2022 14:17:18 - INFO - __main__ - Step 120 Global step 120 Train loss 2.038977 on epoch=13
06/01/2022 14:17:24 - INFO - __main__ - Step 130 Global step 130 Train loss 1.917495 on epoch=14
06/01/2022 14:17:29 - INFO - __main__ - Step 140 Global step 140 Train loss 1.856622 on epoch=15
06/01/2022 14:17:34 - INFO - __main__ - Step 150 Global step 150 Train loss 1.707276 on epoch=16
06/01/2022 14:17:36 - INFO - __main__ - Global step 150 Train loss 2.019343 ACC 0.5188679245283019 on epoch=16
06/01/2022 14:17:42 - INFO - __main__ - Step 160 Global step 160 Train loss 1.948611 on epoch=17
06/01/2022 14:17:47 - INFO - __main__ - Step 170 Global step 170 Train loss 1.409466 on epoch=18
06/01/2022 14:17:52 - INFO - __main__ - Step 180 Global step 180 Train loss 1.315747 on epoch=19
06/01/2022 14:17:58 - INFO - __main__ - Step 190 Global step 190 Train loss 1.345492 on epoch=21
06/01/2022 14:18:03 - INFO - __main__ - Step 200 Global step 200 Train loss 1.145788 on epoch=22
06/01/2022 14:18:05 - INFO - __main__ - Global step 200 Train loss 1.433021 ACC 0.5188679245283019 on epoch=22
06/01/2022 14:18:10 - INFO - __main__ - Step 210 Global step 210 Train loss 1.022647 on epoch=23
06/01/2022 14:18:15 - INFO - __main__ - Step 220 Global step 220 Train loss 1.302557 on epoch=24
06/01/2022 14:18:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.956091 on epoch=25
06/01/2022 14:18:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.780796 on epoch=26
06/01/2022 14:18:31 - INFO - __main__ - Step 250 Global step 250 Train loss 1.206705 on epoch=27
06/01/2022 14:18:33 - INFO - __main__ - Global step 250 Train loss 1.053759 ACC 0.5188679245283019 on epoch=27
06/01/2022 14:18:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.747202 on epoch=28
06/01/2022 14:18:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.794418 on epoch=29
06/01/2022 14:18:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.720784 on epoch=31
06/01/2022 14:18:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.828493 on epoch=32
06/01/2022 14:19:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.786373 on epoch=33
06/01/2022 14:19:02 - INFO - __main__ - Global step 300 Train loss 0.775454 ACC 0.6415094339622641 on epoch=33
06/01/2022 14:19:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.768894 on epoch=34
06/01/2022 14:19:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.590412 on epoch=35
06/01/2022 14:19:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.569617 on epoch=36
06/01/2022 14:19:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.504556 on epoch=37
06/01/2022 14:19:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.597181 on epoch=38
06/01/2022 14:19:31 - INFO - __main__ - Global step 350 Train loss 0.606132 ACC 0.4811320754716981 on epoch=38
06/01/2022 14:19:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.492028 on epoch=39
06/01/2022 14:19:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.467601 on epoch=41
06/01/2022 14:19:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.475757 on epoch=42
06/01/2022 14:19:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.439862 on epoch=43
06/01/2022 14:19:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.497419 on epoch=44
06/01/2022 14:20:00 - INFO - __main__ - Global step 400 Train loss 0.474533 ACC 0.4811320754716981 on epoch=44
06/01/2022 14:20:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.480108 on epoch=45
06/01/2022 14:20:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.355980 on epoch=46
06/01/2022 14:20:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.453483 on epoch=47
06/01/2022 14:20:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.527061 on epoch=48
06/01/2022 14:20:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.450499 on epoch=49
06/01/2022 14:20:29 - INFO - __main__ - Global step 450 Train loss 0.453426 ACC 0.5188679245283019 on epoch=49
06/01/2022 14:20:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.463982 on epoch=51
06/01/2022 14:20:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.426070 on epoch=52
06/01/2022 14:20:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.454256 on epoch=53
06/01/2022 14:20:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.382468 on epoch=54
06/01/2022 14:20:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.365920 on epoch=55
06/01/2022 14:20:58 - INFO - __main__ - Global step 500 Train loss 0.418539 ACC 0.6320754716981132 on epoch=55
06/01/2022 14:21:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.362913 on epoch=56
06/01/2022 14:21:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.361387 on epoch=57
06/01/2022 14:21:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.330313 on epoch=58
06/01/2022 14:21:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.346232 on epoch=59
06/01/2022 14:21:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.427056 on epoch=61
06/01/2022 14:21:27 - INFO - __main__ - Global step 550 Train loss 0.365580 ACC 0.4811320754716981 on epoch=61
06/01/2022 14:21:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.402534 on epoch=62
06/01/2022 14:21:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.325819 on epoch=63
06/01/2022 14:21:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.357840 on epoch=64
06/01/2022 14:21:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.306174 on epoch=65
06/01/2022 14:21:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.378103 on epoch=66
06/01/2022 14:21:56 - INFO - __main__ - Global step 600 Train loss 0.354094 ACC 0.6037735849056604 on epoch=66
06/01/2022 14:22:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.375311 on epoch=67
06/01/2022 14:22:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.372595 on epoch=68
06/01/2022 14:22:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.367145 on epoch=69
06/01/2022 14:22:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.327283 on epoch=71
06/01/2022 14:22:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.316586 on epoch=72
06/01/2022 14:22:24 - INFO - __main__ - Global step 650 Train loss 0.351784 ACC 0.7169811320754716 on epoch=72
06/01/2022 14:22:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.302202 on epoch=73
06/01/2022 14:22:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.306376 on epoch=74
06/01/2022 14:22:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.277542 on epoch=75
06/01/2022 14:22:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.326260 on epoch=76
06/01/2022 14:22:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.327897 on epoch=77
06/01/2022 14:22:54 - INFO - __main__ - Global step 700 Train loss 0.308055 ACC 0.6792452830188679 on epoch=77
06/01/2022 14:22:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.298493 on epoch=78
06/01/2022 14:23:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.297595 on epoch=79
06/01/2022 14:23:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.296789 on epoch=81
06/01/2022 14:23:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.273112 on epoch=82
06/01/2022 14:23:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.307917 on epoch=83
06/01/2022 14:23:23 - INFO - __main__ - Global step 750 Train loss 0.294781 ACC 0.7735849056603774 on epoch=83
06/01/2022 14:23:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.289625 on epoch=84
06/01/2022 14:23:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.271974 on epoch=85
06/01/2022 14:23:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.285903 on epoch=86
06/01/2022 14:23:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.353916 on epoch=87
06/01/2022 14:23:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.305759 on epoch=88
06/01/2022 14:23:52 - INFO - __main__ - Global step 800 Train loss 0.301435 ACC 0.660377358490566 on epoch=88
06/01/2022 14:23:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.239181 on epoch=89
06/01/2022 14:24:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.295610 on epoch=91
06/01/2022 14:24:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.213432 on epoch=92
06/01/2022 14:24:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.282596 on epoch=93
06/01/2022 14:24:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.196267 on epoch=94
06/01/2022 14:24:21 - INFO - __main__ - Global step 850 Train loss 0.245417 ACC 0.7547169811320755 on epoch=94
06/01/2022 14:24:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.256593 on epoch=95
06/01/2022 14:24:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.229882 on epoch=96
06/01/2022 14:24:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.263399 on epoch=97
06/01/2022 14:24:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.241386 on epoch=98
06/01/2022 14:24:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.244861 on epoch=99
06/01/2022 14:24:50 - INFO - __main__ - Global step 900 Train loss 0.247224 ACC 0.7075471698113207 on epoch=99
06/01/2022 14:24:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.216493 on epoch=101
06/01/2022 14:25:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.196154 on epoch=102
06/01/2022 14:25:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.202358 on epoch=103
06/01/2022 14:25:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.209198 on epoch=104
06/01/2022 14:25:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.222845 on epoch=105
06/01/2022 14:25:18 - INFO - __main__ - Global step 950 Train loss 0.209410 ACC 0.8301886792452831 on epoch=105
06/01/2022 14:25:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.202294 on epoch=106
06/01/2022 14:25:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.198045 on epoch=107
06/01/2022 14:25:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.199339 on epoch=108
06/01/2022 14:25:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.191276 on epoch=109
06/01/2022 14:25:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.175426 on epoch=111
06/01/2022 14:25:46 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 14:25:46 - INFO - __main__ - Printing 3 examples
06/01/2022 14:25:46 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/01/2022 14:25:46 - INFO - __main__ - ['contradiction']
06/01/2022 14:25:46 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/01/2022 14:25:46 - INFO - __main__ - ['contradiction']
06/01/2022 14:25:46 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/01/2022 14:25:46 - INFO - __main__ - ['contradiction']
06/01/2022 14:25:46 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:25:46 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:25:47 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 14:25:47 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 14:25:47 - INFO - __main__ - Printing 3 examples
06/01/2022 14:25:47 - INFO - __main__ -  [superglue-cb] premise: A: Yeah, that's crazy. B: and then you come here in the Dallas area, um, I don't believe that people should be allowed to carry guns in their vehicles. [SEP] hypothesis: people should be allowed to carry guns in their vehicles
06/01/2022 14:25:47 - INFO - __main__ - ['contradiction']
06/01/2022 14:25:47 - INFO - __main__ -  [superglue-cb] premise: Jed wondered. He 'd scarcely set eyes on him since the night they 'd had dinner together at the house in Westwood. Nobody had mentioned him either and Jed didn't feel he should ask. [SEP] hypothesis: Jed should ask
06/01/2022 14:25:47 - INFO - __main__ - ['contradiction']
06/01/2022 14:25:47 - INFO - __main__ -  [superglue-cb] premise: A: They have to for international trade. B: Yeah. A: But, I guess it's easier to switch back and forth than it used to be, uh, because uh, of computers coming into everything. B: Uh-huh. Yeah, I don't think switching back and forth is that big a deal. [SEP] hypothesis: switching back and forth is that big a deal
06/01/2022 14:25:47 - INFO - __main__ - ['contradiction']
06/01/2022 14:25:47 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:25:47 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:25:47 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 14:25:47 - INFO - __main__ - Global step 1000 Train loss 0.193276 ACC 0.8113207547169812 on epoch=111
06/01/2022 14:25:47 - INFO - __main__ - save last model!
06/01/2022 14:25:55 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 14:25:55 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 14:25:55 - INFO - __main__ - Printing 3 examples
06/01/2022 14:25:55 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 14:25:55 - INFO - __main__ - ['contradiction']
06/01/2022 14:25:55 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 14:25:55 - INFO - __main__ - ['neutral']
06/01/2022 14:25:55 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 14:25:55 - INFO - __main__ - ['entailment']
06/01/2022 14:25:55 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:25:55 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:25:55 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 14:25:57 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_13_0.0005_8_predictions.txt
06/01/2022 14:25:57 - INFO - __main__ - ACC on test data: 0.6607
06/01/2022 14:25:57 - INFO - __main__ - prefix=superglue-cb_64_13, lr=0.0005, bsz=8, dev_performance=0.8301886792452831, test_performance=0.6607142857142857
06/01/2022 14:25:57 - INFO - __main__ - Running ... prefix=superglue-cb_64_13, lr=0.0003, bsz=8 ...
06/01/2022 14:25:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 14:25:57 - INFO - __main__ - Starting training!
06/01/2022 14:25:58 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 14:25:58 - INFO - __main__ - Printing 3 examples
06/01/2022 14:25:58 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/01/2022 14:25:58 - INFO - __main__ - ['contradiction']
06/01/2022 14:25:58 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/01/2022 14:25:58 - INFO - __main__ - ['contradiction']
06/01/2022 14:25:58 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/01/2022 14:25:58 - INFO - __main__ - ['contradiction']
06/01/2022 14:25:58 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:25:58 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:25:58 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 14:25:58 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 14:25:58 - INFO - __main__ - Printing 3 examples
06/01/2022 14:25:58 - INFO - __main__ -  [superglue-cb] premise: A: Yeah, that's crazy. B: and then you come here in the Dallas area, um, I don't believe that people should be allowed to carry guns in their vehicles. [SEP] hypothesis: people should be allowed to carry guns in their vehicles
06/01/2022 14:25:58 - INFO - __main__ - ['contradiction']
06/01/2022 14:25:58 - INFO - __main__ -  [superglue-cb] premise: Jed wondered. He 'd scarcely set eyes on him since the night they 'd had dinner together at the house in Westwood. Nobody had mentioned him either and Jed didn't feel he should ask. [SEP] hypothesis: Jed should ask
06/01/2022 14:25:58 - INFO - __main__ - ['contradiction']
06/01/2022 14:25:58 - INFO - __main__ -  [superglue-cb] premise: A: They have to for international trade. B: Yeah. A: But, I guess it's easier to switch back and forth than it used to be, uh, because uh, of computers coming into everything. B: Uh-huh. Yeah, I don't think switching back and forth is that big a deal. [SEP] hypothesis: switching back and forth is that big a deal
06/01/2022 14:25:58 - INFO - __main__ - ['contradiction']
06/01/2022 14:25:58 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:25:58 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:25:58 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 14:26:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 14:26:12 - INFO - __main__ - Starting training!
06/01/2022 14:26:17 - INFO - __main__ - Step 10 Global step 10 Train loss 23.924982 on epoch=1
06/01/2022 14:26:22 - INFO - __main__ - Step 20 Global step 20 Train loss 15.720098 on epoch=2
06/01/2022 14:26:27 - INFO - __main__ - Step 30 Global step 30 Train loss 11.379718 on epoch=3
06/01/2022 14:26:32 - INFO - __main__ - Step 40 Global step 40 Train loss 10.190967 on epoch=4
06/01/2022 14:26:37 - INFO - __main__ - Step 50 Global step 50 Train loss 9.773720 on epoch=5
06/01/2022 14:26:40 - INFO - __main__ - Global step 50 Train loss 14.197898 ACC 0.0 on epoch=5
06/01/2022 14:26:45 - INFO - __main__ - Step 60 Global step 60 Train loss 8.908353 on epoch=6
06/01/2022 14:26:51 - INFO - __main__ - Step 70 Global step 70 Train loss 8.632422 on epoch=7
06/01/2022 14:26:56 - INFO - __main__ - Step 80 Global step 80 Train loss 8.140028 on epoch=8
06/01/2022 14:27:01 - INFO - __main__ - Step 90 Global step 90 Train loss 7.625403 on epoch=9
06/01/2022 14:27:06 - INFO - __main__ - Step 100 Global step 100 Train loss 6.329827 on epoch=11
06/01/2022 14:27:09 - INFO - __main__ - Global step 100 Train loss 7.927207 ACC 0.0 on epoch=11
06/01/2022 14:27:14 - INFO - __main__ - Step 110 Global step 110 Train loss 4.797355 on epoch=12
06/01/2022 14:27:19 - INFO - __main__ - Step 120 Global step 120 Train loss 3.930555 on epoch=13
06/01/2022 14:27:24 - INFO - __main__ - Step 130 Global step 130 Train loss 3.387243 on epoch=14
06/01/2022 14:27:30 - INFO - __main__ - Step 140 Global step 140 Train loss 2.685236 on epoch=15
06/01/2022 14:27:35 - INFO - __main__ - Step 150 Global step 150 Train loss 2.903615 on epoch=16
06/01/2022 14:27:37 - INFO - __main__ - Global step 150 Train loss 3.540801 ACC 0.5188679245283019 on epoch=16
06/01/2022 14:27:43 - INFO - __main__ - Step 160 Global step 160 Train loss 2.669778 on epoch=17
06/01/2022 14:27:48 - INFO - __main__ - Step 170 Global step 170 Train loss 1.675627 on epoch=18
06/01/2022 14:27:54 - INFO - __main__ - Step 180 Global step 180 Train loss 2.219128 on epoch=19
06/01/2022 14:27:59 - INFO - __main__ - Step 190 Global step 190 Train loss 2.056608 on epoch=21
06/01/2022 14:28:04 - INFO - __main__ - Step 200 Global step 200 Train loss 2.265900 on epoch=22
06/01/2022 14:28:06 - INFO - __main__ - Global step 200 Train loss 2.177408 ACC 0.5188679245283019 on epoch=22
06/01/2022 14:28:12 - INFO - __main__ - Step 210 Global step 210 Train loss 1.625499 on epoch=23
06/01/2022 14:28:17 - INFO - __main__ - Step 220 Global step 220 Train loss 2.082789 on epoch=24
06/01/2022 14:28:22 - INFO - __main__ - Step 230 Global step 230 Train loss 1.425288 on epoch=25
06/01/2022 14:28:28 - INFO - __main__ - Step 240 Global step 240 Train loss 1.591533 on epoch=26
06/01/2022 14:28:33 - INFO - __main__ - Step 250 Global step 250 Train loss 1.507270 on epoch=27
06/01/2022 14:28:35 - INFO - __main__ - Global step 250 Train loss 1.646476 ACC 0.5188679245283019 on epoch=27
06/01/2022 14:28:40 - INFO - __main__ - Step 260 Global step 260 Train loss 1.241015 on epoch=28
06/01/2022 14:28:46 - INFO - __main__ - Step 270 Global step 270 Train loss 1.319409 on epoch=29
06/01/2022 14:28:51 - INFO - __main__ - Step 280 Global step 280 Train loss 1.209741 on epoch=31
06/01/2022 14:28:56 - INFO - __main__ - Step 290 Global step 290 Train loss 1.140572 on epoch=32
06/01/2022 14:29:01 - INFO - __main__ - Step 300 Global step 300 Train loss 1.232078 on epoch=33
06/01/2022 14:29:04 - INFO - __main__ - Global step 300 Train loss 1.228563 ACC 0.5188679245283019 on epoch=33
06/01/2022 14:29:09 - INFO - __main__ - Step 310 Global step 310 Train loss 1.001169 on epoch=34
06/01/2022 14:29:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.885775 on epoch=35
06/01/2022 14:29:20 - INFO - __main__ - Step 330 Global step 330 Train loss 1.361676 on epoch=36
06/01/2022 14:29:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.657203 on epoch=37
06/01/2022 14:29:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.391436 on epoch=38
06/01/2022 14:29:33 - INFO - __main__ - Global step 350 Train loss 0.859452 ACC 0.6981132075471698 on epoch=38
06/01/2022 14:29:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.308387 on epoch=39
06/01/2022 14:29:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.303755 on epoch=41
06/01/2022 14:29:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.173202 on epoch=42
06/01/2022 14:29:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.208483 on epoch=43
06/01/2022 14:30:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.114971 on epoch=44
06/01/2022 14:30:02 - INFO - __main__ - Global step 400 Train loss 0.221760 ACC 0.6226415094339622 on epoch=44
06/01/2022 14:30:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.136679 on epoch=45
06/01/2022 14:30:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.190028 on epoch=46
06/01/2022 14:30:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.127709 on epoch=47
06/01/2022 14:30:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.082601 on epoch=48
06/01/2022 14:30:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.042720 on epoch=49
06/01/2022 14:30:30 - INFO - __main__ - Global step 450 Train loss 0.115947 ACC 0.8301886792452831 on epoch=49
06/01/2022 14:30:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.029049 on epoch=51
06/01/2022 14:30:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.055265 on epoch=52
06/01/2022 14:30:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.034048 on epoch=53
06/01/2022 14:30:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.008028 on epoch=54
06/01/2022 14:30:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.008842 on epoch=55
06/01/2022 14:31:00 - INFO - __main__ - Global step 500 Train loss 0.027046 ACC 0.9528301886792453 on epoch=55
06/01/2022 14:31:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.007167 on epoch=56
06/01/2022 14:31:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.012484 on epoch=57
06/01/2022 14:31:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.004978 on epoch=58
06/01/2022 14:31:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.032430 on epoch=59
06/01/2022 14:31:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.004042 on epoch=61
06/01/2022 14:31:30 - INFO - __main__ - Global step 550 Train loss 0.012220 ACC 0.9056603773584906 on epoch=61
06/01/2022 14:31:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.002168 on epoch=62
06/01/2022 14:31:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.011167 on epoch=63
06/01/2022 14:31:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.001298 on epoch=64
06/01/2022 14:31:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.013241 on epoch=65
06/01/2022 14:31:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.002597 on epoch=66
06/01/2022 14:31:59 - INFO - __main__ - Global step 600 Train loss 0.006094 ACC 0.9622641509433962 on epoch=66
06/01/2022 14:32:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.009366 on epoch=67
06/01/2022 14:32:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.003547 on epoch=68
06/01/2022 14:32:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.007016 on epoch=69
06/01/2022 14:32:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001232 on epoch=71
06/01/2022 14:32:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001230 on epoch=72
06/01/2022 14:32:29 - INFO - __main__ - Global step 650 Train loss 0.004478 ACC 0.9622641509433962 on epoch=72
06/01/2022 14:32:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000582 on epoch=73
06/01/2022 14:32:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.002045 on epoch=74
06/01/2022 14:32:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000259 on epoch=75
06/01/2022 14:32:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000268 on epoch=76
06/01/2022 14:32:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000219 on epoch=77
06/01/2022 14:32:58 - INFO - __main__ - Global step 700 Train loss 0.000675 ACC 0.9716981132075472 on epoch=77
06/01/2022 14:33:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000674 on epoch=78
06/01/2022 14:33:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000279 on epoch=79
06/01/2022 14:33:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.001070 on epoch=81
06/01/2022 14:33:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.003620 on epoch=82
06/01/2022 14:33:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000503 on epoch=83
06/01/2022 14:33:28 - INFO - __main__ - Global step 750 Train loss 0.001229 ACC 0.9811320754716981 on epoch=83
06/01/2022 14:33:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000179 on epoch=84
06/01/2022 14:33:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000215 on epoch=85
06/01/2022 14:33:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000191 on epoch=86
06/01/2022 14:33:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000421 on epoch=87
06/01/2022 14:33:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000076 on epoch=88
06/01/2022 14:33:58 - INFO - __main__ - Global step 800 Train loss 0.000216 ACC 0.9716981132075472 on epoch=88
06/01/2022 14:34:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000122 on epoch=89
06/01/2022 14:34:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000321 on epoch=91
06/01/2022 14:34:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000058 on epoch=92
06/01/2022 14:34:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000908 on epoch=93
06/01/2022 14:34:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000086 on epoch=94
06/01/2022 14:34:27 - INFO - __main__ - Global step 850 Train loss 0.000299 ACC 0.9622641509433962 on epoch=94
06/01/2022 14:34:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000113 on epoch=95
06/01/2022 14:34:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000246 on epoch=96
06/01/2022 14:34:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000477 on epoch=97
06/01/2022 14:34:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000143 on epoch=98
06/01/2022 14:34:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000038 on epoch=99
06/01/2022 14:34:56 - INFO - __main__ - Global step 900 Train loss 0.000203 ACC 0.9339622641509434 on epoch=99
06/01/2022 14:35:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000194 on epoch=101
06/01/2022 14:35:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000579 on epoch=102
06/01/2022 14:35:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000414 on epoch=103
06/01/2022 14:35:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000036 on epoch=104
06/01/2022 14:35:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000027 on epoch=105
06/01/2022 14:35:25 - INFO - __main__ - Global step 950 Train loss 0.000250 ACC 0.9622641509433962 on epoch=105
06/01/2022 14:35:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000026 on epoch=106
06/01/2022 14:35:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000241 on epoch=107
06/01/2022 14:35:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.001975 on epoch=108
06/01/2022 14:35:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000480 on epoch=109
06/01/2022 14:35:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000112 on epoch=111
06/01/2022 14:35:53 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 14:35:53 - INFO - __main__ - Printing 3 examples
06/01/2022 14:35:53 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/01/2022 14:35:53 - INFO - __main__ - ['contradiction']
06/01/2022 14:35:53 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/01/2022 14:35:53 - INFO - __main__ - ['contradiction']
06/01/2022 14:35:53 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/01/2022 14:35:53 - INFO - __main__ - ['contradiction']
06/01/2022 14:35:53 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:35:53 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:35:53 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 14:35:53 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 14:35:53 - INFO - __main__ - Printing 3 examples
06/01/2022 14:35:53 - INFO - __main__ -  [superglue-cb] premise: A: Yeah, that's crazy. B: and then you come here in the Dallas area, um, I don't believe that people should be allowed to carry guns in their vehicles. [SEP] hypothesis: people should be allowed to carry guns in their vehicles
06/01/2022 14:35:53 - INFO - __main__ - ['contradiction']
06/01/2022 14:35:53 - INFO - __main__ -  [superglue-cb] premise: Jed wondered. He 'd scarcely set eyes on him since the night they 'd had dinner together at the house in Westwood. Nobody had mentioned him either and Jed didn't feel he should ask. [SEP] hypothesis: Jed should ask
06/01/2022 14:35:53 - INFO - __main__ - ['contradiction']
06/01/2022 14:35:53 - INFO - __main__ -  [superglue-cb] premise: A: They have to for international trade. B: Yeah. A: But, I guess it's easier to switch back and forth than it used to be, uh, because uh, of computers coming into everything. B: Uh-huh. Yeah, I don't think switching back and forth is that big a deal. [SEP] hypothesis: switching back and forth is that big a deal
06/01/2022 14:35:53 - INFO - __main__ - ['contradiction']
06/01/2022 14:35:53 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:35:53 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:35:53 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 14:35:54 - INFO - __main__ - Global step 1000 Train loss 0.000567 ACC 0.9528301886792453 on epoch=111
06/01/2022 14:35:54 - INFO - __main__ - save last model!
06/01/2022 14:36:01 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 14:36:02 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 14:36:02 - INFO - __main__ - Printing 3 examples
06/01/2022 14:36:02 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 14:36:02 - INFO - __main__ - ['contradiction']
06/01/2022 14:36:02 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 14:36:02 - INFO - __main__ - ['neutral']
06/01/2022 14:36:02 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 14:36:02 - INFO - __main__ - ['entailment']
06/01/2022 14:36:02 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:36:02 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:36:02 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 14:36:03 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_13_0.0003_8_predictions.txt
06/01/2022 14:36:03 - INFO - __main__ - ACC on test data: 0.8571
06/01/2022 14:36:03 - INFO - __main__ - prefix=superglue-cb_64_13, lr=0.0003, bsz=8, dev_performance=0.9811320754716981, test_performance=0.8571428571428571
06/01/2022 14:36:04 - INFO - __main__ - Running ... prefix=superglue-cb_64_13, lr=0.0002, bsz=8 ...
06/01/2022 14:36:04 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 14:36:04 - INFO - __main__ - Printing 3 examples
06/01/2022 14:36:04 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/01/2022 14:36:04 - INFO - __main__ - ['contradiction']
06/01/2022 14:36:04 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/01/2022 14:36:04 - INFO - __main__ - ['contradiction']
06/01/2022 14:36:04 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/01/2022 14:36:04 - INFO - __main__ - ['contradiction']
06/01/2022 14:36:04 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:36:04 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:36:05 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 14:36:05 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 14:36:05 - INFO - __main__ - Printing 3 examples
06/01/2022 14:36:05 - INFO - __main__ -  [superglue-cb] premise: A: Yeah, that's crazy. B: and then you come here in the Dallas area, um, I don't believe that people should be allowed to carry guns in their vehicles. [SEP] hypothesis: people should be allowed to carry guns in their vehicles
06/01/2022 14:36:05 - INFO - __main__ - ['contradiction']
06/01/2022 14:36:05 - INFO - __main__ -  [superglue-cb] premise: Jed wondered. He 'd scarcely set eyes on him since the night they 'd had dinner together at the house in Westwood. Nobody had mentioned him either and Jed didn't feel he should ask. [SEP] hypothesis: Jed should ask
06/01/2022 14:36:05 - INFO - __main__ - ['contradiction']
06/01/2022 14:36:05 - INFO - __main__ -  [superglue-cb] premise: A: They have to for international trade. B: Yeah. A: But, I guess it's easier to switch back and forth than it used to be, uh, because uh, of computers coming into everything. B: Uh-huh. Yeah, I don't think switching back and forth is that big a deal. [SEP] hypothesis: switching back and forth is that big a deal
06/01/2022 14:36:05 - INFO - __main__ - ['contradiction']
06/01/2022 14:36:05 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:36:05 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:36:05 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 14:36:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 14:36:06 - INFO - __main__ - Starting training!
06/01/2022 14:36:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 14:36:18 - INFO - __main__ - Starting training!
06/01/2022 14:36:22 - INFO - __main__ - Step 10 Global step 10 Train loss 24.697330 on epoch=1
06/01/2022 14:36:27 - INFO - __main__ - Step 20 Global step 20 Train loss 17.505093 on epoch=2
06/01/2022 14:36:32 - INFO - __main__ - Step 30 Global step 30 Train loss 11.119550 on epoch=3
06/01/2022 14:36:37 - INFO - __main__ - Step 40 Global step 40 Train loss 11.809334 on epoch=4
06/01/2022 14:36:43 - INFO - __main__ - Step 50 Global step 50 Train loss 10.038363 on epoch=5
06/01/2022 14:36:45 - INFO - __main__ - Global step 50 Train loss 15.033933 ACC 0.0 on epoch=5
06/01/2022 14:36:51 - INFO - __main__ - Step 60 Global step 60 Train loss 9.821037 on epoch=6
06/01/2022 14:36:56 - INFO - __main__ - Step 70 Global step 70 Train loss 9.615217 on epoch=7
06/01/2022 14:37:01 - INFO - __main__ - Step 80 Global step 80 Train loss 8.755720 on epoch=8
06/01/2022 14:37:07 - INFO - __main__ - Step 90 Global step 90 Train loss 8.264198 on epoch=9
06/01/2022 14:37:12 - INFO - __main__ - Step 100 Global step 100 Train loss 8.506353 on epoch=11
06/01/2022 14:37:15 - INFO - __main__ - Global step 100 Train loss 8.992505 ACC 0.009433962264150943 on epoch=11
06/01/2022 14:37:21 - INFO - __main__ - Step 110 Global step 110 Train loss 7.684809 on epoch=12
06/01/2022 14:37:27 - INFO - __main__ - Step 120 Global step 120 Train loss 7.112596 on epoch=13
06/01/2022 14:37:32 - INFO - __main__ - Step 130 Global step 130 Train loss 6.326894 on epoch=14
06/01/2022 14:37:38 - INFO - __main__ - Step 140 Global step 140 Train loss 6.065604 on epoch=15
06/01/2022 14:37:43 - INFO - __main__ - Step 150 Global step 150 Train loss 5.334359 on epoch=16
06/01/2022 14:37:46 - INFO - __main__ - Global step 150 Train loss 6.504852 ACC 0.0 on epoch=16
06/01/2022 14:37:51 - INFO - __main__ - Step 160 Global step 160 Train loss 4.377375 on epoch=17
06/01/2022 14:37:56 - INFO - __main__ - Step 170 Global step 170 Train loss 3.664469 on epoch=18
06/01/2022 14:38:01 - INFO - __main__ - Step 180 Global step 180 Train loss 2.378241 on epoch=19
06/01/2022 14:38:07 - INFO - __main__ - Step 190 Global step 190 Train loss 2.325562 on epoch=21
06/01/2022 14:38:12 - INFO - __main__ - Step 200 Global step 200 Train loss 2.271977 on epoch=22
06/01/2022 14:38:14 - INFO - __main__ - Global step 200 Train loss 3.003525 ACC 0.5188679245283019 on epoch=22
06/01/2022 14:38:20 - INFO - __main__ - Step 210 Global step 210 Train loss 2.351922 on epoch=23
06/01/2022 14:38:25 - INFO - __main__ - Step 220 Global step 220 Train loss 2.460869 on epoch=24
06/01/2022 14:38:30 - INFO - __main__ - Step 230 Global step 230 Train loss 2.475438 on epoch=25
06/01/2022 14:38:35 - INFO - __main__ - Step 240 Global step 240 Train loss 2.269522 on epoch=26
06/01/2022 14:38:41 - INFO - __main__ - Step 250 Global step 250 Train loss 2.029134 on epoch=27
06/01/2022 14:38:43 - INFO - __main__ - Global step 250 Train loss 2.317377 ACC 0.5188679245283019 on epoch=27
06/01/2022 14:38:48 - INFO - __main__ - Step 260 Global step 260 Train loss 1.436397 on epoch=28
06/01/2022 14:38:53 - INFO - __main__ - Step 270 Global step 270 Train loss 2.016791 on epoch=29
06/01/2022 14:38:59 - INFO - __main__ - Step 280 Global step 280 Train loss 1.632401 on epoch=31
06/01/2022 14:39:04 - INFO - __main__ - Step 290 Global step 290 Train loss 1.692193 on epoch=32
06/01/2022 14:39:09 - INFO - __main__ - Step 300 Global step 300 Train loss 1.741585 on epoch=33
06/01/2022 14:39:11 - INFO - __main__ - Global step 300 Train loss 1.703874 ACC 0.5188679245283019 on epoch=33
06/01/2022 14:39:17 - INFO - __main__ - Step 310 Global step 310 Train loss 1.579524 on epoch=34
06/01/2022 14:39:22 - INFO - __main__ - Step 320 Global step 320 Train loss 1.440944 on epoch=35
06/01/2022 14:39:27 - INFO - __main__ - Step 330 Global step 330 Train loss 1.618488 on epoch=36
06/01/2022 14:39:32 - INFO - __main__ - Step 340 Global step 340 Train loss 1.405578 on epoch=37
06/01/2022 14:39:37 - INFO - __main__ - Step 350 Global step 350 Train loss 1.405621 on epoch=38
06/01/2022 14:39:40 - INFO - __main__ - Global step 350 Train loss 1.490031 ACC 0.5188679245283019 on epoch=38
06/01/2022 14:39:45 - INFO - __main__ - Step 360 Global step 360 Train loss 1.342904 on epoch=39
06/01/2022 14:39:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.859264 on epoch=41
06/01/2022 14:39:55 - INFO - __main__ - Step 380 Global step 380 Train loss 1.513252 on epoch=42
06/01/2022 14:40:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.431699 on epoch=43
06/01/2022 14:40:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.294265 on epoch=44
06/01/2022 14:40:08 - INFO - __main__ - Global step 400 Train loss 0.888277 ACC 0.7547169811320755 on epoch=44
06/01/2022 14:40:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.268911 on epoch=45
06/01/2022 14:40:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.196552 on epoch=46
06/01/2022 14:40:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.152649 on epoch=47
06/01/2022 14:40:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.111165 on epoch=48
06/01/2022 14:40:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.323470 on epoch=49
06/01/2022 14:40:38 - INFO - __main__ - Global step 450 Train loss 0.210549 ACC 0.8679245283018868 on epoch=49
06/01/2022 14:40:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.146157 on epoch=51
06/01/2022 14:40:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.035303 on epoch=52
06/01/2022 14:40:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.088755 on epoch=53
06/01/2022 14:41:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.042121 on epoch=54
06/01/2022 14:41:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.021029 on epoch=55
06/01/2022 14:41:08 - INFO - __main__ - Global step 500 Train loss 0.066673 ACC 0.9056603773584906 on epoch=55
06/01/2022 14:41:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.019431 on epoch=56
06/01/2022 14:41:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.015663 on epoch=57
06/01/2022 14:41:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.016943 on epoch=58
06/01/2022 14:41:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.035483 on epoch=59
06/01/2022 14:41:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.029903 on epoch=61
06/01/2022 14:41:38 - INFO - __main__ - Global step 550 Train loss 0.023484 ACC 0.8867924528301887 on epoch=61
06/01/2022 14:41:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.006117 on epoch=62
06/01/2022 14:41:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.004852 on epoch=63
06/01/2022 14:41:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.004588 on epoch=64
06/01/2022 14:41:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.003858 on epoch=65
06/01/2022 14:42:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.007114 on epoch=66
06/01/2022 14:42:07 - INFO - __main__ - Global step 600 Train loss 0.005306 ACC 0.9433962264150944 on epoch=66
06/01/2022 14:42:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.005280 on epoch=67
06/01/2022 14:42:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.005084 on epoch=68
06/01/2022 14:42:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.004574 on epoch=69
06/01/2022 14:42:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.002453 on epoch=71
06/01/2022 14:42:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.003917 on epoch=72
06/01/2022 14:42:36 - INFO - __main__ - Global step 650 Train loss 0.004262 ACC 0.9245283018867925 on epoch=72
06/01/2022 14:42:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000854 on epoch=73
06/01/2022 14:42:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.002218 on epoch=74
06/01/2022 14:42:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.001148 on epoch=75
06/01/2022 14:42:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.004203 on epoch=76
06/01/2022 14:43:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.030455 on epoch=77
06/01/2022 14:43:06 - INFO - __main__ - Global step 700 Train loss 0.007776 ACC 0.9339622641509434 on epoch=77
06/01/2022 14:43:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000376 on epoch=78
06/01/2022 14:43:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.004126 on epoch=79
06/01/2022 14:43:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.002056 on epoch=81
06/01/2022 14:43:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000224 on epoch=82
06/01/2022 14:43:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001211 on epoch=83
06/01/2022 14:43:35 - INFO - __main__ - Global step 750 Train loss 0.001599 ACC 0.9245283018867925 on epoch=83
06/01/2022 14:43:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000186 on epoch=84
06/01/2022 14:43:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000206 on epoch=85
06/01/2022 14:43:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.001782 on epoch=86
06/01/2022 14:43:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000543 on epoch=87
06/01/2022 14:44:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.076649 on epoch=88
06/01/2022 14:44:04 - INFO - __main__ - Global step 800 Train loss 0.015873 ACC 0.9245283018867925 on epoch=88
06/01/2022 14:44:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.002453 on epoch=89
06/01/2022 14:44:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000886 on epoch=91
06/01/2022 14:44:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.083027 on epoch=92
06/01/2022 14:44:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000359 on epoch=93
06/01/2022 14:44:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000205 on epoch=94
06/01/2022 14:44:33 - INFO - __main__ - Global step 850 Train loss 0.017386 ACC 0.9245283018867925 on epoch=94
06/01/2022 14:44:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.003019 on epoch=95
06/01/2022 14:44:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000587 on epoch=96
06/01/2022 14:44:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001403 on epoch=97
06/01/2022 14:44:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000228 on epoch=98
06/01/2022 14:45:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000337 on epoch=99
06/01/2022 14:45:02 - INFO - __main__ - Global step 900 Train loss 0.001115 ACC 0.9433962264150944 on epoch=99
06/01/2022 14:45:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000113 on epoch=101
06/01/2022 14:45:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000186 on epoch=102
06/01/2022 14:45:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.001539 on epoch=103
06/01/2022 14:45:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000079 on epoch=104
06/01/2022 14:45:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000118 on epoch=105
06/01/2022 14:45:31 - INFO - __main__ - Global step 950 Train loss 0.000407 ACC 0.9245283018867925 on epoch=105
06/01/2022 14:45:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000307 on epoch=106
06/01/2022 14:45:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000193 on epoch=107
06/01/2022 14:45:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.011314 on epoch=108
06/01/2022 14:45:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000216 on epoch=109
06/01/2022 14:45:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000330 on epoch=111
06/01/2022 14:45:59 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 14:45:59 - INFO - __main__ - Printing 3 examples
06/01/2022 14:45:59 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/01/2022 14:45:59 - INFO - __main__ - ['contradiction']
06/01/2022 14:45:59 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/01/2022 14:45:59 - INFO - __main__ - ['contradiction']
06/01/2022 14:45:59 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/01/2022 14:45:59 - INFO - __main__ - ['contradiction']
06/01/2022 14:45:59 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:45:59 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:45:59 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 14:45:59 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 14:45:59 - INFO - __main__ - Printing 3 examples
06/01/2022 14:45:59 - INFO - __main__ -  [superglue-cb] premise: A: Yeah, that's crazy. B: and then you come here in the Dallas area, um, I don't believe that people should be allowed to carry guns in their vehicles. [SEP] hypothesis: people should be allowed to carry guns in their vehicles
06/01/2022 14:45:59 - INFO - __main__ - ['contradiction']
06/01/2022 14:45:59 - INFO - __main__ -  [superglue-cb] premise: Jed wondered. He 'd scarcely set eyes on him since the night they 'd had dinner together at the house in Westwood. Nobody had mentioned him either and Jed didn't feel he should ask. [SEP] hypothesis: Jed should ask
06/01/2022 14:45:59 - INFO - __main__ - ['contradiction']
06/01/2022 14:45:59 - INFO - __main__ -  [superglue-cb] premise: A: They have to for international trade. B: Yeah. A: But, I guess it's easier to switch back and forth than it used to be, uh, because uh, of computers coming into everything. B: Uh-huh. Yeah, I don't think switching back and forth is that big a deal. [SEP] hypothesis: switching back and forth is that big a deal
06/01/2022 14:45:59 - INFO - __main__ - ['contradiction']
06/01/2022 14:45:59 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:45:59 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:46:00 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 14:46:00 - INFO - __main__ - Global step 1000 Train loss 0.002472 ACC 0.9339622641509434 on epoch=111
06/01/2022 14:46:00 - INFO - __main__ - save last model!
06/01/2022 14:46:07 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 14:46:08 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 14:46:08 - INFO - __main__ - Printing 3 examples
06/01/2022 14:46:08 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 14:46:08 - INFO - __main__ - ['contradiction']
06/01/2022 14:46:08 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 14:46:08 - INFO - __main__ - ['neutral']
06/01/2022 14:46:08 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 14:46:08 - INFO - __main__ - ['entailment']
06/01/2022 14:46:08 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:46:08 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:46:08 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 14:46:10 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_13_0.0002_8_predictions.txt
06/01/2022 14:46:10 - INFO - __main__ - ACC on test data: 0.9286
06/01/2022 14:46:10 - INFO - __main__ - prefix=superglue-cb_64_13, lr=0.0002, bsz=8, dev_performance=0.9433962264150944, test_performance=0.9285714285714286
06/01/2022 14:46:10 - INFO - __main__ - Running ... prefix=superglue-cb_64_13, lr=0.0001, bsz=8 ...
06/01/2022 14:46:11 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 14:46:11 - INFO - __main__ - Printing 3 examples
06/01/2022 14:46:11 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/01/2022 14:46:11 - INFO - __main__ - ['contradiction']
06/01/2022 14:46:11 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/01/2022 14:46:11 - INFO - __main__ - ['contradiction']
06/01/2022 14:46:11 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/01/2022 14:46:11 - INFO - __main__ - ['contradiction']
06/01/2022 14:46:11 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:46:11 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:46:11 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 14:46:11 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 14:46:11 - INFO - __main__ - Printing 3 examples
06/01/2022 14:46:11 - INFO - __main__ -  [superglue-cb] premise: A: Yeah, that's crazy. B: and then you come here in the Dallas area, um, I don't believe that people should be allowed to carry guns in their vehicles. [SEP] hypothesis: people should be allowed to carry guns in their vehicles
06/01/2022 14:46:11 - INFO - __main__ - ['contradiction']
06/01/2022 14:46:11 - INFO - __main__ -  [superglue-cb] premise: Jed wondered. He 'd scarcely set eyes on him since the night they 'd had dinner together at the house in Westwood. Nobody had mentioned him either and Jed didn't feel he should ask. [SEP] hypothesis: Jed should ask
06/01/2022 14:46:11 - INFO - __main__ - ['contradiction']
06/01/2022 14:46:11 - INFO - __main__ -  [superglue-cb] premise: A: They have to for international trade. B: Yeah. A: But, I guess it's easier to switch back and forth than it used to be, uh, because uh, of computers coming into everything. B: Uh-huh. Yeah, I don't think switching back and forth is that big a deal. [SEP] hypothesis: switching back and forth is that big a deal
06/01/2022 14:46:11 - INFO - __main__ - ['contradiction']
06/01/2022 14:46:11 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:46:11 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:46:11 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 14:46:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 14:46:12 - INFO - __main__ - Starting training!
06/01/2022 14:46:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 14:46:22 - INFO - __main__ - Starting training!
06/01/2022 14:46:26 - INFO - __main__ - Step 10 Global step 10 Train loss 25.147173 on epoch=1
06/01/2022 14:46:32 - INFO - __main__ - Step 20 Global step 20 Train loss 20.616001 on epoch=2
06/01/2022 14:46:37 - INFO - __main__ - Step 30 Global step 30 Train loss 16.031706 on epoch=3
06/01/2022 14:46:42 - INFO - __main__ - Step 40 Global step 40 Train loss 12.729773 on epoch=4
06/01/2022 14:46:47 - INFO - __main__ - Step 50 Global step 50 Train loss 12.616648 on epoch=5
06/01/2022 14:46:57 - INFO - __main__ - Global step 50 Train loss 17.428261 ACC 0.12264150943396226 on epoch=5
06/01/2022 14:47:03 - INFO - __main__ - Step 60 Global step 60 Train loss 12.035002 on epoch=6
06/01/2022 14:47:09 - INFO - __main__ - Step 70 Global step 70 Train loss 11.588426 on epoch=7
06/01/2022 14:47:14 - INFO - __main__ - Step 80 Global step 80 Train loss 11.349752 on epoch=8
06/01/2022 14:47:19 - INFO - __main__ - Step 90 Global step 90 Train loss 10.731229 on epoch=9
06/01/2022 14:47:24 - INFO - __main__ - Step 100 Global step 100 Train loss 10.487144 on epoch=11
06/01/2022 14:47:27 - INFO - __main__ - Global step 100 Train loss 11.238312 ACC 0.02830188679245283 on epoch=11
06/01/2022 14:47:32 - INFO - __main__ - Step 110 Global step 110 Train loss 10.472921 on epoch=12
06/01/2022 14:47:38 - INFO - __main__ - Step 120 Global step 120 Train loss 9.976351 on epoch=13
06/01/2022 14:47:43 - INFO - __main__ - Step 130 Global step 130 Train loss 9.987292 on epoch=14
06/01/2022 14:47:48 - INFO - __main__ - Step 140 Global step 140 Train loss 9.408585 on epoch=15
06/01/2022 14:47:53 - INFO - __main__ - Step 150 Global step 150 Train loss 9.209500 on epoch=16
06/01/2022 14:47:57 - INFO - __main__ - Global step 150 Train loss 9.810930 ACC 0.0 on epoch=16
06/01/2022 14:48:02 - INFO - __main__ - Step 160 Global step 160 Train loss 8.894313 on epoch=17
06/01/2022 14:48:07 - INFO - __main__ - Step 170 Global step 170 Train loss 8.991362 on epoch=18
06/01/2022 14:48:13 - INFO - __main__ - Step 180 Global step 180 Train loss 8.177936 on epoch=19
06/01/2022 14:48:18 - INFO - __main__ - Step 190 Global step 190 Train loss 8.270940 on epoch=21
06/01/2022 14:48:23 - INFO - __main__ - Step 200 Global step 200 Train loss 8.261183 on epoch=22
06/01/2022 14:48:26 - INFO - __main__ - Global step 200 Train loss 8.519147 ACC 0.0 on epoch=22
06/01/2022 14:48:31 - INFO - __main__ - Step 210 Global step 210 Train loss 8.106992 on epoch=23
06/01/2022 14:48:36 - INFO - __main__ - Step 220 Global step 220 Train loss 7.716924 on epoch=24
06/01/2022 14:48:41 - INFO - __main__ - Step 230 Global step 230 Train loss 7.721731 on epoch=25
06/01/2022 14:48:47 - INFO - __main__ - Step 240 Global step 240 Train loss 7.464058 on epoch=26
06/01/2022 14:48:52 - INFO - __main__ - Step 250 Global step 250 Train loss 7.733467 on epoch=27
06/01/2022 14:48:55 - INFO - __main__ - Global step 250 Train loss 7.748635 ACC 0.0 on epoch=27
06/01/2022 14:49:00 - INFO - __main__ - Step 260 Global step 260 Train loss 6.734822 on epoch=28
06/01/2022 14:49:05 - INFO - __main__ - Step 270 Global step 270 Train loss 6.870839 on epoch=29
06/01/2022 14:49:10 - INFO - __main__ - Step 280 Global step 280 Train loss 6.328700 on epoch=31
06/01/2022 14:49:16 - INFO - __main__ - Step 290 Global step 290 Train loss 5.734807 on epoch=32
06/01/2022 14:49:21 - INFO - __main__ - Step 300 Global step 300 Train loss 5.896907 on epoch=33
06/01/2022 14:49:23 - INFO - __main__ - Global step 300 Train loss 6.313215 ACC 0.0 on epoch=33
06/01/2022 14:49:29 - INFO - __main__ - Step 310 Global step 310 Train loss 5.922376 on epoch=34
06/01/2022 14:49:34 - INFO - __main__ - Step 320 Global step 320 Train loss 5.070184 on epoch=35
06/01/2022 14:49:39 - INFO - __main__ - Step 330 Global step 330 Train loss 4.999193 on epoch=36
06/01/2022 14:49:44 - INFO - __main__ - Step 340 Global step 340 Train loss 4.811059 on epoch=37
06/01/2022 14:49:50 - INFO - __main__ - Step 350 Global step 350 Train loss 3.903706 on epoch=38
06/01/2022 14:49:52 - INFO - __main__ - Global step 350 Train loss 4.941303 ACC 0.0 on epoch=38
06/01/2022 14:49:57 - INFO - __main__ - Step 360 Global step 360 Train loss 3.778008 on epoch=39
06/01/2022 14:50:02 - INFO - __main__ - Step 370 Global step 370 Train loss 3.406866 on epoch=41
06/01/2022 14:50:08 - INFO - __main__ - Step 380 Global step 380 Train loss 3.466531 on epoch=42
06/01/2022 14:50:13 - INFO - __main__ - Step 390 Global step 390 Train loss 2.363129 on epoch=43
06/01/2022 14:50:18 - INFO - __main__ - Step 400 Global step 400 Train loss 2.089317 on epoch=44
06/01/2022 14:50:20 - INFO - __main__ - Global step 400 Train loss 3.020770 ACC 0.5283018867924528 on epoch=44
06/01/2022 14:50:27 - INFO - __main__ - Step 410 Global step 410 Train loss 2.790720 on epoch=45
06/01/2022 14:50:32 - INFO - __main__ - Step 420 Global step 420 Train loss 2.489173 on epoch=46
06/01/2022 14:50:37 - INFO - __main__ - Step 430 Global step 430 Train loss 2.750859 on epoch=47
06/01/2022 14:50:42 - INFO - __main__ - Step 440 Global step 440 Train loss 2.657162 on epoch=48
06/01/2022 14:50:47 - INFO - __main__ - Step 450 Global step 450 Train loss 2.394661 on epoch=49
06/01/2022 14:50:50 - INFO - __main__ - Global step 450 Train loss 2.616515 ACC 0.5188679245283019 on epoch=49
06/01/2022 14:50:55 - INFO - __main__ - Step 460 Global step 460 Train loss 2.265847 on epoch=51
06/01/2022 14:51:00 - INFO - __main__ - Step 470 Global step 470 Train loss 2.299258 on epoch=52
06/01/2022 14:51:05 - INFO - __main__ - Step 480 Global step 480 Train loss 2.752173 on epoch=53
06/01/2022 14:51:11 - INFO - __main__ - Step 490 Global step 490 Train loss 2.802581 on epoch=54
06/01/2022 14:51:16 - INFO - __main__ - Step 500 Global step 500 Train loss 2.677058 on epoch=55
06/01/2022 14:51:18 - INFO - __main__ - Global step 500 Train loss 2.559383 ACC 0.5188679245283019 on epoch=55
06/01/2022 14:51:23 - INFO - __main__ - Step 510 Global step 510 Train loss 1.894881 on epoch=56
06/01/2022 14:51:29 - INFO - __main__ - Step 520 Global step 520 Train loss 2.342319 on epoch=57
06/01/2022 14:51:34 - INFO - __main__ - Step 530 Global step 530 Train loss 2.570181 on epoch=58
06/01/2022 14:51:39 - INFO - __main__ - Step 540 Global step 540 Train loss 2.303474 on epoch=59
06/01/2022 14:51:44 - INFO - __main__ - Step 550 Global step 550 Train loss 2.435325 on epoch=61
06/01/2022 14:51:47 - INFO - __main__ - Global step 550 Train loss 2.309236 ACC 0.5188679245283019 on epoch=61
06/01/2022 14:51:52 - INFO - __main__ - Step 560 Global step 560 Train loss 2.294615 on epoch=62
06/01/2022 14:51:57 - INFO - __main__ - Step 570 Global step 570 Train loss 2.632615 on epoch=63
06/01/2022 14:52:02 - INFO - __main__ - Step 580 Global step 580 Train loss 2.116267 on epoch=64
06/01/2022 14:52:08 - INFO - __main__ - Step 590 Global step 590 Train loss 1.763212 on epoch=65
06/01/2022 14:52:13 - INFO - __main__ - Step 600 Global step 600 Train loss 1.985624 on epoch=66
06/01/2022 14:52:15 - INFO - __main__ - Global step 600 Train loss 2.158467 ACC 0.5188679245283019 on epoch=66
06/01/2022 14:52:20 - INFO - __main__ - Step 610 Global step 610 Train loss 1.848351 on epoch=67
06/01/2022 14:52:25 - INFO - __main__ - Step 620 Global step 620 Train loss 1.586195 on epoch=68
06/01/2022 14:52:31 - INFO - __main__ - Step 630 Global step 630 Train loss 1.854132 on epoch=69
06/01/2022 14:52:36 - INFO - __main__ - Step 640 Global step 640 Train loss 1.767596 on epoch=71
06/01/2022 14:52:41 - INFO - __main__ - Step 650 Global step 650 Train loss 1.872900 on epoch=72
06/01/2022 14:52:43 - INFO - __main__ - Global step 650 Train loss 1.785835 ACC 0.5188679245283019 on epoch=72
06/01/2022 14:52:49 - INFO - __main__ - Step 660 Global step 660 Train loss 1.844959 on epoch=73
06/01/2022 14:52:54 - INFO - __main__ - Step 670 Global step 670 Train loss 1.719749 on epoch=74
06/01/2022 14:52:59 - INFO - __main__ - Step 680 Global step 680 Train loss 1.673323 on epoch=75
06/01/2022 14:53:04 - INFO - __main__ - Step 690 Global step 690 Train loss 1.494144 on epoch=76
06/01/2022 14:53:10 - INFO - __main__ - Step 700 Global step 700 Train loss 1.583122 on epoch=77
06/01/2022 14:53:12 - INFO - __main__ - Global step 700 Train loss 1.663059 ACC 0.5188679245283019 on epoch=77
06/01/2022 14:53:17 - INFO - __main__ - Step 710 Global step 710 Train loss 1.462424 on epoch=78
06/01/2022 14:53:22 - INFO - __main__ - Step 720 Global step 720 Train loss 1.511942 on epoch=79
06/01/2022 14:53:27 - INFO - __main__ - Step 730 Global step 730 Train loss 1.908863 on epoch=81
06/01/2022 14:53:33 - INFO - __main__ - Step 740 Global step 740 Train loss 1.616697 on epoch=82
06/01/2022 14:53:38 - INFO - __main__ - Step 750 Global step 750 Train loss 1.607261 on epoch=83
06/01/2022 14:53:40 - INFO - __main__ - Global step 750 Train loss 1.621437 ACC 0.5188679245283019 on epoch=83
06/01/2022 14:53:45 - INFO - __main__ - Step 760 Global step 760 Train loss 1.457984 on epoch=84
06/01/2022 14:53:51 - INFO - __main__ - Step 770 Global step 770 Train loss 1.734875 on epoch=85
06/01/2022 14:53:56 - INFO - __main__ - Step 780 Global step 780 Train loss 1.334678 on epoch=86
06/01/2022 14:54:01 - INFO - __main__ - Step 790 Global step 790 Train loss 1.377730 on epoch=87
06/01/2022 14:54:06 - INFO - __main__ - Step 800 Global step 800 Train loss 1.623278 on epoch=88
06/01/2022 14:54:09 - INFO - __main__ - Global step 800 Train loss 1.505709 ACC 0.5188679245283019 on epoch=88
06/01/2022 14:54:14 - INFO - __main__ - Step 810 Global step 810 Train loss 1.394113 on epoch=89
06/01/2022 14:54:19 - INFO - __main__ - Step 820 Global step 820 Train loss 1.486911 on epoch=91
06/01/2022 14:54:24 - INFO - __main__ - Step 830 Global step 830 Train loss 1.517009 on epoch=92
06/01/2022 14:54:30 - INFO - __main__ - Step 840 Global step 840 Train loss 1.220236 on epoch=93
06/01/2022 14:54:35 - INFO - __main__ - Step 850 Global step 850 Train loss 1.585908 on epoch=94
06/01/2022 14:54:37 - INFO - __main__ - Global step 850 Train loss 1.440835 ACC 0.5188679245283019 on epoch=94
06/01/2022 14:54:42 - INFO - __main__ - Step 860 Global step 860 Train loss 1.324995 on epoch=95
06/01/2022 14:54:47 - INFO - __main__ - Step 870 Global step 870 Train loss 1.368029 on epoch=96
06/01/2022 14:54:52 - INFO - __main__ - Step 880 Global step 880 Train loss 1.508042 on epoch=97
06/01/2022 14:54:58 - INFO - __main__ - Step 890 Global step 890 Train loss 1.269219 on epoch=98
06/01/2022 14:55:03 - INFO - __main__ - Step 900 Global step 900 Train loss 1.074272 on epoch=99
06/01/2022 14:55:05 - INFO - __main__ - Global step 900 Train loss 1.308911 ACC 0.5188679245283019 on epoch=99
06/01/2022 14:55:10 - INFO - __main__ - Step 910 Global step 910 Train loss 1.195200 on epoch=101
06/01/2022 14:55:16 - INFO - __main__ - Step 920 Global step 920 Train loss 1.329290 on epoch=102
06/01/2022 14:55:21 - INFO - __main__ - Step 930 Global step 930 Train loss 1.144033 on epoch=103
06/01/2022 14:55:26 - INFO - __main__ - Step 940 Global step 940 Train loss 1.099733 on epoch=104
06/01/2022 14:55:31 - INFO - __main__ - Step 950 Global step 950 Train loss 1.351343 on epoch=105
06/01/2022 14:55:34 - INFO - __main__ - Global step 950 Train loss 1.223920 ACC 0.5188679245283019 on epoch=105
06/01/2022 14:55:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.980244 on epoch=106
06/01/2022 14:55:44 - INFO - __main__ - Step 970 Global step 970 Train loss 1.250939 on epoch=107
06/01/2022 14:55:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.992710 on epoch=108
06/01/2022 14:55:55 - INFO - __main__ - Step 990 Global step 990 Train loss 1.053102 on epoch=109
06/01/2022 14:56:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.961216 on epoch=111
06/01/2022 14:56:01 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 14:56:01 - INFO - __main__ - Printing 3 examples
06/01/2022 14:56:01 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/01/2022 14:56:01 - INFO - __main__ - ['contradiction']
06/01/2022 14:56:01 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/01/2022 14:56:01 - INFO - __main__ - ['contradiction']
06/01/2022 14:56:01 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/01/2022 14:56:01 - INFO - __main__ - ['contradiction']
06/01/2022 14:56:01 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:56:01 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:56:01 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 14:56:01 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 14:56:01 - INFO - __main__ - Printing 3 examples
06/01/2022 14:56:01 - INFO - __main__ -  [superglue-cb] premise: B: I wouldn't be surprised. A: You know, because they don't want to send them to daycare. B: I doubt if they would say it was too long. [SEP] hypothesis: it was too long
06/01/2022 14:56:01 - INFO - __main__ - ['contradiction']
06/01/2022 14:56:01 - INFO - __main__ -  [superglue-cb] premise: Most of them young, about his age, stood and talked and drank and laughed. The two girls he had noticed earlier were standing talking to some other girls. Graham hoped they all realised that just because he was standing talking to Slater that didn't mean he was gay too. [SEP] hypothesis: Graham was gay too
06/01/2022 14:56:01 - INFO - __main__ - ['contradiction']
06/01/2022 14:56:01 - INFO - __main__ -  [superglue-cb] premise: A: so you hear so much, you get a little tired of it, but then again, so many people you got to understand only catch it once a day, maybe B: Yep. A: but I think the quality of uh, our news is just, uh, I don't believe it could be better [SEP] hypothesis: it could be better
06/01/2022 14:56:01 - INFO - __main__ - ['contradiction']
06/01/2022 14:56:01 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:56:01 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:56:02 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 14:56:02 - INFO - __main__ - Global step 1000 Train loss 1.047642 ACC 0.5188679245283019 on epoch=111
06/01/2022 14:56:02 - INFO - __main__ - save last model!
06/01/2022 14:56:10 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 14:56:10 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 14:56:10 - INFO - __main__ - Printing 3 examples
06/01/2022 14:56:10 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 14:56:10 - INFO - __main__ - ['contradiction']
06/01/2022 14:56:10 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 14:56:10 - INFO - __main__ - ['neutral']
06/01/2022 14:56:10 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 14:56:10 - INFO - __main__ - ['entailment']
06/01/2022 14:56:10 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:56:10 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:56:10 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 14:56:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_13_0.0001_8_predictions.txt
06/01/2022 14:56:12 - INFO - __main__ - ACC on test data: 0.5000
06/01/2022 14:56:12 - INFO - __main__ - prefix=superglue-cb_64_13, lr=0.0001, bsz=8, dev_performance=0.5283018867924528, test_performance=0.5
06/01/2022 14:56:12 - INFO - __main__ - Running ... prefix=superglue-cb_64_21, lr=0.0005, bsz=8 ...
06/01/2022 14:56:13 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 14:56:13 - INFO - __main__ - Printing 3 examples
06/01/2022 14:56:13 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/01/2022 14:56:13 - INFO - __main__ - ['contradiction']
06/01/2022 14:56:13 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/01/2022 14:56:13 - INFO - __main__ - ['contradiction']
06/01/2022 14:56:13 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/01/2022 14:56:13 - INFO - __main__ - ['contradiction']
06/01/2022 14:56:13 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:56:13 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:56:13 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 14:56:13 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 14:56:13 - INFO - __main__ - Printing 3 examples
06/01/2022 14:56:13 - INFO - __main__ -  [superglue-cb] premise: B: I wouldn't be surprised. A: You know, because they don't want to send them to daycare. B: I doubt if they would say it was too long. [SEP] hypothesis: it was too long
06/01/2022 14:56:13 - INFO - __main__ - ['contradiction']
06/01/2022 14:56:13 - INFO - __main__ -  [superglue-cb] premise: Most of them young, about his age, stood and talked and drank and laughed. The two girls he had noticed earlier were standing talking to some other girls. Graham hoped they all realised that just because he was standing talking to Slater that didn't mean he was gay too. [SEP] hypothesis: Graham was gay too
06/01/2022 14:56:13 - INFO - __main__ - ['contradiction']
06/01/2022 14:56:13 - INFO - __main__ -  [superglue-cb] premise: A: so you hear so much, you get a little tired of it, but then again, so many people you got to understand only catch it once a day, maybe B: Yep. A: but I think the quality of uh, our news is just, uh, I don't believe it could be better [SEP] hypothesis: it could be better
06/01/2022 14:56:13 - INFO - __main__ - ['contradiction']
06/01/2022 14:56:13 - INFO - __main__ - Tokenizing Input ...
06/01/2022 14:56:13 - INFO - __main__ - Tokenizing Output ...
06/01/2022 14:56:13 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 14:56:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 14:56:15 - INFO - __main__ - Starting training!
06/01/2022 14:56:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 14:56:24 - INFO - __main__ - Starting training!
06/01/2022 14:56:29 - INFO - __main__ - Step 10 Global step 10 Train loss 23.440922 on epoch=1
06/01/2022 14:56:34 - INFO - __main__ - Step 20 Global step 20 Train loss 15.554907 on epoch=2
06/01/2022 14:56:39 - INFO - __main__ - Step 30 Global step 30 Train loss 11.417649 on epoch=3
06/01/2022 14:56:45 - INFO - __main__ - Step 40 Global step 40 Train loss 9.311213 on epoch=4
06/01/2022 14:56:50 - INFO - __main__ - Step 50 Global step 50 Train loss 8.545059 on epoch=5
06/01/2022 14:56:52 - INFO - __main__ - Global step 50 Train loss 13.653950 ACC 0.0 on epoch=5
06/01/2022 14:56:58 - INFO - __main__ - Step 60 Global step 60 Train loss 7.460855 on epoch=6
06/01/2022 14:57:03 - INFO - __main__ - Step 70 Global step 70 Train loss 5.856104 on epoch=7
06/01/2022 14:57:09 - INFO - __main__ - Step 80 Global step 80 Train loss 4.236524 on epoch=8
06/01/2022 14:57:14 - INFO - __main__ - Step 90 Global step 90 Train loss 2.857064 on epoch=9
06/01/2022 14:57:19 - INFO - __main__ - Step 100 Global step 100 Train loss 1.805270 on epoch=11
06/01/2022 14:57:50 - INFO - __main__ - Global step 100 Train loss 4.443163 ACC 0.5188679245283019 on epoch=11
06/01/2022 14:57:56 - INFO - __main__ - Step 110 Global step 110 Train loss 1.825682 on epoch=12
06/01/2022 14:58:01 - INFO - __main__ - Step 120 Global step 120 Train loss 2.592306 on epoch=13
06/01/2022 14:58:07 - INFO - __main__ - Step 130 Global step 130 Train loss 1.575133 on epoch=14
06/01/2022 14:58:12 - INFO - __main__ - Step 140 Global step 140 Train loss 1.983454 on epoch=15
06/01/2022 14:58:17 - INFO - __main__ - Step 150 Global step 150 Train loss 1.913110 on epoch=16
06/01/2022 14:58:19 - INFO - __main__ - Global step 150 Train loss 1.977937 ACC 0.5188679245283019 on epoch=16
06/01/2022 14:58:24 - INFO - __main__ - Step 160 Global step 160 Train loss 1.609604 on epoch=17
06/01/2022 14:58:30 - INFO - __main__ - Step 170 Global step 170 Train loss 1.593615 on epoch=18
06/01/2022 14:58:35 - INFO - __main__ - Step 180 Global step 180 Train loss 1.697467 on epoch=19
06/01/2022 14:58:40 - INFO - __main__ - Step 190 Global step 190 Train loss 1.185511 on epoch=21
06/01/2022 14:58:45 - INFO - __main__ - Step 200 Global step 200 Train loss 1.181281 on epoch=22
06/01/2022 14:58:47 - INFO - __main__ - Global step 200 Train loss 1.453496 ACC 0.5188679245283019 on epoch=22
06/01/2022 14:58:53 - INFO - __main__ - Step 210 Global step 210 Train loss 1.106579 on epoch=23
06/01/2022 14:58:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.997043 on epoch=24
06/01/2022 14:59:03 - INFO - __main__ - Step 230 Global step 230 Train loss 1.074616 on epoch=25
06/01/2022 14:59:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.872293 on epoch=26
06/01/2022 14:59:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.608645 on epoch=27
06/01/2022 14:59:16 - INFO - __main__ - Global step 250 Train loss 0.931835 ACC 0.7075471698113207 on epoch=27
06/01/2022 14:59:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.843041 on epoch=28
06/01/2022 14:59:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.458615 on epoch=29
06/01/2022 14:59:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.727459 on epoch=31
06/01/2022 14:59:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.568512 on epoch=32
06/01/2022 14:59:42 - INFO - __main__ - Step 300 Global step 300 Train loss 1.440276 on epoch=33
06/01/2022 14:59:45 - INFO - __main__ - Global step 300 Train loss 0.807581 ACC 0.7735849056603774 on epoch=33
06/01/2022 14:59:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.383803 on epoch=34
06/01/2022 14:59:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.299793 on epoch=35
06/01/2022 15:00:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.312398 on epoch=36
06/01/2022 15:00:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.221615 on epoch=37
06/01/2022 15:00:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.181147 on epoch=38
06/01/2022 15:00:15 - INFO - __main__ - Global step 350 Train loss 0.279751 ACC 0.7169811320754716 on epoch=38
06/01/2022 15:00:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.205178 on epoch=39
06/01/2022 15:00:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.163163 on epoch=41
06/01/2022 15:00:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.088483 on epoch=42
06/01/2022 15:00:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.098352 on epoch=43
06/01/2022 15:00:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.059361 on epoch=44
06/01/2022 15:00:43 - INFO - __main__ - Global step 400 Train loss 0.122907 ACC 0.7547169811320755 on epoch=44
06/01/2022 15:00:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.019364 on epoch=45
06/01/2022 15:00:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.012161 on epoch=46
06/01/2022 15:00:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.003631 on epoch=47
06/01/2022 15:01:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.007948 on epoch=48
06/01/2022 15:01:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.002092 on epoch=49
06/01/2022 15:01:12 - INFO - __main__ - Global step 450 Train loss 0.009039 ACC 0.8113207547169812 on epoch=49
06/01/2022 15:01:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.002535 on epoch=51
06/01/2022 15:01:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.000505 on epoch=52
06/01/2022 15:01:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.086919 on epoch=53
06/01/2022 15:01:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.003317 on epoch=54
06/01/2022 15:01:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.001200 on epoch=55
06/01/2022 15:01:41 - INFO - __main__ - Global step 500 Train loss 0.018895 ACC 0.839622641509434 on epoch=55
06/01/2022 15:01:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.008478 on epoch=56
06/01/2022 15:01:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.002403 on epoch=57
06/01/2022 15:01:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.001388 on epoch=58
06/01/2022 15:02:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.051284 on epoch=59
06/01/2022 15:02:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.001192 on epoch=61
06/01/2022 15:02:10 - INFO - __main__ - Global step 550 Train loss 0.012949 ACC 0.8113207547169812 on epoch=61
06/01/2022 15:02:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001898 on epoch=62
06/01/2022 15:02:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.025250 on epoch=63
06/01/2022 15:02:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.004704 on epoch=64
06/01/2022 15:02:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.005388 on epoch=65
06/01/2022 15:02:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.028115 on epoch=66
06/01/2022 15:02:39 - INFO - __main__ - Global step 600 Train loss 0.013071 ACC 0.9056603773584906 on epoch=66
06/01/2022 15:02:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.002782 on epoch=67
06/01/2022 15:02:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.001046 on epoch=68
06/01/2022 15:02:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.078194 on epoch=69
06/01/2022 15:03:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.002931 on epoch=71
06/01/2022 15:03:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000356 on epoch=72
06/01/2022 15:03:08 - INFO - __main__ - Global step 650 Train loss 0.017062 ACC 0.8301886792452831 on epoch=72
06/01/2022 15:03:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000175 on epoch=73
06/01/2022 15:03:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.019874 on epoch=74
06/01/2022 15:03:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000258 on epoch=75
06/01/2022 15:03:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000077 on epoch=76
06/01/2022 15:03:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000078 on epoch=77
06/01/2022 15:03:37 - INFO - __main__ - Global step 700 Train loss 0.004092 ACC 0.8679245283018868 on epoch=77
06/01/2022 15:03:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000226 on epoch=78
06/01/2022 15:03:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000082 on epoch=79
06/01/2022 15:03:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000124 on epoch=81
06/01/2022 15:03:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.002080 on epoch=82
06/01/2022 15:04:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000146 on epoch=83
06/01/2022 15:04:05 - INFO - __main__ - Global step 750 Train loss 0.000532 ACC 0.839622641509434 on epoch=83
06/01/2022 15:04:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000120 on epoch=84
06/01/2022 15:04:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000091 on epoch=85
06/01/2022 15:04:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000190 on epoch=86
06/01/2022 15:04:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000280 on epoch=87
06/01/2022 15:04:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000097 on epoch=88
06/01/2022 15:04:34 - INFO - __main__ - Global step 800 Train loss 0.000156 ACC 0.839622641509434 on epoch=88
06/01/2022 15:04:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000059 on epoch=89
06/01/2022 15:04:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000021 on epoch=91
06/01/2022 15:04:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000027 on epoch=92
06/01/2022 15:04:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000075 on epoch=93
06/01/2022 15:04:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000049 on epoch=94
06/01/2022 15:05:02 - INFO - __main__ - Global step 850 Train loss 0.000046 ACC 0.839622641509434 on epoch=94
06/01/2022 15:05:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000026 on epoch=95
06/01/2022 15:05:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000034 on epoch=96
06/01/2022 15:05:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000037 on epoch=97
06/01/2022 15:05:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000030 on epoch=98
06/01/2022 15:05:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000049 on epoch=99
06/01/2022 15:05:31 - INFO - __main__ - Global step 900 Train loss 0.000035 ACC 0.839622641509434 on epoch=99
06/01/2022 15:05:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000043 on epoch=101
06/01/2022 15:05:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000015 on epoch=102
06/01/2022 15:05:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000029 on epoch=103
06/01/2022 15:05:51 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000035 on epoch=104
06/01/2022 15:05:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000015 on epoch=105
06/01/2022 15:05:59 - INFO - __main__ - Global step 950 Train loss 0.000028 ACC 0.839622641509434 on epoch=105
06/01/2022 15:06:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000024 on epoch=106
06/01/2022 15:06:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000011 on epoch=107
06/01/2022 15:06:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000033 on epoch=108
06/01/2022 15:06:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000010 on epoch=109
06/01/2022 15:06:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000015 on epoch=111
06/01/2022 15:06:26 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 15:06:26 - INFO - __main__ - Printing 3 examples
06/01/2022 15:06:26 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/01/2022 15:06:26 - INFO - __main__ - ['contradiction']
06/01/2022 15:06:26 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/01/2022 15:06:26 - INFO - __main__ - ['contradiction']
06/01/2022 15:06:26 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/01/2022 15:06:26 - INFO - __main__ - ['contradiction']
06/01/2022 15:06:26 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:06:26 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:06:26 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 15:06:26 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 15:06:26 - INFO - __main__ - Printing 3 examples
06/01/2022 15:06:26 - INFO - __main__ -  [superglue-cb] premise: B: I wouldn't be surprised. A: You know, because they don't want to send them to daycare. B: I doubt if they would say it was too long. [SEP] hypothesis: it was too long
06/01/2022 15:06:26 - INFO - __main__ - ['contradiction']
06/01/2022 15:06:26 - INFO - __main__ -  [superglue-cb] premise: Most of them young, about his age, stood and talked and drank and laughed. The two girls he had noticed earlier were standing talking to some other girls. Graham hoped they all realised that just because he was standing talking to Slater that didn't mean he was gay too. [SEP] hypothesis: Graham was gay too
06/01/2022 15:06:26 - INFO - __main__ - ['contradiction']
06/01/2022 15:06:26 - INFO - __main__ -  [superglue-cb] premise: A: so you hear so much, you get a little tired of it, but then again, so many people you got to understand only catch it once a day, maybe B: Yep. A: but I think the quality of uh, our news is just, uh, I don't believe it could be better [SEP] hypothesis: it could be better
06/01/2022 15:06:26 - INFO - __main__ - ['contradiction']
06/01/2022 15:06:26 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:06:26 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:06:27 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 15:06:27 - INFO - __main__ - Global step 1000 Train loss 0.000019 ACC 0.839622641509434 on epoch=111
06/01/2022 15:06:27 - INFO - __main__ - save last model!
06/01/2022 15:06:34 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 15:06:35 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 15:06:35 - INFO - __main__ - Printing 3 examples
06/01/2022 15:06:35 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 15:06:35 - INFO - __main__ - ['contradiction']
06/01/2022 15:06:35 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 15:06:35 - INFO - __main__ - ['neutral']
06/01/2022 15:06:35 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 15:06:35 - INFO - __main__ - ['entailment']
06/01/2022 15:06:35 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:06:35 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:06:35 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 15:06:37 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_21_0.0005_8_predictions.txt
06/01/2022 15:06:37 - INFO - __main__ - ACC on test data: 0.8750
06/01/2022 15:06:37 - INFO - __main__ - prefix=superglue-cb_64_21, lr=0.0005, bsz=8, dev_performance=0.9056603773584906, test_performance=0.875
06/01/2022 15:06:37 - INFO - __main__ - Running ... prefix=superglue-cb_64_21, lr=0.0003, bsz=8 ...
06/01/2022 15:06:38 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 15:06:38 - INFO - __main__ - Printing 3 examples
06/01/2022 15:06:38 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/01/2022 15:06:38 - INFO - __main__ - ['contradiction']
06/01/2022 15:06:38 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/01/2022 15:06:38 - INFO - __main__ - ['contradiction']
06/01/2022 15:06:38 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/01/2022 15:06:38 - INFO - __main__ - ['contradiction']
06/01/2022 15:06:38 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:06:38 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:06:38 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 15:06:38 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 15:06:38 - INFO - __main__ - Printing 3 examples
06/01/2022 15:06:38 - INFO - __main__ -  [superglue-cb] premise: B: I wouldn't be surprised. A: You know, because they don't want to send them to daycare. B: I doubt if they would say it was too long. [SEP] hypothesis: it was too long
06/01/2022 15:06:38 - INFO - __main__ - ['contradiction']
06/01/2022 15:06:38 - INFO - __main__ -  [superglue-cb] premise: Most of them young, about his age, stood and talked and drank and laughed. The two girls he had noticed earlier were standing talking to some other girls. Graham hoped they all realised that just because he was standing talking to Slater that didn't mean he was gay too. [SEP] hypothesis: Graham was gay too
06/01/2022 15:06:38 - INFO - __main__ - ['contradiction']
06/01/2022 15:06:38 - INFO - __main__ -  [superglue-cb] premise: A: so you hear so much, you get a little tired of it, but then again, so many people you got to understand only catch it once a day, maybe B: Yep. A: but I think the quality of uh, our news is just, uh, I don't believe it could be better [SEP] hypothesis: it could be better
06/01/2022 15:06:38 - INFO - __main__ - ['contradiction']
06/01/2022 15:06:38 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:06:38 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:06:38 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 15:06:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 15:06:39 - INFO - __main__ - Starting training!
06/01/2022 15:06:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 15:06:52 - INFO - __main__ - Starting training!
06/01/2022 15:06:57 - INFO - __main__ - Step 10 Global step 10 Train loss 23.653017 on epoch=1
06/01/2022 15:07:03 - INFO - __main__ - Step 20 Global step 20 Train loss 16.646458 on epoch=2
06/01/2022 15:07:08 - INFO - __main__ - Step 30 Global step 30 Train loss 12.806215 on epoch=3
06/01/2022 15:07:14 - INFO - __main__ - Step 40 Global step 40 Train loss 11.664814 on epoch=4
06/01/2022 15:07:20 - INFO - __main__ - Step 50 Global step 50 Train loss 10.510106 on epoch=5
06/01/2022 15:07:28 - INFO - __main__ - Global step 50 Train loss 15.056123 ACC 0.04716981132075472 on epoch=5
06/01/2022 15:07:34 - INFO - __main__ - Step 60 Global step 60 Train loss 10.510885 on epoch=6
06/01/2022 15:07:40 - INFO - __main__ - Step 70 Global step 70 Train loss 8.982168 on epoch=7
06/01/2022 15:07:46 - INFO - __main__ - Step 80 Global step 80 Train loss 8.361814 on epoch=8
06/01/2022 15:07:52 - INFO - __main__ - Step 90 Global step 90 Train loss 7.722266 on epoch=9
06/01/2022 15:07:57 - INFO - __main__ - Step 100 Global step 100 Train loss 6.959458 on epoch=11
06/01/2022 15:08:00 - INFO - __main__ - Global step 100 Train loss 8.507318 ACC 0.018867924528301886 on epoch=11
06/01/2022 15:08:06 - INFO - __main__ - Step 110 Global step 110 Train loss 6.661760 on epoch=12
06/01/2022 15:08:11 - INFO - __main__ - Step 120 Global step 120 Train loss 5.032765 on epoch=13
06/01/2022 15:08:16 - INFO - __main__ - Step 130 Global step 130 Train loss 3.860115 on epoch=14
06/01/2022 15:08:21 - INFO - __main__ - Step 140 Global step 140 Train loss 3.609350 on epoch=15
06/01/2022 15:08:26 - INFO - __main__ - Step 150 Global step 150 Train loss 3.617733 on epoch=16
06/01/2022 15:08:28 - INFO - __main__ - Global step 150 Train loss 4.556345 ACC 0.5188679245283019 on epoch=16
06/01/2022 15:08:34 - INFO - __main__ - Step 160 Global step 160 Train loss 2.631192 on epoch=17
06/01/2022 15:08:39 - INFO - __main__ - Step 170 Global step 170 Train loss 2.339667 on epoch=18
06/01/2022 15:08:44 - INFO - __main__ - Step 180 Global step 180 Train loss 2.450845 on epoch=19
06/01/2022 15:08:50 - INFO - __main__ - Step 190 Global step 190 Train loss 1.968271 on epoch=21
06/01/2022 15:08:55 - INFO - __main__ - Step 200 Global step 200 Train loss 2.211967 on epoch=22
06/01/2022 15:08:57 - INFO - __main__ - Global step 200 Train loss 2.320389 ACC 0.5188679245283019 on epoch=22
06/01/2022 15:09:02 - INFO - __main__ - Step 210 Global step 210 Train loss 1.838281 on epoch=23
06/01/2022 15:09:07 - INFO - __main__ - Step 220 Global step 220 Train loss 1.906335 on epoch=24
06/01/2022 15:09:12 - INFO - __main__ - Step 230 Global step 230 Train loss 2.049459 on epoch=25
06/01/2022 15:09:17 - INFO - __main__ - Step 240 Global step 240 Train loss 1.522309 on epoch=26
06/01/2022 15:09:23 - INFO - __main__ - Step 250 Global step 250 Train loss 1.354470 on epoch=27
06/01/2022 15:09:25 - INFO - __main__ - Global step 250 Train loss 1.734171 ACC 0.5188679245283019 on epoch=27
06/01/2022 15:09:30 - INFO - __main__ - Step 260 Global step 260 Train loss 1.505681 on epoch=28
06/01/2022 15:09:35 - INFO - __main__ - Step 270 Global step 270 Train loss 1.301862 on epoch=29
06/01/2022 15:09:40 - INFO - __main__ - Step 280 Global step 280 Train loss 1.350408 on epoch=31
06/01/2022 15:09:45 - INFO - __main__ - Step 290 Global step 290 Train loss 1.231961 on epoch=32
06/01/2022 15:09:50 - INFO - __main__ - Step 300 Global step 300 Train loss 1.436890 on epoch=33
06/01/2022 15:09:53 - INFO - __main__ - Global step 300 Train loss 1.365360 ACC 0.5188679245283019 on epoch=33
06/01/2022 15:09:58 - INFO - __main__ - Step 310 Global step 310 Train loss 1.533519 on epoch=34
06/01/2022 15:10:03 - INFO - __main__ - Step 320 Global step 320 Train loss 1.532160 on epoch=35
06/01/2022 15:10:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.886317 on epoch=36
06/01/2022 15:10:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.975686 on epoch=37
06/01/2022 15:10:19 - INFO - __main__ - Step 350 Global step 350 Train loss 1.212319 on epoch=38
06/01/2022 15:10:21 - INFO - __main__ - Global step 350 Train loss 1.228000 ACC 0.5188679245283019 on epoch=38
06/01/2022 15:10:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.804534 on epoch=39
06/01/2022 15:10:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.842822 on epoch=41
06/01/2022 15:10:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.771078 on epoch=42
06/01/2022 15:10:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.762499 on epoch=43
06/01/2022 15:10:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.591280 on epoch=44
06/01/2022 15:10:49 - INFO - __main__ - Global step 400 Train loss 0.754443 ACC 0.7452830188679245 on epoch=44
06/01/2022 15:10:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.618388 on epoch=45
06/01/2022 15:11:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.637794 on epoch=46
06/01/2022 15:11:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.652262 on epoch=47
06/01/2022 15:11:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.514128 on epoch=48
06/01/2022 15:11:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.382363 on epoch=49
06/01/2022 15:11:19 - INFO - __main__ - Global step 450 Train loss 0.560987 ACC 0.7547169811320755 on epoch=49
06/01/2022 15:11:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.330044 on epoch=51
06/01/2022 15:11:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.354857 on epoch=52
06/01/2022 15:11:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.326576 on epoch=53
06/01/2022 15:11:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.259238 on epoch=54
06/01/2022 15:11:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.302694 on epoch=55
06/01/2022 15:11:48 - INFO - __main__ - Global step 500 Train loss 0.314682 ACC 0.5566037735849056 on epoch=55
06/01/2022 15:11:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.273911 on epoch=56
06/01/2022 15:11:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.218377 on epoch=57
06/01/2022 15:12:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.158731 on epoch=58
06/01/2022 15:12:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.181739 on epoch=59
06/01/2022 15:12:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.188239 on epoch=61
06/01/2022 15:12:16 - INFO - __main__ - Global step 550 Train loss 0.204199 ACC 0.7924528301886793 on epoch=61
06/01/2022 15:12:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.111520 on epoch=62
06/01/2022 15:12:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.154309 on epoch=63
06/01/2022 15:12:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.092561 on epoch=64
06/01/2022 15:12:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.129885 on epoch=65
06/01/2022 15:12:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.055579 on epoch=66
06/01/2022 15:12:45 - INFO - __main__ - Global step 600 Train loss 0.108771 ACC 0.8207547169811321 on epoch=66
06/01/2022 15:12:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.079556 on epoch=67
06/01/2022 15:12:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.063554 on epoch=68
06/01/2022 15:13:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.016115 on epoch=69
06/01/2022 15:13:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.026233 on epoch=71
06/01/2022 15:13:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.128264 on epoch=72
06/01/2022 15:13:14 - INFO - __main__ - Global step 650 Train loss 0.062744 ACC 0.9433962264150944 on epoch=72
06/01/2022 15:13:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.058290 on epoch=73
06/01/2022 15:13:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.032669 on epoch=74
06/01/2022 15:13:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.065891 on epoch=75
06/01/2022 15:13:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.138309 on epoch=76
06/01/2022 15:13:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.092925 on epoch=77
06/01/2022 15:13:43 - INFO - __main__ - Global step 700 Train loss 0.077617 ACC 0.7452830188679245 on epoch=77
06/01/2022 15:13:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.187339 on epoch=78
06/01/2022 15:13:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.226266 on epoch=79
06/01/2022 15:13:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.220343 on epoch=81
06/01/2022 15:14:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.188608 on epoch=82
06/01/2022 15:14:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.162458 on epoch=83
06/01/2022 15:14:12 - INFO - __main__ - Global step 750 Train loss 0.197003 ACC 0.7452830188679245 on epoch=83
06/01/2022 15:14:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.225465 on epoch=84
06/01/2022 15:14:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.164705 on epoch=85
06/01/2022 15:14:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.256032 on epoch=86
06/01/2022 15:14:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.136810 on epoch=87
06/01/2022 15:14:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.157525 on epoch=88
06/01/2022 15:14:40 - INFO - __main__ - Global step 800 Train loss 0.188107 ACC 0.6981132075471698 on epoch=88
06/01/2022 15:14:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.146141 on epoch=89
06/01/2022 15:14:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.136201 on epoch=91
06/01/2022 15:14:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.158333 on epoch=92
06/01/2022 15:15:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.164192 on epoch=93
06/01/2022 15:15:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.138922 on epoch=94
06/01/2022 15:15:08 - INFO - __main__ - Global step 850 Train loss 0.148758 ACC 0.6226415094339622 on epoch=94
06/01/2022 15:15:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.125860 on epoch=95
06/01/2022 15:15:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.126505 on epoch=96
06/01/2022 15:15:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.055354 on epoch=97
06/01/2022 15:15:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.067428 on epoch=98
06/01/2022 15:15:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.043823 on epoch=99
06/01/2022 15:15:37 - INFO - __main__ - Global step 900 Train loss 0.083794 ACC 0.7547169811320755 on epoch=99
06/01/2022 15:15:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.041232 on epoch=101
06/01/2022 15:15:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.010601 on epoch=102
06/01/2022 15:15:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.084758 on epoch=103
06/01/2022 15:15:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.040438 on epoch=104
06/01/2022 15:16:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.079667 on epoch=105
06/01/2022 15:16:05 - INFO - __main__ - Global step 950 Train loss 0.051339 ACC 0.7547169811320755 on epoch=105
06/01/2022 15:16:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.067748 on epoch=106
06/01/2022 15:16:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.039290 on epoch=107
06/01/2022 15:16:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.045289 on epoch=108
06/01/2022 15:16:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.019559 on epoch=109
06/01/2022 15:16:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.022807 on epoch=111
06/01/2022 15:16:32 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 15:16:32 - INFO - __main__ - Printing 3 examples
06/01/2022 15:16:32 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/01/2022 15:16:32 - INFO - __main__ - ['contradiction']
06/01/2022 15:16:32 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/01/2022 15:16:32 - INFO - __main__ - ['contradiction']
06/01/2022 15:16:32 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/01/2022 15:16:32 - INFO - __main__ - ['contradiction']
06/01/2022 15:16:32 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:16:32 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:16:32 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 15:16:32 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 15:16:32 - INFO - __main__ - Printing 3 examples
06/01/2022 15:16:32 - INFO - __main__ -  [superglue-cb] premise: B: I wouldn't be surprised. A: You know, because they don't want to send them to daycare. B: I doubt if they would say it was too long. [SEP] hypothesis: it was too long
06/01/2022 15:16:32 - INFO - __main__ - ['contradiction']
06/01/2022 15:16:32 - INFO - __main__ -  [superglue-cb] premise: Most of them young, about his age, stood and talked and drank and laughed. The two girls he had noticed earlier were standing talking to some other girls. Graham hoped they all realised that just because he was standing talking to Slater that didn't mean he was gay too. [SEP] hypothesis: Graham was gay too
06/01/2022 15:16:32 - INFO - __main__ - ['contradiction']
06/01/2022 15:16:32 - INFO - __main__ -  [superglue-cb] premise: A: so you hear so much, you get a little tired of it, but then again, so many people you got to understand only catch it once a day, maybe B: Yep. A: but I think the quality of uh, our news is just, uh, I don't believe it could be better [SEP] hypothesis: it could be better
06/01/2022 15:16:32 - INFO - __main__ - ['contradiction']
06/01/2022 15:16:32 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:16:33 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:16:33 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 15:16:33 - INFO - __main__ - Global step 1000 Train loss 0.038939 ACC 0.6886792452830188 on epoch=111
06/01/2022 15:16:33 - INFO - __main__ - save last model!
06/01/2022 15:16:40 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 15:16:41 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 15:16:41 - INFO - __main__ - Printing 3 examples
06/01/2022 15:16:41 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 15:16:41 - INFO - __main__ - ['contradiction']
06/01/2022 15:16:41 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 15:16:41 - INFO - __main__ - ['neutral']
06/01/2022 15:16:41 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 15:16:41 - INFO - __main__ - ['entailment']
06/01/2022 15:16:41 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:16:41 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:16:41 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 15:16:42 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_21_0.0003_8_predictions.txt
06/01/2022 15:16:42 - INFO - __main__ - ACC on test data: 0.8929
06/01/2022 15:16:43 - INFO - __main__ - prefix=superglue-cb_64_21, lr=0.0003, bsz=8, dev_performance=0.9433962264150944, test_performance=0.8928571428571429
06/01/2022 15:16:43 - INFO - __main__ - Running ... prefix=superglue-cb_64_21, lr=0.0002, bsz=8 ...
06/01/2022 15:16:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 15:16:43 - INFO - __main__ - Starting training!
06/01/2022 15:16:44 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 15:16:44 - INFO - __main__ - Printing 3 examples
06/01/2022 15:16:44 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/01/2022 15:16:44 - INFO - __main__ - ['contradiction']
06/01/2022 15:16:44 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/01/2022 15:16:44 - INFO - __main__ - ['contradiction']
06/01/2022 15:16:44 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/01/2022 15:16:44 - INFO - __main__ - ['contradiction']
06/01/2022 15:16:44 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:16:44 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:16:44 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 15:16:44 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 15:16:44 - INFO - __main__ - Printing 3 examples
06/01/2022 15:16:44 - INFO - __main__ -  [superglue-cb] premise: B: I wouldn't be surprised. A: You know, because they don't want to send them to daycare. B: I doubt if they would say it was too long. [SEP] hypothesis: it was too long
06/01/2022 15:16:44 - INFO - __main__ - ['contradiction']
06/01/2022 15:16:44 - INFO - __main__ -  [superglue-cb] premise: Most of them young, about his age, stood and talked and drank and laughed. The two girls he had noticed earlier were standing talking to some other girls. Graham hoped they all realised that just because he was standing talking to Slater that didn't mean he was gay too. [SEP] hypothesis: Graham was gay too
06/01/2022 15:16:44 - INFO - __main__ - ['contradiction']
06/01/2022 15:16:44 - INFO - __main__ -  [superglue-cb] premise: A: so you hear so much, you get a little tired of it, but then again, so many people you got to understand only catch it once a day, maybe B: Yep. A: but I think the quality of uh, our news is just, uh, I don't believe it could be better [SEP] hypothesis: it could be better
06/01/2022 15:16:44 - INFO - __main__ - ['contradiction']
06/01/2022 15:16:44 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:16:44 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:16:44 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 15:16:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 15:16:57 - INFO - __main__ - Starting training!
06/01/2022 15:17:01 - INFO - __main__ - Step 10 Global step 10 Train loss 23.554293 on epoch=1
06/01/2022 15:17:06 - INFO - __main__ - Step 20 Global step 20 Train loss 20.073963 on epoch=2
06/01/2022 15:17:12 - INFO - __main__ - Step 30 Global step 30 Train loss 13.350389 on epoch=3
06/01/2022 15:17:17 - INFO - __main__ - Step 40 Global step 40 Train loss 11.879620 on epoch=4
06/01/2022 15:17:22 - INFO - __main__ - Step 50 Global step 50 Train loss 11.900293 on epoch=5
06/01/2022 15:17:28 - INFO - __main__ - Global step 50 Train loss 16.151711 ACC 0.05660377358490566 on epoch=5
06/01/2022 15:17:33 - INFO - __main__ - Step 60 Global step 60 Train loss 11.348410 on epoch=6
06/01/2022 15:17:39 - INFO - __main__ - Step 70 Global step 70 Train loss 10.006395 on epoch=7
06/01/2022 15:17:44 - INFO - __main__ - Step 80 Global step 80 Train loss 9.843863 on epoch=8
06/01/2022 15:17:49 - INFO - __main__ - Step 90 Global step 90 Train loss 9.285690 on epoch=9
06/01/2022 15:17:54 - INFO - __main__ - Step 100 Global step 100 Train loss 8.601904 on epoch=11
06/01/2022 15:17:57 - INFO - __main__ - Global step 100 Train loss 9.817252 ACC 0.018867924528301886 on epoch=11
06/01/2022 15:18:02 - INFO - __main__ - Step 110 Global step 110 Train loss 7.998467 on epoch=12
06/01/2022 15:18:08 - INFO - __main__ - Step 120 Global step 120 Train loss 7.267808 on epoch=13
06/01/2022 15:18:13 - INFO - __main__ - Step 130 Global step 130 Train loss 6.918320 on epoch=14
06/01/2022 15:18:18 - INFO - __main__ - Step 140 Global step 140 Train loss 7.118506 on epoch=15
06/01/2022 15:18:24 - INFO - __main__ - Step 150 Global step 150 Train loss 7.010905 on epoch=16
06/01/2022 15:18:26 - INFO - __main__ - Global step 150 Train loss 7.262802 ACC 0.0 on epoch=16
06/01/2022 15:18:32 - INFO - __main__ - Step 160 Global step 160 Train loss 5.906780 on epoch=17
06/01/2022 15:18:37 - INFO - __main__ - Step 170 Global step 170 Train loss 4.885279 on epoch=18
06/01/2022 15:18:42 - INFO - __main__ - Step 180 Global step 180 Train loss 4.462714 on epoch=19
06/01/2022 15:18:48 - INFO - __main__ - Step 190 Global step 190 Train loss 4.022801 on epoch=21
06/01/2022 15:18:53 - INFO - __main__ - Step 200 Global step 200 Train loss 3.313566 on epoch=22
06/01/2022 15:18:55 - INFO - __main__ - Global step 200 Train loss 4.518228 ACC 0.0 on epoch=22
06/01/2022 15:19:00 - INFO - __main__ - Step 210 Global step 210 Train loss 2.535909 on epoch=23
06/01/2022 15:19:06 - INFO - __main__ - Step 220 Global step 220 Train loss 1.935049 on epoch=24
06/01/2022 15:19:11 - INFO - __main__ - Step 230 Global step 230 Train loss 1.765833 on epoch=25
06/01/2022 15:19:16 - INFO - __main__ - Step 240 Global step 240 Train loss 2.162872 on epoch=26
06/01/2022 15:19:22 - INFO - __main__ - Step 250 Global step 250 Train loss 2.068769 on epoch=27
06/01/2022 15:19:24 - INFO - __main__ - Global step 250 Train loss 2.093687 ACC 0.5188679245283019 on epoch=27
06/01/2022 15:19:30 - INFO - __main__ - Step 260 Global step 260 Train loss 1.955537 on epoch=28
06/01/2022 15:19:36 - INFO - __main__ - Step 270 Global step 270 Train loss 1.831385 on epoch=29
06/01/2022 15:19:41 - INFO - __main__ - Step 280 Global step 280 Train loss 1.653291 on epoch=31
06/01/2022 15:19:46 - INFO - __main__ - Step 290 Global step 290 Train loss 1.829574 on epoch=32
06/01/2022 15:19:52 - INFO - __main__ - Step 300 Global step 300 Train loss 1.878141 on epoch=33
06/01/2022 15:19:54 - INFO - __main__ - Global step 300 Train loss 1.829585 ACC 0.5188679245283019 on epoch=33
06/01/2022 15:19:59 - INFO - __main__ - Step 310 Global step 310 Train loss 1.631810 on epoch=34
06/01/2022 15:20:05 - INFO - __main__ - Step 320 Global step 320 Train loss 1.854432 on epoch=35
06/01/2022 15:20:10 - INFO - __main__ - Step 330 Global step 330 Train loss 1.943167 on epoch=36
06/01/2022 15:20:15 - INFO - __main__ - Step 340 Global step 340 Train loss 1.644213 on epoch=37
06/01/2022 15:20:21 - INFO - __main__ - Step 350 Global step 350 Train loss 1.937844 on epoch=38
06/01/2022 15:20:23 - INFO - __main__ - Global step 350 Train loss 1.802293 ACC 0.5188679245283019 on epoch=38
06/01/2022 15:20:29 - INFO - __main__ - Step 360 Global step 360 Train loss 1.483598 on epoch=39
06/01/2022 15:20:34 - INFO - __main__ - Step 370 Global step 370 Train loss 1.499355 on epoch=41
06/01/2022 15:20:39 - INFO - __main__ - Step 380 Global step 380 Train loss 1.364724 on epoch=42
06/01/2022 15:20:45 - INFO - __main__ - Step 390 Global step 390 Train loss 1.442089 on epoch=43
06/01/2022 15:20:50 - INFO - __main__ - Step 400 Global step 400 Train loss 1.534406 on epoch=44
06/01/2022 15:20:52 - INFO - __main__ - Global step 400 Train loss 1.464834 ACC 0.5188679245283019 on epoch=44
06/01/2022 15:20:58 - INFO - __main__ - Step 410 Global step 410 Train loss 1.287994 on epoch=45
06/01/2022 15:21:03 - INFO - __main__ - Step 420 Global step 420 Train loss 1.399549 on epoch=46
06/01/2022 15:21:09 - INFO - __main__ - Step 430 Global step 430 Train loss 1.289492 on epoch=47
06/01/2022 15:21:14 - INFO - __main__ - Step 440 Global step 440 Train loss 1.094559 on epoch=48
06/01/2022 15:21:20 - INFO - __main__ - Step 450 Global step 450 Train loss 1.359284 on epoch=49
06/01/2022 15:21:22 - INFO - __main__ - Global step 450 Train loss 1.286176 ACC 0.839622641509434 on epoch=49
06/01/2022 15:21:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.911130 on epoch=51
06/01/2022 15:21:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.713077 on epoch=52
06/01/2022 15:21:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.943015 on epoch=53
06/01/2022 15:21:44 - INFO - __main__ - Step 490 Global step 490 Train loss 1.002631 on epoch=54
06/01/2022 15:21:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.901392 on epoch=55
06/01/2022 15:21:51 - INFO - __main__ - Global step 500 Train loss 0.894249 ACC 0.7547169811320755 on epoch=55
06/01/2022 15:21:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.610583 on epoch=56
06/01/2022 15:22:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.579153 on epoch=57
06/01/2022 15:22:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.496043 on epoch=58
06/01/2022 15:22:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.387383 on epoch=59
06/01/2022 15:22:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.334184 on epoch=61
06/01/2022 15:22:20 - INFO - __main__ - Global step 550 Train loss 0.481469 ACC 0.5471698113207547 on epoch=61
06/01/2022 15:22:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.201505 on epoch=62
06/01/2022 15:22:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.112712 on epoch=63
06/01/2022 15:22:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.092422 on epoch=64
06/01/2022 15:22:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.231798 on epoch=65
06/01/2022 15:22:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.162305 on epoch=66
06/01/2022 15:22:49 - INFO - __main__ - Global step 600 Train loss 0.160148 ACC 0.9056603773584906 on epoch=66
06/01/2022 15:22:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.083121 on epoch=67
06/01/2022 15:23:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.067775 on epoch=68
06/01/2022 15:23:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.048239 on epoch=69
06/01/2022 15:23:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.057612 on epoch=71
06/01/2022 15:23:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.056824 on epoch=72
06/01/2022 15:23:19 - INFO - __main__ - Global step 650 Train loss 0.062714 ACC 0.8867924528301887 on epoch=72
06/01/2022 15:23:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.016236 on epoch=73
06/01/2022 15:23:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.080460 on epoch=74
06/01/2022 15:23:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.041156 on epoch=75
06/01/2022 15:23:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.014605 on epoch=76
06/01/2022 15:23:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.006212 on epoch=77
06/01/2022 15:23:47 - INFO - __main__ - Global step 700 Train loss 0.031734 ACC 0.8773584905660378 on epoch=77
06/01/2022 15:23:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.006256 on epoch=78
06/01/2022 15:23:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.001920 on epoch=79
06/01/2022 15:24:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.002741 on epoch=81
06/01/2022 15:24:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.006662 on epoch=82
06/01/2022 15:24:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001801 on epoch=83
06/01/2022 15:24:16 - INFO - __main__ - Global step 750 Train loss 0.003876 ACC 0.9339622641509434 on epoch=83
06/01/2022 15:24:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.004388 on epoch=84
06/01/2022 15:24:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.007911 on epoch=85
06/01/2022 15:24:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.037424 on epoch=86
06/01/2022 15:24:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.005778 on epoch=87
06/01/2022 15:24:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.007832 on epoch=88
06/01/2022 15:24:46 - INFO - __main__ - Global step 800 Train loss 0.012667 ACC 0.9528301886792453 on epoch=88
06/01/2022 15:24:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000960 on epoch=89
06/01/2022 15:24:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001058 on epoch=91
06/01/2022 15:25:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.001608 on epoch=92
06/01/2022 15:25:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000804 on epoch=93
06/01/2022 15:25:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000980 on epoch=94
06/01/2022 15:25:16 - INFO - __main__ - Global step 850 Train loss 0.001082 ACC 0.9622641509433962 on epoch=94
06/01/2022 15:25:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000283 on epoch=95
06/01/2022 15:25:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000255 on epoch=96
06/01/2022 15:25:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001567 on epoch=97
06/01/2022 15:25:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.003078 on epoch=98
06/01/2022 15:25:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.003000 on epoch=99
06/01/2022 15:25:46 - INFO - __main__ - Global step 900 Train loss 0.001637 ACC 0.9150943396226415 on epoch=99
06/01/2022 15:25:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.005526 on epoch=101
06/01/2022 15:25:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.004661 on epoch=102
06/01/2022 15:26:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.003573 on epoch=103
06/01/2022 15:26:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000225 on epoch=104
06/01/2022 15:26:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000459 on epoch=105
06/01/2022 15:26:15 - INFO - __main__ - Global step 950 Train loss 0.002889 ACC 0.9339622641509434 on epoch=105
06/01/2022 15:26:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.001226 on epoch=106
06/01/2022 15:26:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000837 on epoch=107
06/01/2022 15:26:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000387 on epoch=108
06/01/2022 15:26:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.001679 on epoch=109
06/01/2022 15:26:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.017215 on epoch=111
06/01/2022 15:26:43 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 15:26:43 - INFO - __main__ - Printing 3 examples
06/01/2022 15:26:43 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/01/2022 15:26:43 - INFO - __main__ - ['contradiction']
06/01/2022 15:26:43 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/01/2022 15:26:43 - INFO - __main__ - ['contradiction']
06/01/2022 15:26:43 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/01/2022 15:26:43 - INFO - __main__ - ['contradiction']
06/01/2022 15:26:43 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:26:43 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:26:44 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 15:26:44 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 15:26:44 - INFO - __main__ - Printing 3 examples
06/01/2022 15:26:44 - INFO - __main__ -  [superglue-cb] premise: B: I wouldn't be surprised. A: You know, because they don't want to send them to daycare. B: I doubt if they would say it was too long. [SEP] hypothesis: it was too long
06/01/2022 15:26:44 - INFO - __main__ - ['contradiction']
06/01/2022 15:26:44 - INFO - __main__ -  [superglue-cb] premise: Most of them young, about his age, stood and talked and drank and laughed. The two girls he had noticed earlier were standing talking to some other girls. Graham hoped they all realised that just because he was standing talking to Slater that didn't mean he was gay too. [SEP] hypothesis: Graham was gay too
06/01/2022 15:26:44 - INFO - __main__ - ['contradiction']
06/01/2022 15:26:44 - INFO - __main__ -  [superglue-cb] premise: A: so you hear so much, you get a little tired of it, but then again, so many people you got to understand only catch it once a day, maybe B: Yep. A: but I think the quality of uh, our news is just, uh, I don't believe it could be better [SEP] hypothesis: it could be better
06/01/2022 15:26:44 - INFO - __main__ - ['contradiction']
06/01/2022 15:26:44 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:26:44 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:26:44 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 15:26:45 - INFO - __main__ - Global step 1000 Train loss 0.004268 ACC 0.9622641509433962 on epoch=111
06/01/2022 15:26:45 - INFO - __main__ - save last model!
06/01/2022 15:26:52 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 15:26:52 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 15:26:52 - INFO - __main__ - Printing 3 examples
06/01/2022 15:26:52 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 15:26:52 - INFO - __main__ - ['contradiction']
06/01/2022 15:26:52 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 15:26:52 - INFO - __main__ - ['neutral']
06/01/2022 15:26:52 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 15:26:52 - INFO - __main__ - ['entailment']
06/01/2022 15:26:52 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:26:52 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:26:52 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 15:26:54 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_21_0.0002_8_predictions.txt
06/01/2022 15:26:54 - INFO - __main__ - ACC on test data: 0.9107
06/01/2022 15:26:54 - INFO - __main__ - prefix=superglue-cb_64_21, lr=0.0002, bsz=8, dev_performance=0.9622641509433962, test_performance=0.9107142857142857
06/01/2022 15:26:54 - INFO - __main__ - Running ... prefix=superglue-cb_64_21, lr=0.0001, bsz=8 ...
06/01/2022 15:26:55 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 15:26:55 - INFO - __main__ - Printing 3 examples
06/01/2022 15:26:55 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/01/2022 15:26:55 - INFO - __main__ - ['contradiction']
06/01/2022 15:26:55 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/01/2022 15:26:55 - INFO - __main__ - ['contradiction']
06/01/2022 15:26:55 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/01/2022 15:26:55 - INFO - __main__ - ['contradiction']
06/01/2022 15:26:55 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:26:55 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:26:55 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 15:26:55 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 15:26:55 - INFO - __main__ - Printing 3 examples
06/01/2022 15:26:55 - INFO - __main__ -  [superglue-cb] premise: B: I wouldn't be surprised. A: You know, because they don't want to send them to daycare. B: I doubt if they would say it was too long. [SEP] hypothesis: it was too long
06/01/2022 15:26:55 - INFO - __main__ - ['contradiction']
06/01/2022 15:26:55 - INFO - __main__ -  [superglue-cb] premise: Most of them young, about his age, stood and talked and drank and laughed. The two girls he had noticed earlier were standing talking to some other girls. Graham hoped they all realised that just because he was standing talking to Slater that didn't mean he was gay too. [SEP] hypothesis: Graham was gay too
06/01/2022 15:26:55 - INFO - __main__ - ['contradiction']
06/01/2022 15:26:55 - INFO - __main__ -  [superglue-cb] premise: A: so you hear so much, you get a little tired of it, but then again, so many people you got to understand only catch it once a day, maybe B: Yep. A: but I think the quality of uh, our news is just, uh, I don't believe it could be better [SEP] hypothesis: it could be better
06/01/2022 15:26:55 - INFO - __main__ - ['contradiction']
06/01/2022 15:26:55 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:26:55 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:26:55 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 15:26:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 15:26:57 - INFO - __main__ - Starting training!
06/01/2022 15:27:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 15:27:08 - INFO - __main__ - Starting training!
06/01/2022 15:27:13 - INFO - __main__ - Step 10 Global step 10 Train loss 24.011484 on epoch=1
06/01/2022 15:27:18 - INFO - __main__ - Step 20 Global step 20 Train loss 22.909649 on epoch=2
06/01/2022 15:27:23 - INFO - __main__ - Step 30 Global step 30 Train loss 16.656357 on epoch=3
06/01/2022 15:27:28 - INFO - __main__ - Step 40 Global step 40 Train loss 14.444601 on epoch=4
06/01/2022 15:27:33 - INFO - __main__ - Step 50 Global step 50 Train loss 13.378406 on epoch=5
06/01/2022 15:28:05 - INFO - __main__ - Global step 50 Train loss 18.280098 ACC 0.0 on epoch=5
06/01/2022 15:28:11 - INFO - __main__ - Step 60 Global step 60 Train loss 12.586603 on epoch=6
06/01/2022 15:28:16 - INFO - __main__ - Step 70 Global step 70 Train loss 11.441758 on epoch=7
06/01/2022 15:28:22 - INFO - __main__ - Step 80 Global step 80 Train loss 10.866364 on epoch=8
06/01/2022 15:28:27 - INFO - __main__ - Step 90 Global step 90 Train loss 10.593402 on epoch=9
06/01/2022 15:28:32 - INFO - __main__ - Step 100 Global step 100 Train loss 11.136671 on epoch=11
06/01/2022 15:28:39 - INFO - __main__ - Global step 100 Train loss 11.324960 ACC 0.0 on epoch=11
06/01/2022 15:28:44 - INFO - __main__ - Step 110 Global step 110 Train loss 10.296659 on epoch=12
06/01/2022 15:28:50 - INFO - __main__ - Step 120 Global step 120 Train loss 10.046508 on epoch=13
06/01/2022 15:28:55 - INFO - __main__ - Step 130 Global step 130 Train loss 9.594469 on epoch=14
06/01/2022 15:29:00 - INFO - __main__ - Step 140 Global step 140 Train loss 9.296155 on epoch=15
06/01/2022 15:29:05 - INFO - __main__ - Step 150 Global step 150 Train loss 9.237595 on epoch=16
06/01/2022 15:29:08 - INFO - __main__ - Global step 150 Train loss 9.694277 ACC 0.0 on epoch=16
06/01/2022 15:29:13 - INFO - __main__ - Step 160 Global step 160 Train loss 9.389636 on epoch=17
06/01/2022 15:29:19 - INFO - __main__ - Step 170 Global step 170 Train loss 9.135920 on epoch=18
06/01/2022 15:29:24 - INFO - __main__ - Step 180 Global step 180 Train loss 8.902267 on epoch=19
06/01/2022 15:29:29 - INFO - __main__ - Step 190 Global step 190 Train loss 8.606803 on epoch=21
06/01/2022 15:29:35 - INFO - __main__ - Step 200 Global step 200 Train loss 8.398530 on epoch=22
06/01/2022 15:29:37 - INFO - __main__ - Global step 200 Train loss 8.886631 ACC 0.0 on epoch=22
06/01/2022 15:29:43 - INFO - __main__ - Step 210 Global step 210 Train loss 7.506261 on epoch=23
06/01/2022 15:29:48 - INFO - __main__ - Step 220 Global step 220 Train loss 7.346427 on epoch=24
06/01/2022 15:29:53 - INFO - __main__ - Step 230 Global step 230 Train loss 8.154463 on epoch=25
06/01/2022 15:29:59 - INFO - __main__ - Step 240 Global step 240 Train loss 8.283686 on epoch=26
06/01/2022 15:30:04 - INFO - __main__ - Step 250 Global step 250 Train loss 7.135968 on epoch=27
06/01/2022 15:30:07 - INFO - __main__ - Global step 250 Train loss 7.685361 ACC 0.0 on epoch=27
06/01/2022 15:30:12 - INFO - __main__ - Step 260 Global step 260 Train loss 7.221343 on epoch=28
06/01/2022 15:30:17 - INFO - __main__ - Step 270 Global step 270 Train loss 6.932429 on epoch=29
06/01/2022 15:30:22 - INFO - __main__ - Step 280 Global step 280 Train loss 6.017797 on epoch=31
06/01/2022 15:30:28 - INFO - __main__ - Step 290 Global step 290 Train loss 5.651006 on epoch=32
06/01/2022 15:30:33 - INFO - __main__ - Step 300 Global step 300 Train loss 5.148446 on epoch=33
06/01/2022 15:30:36 - INFO - __main__ - Global step 300 Train loss 6.194204 ACC 0.0 on epoch=33
06/01/2022 15:30:41 - INFO - __main__ - Step 310 Global step 310 Train loss 4.845959 on epoch=34
06/01/2022 15:30:46 - INFO - __main__ - Step 320 Global step 320 Train loss 5.114598 on epoch=35
06/01/2022 15:30:51 - INFO - __main__ - Step 330 Global step 330 Train loss 4.682482 on epoch=36
06/01/2022 15:30:57 - INFO - __main__ - Step 340 Global step 340 Train loss 3.921600 on epoch=37
06/01/2022 15:31:02 - INFO - __main__ - Step 350 Global step 350 Train loss 3.648624 on epoch=38
06/01/2022 15:31:04 - INFO - __main__ - Global step 350 Train loss 4.442653 ACC 0.4811320754716981 on epoch=38
06/01/2022 15:31:10 - INFO - __main__ - Step 360 Global step 360 Train loss 3.247067 on epoch=39
06/01/2022 15:31:16 - INFO - __main__ - Step 370 Global step 370 Train loss 3.028517 on epoch=41
06/01/2022 15:31:21 - INFO - __main__ - Step 380 Global step 380 Train loss 3.010276 on epoch=42
06/01/2022 15:31:26 - INFO - __main__ - Step 390 Global step 390 Train loss 2.813071 on epoch=43
06/01/2022 15:31:32 - INFO - __main__ - Step 400 Global step 400 Train loss 2.540821 on epoch=44
06/01/2022 15:31:34 - INFO - __main__ - Global step 400 Train loss 2.927950 ACC 0.5471698113207547 on epoch=44
06/01/2022 15:31:40 - INFO - __main__ - Step 410 Global step 410 Train loss 2.343352 on epoch=45
06/01/2022 15:31:45 - INFO - __main__ - Step 420 Global step 420 Train loss 2.711388 on epoch=46
06/01/2022 15:31:50 - INFO - __main__ - Step 430 Global step 430 Train loss 2.261167 on epoch=47
06/01/2022 15:31:56 - INFO - __main__ - Step 440 Global step 440 Train loss 2.408241 on epoch=48
06/01/2022 15:32:01 - INFO - __main__ - Step 450 Global step 450 Train loss 2.506781 on epoch=49
06/01/2022 15:32:03 - INFO - __main__ - Global step 450 Train loss 2.446186 ACC 0.5188679245283019 on epoch=49
06/01/2022 15:32:09 - INFO - __main__ - Step 460 Global step 460 Train loss 1.751584 on epoch=51
06/01/2022 15:32:14 - INFO - __main__ - Step 470 Global step 470 Train loss 2.008033 on epoch=52
06/01/2022 15:32:19 - INFO - __main__ - Step 480 Global step 480 Train loss 2.576959 on epoch=53
06/01/2022 15:32:24 - INFO - __main__ - Step 490 Global step 490 Train loss 2.229645 on epoch=54
06/01/2022 15:32:30 - INFO - __main__ - Step 500 Global step 500 Train loss 2.485942 on epoch=55
06/01/2022 15:32:32 - INFO - __main__ - Global step 500 Train loss 2.210433 ACC 0.5188679245283019 on epoch=55
06/01/2022 15:32:37 - INFO - __main__ - Step 510 Global step 510 Train loss 1.878534 on epoch=56
06/01/2022 15:32:43 - INFO - __main__ - Step 520 Global step 520 Train loss 1.989671 on epoch=57
06/01/2022 15:32:48 - INFO - __main__ - Step 530 Global step 530 Train loss 2.059484 on epoch=58
06/01/2022 15:32:53 - INFO - __main__ - Step 540 Global step 540 Train loss 1.972610 on epoch=59
06/01/2022 15:32:59 - INFO - __main__ - Step 550 Global step 550 Train loss 2.363243 on epoch=61
06/01/2022 15:33:01 - INFO - __main__ - Global step 550 Train loss 2.052708 ACC 0.5188679245283019 on epoch=61
06/01/2022 15:33:06 - INFO - __main__ - Step 560 Global step 560 Train loss 1.813910 on epoch=62
06/01/2022 15:33:12 - INFO - __main__ - Step 570 Global step 570 Train loss 2.520617 on epoch=63
06/01/2022 15:33:17 - INFO - __main__ - Step 580 Global step 580 Train loss 2.180979 on epoch=64
06/01/2022 15:33:22 - INFO - __main__ - Step 590 Global step 590 Train loss 2.329013 on epoch=65
06/01/2022 15:33:28 - INFO - __main__ - Step 600 Global step 600 Train loss 1.743909 on epoch=66
06/01/2022 15:33:30 - INFO - __main__ - Global step 600 Train loss 2.117685 ACC 0.5188679245283019 on epoch=66
06/01/2022 15:33:35 - INFO - __main__ - Step 610 Global step 610 Train loss 1.937656 on epoch=67
06/01/2022 15:33:40 - INFO - __main__ - Step 620 Global step 620 Train loss 2.269608 on epoch=68
06/01/2022 15:33:46 - INFO - __main__ - Step 630 Global step 630 Train loss 2.015442 on epoch=69
06/01/2022 15:33:51 - INFO - __main__ - Step 640 Global step 640 Train loss 2.053108 on epoch=71
06/01/2022 15:33:56 - INFO - __main__ - Step 650 Global step 650 Train loss 1.700276 on epoch=72
06/01/2022 15:33:59 - INFO - __main__ - Global step 650 Train loss 1.995218 ACC 0.5188679245283019 on epoch=72
06/01/2022 15:34:04 - INFO - __main__ - Step 660 Global step 660 Train loss 1.659520 on epoch=73
06/01/2022 15:34:10 - INFO - __main__ - Step 670 Global step 670 Train loss 1.427424 on epoch=74
06/01/2022 15:34:15 - INFO - __main__ - Step 680 Global step 680 Train loss 1.699912 on epoch=75
06/01/2022 15:34:20 - INFO - __main__ - Step 690 Global step 690 Train loss 1.723901 on epoch=76
06/01/2022 15:34:26 - INFO - __main__ - Step 700 Global step 700 Train loss 1.445482 on epoch=77
06/01/2022 15:34:28 - INFO - __main__ - Global step 700 Train loss 1.591248 ACC 0.5188679245283019 on epoch=77
06/01/2022 15:34:34 - INFO - __main__ - Step 710 Global step 710 Train loss 1.403642 on epoch=78
06/01/2022 15:34:39 - INFO - __main__ - Step 720 Global step 720 Train loss 1.488261 on epoch=79
06/01/2022 15:34:45 - INFO - __main__ - Step 730 Global step 730 Train loss 1.664245 on epoch=81
06/01/2022 15:34:50 - INFO - __main__ - Step 740 Global step 740 Train loss 1.524147 on epoch=82
06/01/2022 15:34:55 - INFO - __main__ - Step 750 Global step 750 Train loss 1.254334 on epoch=83
06/01/2022 15:34:57 - INFO - __main__ - Global step 750 Train loss 1.466926 ACC 0.5188679245283019 on epoch=83
06/01/2022 15:35:03 - INFO - __main__ - Step 760 Global step 760 Train loss 1.447921 on epoch=84
06/01/2022 15:35:08 - INFO - __main__ - Step 770 Global step 770 Train loss 1.291507 on epoch=85
06/01/2022 15:35:14 - INFO - __main__ - Step 780 Global step 780 Train loss 1.783994 on epoch=86
06/01/2022 15:35:19 - INFO - __main__ - Step 790 Global step 790 Train loss 1.489548 on epoch=87
06/01/2022 15:35:25 - INFO - __main__ - Step 800 Global step 800 Train loss 1.356704 on epoch=88
06/01/2022 15:35:27 - INFO - __main__ - Global step 800 Train loss 1.473935 ACC 0.5188679245283019 on epoch=88
06/01/2022 15:35:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.985169 on epoch=89
06/01/2022 15:35:38 - INFO - __main__ - Step 820 Global step 820 Train loss 1.515149 on epoch=91
06/01/2022 15:35:44 - INFO - __main__ - Step 830 Global step 830 Train loss 1.049358 on epoch=92
06/01/2022 15:35:49 - INFO - __main__ - Step 840 Global step 840 Train loss 1.444653 on epoch=93
06/01/2022 15:35:55 - INFO - __main__ - Step 850 Global step 850 Train loss 1.304521 on epoch=94
06/01/2022 15:35:57 - INFO - __main__ - Global step 850 Train loss 1.259770 ACC 0.5188679245283019 on epoch=94
06/01/2022 15:36:02 - INFO - __main__ - Step 860 Global step 860 Train loss 1.058175 on epoch=95
06/01/2022 15:36:08 - INFO - __main__ - Step 870 Global step 870 Train loss 1.282215 on epoch=96
06/01/2022 15:36:14 - INFO - __main__ - Step 880 Global step 880 Train loss 1.162180 on epoch=97
06/01/2022 15:36:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.792344 on epoch=98
06/01/2022 15:36:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.903728 on epoch=99
06/01/2022 15:36:26 - INFO - __main__ - Global step 900 Train loss 1.039729 ACC 0.5283018867924528 on epoch=99
06/01/2022 15:36:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.999843 on epoch=101
06/01/2022 15:36:38 - INFO - __main__ - Step 920 Global step 920 Train loss 1.027926 on epoch=102
06/01/2022 15:36:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.879484 on epoch=103
06/01/2022 15:36:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.864076 on epoch=104
06/01/2022 15:36:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.950916 on epoch=105
06/01/2022 15:36:57 - INFO - __main__ - Global step 950 Train loss 0.944449 ACC 0.7924528301886793 on epoch=105
06/01/2022 15:37:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.706993 on epoch=106
06/01/2022 15:37:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.747118 on epoch=107
06/01/2022 15:37:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.847563 on epoch=108
06/01/2022 15:37:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.868680 on epoch=109
06/01/2022 15:37:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.665686 on epoch=111
06/01/2022 15:37:26 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 15:37:26 - INFO - __main__ - Printing 3 examples
06/01/2022 15:37:26 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/01/2022 15:37:26 - INFO - __main__ - ['contradiction']
06/01/2022 15:37:26 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/01/2022 15:37:26 - INFO - __main__ - ['contradiction']
06/01/2022 15:37:26 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/01/2022 15:37:26 - INFO - __main__ - ['contradiction']
06/01/2022 15:37:26 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:37:26 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:37:26 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 15:37:26 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 15:37:26 - INFO - __main__ - Printing 3 examples
06/01/2022 15:37:26 - INFO - __main__ -  [superglue-cb] premise: B: but if they get the little tiny kids saving it now, in five years, when they get bigger, it'll work a little bit more, too. A: Yeah.  True. B: Because it's, we've all got to do it right now. I just, I really amazed to find out that, eighty per cent are filled now, in garbage fills. In five years we're supposed to be at max. A: Uh-huh. B: I don't think I can keep my own garbage. [SEP] hypothesis: she can keep her own garbage
06/01/2022 15:37:26 - INFO - __main__ - ['contradiction']
06/01/2022 15:37:26 - INFO - __main__ -  [superglue-cb] premise: You really don't know anything about me, do you, despite all that wallowing in my mind? As it happens I don't think I'm the right person to lead humanity into the future no. [SEP] hypothesis: she is the right person to lead humanity into the future
06/01/2022 15:37:26 - INFO - __main__ - ['contradiction']
06/01/2022 15:37:26 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/01/2022 15:37:26 - INFO - __main__ - ['contradiction']
06/01/2022 15:37:26 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:37:26 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:37:26 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 15:37:27 - INFO - __main__ - Global step 1000 Train loss 0.767208 ACC 0.8018867924528302 on epoch=111
06/01/2022 15:37:28 - INFO - __main__ - save last model!
06/01/2022 15:37:34 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 15:37:35 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 15:37:35 - INFO - __main__ - Printing 3 examples
06/01/2022 15:37:35 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 15:37:35 - INFO - __main__ - ['contradiction']
06/01/2022 15:37:35 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 15:37:35 - INFO - __main__ - ['neutral']
06/01/2022 15:37:35 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 15:37:35 - INFO - __main__ - ['entailment']
06/01/2022 15:37:35 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:37:35 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:37:35 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 15:37:37 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_21_0.0001_8_predictions.txt
06/01/2022 15:37:37 - INFO - __main__ - ACC on test data: 0.7143
06/01/2022 15:37:37 - INFO - __main__ - prefix=superglue-cb_64_21, lr=0.0001, bsz=8, dev_performance=0.8018867924528302, test_performance=0.7142857142857143
06/01/2022 15:37:37 - INFO - __main__ - Running ... prefix=superglue-cb_64_42, lr=0.0005, bsz=8 ...
06/01/2022 15:37:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 15:37:37 - INFO - __main__ - Starting training!
06/01/2022 15:37:38 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 15:37:38 - INFO - __main__ - Printing 3 examples
06/01/2022 15:37:38 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/01/2022 15:37:38 - INFO - __main__ - ['contradiction']
06/01/2022 15:37:38 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/01/2022 15:37:38 - INFO - __main__ - ['contradiction']
06/01/2022 15:37:38 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/01/2022 15:37:38 - INFO - __main__ - ['contradiction']
06/01/2022 15:37:38 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:37:38 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:37:38 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 15:37:38 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 15:37:38 - INFO - __main__ - Printing 3 examples
06/01/2022 15:37:38 - INFO - __main__ -  [superglue-cb] premise: B: but if they get the little tiny kids saving it now, in five years, when they get bigger, it'll work a little bit more, too. A: Yeah.  True. B: Because it's, we've all got to do it right now. I just, I really amazed to find out that, eighty per cent are filled now, in garbage fills. In five years we're supposed to be at max. A: Uh-huh. B: I don't think I can keep my own garbage. [SEP] hypothesis: she can keep her own garbage
06/01/2022 15:37:38 - INFO - __main__ - ['contradiction']
06/01/2022 15:37:38 - INFO - __main__ -  [superglue-cb] premise: You really don't know anything about me, do you, despite all that wallowing in my mind? As it happens I don't think I'm the right person to lead humanity into the future no. [SEP] hypothesis: she is the right person to lead humanity into the future
06/01/2022 15:37:38 - INFO - __main__ - ['contradiction']
06/01/2022 15:37:38 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/01/2022 15:37:38 - INFO - __main__ - ['contradiction']
06/01/2022 15:37:38 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:37:38 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:37:38 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 15:37:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 15:37:51 - INFO - __main__ - Starting training!
06/01/2022 15:37:56 - INFO - __main__ - Step 10 Global step 10 Train loss 23.954952 on epoch=1
06/01/2022 15:38:01 - INFO - __main__ - Step 20 Global step 20 Train loss 18.358822 on epoch=2
06/01/2022 15:38:07 - INFO - __main__ - Step 30 Global step 30 Train loss 11.083302 on epoch=3
06/01/2022 15:38:13 - INFO - __main__ - Step 40 Global step 40 Train loss 9.608189 on epoch=4
06/01/2022 15:38:18 - INFO - __main__ - Step 50 Global step 50 Train loss 8.592939 on epoch=5
06/01/2022 15:38:21 - INFO - __main__ - Global step 50 Train loss 14.319642 ACC 0.4716981132075472 on epoch=5
06/01/2022 15:38:27 - INFO - __main__ - Step 60 Global step 60 Train loss 7.715857 on epoch=6
06/01/2022 15:38:32 - INFO - __main__ - Step 70 Global step 70 Train loss 6.413529 on epoch=7
06/01/2022 15:38:37 - INFO - __main__ - Step 80 Global step 80 Train loss 5.213591 on epoch=8
06/01/2022 15:38:43 - INFO - __main__ - Step 90 Global step 90 Train loss 3.185202 on epoch=9
06/01/2022 15:38:48 - INFO - __main__ - Step 100 Global step 100 Train loss 2.341134 on epoch=11
06/01/2022 15:38:51 - INFO - __main__ - Global step 100 Train loss 4.973863 ACC 0.5188679245283019 on epoch=11
06/01/2022 15:38:57 - INFO - __main__ - Step 110 Global step 110 Train loss 2.229138 on epoch=12
06/01/2022 15:39:02 - INFO - __main__ - Step 120 Global step 120 Train loss 2.707853 on epoch=13
06/01/2022 15:39:07 - INFO - __main__ - Step 130 Global step 130 Train loss 1.748358 on epoch=14
06/01/2022 15:39:13 - INFO - __main__ - Step 140 Global step 140 Train loss 2.008027 on epoch=15
06/01/2022 15:39:18 - INFO - __main__ - Step 150 Global step 150 Train loss 2.072856 on epoch=16
06/01/2022 15:39:20 - INFO - __main__ - Global step 150 Train loss 2.153247 ACC 0.5188679245283019 on epoch=16
06/01/2022 15:39:26 - INFO - __main__ - Step 160 Global step 160 Train loss 1.788644 on epoch=17
06/01/2022 15:39:31 - INFO - __main__ - Step 170 Global step 170 Train loss 1.414738 on epoch=18
06/01/2022 15:39:36 - INFO - __main__ - Step 180 Global step 180 Train loss 1.655720 on epoch=19
06/01/2022 15:39:41 - INFO - __main__ - Step 190 Global step 190 Train loss 1.309661 on epoch=21
06/01/2022 15:39:47 - INFO - __main__ - Step 200 Global step 200 Train loss 1.048417 on epoch=22
06/01/2022 15:39:49 - INFO - __main__ - Global step 200 Train loss 1.443436 ACC 0.5188679245283019 on epoch=22
06/01/2022 15:39:54 - INFO - __main__ - Step 210 Global step 210 Train loss 1.192848 on epoch=23
06/01/2022 15:40:00 - INFO - __main__ - Step 220 Global step 220 Train loss 1.168597 on epoch=24
06/01/2022 15:40:05 - INFO - __main__ - Step 230 Global step 230 Train loss 1.149321 on epoch=25
06/01/2022 15:40:10 - INFO - __main__ - Step 240 Global step 240 Train loss 1.055105 on epoch=26
06/01/2022 15:40:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.891639 on epoch=27
06/01/2022 15:40:18 - INFO - __main__ - Global step 250 Train loss 1.091502 ACC 0.5188679245283019 on epoch=27
06/01/2022 15:40:23 - INFO - __main__ - Step 260 Global step 260 Train loss 1.004001 on epoch=28
06/01/2022 15:40:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.824489 on epoch=29
06/01/2022 15:40:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.612698 on epoch=31
06/01/2022 15:40:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.918344 on epoch=32
06/01/2022 15:40:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.697796 on epoch=33
06/01/2022 15:40:47 - INFO - __main__ - Global step 300 Train loss 0.811465 ACC 0.5188679245283019 on epoch=33
06/01/2022 15:40:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.682274 on epoch=34
06/01/2022 15:40:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.643619 on epoch=35
06/01/2022 15:41:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.502219 on epoch=36
06/01/2022 15:41:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.545649 on epoch=37
06/01/2022 15:41:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.568291 on epoch=38
06/01/2022 15:41:16 - INFO - __main__ - Global step 350 Train loss 0.588410 ACC 0.5660377358490566 on epoch=38
06/01/2022 15:41:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.602197 on epoch=39
06/01/2022 15:41:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.563395 on epoch=41
06/01/2022 15:41:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.552044 on epoch=42
06/01/2022 15:41:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.542377 on epoch=43
06/01/2022 15:41:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.503284 on epoch=44
06/01/2022 15:41:46 - INFO - __main__ - Global step 400 Train loss 0.552659 ACC 0.5849056603773585 on epoch=44
06/01/2022 15:41:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.479318 on epoch=45
06/01/2022 15:41:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.469110 on epoch=46
06/01/2022 15:42:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.419130 on epoch=47
06/01/2022 15:42:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.451610 on epoch=48
06/01/2022 15:42:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.381531 on epoch=49
06/01/2022 15:42:15 - INFO - __main__ - Global step 450 Train loss 0.440140 ACC 0.8584905660377359 on epoch=49
06/01/2022 15:42:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.370836 on epoch=51
06/01/2022 15:42:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.386263 on epoch=52
06/01/2022 15:42:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.399527 on epoch=53
06/01/2022 15:42:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.446933 on epoch=54
06/01/2022 15:42:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.493159 on epoch=55
06/01/2022 15:42:45 - INFO - __main__ - Global step 500 Train loss 0.419344 ACC 0.7830188679245284 on epoch=55
06/01/2022 15:42:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.484628 on epoch=56
06/01/2022 15:42:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.405930 on epoch=57
06/01/2022 15:43:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.400156 on epoch=58
06/01/2022 15:43:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.311938 on epoch=59
06/01/2022 15:43:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.362124 on epoch=61
06/01/2022 15:43:13 - INFO - __main__ - Global step 550 Train loss 0.392955 ACC 0.8773584905660378 on epoch=61
06/01/2022 15:43:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.385451 on epoch=62
06/01/2022 15:43:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.261900 on epoch=63
06/01/2022 15:43:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.279545 on epoch=64
06/01/2022 15:43:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.337959 on epoch=65
06/01/2022 15:43:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.299230 on epoch=66
06/01/2022 15:43:42 - INFO - __main__ - Global step 600 Train loss 0.312817 ACC 0.8679245283018868 on epoch=66
06/01/2022 15:43:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.251833 on epoch=67
06/01/2022 15:43:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.264473 on epoch=68
06/01/2022 15:43:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.203081 on epoch=69
06/01/2022 15:44:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.203795 on epoch=71
06/01/2022 15:44:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.171935 on epoch=72
06/01/2022 15:44:11 - INFO - __main__ - Global step 650 Train loss 0.219024 ACC 0.8584905660377359 on epoch=72
06/01/2022 15:44:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.215527 on epoch=73
06/01/2022 15:44:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.225338 on epoch=74
06/01/2022 15:44:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.522549 on epoch=75
06/01/2022 15:44:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.203149 on epoch=76
06/01/2022 15:44:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.163963 on epoch=77
06/01/2022 15:44:40 - INFO - __main__ - Global step 700 Train loss 0.266105 ACC 0.8962264150943396 on epoch=77
06/01/2022 15:44:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.128067 on epoch=78
06/01/2022 15:44:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.100706 on epoch=79
06/01/2022 15:44:56 - INFO - __main__ - Step 730 Global step 730 Train loss 0.091293 on epoch=81
06/01/2022 15:45:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.136479 on epoch=82
06/01/2022 15:45:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.111167 on epoch=83
06/01/2022 15:45:09 - INFO - __main__ - Global step 750 Train loss 0.113542 ACC 0.8773584905660378 on epoch=83
06/01/2022 15:45:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.072830 on epoch=84
06/01/2022 15:45:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.129074 on epoch=85
06/01/2022 15:45:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.058444 on epoch=86
06/01/2022 15:45:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.068271 on epoch=87
06/01/2022 15:45:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.077460 on epoch=88
06/01/2022 15:45:38 - INFO - __main__ - Global step 800 Train loss 0.081216 ACC 0.7264150943396226 on epoch=88
06/01/2022 15:45:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.021922 on epoch=89
06/01/2022 15:45:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.004247 on epoch=91
06/01/2022 15:45:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.009077 on epoch=92
06/01/2022 15:45:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.011137 on epoch=93
06/01/2022 15:46:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.042957 on epoch=94
06/01/2022 15:46:06 - INFO - __main__ - Global step 850 Train loss 0.017868 ACC 0.8867924528301887 on epoch=94
06/01/2022 15:46:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.009474 on epoch=95
06/01/2022 15:46:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.003684 on epoch=96
06/01/2022 15:46:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000610 on epoch=97
06/01/2022 15:46:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.012260 on epoch=98
06/01/2022 15:46:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.021177 on epoch=99
06/01/2022 15:46:35 - INFO - __main__ - Global step 900 Train loss 0.009441 ACC 0.8679245283018868 on epoch=99
06/01/2022 15:46:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.077345 on epoch=101
06/01/2022 15:46:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.014512 on epoch=102
06/01/2022 15:46:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.002983 on epoch=103
06/01/2022 15:46:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.001521 on epoch=104
06/01/2022 15:47:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.001818 on epoch=105
06/01/2022 15:47:03 - INFO - __main__ - Global step 950 Train loss 0.019636 ACC 0.8584905660377359 on epoch=105
06/01/2022 15:47:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.002837 on epoch=106
06/01/2022 15:47:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.009522 on epoch=107
06/01/2022 15:47:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.012832 on epoch=108
06/01/2022 15:47:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.005888 on epoch=109
06/01/2022 15:47:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.001600 on epoch=111
06/01/2022 15:47:31 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 15:47:31 - INFO - __main__ - Printing 3 examples
06/01/2022 15:47:31 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/01/2022 15:47:31 - INFO - __main__ - ['contradiction']
06/01/2022 15:47:31 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/01/2022 15:47:31 - INFO - __main__ - ['contradiction']
06/01/2022 15:47:31 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/01/2022 15:47:31 - INFO - __main__ - ['contradiction']
06/01/2022 15:47:31 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:47:31 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:47:32 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 15:47:32 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 15:47:32 - INFO - __main__ - Printing 3 examples
06/01/2022 15:47:32 - INFO - __main__ -  [superglue-cb] premise: B: but if they get the little tiny kids saving it now, in five years, when they get bigger, it'll work a little bit more, too. A: Yeah.  True. B: Because it's, we've all got to do it right now. I just, I really amazed to find out that, eighty per cent are filled now, in garbage fills. In five years we're supposed to be at max. A: Uh-huh. B: I don't think I can keep my own garbage. [SEP] hypothesis: she can keep her own garbage
06/01/2022 15:47:32 - INFO - __main__ - ['contradiction']
06/01/2022 15:47:32 - INFO - __main__ -  [superglue-cb] premise: You really don't know anything about me, do you, despite all that wallowing in my mind? As it happens I don't think I'm the right person to lead humanity into the future no. [SEP] hypothesis: she is the right person to lead humanity into the future
06/01/2022 15:47:32 - INFO - __main__ - ['contradiction']
06/01/2022 15:47:32 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/01/2022 15:47:32 - INFO - __main__ - ['contradiction']
06/01/2022 15:47:32 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:47:32 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:47:32 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 15:47:32 - INFO - __main__ - Global step 1000 Train loss 0.006536 ACC 0.8584905660377359 on epoch=111
06/01/2022 15:47:32 - INFO - __main__ - save last model!
06/01/2022 15:47:39 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 15:47:40 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 15:47:40 - INFO - __main__ - Printing 3 examples
06/01/2022 15:47:40 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 15:47:40 - INFO - __main__ - ['contradiction']
06/01/2022 15:47:40 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 15:47:40 - INFO - __main__ - ['neutral']
06/01/2022 15:47:40 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 15:47:40 - INFO - __main__ - ['entailment']
06/01/2022 15:47:40 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:47:40 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:47:40 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 15:47:41 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_42_0.0005_8_predictions.txt
06/01/2022 15:47:41 - INFO - __main__ - ACC on test data: 0.6964
06/01/2022 15:47:42 - INFO - __main__ - prefix=superglue-cb_64_42, lr=0.0005, bsz=8, dev_performance=0.8962264150943396, test_performance=0.6964285714285714
06/01/2022 15:47:42 - INFO - __main__ - Running ... prefix=superglue-cb_64_42, lr=0.0003, bsz=8 ...
06/01/2022 15:47:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 15:47:42 - INFO - __main__ - Starting training!
06/01/2022 15:47:43 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 15:47:43 - INFO - __main__ - Printing 3 examples
06/01/2022 15:47:43 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/01/2022 15:47:43 - INFO - __main__ - ['contradiction']
06/01/2022 15:47:43 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/01/2022 15:47:43 - INFO - __main__ - ['contradiction']
06/01/2022 15:47:43 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/01/2022 15:47:43 - INFO - __main__ - ['contradiction']
06/01/2022 15:47:43 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:47:43 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:47:43 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 15:47:43 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 15:47:43 - INFO - __main__ - Printing 3 examples
06/01/2022 15:47:43 - INFO - __main__ -  [superglue-cb] premise: B: but if they get the little tiny kids saving it now, in five years, when they get bigger, it'll work a little bit more, too. A: Yeah.  True. B: Because it's, we've all got to do it right now. I just, I really amazed to find out that, eighty per cent are filled now, in garbage fills. In five years we're supposed to be at max. A: Uh-huh. B: I don't think I can keep my own garbage. [SEP] hypothesis: she can keep her own garbage
06/01/2022 15:47:43 - INFO - __main__ - ['contradiction']
06/01/2022 15:47:43 - INFO - __main__ -  [superglue-cb] premise: You really don't know anything about me, do you, despite all that wallowing in my mind? As it happens I don't think I'm the right person to lead humanity into the future no. [SEP] hypothesis: she is the right person to lead humanity into the future
06/01/2022 15:47:43 - INFO - __main__ - ['contradiction']
06/01/2022 15:47:43 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/01/2022 15:47:43 - INFO - __main__ - ['contradiction']
06/01/2022 15:47:43 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:47:43 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:47:43 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 15:47:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 15:47:56 - INFO - __main__ - Starting training!
06/01/2022 15:48:00 - INFO - __main__ - Step 10 Global step 10 Train loss 24.800394 on epoch=1
06/01/2022 15:48:05 - INFO - __main__ - Step 20 Global step 20 Train loss 17.726284 on epoch=2
06/01/2022 15:48:11 - INFO - __main__ - Step 30 Global step 30 Train loss 12.439157 on epoch=3
06/01/2022 15:48:16 - INFO - __main__ - Step 40 Global step 40 Train loss 10.592180 on epoch=4
06/01/2022 15:48:21 - INFO - __main__ - Step 50 Global step 50 Train loss 10.260388 on epoch=5
06/01/2022 15:48:23 - INFO - __main__ - Global step 50 Train loss 15.163680 ACC 0.0 on epoch=5
06/01/2022 15:48:29 - INFO - __main__ - Step 60 Global step 60 Train loss 9.917273 on epoch=6
06/01/2022 15:48:34 - INFO - __main__ - Step 70 Global step 70 Train loss 9.232526 on epoch=7
06/01/2022 15:48:40 - INFO - __main__ - Step 80 Global step 80 Train loss 7.778076 on epoch=8
06/01/2022 15:48:45 - INFO - __main__ - Step 90 Global step 90 Train loss 7.358935 on epoch=9
06/01/2022 15:48:50 - INFO - __main__ - Step 100 Global step 100 Train loss 6.612586 on epoch=11
06/01/2022 15:48:52 - INFO - __main__ - Global step 100 Train loss 8.179879 ACC 0.27358490566037735 on epoch=11
06/01/2022 15:48:58 - INFO - __main__ - Step 110 Global step 110 Train loss 6.017504 on epoch=12
06/01/2022 15:49:04 - INFO - __main__ - Step 120 Global step 120 Train loss 4.717624 on epoch=13
06/01/2022 15:49:09 - INFO - __main__ - Step 130 Global step 130 Train loss 3.975924 on epoch=14
06/01/2022 15:49:14 - INFO - __main__ - Step 140 Global step 140 Train loss 1.729956 on epoch=15
06/01/2022 15:49:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.609913 on epoch=16
06/01/2022 15:49:22 - INFO - __main__ - Global step 150 Train loss 3.410184 ACC 0.5188679245283019 on epoch=16
06/01/2022 15:49:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.504493 on epoch=17
06/01/2022 15:49:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.677619 on epoch=18
06/01/2022 15:49:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.307259 on epoch=19
06/01/2022 15:49:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.180175 on epoch=21
06/01/2022 15:49:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.205988 on epoch=22
06/01/2022 15:49:51 - INFO - __main__ - Global step 200 Train loss 0.375107 ACC 0.5471698113207547 on epoch=22
06/01/2022 15:49:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.199753 on epoch=23
06/01/2022 15:50:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.145721 on epoch=24
06/01/2022 15:50:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.172126 on epoch=25
06/01/2022 15:50:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.106072 on epoch=26
06/01/2022 15:50:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.078387 on epoch=27
06/01/2022 15:50:21 - INFO - __main__ - Global step 250 Train loss 0.140412 ACC 0.7547169811320755 on epoch=27
06/01/2022 15:50:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.060777 on epoch=28
06/01/2022 15:50:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.049583 on epoch=29
06/01/2022 15:50:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.058454 on epoch=31
06/01/2022 15:50:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.021340 on epoch=32
06/01/2022 15:50:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.009627 on epoch=33
06/01/2022 15:50:51 - INFO - __main__ - Global step 300 Train loss 0.039956 ACC 0.8490566037735849 on epoch=33
06/01/2022 15:50:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.010181 on epoch=34
06/01/2022 15:51:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.028790 on epoch=35
06/01/2022 15:51:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.080087 on epoch=36
06/01/2022 15:51:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.019347 on epoch=37
06/01/2022 15:51:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.018061 on epoch=38
06/01/2022 15:51:21 - INFO - __main__ - Global step 350 Train loss 0.031293 ACC 0.8773584905660378 on epoch=38
06/01/2022 15:51:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.001936 on epoch=39
06/01/2022 15:51:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.027936 on epoch=41
06/01/2022 15:51:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.003070 on epoch=42
06/01/2022 15:51:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.002691 on epoch=43
06/01/2022 15:51:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.001162 on epoch=44
06/01/2022 15:51:50 - INFO - __main__ - Global step 400 Train loss 0.007359 ACC 0.8962264150943396 on epoch=44
06/01/2022 15:51:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.062225 on epoch=45
06/01/2022 15:52:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.001411 on epoch=46
06/01/2022 15:52:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.000701 on epoch=47
06/01/2022 15:52:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.001979 on epoch=48
06/01/2022 15:52:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.000341 on epoch=49
06/01/2022 15:52:20 - INFO - __main__ - Global step 450 Train loss 0.013332 ACC 0.9339622641509434 on epoch=49
06/01/2022 15:52:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.001089 on epoch=51
06/01/2022 15:52:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.001415 on epoch=52
06/01/2022 15:52:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.002621 on epoch=53
06/01/2022 15:52:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.000262 on epoch=54
06/01/2022 15:52:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.000735 on epoch=55
06/01/2022 15:52:49 - INFO - __main__ - Global step 500 Train loss 0.001224 ACC 0.9433962264150944 on epoch=55
06/01/2022 15:52:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.003826 on epoch=56
06/01/2022 15:53:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003327 on epoch=57
06/01/2022 15:53:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.000421 on epoch=58
06/01/2022 15:53:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000147 on epoch=59
06/01/2022 15:53:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.019286 on epoch=61
06/01/2022 15:53:19 - INFO - __main__ - Global step 550 Train loss 0.005401 ACC 0.9150943396226415 on epoch=61
06/01/2022 15:53:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000429 on epoch=62
06/01/2022 15:53:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.000691 on epoch=63
06/01/2022 15:53:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.703319 on epoch=64
06/01/2022 15:53:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.240764 on epoch=65
06/01/2022 15:53:45 - INFO - __main__ - Step 600 Global step 600 Train loss 1.857666 on epoch=66
06/01/2022 15:53:47 - INFO - __main__ - Global step 600 Train loss 0.560574 ACC 0.5188679245283019 on epoch=66
06/01/2022 15:53:52 - INFO - __main__ - Step 610 Global step 610 Train loss 1.626715 on epoch=67
06/01/2022 15:53:58 - INFO - __main__ - Step 620 Global step 620 Train loss 1.604679 on epoch=68
06/01/2022 15:54:03 - INFO - __main__ - Step 630 Global step 630 Train loss 1.594885 on epoch=69
06/01/2022 15:54:08 - INFO - __main__ - Step 640 Global step 640 Train loss 1.013544 on epoch=71
06/01/2022 15:54:14 - INFO - __main__ - Step 650 Global step 650 Train loss 1.357878 on epoch=72
06/01/2022 15:54:16 - INFO - __main__ - Global step 650 Train loss 1.439540 ACC 0.5188679245283019 on epoch=72
06/01/2022 15:54:21 - INFO - __main__ - Step 660 Global step 660 Train loss 1.325048 on epoch=73
06/01/2022 15:54:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.783746 on epoch=74
06/01/2022 15:54:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.821470 on epoch=75
06/01/2022 15:54:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.871127 on epoch=76
06/01/2022 15:54:42 - INFO - __main__ - Step 700 Global step 700 Train loss 1.033563 on epoch=77
06/01/2022 15:54:45 - INFO - __main__ - Global step 700 Train loss 0.966991 ACC 0.5188679245283019 on epoch=77
06/01/2022 15:54:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.658684 on epoch=78
06/01/2022 15:54:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.588051 on epoch=79
06/01/2022 15:55:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.637746 on epoch=81
06/01/2022 15:55:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.567309 on epoch=82
06/01/2022 15:55:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.541618 on epoch=83
06/01/2022 15:55:13 - INFO - __main__ - Global step 750 Train loss 0.598682 ACC 0.9056603773584906 on epoch=83
06/01/2022 15:55:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.562421 on epoch=84
06/01/2022 15:55:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.012276 on epoch=85
06/01/2022 15:55:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000068 on epoch=86
06/01/2022 15:55:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.014922 on epoch=87
06/01/2022 15:55:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.001710 on epoch=88
06/01/2022 15:55:42 - INFO - __main__ - Global step 800 Train loss 0.118279 ACC 0.9339622641509434 on epoch=88
06/01/2022 15:55:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.109795 on epoch=89
06/01/2022 15:55:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000094 on epoch=91
06/01/2022 15:55:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000050 on epoch=92
06/01/2022 15:56:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.003232 on epoch=93
06/01/2022 15:56:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000373 on epoch=94
06/01/2022 15:56:11 - INFO - __main__ - Global step 850 Train loss 0.022709 ACC 0.9339622641509434 on epoch=94
06/01/2022 15:56:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000176 on epoch=95
06/01/2022 15:56:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000173 on epoch=96
06/01/2022 15:56:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001024 on epoch=97
06/01/2022 15:56:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000075 on epoch=98
06/01/2022 15:56:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000212 on epoch=99
06/01/2022 15:56:40 - INFO - __main__ - Global step 900 Train loss 0.000332 ACC 0.9339622641509434 on epoch=99
06/01/2022 15:56:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000077 on epoch=101
06/01/2022 15:56:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000052 on epoch=102
06/01/2022 15:56:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000089 on epoch=103
06/01/2022 15:57:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000032 on epoch=104
06/01/2022 15:57:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000091 on epoch=105
06/01/2022 15:57:08 - INFO - __main__ - Global step 950 Train loss 0.000068 ACC 0.9245283018867925 on epoch=105
06/01/2022 15:57:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000048 on epoch=106
06/01/2022 15:57:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.026578 on epoch=107
06/01/2022 15:57:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000080 on epoch=108
06/01/2022 15:57:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.023562 on epoch=109
06/01/2022 15:57:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.039650 on epoch=111
06/01/2022 15:57:36 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 15:57:36 - INFO - __main__ - Printing 3 examples
06/01/2022 15:57:36 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/01/2022 15:57:36 - INFO - __main__ - ['contradiction']
06/01/2022 15:57:36 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/01/2022 15:57:36 - INFO - __main__ - ['contradiction']
06/01/2022 15:57:36 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/01/2022 15:57:36 - INFO - __main__ - ['contradiction']
06/01/2022 15:57:36 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:57:36 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:57:36 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 15:57:36 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 15:57:36 - INFO - __main__ - Printing 3 examples
06/01/2022 15:57:36 - INFO - __main__ -  [superglue-cb] premise: B: but if they get the little tiny kids saving it now, in five years, when they get bigger, it'll work a little bit more, too. A: Yeah.  True. B: Because it's, we've all got to do it right now. I just, I really amazed to find out that, eighty per cent are filled now, in garbage fills. In five years we're supposed to be at max. A: Uh-huh. B: I don't think I can keep my own garbage. [SEP] hypothesis: she can keep her own garbage
06/01/2022 15:57:36 - INFO - __main__ - ['contradiction']
06/01/2022 15:57:36 - INFO - __main__ -  [superglue-cb] premise: You really don't know anything about me, do you, despite all that wallowing in my mind? As it happens I don't think I'm the right person to lead humanity into the future no. [SEP] hypothesis: she is the right person to lead humanity into the future
06/01/2022 15:57:36 - INFO - __main__ - ['contradiction']
06/01/2022 15:57:36 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/01/2022 15:57:36 - INFO - __main__ - ['contradiction']
06/01/2022 15:57:36 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:57:36 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:57:36 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 15:57:37 - INFO - __main__ - Global step 1000 Train loss 0.017984 ACC 0.9150943396226415 on epoch=111
06/01/2022 15:57:37 - INFO - __main__ - save last model!
06/01/2022 15:57:44 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 15:57:45 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 15:57:45 - INFO - __main__ - Printing 3 examples
06/01/2022 15:57:45 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 15:57:45 - INFO - __main__ - ['contradiction']
06/01/2022 15:57:45 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 15:57:45 - INFO - __main__ - ['neutral']
06/01/2022 15:57:45 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 15:57:45 - INFO - __main__ - ['entailment']
06/01/2022 15:57:45 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:57:45 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:57:45 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 15:57:46 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_42_0.0003_8_predictions.txt
06/01/2022 15:57:46 - INFO - __main__ - ACC on test data: 0.9286
06/01/2022 15:57:47 - INFO - __main__ - prefix=superglue-cb_64_42, lr=0.0003, bsz=8, dev_performance=0.9433962264150944, test_performance=0.9285714285714286
06/01/2022 15:57:47 - INFO - __main__ - Running ... prefix=superglue-cb_64_42, lr=0.0002, bsz=8 ...
06/01/2022 15:57:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 15:57:47 - INFO - __main__ - Starting training!
06/01/2022 15:57:48 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 15:57:48 - INFO - __main__ - Printing 3 examples
06/01/2022 15:57:48 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/01/2022 15:57:48 - INFO - __main__ - ['contradiction']
06/01/2022 15:57:48 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/01/2022 15:57:48 - INFO - __main__ - ['contradiction']
06/01/2022 15:57:48 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/01/2022 15:57:48 - INFO - __main__ - ['contradiction']
06/01/2022 15:57:48 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:57:48 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:57:48 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 15:57:48 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 15:57:48 - INFO - __main__ - Printing 3 examples
06/01/2022 15:57:48 - INFO - __main__ -  [superglue-cb] premise: B: but if they get the little tiny kids saving it now, in five years, when they get bigger, it'll work a little bit more, too. A: Yeah.  True. B: Because it's, we've all got to do it right now. I just, I really amazed to find out that, eighty per cent are filled now, in garbage fills. In five years we're supposed to be at max. A: Uh-huh. B: I don't think I can keep my own garbage. [SEP] hypothesis: she can keep her own garbage
06/01/2022 15:57:48 - INFO - __main__ - ['contradiction']
06/01/2022 15:57:48 - INFO - __main__ -  [superglue-cb] premise: You really don't know anything about me, do you, despite all that wallowing in my mind? As it happens I don't think I'm the right person to lead humanity into the future no. [SEP] hypothesis: she is the right person to lead humanity into the future
06/01/2022 15:57:48 - INFO - __main__ - ['contradiction']
06/01/2022 15:57:48 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/01/2022 15:57:48 - INFO - __main__ - ['contradiction']
06/01/2022 15:57:48 - INFO - __main__ - Tokenizing Input ...
06/01/2022 15:57:48 - INFO - __main__ - Tokenizing Output ...
06/01/2022 15:57:48 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 15:58:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 15:58:01 - INFO - __main__ - Starting training!
06/01/2022 15:58:06 - INFO - __main__ - Step 10 Global step 10 Train loss 23.359669 on epoch=1
06/01/2022 15:58:11 - INFO - __main__ - Step 20 Global step 20 Train loss 18.197792 on epoch=2
06/01/2022 15:58:16 - INFO - __main__ - Step 30 Global step 30 Train loss 13.540716 on epoch=3
06/01/2022 15:58:21 - INFO - __main__ - Step 40 Global step 40 Train loss 11.974695 on epoch=4
06/01/2022 15:58:26 - INFO - __main__ - Step 50 Global step 50 Train loss 11.152766 on epoch=5
06/01/2022 15:58:29 - INFO - __main__ - Global step 50 Train loss 15.645127 ACC 0.0 on epoch=5
06/01/2022 15:58:35 - INFO - __main__ - Step 60 Global step 60 Train loss 11.289784 on epoch=6
06/01/2022 15:58:40 - INFO - __main__ - Step 70 Global step 70 Train loss 9.370264 on epoch=7
06/01/2022 15:58:46 - INFO - __main__ - Step 80 Global step 80 Train loss 9.705342 on epoch=8
06/01/2022 15:58:51 - INFO - __main__ - Step 90 Global step 90 Train loss 8.789651 on epoch=9
06/01/2022 15:58:56 - INFO - __main__ - Step 100 Global step 100 Train loss 8.448669 on epoch=11
06/01/2022 15:58:59 - INFO - __main__ - Global step 100 Train loss 9.520742 ACC 0.0 on epoch=11
06/01/2022 15:59:04 - INFO - __main__ - Step 110 Global step 110 Train loss 7.603230 on epoch=12
06/01/2022 15:59:10 - INFO - __main__ - Step 120 Global step 120 Train loss 7.171333 on epoch=13
06/01/2022 15:59:15 - INFO - __main__ - Step 130 Global step 130 Train loss 6.956274 on epoch=14
06/01/2022 15:59:20 - INFO - __main__ - Step 140 Global step 140 Train loss 7.170659 on epoch=15
06/01/2022 15:59:26 - INFO - __main__ - Step 150 Global step 150 Train loss 6.267158 on epoch=16
06/01/2022 15:59:28 - INFO - __main__ - Global step 150 Train loss 7.033731 ACC 0.0 on epoch=16
06/01/2022 15:59:34 - INFO - __main__ - Step 160 Global step 160 Train loss 4.694421 on epoch=17
06/01/2022 15:59:39 - INFO - __main__ - Step 170 Global step 170 Train loss 4.114920 on epoch=18
06/01/2022 15:59:44 - INFO - __main__ - Step 180 Global step 180 Train loss 3.078619 on epoch=19
06/01/2022 15:59:50 - INFO - __main__ - Step 190 Global step 190 Train loss 2.270460 on epoch=21
06/01/2022 15:59:55 - INFO - __main__ - Step 200 Global step 200 Train loss 1.438524 on epoch=22
06/01/2022 15:59:57 - INFO - __main__ - Global step 200 Train loss 3.119389 ACC 0.6415094339622641 on epoch=22
06/01/2022 16:00:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.548194 on epoch=23
06/01/2022 16:00:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.392188 on epoch=24
06/01/2022 16:00:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.381095 on epoch=25
06/01/2022 16:00:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.357240 on epoch=26
06/01/2022 16:00:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.351311 on epoch=27
06/01/2022 16:00:27 - INFO - __main__ - Global step 250 Train loss 0.406005 ACC 0.7264150943396226 on epoch=27
06/01/2022 16:00:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.394459 on epoch=28
06/01/2022 16:00:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.244859 on epoch=29
06/01/2022 16:00:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.259420 on epoch=31
06/01/2022 16:00:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.280057 on epoch=32
06/01/2022 16:00:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.287728 on epoch=33
06/01/2022 16:00:56 - INFO - __main__ - Global step 300 Train loss 0.293305 ACC 0.8490566037735849 on epoch=33
06/01/2022 16:01:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.169507 on epoch=34
06/01/2022 16:01:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.159613 on epoch=35
06/01/2022 16:01:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.148736 on epoch=36
06/01/2022 16:01:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.184802 on epoch=37
06/01/2022 16:01:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.165423 on epoch=38
06/01/2022 16:01:25 - INFO - __main__ - Global step 350 Train loss 0.165616 ACC 0.8962264150943396 on epoch=38
06/01/2022 16:01:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.095272 on epoch=39
06/01/2022 16:01:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.055404 on epoch=41
06/01/2022 16:01:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.042695 on epoch=42
06/01/2022 16:01:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.018776 on epoch=43
06/01/2022 16:01:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.077919 on epoch=44
06/01/2022 16:01:55 - INFO - __main__ - Global step 400 Train loss 0.058013 ACC 0.8962264150943396 on epoch=44
06/01/2022 16:02:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.027334 on epoch=45
06/01/2022 16:02:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.006335 on epoch=46
06/01/2022 16:02:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.021787 on epoch=47
06/01/2022 16:02:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.007625 on epoch=48
06/01/2022 16:02:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.008957 on epoch=49
06/01/2022 16:02:24 - INFO - __main__ - Global step 450 Train loss 0.014408 ACC 0.8867924528301887 on epoch=49
06/01/2022 16:02:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.037164 on epoch=51
06/01/2022 16:02:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.004947 on epoch=52
06/01/2022 16:02:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.015657 on epoch=53
06/01/2022 16:02:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.026145 on epoch=54
06/01/2022 16:02:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.013581 on epoch=55
06/01/2022 16:02:52 - INFO - __main__ - Global step 500 Train loss 0.019499 ACC 0.9245283018867925 on epoch=55
06/01/2022 16:02:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.012279 on epoch=56
06/01/2022 16:03:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.006163 on epoch=57
06/01/2022 16:03:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.002547 on epoch=58
06/01/2022 16:03:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.000617 on epoch=59
06/01/2022 16:03:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.013710 on epoch=61
06/01/2022 16:03:22 - INFO - __main__ - Global step 550 Train loss 0.007063 ACC 0.9056603773584906 on epoch=61
06/01/2022 16:03:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.002860 on epoch=62
06/01/2022 16:03:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.019105 on epoch=63
06/01/2022 16:03:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000998 on epoch=64
06/01/2022 16:03:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.044793 on epoch=65
06/01/2022 16:03:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.012064 on epoch=66
06/01/2022 16:03:50 - INFO - __main__ - Global step 600 Train loss 0.015964 ACC 0.8867924528301887 on epoch=66
06/01/2022 16:03:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000795 on epoch=67
06/01/2022 16:04:01 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000678 on epoch=68
06/01/2022 16:04:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.007190 on epoch=69
06/01/2022 16:04:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000430 on epoch=71
06/01/2022 16:04:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.001694 on epoch=72
06/01/2022 16:04:19 - INFO - __main__ - Global step 650 Train loss 0.002158 ACC 0.9339622641509434 on epoch=72
06/01/2022 16:04:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000337 on epoch=73
06/01/2022 16:04:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.000200 on epoch=74
06/01/2022 16:04:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.024915 on epoch=75
06/01/2022 16:04:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000497 on epoch=76
06/01/2022 16:04:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000319 on epoch=77
06/01/2022 16:04:49 - INFO - __main__ - Global step 700 Train loss 0.005253 ACC 0.9150943396226415 on epoch=77
06/01/2022 16:04:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.069718 on epoch=78
06/01/2022 16:04:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.026519 on epoch=79
06/01/2022 16:05:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.043863 on epoch=81
06/01/2022 16:05:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000181 on epoch=82
06/01/2022 16:05:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.020153 on epoch=83
06/01/2022 16:05:18 - INFO - __main__ - Global step 750 Train loss 0.032087 ACC 0.8679245283018868 on epoch=83
06/01/2022 16:05:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.018640 on epoch=84
06/01/2022 16:05:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.013635 on epoch=85
06/01/2022 16:05:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.002525 on epoch=86
06/01/2022 16:05:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.047260 on epoch=87
06/01/2022 16:05:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000201 on epoch=88
06/01/2022 16:05:46 - INFO - __main__ - Global step 800 Train loss 0.016452 ACC 0.9150943396226415 on epoch=88
06/01/2022 16:05:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.001418 on epoch=89
06/01/2022 16:05:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000119 on epoch=91
06/01/2022 16:06:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.048477 on epoch=92
06/01/2022 16:06:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000667 on epoch=93
06/01/2022 16:06:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.036017 on epoch=94
06/01/2022 16:06:15 - INFO - __main__ - Global step 850 Train loss 0.017339 ACC 0.9433962264150944 on epoch=94
06/01/2022 16:06:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000344 on epoch=95
06/01/2022 16:06:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000092 on epoch=96
06/01/2022 16:06:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.046673 on epoch=97
06/01/2022 16:06:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.058683 on epoch=98
06/01/2022 16:06:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000022 on epoch=99
06/01/2022 16:06:44 - INFO - __main__ - Global step 900 Train loss 0.021163 ACC 0.9433962264150944 on epoch=99
06/01/2022 16:06:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000208 on epoch=101
06/01/2022 16:06:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.001076 on epoch=102
06/01/2022 16:07:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.001597 on epoch=103
06/01/2022 16:07:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.079483 on epoch=104
06/01/2022 16:07:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000045 on epoch=105
06/01/2022 16:07:13 - INFO - __main__ - Global step 950 Train loss 0.016482 ACC 0.9150943396226415 on epoch=105
06/01/2022 16:07:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.035136 on epoch=106
06/01/2022 16:07:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.007568 on epoch=107
06/01/2022 16:07:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000034 on epoch=108
06/01/2022 16:07:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000216 on epoch=109
06/01/2022 16:07:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000187 on epoch=111
06/01/2022 16:07:41 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 16:07:41 - INFO - __main__ - Printing 3 examples
06/01/2022 16:07:41 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/01/2022 16:07:41 - INFO - __main__ - ['contradiction']
06/01/2022 16:07:41 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/01/2022 16:07:41 - INFO - __main__ - ['contradiction']
06/01/2022 16:07:41 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/01/2022 16:07:41 - INFO - __main__ - ['contradiction']
06/01/2022 16:07:41 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:07:41 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:07:41 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 16:07:41 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 16:07:41 - INFO - __main__ - Printing 3 examples
06/01/2022 16:07:41 - INFO - __main__ -  [superglue-cb] premise: B: but if they get the little tiny kids saving it now, in five years, when they get bigger, it'll work a little bit more, too. A: Yeah.  True. B: Because it's, we've all got to do it right now. I just, I really amazed to find out that, eighty per cent are filled now, in garbage fills. In five years we're supposed to be at max. A: Uh-huh. B: I don't think I can keep my own garbage. [SEP] hypothesis: she can keep her own garbage
06/01/2022 16:07:41 - INFO - __main__ - ['contradiction']
06/01/2022 16:07:41 - INFO - __main__ -  [superglue-cb] premise: You really don't know anything about me, do you, despite all that wallowing in my mind? As it happens I don't think I'm the right person to lead humanity into the future no. [SEP] hypothesis: she is the right person to lead humanity into the future
06/01/2022 16:07:41 - INFO - __main__ - ['contradiction']
06/01/2022 16:07:41 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/01/2022 16:07:41 - INFO - __main__ - ['contradiction']
06/01/2022 16:07:41 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:07:41 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:07:41 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 16:07:42 - INFO - __main__ - Global step 1000 Train loss 0.008628 ACC 0.9433962264150944 on epoch=111
06/01/2022 16:07:42 - INFO - __main__ - save last model!
06/01/2022 16:07:49 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 16:07:50 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 16:07:50 - INFO - __main__ - Printing 3 examples
06/01/2022 16:07:50 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 16:07:50 - INFO - __main__ - ['contradiction']
06/01/2022 16:07:50 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 16:07:50 - INFO - __main__ - ['neutral']
06/01/2022 16:07:50 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 16:07:50 - INFO - __main__ - ['entailment']
06/01/2022 16:07:50 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:07:50 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:07:50 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 16:07:51 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_42_0.0002_8_predictions.txt
06/01/2022 16:07:51 - INFO - __main__ - ACC on test data: 0.9107
06/01/2022 16:07:52 - INFO - __main__ - prefix=superglue-cb_64_42, lr=0.0002, bsz=8, dev_performance=0.9433962264150944, test_performance=0.9107142857142857
06/01/2022 16:07:52 - INFO - __main__ - Running ... prefix=superglue-cb_64_42, lr=0.0001, bsz=8 ...
06/01/2022 16:07:53 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 16:07:53 - INFO - __main__ - Printing 3 examples
06/01/2022 16:07:53 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/01/2022 16:07:53 - INFO - __main__ - ['contradiction']
06/01/2022 16:07:53 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/01/2022 16:07:53 - INFO - __main__ - ['contradiction']
06/01/2022 16:07:53 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/01/2022 16:07:53 - INFO - __main__ - ['contradiction']
06/01/2022 16:07:53 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:07:53 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:07:53 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 16:07:53 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 16:07:53 - INFO - __main__ - Printing 3 examples
06/01/2022 16:07:53 - INFO - __main__ -  [superglue-cb] premise: B: but if they get the little tiny kids saving it now, in five years, when they get bigger, it'll work a little bit more, too. A: Yeah.  True. B: Because it's, we've all got to do it right now. I just, I really amazed to find out that, eighty per cent are filled now, in garbage fills. In five years we're supposed to be at max. A: Uh-huh. B: I don't think I can keep my own garbage. [SEP] hypothesis: she can keep her own garbage
06/01/2022 16:07:53 - INFO - __main__ - ['contradiction']
06/01/2022 16:07:53 - INFO - __main__ -  [superglue-cb] premise: You really don't know anything about me, do you, despite all that wallowing in my mind? As it happens I don't think I'm the right person to lead humanity into the future no. [SEP] hypothesis: she is the right person to lead humanity into the future
06/01/2022 16:07:53 - INFO - __main__ - ['contradiction']
06/01/2022 16:07:53 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/01/2022 16:07:53 - INFO - __main__ - ['contradiction']
06/01/2022 16:07:53 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:07:53 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:07:53 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 16:07:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 16:07:54 - INFO - __main__ - Starting training!
06/01/2022 16:08:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 16:08:06 - INFO - __main__ - Starting training!
06/01/2022 16:08:10 - INFO - __main__ - Step 10 Global step 10 Train loss 24.682190 on epoch=1
06/01/2022 16:08:15 - INFO - __main__ - Step 20 Global step 20 Train loss 22.496983 on epoch=2
06/01/2022 16:08:21 - INFO - __main__ - Step 30 Global step 30 Train loss 17.633089 on epoch=3
06/01/2022 16:08:26 - INFO - __main__ - Step 40 Global step 40 Train loss 14.084476 on epoch=4
06/01/2022 16:08:31 - INFO - __main__ - Step 50 Global step 50 Train loss 13.379910 on epoch=5
06/01/2022 16:09:03 - INFO - __main__ - Global step 50 Train loss 18.455328 ACC 0.0 on epoch=5
06/01/2022 16:09:09 - INFO - __main__ - Step 60 Global step 60 Train loss 13.367467 on epoch=6
06/01/2022 16:09:14 - INFO - __main__ - Step 70 Global step 70 Train loss 12.451441 on epoch=7
06/01/2022 16:09:19 - INFO - __main__ - Step 80 Global step 80 Train loss 12.418398 on epoch=8
06/01/2022 16:09:24 - INFO - __main__ - Step 90 Global step 90 Train loss 11.912671 on epoch=9
06/01/2022 16:09:30 - INFO - __main__ - Step 100 Global step 100 Train loss 11.028983 on epoch=11
06/01/2022 16:09:32 - INFO - __main__ - Global step 100 Train loss 12.235792 ACC 0.0 on epoch=11
06/01/2022 16:09:38 - INFO - __main__ - Step 110 Global step 110 Train loss 10.343380 on epoch=12
06/01/2022 16:09:43 - INFO - __main__ - Step 120 Global step 120 Train loss 10.979898 on epoch=13
06/01/2022 16:09:48 - INFO - __main__ - Step 130 Global step 130 Train loss 10.782425 on epoch=14
06/01/2022 16:09:54 - INFO - __main__ - Step 140 Global step 140 Train loss 10.515232 on epoch=15
06/01/2022 16:09:59 - INFO - __main__ - Step 150 Global step 150 Train loss 10.045980 on epoch=16
06/01/2022 16:10:02 - INFO - __main__ - Global step 150 Train loss 10.533383 ACC 0.018867924528301886 on epoch=16
06/01/2022 16:10:08 - INFO - __main__ - Step 160 Global step 160 Train loss 9.705858 on epoch=17
06/01/2022 16:10:14 - INFO - __main__ - Step 170 Global step 170 Train loss 10.041662 on epoch=18
06/01/2022 16:10:19 - INFO - __main__ - Step 180 Global step 180 Train loss 8.892129 on epoch=19
06/01/2022 16:10:24 - INFO - __main__ - Step 190 Global step 190 Train loss 8.395281 on epoch=21
06/01/2022 16:10:30 - INFO - __main__ - Step 200 Global step 200 Train loss 8.806353 on epoch=22
06/01/2022 16:10:32 - INFO - __main__ - Global step 200 Train loss 9.168257 ACC 0.0 on epoch=22
06/01/2022 16:10:37 - INFO - __main__ - Step 210 Global step 210 Train loss 8.461468 on epoch=23
06/01/2022 16:10:43 - INFO - __main__ - Step 220 Global step 220 Train loss 8.256670 on epoch=24
06/01/2022 16:10:48 - INFO - __main__ - Step 230 Global step 230 Train loss 8.382830 on epoch=25
06/01/2022 16:10:53 - INFO - __main__ - Step 240 Global step 240 Train loss 7.991461 on epoch=26
06/01/2022 16:10:59 - INFO - __main__ - Step 250 Global step 250 Train loss 7.746576 on epoch=27
06/01/2022 16:11:00 - INFO - __main__ - Global step 250 Train loss 8.167800 ACC 0.0 on epoch=27
06/01/2022 16:11:06 - INFO - __main__ - Step 260 Global step 260 Train loss 7.671520 on epoch=28
06/01/2022 16:11:11 - INFO - __main__ - Step 270 Global step 270 Train loss 7.072852 on epoch=29
06/01/2022 16:11:16 - INFO - __main__ - Step 280 Global step 280 Train loss 6.628814 on epoch=31
06/01/2022 16:11:22 - INFO - __main__ - Step 290 Global step 290 Train loss 6.804501 on epoch=32
06/01/2022 16:11:27 - INFO - __main__ - Step 300 Global step 300 Train loss 6.682400 on epoch=33
06/01/2022 16:11:29 - INFO - __main__ - Global step 300 Train loss 6.972017 ACC 0.018867924528301886 on epoch=33
06/01/2022 16:11:34 - INFO - __main__ - Step 310 Global step 310 Train loss 6.457572 on epoch=34
06/01/2022 16:11:40 - INFO - __main__ - Step 320 Global step 320 Train loss 5.802148 on epoch=35
06/01/2022 16:11:45 - INFO - __main__ - Step 330 Global step 330 Train loss 6.412162 on epoch=36
06/01/2022 16:11:50 - INFO - __main__ - Step 340 Global step 340 Train loss 4.787469 on epoch=37
06/01/2022 16:11:55 - INFO - __main__ - Step 350 Global step 350 Train loss 4.881675 on epoch=38
06/01/2022 16:11:57 - INFO - __main__ - Global step 350 Train loss 5.668206 ACC 0.0 on epoch=38
06/01/2022 16:12:02 - INFO - __main__ - Step 360 Global step 360 Train loss 4.599889 on epoch=39
06/01/2022 16:12:08 - INFO - __main__ - Step 370 Global step 370 Train loss 4.124937 on epoch=41
06/01/2022 16:12:13 - INFO - __main__ - Step 380 Global step 380 Train loss 3.370105 on epoch=42
06/01/2022 16:12:18 - INFO - __main__ - Step 390 Global step 390 Train loss 3.187677 on epoch=43
06/01/2022 16:12:23 - INFO - __main__ - Step 400 Global step 400 Train loss 3.272949 on epoch=44
06/01/2022 16:12:25 - INFO - __main__ - Global step 400 Train loss 3.711111 ACC 0.41509433962264153 on epoch=44
06/01/2022 16:12:31 - INFO - __main__ - Step 410 Global step 410 Train loss 3.078171 on epoch=45
06/01/2022 16:12:37 - INFO - __main__ - Step 420 Global step 420 Train loss 2.805573 on epoch=46
06/01/2022 16:12:42 - INFO - __main__ - Step 430 Global step 430 Train loss 2.345901 on epoch=47
06/01/2022 16:12:47 - INFO - __main__ - Step 440 Global step 440 Train loss 3.074655 on epoch=48
06/01/2022 16:12:52 - INFO - __main__ - Step 450 Global step 450 Train loss 2.661410 on epoch=49
06/01/2022 16:12:54 - INFO - __main__ - Global step 450 Train loss 2.793142 ACC 0.5188679245283019 on epoch=49
06/01/2022 16:13:00 - INFO - __main__ - Step 460 Global step 460 Train loss 2.288468 on epoch=51
06/01/2022 16:13:06 - INFO - __main__ - Step 470 Global step 470 Train loss 2.225222 on epoch=52
06/01/2022 16:13:11 - INFO - __main__ - Step 480 Global step 480 Train loss 2.342331 on epoch=53
06/01/2022 16:13:16 - INFO - __main__ - Step 490 Global step 490 Train loss 2.072229 on epoch=54
06/01/2022 16:13:21 - INFO - __main__ - Step 500 Global step 500 Train loss 2.914873 on epoch=55
06/01/2022 16:13:23 - INFO - __main__ - Global step 500 Train loss 2.368625 ACC 0.5188679245283019 on epoch=55
06/01/2022 16:13:28 - INFO - __main__ - Step 510 Global step 510 Train loss 2.661079 on epoch=56
06/01/2022 16:13:34 - INFO - __main__ - Step 520 Global step 520 Train loss 2.109654 on epoch=57
06/01/2022 16:13:39 - INFO - __main__ - Step 530 Global step 530 Train loss 2.355927 on epoch=58
06/01/2022 16:13:44 - INFO - __main__ - Step 540 Global step 540 Train loss 1.922567 on epoch=59
06/01/2022 16:13:49 - INFO - __main__ - Step 550 Global step 550 Train loss 2.659873 on epoch=61
06/01/2022 16:13:51 - INFO - __main__ - Global step 550 Train loss 2.341820 ACC 0.5188679245283019 on epoch=61
06/01/2022 16:13:57 - INFO - __main__ - Step 560 Global step 560 Train loss 1.826523 on epoch=62
06/01/2022 16:14:02 - INFO - __main__ - Step 570 Global step 570 Train loss 2.043212 on epoch=63
06/01/2022 16:14:07 - INFO - __main__ - Step 580 Global step 580 Train loss 2.201346 on epoch=64
06/01/2022 16:14:12 - INFO - __main__ - Step 590 Global step 590 Train loss 2.236694 on epoch=65
06/01/2022 16:14:17 - INFO - __main__ - Step 600 Global step 600 Train loss 1.966653 on epoch=66
06/01/2022 16:14:20 - INFO - __main__ - Global step 600 Train loss 2.054886 ACC 0.5188679245283019 on epoch=66
06/01/2022 16:14:25 - INFO - __main__ - Step 610 Global step 610 Train loss 1.843628 on epoch=67
06/01/2022 16:14:30 - INFO - __main__ - Step 620 Global step 620 Train loss 1.627066 on epoch=68
06/01/2022 16:14:35 - INFO - __main__ - Step 630 Global step 630 Train loss 2.197802 on epoch=69
06/01/2022 16:14:41 - INFO - __main__ - Step 640 Global step 640 Train loss 2.008249 on epoch=71
06/01/2022 16:14:46 - INFO - __main__ - Step 650 Global step 650 Train loss 2.022032 on epoch=72
06/01/2022 16:14:48 - INFO - __main__ - Global step 650 Train loss 1.939755 ACC 0.5188679245283019 on epoch=72
06/01/2022 16:14:53 - INFO - __main__ - Step 660 Global step 660 Train loss 1.864735 on epoch=73
06/01/2022 16:14:59 - INFO - __main__ - Step 670 Global step 670 Train loss 1.723993 on epoch=74
06/01/2022 16:15:04 - INFO - __main__ - Step 680 Global step 680 Train loss 2.468799 on epoch=75
06/01/2022 16:15:09 - INFO - __main__ - Step 690 Global step 690 Train loss 2.394815 on epoch=76
06/01/2022 16:15:15 - INFO - __main__ - Step 700 Global step 700 Train loss 1.710115 on epoch=77
06/01/2022 16:15:17 - INFO - __main__ - Global step 700 Train loss 2.032491 ACC 0.5188679245283019 on epoch=77
06/01/2022 16:15:22 - INFO - __main__ - Step 710 Global step 710 Train loss 1.505793 on epoch=78
06/01/2022 16:15:27 - INFO - __main__ - Step 720 Global step 720 Train loss 1.506079 on epoch=79
06/01/2022 16:15:33 - INFO - __main__ - Step 730 Global step 730 Train loss 1.617433 on epoch=81
06/01/2022 16:15:38 - INFO - __main__ - Step 740 Global step 740 Train loss 1.426181 on epoch=82
06/01/2022 16:15:43 - INFO - __main__ - Step 750 Global step 750 Train loss 1.589990 on epoch=83
06/01/2022 16:15:45 - INFO - __main__ - Global step 750 Train loss 1.529095 ACC 0.5188679245283019 on epoch=83
06/01/2022 16:15:51 - INFO - __main__ - Step 760 Global step 760 Train loss 1.680254 on epoch=84
06/01/2022 16:15:56 - INFO - __main__ - Step 770 Global step 770 Train loss 1.735624 on epoch=85
06/01/2022 16:16:01 - INFO - __main__ - Step 780 Global step 780 Train loss 2.078688 on epoch=86
06/01/2022 16:16:06 - INFO - __main__ - Step 790 Global step 790 Train loss 1.438555 on epoch=87
06/01/2022 16:16:12 - INFO - __main__ - Step 800 Global step 800 Train loss 1.949449 on epoch=88
06/01/2022 16:16:14 - INFO - __main__ - Global step 800 Train loss 1.776514 ACC 0.5188679245283019 on epoch=88
06/01/2022 16:16:19 - INFO - __main__ - Step 810 Global step 810 Train loss 1.838642 on epoch=89
06/01/2022 16:16:25 - INFO - __main__ - Step 820 Global step 820 Train loss 1.497087 on epoch=91
06/01/2022 16:16:30 - INFO - __main__ - Step 830 Global step 830 Train loss 1.096739 on epoch=92
06/01/2022 16:16:35 - INFO - __main__ - Step 840 Global step 840 Train loss 1.746085 on epoch=93
06/01/2022 16:16:40 - INFO - __main__ - Step 850 Global step 850 Train loss 1.434182 on epoch=94
06/01/2022 16:16:43 - INFO - __main__ - Global step 850 Train loss 1.522547 ACC 0.5188679245283019 on epoch=94
06/01/2022 16:16:48 - INFO - __main__ - Step 860 Global step 860 Train loss 1.326622 on epoch=95
06/01/2022 16:16:53 - INFO - __main__ - Step 870 Global step 870 Train loss 1.506450 on epoch=96
06/01/2022 16:16:58 - INFO - __main__ - Step 880 Global step 880 Train loss 1.417332 on epoch=97
06/01/2022 16:17:04 - INFO - __main__ - Step 890 Global step 890 Train loss 1.522428 on epoch=98
06/01/2022 16:17:09 - INFO - __main__ - Step 900 Global step 900 Train loss 1.350752 on epoch=99
06/01/2022 16:17:11 - INFO - __main__ - Global step 900 Train loss 1.424717 ACC 0.5188679245283019 on epoch=99
06/01/2022 16:17:16 - INFO - __main__ - Step 910 Global step 910 Train loss 1.496220 on epoch=101
06/01/2022 16:17:22 - INFO - __main__ - Step 920 Global step 920 Train loss 1.229094 on epoch=102
06/01/2022 16:17:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.906010 on epoch=103
06/01/2022 16:17:32 - INFO - __main__ - Step 940 Global step 940 Train loss 1.811888 on epoch=104
06/01/2022 16:17:38 - INFO - __main__ - Step 950 Global step 950 Train loss 1.322748 on epoch=105
06/01/2022 16:17:40 - INFO - __main__ - Global step 950 Train loss 1.353192 ACC 0.5188679245283019 on epoch=105
06/01/2022 16:17:45 - INFO - __main__ - Step 960 Global step 960 Train loss 1.215506 on epoch=106
06/01/2022 16:17:51 - INFO - __main__ - Step 970 Global step 970 Train loss 1.505746 on epoch=107
06/01/2022 16:17:56 - INFO - __main__ - Step 980 Global step 980 Train loss 1.464392 on epoch=108
06/01/2022 16:18:01 - INFO - __main__ - Step 990 Global step 990 Train loss 1.279175 on epoch=109
06/01/2022 16:18:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.336925 on epoch=111
06/01/2022 16:18:07 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 16:18:07 - INFO - __main__ - Printing 3 examples
06/01/2022 16:18:07 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/01/2022 16:18:07 - INFO - __main__ - ['contradiction']
06/01/2022 16:18:07 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/01/2022 16:18:07 - INFO - __main__ - ['contradiction']
06/01/2022 16:18:07 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/01/2022 16:18:07 - INFO - __main__ - ['contradiction']
06/01/2022 16:18:07 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:18:08 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:18:08 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 16:18:08 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 16:18:08 - INFO - __main__ - Printing 3 examples
06/01/2022 16:18:08 - INFO - __main__ -  [superglue-cb] premise: A: Oh, yes. Animals have a way of talking. B: Alfie did. I tell you if I could have gotten a hold of that cat that day. A: I don't know uh, that I'd trade my dog in for the world. [SEP] hypothesis: he would trade his dog in for the world
06/01/2022 16:18:08 - INFO - __main__ - ['contradiction']
06/01/2022 16:18:08 - INFO - __main__ -  [superglue-cb] premise: B: but I found that, uh, it was made of some material which actually ended up rusting uh, after, A: Oh. B: even, despite, you know, diligent washing, it got rusty after about, uh, three weeks of use. And I don't think it was my fault because you know, I had made a point of like drying it off and cleaning it [SEP] hypothesis: it was his fault
06/01/2022 16:18:08 - INFO - __main__ - ['contradiction']
06/01/2022 16:18:08 - INFO - __main__ -  [superglue-cb] premise: A: I spend a lot of time reading about these things. I'm quite interested. I find it very exciting for the coverage we have now, today. B: Yes and I think we do get pretty good coverage. I don't feel that the American people is being shortchanged by uh, the news coverage. [SEP] hypothesis: the American people are being shortchanged by the news coverage
06/01/2022 16:18:08 - INFO - __main__ - ['contradiction']
06/01/2022 16:18:08 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:18:08 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:18:08 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 16:18:08 - INFO - __main__ - Global step 1000 Train loss 1.360349 ACC 0.5188679245283019 on epoch=111
06/01/2022 16:18:08 - INFO - __main__ - save last model!
06/01/2022 16:18:15 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 16:18:16 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 16:18:16 - INFO - __main__ - Printing 3 examples
06/01/2022 16:18:16 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 16:18:16 - INFO - __main__ - ['contradiction']
06/01/2022 16:18:16 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 16:18:16 - INFO - __main__ - ['neutral']
06/01/2022 16:18:16 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 16:18:16 - INFO - __main__ - ['entailment']
06/01/2022 16:18:16 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:18:16 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:18:16 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 16:18:17 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_42_0.0001_8_predictions.txt
06/01/2022 16:18:17 - INFO - __main__ - ACC on test data: 0.5000
06/01/2022 16:18:18 - INFO - __main__ - prefix=superglue-cb_64_42, lr=0.0001, bsz=8, dev_performance=0.5188679245283019, test_performance=0.5
06/01/2022 16:18:18 - INFO - __main__ - Running ... prefix=superglue-cb_64_87, lr=0.0005, bsz=8 ...
06/01/2022 16:18:19 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 16:18:19 - INFO - __main__ - Printing 3 examples
06/01/2022 16:18:19 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/01/2022 16:18:19 - INFO - __main__ - ['contradiction']
06/01/2022 16:18:19 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/01/2022 16:18:19 - INFO - __main__ - ['contradiction']
06/01/2022 16:18:19 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/01/2022 16:18:19 - INFO - __main__ - ['contradiction']
06/01/2022 16:18:19 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:18:19 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:18:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 16:18:19 - INFO - __main__ - Starting training!
06/01/2022 16:18:19 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 16:18:19 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 16:18:19 - INFO - __main__ - Printing 3 examples
06/01/2022 16:18:19 - INFO - __main__ -  [superglue-cb] premise: A: Oh, yes. Animals have a way of talking. B: Alfie did. I tell you if I could have gotten a hold of that cat that day. A: I don't know uh, that I'd trade my dog in for the world. [SEP] hypothesis: he would trade his dog in for the world
06/01/2022 16:18:19 - INFO - __main__ - ['contradiction']
06/01/2022 16:18:19 - INFO - __main__ -  [superglue-cb] premise: B: but I found that, uh, it was made of some material which actually ended up rusting uh, after, A: Oh. B: even, despite, you know, diligent washing, it got rusty after about, uh, three weeks of use. And I don't think it was my fault because you know, I had made a point of like drying it off and cleaning it [SEP] hypothesis: it was his fault
06/01/2022 16:18:19 - INFO - __main__ - ['contradiction']
06/01/2022 16:18:19 - INFO - __main__ -  [superglue-cb] premise: A: I spend a lot of time reading about these things. I'm quite interested. I find it very exciting for the coverage we have now, today. B: Yes and I think we do get pretty good coverage. I don't feel that the American people is being shortchanged by uh, the news coverage. [SEP] hypothesis: the American people are being shortchanged by the news coverage
06/01/2022 16:18:19 - INFO - __main__ - ['contradiction']
06/01/2022 16:18:19 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:18:19 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:18:19 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 16:18:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 16:18:30 - INFO - __main__ - Starting training!
06/01/2022 16:18:35 - INFO - __main__ - Step 10 Global step 10 Train loss 24.915854 on epoch=1
06/01/2022 16:18:40 - INFO - __main__ - Step 20 Global step 20 Train loss 20.964628 on epoch=2
06/01/2022 16:18:45 - INFO - __main__ - Step 30 Global step 30 Train loss 19.236868 on epoch=3
06/01/2022 16:18:50 - INFO - __main__ - Step 40 Global step 40 Train loss 15.680084 on epoch=4
06/01/2022 16:18:55 - INFO - __main__ - Step 50 Global step 50 Train loss 13.388837 on epoch=5
06/01/2022 16:18:59 - INFO - __main__ - Global step 50 Train loss 18.837255 ACC 0.0 on epoch=5
06/01/2022 16:19:05 - INFO - __main__ - Step 60 Global step 60 Train loss 11.833980 on epoch=6
06/01/2022 16:19:10 - INFO - __main__ - Step 70 Global step 70 Train loss 10.651770 on epoch=7
06/01/2022 16:19:15 - INFO - __main__ - Step 80 Global step 80 Train loss 9.009309 on epoch=8
06/01/2022 16:19:20 - INFO - __main__ - Step 90 Global step 90 Train loss 7.789870 on epoch=9
06/01/2022 16:19:26 - INFO - __main__ - Step 100 Global step 100 Train loss 5.321918 on epoch=11
06/01/2022 16:19:28 - INFO - __main__ - Global step 100 Train loss 8.921370 ACC 0.0 on epoch=11
06/01/2022 16:19:34 - INFO - __main__ - Step 110 Global step 110 Train loss 3.931998 on epoch=12
06/01/2022 16:19:39 - INFO - __main__ - Step 120 Global step 120 Train loss 2.999552 on epoch=13
06/01/2022 16:19:44 - INFO - __main__ - Step 130 Global step 130 Train loss 2.435135 on epoch=14
06/01/2022 16:19:50 - INFO - __main__ - Step 140 Global step 140 Train loss 2.917513 on epoch=15
06/01/2022 16:19:55 - INFO - __main__ - Step 150 Global step 150 Train loss 2.349786 on epoch=16
06/01/2022 16:19:57 - INFO - __main__ - Global step 150 Train loss 2.926797 ACC 0.5188679245283019 on epoch=16
06/01/2022 16:20:03 - INFO - __main__ - Step 160 Global step 160 Train loss 1.532794 on epoch=17
06/01/2022 16:20:08 - INFO - __main__ - Step 170 Global step 170 Train loss 1.645698 on epoch=18
06/01/2022 16:20:14 - INFO - __main__ - Step 180 Global step 180 Train loss 1.441521 on epoch=19
06/01/2022 16:20:19 - INFO - __main__ - Step 190 Global step 190 Train loss 1.902604 on epoch=21
06/01/2022 16:20:24 - INFO - __main__ - Step 200 Global step 200 Train loss 1.361090 on epoch=22
06/01/2022 16:20:27 - INFO - __main__ - Global step 200 Train loss 1.576741 ACC 0.5188679245283019 on epoch=22
06/01/2022 16:20:32 - INFO - __main__ - Step 210 Global step 210 Train loss 1.336066 on epoch=23
06/01/2022 16:20:37 - INFO - __main__ - Step 220 Global step 220 Train loss 1.402834 on epoch=24
06/01/2022 16:20:43 - INFO - __main__ - Step 230 Global step 230 Train loss 1.237994 on epoch=25
06/01/2022 16:20:48 - INFO - __main__ - Step 240 Global step 240 Train loss 1.019971 on epoch=26
06/01/2022 16:20:54 - INFO - __main__ - Step 250 Global step 250 Train loss 1.020875 on epoch=27
06/01/2022 16:20:56 - INFO - __main__ - Global step 250 Train loss 1.203548 ACC 0.5188679245283019 on epoch=27
06/01/2022 16:21:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.804680 on epoch=28
06/01/2022 16:21:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.974526 on epoch=29
06/01/2022 16:21:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.853160 on epoch=31
06/01/2022 16:21:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.756363 on epoch=32
06/01/2022 16:21:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.652533 on epoch=33
06/01/2022 16:21:25 - INFO - __main__ - Global step 300 Train loss 0.808252 ACC 0.5188679245283019 on epoch=33
06/01/2022 16:21:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.652777 on epoch=34
06/01/2022 16:21:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.528734 on epoch=35
06/01/2022 16:21:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.566146 on epoch=36
06/01/2022 16:21:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.545468 on epoch=37
06/01/2022 16:21:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.464800 on epoch=38
06/01/2022 16:21:55 - INFO - __main__ - Global step 350 Train loss 0.551585 ACC 0.5188679245283019 on epoch=38
06/01/2022 16:22:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.541692 on epoch=39
06/01/2022 16:22:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.721420 on epoch=41
06/01/2022 16:22:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.461309 on epoch=42
06/01/2022 16:22:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.458304 on epoch=43
06/01/2022 16:22:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.506594 on epoch=44
06/01/2022 16:22:25 - INFO - __main__ - Global step 400 Train loss 0.537864 ACC 0.4811320754716981 on epoch=44
06/01/2022 16:22:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.523083 on epoch=45
06/01/2022 16:22:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.435301 on epoch=46
06/01/2022 16:22:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.381933 on epoch=47
06/01/2022 16:22:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.418552 on epoch=48
06/01/2022 16:22:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.437642 on epoch=49
06/01/2022 16:22:54 - INFO - __main__ - Global step 450 Train loss 0.439302 ACC 0.8018867924528302 on epoch=49
06/01/2022 16:23:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.320282 on epoch=51
06/01/2022 16:23:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.342928 on epoch=52
06/01/2022 16:23:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.341680 on epoch=53
06/01/2022 16:23:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.289979 on epoch=54
06/01/2022 16:23:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.314857 on epoch=55
06/01/2022 16:23:23 - INFO - __main__ - Global step 500 Train loss 0.321945 ACC 0.8207547169811321 on epoch=55
06/01/2022 16:23:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.358029 on epoch=56
06/01/2022 16:23:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.365590 on epoch=57
06/01/2022 16:23:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.296163 on epoch=58
06/01/2022 16:23:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.289606 on epoch=59
06/01/2022 16:23:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.239791 on epoch=61
06/01/2022 16:23:53 - INFO - __main__ - Global step 550 Train loss 0.309836 ACC 0.8679245283018868 on epoch=61
06/01/2022 16:23:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.378477 on epoch=62
06/01/2022 16:24:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.251499 on epoch=63
06/01/2022 16:24:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.268588 on epoch=64
06/01/2022 16:24:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.287710 on epoch=65
06/01/2022 16:24:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.283195 on epoch=66
06/01/2022 16:24:23 - INFO - __main__ - Global step 600 Train loss 0.293894 ACC 0.8584905660377359 on epoch=66
06/01/2022 16:24:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.203186 on epoch=67
06/01/2022 16:24:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.237435 on epoch=68
06/01/2022 16:24:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.236061 on epoch=69
06/01/2022 16:24:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.145612 on epoch=71
06/01/2022 16:24:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.191233 on epoch=72
06/01/2022 16:24:51 - INFO - __main__ - Global step 650 Train loss 0.202705 ACC 0.9622641509433962 on epoch=72
06/01/2022 16:24:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.140747 on epoch=73
06/01/2022 16:25:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.175984 on epoch=74
06/01/2022 16:25:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.179128 on epoch=75
06/01/2022 16:25:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.151808 on epoch=76
06/01/2022 16:25:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.100970 on epoch=77
06/01/2022 16:25:21 - INFO - __main__ - Global step 700 Train loss 0.149727 ACC 0.8867924528301887 on epoch=77
06/01/2022 16:25:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.115799 on epoch=78
06/01/2022 16:25:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.079916 on epoch=79
06/01/2022 16:25:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.025946 on epoch=81
06/01/2022 16:25:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.051637 on epoch=82
06/01/2022 16:25:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.050719 on epoch=83
06/01/2022 16:25:50 - INFO - __main__ - Global step 750 Train loss 0.064803 ACC 0.9716981132075472 on epoch=83
06/01/2022 16:25:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.030839 on epoch=84
06/01/2022 16:26:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.013419 on epoch=85
06/01/2022 16:26:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.031276 on epoch=86
06/01/2022 16:26:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.012157 on epoch=87
06/01/2022 16:26:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.045715 on epoch=88
06/01/2022 16:26:19 - INFO - __main__ - Global step 800 Train loss 0.026681 ACC 0.9056603773584906 on epoch=88
06/01/2022 16:26:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.012268 on epoch=89
06/01/2022 16:26:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.010231 on epoch=91
06/01/2022 16:26:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.016153 on epoch=92
06/01/2022 16:26:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.003235 on epoch=93
06/01/2022 16:26:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.004294 on epoch=94
06/01/2022 16:26:48 - INFO - __main__ - Global step 850 Train loss 0.009236 ACC 0.9528301886792453 on epoch=94
06/01/2022 16:26:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.026034 on epoch=95
06/01/2022 16:26:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.020417 on epoch=96
06/01/2022 16:27:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000581 on epoch=97
06/01/2022 16:27:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.001582 on epoch=98
06/01/2022 16:27:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000725 on epoch=99
06/01/2022 16:27:17 - INFO - __main__ - Global step 900 Train loss 0.009868 ACC 0.9622641509433962 on epoch=99
06/01/2022 16:27:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000450 on epoch=101
06/01/2022 16:27:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.003981 on epoch=102
06/01/2022 16:27:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000742 on epoch=103
06/01/2022 16:27:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.001668 on epoch=104
06/01/2022 16:27:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000277 on epoch=105
06/01/2022 16:27:46 - INFO - __main__ - Global step 950 Train loss 0.001424 ACC 0.9622641509433962 on epoch=105
06/01/2022 16:27:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.025978 on epoch=106
06/01/2022 16:27:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.004236 on epoch=107
06/01/2022 16:28:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000330 on epoch=108
06/01/2022 16:28:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000523 on epoch=109
06/01/2022 16:28:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.001533 on epoch=111
06/01/2022 16:28:13 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 16:28:13 - INFO - __main__ - Printing 3 examples
06/01/2022 16:28:13 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/01/2022 16:28:13 - INFO - __main__ - ['contradiction']
06/01/2022 16:28:13 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/01/2022 16:28:13 - INFO - __main__ - ['contradiction']
06/01/2022 16:28:13 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/01/2022 16:28:13 - INFO - __main__ - ['contradiction']
06/01/2022 16:28:13 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:28:13 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:28:13 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 16:28:13 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 16:28:13 - INFO - __main__ - Printing 3 examples
06/01/2022 16:28:13 - INFO - __main__ -  [superglue-cb] premise: A: Oh, yes. Animals have a way of talking. B: Alfie did. I tell you if I could have gotten a hold of that cat that day. A: I don't know uh, that I'd trade my dog in for the world. [SEP] hypothesis: he would trade his dog in for the world
06/01/2022 16:28:13 - INFO - __main__ - ['contradiction']
06/01/2022 16:28:13 - INFO - __main__ -  [superglue-cb] premise: B: but I found that, uh, it was made of some material which actually ended up rusting uh, after, A: Oh. B: even, despite, you know, diligent washing, it got rusty after about, uh, three weeks of use. And I don't think it was my fault because you know, I had made a point of like drying it off and cleaning it [SEP] hypothesis: it was his fault
06/01/2022 16:28:13 - INFO - __main__ - ['contradiction']
06/01/2022 16:28:13 - INFO - __main__ -  [superglue-cb] premise: A: I spend a lot of time reading about these things. I'm quite interested. I find it very exciting for the coverage we have now, today. B: Yes and I think we do get pretty good coverage. I don't feel that the American people is being shortchanged by uh, the news coverage. [SEP] hypothesis: the American people are being shortchanged by the news coverage
06/01/2022 16:28:13 - INFO - __main__ - ['contradiction']
06/01/2022 16:28:13 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:28:13 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:28:13 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 16:28:14 - INFO - __main__ - Global step 1000 Train loss 0.006520 ACC 0.9811320754716981 on epoch=111
06/01/2022 16:28:15 - INFO - __main__ - save last model!
06/01/2022 16:28:22 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 16:28:23 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 16:28:23 - INFO - __main__ - Printing 3 examples
06/01/2022 16:28:23 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 16:28:23 - INFO - __main__ - ['contradiction']
06/01/2022 16:28:23 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 16:28:23 - INFO - __main__ - ['neutral']
06/01/2022 16:28:23 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 16:28:23 - INFO - __main__ - ['entailment']
06/01/2022 16:28:23 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:28:23 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:28:23 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 16:28:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 16:28:24 - INFO - __main__ - Starting training!
06/01/2022 16:28:25 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_87_0.0005_8_predictions.txt
06/01/2022 16:28:25 - INFO - __main__ - ACC on test data: 0.8750
06/01/2022 16:28:25 - INFO - __main__ - prefix=superglue-cb_64_87, lr=0.0005, bsz=8, dev_performance=0.9811320754716981, test_performance=0.875
06/01/2022 16:28:25 - INFO - __main__ - Running ... prefix=superglue-cb_64_87, lr=0.0003, bsz=8 ...
06/01/2022 16:28:26 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 16:28:26 - INFO - __main__ - Printing 3 examples
06/01/2022 16:28:26 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/01/2022 16:28:26 - INFO - __main__ - ['contradiction']
06/01/2022 16:28:26 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/01/2022 16:28:26 - INFO - __main__ - ['contradiction']
06/01/2022 16:28:26 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/01/2022 16:28:26 - INFO - __main__ - ['contradiction']
06/01/2022 16:28:26 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:28:26 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:28:26 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 16:28:26 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 16:28:26 - INFO - __main__ - Printing 3 examples
06/01/2022 16:28:26 - INFO - __main__ -  [superglue-cb] premise: A: Oh, yes. Animals have a way of talking. B: Alfie did. I tell you if I could have gotten a hold of that cat that day. A: I don't know uh, that I'd trade my dog in for the world. [SEP] hypothesis: he would trade his dog in for the world
06/01/2022 16:28:26 - INFO - __main__ - ['contradiction']
06/01/2022 16:28:26 - INFO - __main__ -  [superglue-cb] premise: B: but I found that, uh, it was made of some material which actually ended up rusting uh, after, A: Oh. B: even, despite, you know, diligent washing, it got rusty after about, uh, three weeks of use. And I don't think it was my fault because you know, I had made a point of like drying it off and cleaning it [SEP] hypothesis: it was his fault
06/01/2022 16:28:26 - INFO - __main__ - ['contradiction']
06/01/2022 16:28:26 - INFO - __main__ -  [superglue-cb] premise: A: I spend a lot of time reading about these things. I'm quite interested. I find it very exciting for the coverage we have now, today. B: Yes and I think we do get pretty good coverage. I don't feel that the American people is being shortchanged by uh, the news coverage. [SEP] hypothesis: the American people are being shortchanged by the news coverage
06/01/2022 16:28:26 - INFO - __main__ - ['contradiction']
06/01/2022 16:28:26 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:28:26 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:28:26 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 16:28:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 16:28:38 - INFO - __main__ - Starting training!
06/01/2022 16:28:42 - INFO - __main__ - Step 10 Global step 10 Train loss 24.020485 on epoch=1
06/01/2022 16:28:47 - INFO - __main__ - Step 20 Global step 20 Train loss 18.704966 on epoch=2
06/01/2022 16:28:52 - INFO - __main__ - Step 30 Global step 30 Train loss 14.529434 on epoch=3
06/01/2022 16:28:57 - INFO - __main__ - Step 40 Global step 40 Train loss 11.728481 on epoch=4
06/01/2022 16:29:03 - INFO - __main__ - Step 50 Global step 50 Train loss 11.178771 on epoch=5
06/01/2022 16:29:05 - INFO - __main__ - Global step 50 Train loss 16.032427 ACC 0.009433962264150943 on epoch=5
06/01/2022 16:29:11 - INFO - __main__ - Step 60 Global step 60 Train loss 10.302401 on epoch=6
06/01/2022 16:29:16 - INFO - __main__ - Step 70 Global step 70 Train loss 9.262691 on epoch=7
06/01/2022 16:29:22 - INFO - __main__ - Step 80 Global step 80 Train loss 8.177723 on epoch=8
06/01/2022 16:29:27 - INFO - __main__ - Step 90 Global step 90 Train loss 7.401264 on epoch=9
06/01/2022 16:29:32 - INFO - __main__ - Step 100 Global step 100 Train loss 6.776959 on epoch=11
06/01/2022 16:29:34 - INFO - __main__ - Global step 100 Train loss 8.384207 ACC 0.0 on epoch=11
06/01/2022 16:29:39 - INFO - __main__ - Step 110 Global step 110 Train loss 5.347444 on epoch=12
06/01/2022 16:29:45 - INFO - __main__ - Step 120 Global step 120 Train loss 4.583624 on epoch=13
06/01/2022 16:29:50 - INFO - __main__ - Step 130 Global step 130 Train loss 4.060931 on epoch=14
06/01/2022 16:29:55 - INFO - __main__ - Step 140 Global step 140 Train loss 3.176237 on epoch=15
06/01/2022 16:30:00 - INFO - __main__ - Step 150 Global step 150 Train loss 2.601489 on epoch=16
06/01/2022 16:30:02 - INFO - __main__ - Global step 150 Train loss 3.953945 ACC 0.0 on epoch=16
06/01/2022 16:30:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.490452 on epoch=17
06/01/2022 16:30:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.537126 on epoch=18
06/01/2022 16:30:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.472756 on epoch=19
06/01/2022 16:30:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.418197 on epoch=21
06/01/2022 16:30:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.363603 on epoch=22
06/01/2022 16:30:30 - INFO - __main__ - Global step 200 Train loss 0.456427 ACC 0.5188679245283019 on epoch=22
06/01/2022 16:30:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.416382 on epoch=23
06/01/2022 16:30:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.379531 on epoch=24
06/01/2022 16:30:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.414546 on epoch=25
06/01/2022 16:30:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.398721 on epoch=26
06/01/2022 16:30:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.374478 on epoch=27
06/01/2022 16:30:59 - INFO - __main__ - Global step 250 Train loss 0.396732 ACC 0.5188679245283019 on epoch=27
06/01/2022 16:31:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.793266 on epoch=28
06/01/2022 16:31:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.399564 on epoch=29
06/01/2022 16:31:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.352452 on epoch=31
06/01/2022 16:31:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.322588 on epoch=32
06/01/2022 16:31:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.392935 on epoch=33
06/01/2022 16:31:27 - INFO - __main__ - Global step 300 Train loss 0.452161 ACC 0.11320754716981132 on epoch=33
06/01/2022 16:31:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.381892 on epoch=34
06/01/2022 16:31:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.391937 on epoch=35
06/01/2022 16:31:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.403083 on epoch=36
06/01/2022 16:31:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.392761 on epoch=37
06/01/2022 16:31:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.353581 on epoch=38
06/01/2022 16:31:56 - INFO - __main__ - Global step 350 Train loss 0.384651 ACC 0.5283018867924528 on epoch=38
06/01/2022 16:32:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.376446 on epoch=39
06/01/2022 16:32:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.362966 on epoch=41
06/01/2022 16:32:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.344865 on epoch=42
06/01/2022 16:32:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.353854 on epoch=43
06/01/2022 16:32:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.312635 on epoch=44
06/01/2022 16:32:25 - INFO - __main__ - Global step 400 Train loss 0.350153 ACC 0.4811320754716981 on epoch=44
06/01/2022 16:32:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.344341 on epoch=45
06/01/2022 16:32:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.373408 on epoch=46
06/01/2022 16:32:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.363365 on epoch=47
06/01/2022 16:32:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.342136 on epoch=48
06/01/2022 16:32:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.329044 on epoch=49
06/01/2022 16:32:54 - INFO - __main__ - Global step 450 Train loss 0.350459 ACC 0.5471698113207547 on epoch=49
06/01/2022 16:33:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.312223 on epoch=51
06/01/2022 16:33:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.326117 on epoch=52
06/01/2022 16:33:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.324179 on epoch=53
06/01/2022 16:33:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.392879 on epoch=54
06/01/2022 16:33:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.352128 on epoch=55
06/01/2022 16:33:23 - INFO - __main__ - Global step 500 Train loss 0.341505 ACC 0.2830188679245283 on epoch=55
06/01/2022 16:33:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.330742 on epoch=56
06/01/2022 16:33:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.319909 on epoch=57
06/01/2022 16:33:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.330640 on epoch=58
06/01/2022 16:33:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.333809 on epoch=59
06/01/2022 16:33:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.356155 on epoch=61
06/01/2022 16:33:52 - INFO - __main__ - Global step 550 Train loss 0.334251 ACC 0.5188679245283019 on epoch=61
06/01/2022 16:33:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.321286 on epoch=62
06/01/2022 16:34:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.344776 on epoch=63
06/01/2022 16:34:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.313713 on epoch=64
06/01/2022 16:34:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.354439 on epoch=65
06/01/2022 16:34:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.324712 on epoch=66
06/01/2022 16:34:21 - INFO - __main__ - Global step 600 Train loss 0.331785 ACC 0.5 on epoch=66
06/01/2022 16:34:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.328124 on epoch=67
06/01/2022 16:34:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.322203 on epoch=68
06/01/2022 16:34:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.341838 on epoch=69
06/01/2022 16:34:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.306137 on epoch=71
06/01/2022 16:34:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.338654 on epoch=72
06/01/2022 16:34:49 - INFO - __main__ - Global step 650 Train loss 0.327391 ACC 0.4528301886792453 on epoch=72
06/01/2022 16:34:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.312757 on epoch=73
06/01/2022 16:35:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.328976 on epoch=74
06/01/2022 16:35:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.359240 on epoch=75
06/01/2022 16:35:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.315441 on epoch=76
06/01/2022 16:35:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.292996 on epoch=77
06/01/2022 16:35:17 - INFO - __main__ - Global step 700 Train loss 0.321882 ACC 0.5188679245283019 on epoch=77
06/01/2022 16:35:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.336614 on epoch=78
06/01/2022 16:35:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.376536 on epoch=79
06/01/2022 16:35:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.281595 on epoch=81
06/01/2022 16:35:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.300546 on epoch=82
06/01/2022 16:35:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.270042 on epoch=83
06/01/2022 16:35:46 - INFO - __main__ - Global step 750 Train loss 0.313067 ACC 0.5188679245283019 on epoch=83
06/01/2022 16:35:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.288658 on epoch=84
06/01/2022 16:35:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.327125 on epoch=85
06/01/2022 16:36:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.308453 on epoch=86
06/01/2022 16:36:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.329892 on epoch=87
06/01/2022 16:36:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.316777 on epoch=88
06/01/2022 16:36:15 - INFO - __main__ - Global step 800 Train loss 0.314181 ACC 0.5566037735849056 on epoch=88
06/01/2022 16:36:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.278405 on epoch=89
06/01/2022 16:36:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.293416 on epoch=91
06/01/2022 16:36:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.293670 on epoch=92
06/01/2022 16:36:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.309447 on epoch=93
06/01/2022 16:36:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.259771 on epoch=94
06/01/2022 16:36:44 - INFO - __main__ - Global step 850 Train loss 0.286942 ACC 0.5849056603773585 on epoch=94
06/01/2022 16:36:50 - INFO - __main__ - Step 860 Global step 860 Train loss 0.728960 on epoch=95
06/01/2022 16:36:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.626441 on epoch=96
06/01/2022 16:37:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.321647 on epoch=97
06/01/2022 16:37:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.324262 on epoch=98
06/01/2022 16:37:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.272313 on epoch=99
06/01/2022 16:37:13 - INFO - __main__ - Global step 900 Train loss 0.454725 ACC 0.5188679245283019 on epoch=99
06/01/2022 16:37:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.256846 on epoch=101
06/01/2022 16:37:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.256370 on epoch=102
06/01/2022 16:37:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.271622 on epoch=103
06/01/2022 16:37:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.258983 on epoch=104
06/01/2022 16:37:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.295333 on epoch=105
06/01/2022 16:37:42 - INFO - __main__ - Global step 950 Train loss 0.267831 ACC 0.5188679245283019 on epoch=105
06/01/2022 16:37:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.261412 on epoch=106
06/01/2022 16:37:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.259666 on epoch=107
06/01/2022 16:37:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.268933 on epoch=108
06/01/2022 16:38:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.238293 on epoch=109
06/01/2022 16:38:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.260188 on epoch=111
06/01/2022 16:38:10 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 16:38:10 - INFO - __main__ - Printing 3 examples
06/01/2022 16:38:10 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/01/2022 16:38:10 - INFO - __main__ - ['contradiction']
06/01/2022 16:38:10 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/01/2022 16:38:10 - INFO - __main__ - ['contradiction']
06/01/2022 16:38:10 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/01/2022 16:38:10 - INFO - __main__ - ['contradiction']
06/01/2022 16:38:10 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:38:10 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:38:10 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 16:38:10 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 16:38:10 - INFO - __main__ - Printing 3 examples
06/01/2022 16:38:10 - INFO - __main__ -  [superglue-cb] premise: A: Oh, yes. Animals have a way of talking. B: Alfie did. I tell you if I could have gotten a hold of that cat that day. A: I don't know uh, that I'd trade my dog in for the world. [SEP] hypothesis: he would trade his dog in for the world
06/01/2022 16:38:10 - INFO - __main__ - ['contradiction']
06/01/2022 16:38:10 - INFO - __main__ -  [superglue-cb] premise: B: but I found that, uh, it was made of some material which actually ended up rusting uh, after, A: Oh. B: even, despite, you know, diligent washing, it got rusty after about, uh, three weeks of use. And I don't think it was my fault because you know, I had made a point of like drying it off and cleaning it [SEP] hypothesis: it was his fault
06/01/2022 16:38:10 - INFO - __main__ - ['contradiction']
06/01/2022 16:38:10 - INFO - __main__ -  [superglue-cb] premise: A: I spend a lot of time reading about these things. I'm quite interested. I find it very exciting for the coverage we have now, today. B: Yes and I think we do get pretty good coverage. I don't feel that the American people is being shortchanged by uh, the news coverage. [SEP] hypothesis: the American people are being shortchanged by the news coverage
06/01/2022 16:38:10 - INFO - __main__ - ['contradiction']
06/01/2022 16:38:10 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:38:10 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:38:10 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 16:38:11 - INFO - __main__ - Global step 1000 Train loss 0.257698 ACC 0.5377358490566038 on epoch=111
06/01/2022 16:38:11 - INFO - __main__ - save last model!
06/01/2022 16:38:18 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 16:38:19 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 16:38:19 - INFO - __main__ - Printing 3 examples
06/01/2022 16:38:19 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 16:38:19 - INFO - __main__ - ['contradiction']
06/01/2022 16:38:19 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 16:38:19 - INFO - __main__ - ['neutral']
06/01/2022 16:38:19 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 16:38:19 - INFO - __main__ - ['entailment']
06/01/2022 16:38:19 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:38:19 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:38:19 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 16:38:20 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_87_0.0003_8_predictions.txt
06/01/2022 16:38:20 - INFO - __main__ - ACC on test data: 0.5000
06/01/2022 16:38:21 - INFO - __main__ - prefix=superglue-cb_64_87, lr=0.0003, bsz=8, dev_performance=0.5849056603773585, test_performance=0.5
06/01/2022 16:38:21 - INFO - __main__ - Running ... prefix=superglue-cb_64_87, lr=0.0002, bsz=8 ...
06/01/2022 16:38:22 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 16:38:22 - INFO - __main__ - Printing 3 examples
06/01/2022 16:38:22 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/01/2022 16:38:22 - INFO - __main__ - ['contradiction']
06/01/2022 16:38:22 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/01/2022 16:38:22 - INFO - __main__ - ['contradiction']
06/01/2022 16:38:22 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/01/2022 16:38:22 - INFO - __main__ - ['contradiction']
06/01/2022 16:38:22 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:38:22 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:38:22 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 16:38:22 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 16:38:22 - INFO - __main__ - Printing 3 examples
06/01/2022 16:38:22 - INFO - __main__ -  [superglue-cb] premise: A: Oh, yes. Animals have a way of talking. B: Alfie did. I tell you if I could have gotten a hold of that cat that day. A: I don't know uh, that I'd trade my dog in for the world. [SEP] hypothesis: he would trade his dog in for the world
06/01/2022 16:38:22 - INFO - __main__ - ['contradiction']
06/01/2022 16:38:22 - INFO - __main__ -  [superglue-cb] premise: B: but I found that, uh, it was made of some material which actually ended up rusting uh, after, A: Oh. B: even, despite, you know, diligent washing, it got rusty after about, uh, three weeks of use. And I don't think it was my fault because you know, I had made a point of like drying it off and cleaning it [SEP] hypothesis: it was his fault
06/01/2022 16:38:22 - INFO - __main__ - ['contradiction']
06/01/2022 16:38:22 - INFO - __main__ -  [superglue-cb] premise: A: I spend a lot of time reading about these things. I'm quite interested. I find it very exciting for the coverage we have now, today. B: Yes and I think we do get pretty good coverage. I don't feel that the American people is being shortchanged by uh, the news coverage. [SEP] hypothesis: the American people are being shortchanged by the news coverage
06/01/2022 16:38:22 - INFO - __main__ - ['contradiction']
06/01/2022 16:38:22 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:38:22 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:38:22 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 16:38:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 16:38:23 - INFO - __main__ - Starting training!
06/01/2022 16:38:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 16:38:35 - INFO - __main__ - Starting training!
06/01/2022 16:38:39 - INFO - __main__ - Step 10 Global step 10 Train loss 24.085682 on epoch=1
06/01/2022 16:38:44 - INFO - __main__ - Step 20 Global step 20 Train loss 15.940378 on epoch=2
06/01/2022 16:38:49 - INFO - __main__ - Step 30 Global step 30 Train loss 11.892225 on epoch=3
06/01/2022 16:38:54 - INFO - __main__ - Step 40 Global step 40 Train loss 11.064639 on epoch=4
06/01/2022 16:39:00 - INFO - __main__ - Step 50 Global step 50 Train loss 10.619817 on epoch=5
06/01/2022 16:39:02 - INFO - __main__ - Global step 50 Train loss 14.720548 ACC 0.0 on epoch=5
06/01/2022 16:39:08 - INFO - __main__ - Step 60 Global step 60 Train loss 10.122177 on epoch=6
06/01/2022 16:39:13 - INFO - __main__ - Step 70 Global step 70 Train loss 9.961683 on epoch=7
06/01/2022 16:39:18 - INFO - __main__ - Step 80 Global step 80 Train loss 8.409575 on epoch=8
06/01/2022 16:39:24 - INFO - __main__ - Step 90 Global step 90 Train loss 8.393325 on epoch=9
06/01/2022 16:39:29 - INFO - __main__ - Step 100 Global step 100 Train loss 7.688646 on epoch=11
06/01/2022 16:39:32 - INFO - __main__ - Global step 100 Train loss 8.915081 ACC 0.0 on epoch=11
06/01/2022 16:39:37 - INFO - __main__ - Step 110 Global step 110 Train loss 8.062186 on epoch=12
06/01/2022 16:39:42 - INFO - __main__ - Step 120 Global step 120 Train loss 6.820022 on epoch=13
06/01/2022 16:39:47 - INFO - __main__ - Step 130 Global step 130 Train loss 6.219371 on epoch=14
06/01/2022 16:39:52 - INFO - __main__ - Step 140 Global step 140 Train loss 6.088895 on epoch=15
06/01/2022 16:39:58 - INFO - __main__ - Step 150 Global step 150 Train loss 5.807525 on epoch=16
06/01/2022 16:40:00 - INFO - __main__ - Global step 150 Train loss 6.599600 ACC 0.0 on epoch=16
06/01/2022 16:40:06 - INFO - __main__ - Step 160 Global step 160 Train loss 4.879725 on epoch=17
06/01/2022 16:40:11 - INFO - __main__ - Step 170 Global step 170 Train loss 4.290607 on epoch=18
06/01/2022 16:40:16 - INFO - __main__ - Step 180 Global step 180 Train loss 3.057851 on epoch=19
06/01/2022 16:40:21 - INFO - __main__ - Step 190 Global step 190 Train loss 2.636326 on epoch=21
06/01/2022 16:40:26 - INFO - __main__ - Step 200 Global step 200 Train loss 2.564895 on epoch=22
06/01/2022 16:40:29 - INFO - __main__ - Global step 200 Train loss 3.485881 ACC 0.5188679245283019 on epoch=22
06/01/2022 16:40:34 - INFO - __main__ - Step 210 Global step 210 Train loss 2.431816 on epoch=23
06/01/2022 16:40:40 - INFO - __main__ - Step 220 Global step 220 Train loss 2.114103 on epoch=24
06/01/2022 16:40:45 - INFO - __main__ - Step 230 Global step 230 Train loss 2.545808 on epoch=25
06/01/2022 16:40:50 - INFO - __main__ - Step 240 Global step 240 Train loss 2.412219 on epoch=26
06/01/2022 16:40:55 - INFO - __main__ - Step 250 Global step 250 Train loss 2.046585 on epoch=27
06/01/2022 16:40:58 - INFO - __main__ - Global step 250 Train loss 2.310106 ACC 0.5188679245283019 on epoch=27
06/01/2022 16:41:03 - INFO - __main__ - Step 260 Global step 260 Train loss 1.643469 on epoch=28
06/01/2022 16:41:08 - INFO - __main__ - Step 270 Global step 270 Train loss 1.544569 on epoch=29
06/01/2022 16:41:14 - INFO - __main__ - Step 280 Global step 280 Train loss 1.994277 on epoch=31
06/01/2022 16:41:19 - INFO - __main__ - Step 290 Global step 290 Train loss 2.012296 on epoch=32
06/01/2022 16:41:24 - INFO - __main__ - Step 300 Global step 300 Train loss 1.627864 on epoch=33
06/01/2022 16:41:26 - INFO - __main__ - Global step 300 Train loss 1.764495 ACC 0.5188679245283019 on epoch=33
06/01/2022 16:41:31 - INFO - __main__ - Step 310 Global step 310 Train loss 1.478127 on epoch=34
06/01/2022 16:41:37 - INFO - __main__ - Step 320 Global step 320 Train loss 1.935733 on epoch=35
06/01/2022 16:41:42 - INFO - __main__ - Step 330 Global step 330 Train loss 1.259404 on epoch=36
06/01/2022 16:41:47 - INFO - __main__ - Step 340 Global step 340 Train loss 1.628006 on epoch=37
06/01/2022 16:41:52 - INFO - __main__ - Step 350 Global step 350 Train loss 1.182823 on epoch=38
06/01/2022 16:41:55 - INFO - __main__ - Global step 350 Train loss 1.496819 ACC 0.5188679245283019 on epoch=38
06/01/2022 16:42:00 - INFO - __main__ - Step 360 Global step 360 Train loss 1.042237 on epoch=39
06/01/2022 16:42:05 - INFO - __main__ - Step 370 Global step 370 Train loss 1.216301 on epoch=41
06/01/2022 16:42:10 - INFO - __main__ - Step 380 Global step 380 Train loss 1.294223 on epoch=42
06/01/2022 16:42:16 - INFO - __main__ - Step 390 Global step 390 Train loss 1.168282 on epoch=43
06/01/2022 16:42:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.809629 on epoch=44
06/01/2022 16:42:23 - INFO - __main__ - Global step 400 Train loss 1.106134 ACC 0.5377358490566038 on epoch=44
06/01/2022 16:42:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.983139 on epoch=45
06/01/2022 16:42:34 - INFO - __main__ - Step 420 Global step 420 Train loss 1.250710 on epoch=46
06/01/2022 16:42:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.813018 on epoch=47
06/01/2022 16:42:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.916559 on epoch=48
06/01/2022 16:42:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.752192 on epoch=49
06/01/2022 16:42:53 - INFO - __main__ - Global step 450 Train loss 0.943124 ACC 0.8018867924528302 on epoch=49
06/01/2022 16:42:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.931452 on epoch=51
06/01/2022 16:43:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.691130 on epoch=52
06/01/2022 16:43:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.476389 on epoch=53
06/01/2022 16:43:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.311474 on epoch=54
06/01/2022 16:43:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.280911 on epoch=55
06/01/2022 16:43:22 - INFO - __main__ - Global step 500 Train loss 0.538271 ACC 0.6981132075471698 on epoch=55
06/01/2022 16:43:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.128074 on epoch=56
06/01/2022 16:43:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.192767 on epoch=57
06/01/2022 16:43:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.121095 on epoch=58
06/01/2022 16:43:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.130779 on epoch=59
06/01/2022 16:43:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.136380 on epoch=61
06/01/2022 16:43:50 - INFO - __main__ - Global step 550 Train loss 0.141819 ACC 0.7830188679245284 on epoch=61
06/01/2022 16:43:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.101267 on epoch=62
06/01/2022 16:44:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.112292 on epoch=63
06/01/2022 16:44:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.088150 on epoch=64
06/01/2022 16:44:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.089286 on epoch=65
06/01/2022 16:44:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.050999 on epoch=66
06/01/2022 16:44:19 - INFO - __main__ - Global step 600 Train loss 0.088399 ACC 0.8962264150943396 on epoch=66
06/01/2022 16:44:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.052055 on epoch=67
06/01/2022 16:44:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.045423 on epoch=68
06/01/2022 16:44:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.025599 on epoch=69
06/01/2022 16:44:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.031344 on epoch=71
06/01/2022 16:44:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.031162 on epoch=72
06/01/2022 16:44:48 - INFO - __main__ - Global step 650 Train loss 0.037116 ACC 0.9245283018867925 on epoch=72
06/01/2022 16:44:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.031936 on epoch=73
06/01/2022 16:44:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.011964 on epoch=74
06/01/2022 16:45:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.011936 on epoch=75
06/01/2022 16:45:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.068141 on epoch=76
06/01/2022 16:45:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.206688 on epoch=77
06/01/2022 16:45:17 - INFO - __main__ - Global step 700 Train loss 0.066133 ACC 0.5377358490566038 on epoch=77
06/01/2022 16:45:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.354995 on epoch=78
06/01/2022 16:45:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.157116 on epoch=79
06/01/2022 16:45:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.006564 on epoch=81
06/01/2022 16:45:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.052453 on epoch=82
06/01/2022 16:45:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.030535 on epoch=83
06/01/2022 16:45:46 - INFO - __main__ - Global step 750 Train loss 0.120333 ACC 0.9528301886792453 on epoch=83
06/01/2022 16:45:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.032765 on epoch=84
06/01/2022 16:45:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.030971 on epoch=85
06/01/2022 16:46:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.016146 on epoch=86
06/01/2022 16:46:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.009281 on epoch=87
06/01/2022 16:46:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.009990 on epoch=88
06/01/2022 16:46:15 - INFO - __main__ - Global step 800 Train loss 0.019831 ACC 0.8773584905660378 on epoch=88
06/01/2022 16:46:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.065979 on epoch=89
06/01/2022 16:46:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.124440 on epoch=91
06/01/2022 16:46:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.343909 on epoch=92
06/01/2022 16:46:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.570380 on epoch=93
06/01/2022 16:46:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.407335 on epoch=94
06/01/2022 16:46:43 - INFO - __main__ - Global step 850 Train loss 0.302408 ACC 0.6037735849056604 on epoch=94
06/01/2022 16:46:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.498684 on epoch=95
06/01/2022 16:46:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.458565 on epoch=96
06/01/2022 16:46:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.330937 on epoch=97
06/01/2022 16:47:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.353465 on epoch=98
06/01/2022 16:47:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.422884 on epoch=99
06/01/2022 16:47:12 - INFO - __main__ - Global step 900 Train loss 0.412907 ACC 0.6792452830188679 on epoch=99
06/01/2022 16:47:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.308729 on epoch=101
06/01/2022 16:47:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.363556 on epoch=102
06/01/2022 16:47:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.385801 on epoch=103
06/01/2022 16:47:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.325809 on epoch=104
06/01/2022 16:47:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.324916 on epoch=105
06/01/2022 16:47:41 - INFO - __main__ - Global step 950 Train loss 0.341762 ACC 0.6320754716981132 on epoch=105
06/01/2022 16:47:46 - INFO - __main__ - Step 960 Global step 960 Train loss 0.334321 on epoch=106
06/01/2022 16:47:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.361589 on epoch=107
06/01/2022 16:47:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.339174 on epoch=108
06/01/2022 16:48:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.313269 on epoch=109
06/01/2022 16:48:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.377200 on epoch=111
06/01/2022 16:48:08 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 16:48:08 - INFO - __main__ - Printing 3 examples
06/01/2022 16:48:08 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/01/2022 16:48:08 - INFO - __main__ - ['contradiction']
06/01/2022 16:48:08 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/01/2022 16:48:08 - INFO - __main__ - ['contradiction']
06/01/2022 16:48:08 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/01/2022 16:48:08 - INFO - __main__ - ['contradiction']
06/01/2022 16:48:08 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:48:08 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:48:08 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 16:48:08 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 16:48:08 - INFO - __main__ - Printing 3 examples
06/01/2022 16:48:08 - INFO - __main__ -  [superglue-cb] premise: A: Oh, yes. Animals have a way of talking. B: Alfie did. I tell you if I could have gotten a hold of that cat that day. A: I don't know uh, that I'd trade my dog in for the world. [SEP] hypothesis: he would trade his dog in for the world
06/01/2022 16:48:08 - INFO - __main__ - ['contradiction']
06/01/2022 16:48:08 - INFO - __main__ -  [superglue-cb] premise: B: but I found that, uh, it was made of some material which actually ended up rusting uh, after, A: Oh. B: even, despite, you know, diligent washing, it got rusty after about, uh, three weeks of use. And I don't think it was my fault because you know, I had made a point of like drying it off and cleaning it [SEP] hypothesis: it was his fault
06/01/2022 16:48:08 - INFO - __main__ - ['contradiction']
06/01/2022 16:48:08 - INFO - __main__ -  [superglue-cb] premise: A: I spend a lot of time reading about these things. I'm quite interested. I find it very exciting for the coverage we have now, today. B: Yes and I think we do get pretty good coverage. I don't feel that the American people is being shortchanged by uh, the news coverage. [SEP] hypothesis: the American people are being shortchanged by the news coverage
06/01/2022 16:48:08 - INFO - __main__ - ['contradiction']
06/01/2022 16:48:08 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:48:08 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:48:08 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 16:48:09 - INFO - __main__ - Global step 1000 Train loss 0.345110 ACC 0.660377358490566 on epoch=111
06/01/2022 16:48:09 - INFO - __main__ - save last model!
06/01/2022 16:48:16 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 16:48:17 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 16:48:17 - INFO - __main__ - Printing 3 examples
06/01/2022 16:48:17 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 16:48:17 - INFO - __main__ - ['contradiction']
06/01/2022 16:48:17 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 16:48:17 - INFO - __main__ - ['neutral']
06/01/2022 16:48:17 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 16:48:17 - INFO - __main__ - ['entailment']
06/01/2022 16:48:17 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:48:17 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:48:17 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 16:48:18 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_87_0.0002_8_predictions.txt
06/01/2022 16:48:18 - INFO - __main__ - ACC on test data: 0.9107
06/01/2022 16:48:19 - INFO - __main__ - prefix=superglue-cb_64_87, lr=0.0002, bsz=8, dev_performance=0.9528301886792453, test_performance=0.9107142857142857
06/01/2022 16:48:19 - INFO - __main__ - Running ... prefix=superglue-cb_64_87, lr=0.0001, bsz=8 ...
06/01/2022 16:48:19 - INFO - __main__ - Start tokenizing ... 144 instances
06/01/2022 16:48:19 - INFO - __main__ - Printing 3 examples
06/01/2022 16:48:19 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/01/2022 16:48:19 - INFO - __main__ - ['contradiction']
06/01/2022 16:48:19 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/01/2022 16:48:19 - INFO - __main__ - ['contradiction']
06/01/2022 16:48:19 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/01/2022 16:48:19 - INFO - __main__ - ['contradiction']
06/01/2022 16:48:19 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:48:20 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:48:20 - INFO - __main__ - Loaded 144 examples from train data
06/01/2022 16:48:20 - INFO - __main__ - Start tokenizing ... 106 instances
06/01/2022 16:48:20 - INFO - __main__ - Printing 3 examples
06/01/2022 16:48:20 - INFO - __main__ -  [superglue-cb] premise: A: Oh, yes. Animals have a way of talking. B: Alfie did. I tell you if I could have gotten a hold of that cat that day. A: I don't know uh, that I'd trade my dog in for the world. [SEP] hypothesis: he would trade his dog in for the world
06/01/2022 16:48:20 - INFO - __main__ - ['contradiction']
06/01/2022 16:48:20 - INFO - __main__ -  [superglue-cb] premise: B: but I found that, uh, it was made of some material which actually ended up rusting uh, after, A: Oh. B: even, despite, you know, diligent washing, it got rusty after about, uh, three weeks of use. And I don't think it was my fault because you know, I had made a point of like drying it off and cleaning it [SEP] hypothesis: it was his fault
06/01/2022 16:48:20 - INFO - __main__ - ['contradiction']
06/01/2022 16:48:20 - INFO - __main__ -  [superglue-cb] premise: A: I spend a lot of time reading about these things. I'm quite interested. I find it very exciting for the coverage we have now, today. B: Yes and I think we do get pretty good coverage. I don't feel that the American people is being shortchanged by uh, the news coverage. [SEP] hypothesis: the American people are being shortchanged by the news coverage
06/01/2022 16:48:20 - INFO - __main__ - ['contradiction']
06/01/2022 16:48:20 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:48:20 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:48:20 - INFO - __main__ - Loaded 106 examples from dev data
06/01/2022 16:48:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 16:48:22 - INFO - __main__ - Starting training!
06/01/2022 16:48:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/01/2022 16:48:32 - INFO - __main__ - Starting training!
06/01/2022 16:48:37 - INFO - __main__ - Step 10 Global step 10 Train loss 24.491127 on epoch=1
06/01/2022 16:48:42 - INFO - __main__ - Step 20 Global step 20 Train loss 22.932814 on epoch=2
06/01/2022 16:48:47 - INFO - __main__ - Step 30 Global step 30 Train loss 18.330435 on epoch=3
06/01/2022 16:48:52 - INFO - __main__ - Step 40 Global step 40 Train loss 14.039503 on epoch=4
06/01/2022 16:48:57 - INFO - __main__ - Step 50 Global step 50 Train loss 13.225612 on epoch=5
06/01/2022 16:49:33 - INFO - __main__ - Global step 50 Train loss 18.603899 ACC 0.0 on epoch=5
06/01/2022 16:49:39 - INFO - __main__ - Step 60 Global step 60 Train loss 13.302351 on epoch=6
06/01/2022 16:49:44 - INFO - __main__ - Step 70 Global step 70 Train loss 12.154014 on epoch=7
06/01/2022 16:49:49 - INFO - __main__ - Step 80 Global step 80 Train loss 11.930552 on epoch=8
06/01/2022 16:49:55 - INFO - __main__ - Step 90 Global step 90 Train loss 11.144593 on epoch=9
06/01/2022 16:50:00 - INFO - __main__ - Step 100 Global step 100 Train loss 10.792566 on epoch=11
06/01/2022 16:50:07 - INFO - __main__ - Global step 100 Train loss 11.864816 ACC 0.009433962264150943 on epoch=11
06/01/2022 16:50:13 - INFO - __main__ - Step 110 Global step 110 Train loss 10.882274 on epoch=12
06/01/2022 16:50:18 - INFO - __main__ - Step 120 Global step 120 Train loss 10.047895 on epoch=13
06/01/2022 16:50:23 - INFO - __main__ - Step 130 Global step 130 Train loss 10.322675 on epoch=14
06/01/2022 16:50:29 - INFO - __main__ - Step 140 Global step 140 Train loss 9.843970 on epoch=15
06/01/2022 16:50:34 - INFO - __main__ - Step 150 Global step 150 Train loss 9.576445 on epoch=16
06/01/2022 16:50:37 - INFO - __main__ - Global step 150 Train loss 10.134653 ACC 0.009433962264150943 on epoch=16
06/01/2022 16:50:42 - INFO - __main__ - Step 160 Global step 160 Train loss 8.966557 on epoch=17
06/01/2022 16:50:48 - INFO - __main__ - Step 170 Global step 170 Train loss 8.931772 on epoch=18
06/01/2022 16:50:53 - INFO - __main__ - Step 180 Global step 180 Train loss 8.152128 on epoch=19
06/01/2022 16:50:58 - INFO - __main__ - Step 190 Global step 190 Train loss 8.579656 on epoch=21
06/01/2022 16:51:03 - INFO - __main__ - Step 200 Global step 200 Train loss 8.143074 on epoch=22
06/01/2022 16:51:06 - INFO - __main__ - Global step 200 Train loss 8.554638 ACC 0.0 on epoch=22
06/01/2022 16:51:11 - INFO - __main__ - Step 210 Global step 210 Train loss 7.964198 on epoch=23
06/01/2022 16:51:17 - INFO - __main__ - Step 220 Global step 220 Train loss 7.492162 on epoch=24
06/01/2022 16:51:22 - INFO - __main__ - Step 230 Global step 230 Train loss 7.864696 on epoch=25
06/01/2022 16:51:27 - INFO - __main__ - Step 240 Global step 240 Train loss 7.907701 on epoch=26
06/01/2022 16:51:32 - INFO - __main__ - Step 250 Global step 250 Train loss 7.762965 on epoch=27
06/01/2022 16:51:35 - INFO - __main__ - Global step 250 Train loss 7.798344 ACC 0.0 on epoch=27
06/01/2022 16:51:40 - INFO - __main__ - Step 260 Global step 260 Train loss 7.175750 on epoch=28
06/01/2022 16:51:45 - INFO - __main__ - Step 270 Global step 270 Train loss 7.359769 on epoch=29
06/01/2022 16:51:51 - INFO - __main__ - Step 280 Global step 280 Train loss 6.111416 on epoch=31
06/01/2022 16:51:56 - INFO - __main__ - Step 290 Global step 290 Train loss 6.673764 on epoch=32
06/01/2022 16:52:01 - INFO - __main__ - Step 300 Global step 300 Train loss 5.833300 on epoch=33
06/01/2022 16:52:04 - INFO - __main__ - Global step 300 Train loss 6.630800 ACC 0.0 on epoch=33
06/01/2022 16:52:09 - INFO - __main__ - Step 310 Global step 310 Train loss 5.687178 on epoch=34
06/01/2022 16:52:14 - INFO - __main__ - Step 320 Global step 320 Train loss 5.743254 on epoch=35
06/01/2022 16:52:19 - INFO - __main__ - Step 330 Global step 330 Train loss 5.313008 on epoch=36
06/01/2022 16:52:25 - INFO - __main__ - Step 340 Global step 340 Train loss 4.613972 on epoch=37
06/01/2022 16:52:30 - INFO - __main__ - Step 350 Global step 350 Train loss 4.546390 on epoch=38
06/01/2022 16:52:33 - INFO - __main__ - Global step 350 Train loss 5.180760 ACC 0.0 on epoch=38
06/01/2022 16:52:38 - INFO - __main__ - Step 360 Global step 360 Train loss 4.282154 on epoch=39
06/01/2022 16:52:43 - INFO - __main__ - Step 370 Global step 370 Train loss 3.607633 on epoch=41
06/01/2022 16:52:48 - INFO - __main__ - Step 380 Global step 380 Train loss 2.942854 on epoch=42
06/01/2022 16:52:54 - INFO - __main__ - Step 390 Global step 390 Train loss 3.077740 on epoch=43
06/01/2022 16:52:59 - INFO - __main__ - Step 400 Global step 400 Train loss 2.873763 on epoch=44
06/01/2022 16:53:01 - INFO - __main__ - Global step 400 Train loss 3.356829 ACC 0.5377358490566038 on epoch=44
06/01/2022 16:53:07 - INFO - __main__ - Step 410 Global step 410 Train loss 3.157148 on epoch=45
06/01/2022 16:53:12 - INFO - __main__ - Step 420 Global step 420 Train loss 2.680948 on epoch=46
06/01/2022 16:53:18 - INFO - __main__ - Step 430 Global step 430 Train loss 1.827125 on epoch=47
06/01/2022 16:53:23 - INFO - __main__ - Step 440 Global step 440 Train loss 2.449844 on epoch=48
06/01/2022 16:53:29 - INFO - __main__ - Step 450 Global step 450 Train loss 2.157399 on epoch=49
06/01/2022 16:53:30 - INFO - __main__ - Global step 450 Train loss 2.454493 ACC 0.5188679245283019 on epoch=49
06/01/2022 16:53:36 - INFO - __main__ - Step 460 Global step 460 Train loss 2.406565 on epoch=51
06/01/2022 16:53:41 - INFO - __main__ - Step 470 Global step 470 Train loss 2.159463 on epoch=52
06/01/2022 16:53:46 - INFO - __main__ - Step 480 Global step 480 Train loss 2.579845 on epoch=53
06/01/2022 16:53:52 - INFO - __main__ - Step 490 Global step 490 Train loss 2.262371 on epoch=54
06/01/2022 16:53:57 - INFO - __main__ - Step 500 Global step 500 Train loss 2.717246 on epoch=55
06/01/2022 16:53:59 - INFO - __main__ - Global step 500 Train loss 2.425098 ACC 0.5188679245283019 on epoch=55
06/01/2022 16:54:04 - INFO - __main__ - Step 510 Global step 510 Train loss 1.838882 on epoch=56
06/01/2022 16:54:09 - INFO - __main__ - Step 520 Global step 520 Train loss 2.148664 on epoch=57
06/01/2022 16:54:14 - INFO - __main__ - Step 530 Global step 530 Train loss 2.535747 on epoch=58
06/01/2022 16:54:20 - INFO - __main__ - Step 540 Global step 540 Train loss 2.420055 on epoch=59
06/01/2022 16:54:25 - INFO - __main__ - Step 550 Global step 550 Train loss 2.121536 on epoch=61
06/01/2022 16:54:27 - INFO - __main__ - Global step 550 Train loss 2.212977 ACC 0.5188679245283019 on epoch=61
06/01/2022 16:54:32 - INFO - __main__ - Step 560 Global step 560 Train loss 1.839152 on epoch=62
06/01/2022 16:54:38 - INFO - __main__ - Step 570 Global step 570 Train loss 1.987174 on epoch=63
06/01/2022 16:54:43 - INFO - __main__ - Step 580 Global step 580 Train loss 1.950737 on epoch=64
06/01/2022 16:54:48 - INFO - __main__ - Step 590 Global step 590 Train loss 1.962545 on epoch=65
06/01/2022 16:54:54 - INFO - __main__ - Step 600 Global step 600 Train loss 1.831679 on epoch=66
06/01/2022 16:54:56 - INFO - __main__ - Global step 600 Train loss 1.914257 ACC 0.5188679245283019 on epoch=66
06/01/2022 16:55:01 - INFO - __main__ - Step 610 Global step 610 Train loss 1.968188 on epoch=67
06/01/2022 16:55:06 - INFO - __main__ - Step 620 Global step 620 Train loss 1.543310 on epoch=68
06/01/2022 16:55:12 - INFO - __main__ - Step 630 Global step 630 Train loss 1.900055 on epoch=69
06/01/2022 16:55:17 - INFO - __main__ - Step 640 Global step 640 Train loss 1.819045 on epoch=71
06/01/2022 16:55:22 - INFO - __main__ - Step 650 Global step 650 Train loss 2.072583 on epoch=72
06/01/2022 16:55:24 - INFO - __main__ - Global step 650 Train loss 1.860636 ACC 0.5188679245283019 on epoch=72
06/01/2022 16:55:29 - INFO - __main__ - Step 660 Global step 660 Train loss 1.584234 on epoch=73
06/01/2022 16:55:35 - INFO - __main__ - Step 670 Global step 670 Train loss 1.642445 on epoch=74
06/01/2022 16:55:40 - INFO - __main__ - Step 680 Global step 680 Train loss 1.798015 on epoch=75
06/01/2022 16:55:45 - INFO - __main__ - Step 690 Global step 690 Train loss 1.743417 on epoch=76
06/01/2022 16:55:51 - INFO - __main__ - Step 700 Global step 700 Train loss 1.710875 on epoch=77
06/01/2022 16:55:53 - INFO - __main__ - Global step 700 Train loss 1.695797 ACC 0.5188679245283019 on epoch=77
06/01/2022 16:55:58 - INFO - __main__ - Step 710 Global step 710 Train loss 1.569644 on epoch=78
06/01/2022 16:56:03 - INFO - __main__ - Step 720 Global step 720 Train loss 1.374200 on epoch=79
06/01/2022 16:56:09 - INFO - __main__ - Step 730 Global step 730 Train loss 1.589199 on epoch=81
06/01/2022 16:56:14 - INFO - __main__ - Step 740 Global step 740 Train loss 1.404055 on epoch=82
06/01/2022 16:56:19 - INFO - __main__ - Step 750 Global step 750 Train loss 1.611532 on epoch=83
06/01/2022 16:56:22 - INFO - __main__ - Global step 750 Train loss 1.509726 ACC 0.5188679245283019 on epoch=83
06/01/2022 16:56:27 - INFO - __main__ - Step 760 Global step 760 Train loss 1.182929 on epoch=84
06/01/2022 16:56:32 - INFO - __main__ - Step 770 Global step 770 Train loss 1.138497 on epoch=85
06/01/2022 16:56:37 - INFO - __main__ - Step 780 Global step 780 Train loss 1.279107 on epoch=86
06/01/2022 16:56:43 - INFO - __main__ - Step 790 Global step 790 Train loss 1.429458 on epoch=87
06/01/2022 16:56:48 - INFO - __main__ - Step 800 Global step 800 Train loss 1.479750 on epoch=88
06/01/2022 16:56:50 - INFO - __main__ - Global step 800 Train loss 1.301948 ACC 0.5188679245283019 on epoch=88
06/01/2022 16:56:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.903466 on epoch=89
06/01/2022 16:57:01 - INFO - __main__ - Step 820 Global step 820 Train loss 1.478670 on epoch=91
06/01/2022 16:57:06 - INFO - __main__ - Step 830 Global step 830 Train loss 1.218489 on epoch=92
06/01/2022 16:57:11 - INFO - __main__ - Step 840 Global step 840 Train loss 1.254296 on epoch=93
06/01/2022 16:57:16 - INFO - __main__ - Step 850 Global step 850 Train loss 1.222481 on epoch=94
06/01/2022 16:57:19 - INFO - __main__ - Global step 850 Train loss 1.215480 ACC 0.8301886792452831 on epoch=94
06/01/2022 16:57:25 - INFO - __main__ - Step 860 Global step 860 Train loss 1.259675 on epoch=95
06/01/2022 16:57:30 - INFO - __main__ - Step 870 Global step 870 Train loss 1.189397 on epoch=96
06/01/2022 16:57:35 - INFO - __main__ - Step 880 Global step 880 Train loss 1.152287 on epoch=97
06/01/2022 16:57:40 - INFO - __main__ - Step 890 Global step 890 Train loss 1.102982 on epoch=98
06/01/2022 16:57:46 - INFO - __main__ - Step 900 Global step 900 Train loss 1.330455 on epoch=99
06/01/2022 16:57:48 - INFO - __main__ - Global step 900 Train loss 1.206959 ACC 0.7735849056603774 on epoch=99
06/01/2022 16:57:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.845907 on epoch=101
06/01/2022 16:57:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.832220 on epoch=102
06/01/2022 16:58:04 - INFO - __main__ - Step 930 Global step 930 Train loss 1.075271 on epoch=103
06/01/2022 16:58:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.982586 on epoch=104
06/01/2022 16:58:15 - INFO - __main__ - Step 950 Global step 950 Train loss 1.163231 on epoch=105
06/01/2022 16:58:17 - INFO - __main__ - Global step 950 Train loss 0.979843 ACC 0.8490566037735849 on epoch=105
06/01/2022 16:58:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.906238 on epoch=106
06/01/2022 16:58:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.858820 on epoch=107
06/01/2022 16:58:34 - INFO - __main__ - Step 980 Global step 980 Train loss 1.064545 on epoch=108
06/01/2022 16:58:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.622057 on epoch=109
06/01/2022 16:58:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.796417 on epoch=111
06/01/2022 16:58:47 - INFO - __main__ - Global step 1000 Train loss 0.849615 ACC 0.7641509433962265 on epoch=111
06/01/2022 16:58:47 - INFO - __main__ - save last model!
06/01/2022 16:58:54 - INFO - __main__ - Loading checkpoint on the fly
06/01/2022 16:58:54 - INFO - __main__ - Start tokenizing ... 56 instances
06/01/2022 16:58:54 - INFO - __main__ - Printing 3 examples
06/01/2022 16:58:54 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/01/2022 16:58:54 - INFO - __main__ - ['contradiction']
06/01/2022 16:58:54 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/01/2022 16:58:54 - INFO - __main__ - ['neutral']
06/01/2022 16:58:54 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/01/2022 16:58:54 - INFO - __main__ - ['entailment']
06/01/2022 16:58:54 - INFO - __main__ - Tokenizing Input ...
06/01/2022 16:58:54 - INFO - __main__ - Tokenizing Output ...
06/01/2022 16:58:54 - INFO - __main__ - Loaded 56 examples from test data
06/01/2022 16:58:56 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down64shot/singletask-superglue-cb/superglue-cb_64_87_0.0001_8_predictions.txt
06/01/2022 16:58:56 - INFO - __main__ - ACC on test data: 0.6964
06/01/2022 16:58:56 - INFO - __main__ - prefix=superglue-cb_64_87, lr=0.0001, bsz=8, dev_performance=0.8490566037735849, test_performance=0.6964285714285714
