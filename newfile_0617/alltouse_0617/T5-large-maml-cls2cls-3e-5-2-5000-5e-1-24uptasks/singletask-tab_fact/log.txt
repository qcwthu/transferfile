05/31/2022 11:45:13 - INFO - __main__ - Namespace(task_dir='data/tab_fact/', task_name='tab_fact', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/31/2022 11:45:13 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/singletask-tab_fact
05/31/2022 11:45:13 - INFO - __main__ - Namespace(task_dir='data/tab_fact/', task_name='tab_fact', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/31/2022 11:45:13 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/singletask-tab_fact
05/31/2022 11:45:14 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/31/2022 11:45:14 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/31/2022 11:45:14 - INFO - __main__ - args.device: cuda:0
05/31/2022 11:45:14 - INFO - __main__ - Using 2 gpus
05/31/2022 11:45:14 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_16_100', 'tab_fact_16_13', 'tab_fact_16_21', 'tab_fact_16_42', 'tab_fact_16_87']
05/31/2022 11:45:14 - INFO - __main__ - args.device: cuda:1
05/31/2022 11:45:14 - INFO - __main__ - Using 2 gpus
05/31/2022 11:45:14 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_16_100', 'tab_fact_16_13', 'tab_fact_16_21', 'tab_fact_16_42', 'tab_fact_16_87']
05/31/2022 11:45:19 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.5, bsz=8 ...
05/31/2022 11:45:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 11:45:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 11:45:20 - INFO - __main__ - Printing 3 examples
05/31/2022 11:45:20 - INFO - __main__ - Printing 3 examples
05/31/2022 11:45:20 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/31/2022 11:45:20 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/31/2022 11:45:20 - INFO - __main__ - ['refuted']
05/31/2022 11:45:20 - INFO - __main__ - ['refuted']
05/31/2022 11:45:20 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/31/2022 11:45:20 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/31/2022 11:45:20 - INFO - __main__ - ['refuted']
05/31/2022 11:45:20 - INFO - __main__ - ['refuted']
05/31/2022 11:45:20 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/31/2022 11:45:20 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/31/2022 11:45:20 - INFO - __main__ - ['refuted']
05/31/2022 11:45:20 - INFO - __main__ - ['refuted']
05/31/2022 11:45:20 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:45:20 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:45:20 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:45:20 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:45:20 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 11:45:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 11:45:20 - INFO - __main__ - Printing 3 examples
05/31/2022 11:45:20 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/31/2022 11:45:20 - INFO - __main__ - ['refuted']
05/31/2022 11:45:20 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/31/2022 11:45:20 - INFO - __main__ - ['refuted']
05/31/2022 11:45:20 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/31/2022 11:45:20 - INFO - __main__ - ['refuted']
05/31/2022 11:45:20 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:45:20 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 11:45:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 11:45:20 - INFO - __main__ - Printing 3 examples
05/31/2022 11:45:20 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/31/2022 11:45:20 - INFO - __main__ - ['refuted']
05/31/2022 11:45:20 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/31/2022 11:45:20 - INFO - __main__ - ['refuted']
05/31/2022 11:45:20 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/31/2022 11:45:20 - INFO - __main__ - ['refuted']
05/31/2022 11:45:20 - INFO - __main__ - Tokenizing Input ...
05/31/2022 11:45:20 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:45:20 - INFO - __main__ - Tokenizing Output ...
05/31/2022 11:45:20 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 11:45:20 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 11:45:38 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 11:45:38 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 11:45:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 11:45:39 - INFO - __main__ - Starting training!
05/31/2022 11:45:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 11:45:44 - INFO - __main__ - Starting training!
05/31/2022 11:45:49 - INFO - __main__ - Step 10 Global step 10 Train loss 2.83 on epoch=4
05/31/2022 11:45:53 - INFO - __main__ - Step 20 Global step 20 Train loss 0.61 on epoch=9
05/31/2022 11:45:58 - INFO - __main__ - Step 30 Global step 30 Train loss 0.37 on epoch=14
05/31/2022 11:46:02 - INFO - __main__ - Step 40 Global step 40 Train loss 0.28 on epoch=19
05/31/2022 11:46:06 - INFO - __main__ - Step 50 Global step 50 Train loss 0.30 on epoch=24
05/31/2022 11:46:08 - INFO - __main__ - Global step 50 Train loss 0.88 Classification-F1 0.3333333333333333 on epoch=24
05/31/2022 11:46:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
05/31/2022 11:46:12 - INFO - __main__ - Step 60 Global step 60 Train loss 0.28 on epoch=29
05/31/2022 11:46:17 - INFO - __main__ - Step 70 Global step 70 Train loss 0.25 on epoch=34
05/31/2022 11:46:21 - INFO - __main__ - Step 80 Global step 80 Train loss 0.22 on epoch=39
05/31/2022 11:46:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.26 on epoch=44
05/31/2022 11:46:30 - INFO - __main__ - Step 100 Global step 100 Train loss 0.22 on epoch=49
05/31/2022 11:46:31 - INFO - __main__ - Global step 100 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=49
05/31/2022 11:46:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.26 on epoch=54
05/31/2022 11:46:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.24 on epoch=59
05/31/2022 11:46:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.24 on epoch=64
05/31/2022 11:46:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.23 on epoch=69
05/31/2022 11:46:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.19 on epoch=74
05/31/2022 11:46:54 - INFO - __main__ - Global step 150 Train loss 0.23 Classification-F1 0.4181818181818182 on epoch=74
05/31/2022 11:46:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4181818181818182 on epoch=74, global_step=150
05/31/2022 11:46:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.21 on epoch=79
05/31/2022 11:47:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.20 on epoch=84
05/31/2022 11:47:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.23 on epoch=89
05/31/2022 11:47:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.22 on epoch=94
05/31/2022 11:47:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.21 on epoch=99
05/31/2022 11:47:17 - INFO - __main__ - Global step 200 Train loss 0.21 Classification-F1 0.4181818181818182 on epoch=99
05/31/2022 11:47:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.19 on epoch=104
05/31/2022 11:47:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.19 on epoch=109
05/31/2022 11:47:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.19 on epoch=114
05/31/2022 11:47:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=119
05/31/2022 11:47:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.21 on epoch=124
05/31/2022 11:47:40 - INFO - __main__ - Global step 250 Train loss 0.20 Classification-F1 0.2571428571428571 on epoch=124
05/31/2022 11:47:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.19 on epoch=129
05/31/2022 11:47:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.18 on epoch=134
05/31/2022 11:47:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.17 on epoch=139
05/31/2022 11:47:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.14 on epoch=144
05/31/2022 11:48:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.15 on epoch=149
05/31/2022 11:48:03 - INFO - __main__ - Global step 300 Train loss 0.17 Classification-F1 0.464039408866995 on epoch=149
05/31/2022 11:48:04 - INFO - __main__ - Saving model with best Classification-F1: 0.4181818181818182 -> 0.464039408866995 on epoch=149, global_step=300
05/31/2022 11:48:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.11 on epoch=154
05/31/2022 11:48:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.16 on epoch=159
05/31/2022 11:48:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.11 on epoch=164
05/31/2022 11:48:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.14 on epoch=169
05/31/2022 11:48:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.13 on epoch=174
05/31/2022 11:48:27 - INFO - __main__ - Global step 350 Train loss 0.13 Classification-F1 0.2821052631578947 on epoch=174
05/31/2022 11:48:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.12 on epoch=179
05/31/2022 11:48:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.13 on epoch=184
05/31/2022 11:48:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.06 on epoch=189
05/31/2022 11:48:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.11 on epoch=194
05/31/2022 11:48:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.07 on epoch=199
05/31/2022 11:48:50 - INFO - __main__ - Global step 400 Train loss 0.10 Classification-F1 0.2654320987654321 on epoch=199
05/31/2022 11:48:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.08 on epoch=204
05/31/2022 11:48:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.04 on epoch=209
05/31/2022 11:49:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.06 on epoch=214
05/31/2022 11:49:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.09 on epoch=219
05/31/2022 11:49:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.07 on epoch=224
05/31/2022 11:49:13 - INFO - __main__ - Global step 450 Train loss 0.07 Classification-F1 0.22116402116402112 on epoch=224
05/31/2022 11:49:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.05 on epoch=229
05/31/2022 11:49:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.04 on epoch=234
05/31/2022 11:49:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.05 on epoch=239
05/31/2022 11:49:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.06 on epoch=244
05/31/2022 11:49:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.03 on epoch=249
05/31/2022 11:49:36 - INFO - __main__ - Global step 500 Train loss 0.05 Classification-F1 0.4420512820512821 on epoch=249
05/31/2022 11:49:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.03 on epoch=254
05/31/2022 11:49:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.05 on epoch=259
05/31/2022 11:49:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.04 on epoch=264
05/31/2022 11:49:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.02 on epoch=269
05/31/2022 11:49:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=274
05/31/2022 11:49:59 - INFO - __main__ - Global step 550 Train loss 0.03 Classification-F1 0.29971988795518206 on epoch=274
05/31/2022 11:50:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.03 on epoch=279
05/31/2022 11:50:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
05/31/2022 11:50:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
05/31/2022 11:50:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=294
05/31/2022 11:50:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=299
05/31/2022 11:50:22 - INFO - __main__ - Global step 600 Train loss 0.02 Classification-F1 0.2713675213675214 on epoch=299
05/31/2022 11:50:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=304
05/31/2022 11:50:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
05/31/2022 11:50:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
05/31/2022 11:50:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
05/31/2022 11:50:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
05/31/2022 11:50:45 - INFO - __main__ - Global step 650 Train loss 0.02 Classification-F1 0.39999999999999997 on epoch=324
05/31/2022 11:50:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=329
05/31/2022 11:50:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
05/31/2022 11:50:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
05/31/2022 11:51:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
05/31/2022 11:51:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
05/31/2022 11:51:08 - INFO - __main__ - Global step 700 Train loss 0.02 Classification-F1 0.2821052631578947 on epoch=349
05/31/2022 11:51:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
05/31/2022 11:51:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
05/31/2022 11:51:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
05/31/2022 11:51:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
05/31/2022 11:51:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
05/31/2022 11:51:31 - INFO - __main__ - Global step 750 Train loss 0.02 Classification-F1 0.2821052631578947 on epoch=374
05/31/2022 11:51:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
05/31/2022 11:51:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
05/31/2022 11:51:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
05/31/2022 11:51:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
05/31/2022 11:51:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
05/31/2022 11:51:54 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.4420512820512821 on epoch=399
05/31/2022 11:51:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
05/31/2022 11:52:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
05/31/2022 11:52:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
05/31/2022 11:52:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
05/31/2022 11:52:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
05/31/2022 11:52:17 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.2991452991452992 on epoch=424
05/31/2022 11:52:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
05/31/2022 11:52:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
05/31/2022 11:52:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
05/31/2022 11:52:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
05/31/2022 11:52:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
05/31/2022 11:52:40 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.2554385964912281 on epoch=449
05/31/2022 11:52:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
05/31/2022 11:52:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
05/31/2022 11:52:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
05/31/2022 11:52:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
05/31/2022 11:53:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
05/31/2022 11:53:03 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.25427350427350426 on epoch=474
05/31/2022 11:53:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
05/31/2022 11:53:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
05/31/2022 11:53:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
05/31/2022 11:53:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
05/31/2022 11:53:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
05/31/2022 11:53:26 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.39999999999999997 on epoch=499
05/31/2022 11:53:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
05/31/2022 11:53:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
05/31/2022 11:53:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
05/31/2022 11:53:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
05/31/2022 11:53:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
05/31/2022 11:53:50 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.41700404858299595 on epoch=524
05/31/2022 11:53:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
05/31/2022 11:53:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
05/31/2022 11:54:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
05/31/2022 11:54:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
05/31/2022 11:54:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
05/31/2022 11:54:13 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.2647262647262647 on epoch=549
05/31/2022 11:54:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
05/31/2022 11:54:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
05/31/2022 11:54:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
05/31/2022 11:54:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
05/31/2022 11:54:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
05/31/2022 11:54:36 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=574
05/31/2022 11:54:36 - INFO - __main__ - Saving model with best Classification-F1: 0.464039408866995 -> 0.4817813765182186 on epoch=574, global_step=1150
05/31/2022 11:54:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
05/31/2022 11:54:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
05/31/2022 11:54:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
05/31/2022 11:54:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
05/31/2022 11:54:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
05/31/2022 11:54:59 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.39139139139139134 on epoch=599
05/31/2022 11:55:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
05/31/2022 11:55:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
05/31/2022 11:55:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
05/31/2022 11:55:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
05/31/2022 11:55:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
05/31/2022 11:55:22 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.2991452991452992 on epoch=624
05/31/2022 11:55:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
05/31/2022 11:55:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
05/31/2022 11:55:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
05/31/2022 11:55:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
05/31/2022 11:55:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
05/31/2022 11:55:45 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.20162162162162162 on epoch=649
05/31/2022 11:55:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
05/31/2022 11:55:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
05/31/2022 11:55:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
05/31/2022 11:56:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
05/31/2022 11:56:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
05/31/2022 11:56:08 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=674
05/31/2022 11:56:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
05/31/2022 11:56:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
05/31/2022 11:56:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
05/31/2022 11:56:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
05/31/2022 11:56:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
05/31/2022 11:56:32 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.20192307692307693 on epoch=699
05/31/2022 11:56:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
05/31/2022 11:56:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
05/31/2022 11:56:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
05/31/2022 11:56:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=719
05/31/2022 11:56:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
05/31/2022 11:56:55 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.2713675213675214 on epoch=724
05/31/2022 11:56:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
05/31/2022 11:57:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
05/31/2022 11:57:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
05/31/2022 11:57:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
05/31/2022 11:57:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
05/31/2022 11:57:18 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.3144016227180528 on epoch=749
05/31/2022 11:57:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
05/31/2022 11:57:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
05/31/2022 11:57:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
05/31/2022 11:57:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
05/31/2022 11:57:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
05/31/2022 11:57:41 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=774
05/31/2022 11:57:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
05/31/2022 11:57:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
05/31/2022 11:57:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
05/31/2022 11:57:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
05/31/2022 11:58:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
05/31/2022 11:58:04 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=799
05/31/2022 11:58:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
05/31/2022 11:58:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
05/31/2022 11:58:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
05/31/2022 11:58:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
05/31/2022 11:58:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
05/31/2022 11:58:27 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.32631578947368417 on epoch=824
05/31/2022 11:58:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
05/31/2022 11:58:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
05/31/2022 11:58:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
05/31/2022 11:58:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
05/31/2022 11:58:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
05/31/2022 11:58:50 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=849
05/31/2022 11:58:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
05/31/2022 11:58:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
05/31/2022 11:59:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
05/31/2022 11:59:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
05/31/2022 11:59:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
05/31/2022 11:59:13 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.32631578947368417 on epoch=874
05/31/2022 11:59:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
05/31/2022 11:59:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
05/31/2022 11:59:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
05/31/2022 11:59:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
05/31/2022 11:59:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
05/31/2022 11:59:36 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.2991452991452992 on epoch=899
05/31/2022 11:59:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
05/31/2022 11:59:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
05/31/2022 11:59:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
05/31/2022 11:59:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
05/31/2022 11:59:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
05/31/2022 11:59:59 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.32631578947368417 on epoch=924
05/31/2022 12:00:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
05/31/2022 12:00:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
05/31/2022 12:00:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
05/31/2022 12:00:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
05/31/2022 12:00:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
05/31/2022 12:00:22 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.32631578947368417 on epoch=949
05/31/2022 12:00:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
05/31/2022 12:00:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
05/31/2022 12:00:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
05/31/2022 12:00:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
05/31/2022 12:00:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
05/31/2022 12:00:46 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=974
05/31/2022 12:00:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
05/31/2022 12:00:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
05/31/2022 12:00:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
05/31/2022 12:01:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
05/31/2022 12:01:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
05/31/2022 12:01:09 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.32631578947368417 on epoch=999
05/31/2022 12:01:09 - INFO - __main__ - save last model!
05/31/2022 12:01:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 12:01:09 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 12:01:09 - INFO - __main__ - Printing 3 examples
05/31/2022 12:01:09 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/31/2022 12:01:09 - INFO - __main__ - ['refuted']
05/31/2022 12:01:09 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/31/2022 12:01:09 - INFO - __main__ - ['refuted']
05/31/2022 12:01:09 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/31/2022 12:01:09 - INFO - __main__ - ['refuted']
05/31/2022 12:01:09 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:01:09 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:01:09 - INFO - __main__ - Start tokenizing ... 12792 instances
05/31/2022 12:01:09 - INFO - __main__ - Printing 3 examples
05/31/2022 12:01:09 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 12:01:09 - INFO - __main__ - ['entailed']
05/31/2022 12:01:09 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 12:01:09 - INFO - __main__ - ['entailed']
05/31/2022 12:01:09 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 12:01:09 - INFO - __main__ - ['entailed']
05/31/2022 12:01:09 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:01:09 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 12:01:09 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 12:01:09 - INFO - __main__ - Printing 3 examples
05/31/2022 12:01:09 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/31/2022 12:01:09 - INFO - __main__ - ['refuted']
05/31/2022 12:01:09 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/31/2022 12:01:09 - INFO - __main__ - ['refuted']
05/31/2022 12:01:09 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/31/2022 12:01:09 - INFO - __main__ - ['refuted']
05/31/2022 12:01:09 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:01:09 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:01:09 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 12:01:24 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 12:01:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 12:01:25 - INFO - __main__ - Starting training!
05/31/2022 12:01:33 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:01:45 - INFO - __main__ - Loaded 12792 examples from test data
05/31/2022 12:09:55 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/singletask-tab_fact/tab_fact_16_100_0.5_8_predictions.txt
05/31/2022 12:09:55 - INFO - __main__ - Classification-F1 on test data: 0.0224
05/31/2022 12:09:55 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.5, bsz=8, dev_performance=0.4817813765182186, test_performance=0.022355492604330847
05/31/2022 12:09:55 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.4, bsz=8 ...
05/31/2022 12:09:56 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 12:09:56 - INFO - __main__ - Printing 3 examples
05/31/2022 12:09:56 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/31/2022 12:09:56 - INFO - __main__ - ['refuted']
05/31/2022 12:09:56 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/31/2022 12:09:56 - INFO - __main__ - ['refuted']
05/31/2022 12:09:56 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/31/2022 12:09:56 - INFO - __main__ - ['refuted']
05/31/2022 12:09:56 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:09:56 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:09:56 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 12:09:56 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 12:09:56 - INFO - __main__ - Printing 3 examples
05/31/2022 12:09:56 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/31/2022 12:09:56 - INFO - __main__ - ['refuted']
05/31/2022 12:09:56 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/31/2022 12:09:56 - INFO - __main__ - ['refuted']
05/31/2022 12:09:56 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/31/2022 12:09:56 - INFO - __main__ - ['refuted']
05/31/2022 12:09:56 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:09:56 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:09:56 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 12:10:15 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 12:10:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 12:10:16 - INFO - __main__ - Starting training!
05/31/2022 12:10:21 - INFO - __main__ - Step 10 Global step 10 Train loss 3.20 on epoch=4
05/31/2022 12:10:25 - INFO - __main__ - Step 20 Global step 20 Train loss 0.92 on epoch=9
05/31/2022 12:10:30 - INFO - __main__ - Step 30 Global step 30 Train loss 0.40 on epoch=14
05/31/2022 12:10:34 - INFO - __main__ - Step 40 Global step 40 Train loss 0.32 on epoch=19
05/31/2022 12:10:39 - INFO - __main__ - Step 50 Global step 50 Train loss 0.30 on epoch=24
05/31/2022 12:10:40 - INFO - __main__ - Global step 50 Train loss 1.03 Classification-F1 0.3333333333333333 on epoch=24
05/31/2022 12:10:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
05/31/2022 12:10:44 - INFO - __main__ - Step 60 Global step 60 Train loss 0.27 on epoch=29
05/31/2022 12:10:49 - INFO - __main__ - Step 70 Global step 70 Train loss 0.26 on epoch=34
05/31/2022 12:10:53 - INFO - __main__ - Step 80 Global step 80 Train loss 0.24 on epoch=39
05/31/2022 12:10:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.24 on epoch=44
05/31/2022 12:11:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.22 on epoch=49
05/31/2022 12:11:03 - INFO - __main__ - Global step 100 Train loss 0.25 Classification-F1 0.4385964912280702 on epoch=49
05/31/2022 12:11:03 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4385964912280702 on epoch=49, global_step=100
05/31/2022 12:11:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.24 on epoch=54
05/31/2022 12:11:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.26 on epoch=59
05/31/2022 12:11:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.29 on epoch=64
05/31/2022 12:11:21 - INFO - __main__ - Step 140 Global step 140 Train loss 0.21 on epoch=69
05/31/2022 12:11:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.21 on epoch=74
05/31/2022 12:11:27 - INFO - __main__ - Global step 150 Train loss 0.24 Classification-F1 0.39756367663344405 on epoch=74
05/31/2022 12:11:32 - INFO - __main__ - Step 160 Global step 160 Train loss 0.20 on epoch=79
05/31/2022 12:11:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.21 on epoch=84
05/31/2022 12:11:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.21 on epoch=89
05/31/2022 12:11:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.19 on epoch=94
05/31/2022 12:11:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.22 on epoch=99
05/31/2022 12:11:50 - INFO - __main__ - Global step 200 Train loss 0.21 Classification-F1 0.39756367663344405 on epoch=99
05/31/2022 12:11:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.20 on epoch=104
05/31/2022 12:11:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.19 on epoch=109
05/31/2022 12:12:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.22 on epoch=114
05/31/2022 12:12:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.21 on epoch=119
05/31/2022 12:12:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.17 on epoch=124
05/31/2022 12:12:14 - INFO - __main__ - Global step 250 Train loss 0.20 Classification-F1 0.4666666666666667 on epoch=124
05/31/2022 12:12:14 - INFO - __main__ - Saving model with best Classification-F1: 0.4385964912280702 -> 0.4666666666666667 on epoch=124, global_step=250
05/31/2022 12:12:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.20 on epoch=129
05/31/2022 12:12:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.19 on epoch=134
05/31/2022 12:12:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.19 on epoch=139
05/31/2022 12:12:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.16 on epoch=144
05/31/2022 12:12:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.17 on epoch=149
05/31/2022 12:12:37 - INFO - __main__ - Global step 300 Train loss 0.18 Classification-F1 0.4666666666666667 on epoch=149
05/31/2022 12:12:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.16 on epoch=154
05/31/2022 12:12:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.15 on epoch=159
05/31/2022 12:12:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.17 on epoch=164
05/31/2022 12:12:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.16 on epoch=169
05/31/2022 12:12:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.19 on epoch=174
05/31/2022 12:13:01 - INFO - __main__ - Global step 350 Train loss 0.17 Classification-F1 0.31717171717171716 on epoch=174
05/31/2022 12:13:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.15 on epoch=179
05/31/2022 12:13:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.16 on epoch=184
05/31/2022 12:13:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.14 on epoch=189
05/31/2022 12:13:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.11 on epoch=194
05/31/2022 12:13:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.11 on epoch=199
05/31/2022 12:13:24 - INFO - __main__ - Global step 400 Train loss 0.13 Classification-F1 0.4682306940371457 on epoch=199
05/31/2022 12:13:24 - INFO - __main__ - Saving model with best Classification-F1: 0.4666666666666667 -> 0.4682306940371457 on epoch=199, global_step=400
05/31/2022 12:13:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=204
05/31/2022 12:13:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.09 on epoch=209
05/31/2022 12:13:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.06 on epoch=214
05/31/2022 12:13:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.08 on epoch=219
05/31/2022 12:13:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.08 on epoch=224
05/31/2022 12:13:48 - INFO - __main__ - Global step 450 Train loss 0.08 Classification-F1 0.4920634920634921 on epoch=224
05/31/2022 12:13:48 - INFO - __main__ - Saving model with best Classification-F1: 0.4682306940371457 -> 0.4920634920634921 on epoch=224, global_step=450
05/31/2022 12:13:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.08 on epoch=229
05/31/2022 12:13:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.08 on epoch=234
05/31/2022 12:14:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.06 on epoch=239
05/31/2022 12:14:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.09 on epoch=244
05/31/2022 12:14:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
05/31/2022 12:14:11 - INFO - __main__ - Global step 500 Train loss 0.08 Classification-F1 0.22835497835497837 on epoch=249
05/31/2022 12:14:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.04 on epoch=254
05/31/2022 12:14:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=259
05/31/2022 12:14:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.04 on epoch=264
05/31/2022 12:14:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=269
05/31/2022 12:14:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.04 on epoch=274
05/31/2022 12:14:35 - INFO - __main__ - Global step 550 Train loss 0.05 Classification-F1 0.2904761904761905 on epoch=274
05/31/2022 12:14:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=279
05/31/2022 12:14:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.03 on epoch=284
05/31/2022 12:14:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.04 on epoch=289
05/31/2022 12:14:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
05/31/2022 12:14:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.02 on epoch=299
05/31/2022 12:14:58 - INFO - __main__ - Global step 600 Train loss 0.03 Classification-F1 0.2718052738336714 on epoch=299
05/31/2022 12:15:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.02 on epoch=304
05/31/2022 12:15:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.06 on epoch=309
05/31/2022 12:15:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
05/31/2022 12:15:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
05/31/2022 12:15:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.04 on epoch=324
05/31/2022 12:15:22 - INFO - __main__ - Global step 650 Train loss 0.04 Classification-F1 0.1466973886328725 on epoch=324
05/31/2022 12:15:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=329
05/31/2022 12:15:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
05/31/2022 12:15:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=339
05/31/2022 12:15:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
05/31/2022 12:15:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
05/31/2022 12:15:45 - INFO - __main__ - Global step 700 Train loss 0.02 Classification-F1 0.22163865546218486 on epoch=349
05/31/2022 12:15:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
05/31/2022 12:15:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
05/31/2022 12:15:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
05/31/2022 12:16:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
05/31/2022 12:16:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
05/31/2022 12:16:09 - INFO - __main__ - Global step 750 Train loss 0.02 Classification-F1 0.22116402116402112 on epoch=374
05/31/2022 12:16:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
05/31/2022 12:16:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
05/31/2022 12:16:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
05/31/2022 12:16:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
05/31/2022 12:16:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
05/31/2022 12:16:32 - INFO - __main__ - Global step 800 Train loss 0.02 Classification-F1 0.13951734539969834 on epoch=399
05/31/2022 12:16:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
05/31/2022 12:16:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
05/31/2022 12:16:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
05/31/2022 12:16:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
05/31/2022 12:16:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
05/31/2022 12:16:56 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.23908523908523904 on epoch=424
05/31/2022 12:17:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
05/31/2022 12:17:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
05/31/2022 12:17:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
05/31/2022 12:17:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
05/31/2022 12:17:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
05/31/2022 12:17:19 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.17244125846276384 on epoch=449
05/31/2022 12:17:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
05/31/2022 12:17:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
05/31/2022 12:17:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
05/31/2022 12:17:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
05/31/2022 12:17:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
05/31/2022 12:17:43 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.13438735177865613 on epoch=474
05/31/2022 12:17:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
05/31/2022 12:17:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
05/31/2022 12:17:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
05/31/2022 12:18:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
05/31/2022 12:18:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
05/31/2022 12:18:06 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.22116402116402112 on epoch=499
05/31/2022 12:18:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
05/31/2022 12:18:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
05/31/2022 12:18:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
05/31/2022 12:18:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
05/31/2022 12:18:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
05/31/2022 12:18:30 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.20264550264550263 on epoch=524
05/31/2022 12:18:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
05/31/2022 12:18:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
05/31/2022 12:18:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
05/31/2022 12:18:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
05/31/2022 12:18:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
05/31/2022 12:18:53 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.14385614385614387 on epoch=549
05/31/2022 12:18:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
05/31/2022 12:19:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
05/31/2022 12:19:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
05/31/2022 12:19:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
05/31/2022 12:19:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
05/31/2022 12:19:17 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.20264550264550263 on epoch=574
05/31/2022 12:19:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
05/31/2022 12:19:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
05/31/2022 12:19:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
05/31/2022 12:19:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
05/31/2022 12:19:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
05/31/2022 12:19:40 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.08701298701298701 on epoch=599
05/31/2022 12:19:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
05/31/2022 12:19:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
05/31/2022 12:19:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
05/31/2022 12:19:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
05/31/2022 12:20:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
05/31/2022 12:20:03 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.12990196078431374 on epoch=624
05/31/2022 12:20:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
05/31/2022 12:20:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
05/31/2022 12:20:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
05/31/2022 12:20:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
05/31/2022 12:20:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
05/31/2022 12:20:27 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.1538823529411765 on epoch=649
05/31/2022 12:20:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
05/31/2022 12:20:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
05/31/2022 12:20:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
05/31/2022 12:20:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
05/31/2022 12:20:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
05/31/2022 12:20:50 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.10991596638655464 on epoch=674
05/31/2022 12:20:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
05/31/2022 12:20:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
05/31/2022 12:21:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
05/31/2022 12:21:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
05/31/2022 12:21:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
05/31/2022 12:21:14 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.1257142857142857 on epoch=699
05/31/2022 12:21:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
05/31/2022 12:21:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
05/31/2022 12:21:27 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
05/31/2022 12:21:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
05/31/2022 12:21:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
05/31/2022 12:21:37 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.09831410825199645 on epoch=724
05/31/2022 12:21:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
05/31/2022 12:21:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
05/31/2022 12:21:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
05/31/2022 12:21:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
05/31/2022 12:21:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
05/31/2022 12:22:01 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.1257142857142857 on epoch=749
05/31/2022 12:22:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
05/31/2022 12:22:09 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
05/31/2022 12:22:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
05/31/2022 12:22:18 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
05/31/2022 12:22:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
05/31/2022 12:22:24 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.2654320987654321 on epoch=774
05/31/2022 12:22:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
05/31/2022 12:22:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
05/31/2022 12:22:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
05/31/2022 12:22:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
05/31/2022 12:22:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
05/31/2022 12:22:48 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.20264550264550263 on epoch=799
05/31/2022 12:22:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
05/31/2022 12:22:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
05/31/2022 12:23:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
05/31/2022 12:23:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
05/31/2022 12:23:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
05/31/2022 12:23:11 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.14126984126984124 on epoch=824
05/31/2022 12:23:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
05/31/2022 12:23:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
05/31/2022 12:23:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
05/31/2022 12:23:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
05/31/2022 12:23:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
05/31/2022 12:23:35 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.13515406162464985 on epoch=849
05/31/2022 12:23:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
05/31/2022 12:23:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
05/31/2022 12:23:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
05/31/2022 12:23:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
05/31/2022 12:23:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
05/31/2022 12:23:58 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.12108843537414964 on epoch=874
05/31/2022 12:24:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
05/31/2022 12:24:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
05/31/2022 12:24:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
05/31/2022 12:24:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
05/31/2022 12:24:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
05/31/2022 12:24:22 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.11895351025785808 on epoch=899
05/31/2022 12:24:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
05/31/2022 12:24:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=909
05/31/2022 12:24:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
05/31/2022 12:24:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
05/31/2022 12:24:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
05/31/2022 12:24:45 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.15085714285714286 on epoch=924
05/31/2022 12:24:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
05/31/2022 12:24:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
05/31/2022 12:24:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
05/31/2022 12:25:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
05/31/2022 12:25:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
05/31/2022 12:25:09 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.12738095238095237 on epoch=949
05/31/2022 12:25:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
05/31/2022 12:25:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
05/31/2022 12:25:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
05/31/2022 12:25:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
05/31/2022 12:25:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
05/31/2022 12:25:32 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.09056956115779645 on epoch=974
05/31/2022 12:25:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
05/31/2022 12:25:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
05/31/2022 12:25:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
05/31/2022 12:25:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
05/31/2022 12:25:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
05/31/2022 12:25:56 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 12:25:56 - INFO - __main__ - Printing 3 examples
05/31/2022 12:25:56 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/31/2022 12:25:56 - INFO - __main__ - ['refuted']
05/31/2022 12:25:56 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/31/2022 12:25:56 - INFO - __main__ - ['refuted']
05/31/2022 12:25:56 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/31/2022 12:25:56 - INFO - __main__ - ['refuted']
05/31/2022 12:25:56 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:25:56 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.09689440993788818 on epoch=999
05/31/2022 12:25:56 - INFO - __main__ - save last model!
05/31/2022 12:25:56 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:25:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 12:25:56 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 12:25:56 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 12:25:56 - INFO - __main__ - Printing 3 examples
05/31/2022 12:25:56 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/31/2022 12:25:56 - INFO - __main__ - ['refuted']
05/31/2022 12:25:56 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/31/2022 12:25:56 - INFO - __main__ - ['refuted']
05/31/2022 12:25:56 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/31/2022 12:25:56 - INFO - __main__ - ['refuted']
05/31/2022 12:25:56 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:25:56 - INFO - __main__ - Start tokenizing ... 12792 instances
05/31/2022 12:25:56 - INFO - __main__ - Printing 3 examples
05/31/2022 12:25:56 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 12:25:56 - INFO - __main__ - ['entailed']
05/31/2022 12:25:56 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 12:25:56 - INFO - __main__ - ['entailed']
05/31/2022 12:25:56 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 12:25:56 - INFO - __main__ - ['entailed']
05/31/2022 12:25:56 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:25:56 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:25:56 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 12:26:15 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 12:26:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 12:26:16 - INFO - __main__ - Starting training!
05/31/2022 12:26:21 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:26:34 - INFO - __main__ - Loaded 12792 examples from test data
05/31/2022 12:34:41 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/singletask-tab_fact/tab_fact_16_100_0.4_8_predictions.txt
05/31/2022 12:34:41 - INFO - __main__ - Classification-F1 on test data: 0.0117
05/31/2022 12:34:41 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.4, bsz=8, dev_performance=0.4920634920634921, test_performance=0.011705593963427777
05/31/2022 12:34:41 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.3, bsz=8 ...
05/31/2022 12:34:42 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 12:34:42 - INFO - __main__ - Printing 3 examples
05/31/2022 12:34:42 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/31/2022 12:34:42 - INFO - __main__ - ['refuted']
05/31/2022 12:34:42 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/31/2022 12:34:42 - INFO - __main__ - ['refuted']
05/31/2022 12:34:42 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/31/2022 12:34:42 - INFO - __main__ - ['refuted']
05/31/2022 12:34:42 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:34:42 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:34:42 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 12:34:42 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 12:34:42 - INFO - __main__ - Printing 3 examples
05/31/2022 12:34:42 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/31/2022 12:34:42 - INFO - __main__ - ['refuted']
05/31/2022 12:34:42 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/31/2022 12:34:42 - INFO - __main__ - ['refuted']
05/31/2022 12:34:42 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/31/2022 12:34:42 - INFO - __main__ - ['refuted']
05/31/2022 12:34:42 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:34:42 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:34:42 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 12:34:57 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 12:34:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 12:34:58 - INFO - __main__ - Starting training!
05/31/2022 12:35:03 - INFO - __main__ - Step 10 Global step 10 Train loss 3.41 on epoch=4
05/31/2022 12:35:07 - INFO - __main__ - Step 20 Global step 20 Train loss 1.62 on epoch=9
05/31/2022 12:35:12 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=14
05/31/2022 12:35:16 - INFO - __main__ - Step 40 Global step 40 Train loss 0.39 on epoch=19
05/31/2022 12:35:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.30 on epoch=24
05/31/2022 12:35:22 - INFO - __main__ - Global step 50 Train loss 1.25 Classification-F1 0.3191489361702127 on epoch=24
05/31/2022 12:35:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3191489361702127 on epoch=24, global_step=50
05/31/2022 12:35:26 - INFO - __main__ - Step 60 Global step 60 Train loss 0.30 on epoch=29
05/31/2022 12:35:30 - INFO - __main__ - Step 70 Global step 70 Train loss 0.25 on epoch=34
05/31/2022 12:35:35 - INFO - __main__ - Step 80 Global step 80 Train loss 0.26 on epoch=39
05/31/2022 12:35:39 - INFO - __main__ - Step 90 Global step 90 Train loss 0.32 on epoch=44
05/31/2022 12:35:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.27 on epoch=49
05/31/2022 12:35:45 - INFO - __main__ - Global step 100 Train loss 0.28 Classification-F1 0.39756367663344405 on epoch=49
05/31/2022 12:35:45 - INFO - __main__ - Saving model with best Classification-F1: 0.3191489361702127 -> 0.39756367663344405 on epoch=49, global_step=100
05/31/2022 12:35:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.24 on epoch=54
05/31/2022 12:35:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.22 on epoch=59
05/31/2022 12:35:58 - INFO - __main__ - Step 130 Global step 130 Train loss 0.28 on epoch=64
05/31/2022 12:36:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.24 on epoch=69
05/31/2022 12:36:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.22 on epoch=74
05/31/2022 12:36:08 - INFO - __main__ - Global step 150 Train loss 0.24 Classification-F1 0.39756367663344405 on epoch=74
05/31/2022 12:36:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=79
05/31/2022 12:36:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.23 on epoch=84
05/31/2022 12:36:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.24 on epoch=89
05/31/2022 12:36:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.21 on epoch=94
05/31/2022 12:36:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.21 on epoch=99
05/31/2022 12:36:31 - INFO - __main__ - Global step 200 Train loss 0.23 Classification-F1 0.39756367663344405 on epoch=99
05/31/2022 12:36:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.25 on epoch=104
05/31/2022 12:36:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.20 on epoch=109
05/31/2022 12:36:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.18 on epoch=114
05/31/2022 12:36:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.19 on epoch=119
05/31/2022 12:36:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.19 on epoch=124
05/31/2022 12:36:55 - INFO - __main__ - Global step 250 Train loss 0.20 Classification-F1 0.2698412698412698 on epoch=124
05/31/2022 12:36:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.19 on epoch=129
05/31/2022 12:37:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.19 on epoch=134
05/31/2022 12:37:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.20 on epoch=139
05/31/2022 12:37:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.20 on epoch=144
05/31/2022 12:37:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.14 on epoch=149
05/31/2022 12:37:18 - INFO - __main__ - Global step 300 Train loss 0.18 Classification-F1 0.2909090909090909 on epoch=149
05/31/2022 12:37:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.14 on epoch=154
05/31/2022 12:37:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.15 on epoch=159
05/31/2022 12:37:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.18 on epoch=164
05/31/2022 12:37:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.16 on epoch=169
05/31/2022 12:37:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.16 on epoch=174
05/31/2022 12:37:41 - INFO - __main__ - Global step 350 Train loss 0.16 Classification-F1 0.21906354515050164 on epoch=174
05/31/2022 12:37:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.15 on epoch=179
05/31/2022 12:37:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.16 on epoch=184
05/31/2022 12:37:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.14 on epoch=189
05/31/2022 12:37:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.10 on epoch=194
05/31/2022 12:38:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.09 on epoch=199
05/31/2022 12:38:04 - INFO - __main__ - Global step 400 Train loss 0.13 Classification-F1 0.20624303232998886 on epoch=199
05/31/2022 12:38:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.11 on epoch=204
05/31/2022 12:38:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.11 on epoch=209
05/31/2022 12:38:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.12 on epoch=214
05/31/2022 12:38:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.09 on epoch=219
05/31/2022 12:38:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.11 on epoch=224
05/31/2022 12:38:27 - INFO - __main__ - Global step 450 Train loss 0.11 Classification-F1 0.24166666666666667 on epoch=224
05/31/2022 12:38:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.09 on epoch=229
05/31/2022 12:38:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.09 on epoch=234
05/31/2022 12:38:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.12 on epoch=239
05/31/2022 12:38:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.09 on epoch=244
05/31/2022 12:38:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.09 on epoch=249
05/31/2022 12:38:50 - INFO - __main__ - Global step 500 Train loss 0.10 Classification-F1 0.4231177094379639 on epoch=249
05/31/2022 12:38:51 - INFO - __main__ - Saving model with best Classification-F1: 0.39756367663344405 -> 0.4231177094379639 on epoch=249, global_step=500
05/31/2022 12:38:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.09 on epoch=254
05/31/2022 12:38:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.09 on epoch=259
05/31/2022 12:39:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.07 on epoch=264
05/31/2022 12:39:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.07 on epoch=269
05/31/2022 12:39:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
05/31/2022 12:39:14 - INFO - __main__ - Global step 550 Train loss 0.08 Classification-F1 0.17995642701525055 on epoch=274
05/31/2022 12:39:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.11 on epoch=279
05/31/2022 12:39:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.06 on epoch=284
05/31/2022 12:39:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.09 on epoch=289
05/31/2022 12:39:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.05 on epoch=294
05/31/2022 12:39:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=299
05/31/2022 12:39:37 - INFO - __main__ - Global step 600 Train loss 0.07 Classification-F1 0.22864864864864864 on epoch=299
05/31/2022 12:39:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=304
05/31/2022 12:39:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
05/31/2022 12:39:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.06 on epoch=314
05/31/2022 12:39:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
05/31/2022 12:39:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=324
05/31/2022 12:40:00 - INFO - __main__ - Global step 650 Train loss 0.04 Classification-F1 0.3264033264033264 on epoch=324
05/31/2022 12:40:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
05/31/2022 12:40:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
05/31/2022 12:40:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
05/31/2022 12:40:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
05/31/2022 12:40:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=349
05/31/2022 12:40:23 - INFO - __main__ - Global step 700 Train loss 0.04 Classification-F1 0.2827442827442827 on epoch=349
05/31/2022 12:40:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
05/31/2022 12:40:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
05/31/2022 12:40:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=364
05/31/2022 12:40:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
05/31/2022 12:40:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
05/31/2022 12:40:46 - INFO - __main__ - Global step 750 Train loss 0.03 Classification-F1 0.29964912280701755 on epoch=374
05/31/2022 12:40:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
05/31/2022 12:40:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
05/31/2022 12:41:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=389
05/31/2022 12:41:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
05/31/2022 12:41:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
05/31/2022 12:41:10 - INFO - __main__ - Global step 800 Train loss 0.02 Classification-F1 0.17511111111111113 on epoch=399
05/31/2022 12:41:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=404
05/31/2022 12:41:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=409
05/31/2022 12:41:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
05/31/2022 12:41:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
05/31/2022 12:41:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
05/31/2022 12:41:33 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.3162393162393162 on epoch=424
05/31/2022 12:41:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
05/31/2022 12:41:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
05/31/2022 12:41:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
05/31/2022 12:41:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
05/31/2022 12:41:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.15 on epoch=449
05/31/2022 12:41:56 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.24890350877192985 on epoch=449
05/31/2022 12:42:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
05/31/2022 12:42:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
05/31/2022 12:42:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
05/31/2022 12:42:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=469
05/31/2022 12:42:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
05/31/2022 12:42:19 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.22116402116402112 on epoch=474
05/31/2022 12:42:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
05/31/2022 12:42:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
05/31/2022 12:42:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
05/31/2022 12:42:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
05/31/2022 12:42:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
05/31/2022 12:42:43 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.16111111111111112 on epoch=499
05/31/2022 12:42:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
05/31/2022 12:42:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
05/31/2022 12:42:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
05/31/2022 12:43:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
05/31/2022 12:43:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
05/31/2022 12:43:06 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.09727891156462584 on epoch=524
05/31/2022 12:43:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
05/31/2022 12:43:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
05/31/2022 12:43:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
05/31/2022 12:43:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
05/31/2022 12:43:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
05/31/2022 12:43:30 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.1526374859708193 on epoch=549
05/31/2022 12:43:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
05/31/2022 12:43:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
05/31/2022 12:43:43 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
05/31/2022 12:43:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
05/31/2022 12:43:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
05/31/2022 12:43:53 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.29488536155202816 on epoch=574
05/31/2022 12:43:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
05/31/2022 12:44:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
05/31/2022 12:44:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
05/31/2022 12:44:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
05/31/2022 12:44:15 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=599
05/31/2022 12:44:16 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.20500000000000002 on epoch=599
05/31/2022 12:44:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
05/31/2022 12:44:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
05/31/2022 12:44:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
05/31/2022 12:44:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
05/31/2022 12:44:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
05/31/2022 12:44:40 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.19477124183006536 on epoch=624
05/31/2022 12:44:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
05/31/2022 12:44:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
05/31/2022 12:44:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
05/31/2022 12:44:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
05/31/2022 12:45:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
05/31/2022 12:45:04 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.2247252747252747 on epoch=649
05/31/2022 12:45:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
05/31/2022 12:45:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
05/31/2022 12:45:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
05/31/2022 12:45:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
05/31/2022 12:45:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
05/31/2022 12:45:27 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.3142857142857143 on epoch=674
05/31/2022 12:45:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
05/31/2022 12:45:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
05/31/2022 12:45:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
05/31/2022 12:45:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
05/31/2022 12:45:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
05/31/2022 12:45:51 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.19477124183006536 on epoch=699
05/31/2022 12:45:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
05/31/2022 12:46:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
05/31/2022 12:46:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
05/31/2022 12:46:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
05/31/2022 12:46:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
05/31/2022 12:46:14 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.17995642701525055 on epoch=724
05/31/2022 12:46:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
05/31/2022 12:46:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
05/31/2022 12:46:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
05/31/2022 12:46:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
05/31/2022 12:46:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
05/31/2022 12:46:38 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.17995642701525055 on epoch=749
05/31/2022 12:46:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
05/31/2022 12:46:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
05/31/2022 12:46:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
05/31/2022 12:46:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
05/31/2022 12:47:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
05/31/2022 12:47:02 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.1415686274509804 on epoch=774
05/31/2022 12:47:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
05/31/2022 12:47:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
05/31/2022 12:47:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
05/31/2022 12:47:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
05/31/2022 12:47:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
05/31/2022 12:47:25 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.23504273504273504 on epoch=799
05/31/2022 12:47:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
05/31/2022 12:47:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
05/31/2022 12:47:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
05/31/2022 12:47:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
05/31/2022 12:47:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
05/31/2022 12:47:49 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.2394957983193277 on epoch=824
05/31/2022 12:47:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
05/31/2022 12:47:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
05/31/2022 12:48:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
05/31/2022 12:48:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
05/31/2022 12:48:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
05/31/2022 12:48:12 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.19477124183006536 on epoch=849
05/31/2022 12:48:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
05/31/2022 12:48:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
05/31/2022 12:48:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
05/31/2022 12:48:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=869
05/31/2022 12:48:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
05/31/2022 12:48:36 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.1828054298642534 on epoch=874
05/31/2022 12:48:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
05/31/2022 12:48:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
05/31/2022 12:48:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
05/31/2022 12:48:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
05/31/2022 12:48:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
05/31/2022 12:49:00 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.16988235294117646 on epoch=899
05/31/2022 12:49:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=904
05/31/2022 12:49:09 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
05/31/2022 12:49:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
05/31/2022 12:49:18 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
05/31/2022 12:49:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
05/31/2022 12:49:23 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.21282327586206895 on epoch=924
05/31/2022 12:49:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
05/31/2022 12:49:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
05/31/2022 12:49:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
05/31/2022 12:49:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
05/31/2022 12:49:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
05/31/2022 12:49:47 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.16685714285714287 on epoch=949
05/31/2022 12:49:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
05/31/2022 12:49:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
05/31/2022 12:50:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
05/31/2022 12:50:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
05/31/2022 12:50:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
05/31/2022 12:50:11 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.1415686274509804 on epoch=974
05/31/2022 12:50:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
05/31/2022 12:50:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
05/31/2022 12:50:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
05/31/2022 12:50:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
05/31/2022 12:50:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
05/31/2022 12:50:34 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.1828054298642534 on epoch=999
05/31/2022 12:50:34 - INFO - __main__ - save last model!
05/31/2022 12:50:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 12:50:34 - INFO - __main__ - Start tokenizing ... 12792 instances
05/31/2022 12:50:34 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 12:50:34 - INFO - __main__ - Printing 3 examples
05/31/2022 12:50:34 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/31/2022 12:50:34 - INFO - __main__ - ['refuted']
05/31/2022 12:50:34 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/31/2022 12:50:34 - INFO - __main__ - ['refuted']
05/31/2022 12:50:34 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/31/2022 12:50:34 - INFO - __main__ - ['refuted']
05/31/2022 12:50:34 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:50:34 - INFO - __main__ - Printing 3 examples
05/31/2022 12:50:34 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 12:50:34 - INFO - __main__ - ['entailed']
05/31/2022 12:50:34 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 12:50:34 - INFO - __main__ - ['entailed']
05/31/2022 12:50:34 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 12:50:34 - INFO - __main__ - ['entailed']
05/31/2022 12:50:34 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:50:34 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:50:34 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 12:50:34 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 12:50:34 - INFO - __main__ - Printing 3 examples
05/31/2022 12:50:34 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/31/2022 12:50:34 - INFO - __main__ - ['refuted']
05/31/2022 12:50:34 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/31/2022 12:50:34 - INFO - __main__ - ['refuted']
05/31/2022 12:50:34 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/31/2022 12:50:34 - INFO - __main__ - ['refuted']
05/31/2022 12:50:34 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:50:35 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:50:35 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 12:50:53 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 12:50:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 12:50:54 - INFO - __main__ - Starting training!
05/31/2022 12:50:59 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:51:12 - INFO - __main__ - Loaded 12792 examples from test data
05/31/2022 12:59:27 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/singletask-tab_fact/tab_fact_16_100_0.3_8_predictions.txt
05/31/2022 12:59:27 - INFO - __main__ - Classification-F1 on test data: 0.0206
05/31/2022 12:59:27 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.3, bsz=8, dev_performance=0.4231177094379639, test_performance=0.02057724911256211
05/31/2022 12:59:27 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.2, bsz=8 ...
05/31/2022 12:59:28 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 12:59:28 - INFO - __main__ - Printing 3 examples
05/31/2022 12:59:28 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/31/2022 12:59:28 - INFO - __main__ - ['refuted']
05/31/2022 12:59:28 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/31/2022 12:59:28 - INFO - __main__ - ['refuted']
05/31/2022 12:59:28 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/31/2022 12:59:28 - INFO - __main__ - ['refuted']
05/31/2022 12:59:28 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:59:28 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:59:28 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 12:59:28 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 12:59:28 - INFO - __main__ - Printing 3 examples
05/31/2022 12:59:28 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
05/31/2022 12:59:28 - INFO - __main__ - ['refuted']
05/31/2022 12:59:28 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956#î isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
05/31/2022 12:59:28 - INFO - __main__ - ['refuted']
05/31/2022 12:59:28 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mamá también#mexico#alfonso cuarón [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educación#spain#pedro almodóvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'armée des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 săptămni şi 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de été#france#olivier assayas [n] 
05/31/2022 12:59:28 - INFO - __main__ - ['refuted']
05/31/2022 12:59:28 - INFO - __main__ - Tokenizing Input ...
05/31/2022 12:59:28 - INFO - __main__ - Tokenizing Output ...
05/31/2022 12:59:28 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 12:59:47 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 12:59:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 12:59:48 - INFO - __main__ - Starting training!
05/31/2022 12:59:53 - INFO - __main__ - Step 10 Global step 10 Train loss 4.07 on epoch=4
05/31/2022 12:59:58 - INFO - __main__ - Step 20 Global step 20 Train loss 2.41 on epoch=9
05/31/2022 13:00:02 - INFO - __main__ - Step 30 Global step 30 Train loss 1.26 on epoch=14
05/31/2022 13:00:07 - INFO - __main__ - Step 40 Global step 40 Train loss 0.66 on epoch=19
05/31/2022 13:00:11 - INFO - __main__ - Step 50 Global step 50 Train loss 0.40 on epoch=24
05/31/2022 13:00:12 - INFO - __main__ - Global step 50 Train loss 1.76 Classification-F1 0.3191489361702127 on epoch=24
05/31/2022 13:00:12 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3191489361702127 on epoch=24, global_step=50
05/31/2022 13:00:17 - INFO - __main__ - Step 60 Global step 60 Train loss 0.32 on epoch=29
05/31/2022 13:00:21 - INFO - __main__ - Step 70 Global step 70 Train loss 0.31 on epoch=34
05/31/2022 13:00:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.30 on epoch=39
05/31/2022 13:00:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.25 on epoch=44
05/31/2022 13:00:35 - INFO - __main__ - Step 100 Global step 100 Train loss 0.30 on epoch=49
05/31/2022 13:00:36 - INFO - __main__ - Global step 100 Train loss 0.30 Classification-F1 0.3191489361702127 on epoch=49
05/31/2022 13:00:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.24 on epoch=54
05/31/2022 13:00:45 - INFO - __main__ - Step 120 Global step 120 Train loss 0.26 on epoch=59
05/31/2022 13:00:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.26 on epoch=64
05/31/2022 13:00:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.28 on epoch=69
05/31/2022 13:00:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.23 on epoch=74
05/31/2022 13:01:00 - INFO - __main__ - Global step 150 Train loss 0.25 Classification-F1 0.4385964912280702 on epoch=74
05/31/2022 13:01:00 - INFO - __main__ - Saving model with best Classification-F1: 0.3191489361702127 -> 0.4385964912280702 on epoch=74, global_step=150
05/31/2022 13:01:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=79
05/31/2022 13:01:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.30 on epoch=84
05/31/2022 13:01:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.23 on epoch=89
05/31/2022 13:01:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.24 on epoch=94
05/31/2022 13:01:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.19 on epoch=99
05/31/2022 13:01:24 - INFO - __main__ - Global step 200 Train loss 0.24 Classification-F1 0.4385964912280702 on epoch=99
05/31/2022 13:01:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.19 on epoch=104
05/31/2022 13:01:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.25 on epoch=109
05/31/2022 13:01:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.23 on epoch=114
05/31/2022 13:01:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.22 on epoch=119
05/31/2022 13:01:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=124
05/31/2022 13:01:48 - INFO - __main__ - Global step 250 Train loss 0.22 Classification-F1 0.4385964912280702 on epoch=124
05/31/2022 13:01:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.21 on epoch=129
05/31/2022 13:01:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.21 on epoch=134
05/31/2022 13:02:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.19 on epoch=139
05/31/2022 13:02:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.22 on epoch=144
05/31/2022 13:02:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.21 on epoch=149
05/31/2022 13:02:12 - INFO - __main__ - Global step 300 Train loss 0.21 Classification-F1 0.39756367663344405 on epoch=149
05/31/2022 13:02:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=154
05/31/2022 13:02:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.21 on epoch=159
05/31/2022 13:02:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.18 on epoch=164
05/31/2022 13:02:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=169
05/31/2022 13:02:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.21 on epoch=174
05/31/2022 13:02:36 - INFO - __main__ - Global step 350 Train loss 0.20 Classification-F1 0.4181818181818182 on epoch=174
05/31/2022 13:02:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.22 on epoch=179
05/31/2022 13:02:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=184
05/31/2022 13:02:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=189
05/31/2022 13:02:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.19 on epoch=194
05/31/2022 13:02:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.18 on epoch=199
05/31/2022 13:02:59 - INFO - __main__ - Global step 400 Train loss 0.20 Classification-F1 0.4420512820512821 on epoch=199
05/31/2022 13:02:59 - INFO - __main__ - Saving model with best Classification-F1: 0.4385964912280702 -> 0.4420512820512821 on epoch=199, global_step=400
05/31/2022 13:03:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.18 on epoch=204
05/31/2022 13:03:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.19 on epoch=209
05/31/2022 13:03:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.19 on epoch=214
05/31/2022 13:03:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=219
05/31/2022 13:03:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=224
05/31/2022 13:03:23 - INFO - __main__ - Global step 450 Train loss 0.19 Classification-F1 0.4666666666666667 on epoch=224
05/31/2022 13:03:23 - INFO - __main__ - Saving model with best Classification-F1: 0.4420512820512821 -> 0.4666666666666667 on epoch=224, global_step=450
05/31/2022 13:03:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=229
05/31/2022 13:03:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.15 on epoch=234
05/31/2022 13:03:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.16 on epoch=239
05/31/2022 13:03:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.15 on epoch=244
05/31/2022 13:03:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.18 on epoch=249
05/31/2022 13:03:47 - INFO - __main__ - Global step 500 Train loss 0.17 Classification-F1 0.3162393162393162 on epoch=249
05/31/2022 13:03:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.16 on epoch=254
05/31/2022 13:03:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.13 on epoch=259
05/31/2022 13:04:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.12 on epoch=264
05/31/2022 13:04:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=269
05/31/2022 13:04:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.17 on epoch=274
05/31/2022 13:04:11 - INFO - __main__ - Global step 550 Train loss 0.15 Classification-F1 0.46843853820598 on epoch=274
05/31/2022 13:04:11 - INFO - __main__ - Saving model with best Classification-F1: 0.4666666666666667 -> 0.46843853820598 on epoch=274, global_step=550
05/31/2022 13:04:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.12 on epoch=279
05/31/2022 13:04:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.18 on epoch=284
05/31/2022 13:04:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=289
05/31/2022 13:04:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.14 on epoch=294
05/31/2022 13:04:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.11 on epoch=299
05/31/2022 13:04:35 - INFO - __main__ - Global step 600 Train loss 0.14 Classification-F1 0.46843853820598 on epoch=299
05/31/2022 13:04:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=304
05/31/2022 13:04:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.12 on epoch=309
05/31/2022 13:04:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.12 on epoch=314
05/31/2022 13:04:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.08 on epoch=319
05/31/2022 13:04:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.08 on epoch=324
05/31/2022 13:04:58 - INFO - __main__ - Global step 650 Train loss 0.11 Classification-F1 0.3172043010752688 on epoch=324
05/31/2022 13:05:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=329
05/31/2022 13:05:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=334
05/31/2022 13:05:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=339
05/31/2022 13:05:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=344
05/31/2022 13:05:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=349
05/31/2022 13:05:22 - INFO - __main__ - Global step 700 Train loss 0.11 Classification-F1 0.4909862142099682 on epoch=349
05/31/2022 13:05:22 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.4909862142099682 on epoch=349, global_step=700
05/31/2022 13:05:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=354
05/31/2022 13:05:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=359
05/31/2022 13:05:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=364
05/31/2022 13:05:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=369
05/31/2022 13:05:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=374
05/31/2022 13:05:46 - INFO - __main__ - Global step 750 Train loss 0.08 Classification-F1 0.3197492163009405 on epoch=374
05/31/2022 13:05:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=379
05/31/2022 13:05:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=384
05/31/2022 13:06:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=389
05/31/2022 13:06:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=394
05/31/2022 13:06:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
05/31/2022 13:06:10 - INFO - __main__ - Global step 800 Train loss 0.06 Classification-F1 0.5076923076923077 on epoch=399
05/31/2022 13:06:10 - INFO - __main__ - Saving model with best Classification-F1: 0.4909862142099682 -> 0.5076923076923077 on epoch=399, global_step=800
05/31/2022 13:06:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=404
05/31/2022 13:06:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
05/31/2022 13:06:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
05/31/2022 13:06:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
05/31/2022 13:06:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=424
05/31/2022 13:06:34 - INFO - __main__ - Global step 850 Train loss 0.06 Classification-F1 0.25705329153605017 on epoch=424
05/31/2022 13:06:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
05/31/2022 13:06:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=434
05/31/2022 13:06:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
05/31/2022 13:06:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
05/31/2022 13:06:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
05/31/2022 13:06:58 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.3333333333333333 on epoch=449
05/31/2022 13:07:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=454
05/31/2022 13:07:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
05/31/2022 13:07:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=464
05/31/2022 13:07:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=469
05/31/2022 13:07:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
05/31/2022 13:07:21 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.5270935960591133 on epoch=474
05/31/2022 13:07:21 - INFO - __main__ - Saving model with best Classification-F1: 0.5076923076923077 -> 0.5270935960591133 on epoch=474, global_step=950
05/31/2022 13:07:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=479
05/31/2022 13:07:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=484
05/31/2022 13:07:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
05/31/2022 13:07:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
05/31/2022 13:07:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
05/31/2022 13:07:45 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.23504273504273504 on epoch=499
05/31/2022 13:07:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
05/31/2022 13:07:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
05/31/2022 13:07:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
05/31/2022 13:08:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
05/31/2022 13:08:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
05/31/2022 13:08:09 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.19655172413793104 on epoch=524
05/31/2022 13:08:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
05/31/2022 13:08:18 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
05/31/2022 13:08:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
05/31/2022 13:08:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
05/31/2022 13:08:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
05/31/2022 13:08:33 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.2394957983193277 on epoch=549
05/31/2022 13:08:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
05/31/2022 13:08:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
05/31/2022 13:08:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
05/31/2022 13:08:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
05/31/2022 13:08:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
05/31/2022 13:08:56 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.25705329153605017 on epoch=574
05/31/2022 13:09:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
05/31/2022 13:09:05 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
05/31/2022 13:09:10 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
05/31/2022 13:09:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
05/31/2022 13:09:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
05/31/2022 13:09:20 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.20909090909090908 on epoch=599
05/31/2022 13:09:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
05/31/2022 13:09:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=609
05/31/2022 13:09:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
05/31/2022 13:09:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
05/31/2022 13:09:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
05/31/2022 13:09:44 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.26821705426356585 on epoch=624
05/31/2022 13:09:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
05/31/2022 13:09:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
05/31/2022 13:09:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
05/31/2022 13:10:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
05/31/2022 13:10:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
05/31/2022 13:10:08 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.35714285714285715 on epoch=649
05/31/2022 13:10:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
05/31/2022 13:10:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
05/31/2022 13:10:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
05/31/2022 13:10:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
05/31/2022 13:10:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
05/31/2022 13:10:32 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.20909090909090908 on epoch=674
05/31/2022 13:10:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
05/31/2022 13:10:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=684
05/31/2022 13:10:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
05/31/2022 13:10:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
05/31/2022 13:10:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
05/31/2022 13:10:55 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.1683982683982684 on epoch=699
05/31/2022 13:11:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
05/31/2022 13:11:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
05/31/2022 13:11:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
05/31/2022 13:11:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
05/31/2022 13:11:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
05/31/2022 13:11:19 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.20909090909090908 on epoch=724
05/31/2022 13:11:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
05/31/2022 13:11:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
05/31/2022 13:11:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
05/31/2022 13:11:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
05/31/2022 13:11:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
05/31/2022 13:11:43 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.20909090909090908 on epoch=749
05/31/2022 13:11:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
05/31/2022 13:11:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
05/31/2022 13:11:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
05/31/2022 13:12:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
05/31/2022 13:12:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
05/31/2022 13:12:07 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.24166666666666667 on epoch=774
05/31/2022 13:12:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
05/31/2022 13:12:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
05/31/2022 13:12:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
05/31/2022 13:12:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
05/31/2022 13:12:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=799
05/31/2022 13:12:31 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.19477124183006536 on epoch=799
05/31/2022 13:12:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
05/31/2022 13:12:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
05/31/2022 13:12:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
05/31/2022 13:12:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
05/31/2022 13:12:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
05/31/2022 13:12:54 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.19480519480519481 on epoch=824
05/31/2022 13:12:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
05/31/2022 13:13:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
05/31/2022 13:13:08 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
05/31/2022 13:13:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
05/31/2022 13:13:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
05/31/2022 13:13:18 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.25396825396825395 on epoch=849
05/31/2022 13:13:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
05/31/2022 13:13:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
05/31/2022 13:13:31 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
05/31/2022 13:13:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
05/31/2022 13:13:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
05/31/2022 13:13:42 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.25396825396825395 on epoch=874
05/31/2022 13:13:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
05/31/2022 13:13:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
05/31/2022 13:13:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
05/31/2022 13:14:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
05/31/2022 13:14:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
05/31/2022 13:14:05 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.25705329153605017 on epoch=899
05/31/2022 13:14:10 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
05/31/2022 13:14:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
05/31/2022 13:14:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
05/31/2022 13:14:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
05/31/2022 13:14:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
05/31/2022 13:14:29 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.25705329153605017 on epoch=924
05/31/2022 13:14:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
05/31/2022 13:14:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
05/31/2022 13:14:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
05/31/2022 13:14:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
05/31/2022 13:14:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
05/31/2022 13:14:53 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.2394957983193277 on epoch=949
05/31/2022 13:14:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
05/31/2022 13:15:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
05/31/2022 13:15:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
05/31/2022 13:15:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
05/31/2022 13:15:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
05/31/2022 13:15:17 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.24166666666666667 on epoch=974
05/31/2022 13:15:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
05/31/2022 13:15:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
05/31/2022 13:15:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
05/31/2022 13:15:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
05/31/2022 13:15:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
05/31/2022 13:15:41 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.3427377220480669 on epoch=999
05/31/2022 13:15:41 - INFO - __main__ - save last model!
05/31/2022 13:15:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 13:15:41 - INFO - __main__ - Start tokenizing ... 12792 instances
05/31/2022 13:15:41 - INFO - __main__ - Printing 3 examples
05/31/2022 13:15:41 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 13:15:41 - INFO - __main__ - ['entailed']
05/31/2022 13:15:41 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 13:15:41 - INFO - __main__ - ['entailed']
05/31/2022 13:15:41 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 13:15:41 - INFO - __main__ - ['entailed']
05/31/2022 13:15:41 - INFO - __main__ - Tokenizing Input ...
05/31/2022 13:15:41 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 13:15:41 - INFO - __main__ - Printing 3 examples
05/31/2022 13:15:41 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/31/2022 13:15:41 - INFO - __main__ - ['refuted']
05/31/2022 13:15:41 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/31/2022 13:15:41 - INFO - __main__ - ['refuted']
05/31/2022 13:15:41 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/31/2022 13:15:41 - INFO - __main__ - ['refuted']
05/31/2022 13:15:41 - INFO - __main__ - Tokenizing Input ...
05/31/2022 13:15:41 - INFO - __main__ - Tokenizing Output ...
05/31/2022 13:15:41 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 13:15:41 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 13:15:41 - INFO - __main__ - Printing 3 examples
05/31/2022 13:15:41 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/31/2022 13:15:41 - INFO - __main__ - ['refuted']
05/31/2022 13:15:41 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/31/2022 13:15:41 - INFO - __main__ - ['refuted']
05/31/2022 13:15:41 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/31/2022 13:15:41 - INFO - __main__ - ['refuted']
05/31/2022 13:15:41 - INFO - __main__ - Tokenizing Input ...
05/31/2022 13:15:41 - INFO - __main__ - Tokenizing Output ...
05/31/2022 13:15:41 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 13:15:57 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 13:15:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 13:15:58 - INFO - __main__ - Starting training!
05/31/2022 13:16:05 - INFO - __main__ - Tokenizing Output ...
05/31/2022 13:16:18 - INFO - __main__ - Loaded 12792 examples from test data
05/31/2022 13:24:28 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/singletask-tab_fact/tab_fact_16_100_0.2_8_predictions.txt
05/31/2022 13:24:28 - INFO - __main__ - Classification-F1 on test data: 0.0514
05/31/2022 13:24:29 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.2, bsz=8, dev_performance=0.5270935960591133, test_performance=0.05136124560336816
05/31/2022 13:24:29 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.5, bsz=8 ...
05/31/2022 13:24:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 13:24:29 - INFO - __main__ - Printing 3 examples
05/31/2022 13:24:29 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/31/2022 13:24:29 - INFO - __main__ - ['refuted']
05/31/2022 13:24:29 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/31/2022 13:24:29 - INFO - __main__ - ['refuted']
05/31/2022 13:24:29 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/31/2022 13:24:29 - INFO - __main__ - ['refuted']
05/31/2022 13:24:29 - INFO - __main__ - Tokenizing Input ...
05/31/2022 13:24:30 - INFO - __main__ - Tokenizing Output ...
05/31/2022 13:24:30 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 13:24:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 13:24:30 - INFO - __main__ - Printing 3 examples
05/31/2022 13:24:30 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/31/2022 13:24:30 - INFO - __main__ - ['refuted']
05/31/2022 13:24:30 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/31/2022 13:24:30 - INFO - __main__ - ['refuted']
05/31/2022 13:24:30 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/31/2022 13:24:30 - INFO - __main__ - ['refuted']
05/31/2022 13:24:30 - INFO - __main__ - Tokenizing Input ...
05/31/2022 13:24:30 - INFO - __main__ - Tokenizing Output ...
05/31/2022 13:24:30 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 13:24:48 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 13:24:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 13:24:49 - INFO - __main__ - Starting training!
05/31/2022 13:24:55 - INFO - __main__ - Step 10 Global step 10 Train loss 2.63 on epoch=4
05/31/2022 13:24:59 - INFO - __main__ - Step 20 Global step 20 Train loss 0.66 on epoch=9
05/31/2022 13:25:03 - INFO - __main__ - Step 30 Global step 30 Train loss 0.32 on epoch=14
05/31/2022 13:25:08 - INFO - __main__ - Step 40 Global step 40 Train loss 0.30 on epoch=19
05/31/2022 13:25:12 - INFO - __main__ - Step 50 Global step 50 Train loss 0.27 on epoch=24
05/31/2022 13:25:14 - INFO - __main__ - Global step 50 Train loss 0.84 Classification-F1 0.39756367663344405 on epoch=24
05/31/2022 13:25:14 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.39756367663344405 on epoch=24, global_step=50
05/31/2022 13:25:18 - INFO - __main__ - Step 60 Global step 60 Train loss 0.27 on epoch=29
05/31/2022 13:25:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.24 on epoch=34
05/31/2022 13:25:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.23 on epoch=39
05/31/2022 13:25:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.17 on epoch=44
05/31/2022 13:25:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.15 on epoch=49
05/31/2022 13:25:38 - INFO - __main__ - Global step 100 Train loss 0.21 Classification-F1 0.4666666666666667 on epoch=49
05/31/2022 13:25:38 - INFO - __main__ - Saving model with best Classification-F1: 0.39756367663344405 -> 0.4666666666666667 on epoch=49, global_step=100
05/31/2022 13:25:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.14 on epoch=54
05/31/2022 13:25:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.15 on epoch=59
05/31/2022 13:25:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.15 on epoch=64
05/31/2022 13:25:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.15 on epoch=69
05/31/2022 13:26:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.13 on epoch=74
05/31/2022 13:26:02 - INFO - __main__ - Global step 150 Train loss 0.15 Classification-F1 0.4420512820512821 on epoch=74
05/31/2022 13:26:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.15 on epoch=79
05/31/2022 13:26:11 - INFO - __main__ - Step 170 Global step 170 Train loss 0.13 on epoch=84
05/31/2022 13:26:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.13 on epoch=89
05/31/2022 13:26:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.10 on epoch=94
05/31/2022 13:26:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.08 on epoch=99
05/31/2022 13:26:26 - INFO - __main__ - Global step 200 Train loss 0.12 Classification-F1 0.4285714285714286 on epoch=99
05/31/2022 13:26:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.09 on epoch=104
05/31/2022 13:26:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.07 on epoch=109
05/31/2022 13:26:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.05 on epoch=114
05/31/2022 13:26:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.02 on epoch=119
05/31/2022 13:26:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.04 on epoch=124
05/31/2022 13:26:50 - INFO - __main__ - Global step 250 Train loss 0.06 Classification-F1 0.4231177094379639 on epoch=124
05/31/2022 13:26:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.02 on epoch=129
05/31/2022 13:26:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.02 on epoch=134
05/31/2022 13:27:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.02 on epoch=139
05/31/2022 13:27:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.01 on epoch=144
05/31/2022 13:27:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.02 on epoch=149
05/31/2022 13:27:14 - INFO - __main__ - Global step 300 Train loss 0.02 Classification-F1 0.4980392156862745 on epoch=149
05/31/2022 13:27:14 - INFO - __main__ - Saving model with best Classification-F1: 0.4666666666666667 -> 0.4980392156862745 on epoch=149, global_step=300
05/31/2022 13:27:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.05 on epoch=154
05/31/2022 13:27:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.01 on epoch=159
05/31/2022 13:27:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.02 on epoch=164
05/31/2022 13:27:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.02 on epoch=169
05/31/2022 13:27:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.01 on epoch=174
05/31/2022 13:27:38 - INFO - __main__ - Global step 350 Train loss 0.02 Classification-F1 0.4980392156862745 on epoch=174
05/31/2022 13:27:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.02 on epoch=179
05/31/2022 13:27:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.00 on epoch=184
05/31/2022 13:27:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.06 on epoch=189
05/31/2022 13:27:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.00 on epoch=194
05/31/2022 13:28:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
05/31/2022 13:28:02 - INFO - __main__ - Global step 400 Train loss 0.02 Classification-F1 0.2949494949494949 on epoch=199
05/31/2022 13:28:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.03 on epoch=204
05/31/2022 13:28:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
05/31/2022 13:28:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
05/31/2022 13:28:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.03 on epoch=219
05/31/2022 13:28:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.03 on epoch=224
05/31/2022 13:28:26 - INFO - __main__ - Global step 450 Train loss 0.02 Classification-F1 0.39139139139139134 on epoch=224
05/31/2022 13:28:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.01 on epoch=229
05/31/2022 13:28:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
05/31/2022 13:28:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
05/31/2022 13:28:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
05/31/2022 13:28:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
05/31/2022 13:28:49 - INFO - __main__ - Global step 500 Train loss 0.02 Classification-F1 0.30980392156862746 on epoch=249
05/31/2022 13:28:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
05/31/2022 13:28:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
05/31/2022 13:29:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.01 on epoch=264
05/31/2022 13:29:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
05/31/2022 13:29:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
05/31/2022 13:29:13 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 0.39139139139139134 on epoch=274
05/31/2022 13:29:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.01 on epoch=279
05/31/2022 13:29:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
05/31/2022 13:29:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
05/31/2022 13:29:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
05/31/2022 13:29:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
05/31/2022 13:29:37 - INFO - __main__ - Global step 600 Train loss 0.00 Classification-F1 0.37254901960784315 on epoch=299
05/31/2022 13:29:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
05/31/2022 13:29:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
05/31/2022 13:29:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
05/31/2022 13:29:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
05/31/2022 13:29:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
05/31/2022 13:30:00 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 0.37254901960784315 on epoch=324
05/31/2022 13:30:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
05/31/2022 13:30:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
05/31/2022 13:30:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
05/31/2022 13:30:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
05/31/2022 13:30:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
05/31/2022 13:30:23 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.2759103641456582 on epoch=349
05/31/2022 13:30:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
05/31/2022 13:30:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
05/31/2022 13:30:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
05/31/2022 13:30:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
05/31/2022 13:30:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
05/31/2022 13:30:47 - INFO - __main__ - Global step 750 Train loss 0.00 Classification-F1 0.29479377958079783 on epoch=374
05/31/2022 13:30:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
05/31/2022 13:30:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
05/31/2022 13:31:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
05/31/2022 13:31:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
05/31/2022 13:31:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
05/31/2022 13:31:11 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.41700404858299595 on epoch=399
05/31/2022 13:31:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
05/31/2022 13:31:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
05/31/2022 13:31:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
05/31/2022 13:31:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
05/31/2022 13:31:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
05/31/2022 13:31:34 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=424
05/31/2022 13:31:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
05/31/2022 13:31:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
05/31/2022 13:31:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
05/31/2022 13:31:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
05/31/2022 13:31:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
05/31/2022 13:31:58 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.39139139139139134 on epoch=449
05/31/2022 13:32:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
05/31/2022 13:32:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
05/31/2022 13:32:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
05/31/2022 13:32:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
05/31/2022 13:32:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
05/31/2022 13:32:21 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.40566959921798634 on epoch=474
05/31/2022 13:32:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
05/31/2022 13:32:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
05/31/2022 13:32:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
05/31/2022 13:32:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
05/31/2022 13:32:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
05/31/2022 13:32:45 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=499
05/31/2022 13:32:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
05/31/2022 13:32:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
05/31/2022 13:32:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
05/31/2022 13:33:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
05/31/2022 13:33:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
05/31/2022 13:33:08 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.41700404858299595 on epoch=524
05/31/2022 13:33:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
05/31/2022 13:33:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
05/31/2022 13:33:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
05/31/2022 13:33:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
05/31/2022 13:33:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
05/31/2022 13:33:32 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.4666666666666667 on epoch=549
05/31/2022 13:33:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
05/31/2022 13:33:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
05/31/2022 13:33:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
05/31/2022 13:33:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
05/31/2022 13:33:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
05/31/2022 13:33:55 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.4666666666666667 on epoch=574
05/31/2022 13:34:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
05/31/2022 13:34:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
05/31/2022 13:34:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
05/31/2022 13:34:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
05/31/2022 13:34:18 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
05/31/2022 13:34:19 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=599
05/31/2022 13:34:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
05/31/2022 13:34:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
05/31/2022 13:34:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
05/31/2022 13:34:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
05/31/2022 13:34:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
05/31/2022 13:34:43 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.25252525252525254 on epoch=624
05/31/2022 13:34:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
05/31/2022 13:34:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
05/31/2022 13:34:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
05/31/2022 13:35:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
05/31/2022 13:35:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
05/31/2022 13:35:06 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.3125 on epoch=649
05/31/2022 13:35:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
05/31/2022 13:35:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
05/31/2022 13:35:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
05/31/2022 13:35:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
05/31/2022 13:35:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
05/31/2022 13:35:30 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.34310850439882695 on epoch=674
05/31/2022 13:35:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
05/31/2022 13:35:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
05/31/2022 13:35:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
05/31/2022 13:35:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
05/31/2022 13:35:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
05/31/2022 13:35:53 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.17436974789915968 on epoch=699
05/31/2022 13:35:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
05/31/2022 13:36:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
05/31/2022 13:36:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
05/31/2022 13:36:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
05/31/2022 13:36:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
05/31/2022 13:36:17 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.3125 on epoch=724
05/31/2022 13:36:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
05/31/2022 13:36:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
05/31/2022 13:36:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
05/31/2022 13:36:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
05/31/2022 13:36:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
05/31/2022 13:36:41 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.2323232323232323 on epoch=749
05/31/2022 13:36:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
05/31/2022 13:36:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
05/31/2022 13:36:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
05/31/2022 13:36:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
05/31/2022 13:37:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
05/31/2022 13:37:04 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.33793103448275863 on epoch=774
05/31/2022 13:37:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
05/31/2022 13:37:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
05/31/2022 13:37:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
05/31/2022 13:37:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
05/31/2022 13:37:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
05/31/2022 13:37:28 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.39139139139139134 on epoch=799
05/31/2022 13:37:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
05/31/2022 13:37:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
05/31/2022 13:37:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
05/31/2022 13:37:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
05/31/2022 13:37:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
05/31/2022 13:37:51 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.34310850439882695 on epoch=824
05/31/2022 13:37:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
05/31/2022 13:38:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
05/31/2022 13:38:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
05/31/2022 13:38:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
05/31/2022 13:38:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
05/31/2022 13:38:15 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.34310850439882695 on epoch=849
05/31/2022 13:38:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
05/31/2022 13:38:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
05/31/2022 13:38:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
05/31/2022 13:38:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
05/31/2022 13:38:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
05/31/2022 13:38:39 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.34310850439882695 on epoch=874
05/31/2022 13:38:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
05/31/2022 13:38:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
05/31/2022 13:38:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
05/31/2022 13:38:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
05/31/2022 13:39:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
05/31/2022 13:39:02 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.34310850439882695 on epoch=899
05/31/2022 13:39:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
05/31/2022 13:39:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
05/31/2022 13:39:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
05/31/2022 13:39:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
05/31/2022 13:39:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
05/31/2022 13:39:26 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.34310850439882695 on epoch=924
05/31/2022 13:39:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
05/31/2022 13:39:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
05/31/2022 13:39:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
05/31/2022 13:39:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
05/31/2022 13:39:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
05/31/2022 13:39:49 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.34310850439882695 on epoch=949
05/31/2022 13:39:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
05/31/2022 13:39:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
05/31/2022 13:40:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
05/31/2022 13:40:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
05/31/2022 13:40:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
05/31/2022 13:40:13 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.2805474095796677 on epoch=974
05/31/2022 13:40:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
05/31/2022 13:40:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
05/31/2022 13:40:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
05/31/2022 13:40:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
05/31/2022 13:40:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
05/31/2022 13:40:36 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.4009852216748768 on epoch=999
05/31/2022 13:40:36 - INFO - __main__ - save last model!
05/31/2022 13:40:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 13:40:37 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 13:40:37 - INFO - __main__ - Printing 3 examples
05/31/2022 13:40:37 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/31/2022 13:40:37 - INFO - __main__ - ['refuted']
05/31/2022 13:40:37 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/31/2022 13:40:37 - INFO - __main__ - ['refuted']
05/31/2022 13:40:37 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/31/2022 13:40:37 - INFO - __main__ - ['refuted']
05/31/2022 13:40:37 - INFO - __main__ - Tokenizing Input ...
05/31/2022 13:40:37 - INFO - __main__ - Start tokenizing ... 12792 instances
05/31/2022 13:40:37 - INFO - __main__ - Printing 3 examples
05/31/2022 13:40:37 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 13:40:37 - INFO - __main__ - ['entailed']
05/31/2022 13:40:37 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 13:40:37 - INFO - __main__ - ['entailed']
05/31/2022 13:40:37 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 13:40:37 - INFO - __main__ - ['entailed']
05/31/2022 13:40:37 - INFO - __main__ - Tokenizing Input ...
05/31/2022 13:40:37 - INFO - __main__ - Tokenizing Output ...
05/31/2022 13:40:37 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 13:40:37 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 13:40:37 - INFO - __main__ - Printing 3 examples
05/31/2022 13:40:37 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/31/2022 13:40:37 - INFO - __main__ - ['refuted']
05/31/2022 13:40:37 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/31/2022 13:40:37 - INFO - __main__ - ['refuted']
05/31/2022 13:40:37 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/31/2022 13:40:37 - INFO - __main__ - ['refuted']
05/31/2022 13:40:37 - INFO - __main__ - Tokenizing Input ...
05/31/2022 13:40:37 - INFO - __main__ - Tokenizing Output ...
05/31/2022 13:40:37 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 13:40:52 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 13:40:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 13:40:53 - INFO - __main__ - Starting training!
05/31/2022 13:41:01 - INFO - __main__ - Tokenizing Output ...
05/31/2022 13:41:14 - INFO - __main__ - Loaded 12792 examples from test data
05/31/2022 13:49:28 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/singletask-tab_fact/tab_fact_16_13_0.5_8_predictions.txt
05/31/2022 13:49:28 - INFO - __main__ - Classification-F1 on test data: 0.1981
05/31/2022 13:49:28 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.5, bsz=8, dev_performance=0.4980392156862745, test_performance=0.19811595531773754
05/31/2022 13:49:28 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.4, bsz=8 ...
05/31/2022 13:49:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 13:49:29 - INFO - __main__ - Printing 3 examples
05/31/2022 13:49:29 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/31/2022 13:49:29 - INFO - __main__ - ['refuted']
05/31/2022 13:49:29 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/31/2022 13:49:29 - INFO - __main__ - ['refuted']
05/31/2022 13:49:29 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/31/2022 13:49:29 - INFO - __main__ - ['refuted']
05/31/2022 13:49:29 - INFO - __main__ - Tokenizing Input ...
05/31/2022 13:49:29 - INFO - __main__ - Tokenizing Output ...
05/31/2022 13:49:29 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 13:49:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 13:49:29 - INFO - __main__ - Printing 3 examples
05/31/2022 13:49:29 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/31/2022 13:49:29 - INFO - __main__ - ['refuted']
05/31/2022 13:49:29 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/31/2022 13:49:29 - INFO - __main__ - ['refuted']
05/31/2022 13:49:29 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/31/2022 13:49:29 - INFO - __main__ - ['refuted']
05/31/2022 13:49:29 - INFO - __main__ - Tokenizing Input ...
05/31/2022 13:49:29 - INFO - __main__ - Tokenizing Output ...
05/31/2022 13:49:29 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 13:49:48 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 13:49:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 13:49:49 - INFO - __main__ - Starting training!
05/31/2022 13:49:54 - INFO - __main__ - Step 10 Global step 10 Train loss 3.46 on epoch=4
05/31/2022 13:49:59 - INFO - __main__ - Step 20 Global step 20 Train loss 1.15 on epoch=9
05/31/2022 13:50:03 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=14
05/31/2022 13:50:07 - INFO - __main__ - Step 40 Global step 40 Train loss 0.28 on epoch=19
05/31/2022 13:50:12 - INFO - __main__ - Step 50 Global step 50 Train loss 0.27 on epoch=24
05/31/2022 13:50:13 - INFO - __main__ - Global step 50 Train loss 1.13 Classification-F1 0.3333333333333333 on epoch=24
05/31/2022 13:50:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
05/31/2022 13:50:18 - INFO - __main__ - Step 60 Global step 60 Train loss 0.30 on epoch=29
05/31/2022 13:50:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.23 on epoch=34
05/31/2022 13:50:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.25 on epoch=39
05/31/2022 13:50:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.24 on epoch=44
05/31/2022 13:50:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.28 on epoch=49
05/31/2022 13:50:37 - INFO - __main__ - Global step 100 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=49
05/31/2022 13:50:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.21 on epoch=54
05/31/2022 13:50:46 - INFO - __main__ - Step 120 Global step 120 Train loss 0.25 on epoch=59
05/31/2022 13:50:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.21 on epoch=64
05/31/2022 13:50:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.19 on epoch=69
05/31/2022 13:51:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.22 on epoch=74
05/31/2022 13:51:01 - INFO - __main__ - Global step 150 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=74
05/31/2022 13:51:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.16 on epoch=79
05/31/2022 13:51:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.19 on epoch=84
05/31/2022 13:51:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.16 on epoch=89
05/31/2022 13:51:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.15 on epoch=94
05/31/2022 13:51:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.16 on epoch=99
05/31/2022 13:51:25 - INFO - __main__ - Global step 200 Train loss 0.16 Classification-F1 0.4589371980676329 on epoch=99
05/31/2022 13:51:25 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4589371980676329 on epoch=99, global_step=200
05/31/2022 13:51:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.16 on epoch=104
05/31/2022 13:51:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.13 on epoch=109
05/31/2022 13:51:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.12 on epoch=114
05/31/2022 13:51:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.13 on epoch=119
05/31/2022 13:51:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.10 on epoch=124
05/31/2022 13:51:49 - INFO - __main__ - Global step 250 Train loss 0.13 Classification-F1 0.30229120473022913 on epoch=124
05/31/2022 13:51:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.12 on epoch=129
05/31/2022 13:51:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.10 on epoch=134
05/31/2022 13:52:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.10 on epoch=139
05/31/2022 13:52:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.06 on epoch=144
05/31/2022 13:52:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.10 on epoch=149
05/31/2022 13:52:13 - INFO - __main__ - Global step 300 Train loss 0.09 Classification-F1 0.2698412698412698 on epoch=149
05/31/2022 13:52:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.07 on epoch=154
05/31/2022 13:52:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.06 on epoch=159
05/31/2022 13:52:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.06 on epoch=164
05/31/2022 13:52:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.09 on epoch=169
05/31/2022 13:52:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.04 on epoch=174
05/31/2022 13:52:36 - INFO - __main__ - Global step 350 Train loss 0.06 Classification-F1 0.2827442827442827 on epoch=174
05/31/2022 13:52:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.04 on epoch=179
05/31/2022 13:52:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.05 on epoch=184
05/31/2022 13:52:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.04 on epoch=189
05/31/2022 13:52:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.05 on epoch=194
05/31/2022 13:52:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.03 on epoch=199
05/31/2022 13:53:00 - INFO - __main__ - Global step 400 Train loss 0.04 Classification-F1 0.3373737373737374 on epoch=199
05/31/2022 13:53:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.04 on epoch=204
05/31/2022 13:53:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.02 on epoch=209
05/31/2022 13:53:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.03 on epoch=214
05/31/2022 13:53:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.03 on epoch=219
05/31/2022 13:53:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.02 on epoch=224
05/31/2022 13:53:24 - INFO - __main__ - Global step 450 Train loss 0.03 Classification-F1 0.3810483870967742 on epoch=224
05/31/2022 13:53:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.03 on epoch=229
05/31/2022 13:53:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.03 on epoch=234
05/31/2022 13:53:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.01 on epoch=239
05/31/2022 13:53:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.01 on epoch=244
05/31/2022 13:53:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.01 on epoch=249
05/31/2022 13:53:47 - INFO - __main__ - Global step 500 Train loss 0.01 Classification-F1 0.2904761904761905 on epoch=249
05/31/2022 13:53:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.01 on epoch=254
05/31/2022 13:53:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.01 on epoch=259
05/31/2022 13:54:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.01 on epoch=264
05/31/2022 13:54:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.04 on epoch=269
05/31/2022 13:54:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
05/31/2022 13:54:11 - INFO - __main__ - Global step 550 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=274
05/31/2022 13:54:11 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.464039408866995 on epoch=274, global_step=550
05/31/2022 13:54:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.03 on epoch=279
05/31/2022 13:54:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
05/31/2022 13:54:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.02 on epoch=289
05/31/2022 13:54:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
05/31/2022 13:54:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
05/31/2022 13:54:35 - INFO - __main__ - Global step 600 Train loss 0.01 Classification-F1 0.1683982683982684 on epoch=299
05/31/2022 13:54:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
05/31/2022 13:54:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
05/31/2022 13:54:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
05/31/2022 13:54:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
05/31/2022 13:54:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
05/31/2022 13:54:58 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 0.3430555555555556 on epoch=324
05/31/2022 13:55:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
05/31/2022 13:55:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
05/31/2022 13:55:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
05/31/2022 13:55:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
05/31/2022 13:55:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
05/31/2022 13:55:22 - INFO - __main__ - Global step 700 Train loss 0.01 Classification-F1 0.3387096774193548 on epoch=349
05/31/2022 13:55:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
05/31/2022 13:55:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
05/31/2022 13:55:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
05/31/2022 13:55:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
05/31/2022 13:55:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
05/31/2022 13:55:46 - INFO - __main__ - Global step 750 Train loss 0.00 Classification-F1 0.18322580645161293 on epoch=374
05/31/2022 13:55:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
05/31/2022 13:55:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
05/31/2022 13:55:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
05/31/2022 13:56:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
05/31/2022 13:56:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
05/31/2022 13:56:09 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 0.2563025210084034 on epoch=399
05/31/2022 13:56:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
05/31/2022 13:56:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
05/31/2022 13:56:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
05/31/2022 13:56:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
05/31/2022 13:56:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
05/31/2022 13:56:33 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.1967741935483871 on epoch=424
05/31/2022 13:56:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
05/31/2022 13:56:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
05/31/2022 13:56:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
05/31/2022 13:56:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
05/31/2022 13:56:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
05/31/2022 13:56:57 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.2142857142857143 on epoch=449
05/31/2022 13:57:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
05/31/2022 13:57:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
05/31/2022 13:57:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
05/31/2022 13:57:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
05/31/2022 13:57:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
05/31/2022 13:57:21 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.5 on epoch=474
05/31/2022 13:57:21 - INFO - __main__ - Saving model with best Classification-F1: 0.464039408866995 -> 0.5 on epoch=474, global_step=950
05/31/2022 13:57:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
05/31/2022 13:57:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
05/31/2022 13:57:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
05/31/2022 13:57:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
05/31/2022 13:57:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
05/31/2022 13:57:44 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.5 on epoch=499
05/31/2022 13:57:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
05/31/2022 13:57:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
05/31/2022 13:57:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
05/31/2022 13:58:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
05/31/2022 13:58:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
05/31/2022 13:58:08 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.3144016227180528 on epoch=524
05/31/2022 13:58:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
05/31/2022 13:58:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
05/31/2022 13:58:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
05/31/2022 13:58:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
05/31/2022 13:58:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
05/31/2022 13:58:32 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.3595430107526882 on epoch=549
05/31/2022 13:58:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
05/31/2022 13:58:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
05/31/2022 13:58:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
05/31/2022 13:58:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
05/31/2022 13:58:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
05/31/2022 13:58:55 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.3595430107526882 on epoch=574
05/31/2022 13:59:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
05/31/2022 13:59:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
05/31/2022 13:59:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
05/31/2022 13:59:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
05/31/2022 13:59:18 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
05/31/2022 13:59:19 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.35959595959595964 on epoch=599
05/31/2022 13:59:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
05/31/2022 13:59:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
05/31/2022 13:59:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
05/31/2022 13:59:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
05/31/2022 13:59:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
05/31/2022 13:59:43 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.3595430107526882 on epoch=624
05/31/2022 13:59:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
05/31/2022 13:59:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
05/31/2022 13:59:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
05/31/2022 14:00:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
05/31/2022 14:00:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
05/31/2022 14:00:07 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.4980392156862745 on epoch=649
05/31/2022 14:00:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
05/31/2022 14:00:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
05/31/2022 14:00:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
05/31/2022 14:00:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
05/31/2022 14:00:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
05/31/2022 14:00:30 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.3595430107526882 on epoch=674
05/31/2022 14:00:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
05/31/2022 14:00:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
05/31/2022 14:00:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
05/31/2022 14:00:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
05/31/2022 14:00:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
05/31/2022 14:00:54 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=699
05/31/2022 14:00:54 - INFO - __main__ - Saving model with best Classification-F1: 0.5 -> 0.5307917888563051 on epoch=699, global_step=1400
05/31/2022 14:00:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
05/31/2022 14:01:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
05/31/2022 14:01:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
05/31/2022 14:01:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
05/31/2022 14:01:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
05/31/2022 14:01:18 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=724
05/31/2022 14:01:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
05/31/2022 14:01:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
05/31/2022 14:01:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
05/31/2022 14:01:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
05/31/2022 14:01:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
05/31/2022 14:01:41 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.3387096774193548 on epoch=749
05/31/2022 14:01:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
05/31/2022 14:01:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
05/31/2022 14:01:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
05/31/2022 14:01:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
05/31/2022 14:02:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
05/31/2022 14:02:05 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.5 on epoch=774
05/31/2022 14:02:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
05/31/2022 14:02:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
05/31/2022 14:02:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
05/31/2022 14:02:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
05/31/2022 14:02:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
05/31/2022 14:02:29 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.5 on epoch=799
05/31/2022 14:02:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
05/31/2022 14:02:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
05/31/2022 14:02:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
05/31/2022 14:02:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
05/31/2022 14:02:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
05/31/2022 14:02:52 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.5625 on epoch=824
05/31/2022 14:02:52 - INFO - __main__ - Saving model with best Classification-F1: 0.5307917888563051 -> 0.5625 on epoch=824, global_step=1650
05/31/2022 14:02:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
05/31/2022 14:03:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
05/31/2022 14:03:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
05/31/2022 14:03:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
05/31/2022 14:03:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
05/31/2022 14:03:16 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=849
05/31/2022 14:03:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
05/31/2022 14:03:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
05/31/2022 14:03:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
05/31/2022 14:03:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
05/31/2022 14:03:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
05/31/2022 14:03:40 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.3144016227180528 on epoch=874
05/31/2022 14:03:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
05/31/2022 14:03:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
05/31/2022 14:03:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
05/31/2022 14:03:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
05/31/2022 14:04:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
05/31/2022 14:04:04 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.3373737373737374 on epoch=899
05/31/2022 14:04:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
05/31/2022 14:04:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
05/31/2022 14:04:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
05/31/2022 14:04:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
05/31/2022 14:04:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
05/31/2022 14:04:28 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.3144016227180528 on epoch=924
05/31/2022 14:04:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
05/31/2022 14:04:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
05/31/2022 14:04:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
05/31/2022 14:04:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
05/31/2022 14:04:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
05/31/2022 14:04:51 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5625 on epoch=949
05/31/2022 14:04:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
05/31/2022 14:05:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
05/31/2022 14:05:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
05/31/2022 14:05:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
05/31/2022 14:05:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
05/31/2022 14:05:15 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=974
05/31/2022 14:05:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
05/31/2022 14:05:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
05/31/2022 14:05:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
05/31/2022 14:05:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
05/31/2022 14:05:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
05/31/2022 14:05:39 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5625 on epoch=999
05/31/2022 14:05:39 - INFO - __main__ - save last model!
05/31/2022 14:05:39 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 14:05:39 - INFO - __main__ - Printing 3 examples
05/31/2022 14:05:39 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/31/2022 14:05:39 - INFO - __main__ - ['refuted']
05/31/2022 14:05:39 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/31/2022 14:05:39 - INFO - __main__ - ['refuted']
05/31/2022 14:05:39 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/31/2022 14:05:39 - INFO - __main__ - ['refuted']
05/31/2022 14:05:39 - INFO - __main__ - Tokenizing Input ...
05/31/2022 14:05:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 14:05:39 - INFO - __main__ - Tokenizing Output ...
05/31/2022 14:05:39 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 14:05:39 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 14:05:39 - INFO - __main__ - Printing 3 examples
05/31/2022 14:05:39 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/31/2022 14:05:39 - INFO - __main__ - ['refuted']
05/31/2022 14:05:39 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/31/2022 14:05:39 - INFO - __main__ - ['refuted']
05/31/2022 14:05:39 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/31/2022 14:05:39 - INFO - __main__ - ['refuted']
05/31/2022 14:05:39 - INFO - __main__ - Tokenizing Input ...
05/31/2022 14:05:39 - INFO - __main__ - Start tokenizing ... 12792 instances
05/31/2022 14:05:39 - INFO - __main__ - Printing 3 examples
05/31/2022 14:05:39 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 14:05:39 - INFO - __main__ - ['entailed']
05/31/2022 14:05:39 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 14:05:39 - INFO - __main__ - ['entailed']
05/31/2022 14:05:39 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 14:05:39 - INFO - __main__ - ['entailed']
05/31/2022 14:05:39 - INFO - __main__ - Tokenizing Input ...
05/31/2022 14:05:39 - INFO - __main__ - Tokenizing Output ...
05/31/2022 14:05:39 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 14:05:55 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 14:05:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 14:05:55 - INFO - __main__ - Starting training!
05/31/2022 14:06:03 - INFO - __main__ - Tokenizing Output ...
05/31/2022 14:06:16 - INFO - __main__ - Loaded 12792 examples from test data
05/31/2022 14:14:28 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/singletask-tab_fact/tab_fact_16_13_0.4_8_predictions.txt
05/31/2022 14:14:28 - INFO - __main__ - Classification-F1 on test data: 0.0229
05/31/2022 14:14:29 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.4, bsz=8, dev_performance=0.5625, test_performance=0.02290405692191192
05/31/2022 14:14:29 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.3, bsz=8 ...
05/31/2022 14:14:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 14:14:30 - INFO - __main__ - Printing 3 examples
05/31/2022 14:14:30 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/31/2022 14:14:30 - INFO - __main__ - ['refuted']
05/31/2022 14:14:30 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/31/2022 14:14:30 - INFO - __main__ - ['refuted']
05/31/2022 14:14:30 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/31/2022 14:14:30 - INFO - __main__ - ['refuted']
05/31/2022 14:14:30 - INFO - __main__ - Tokenizing Input ...
05/31/2022 14:14:30 - INFO - __main__ - Tokenizing Output ...
05/31/2022 14:14:30 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 14:14:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 14:14:30 - INFO - __main__ - Printing 3 examples
05/31/2022 14:14:30 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/31/2022 14:14:30 - INFO - __main__ - ['refuted']
05/31/2022 14:14:30 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/31/2022 14:14:30 - INFO - __main__ - ['refuted']
05/31/2022 14:14:30 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/31/2022 14:14:30 - INFO - __main__ - ['refuted']
05/31/2022 14:14:30 - INFO - __main__ - Tokenizing Input ...
05/31/2022 14:14:30 - INFO - __main__ - Tokenizing Output ...
05/31/2022 14:14:30 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 14:14:45 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 14:14:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 14:14:46 - INFO - __main__ - Starting training!
05/31/2022 14:14:51 - INFO - __main__ - Step 10 Global step 10 Train loss 3.84 on epoch=4
05/31/2022 14:14:56 - INFO - __main__ - Step 20 Global step 20 Train loss 1.48 on epoch=9
05/31/2022 14:15:00 - INFO - __main__ - Step 30 Global step 30 Train loss 0.62 on epoch=14
05/31/2022 14:15:05 - INFO - __main__ - Step 40 Global step 40 Train loss 0.36 on epoch=19
05/31/2022 14:15:09 - INFO - __main__ - Step 50 Global step 50 Train loss 0.33 on epoch=24
05/31/2022 14:15:10 - INFO - __main__ - Global step 50 Train loss 1.33 Classification-F1 0.3333333333333333 on epoch=24
05/31/2022 14:15:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
05/31/2022 14:15:15 - INFO - __main__ - Step 60 Global step 60 Train loss 0.26 on epoch=29
05/31/2022 14:15:19 - INFO - __main__ - Step 70 Global step 70 Train loss 0.23 on epoch=34
05/31/2022 14:15:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.24 on epoch=39
05/31/2022 14:15:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.25 on epoch=44
05/31/2022 14:15:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.21 on epoch=49
05/31/2022 14:15:34 - INFO - __main__ - Global step 100 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=49
05/31/2022 14:15:39 - INFO - __main__ - Step 110 Global step 110 Train loss 0.25 on epoch=54
05/31/2022 14:15:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.23 on epoch=59
05/31/2022 14:15:48 - INFO - __main__ - Step 130 Global step 130 Train loss 0.18 on epoch=64
05/31/2022 14:15:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.21 on epoch=69
05/31/2022 14:15:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.19 on epoch=74
05/31/2022 14:15:58 - INFO - __main__ - Global step 150 Train loss 0.21 Classification-F1 0.4181818181818182 on epoch=74
05/31/2022 14:15:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4181818181818182 on epoch=74, global_step=150
05/31/2022 14:16:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.17 on epoch=79
05/31/2022 14:16:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.19 on epoch=84
05/31/2022 14:16:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.11 on epoch=89
05/31/2022 14:16:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.12 on epoch=94
05/31/2022 14:16:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.16 on epoch=99
05/31/2022 14:16:21 - INFO - __main__ - Global step 200 Train loss 0.15 Classification-F1 0.4420512820512821 on epoch=99
05/31/2022 14:16:22 - INFO - __main__ - Saving model with best Classification-F1: 0.4181818181818182 -> 0.4420512820512821 on epoch=99, global_step=200
05/31/2022 14:16:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.13 on epoch=104
05/31/2022 14:16:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.11 on epoch=109
05/31/2022 14:16:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.12 on epoch=114
05/31/2022 14:16:39 - INFO - __main__ - Step 240 Global step 240 Train loss 0.10 on epoch=119
05/31/2022 14:16:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.11 on epoch=124
05/31/2022 14:16:45 - INFO - __main__ - Global step 250 Train loss 0.12 Classification-F1 0.4420512820512821 on epoch=124
05/31/2022 14:16:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.08 on epoch=129
05/31/2022 14:16:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.09 on epoch=134
05/31/2022 14:16:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.05 on epoch=139
05/31/2022 14:17:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.06 on epoch=144
05/31/2022 14:17:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.07 on epoch=149
05/31/2022 14:17:09 - INFO - __main__ - Global step 300 Train loss 0.07 Classification-F1 0.3764102564102564 on epoch=149
05/31/2022 14:17:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.07 on epoch=154
05/31/2022 14:17:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.04 on epoch=159
05/31/2022 14:17:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.06 on epoch=164
05/31/2022 14:17:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.03 on epoch=169
05/31/2022 14:17:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.04 on epoch=174
05/31/2022 14:17:33 - INFO - __main__ - Global step 350 Train loss 0.05 Classification-F1 0.37662337662337664 on epoch=174
05/31/2022 14:17:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.02 on epoch=179
05/31/2022 14:17:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.04 on epoch=184
05/31/2022 14:17:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.04 on epoch=189
05/31/2022 14:17:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.07 on epoch=194
05/31/2022 14:17:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.02 on epoch=199
05/31/2022 14:17:56 - INFO - __main__ - Global step 400 Train loss 0.04 Classification-F1 0.464039408866995 on epoch=199
05/31/2022 14:17:56 - INFO - __main__ - Saving model with best Classification-F1: 0.4420512820512821 -> 0.464039408866995 on epoch=199, global_step=400
05/31/2022 14:18:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.04 on epoch=204
05/31/2022 14:18:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.03 on epoch=209
05/31/2022 14:18:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.02 on epoch=214
05/31/2022 14:18:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.02 on epoch=219
05/31/2022 14:18:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.01 on epoch=224
05/31/2022 14:18:20 - INFO - __main__ - Global step 450 Train loss 0.02 Classification-F1 0.5307917888563051 on epoch=224
05/31/2022 14:18:20 - INFO - __main__ - Saving model with best Classification-F1: 0.464039408866995 -> 0.5307917888563051 on epoch=224, global_step=450
05/31/2022 14:18:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.02 on epoch=229
05/31/2022 14:18:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.04 on epoch=234
05/31/2022 14:18:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.02 on epoch=239
05/31/2022 14:18:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.01 on epoch=244
05/31/2022 14:18:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.01 on epoch=249
05/31/2022 14:18:44 - INFO - __main__ - Global step 500 Train loss 0.02 Classification-F1 0.4980392156862745 on epoch=249
05/31/2022 14:18:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.01 on epoch=254
05/31/2022 14:18:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.01 on epoch=259
05/31/2022 14:18:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.02 on epoch=264
05/31/2022 14:19:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.01 on epoch=269
05/31/2022 14:19:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.01 on epoch=274
05/31/2022 14:19:07 - INFO - __main__ - Global step 550 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=274
05/31/2022 14:19:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.01 on epoch=279
05/31/2022 14:19:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.04 on epoch=284
05/31/2022 14:19:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.03 on epoch=289
05/31/2022 14:19:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
05/31/2022 14:19:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.02 on epoch=299
05/31/2022 14:19:31 - INFO - __main__ - Global step 600 Train loss 0.02 Classification-F1 0.43529411764705883 on epoch=299
05/31/2022 14:19:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
05/31/2022 14:19:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
05/31/2022 14:19:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
05/31/2022 14:19:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
05/31/2022 14:19:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
05/31/2022 14:19:55 - INFO - __main__ - Global step 650 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=324
05/31/2022 14:19:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
05/31/2022 14:20:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
05/31/2022 14:20:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
05/31/2022 14:20:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
05/31/2022 14:20:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
05/31/2022 14:20:18 - INFO - __main__ - Global step 700 Train loss 0.01 Classification-F1 0.2963709677419355 on epoch=349
05/31/2022 14:20:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
05/31/2022 14:20:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
05/31/2022 14:20:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
05/31/2022 14:20:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
05/31/2022 14:20:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
05/31/2022 14:20:42 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=374
05/31/2022 14:20:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
05/31/2022 14:20:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
05/31/2022 14:20:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
05/31/2022 14:21:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
05/31/2022 14:21:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
05/31/2022 14:21:06 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.2901234567901234 on epoch=399
05/31/2022 14:21:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
05/31/2022 14:21:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
05/31/2022 14:21:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
05/31/2022 14:21:24 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
05/31/2022 14:21:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
05/31/2022 14:21:30 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.30864197530864196 on epoch=424
05/31/2022 14:21:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
05/31/2022 14:21:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
05/31/2022 14:21:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
05/31/2022 14:21:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
05/31/2022 14:21:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
05/31/2022 14:21:53 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.3144016227180528 on epoch=449
05/31/2022 14:21:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
05/31/2022 14:22:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
05/31/2022 14:22:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
05/31/2022 14:22:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
05/31/2022 14:22:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
05/31/2022 14:22:17 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=474
05/31/2022 14:22:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
05/31/2022 14:22:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
05/31/2022 14:22:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
05/31/2022 14:22:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
05/31/2022 14:22:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
05/31/2022 14:22:41 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=499
05/31/2022 14:22:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
05/31/2022 14:22:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
05/31/2022 14:22:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
05/31/2022 14:22:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
05/31/2022 14:23:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
05/31/2022 14:23:04 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.30864197530864196 on epoch=524
05/31/2022 14:23:09 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
05/31/2022 14:23:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
05/31/2022 14:23:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
05/31/2022 14:23:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
05/31/2022 14:23:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
05/31/2022 14:23:28 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=549
05/31/2022 14:23:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
05/31/2022 14:23:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
05/31/2022 14:23:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
05/31/2022 14:23:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
05/31/2022 14:23:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
05/31/2022 14:23:51 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=574
05/31/2022 14:23:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
05/31/2022 14:24:00 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
05/31/2022 14:24:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
05/31/2022 14:24:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
05/31/2022 14:24:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
05/31/2022 14:24:15 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=599
05/31/2022 14:24:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
05/31/2022 14:24:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
05/31/2022 14:24:28 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
05/31/2022 14:24:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
05/31/2022 14:24:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
05/31/2022 14:24:39 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=624
05/31/2022 14:24:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
05/31/2022 14:24:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
05/31/2022 14:24:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
05/31/2022 14:24:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
05/31/2022 14:25:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
05/31/2022 14:25:02 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=649
05/31/2022 14:25:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
05/31/2022 14:25:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=659
05/31/2022 14:25:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
05/31/2022 14:25:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
05/31/2022 14:25:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
05/31/2022 14:25:26 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=674
05/31/2022 14:25:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
05/31/2022 14:25:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
05/31/2022 14:25:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
05/31/2022 14:25:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
05/31/2022 14:25:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
05/31/2022 14:25:49 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=699
05/31/2022 14:25:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
05/31/2022 14:25:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
05/31/2022 14:26:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
05/31/2022 14:26:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
05/31/2022 14:26:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
05/31/2022 14:26:13 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.4666666666666667 on epoch=724
05/31/2022 14:26:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
05/31/2022 14:26:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
05/31/2022 14:26:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
05/31/2022 14:26:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
05/31/2022 14:26:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
05/31/2022 14:26:36 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=749
05/31/2022 14:26:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
05/31/2022 14:26:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
05/31/2022 14:26:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
05/31/2022 14:26:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
05/31/2022 14:26:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=774
05/31/2022 14:27:00 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=774
05/31/2022 14:27:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
05/31/2022 14:27:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
05/31/2022 14:27:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
05/31/2022 14:27:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
05/31/2022 14:27:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
05/31/2022 14:27:23 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.3333333333333333 on epoch=799
05/31/2022 14:27:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
05/31/2022 14:27:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
05/31/2022 14:27:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
05/31/2022 14:27:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
05/31/2022 14:27:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
05/31/2022 14:27:47 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=824
05/31/2022 14:27:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
05/31/2022 14:27:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
05/31/2022 14:28:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
05/31/2022 14:28:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=844
05/31/2022 14:28:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
05/31/2022 14:28:11 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.2901234567901234 on epoch=849
05/31/2022 14:28:15 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
05/31/2022 14:28:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
05/31/2022 14:28:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
05/31/2022 14:28:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
05/31/2022 14:28:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
05/31/2022 14:28:34 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=874
05/31/2022 14:28:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
05/31/2022 14:28:43 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
05/31/2022 14:28:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
05/31/2022 14:28:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
05/31/2022 14:28:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
05/31/2022 14:28:58 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=899
05/31/2022 14:29:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
05/31/2022 14:29:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
05/31/2022 14:29:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
05/31/2022 14:29:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
05/31/2022 14:29:20 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
05/31/2022 14:29:22 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=924
05/31/2022 14:29:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
05/31/2022 14:29:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
05/31/2022 14:29:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
05/31/2022 14:29:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
05/31/2022 14:29:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=949
05/31/2022 14:29:45 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=949
05/31/2022 14:29:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
05/31/2022 14:29:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
05/31/2022 14:29:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
05/31/2022 14:30:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
05/31/2022 14:30:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
05/31/2022 14:30:09 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=974
05/31/2022 14:30:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
05/31/2022 14:30:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
05/31/2022 14:30:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
05/31/2022 14:30:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
05/31/2022 14:30:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
05/31/2022 14:30:32 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=999
05/31/2022 14:30:32 - INFO - __main__ - save last model!
05/31/2022 14:30:32 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 14:30:32 - INFO - __main__ - Printing 3 examples
05/31/2022 14:30:32 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/31/2022 14:30:32 - INFO - __main__ - ['refuted']
05/31/2022 14:30:32 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/31/2022 14:30:32 - INFO - __main__ - ['refuted']
05/31/2022 14:30:32 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/31/2022 14:30:32 - INFO - __main__ - ['refuted']
05/31/2022 14:30:32 - INFO - __main__ - Tokenizing Input ...
05/31/2022 14:30:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 14:30:32 - INFO - __main__ - Tokenizing Output ...
05/31/2022 14:30:32 - INFO - __main__ - Start tokenizing ... 12792 instances
05/31/2022 14:30:32 - INFO - __main__ - Printing 3 examples
05/31/2022 14:30:32 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 14:30:32 - INFO - __main__ - ['entailed']
05/31/2022 14:30:32 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 14:30:32 - INFO - __main__ - ['entailed']
05/31/2022 14:30:32 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 14:30:32 - INFO - __main__ - ['entailed']
05/31/2022 14:30:32 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 14:30:32 - INFO - __main__ - Tokenizing Input ...
05/31/2022 14:30:32 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 14:30:32 - INFO - __main__ - Printing 3 examples
05/31/2022 14:30:32 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/31/2022 14:30:32 - INFO - __main__ - ['refuted']
05/31/2022 14:30:32 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/31/2022 14:30:32 - INFO - __main__ - ['refuted']
05/31/2022 14:30:32 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/31/2022 14:30:32 - INFO - __main__ - ['refuted']
05/31/2022 14:30:32 - INFO - __main__ - Tokenizing Input ...
05/31/2022 14:30:32 - INFO - __main__ - Tokenizing Output ...
05/31/2022 14:30:32 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 14:30:48 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 14:30:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 14:30:49 - INFO - __main__ - Starting training!
05/31/2022 14:30:57 - INFO - __main__ - Tokenizing Output ...
05/31/2022 14:31:10 - INFO - __main__ - Loaded 12792 examples from test data
05/31/2022 14:39:21 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/singletask-tab_fact/tab_fact_16_13_0.3_8_predictions.txt
05/31/2022 14:39:21 - INFO - __main__ - Classification-F1 on test data: 0.2441
05/31/2022 14:39:21 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.3, bsz=8, dev_performance=0.5307917888563051, test_performance=0.24413422319587885
05/31/2022 14:39:21 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.2, bsz=8 ...
05/31/2022 14:39:22 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 14:39:22 - INFO - __main__ - Printing 3 examples
05/31/2022 14:39:22 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/31/2022 14:39:22 - INFO - __main__ - ['refuted']
05/31/2022 14:39:22 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/31/2022 14:39:22 - INFO - __main__ - ['refuted']
05/31/2022 14:39:22 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/31/2022 14:39:22 - INFO - __main__ - ['refuted']
05/31/2022 14:39:22 - INFO - __main__ - Tokenizing Input ...
05/31/2022 14:39:22 - INFO - __main__ - Tokenizing Output ...
05/31/2022 14:39:22 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 14:39:22 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 14:39:22 - INFO - __main__ - Printing 3 examples
05/31/2022 14:39:22 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlaváčková matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rüütel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana cîrstea#danica krstajić olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana cîrstea mădălina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovská andrea hlaváčková#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la coruña#hard#marina erakovic#andrea hlaváčková justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandžić magdaléna rybáriková#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdošová#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tímea babos#jarmila gajdošová jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugić - salkić#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinová#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea benítez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
05/31/2022 14:39:22 - INFO - __main__ - ['refuted']
05/31/2022 14:39:22 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
05/31/2022 14:39:22 - INFO - __main__ - ['refuted']
05/31/2022 14:39:22 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
05/31/2022 14:39:22 - INFO - __main__ - ['refuted']
05/31/2022 14:39:22 - INFO - __main__ - Tokenizing Input ...
05/31/2022 14:39:22 - INFO - __main__ - Tokenizing Output ...
05/31/2022 14:39:22 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 14:39:41 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 14:39:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 14:39:42 - INFO - __main__ - Starting training!
05/31/2022 14:39:47 - INFO - __main__ - Step 10 Global step 10 Train loss 3.92 on epoch=4
05/31/2022 14:39:52 - INFO - __main__ - Step 20 Global step 20 Train loss 2.23 on epoch=9
05/31/2022 14:39:56 - INFO - __main__ - Step 30 Global step 30 Train loss 1.30 on epoch=14
05/31/2022 14:40:00 - INFO - __main__ - Step 40 Global step 40 Train loss 0.63 on epoch=19
05/31/2022 14:40:05 - INFO - __main__ - Step 50 Global step 50 Train loss 0.43 on epoch=24
05/31/2022 14:40:06 - INFO - __main__ - Global step 50 Train loss 1.70 Classification-F1 0.3333333333333333 on epoch=24
05/31/2022 14:40:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
05/31/2022 14:40:11 - INFO - __main__ - Step 60 Global step 60 Train loss 0.34 on epoch=29
05/31/2022 14:40:15 - INFO - __main__ - Step 70 Global step 70 Train loss 0.30 on epoch=34
05/31/2022 14:40:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.28 on epoch=39
05/31/2022 14:40:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.27 on epoch=44
05/31/2022 14:40:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.29 on epoch=49
05/31/2022 14:40:30 - INFO - __main__ - Global step 100 Train loss 0.30 Classification-F1 0.3992490613266583 on epoch=49
05/31/2022 14:40:30 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=49, global_step=100
05/31/2022 14:40:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.24 on epoch=54
05/31/2022 14:40:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.23 on epoch=59
05/31/2022 14:40:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.21 on epoch=64
05/31/2022 14:40:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.20 on epoch=69
05/31/2022 14:40:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.24 on epoch=74
05/31/2022 14:40:54 - INFO - __main__ - Global step 150 Train loss 0.23 Classification-F1 0.39139139139139134 on epoch=74
05/31/2022 14:40:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.16 on epoch=79
05/31/2022 14:41:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.19 on epoch=84
05/31/2022 14:41:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.16 on epoch=89
05/31/2022 14:41:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.15 on epoch=94
05/31/2022 14:41:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.15 on epoch=99
05/31/2022 14:41:17 - INFO - __main__ - Global step 200 Train loss 0.16 Classification-F1 0.41700404858299595 on epoch=99
05/31/2022 14:41:17 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.41700404858299595 on epoch=99, global_step=200
05/31/2022 14:41:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.14 on epoch=104
05/31/2022 14:41:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.18 on epoch=109
05/31/2022 14:41:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.20 on epoch=114
05/31/2022 14:41:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.14 on epoch=119
05/31/2022 14:41:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.12 on epoch=124
05/31/2022 14:41:41 - INFO - __main__ - Global step 250 Train loss 0.16 Classification-F1 0.4231177094379639 on epoch=124
05/31/2022 14:41:41 - INFO - __main__ - Saving model with best Classification-F1: 0.41700404858299595 -> 0.4231177094379639 on epoch=124, global_step=250
05/31/2022 14:41:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.15 on epoch=129
05/31/2022 14:41:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.10 on epoch=134
05/31/2022 14:41:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.11 on epoch=139
05/31/2022 14:41:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.08 on epoch=144
05/31/2022 14:42:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.09 on epoch=149
05/31/2022 14:42:05 - INFO - __main__ - Global step 300 Train loss 0.11 Classification-F1 0.41700404858299595 on epoch=149
05/31/2022 14:42:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.10 on epoch=154
05/31/2022 14:42:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.09 on epoch=159
05/31/2022 14:42:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.09 on epoch=164
05/31/2022 14:42:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.11 on epoch=169
05/31/2022 14:42:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.09 on epoch=174
05/31/2022 14:42:29 - INFO - __main__ - Global step 350 Train loss 0.10 Classification-F1 0.5270935960591133 on epoch=174
05/31/2022 14:42:29 - INFO - __main__ - Saving model with best Classification-F1: 0.4231177094379639 -> 0.5270935960591133 on epoch=174, global_step=350
05/31/2022 14:42:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.08 on epoch=179
05/31/2022 14:42:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.06 on epoch=184
05/31/2022 14:42:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.07 on epoch=189
05/31/2022 14:42:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.05 on epoch=194
05/31/2022 14:42:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.05 on epoch=199
05/31/2022 14:42:52 - INFO - __main__ - Global step 400 Train loss 0.06 Classification-F1 0.5270935960591133 on epoch=199
05/31/2022 14:42:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.05 on epoch=204
05/31/2022 14:43:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.05 on epoch=209
05/31/2022 14:43:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.06 on epoch=214
05/31/2022 14:43:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.06 on epoch=219
05/31/2022 14:43:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.05 on epoch=224
05/31/2022 14:43:16 - INFO - __main__ - Global step 450 Train loss 0.06 Classification-F1 0.5270935960591133 on epoch=224
05/31/2022 14:43:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.05 on epoch=229
05/31/2022 14:43:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.04 on epoch=234
05/31/2022 14:43:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.04 on epoch=239
05/31/2022 14:43:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.02 on epoch=244
05/31/2022 14:43:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.02 on epoch=249
05/31/2022 14:43:40 - INFO - __main__ - Global step 500 Train loss 0.04 Classification-F1 0.35699797160243407 on epoch=249
05/31/2022 14:43:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.02 on epoch=254
05/31/2022 14:43:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.03 on epoch=259
05/31/2022 14:43:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.03 on epoch=264
05/31/2022 14:43:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.01 on epoch=269
05/31/2022 14:44:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.04 on epoch=274
05/31/2022 14:44:04 - INFO - __main__ - Global step 550 Train loss 0.03 Classification-F1 0.4682306940371457 on epoch=274
05/31/2022 14:44:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.04 on epoch=279
05/31/2022 14:44:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.04 on epoch=284
05/31/2022 14:44:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
05/31/2022 14:44:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
05/31/2022 14:44:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.02 on epoch=299
05/31/2022 14:44:27 - INFO - __main__ - Global step 600 Train loss 0.03 Classification-F1 0.5933528836754642 on epoch=299
05/31/2022 14:44:27 - INFO - __main__ - Saving model with best Classification-F1: 0.5270935960591133 -> 0.5933528836754642 on epoch=299, global_step=600
05/31/2022 14:44:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.05 on epoch=304
05/31/2022 14:44:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.03 on epoch=309
05/31/2022 14:44:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
05/31/2022 14:44:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
05/31/2022 14:44:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
05/31/2022 14:44:51 - INFO - __main__ - Global step 650 Train loss 0.03 Classification-F1 0.5901477832512315 on epoch=324
05/31/2022 14:44:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
05/31/2022 14:45:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=334
05/31/2022 14:45:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
05/31/2022 14:45:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
05/31/2022 14:45:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
05/31/2022 14:45:15 - INFO - __main__ - Global step 700 Train loss 0.02 Classification-F1 0.4920634920634921 on epoch=349
05/31/2022 14:45:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
05/31/2022 14:45:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
05/31/2022 14:45:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
05/31/2022 14:45:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
05/31/2022 14:45:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
05/31/2022 14:45:38 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=374
05/31/2022 14:45:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
05/31/2022 14:45:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
05/31/2022 14:45:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
05/31/2022 14:45:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
05/31/2022 14:46:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
05/31/2022 14:46:02 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=399
05/31/2022 14:46:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
05/31/2022 14:46:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
05/31/2022 14:46:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
05/31/2022 14:46:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
05/31/2022 14:46:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
05/31/2022 14:46:26 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=424
05/31/2022 14:46:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
05/31/2022 14:46:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
05/31/2022 14:46:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
05/31/2022 14:46:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
05/31/2022 14:46:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
05/31/2022 14:46:49 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=449
05/31/2022 14:46:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
05/31/2022 14:46:58 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
05/31/2022 14:47:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
05/31/2022 14:47:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
05/31/2022 14:47:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
05/31/2022 14:47:13 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.22499999999999998 on epoch=474
05/31/2022 14:47:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
05/31/2022 14:47:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
05/31/2022 14:47:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
05/31/2022 14:47:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
05/31/2022 14:47:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
05/31/2022 14:47:37 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.3373737373737374 on epoch=499
05/31/2022 14:47:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
05/31/2022 14:47:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
05/31/2022 14:47:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
05/31/2022 14:47:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
05/31/2022 14:47:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
05/31/2022 14:48:00 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.5625 on epoch=524
05/31/2022 14:48:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
05/31/2022 14:48:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
05/31/2022 14:48:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
05/31/2022 14:48:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
05/31/2022 14:48:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
05/31/2022 14:48:24 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.5933528836754642 on epoch=549
05/31/2022 14:48:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
05/31/2022 14:48:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
05/31/2022 14:48:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
05/31/2022 14:48:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
05/31/2022 14:48:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
05/31/2022 14:48:47 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.4980392156862745 on epoch=574
05/31/2022 14:48:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
05/31/2022 14:48:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
05/31/2022 14:49:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
05/31/2022 14:49:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
05/31/2022 14:49:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
05/31/2022 14:49:11 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.35714285714285715 on epoch=599
05/31/2022 14:49:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
05/31/2022 14:49:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
05/31/2022 14:49:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
05/31/2022 14:49:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
05/31/2022 14:49:33 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
05/31/2022 14:49:35 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=624
05/31/2022 14:49:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
05/31/2022 14:49:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
05/31/2022 14:49:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
05/31/2022 14:49:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
05/31/2022 14:49:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
05/31/2022 14:49:58 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.3142857142857143 on epoch=649
05/31/2022 14:50:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
05/31/2022 14:50:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
05/31/2022 14:50:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
05/31/2022 14:50:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
05/31/2022 14:50:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
05/31/2022 14:50:22 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=674
05/31/2022 14:50:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
05/31/2022 14:50:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
05/31/2022 14:50:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
05/31/2022 14:50:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=694
05/31/2022 14:50:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
05/31/2022 14:50:46 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.24193548387096775 on epoch=699
05/31/2022 14:50:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
05/31/2022 14:50:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
05/31/2022 14:50:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
05/31/2022 14:51:04 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
05/31/2022 14:51:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
05/31/2022 14:51:09 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.3218390804597701 on epoch=724
05/31/2022 14:51:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
05/31/2022 14:51:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
05/31/2022 14:51:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
05/31/2022 14:51:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
05/31/2022 14:51:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
05/31/2022 14:51:33 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.2901234567901234 on epoch=749
05/31/2022 14:51:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
05/31/2022 14:51:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
05/31/2022 14:51:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
05/31/2022 14:51:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
05/31/2022 14:51:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
05/31/2022 14:51:57 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.22645502645502646 on epoch=774
05/31/2022 14:52:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
05/31/2022 14:52:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
05/31/2022 14:52:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
05/31/2022 14:52:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
05/31/2022 14:52:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
05/31/2022 14:52:20 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.35959595959595964 on epoch=799
05/31/2022 14:52:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
05/31/2022 14:52:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
05/31/2022 14:52:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
05/31/2022 14:52:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
05/31/2022 14:52:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
05/31/2022 14:52:44 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.25476190476190474 on epoch=824
05/31/2022 14:52:49 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
05/31/2022 14:52:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
05/31/2022 14:52:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
05/31/2022 14:53:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
05/31/2022 14:53:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
05/31/2022 14:53:08 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.29109062980030725 on epoch=849
05/31/2022 14:53:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
05/31/2022 14:53:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
05/31/2022 14:53:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
05/31/2022 14:53:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
05/31/2022 14:53:30 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
05/31/2022 14:53:31 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.2514285714285714 on epoch=874
05/31/2022 14:53:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
05/31/2022 14:53:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
05/31/2022 14:53:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
05/31/2022 14:53:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
05/31/2022 14:53:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
05/31/2022 14:53:55 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.29109062980030725 on epoch=899
05/31/2022 14:53:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
05/31/2022 14:54:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
05/31/2022 14:54:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=914
05/31/2022 14:54:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
05/31/2022 14:54:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
05/31/2022 14:54:19 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.2514285714285714 on epoch=924
05/31/2022 14:54:23 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
05/31/2022 14:54:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
05/31/2022 14:54:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
05/31/2022 14:54:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
05/31/2022 14:54:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
05/31/2022 14:54:42 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.2790346907993967 on epoch=949
05/31/2022 14:54:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
05/31/2022 14:54:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
05/31/2022 14:54:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
05/31/2022 14:55:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
05/31/2022 14:55:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
05/31/2022 14:55:06 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.3484195402298851 on epoch=974
05/31/2022 14:55:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
05/31/2022 14:55:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
05/31/2022 14:55:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
05/31/2022 14:55:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
05/31/2022 14:55:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
05/31/2022 14:55:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 14:55:29 - INFO - __main__ - Printing 3 examples
05/31/2022 14:55:29 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/31/2022 14:55:29 - INFO - __main__ - ['entailed']
05/31/2022 14:55:29 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/31/2022 14:55:29 - INFO - __main__ - ['entailed']
05/31/2022 14:55:29 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/31/2022 14:55:29 - INFO - __main__ - ['entailed']
05/31/2022 14:55:29 - INFO - __main__ - Tokenizing Input ...
05/31/2022 14:55:29 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.349820788530466 on epoch=999
05/31/2022 14:55:29 - INFO - __main__ - save last model!
05/31/2022 14:55:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 14:55:29 - INFO - __main__ - Tokenizing Output ...
05/31/2022 14:55:29 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 14:55:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 14:55:29 - INFO - __main__ - Printing 3 examples
05/31/2022 14:55:29 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/31/2022 14:55:29 - INFO - __main__ - ['entailed']
05/31/2022 14:55:29 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/31/2022 14:55:29 - INFO - __main__ - ['entailed']
05/31/2022 14:55:29 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/31/2022 14:55:29 - INFO - __main__ - ['entailed']
05/31/2022 14:55:29 - INFO - __main__ - Tokenizing Input ...
05/31/2022 14:55:29 - INFO - __main__ - Start tokenizing ... 12792 instances
05/31/2022 14:55:29 - INFO - __main__ - Printing 3 examples
05/31/2022 14:55:29 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 14:55:29 - INFO - __main__ - ['entailed']
05/31/2022 14:55:29 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 14:55:29 - INFO - __main__ - ['entailed']
05/31/2022 14:55:29 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 14:55:29 - INFO - __main__ - ['entailed']
05/31/2022 14:55:29 - INFO - __main__ - Tokenizing Input ...
05/31/2022 14:55:30 - INFO - __main__ - Tokenizing Output ...
05/31/2022 14:55:30 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 14:55:48 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 14:55:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 14:55:49 - INFO - __main__ - Starting training!
05/31/2022 14:55:54 - INFO - __main__ - Tokenizing Output ...
05/31/2022 14:56:06 - INFO - __main__ - Loaded 12792 examples from test data
05/31/2022 15:04:17 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/singletask-tab_fact/tab_fact_16_13_0.2_8_predictions.txt
05/31/2022 15:04:17 - INFO - __main__ - Classification-F1 on test data: 0.1623
05/31/2022 15:04:18 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.2, bsz=8, dev_performance=0.5933528836754642, test_performance=0.16225750645845408
05/31/2022 15:04:18 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.5, bsz=8 ...
05/31/2022 15:04:19 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 15:04:19 - INFO - __main__ - Printing 3 examples
05/31/2022 15:04:19 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/31/2022 15:04:19 - INFO - __main__ - ['entailed']
05/31/2022 15:04:19 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/31/2022 15:04:19 - INFO - __main__ - ['entailed']
05/31/2022 15:04:19 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/31/2022 15:04:19 - INFO - __main__ - ['entailed']
05/31/2022 15:04:19 - INFO - __main__ - Tokenizing Input ...
05/31/2022 15:04:19 - INFO - __main__ - Tokenizing Output ...
05/31/2022 15:04:19 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 15:04:19 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 15:04:19 - INFO - __main__ - Printing 3 examples
05/31/2022 15:04:19 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/31/2022 15:04:19 - INFO - __main__ - ['entailed']
05/31/2022 15:04:19 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/31/2022 15:04:19 - INFO - __main__ - ['entailed']
05/31/2022 15:04:19 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/31/2022 15:04:19 - INFO - __main__ - ['entailed']
05/31/2022 15:04:19 - INFO - __main__ - Tokenizing Input ...
05/31/2022 15:04:19 - INFO - __main__ - Tokenizing Output ...
05/31/2022 15:04:19 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 15:04:37 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 15:04:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 15:04:38 - INFO - __main__ - Starting training!
05/31/2022 15:04:43 - INFO - __main__ - Step 10 Global step 10 Train loss 2.78 on epoch=4
05/31/2022 15:04:47 - INFO - __main__ - Step 20 Global step 20 Train loss 0.73 on epoch=9
05/31/2022 15:04:52 - INFO - __main__ - Step 30 Global step 30 Train loss 0.39 on epoch=14
05/31/2022 15:04:56 - INFO - __main__ - Step 40 Global step 40 Train loss 0.34 on epoch=19
05/31/2022 15:05:01 - INFO - __main__ - Step 50 Global step 50 Train loss 0.29 on epoch=24
05/31/2022 15:05:02 - INFO - __main__ - Global step 50 Train loss 0.91 Classification-F1 0.3333333333333333 on epoch=24
05/31/2022 15:05:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
05/31/2022 15:05:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.28 on epoch=29
05/31/2022 15:05:11 - INFO - __main__ - Step 70 Global step 70 Train loss 0.26 on epoch=34
05/31/2022 15:05:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.25 on epoch=39
05/31/2022 15:05:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.24 on epoch=44
05/31/2022 15:05:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.22 on epoch=49
05/31/2022 15:05:26 - INFO - __main__ - Global step 100 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=49
05/31/2022 15:05:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.20 on epoch=54
05/31/2022 15:05:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.25 on epoch=59
05/31/2022 15:05:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.22 on epoch=64
05/31/2022 15:05:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.17 on epoch=69
05/31/2022 15:05:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.20 on epoch=74
05/31/2022 15:05:50 - INFO - __main__ - Global step 150 Train loss 0.21 Classification-F1 0.4817813765182186 on epoch=74
05/31/2022 15:05:50 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4817813765182186 on epoch=74, global_step=150
05/31/2022 15:05:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.24 on epoch=79
05/31/2022 15:05:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.17 on epoch=84
05/31/2022 15:06:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.20 on epoch=89
05/31/2022 15:06:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.19 on epoch=94
05/31/2022 15:06:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.20 on epoch=99
05/31/2022 15:06:14 - INFO - __main__ - Global step 200 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=99
05/31/2022 15:06:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.20 on epoch=104
05/31/2022 15:06:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.18 on epoch=109
05/31/2022 15:06:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.17 on epoch=114
05/31/2022 15:06:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.19 on epoch=119
05/31/2022 15:06:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.18 on epoch=124
05/31/2022 15:06:38 - INFO - __main__ - Global step 250 Train loss 0.18 Classification-F1 0.22649572649572647 on epoch=124
05/31/2022 15:06:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.18 on epoch=129
05/31/2022 15:06:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.19 on epoch=134
05/31/2022 15:06:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.17 on epoch=139
05/31/2022 15:06:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.16 on epoch=144
05/31/2022 15:07:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.16 on epoch=149
05/31/2022 15:07:02 - INFO - __main__ - Global step 300 Train loss 0.17 Classification-F1 0.3333333333333333 on epoch=149
05/31/2022 15:07:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.14 on epoch=154
05/31/2022 15:07:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.11 on epoch=159
05/31/2022 15:07:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.14 on epoch=164
05/31/2022 15:07:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.16 on epoch=169
05/31/2022 15:07:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.15 on epoch=174
05/31/2022 15:07:26 - INFO - __main__ - Global step 350 Train loss 0.14 Classification-F1 0.37662337662337664 on epoch=174
05/31/2022 15:07:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.14 on epoch=179
05/31/2022 15:07:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.12 on epoch=184
05/31/2022 15:07:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=189
05/31/2022 15:07:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.15 on epoch=194
05/31/2022 15:07:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.09 on epoch=199
05/31/2022 15:07:49 - INFO - __main__ - Global step 400 Train loss 0.14 Classification-F1 0.2468253968253968 on epoch=199
05/31/2022 15:07:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.13 on epoch=204
05/31/2022 15:07:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.16 on epoch=209
05/31/2022 15:08:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.13 on epoch=214
05/31/2022 15:08:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=219
05/31/2022 15:08:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.12 on epoch=224
05/31/2022 15:08:13 - INFO - __main__ - Global step 450 Train loss 0.13 Classification-F1 0.3552492046659597 on epoch=224
05/31/2022 15:08:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.10 on epoch=229
05/31/2022 15:08:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.09 on epoch=234
05/31/2022 15:08:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.14 on epoch=239
05/31/2022 15:08:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.11 on epoch=244
05/31/2022 15:08:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.09 on epoch=249
05/31/2022 15:08:37 - INFO - __main__ - Global step 500 Train loss 0.10 Classification-F1 0.3191489361702127 on epoch=249
05/31/2022 15:08:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.09 on epoch=254
05/31/2022 15:08:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.08 on epoch=259
05/31/2022 15:08:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.11 on epoch=264
05/31/2022 15:08:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.05 on epoch=269
05/31/2022 15:08:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
05/31/2022 15:09:00 - INFO - __main__ - Global step 550 Train loss 0.07 Classification-F1 0.375 on epoch=274
05/31/2022 15:09:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.07 on epoch=279
05/31/2022 15:09:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.03 on epoch=284
05/31/2022 15:09:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=289
05/31/2022 15:09:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.06 on epoch=294
05/31/2022 15:09:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=299
05/31/2022 15:09:24 - INFO - __main__ - Global step 600 Train loss 0.06 Classification-F1 0.3144016227180528 on epoch=299
05/31/2022 15:09:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=304
05/31/2022 15:09:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.03 on epoch=309
05/31/2022 15:09:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
05/31/2022 15:09:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
05/31/2022 15:09:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
05/31/2022 15:09:48 - INFO - __main__ - Global step 650 Train loss 0.03 Classification-F1 0.2821052631578947 on epoch=324
05/31/2022 15:09:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
05/31/2022 15:09:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=334
05/31/2022 15:10:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
05/31/2022 15:10:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
05/31/2022 15:10:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
05/31/2022 15:10:11 - INFO - __main__ - Global step 700 Train loss 0.03 Classification-F1 0.2827442827442827 on epoch=349
05/31/2022 15:10:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
05/31/2022 15:10:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
05/31/2022 15:10:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
05/31/2022 15:10:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
05/31/2022 15:10:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
05/31/2022 15:10:35 - INFO - __main__ - Global step 750 Train loss 0.03 Classification-F1 0.3107692307692308 on epoch=374
05/31/2022 15:10:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
05/31/2022 15:10:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
05/31/2022 15:10:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
05/31/2022 15:10:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
05/31/2022 15:10:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
05/31/2022 15:10:59 - INFO - __main__ - Global step 800 Train loss 0.02 Classification-F1 0.4285714285714286 on epoch=399
05/31/2022 15:11:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
05/31/2022 15:11:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
05/31/2022 15:11:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
05/31/2022 15:11:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
05/31/2022 15:11:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
05/31/2022 15:11:22 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.19226750261233022 on epoch=424
05/31/2022 15:11:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
05/31/2022 15:11:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
05/31/2022 15:11:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
05/31/2022 15:11:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
05/31/2022 15:11:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
05/31/2022 15:11:46 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.28066378066378067 on epoch=449
05/31/2022 15:11:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
05/31/2022 15:11:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
05/31/2022 15:11:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
05/31/2022 15:12:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
05/31/2022 15:12:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
05/31/2022 15:12:09 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.4009852216748768 on epoch=474
05/31/2022 15:12:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
05/31/2022 15:12:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
05/31/2022 15:12:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
05/31/2022 15:12:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
05/31/2022 15:12:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
05/31/2022 15:12:33 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.2714285714285714 on epoch=499
05/31/2022 15:12:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
05/31/2022 15:12:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
05/31/2022 15:12:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
05/31/2022 15:12:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
05/31/2022 15:12:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
05/31/2022 15:12:57 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.24691358024691357 on epoch=524
05/31/2022 15:13:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
05/31/2022 15:13:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
05/31/2022 15:13:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
05/31/2022 15:13:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
05/31/2022 15:13:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
05/31/2022 15:13:20 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.4285714285714286 on epoch=549
05/31/2022 15:13:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
05/31/2022 15:13:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
05/31/2022 15:13:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
05/31/2022 15:13:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
05/31/2022 15:13:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
05/31/2022 15:13:44 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=574
05/31/2022 15:13:44 - INFO - __main__ - Saving model with best Classification-F1: 0.4817813765182186 -> 0.4980392156862745 on epoch=574, global_step=1150
05/31/2022 15:13:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
05/31/2022 15:13:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
05/31/2022 15:13:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
05/31/2022 15:14:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
05/31/2022 15:14:06 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
05/31/2022 15:14:07 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.5465587044534412 on epoch=599
05/31/2022 15:14:07 - INFO - __main__ - Saving model with best Classification-F1: 0.4980392156862745 -> 0.5465587044534412 on epoch=599, global_step=1200
05/31/2022 15:14:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
05/31/2022 15:14:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
05/31/2022 15:14:21 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
05/31/2022 15:14:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
05/31/2022 15:14:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
05/31/2022 15:14:31 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.3144016227180528 on epoch=624
05/31/2022 15:14:36 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
05/31/2022 15:14:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
05/31/2022 15:14:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
05/31/2022 15:14:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
05/31/2022 15:14:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
05/31/2022 15:14:55 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.4009852216748768 on epoch=649
05/31/2022 15:14:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
05/31/2022 15:15:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
05/31/2022 15:15:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
05/31/2022 15:15:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
05/31/2022 15:15:17 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
05/31/2022 15:15:19 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.40566959921798634 on epoch=674
05/31/2022 15:15:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
05/31/2022 15:15:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
05/31/2022 15:15:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
05/31/2022 15:15:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
05/31/2022 15:15:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
05/31/2022 15:15:42 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.43529411764705883 on epoch=699
05/31/2022 15:15:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
05/31/2022 15:15:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
05/31/2022 15:15:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
05/31/2022 15:16:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
05/31/2022 15:16:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
05/31/2022 15:16:06 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.18275862068965515 on epoch=724
05/31/2022 15:16:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
05/31/2022 15:16:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
05/31/2022 15:16:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
05/31/2022 15:16:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
05/31/2022 15:16:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
05/31/2022 15:16:30 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.19525862068965516 on epoch=749
05/31/2022 15:16:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
05/31/2022 15:16:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
05/31/2022 15:16:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
05/31/2022 15:16:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
05/31/2022 15:16:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
05/31/2022 15:16:53 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.3995943204868155 on epoch=774
05/31/2022 15:16:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
05/31/2022 15:17:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
05/31/2022 15:17:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
05/31/2022 15:17:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
05/31/2022 15:17:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
05/31/2022 15:17:17 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.2542016806722689 on epoch=799
05/31/2022 15:17:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
05/31/2022 15:17:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
05/31/2022 15:17:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
05/31/2022 15:17:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
05/31/2022 15:17:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
05/31/2022 15:17:41 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.3333333333333333 on epoch=824
05/31/2022 15:17:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
05/31/2022 15:17:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
05/31/2022 15:17:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
05/31/2022 15:17:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
05/31/2022 15:18:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
05/31/2022 15:18:04 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.3333333333333333 on epoch=849
05/31/2022 15:18:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
05/31/2022 15:18:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
05/31/2022 15:18:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
05/31/2022 15:18:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
05/31/2022 15:18:26 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
05/31/2022 15:18:28 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.3142857142857143 on epoch=874
05/31/2022 15:18:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
05/31/2022 15:18:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
05/31/2022 15:18:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
05/31/2022 15:18:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
05/31/2022 15:18:50 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
05/31/2022 15:18:51 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=899
05/31/2022 15:18:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
05/31/2022 15:19:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
05/31/2022 15:19:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
05/31/2022 15:19:09 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
05/31/2022 15:19:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
05/31/2022 15:19:15 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=924
05/31/2022 15:19:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
05/31/2022 15:19:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
05/31/2022 15:19:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
05/31/2022 15:19:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
05/31/2022 15:19:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
05/31/2022 15:19:38 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.26262626262626265 on epoch=949
05/31/2022 15:19:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
05/31/2022 15:19:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
05/31/2022 15:19:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
05/31/2022 15:19:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
05/31/2022 15:20:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
05/31/2022 15:20:02 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.2517361111111111 on epoch=974
05/31/2022 15:20:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
05/31/2022 15:20:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
05/31/2022 15:20:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
05/31/2022 15:20:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
05/31/2022 15:20:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
05/31/2022 15:20:25 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.4375 on epoch=999
05/31/2022 15:20:25 - INFO - __main__ - save last model!
05/31/2022 15:20:25 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 15:20:25 - INFO - __main__ - Printing 3 examples
05/31/2022 15:20:25 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/31/2022 15:20:25 - INFO - __main__ - ['entailed']
05/31/2022 15:20:25 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/31/2022 15:20:25 - INFO - __main__ - ['entailed']
05/31/2022 15:20:25 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/31/2022 15:20:25 - INFO - __main__ - ['entailed']
05/31/2022 15:20:25 - INFO - __main__ - Tokenizing Input ...
05/31/2022 15:20:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 15:20:25 - INFO - __main__ - Tokenizing Output ...
05/31/2022 15:20:25 - INFO - __main__ - Start tokenizing ... 12792 instances
05/31/2022 15:20:25 - INFO - __main__ - Printing 3 examples
05/31/2022 15:20:25 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 15:20:25 - INFO - __main__ - ['entailed']
05/31/2022 15:20:25 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 15:20:25 - INFO - __main__ - ['entailed']
05/31/2022 15:20:25 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 15:20:25 - INFO - __main__ - ['entailed']
05/31/2022 15:20:25 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 15:20:25 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 15:20:25 - INFO - __main__ - Printing 3 examples
05/31/2022 15:20:25 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/31/2022 15:20:25 - INFO - __main__ - ['entailed']
05/31/2022 15:20:25 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/31/2022 15:20:25 - INFO - __main__ - ['entailed']
05/31/2022 15:20:25 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/31/2022 15:20:25 - INFO - __main__ - ['entailed']
05/31/2022 15:20:25 - INFO - __main__ - Tokenizing Input ...
05/31/2022 15:20:25 - INFO - __main__ - Tokenizing Input ...
05/31/2022 15:20:25 - INFO - __main__ - Tokenizing Output ...
05/31/2022 15:20:25 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 15:20:41 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 15:20:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 15:20:42 - INFO - __main__ - Starting training!
05/31/2022 15:20:51 - INFO - __main__ - Tokenizing Output ...
05/31/2022 15:21:03 - INFO - __main__ - Loaded 12792 examples from test data
05/31/2022 15:29:20 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/singletask-tab_fact/tab_fact_16_21_0.5_8_predictions.txt
05/31/2022 15:29:20 - INFO - __main__ - Classification-F1 on test data: 0.0285
05/31/2022 15:29:20 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.5, bsz=8, dev_performance=0.5465587044534412, test_performance=0.02850276007519567
05/31/2022 15:29:20 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.4, bsz=8 ...
05/31/2022 15:29:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 15:29:21 - INFO - __main__ - Printing 3 examples
05/31/2022 15:29:21 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/31/2022 15:29:21 - INFO - __main__ - ['entailed']
05/31/2022 15:29:21 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/31/2022 15:29:21 - INFO - __main__ - ['entailed']
05/31/2022 15:29:21 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/31/2022 15:29:21 - INFO - __main__ - ['entailed']
05/31/2022 15:29:21 - INFO - __main__ - Tokenizing Input ...
05/31/2022 15:29:21 - INFO - __main__ - Tokenizing Output ...
05/31/2022 15:29:21 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 15:29:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 15:29:21 - INFO - __main__ - Printing 3 examples
05/31/2022 15:29:21 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/31/2022 15:29:21 - INFO - __main__ - ['entailed']
05/31/2022 15:29:21 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/31/2022 15:29:21 - INFO - __main__ - ['entailed']
05/31/2022 15:29:21 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/31/2022 15:29:21 - INFO - __main__ - ['entailed']
05/31/2022 15:29:21 - INFO - __main__ - Tokenizing Input ...
05/31/2022 15:29:21 - INFO - __main__ - Tokenizing Output ...
05/31/2022 15:29:21 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 15:29:37 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 15:29:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 15:29:37 - INFO - __main__ - Starting training!
05/31/2022 15:29:42 - INFO - __main__ - Step 10 Global step 10 Train loss 2.79 on epoch=4
05/31/2022 15:29:47 - INFO - __main__ - Step 20 Global step 20 Train loss 1.05 on epoch=9
05/31/2022 15:29:51 - INFO - __main__ - Step 30 Global step 30 Train loss 0.37 on epoch=14
05/31/2022 15:29:56 - INFO - __main__ - Step 40 Global step 40 Train loss 0.32 on epoch=19
05/31/2022 15:30:00 - INFO - __main__ - Step 50 Global step 50 Train loss 0.28 on epoch=24
05/31/2022 15:30:02 - INFO - __main__ - Global step 50 Train loss 0.96 Classification-F1 0.4385964912280702 on epoch=24
05/31/2022 15:30:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.4385964912280702 on epoch=24, global_step=50
05/31/2022 15:30:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.32 on epoch=29
05/31/2022 15:30:11 - INFO - __main__ - Step 70 Global step 70 Train loss 0.29 on epoch=34
05/31/2022 15:30:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.23 on epoch=39
05/31/2022 15:30:19 - INFO - __main__ - Step 90 Global step 90 Train loss 0.22 on epoch=44
05/31/2022 15:30:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.22 on epoch=49
05/31/2022 15:30:25 - INFO - __main__ - Global step 100 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=49
05/31/2022 15:30:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.28 on epoch=54
05/31/2022 15:30:34 - INFO - __main__ - Step 120 Global step 120 Train loss 0.25 on epoch=59
05/31/2022 15:30:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.24 on epoch=64
05/31/2022 15:30:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.22 on epoch=69
05/31/2022 15:30:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.25 on epoch=74
05/31/2022 15:30:49 - INFO - __main__ - Global step 150 Train loss 0.25 Classification-F1 0.5901477832512315 on epoch=74
05/31/2022 15:30:49 - INFO - __main__ - Saving model with best Classification-F1: 0.4385964912280702 -> 0.5901477832512315 on epoch=74, global_step=150
05/31/2022 15:30:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.23 on epoch=79
05/31/2022 15:30:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.23 on epoch=84
05/31/2022 15:31:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.23 on epoch=89
05/31/2022 15:31:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.23 on epoch=94
05/31/2022 15:31:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.24 on epoch=99
05/31/2022 15:31:12 - INFO - __main__ - Global step 200 Train loss 0.23 Classification-F1 0.3992490613266583 on epoch=99
05/31/2022 15:31:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.22 on epoch=104
05/31/2022 15:31:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.19 on epoch=109
05/31/2022 15:31:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.20 on epoch=114
05/31/2022 15:31:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.19 on epoch=119
05/31/2022 15:31:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.19 on epoch=124
05/31/2022 15:31:36 - INFO - __main__ - Global step 250 Train loss 0.20 Classification-F1 0.36374269005847953 on epoch=124
05/31/2022 15:31:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.22 on epoch=129
05/31/2022 15:31:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.19 on epoch=134
05/31/2022 15:31:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.18 on epoch=139
05/31/2022 15:31:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.18 on epoch=144
05/31/2022 15:31:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.19 on epoch=149
05/31/2022 15:31:59 - INFO - __main__ - Global step 300 Train loss 0.19 Classification-F1 0.39756367663344405 on epoch=149
05/31/2022 15:32:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=154
05/31/2022 15:32:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.19 on epoch=159
05/31/2022 15:32:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.17 on epoch=164
05/31/2022 15:32:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.17 on epoch=169
05/31/2022 15:32:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.18 on epoch=174
05/31/2022 15:32:23 - INFO - __main__ - Global step 350 Train loss 0.18 Classification-F1 0.3083778966131907 on epoch=174
05/31/2022 15:32:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.19 on epoch=179
05/31/2022 15:32:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.14 on epoch=184
05/31/2022 15:32:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.17 on epoch=189
05/31/2022 15:32:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.15 on epoch=194
05/31/2022 15:32:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.18 on epoch=199
05/31/2022 15:32:46 - INFO - __main__ - Global step 400 Train loss 0.16 Classification-F1 0.2860310421286031 on epoch=199
05/31/2022 15:32:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.13 on epoch=204
05/31/2022 15:32:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.18 on epoch=209
05/31/2022 15:32:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.17 on epoch=214
05/31/2022 15:33:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.13 on epoch=219
05/31/2022 15:33:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.14 on epoch=224
05/31/2022 15:33:09 - INFO - __main__ - Global step 450 Train loss 0.15 Classification-F1 0.22414414414414416 on epoch=224
05/31/2022 15:33:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.15 on epoch=229
05/31/2022 15:33:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.13 on epoch=234
05/31/2022 15:33:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.10 on epoch=239
05/31/2022 15:33:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.09 on epoch=244
05/31/2022 15:33:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.09 on epoch=249
05/31/2022 15:33:33 - INFO - __main__ - Global step 500 Train loss 0.11 Classification-F1 0.4009852216748768 on epoch=249
05/31/2022 15:33:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.09 on epoch=254
05/31/2022 15:33:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.08 on epoch=259
05/31/2022 15:33:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.10 on epoch=264
05/31/2022 15:33:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=269
05/31/2022 15:33:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
05/31/2022 15:33:57 - INFO - __main__ - Global step 550 Train loss 0.08 Classification-F1 0.13859020310633213 on epoch=274
05/31/2022 15:34:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.09 on epoch=279
05/31/2022 15:34:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=284
05/31/2022 15:34:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=289
05/31/2022 15:34:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.08 on epoch=294
05/31/2022 15:34:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=299
05/31/2022 15:34:20 - INFO - __main__ - Global step 600 Train loss 0.07 Classification-F1 0.3142857142857143 on epoch=299
05/31/2022 15:34:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=304
05/31/2022 15:34:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
05/31/2022 15:34:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=314
05/31/2022 15:34:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
05/31/2022 15:34:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
05/31/2022 15:34:43 - INFO - __main__ - Global step 650 Train loss 0.03 Classification-F1 0.29479377958079783 on epoch=324
05/31/2022 15:34:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
05/31/2022 15:34:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
05/31/2022 15:34:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
05/31/2022 15:35:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
05/31/2022 15:35:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=349
05/31/2022 15:35:07 - INFO - __main__ - Global step 700 Train loss 0.04 Classification-F1 0.43529411764705883 on epoch=349
05/31/2022 15:35:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
05/31/2022 15:35:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
05/31/2022 15:35:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
05/31/2022 15:35:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
05/31/2022 15:35:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
05/31/2022 15:35:31 - INFO - __main__ - Global step 750 Train loss 0.02 Classification-F1 0.4682306940371457 on epoch=374
05/31/2022 15:35:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
05/31/2022 15:35:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
05/31/2022 15:35:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
05/31/2022 15:35:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
05/31/2022 15:35:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
05/31/2022 15:35:54 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.25219743069641654 on epoch=399
05/31/2022 15:35:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
05/31/2022 15:36:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=409
05/31/2022 15:36:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
05/31/2022 15:36:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
05/31/2022 15:36:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
05/31/2022 15:36:18 - INFO - __main__ - Global step 850 Train loss 0.02 Classification-F1 0.43529411764705883 on epoch=424
05/31/2022 15:36:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=429
05/31/2022 15:36:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=434
05/31/2022 15:36:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
05/31/2022 15:36:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
05/31/2022 15:36:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
05/31/2022 15:36:41 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.43529411764705883 on epoch=449
05/31/2022 15:36:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
05/31/2022 15:36:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
05/31/2022 15:36:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
05/31/2022 15:36:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
05/31/2022 15:37:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
05/31/2022 15:37:05 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.4285714285714286 on epoch=474
05/31/2022 15:37:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
05/31/2022 15:37:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
05/31/2022 15:37:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
05/31/2022 15:37:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
05/31/2022 15:37:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
05/31/2022 15:37:29 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.30864197530864196 on epoch=499
05/31/2022 15:37:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
05/31/2022 15:37:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
05/31/2022 15:37:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
05/31/2022 15:37:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
05/31/2022 15:37:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
05/31/2022 15:37:52 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.5 on epoch=524
05/31/2022 15:37:57 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
05/31/2022 15:38:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
05/31/2022 15:38:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
05/31/2022 15:38:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
05/31/2022 15:38:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
05/31/2022 15:38:16 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.4682306940371457 on epoch=549
05/31/2022 15:38:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
05/31/2022 15:38:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
05/31/2022 15:38:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
05/31/2022 15:38:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
05/31/2022 15:38:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
05/31/2022 15:38:39 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.5 on epoch=574
05/31/2022 15:38:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
05/31/2022 15:38:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
05/31/2022 15:38:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
05/31/2022 15:38:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
05/31/2022 15:39:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
05/31/2022 15:39:03 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=599
05/31/2022 15:39:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
05/31/2022 15:39:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
05/31/2022 15:39:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
05/31/2022 15:39:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
05/31/2022 15:39:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
05/31/2022 15:39:27 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=624
05/31/2022 15:39:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
05/31/2022 15:39:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
05/31/2022 15:39:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
05/31/2022 15:39:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
05/31/2022 15:39:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
05/31/2022 15:39:50 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=649
05/31/2022 15:39:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
05/31/2022 15:39:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
05/31/2022 15:40:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
05/31/2022 15:40:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
05/31/2022 15:40:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
05/31/2022 15:40:14 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.5 on epoch=674
05/31/2022 15:40:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
05/31/2022 15:40:23 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
05/31/2022 15:40:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
05/31/2022 15:40:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
05/31/2022 15:40:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
05/31/2022 15:40:37 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.5 on epoch=699
05/31/2022 15:40:42 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
05/31/2022 15:40:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
05/31/2022 15:40:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
05/31/2022 15:40:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
05/31/2022 15:40:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
05/31/2022 15:41:01 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.43529411764705883 on epoch=724
05/31/2022 15:41:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
05/31/2022 15:41:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
05/31/2022 15:41:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
05/31/2022 15:41:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
05/31/2022 15:41:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
05/31/2022 15:41:24 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.5 on epoch=749
05/31/2022 15:41:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
05/31/2022 15:41:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
05/31/2022 15:41:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
05/31/2022 15:41:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=769
05/31/2022 15:41:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
05/31/2022 15:41:48 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=774
05/31/2022 15:41:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
05/31/2022 15:41:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
05/31/2022 15:42:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
05/31/2022 15:42:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
05/31/2022 15:42:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
05/31/2022 15:42:11 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.3522267206477733 on epoch=799
05/31/2022 15:42:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
05/31/2022 15:42:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
05/31/2022 15:42:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
05/31/2022 15:42:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
05/31/2022 15:42:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
05/31/2022 15:42:35 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.3522267206477733 on epoch=824
05/31/2022 15:42:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
05/31/2022 15:42:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
05/31/2022 15:42:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
05/31/2022 15:42:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
05/31/2022 15:42:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
05/31/2022 15:42:58 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=849
05/31/2022 15:43:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
05/31/2022 15:43:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
05/31/2022 15:43:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
05/31/2022 15:43:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
05/31/2022 15:43:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
05/31/2022 15:43:22 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=874
05/31/2022 15:43:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
05/31/2022 15:43:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
05/31/2022 15:43:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
05/31/2022 15:43:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
05/31/2022 15:43:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
05/31/2022 15:43:45 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=899
05/31/2022 15:43:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
05/31/2022 15:43:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
05/31/2022 15:43:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
05/31/2022 15:44:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
05/31/2022 15:44:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
05/31/2022 15:44:09 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=924
05/31/2022 15:44:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
05/31/2022 15:44:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
05/31/2022 15:44:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
05/31/2022 15:44:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
05/31/2022 15:44:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
05/31/2022 15:44:32 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.39999999999999997 on epoch=949
05/31/2022 15:44:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
05/31/2022 15:44:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
05/31/2022 15:44:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
05/31/2022 15:44:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
05/31/2022 15:44:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
05/31/2022 15:44:56 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.3522267206477733 on epoch=974
05/31/2022 15:45:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
05/31/2022 15:45:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
05/31/2022 15:45:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
05/31/2022 15:45:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
05/31/2022 15:45:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
05/31/2022 15:45:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 15:45:20 - INFO - __main__ - Printing 3 examples
05/31/2022 15:45:20 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/31/2022 15:45:20 - INFO - __main__ - ['entailed']
05/31/2022 15:45:20 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/31/2022 15:45:20 - INFO - __main__ - ['entailed']
05/31/2022 15:45:20 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/31/2022 15:45:20 - INFO - __main__ - ['entailed']
05/31/2022 15:45:20 - INFO - __main__ - Tokenizing Input ...
05/31/2022 15:45:20 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.39999999999999997 on epoch=999
05/31/2022 15:45:20 - INFO - __main__ - save last model!
05/31/2022 15:45:20 - INFO - __main__ - Tokenizing Output ...
05/31/2022 15:45:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 15:45:20 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 15:45:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 15:45:20 - INFO - __main__ - Printing 3 examples
05/31/2022 15:45:20 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/31/2022 15:45:20 - INFO - __main__ - ['entailed']
05/31/2022 15:45:20 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/31/2022 15:45:20 - INFO - __main__ - ['entailed']
05/31/2022 15:45:20 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/31/2022 15:45:20 - INFO - __main__ - ['entailed']
05/31/2022 15:45:20 - INFO - __main__ - Tokenizing Input ...
05/31/2022 15:45:20 - INFO - __main__ - Start tokenizing ... 12792 instances
05/31/2022 15:45:20 - INFO - __main__ - Printing 3 examples
05/31/2022 15:45:20 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 15:45:20 - INFO - __main__ - ['entailed']
05/31/2022 15:45:20 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 15:45:20 - INFO - __main__ - ['entailed']
05/31/2022 15:45:20 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 15:45:20 - INFO - __main__ - ['entailed']
05/31/2022 15:45:20 - INFO - __main__ - Tokenizing Output ...
05/31/2022 15:45:20 - INFO - __main__ - Tokenizing Input ...
05/31/2022 15:45:20 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 15:45:36 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 15:45:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 15:45:36 - INFO - __main__ - Starting training!
05/31/2022 15:45:44 - INFO - __main__ - Tokenizing Output ...
05/31/2022 15:45:57 - INFO - __main__ - Loaded 12792 examples from test data
05/31/2022 15:54:18 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/singletask-tab_fact/tab_fact_16_21_0.4_8_predictions.txt
05/31/2022 15:54:18 - INFO - __main__ - Classification-F1 on test data: 0.0866
05/31/2022 15:54:18 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.4, bsz=8, dev_performance=0.5901477832512315, test_performance=0.086642412029579
05/31/2022 15:54:18 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.3, bsz=8 ...
05/31/2022 15:54:19 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 15:54:19 - INFO - __main__ - Printing 3 examples
05/31/2022 15:54:19 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/31/2022 15:54:19 - INFO - __main__ - ['entailed']
05/31/2022 15:54:19 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/31/2022 15:54:19 - INFO - __main__ - ['entailed']
05/31/2022 15:54:19 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/31/2022 15:54:19 - INFO - __main__ - ['entailed']
05/31/2022 15:54:19 - INFO - __main__ - Tokenizing Input ...
05/31/2022 15:54:19 - INFO - __main__ - Tokenizing Output ...
05/31/2022 15:54:19 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 15:54:19 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 15:54:19 - INFO - __main__ - Printing 3 examples
05/31/2022 15:54:19 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/31/2022 15:54:19 - INFO - __main__ - ['entailed']
05/31/2022 15:54:19 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/31/2022 15:54:19 - INFO - __main__ - ['entailed']
05/31/2022 15:54:19 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/31/2022 15:54:19 - INFO - __main__ - ['entailed']
05/31/2022 15:54:19 - INFO - __main__ - Tokenizing Input ...
05/31/2022 15:54:19 - INFO - __main__ - Tokenizing Output ...
05/31/2022 15:54:20 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 15:54:35 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 15:54:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 15:54:35 - INFO - __main__ - Starting training!
05/31/2022 15:54:40 - INFO - __main__ - Step 10 Global step 10 Train loss 3.54 on epoch=4
05/31/2022 15:54:45 - INFO - __main__ - Step 20 Global step 20 Train loss 1.42 on epoch=9
05/31/2022 15:54:49 - INFO - __main__ - Step 30 Global step 30 Train loss 0.55 on epoch=14
05/31/2022 15:54:53 - INFO - __main__ - Step 40 Global step 40 Train loss 0.41 on epoch=19
05/31/2022 15:54:58 - INFO - __main__ - Step 50 Global step 50 Train loss 0.36 on epoch=24
05/31/2022 15:54:59 - INFO - __main__ - Global step 50 Train loss 1.26 Classification-F1 0.3333333333333333 on epoch=24
05/31/2022 15:54:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
05/31/2022 15:55:03 - INFO - __main__ - Step 60 Global step 60 Train loss 0.31 on epoch=29
05/31/2022 15:55:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.27 on epoch=34
05/31/2022 15:55:12 - INFO - __main__ - Step 80 Global step 80 Train loss 0.31 on epoch=39
05/31/2022 15:55:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.27 on epoch=44
05/31/2022 15:55:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.21 on epoch=49
05/31/2022 15:55:22 - INFO - __main__ - Global step 100 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=49
05/31/2022 15:55:27 - INFO - __main__ - Step 110 Global step 110 Train loss 0.25 on epoch=54
05/31/2022 15:55:31 - INFO - __main__ - Step 120 Global step 120 Train loss 0.24 on epoch=59
05/31/2022 15:55:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.23 on epoch=64
05/31/2022 15:55:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=69
05/31/2022 15:55:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.27 on epoch=74
05/31/2022 15:55:46 - INFO - __main__ - Global step 150 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=74
05/31/2022 15:55:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.23 on epoch=79
05/31/2022 15:55:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.23 on epoch=84
05/31/2022 15:55:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.23 on epoch=89
05/31/2022 15:56:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.23 on epoch=94
05/31/2022 15:56:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.23 on epoch=99
05/31/2022 15:56:09 - INFO - __main__ - Global step 200 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=99
05/31/2022 15:56:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.21 on epoch=104
05/31/2022 15:56:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.22 on epoch=109
05/31/2022 15:56:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.23 on epoch=114
05/31/2022 15:56:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=119
05/31/2022 15:56:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=124
05/31/2022 15:56:33 - INFO - __main__ - Global step 250 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=124
05/31/2022 15:56:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.19 on epoch=129
05/31/2022 15:56:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.19 on epoch=134
05/31/2022 15:56:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.17 on epoch=139
05/31/2022 15:56:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.19 on epoch=144
05/31/2022 15:56:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.21 on epoch=149
05/31/2022 15:56:56 - INFO - __main__ - Global step 300 Train loss 0.19 Classification-F1 0.4009852216748768 on epoch=149
05/31/2022 15:56:56 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4009852216748768 on epoch=149, global_step=300
05/31/2022 15:57:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=154
05/31/2022 15:57:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.18 on epoch=159
05/31/2022 15:57:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.17 on epoch=164
05/31/2022 15:57:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.18 on epoch=169
05/31/2022 15:57:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.20 on epoch=174
05/31/2022 15:57:19 - INFO - __main__ - Global step 350 Train loss 0.19 Classification-F1 0.25396825396825395 on epoch=174
05/31/2022 15:57:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.19 on epoch=179
05/31/2022 15:57:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.16 on epoch=184
05/31/2022 15:57:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.16 on epoch=189
05/31/2022 15:57:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.18 on epoch=194
05/31/2022 15:57:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.15 on epoch=199
05/31/2022 15:57:43 - INFO - __main__ - Global step 400 Train loss 0.17 Classification-F1 0.37254901960784315 on epoch=199
05/31/2022 15:57:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.19 on epoch=204
05/31/2022 15:57:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.15 on epoch=209
05/31/2022 15:57:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.14 on epoch=214
05/31/2022 15:58:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.15 on epoch=219
05/31/2022 15:58:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.12 on epoch=224
05/31/2022 15:58:06 - INFO - __main__ - Global step 450 Train loss 0.15 Classification-F1 0.30980392156862746 on epoch=224
05/31/2022 15:58:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.13 on epoch=229
05/31/2022 15:58:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.13 on epoch=234
05/31/2022 15:58:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.14 on epoch=239
05/31/2022 15:58:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.16 on epoch=244
05/31/2022 15:58:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.12 on epoch=249
05/31/2022 15:58:30 - INFO - __main__ - Global step 500 Train loss 0.14 Classification-F1 0.3071253071253071 on epoch=249
05/31/2022 15:58:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.14 on epoch=254
05/31/2022 15:58:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.10 on epoch=259
05/31/2022 15:58:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.13 on epoch=264
05/31/2022 15:58:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.08 on epoch=269
05/31/2022 15:58:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.08 on epoch=274
05/31/2022 15:58:53 - INFO - __main__ - Global step 550 Train loss 0.10 Classification-F1 0.22695035460992907 on epoch=274
05/31/2022 15:58:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=279
05/31/2022 15:59:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.08 on epoch=284
05/31/2022 15:59:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
05/31/2022 15:59:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.11 on epoch=294
05/31/2022 15:59:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.09 on epoch=299
05/31/2022 15:59:17 - INFO - __main__ - Global step 600 Train loss 0.08 Classification-F1 0.3264033264033264 on epoch=299
05/31/2022 15:59:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.05 on epoch=304
05/31/2022 15:59:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
05/31/2022 15:59:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.07 on epoch=314
05/31/2022 15:59:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
05/31/2022 15:59:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
05/31/2022 15:59:40 - INFO - __main__ - Global step 650 Train loss 0.04 Classification-F1 0.23232323232323232 on epoch=324
05/31/2022 15:59:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.11 on epoch=329
05/31/2022 15:59:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=334
05/31/2022 15:59:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
05/31/2022 15:59:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
05/31/2022 16:00:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
05/31/2022 16:00:04 - INFO - __main__ - Global step 700 Train loss 0.04 Classification-F1 0.2873806998939555 on epoch=349
05/31/2022 16:00:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
05/31/2022 16:00:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=359
05/31/2022 16:00:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=364
05/31/2022 16:00:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
05/31/2022 16:00:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
05/31/2022 16:00:27 - INFO - __main__ - Global step 750 Train loss 0.03 Classification-F1 0.2963709677419355 on epoch=374
05/31/2022 16:00:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
05/31/2022 16:00:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=384
05/31/2022 16:00:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
05/31/2022 16:00:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
05/31/2022 16:00:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
05/31/2022 16:00:51 - INFO - __main__ - Global step 800 Train loss 0.03 Classification-F1 0.2904761904761905 on epoch=399
05/31/2022 16:00:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=404
05/31/2022 16:00:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
05/31/2022 16:01:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
05/31/2022 16:01:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
05/31/2022 16:01:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
05/31/2022 16:01:14 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.43529411764705883 on epoch=424
05/31/2022 16:01:14 - INFO - __main__ - Saving model with best Classification-F1: 0.4009852216748768 -> 0.43529411764705883 on epoch=424, global_step=850
05/31/2022 16:01:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
05/31/2022 16:01:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=434
05/31/2022 16:01:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
05/31/2022 16:01:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
05/31/2022 16:01:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
05/31/2022 16:01:37 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.4554554554554554 on epoch=449
05/31/2022 16:01:37 - INFO - __main__ - Saving model with best Classification-F1: 0.43529411764705883 -> 0.4554554554554554 on epoch=449, global_step=900
05/31/2022 16:01:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
05/31/2022 16:01:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
05/31/2022 16:01:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
05/31/2022 16:01:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
05/31/2022 16:02:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
05/31/2022 16:02:01 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.3595430107526882 on epoch=474
05/31/2022 16:02:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
05/31/2022 16:02:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
05/31/2022 16:02:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
05/31/2022 16:02:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
05/31/2022 16:02:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
05/31/2022 16:02:24 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.2246376811594203 on epoch=499
05/31/2022 16:02:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
05/31/2022 16:02:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
05/31/2022 16:02:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
05/31/2022 16:02:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
05/31/2022 16:02:47 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
05/31/2022 16:02:48 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=524
05/31/2022 16:02:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
05/31/2022 16:02:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
05/31/2022 16:03:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
05/31/2022 16:03:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
05/31/2022 16:03:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
05/31/2022 16:03:11 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=549
05/31/2022 16:03:11 - INFO - __main__ - Saving model with best Classification-F1: 0.4554554554554554 -> 0.4817813765182186 on epoch=549, global_step=1100
05/31/2022 16:03:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
05/31/2022 16:03:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
05/31/2022 16:03:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
05/31/2022 16:03:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
05/31/2022 16:03:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
05/31/2022 16:03:35 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.24569892473118282 on epoch=574
05/31/2022 16:03:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
05/31/2022 16:03:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
05/31/2022 16:03:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
05/31/2022 16:03:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
05/31/2022 16:03:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
05/31/2022 16:03:58 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.2963709677419355 on epoch=599
05/31/2022 16:04:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
05/31/2022 16:04:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
05/31/2022 16:04:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
05/31/2022 16:04:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
05/31/2022 16:04:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
05/31/2022 16:04:21 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=624
05/31/2022 16:04:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
05/31/2022 16:04:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
05/31/2022 16:04:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
05/31/2022 16:04:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
05/31/2022 16:04:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
05/31/2022 16:04:45 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.2963709677419355 on epoch=649
05/31/2022 16:04:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
05/31/2022 16:04:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
05/31/2022 16:04:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
05/31/2022 16:05:02 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
05/31/2022 16:05:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
05/31/2022 16:05:08 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=674
05/31/2022 16:05:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
05/31/2022 16:05:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
05/31/2022 16:05:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
05/31/2022 16:05:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
05/31/2022 16:05:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
05/31/2022 16:05:31 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=699
05/31/2022 16:05:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
05/31/2022 16:05:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
05/31/2022 16:05:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
05/31/2022 16:05:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
05/31/2022 16:05:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
05/31/2022 16:05:55 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.2963709677419355 on epoch=724
05/31/2022 16:05:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
05/31/2022 16:06:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
05/31/2022 16:06:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
05/31/2022 16:06:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
05/31/2022 16:06:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
05/31/2022 16:06:18 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.43529411764705883 on epoch=749
05/31/2022 16:06:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
05/31/2022 16:06:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
05/31/2022 16:06:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
05/31/2022 16:06:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
05/31/2022 16:06:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
05/31/2022 16:06:42 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=774
05/31/2022 16:06:42 - INFO - __main__ - Saving model with best Classification-F1: 0.4817813765182186 -> 0.4980392156862745 on epoch=774, global_step=1550
05/31/2022 16:06:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
05/31/2022 16:06:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
05/31/2022 16:06:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
05/31/2022 16:06:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
05/31/2022 16:07:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
05/31/2022 16:07:05 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.2718052738336714 on epoch=799
05/31/2022 16:07:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
05/31/2022 16:07:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
05/31/2022 16:07:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
05/31/2022 16:07:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
05/31/2022 16:07:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
05/31/2022 16:07:29 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.2718052738336714 on epoch=824
05/31/2022 16:07:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
05/31/2022 16:07:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
05/31/2022 16:07:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
05/31/2022 16:07:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
05/31/2022 16:07:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
05/31/2022 16:07:52 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=849
05/31/2022 16:07:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
05/31/2022 16:08:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
05/31/2022 16:08:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
05/31/2022 16:08:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
05/31/2022 16:08:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
05/31/2022 16:08:16 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.3333333333333333 on epoch=874
05/31/2022 16:08:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
05/31/2022 16:08:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
05/31/2022 16:08:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
05/31/2022 16:08:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=894
05/31/2022 16:08:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
05/31/2022 16:08:39 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=899
05/31/2022 16:08:39 - INFO - __main__ - Saving model with best Classification-F1: 0.4980392156862745 -> 0.5195195195195195 on epoch=899, global_step=1800
05/31/2022 16:08:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
05/31/2022 16:08:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
05/31/2022 16:08:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
05/31/2022 16:08:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
05/31/2022 16:09:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
05/31/2022 16:09:03 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.27474747474747474 on epoch=924
05/31/2022 16:09:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
05/31/2022 16:09:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
05/31/2022 16:09:16 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
05/31/2022 16:09:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
05/31/2022 16:09:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
05/31/2022 16:09:26 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.40566959921798634 on epoch=949
05/31/2022 16:09:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
05/31/2022 16:09:35 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
05/31/2022 16:09:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
05/31/2022 16:09:44 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
05/31/2022 16:09:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
05/31/2022 16:09:49 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=974
05/31/2022 16:09:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
05/31/2022 16:09:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
05/31/2022 16:10:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
05/31/2022 16:10:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
05/31/2022 16:10:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
05/31/2022 16:10:13 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 16:10:13 - INFO - __main__ - Printing 3 examples
05/31/2022 16:10:13 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/31/2022 16:10:13 - INFO - __main__ - ['entailed']
05/31/2022 16:10:13 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/31/2022 16:10:13 - INFO - __main__ - ['entailed']
05/31/2022 16:10:13 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/31/2022 16:10:13 - INFO - __main__ - ['entailed']
05/31/2022 16:10:13 - INFO - __main__ - Tokenizing Input ...
05/31/2022 16:10:13 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.3142857142857143 on epoch=999
05/31/2022 16:10:13 - INFO - __main__ - save last model!
05/31/2022 16:10:13 - INFO - __main__ - Tokenizing Output ...
05/31/2022 16:10:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 16:10:13 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 16:10:13 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 16:10:13 - INFO - __main__ - Printing 3 examples
05/31/2022 16:10:13 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/31/2022 16:10:13 - INFO - __main__ - ['entailed']
05/31/2022 16:10:13 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/31/2022 16:10:13 - INFO - __main__ - ['entailed']
05/31/2022 16:10:13 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/31/2022 16:10:13 - INFO - __main__ - ['entailed']
05/31/2022 16:10:13 - INFO - __main__ - Tokenizing Input ...
05/31/2022 16:10:13 - INFO - __main__ - Start tokenizing ... 12792 instances
05/31/2022 16:10:13 - INFO - __main__ - Printing 3 examples
05/31/2022 16:10:13 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 16:10:13 - INFO - __main__ - ['entailed']
05/31/2022 16:10:13 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 16:10:13 - INFO - __main__ - ['entailed']
05/31/2022 16:10:13 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 16:10:13 - INFO - __main__ - ['entailed']
05/31/2022 16:10:13 - INFO - __main__ - Tokenizing Input ...
05/31/2022 16:10:13 - INFO - __main__ - Tokenizing Output ...
05/31/2022 16:10:13 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 16:10:29 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 16:10:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 16:10:29 - INFO - __main__ - Starting training!
05/31/2022 16:10:38 - INFO - __main__ - Tokenizing Output ...
05/31/2022 16:10:50 - INFO - __main__ - Loaded 12792 examples from test data
05/31/2022 16:19:03 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/singletask-tab_fact/tab_fact_16_21_0.3_8_predictions.txt
05/31/2022 16:19:03 - INFO - __main__ - Classification-F1 on test data: 0.1226
05/31/2022 16:19:03 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.3, bsz=8, dev_performance=0.5195195195195195, test_performance=0.12263457136528097
05/31/2022 16:19:03 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.2, bsz=8 ...
05/31/2022 16:19:04 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 16:19:04 - INFO - __main__ - Printing 3 examples
05/31/2022 16:19:04 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/31/2022 16:19:04 - INFO - __main__ - ['entailed']
05/31/2022 16:19:04 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/31/2022 16:19:04 - INFO - __main__ - ['entailed']
05/31/2022 16:19:04 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/31/2022 16:19:04 - INFO - __main__ - ['entailed']
05/31/2022 16:19:04 - INFO - __main__ - Tokenizing Input ...
05/31/2022 16:19:04 - INFO - __main__ - Tokenizing Output ...
05/31/2022 16:19:04 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 16:19:04 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 16:19:04 - INFO - __main__ - Printing 3 examples
05/31/2022 16:19:04 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
05/31/2022 16:19:04 - INFO - __main__ - ['entailed']
05/31/2022 16:19:04 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
05/31/2022 16:19:04 - INFO - __main__ - ['entailed']
05/31/2022 16:19:04 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
05/31/2022 16:19:04 - INFO - __main__ - ['entailed']
05/31/2022 16:19:04 - INFO - __main__ - Tokenizing Input ...
05/31/2022 16:19:04 - INFO - __main__ - Tokenizing Output ...
05/31/2022 16:19:04 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 16:19:19 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 16:19:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 16:19:20 - INFO - __main__ - Starting training!
05/31/2022 16:19:25 - INFO - __main__ - Step 10 Global step 10 Train loss 3.60 on epoch=4
05/31/2022 16:19:29 - INFO - __main__ - Step 20 Global step 20 Train loss 2.13 on epoch=9
05/31/2022 16:19:34 - INFO - __main__ - Step 30 Global step 30 Train loss 0.95 on epoch=14
05/31/2022 16:19:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.54 on epoch=19
05/31/2022 16:19:42 - INFO - __main__ - Step 50 Global step 50 Train loss 0.39 on epoch=24
05/31/2022 16:19:44 - INFO - __main__ - Global step 50 Train loss 1.52 Classification-F1 0.3992490613266583 on epoch=24
05/31/2022 16:19:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3992490613266583 on epoch=24, global_step=50
05/31/2022 16:19:48 - INFO - __main__ - Step 60 Global step 60 Train loss 0.31 on epoch=29
05/31/2022 16:19:53 - INFO - __main__ - Step 70 Global step 70 Train loss 0.30 on epoch=34
05/31/2022 16:19:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.26 on epoch=39
05/31/2022 16:20:02 - INFO - __main__ - Step 90 Global step 90 Train loss 0.32 on epoch=44
05/31/2022 16:20:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.29 on epoch=49
05/31/2022 16:20:07 - INFO - __main__ - Global step 100 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=49
05/31/2022 16:20:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.26 on epoch=54
05/31/2022 16:20:16 - INFO - __main__ - Step 120 Global step 120 Train loss 0.27 on epoch=59
05/31/2022 16:20:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.26 on epoch=64
05/31/2022 16:20:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.29 on epoch=69
05/31/2022 16:20:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.24 on epoch=74
05/31/2022 16:20:31 - INFO - __main__ - Global step 150 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=74
05/31/2022 16:20:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.25 on epoch=79
05/31/2022 16:20:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.26 on epoch=84
05/31/2022 16:20:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.25 on epoch=89
05/31/2022 16:20:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=94
05/31/2022 16:20:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.22 on epoch=99
05/31/2022 16:20:54 - INFO - __main__ - Global step 200 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=99
05/31/2022 16:20:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.25 on epoch=104
05/31/2022 16:21:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.21 on epoch=109
05/31/2022 16:21:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.23 on epoch=114
05/31/2022 16:21:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.22 on epoch=119
05/31/2022 16:21:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=124
05/31/2022 16:21:17 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=124
05/31/2022 16:21:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.20 on epoch=129
05/31/2022 16:21:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.23 on epoch=134
05/31/2022 16:21:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=139
05/31/2022 16:21:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=144
05/31/2022 16:21:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=149
05/31/2022 16:21:40 - INFO - __main__ - Global step 300 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=149
05/31/2022 16:21:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.21 on epoch=154
05/31/2022 16:21:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.19 on epoch=159
05/31/2022 16:21:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.20 on epoch=164
05/31/2022 16:21:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.21 on epoch=169
05/31/2022 16:22:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.20 on epoch=174
05/31/2022 16:22:04 - INFO - __main__ - Global step 350 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=174
05/31/2022 16:22:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=179
05/31/2022 16:22:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=184
05/31/2022 16:22:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=189
05/31/2022 16:22:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.19 on epoch=194
05/31/2022 16:22:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=199
05/31/2022 16:22:27 - INFO - __main__ - Global step 400 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=199
05/31/2022 16:22:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.18 on epoch=204
05/31/2022 16:22:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.20 on epoch=209
05/31/2022 16:22:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=214
05/31/2022 16:22:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.19 on epoch=219
05/31/2022 16:22:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=224
05/31/2022 16:22:51 - INFO - __main__ - Global step 450 Train loss 0.20 Classification-F1 0.3454545454545454 on epoch=224
05/31/2022 16:22:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.17 on epoch=229
05/31/2022 16:22:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=234
05/31/2022 16:23:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.19 on epoch=239
05/31/2022 16:23:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.17 on epoch=244
05/31/2022 16:23:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.18 on epoch=249
05/31/2022 16:23:14 - INFO - __main__ - Global step 500 Train loss 0.19 Classification-F1 0.37662337662337664 on epoch=249
05/31/2022 16:23:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.19 on epoch=254
05/31/2022 16:23:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=259
05/31/2022 16:23:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.18 on epoch=264
05/31/2022 16:23:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.17 on epoch=269
05/31/2022 16:23:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.17 on epoch=274
05/31/2022 16:23:37 - INFO - __main__ - Global step 550 Train loss 0.18 Classification-F1 0.2540322580645162 on epoch=274
05/31/2022 16:23:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=279
05/31/2022 16:23:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.14 on epoch=284
05/31/2022 16:23:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=289
05/31/2022 16:23:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=294
05/31/2022 16:23:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=299
05/31/2022 16:24:00 - INFO - __main__ - Global step 600 Train loss 0.18 Classification-F1 0.18450390189520627 on epoch=299
05/31/2022 16:24:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.16 on epoch=304
05/31/2022 16:24:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.15 on epoch=309
05/31/2022 16:24:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.16 on epoch=314
05/31/2022 16:24:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=319
05/31/2022 16:24:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.16 on epoch=324
05/31/2022 16:24:24 - INFO - __main__ - Global step 650 Train loss 0.17 Classification-F1 0.3273273273273273 on epoch=324
05/31/2022 16:24:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=329
05/31/2022 16:24:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.14 on epoch=334
05/31/2022 16:24:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.15 on epoch=339
05/31/2022 16:24:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=344
05/31/2022 16:24:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=349
05/31/2022 16:24:47 - INFO - __main__ - Global step 700 Train loss 0.14 Classification-F1 0.24130434782608692 on epoch=349
05/31/2022 16:24:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.14 on epoch=354
05/31/2022 16:24:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=359
05/31/2022 16:25:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=364
05/31/2022 16:25:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.13 on epoch=369
05/31/2022 16:25:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.12 on epoch=374
05/31/2022 16:25:11 - INFO - __main__ - Global step 750 Train loss 0.13 Classification-F1 0.18353658536585366 on epoch=374
05/31/2022 16:25:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=379
05/31/2022 16:25:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=384
05/31/2022 16:25:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=389
05/31/2022 16:25:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=394
05/31/2022 16:25:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.14 on epoch=399
05/31/2022 16:25:34 - INFO - __main__ - Global step 800 Train loss 0.12 Classification-F1 0.19999999999999998 on epoch=399
05/31/2022 16:25:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=404
05/31/2022 16:25:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=409
05/31/2022 16:25:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.11 on epoch=414
05/31/2022 16:25:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.10 on epoch=419
05/31/2022 16:25:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=424
05/31/2022 16:25:58 - INFO - __main__ - Global step 850 Train loss 0.11 Classification-F1 0.18253968253968256 on epoch=424
05/31/2022 16:26:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=429
05/31/2022 16:26:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.13 on epoch=434
05/31/2022 16:26:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=439
05/31/2022 16:26:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=444
05/31/2022 16:26:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=449
05/31/2022 16:26:22 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.2174638487208009 on epoch=449
05/31/2022 16:26:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=454
05/31/2022 16:26:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=459
05/31/2022 16:26:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.09 on epoch=464
05/31/2022 16:26:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=469
05/31/2022 16:26:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.08 on epoch=474
05/31/2022 16:26:46 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.2033169533169533 on epoch=474
05/31/2022 16:26:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=479
05/31/2022 16:26:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=484
05/31/2022 16:26:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=489
05/31/2022 16:27:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=494
05/31/2022 16:27:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=499
05/31/2022 16:27:10 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.2845878136200717 on epoch=499
05/31/2022 16:27:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=504
05/31/2022 16:27:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=509
05/31/2022 16:27:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=514
05/31/2022 16:27:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=519
05/31/2022 16:27:32 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=524
05/31/2022 16:27:34 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.4920634920634921 on epoch=524
05/31/2022 16:27:34 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4920634920634921 on epoch=524, global_step=1050
05/31/2022 16:27:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=529
05/31/2022 16:27:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=534
05/31/2022 16:27:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
05/31/2022 16:27:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
05/31/2022 16:27:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=549
05/31/2022 16:27:58 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.4909862142099682 on epoch=549
05/31/2022 16:28:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
05/31/2022 16:28:07 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=559
05/31/2022 16:28:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
05/31/2022 16:28:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=569
05/31/2022 16:28:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
05/31/2022 16:28:21 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.375 on epoch=574
05/31/2022 16:28:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=579
05/31/2022 16:28:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=584
05/31/2022 16:28:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=589
05/31/2022 16:28:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
05/31/2022 16:28:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
05/31/2022 16:28:45 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.33793103448275863 on epoch=599
05/31/2022 16:28:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
05/31/2022 16:28:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
05/31/2022 16:28:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
05/31/2022 16:29:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=619
05/31/2022 16:29:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
05/31/2022 16:29:09 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.3650793650793651 on epoch=624
05/31/2022 16:29:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=629
05/31/2022 16:29:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
05/31/2022 16:29:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
05/31/2022 16:29:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=644
05/31/2022 16:29:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
05/31/2022 16:29:33 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.16551724137931037 on epoch=649
05/31/2022 16:29:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
05/31/2022 16:29:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
05/31/2022 16:29:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=664
05/31/2022 16:29:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=669
05/31/2022 16:29:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
05/31/2022 16:29:56 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.19222689075630253 on epoch=674
05/31/2022 16:30:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
05/31/2022 16:30:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
05/31/2022 16:30:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=689
05/31/2022 16:30:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=694
05/31/2022 16:30:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
05/31/2022 16:30:20 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.24166666666666667 on epoch=699
05/31/2022 16:30:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
05/31/2022 16:30:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
05/31/2022 16:30:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
05/31/2022 16:30:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
05/31/2022 16:30:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
05/31/2022 16:30:45 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.22499999999999998 on epoch=724
05/31/2022 16:30:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=729
05/31/2022 16:30:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
05/31/2022 16:30:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
05/31/2022 16:31:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
05/31/2022 16:31:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
05/31/2022 16:31:09 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.18550724637681154 on epoch=749
05/31/2022 16:31:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=754
05/31/2022 16:31:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
05/31/2022 16:31:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
05/31/2022 16:31:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
05/31/2022 16:31:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=774
05/31/2022 16:31:33 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.27523602033405953 on epoch=774
05/31/2022 16:31:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
05/31/2022 16:31:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=784
05/31/2022 16:31:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
05/31/2022 16:31:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
05/31/2022 16:31:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
05/31/2022 16:31:57 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.2692307692307692 on epoch=799
05/31/2022 16:32:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
05/31/2022 16:32:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
05/31/2022 16:32:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
05/31/2022 16:32:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
05/31/2022 16:32:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
05/31/2022 16:32:20 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=824
05/31/2022 16:32:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
05/31/2022 16:32:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
05/31/2022 16:32:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
05/31/2022 16:32:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
05/31/2022 16:32:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
05/31/2022 16:32:44 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.39139139139139134 on epoch=849
05/31/2022 16:32:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
05/31/2022 16:32:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
05/31/2022 16:32:57 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
05/31/2022 16:33:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
05/31/2022 16:33:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
05/31/2022 16:33:08 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.39139139139139134 on epoch=874
05/31/2022 16:33:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
05/31/2022 16:33:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
05/31/2022 16:33:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
05/31/2022 16:33:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
05/31/2022 16:33:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
05/31/2022 16:33:31 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.2006674082313682 on epoch=899
05/31/2022 16:33:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
05/31/2022 16:33:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
05/31/2022 16:33:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
05/31/2022 16:33:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=919
05/31/2022 16:33:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
05/31/2022 16:33:55 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.1841216216216216 on epoch=924
05/31/2022 16:33:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
05/31/2022 16:34:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
05/31/2022 16:34:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
05/31/2022 16:34:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
05/31/2022 16:34:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
05/31/2022 16:34:18 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.19407894736842105 on epoch=949
05/31/2022 16:34:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
05/31/2022 16:34:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
05/31/2022 16:34:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
05/31/2022 16:34:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
05/31/2022 16:34:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
05/31/2022 16:34:42 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.18626373626373624 on epoch=974
05/31/2022 16:34:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
05/31/2022 16:34:50 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
05/31/2022 16:34:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
05/31/2022 16:34:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
05/31/2022 16:35:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
05/31/2022 16:35:05 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.3144016227180528 on epoch=999
05/31/2022 16:35:05 - INFO - __main__ - save last model!
05/31/2022 16:35:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 16:35:05 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 16:35:05 - INFO - __main__ - Printing 3 examples
05/31/2022 16:35:05 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/31/2022 16:35:05 - INFO - __main__ - ['refuted']
05/31/2022 16:35:05 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/31/2022 16:35:05 - INFO - __main__ - ['refuted']
05/31/2022 16:35:05 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/31/2022 16:35:05 - INFO - __main__ - ['refuted']
05/31/2022 16:35:05 - INFO - __main__ - Tokenizing Input ...
05/31/2022 16:35:05 - INFO - __main__ - Start tokenizing ... 12792 instances
05/31/2022 16:35:05 - INFO - __main__ - Printing 3 examples
05/31/2022 16:35:05 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 16:35:05 - INFO - __main__ - ['entailed']
05/31/2022 16:35:05 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 16:35:05 - INFO - __main__ - ['entailed']
05/31/2022 16:35:05 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/31/2022 16:35:05 - INFO - __main__ - ['entailed']
05/31/2022 16:35:05 - INFO - __main__ - Tokenizing Input ...
05/31/2022 16:35:05 - INFO - __main__ - Tokenizing Output ...
05/31/2022 16:35:05 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 16:35:05 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 16:35:05 - INFO - __main__ - Printing 3 examples
05/31/2022 16:35:05 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/31/2022 16:35:05 - INFO - __main__ - ['refuted']
05/31/2022 16:35:05 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/31/2022 16:35:05 - INFO - __main__ - ['refuted']
05/31/2022 16:35:05 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/31/2022 16:35:05 - INFO - __main__ - ['refuted']
05/31/2022 16:35:05 - INFO - __main__ - Tokenizing Input ...
05/31/2022 16:35:05 - INFO - __main__ - Tokenizing Output ...
05/31/2022 16:35:05 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 16:35:24 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 16:35:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 16:35:25 - INFO - __main__ - Starting training!
05/31/2022 16:35:30 - INFO - __main__ - Tokenizing Output ...
05/31/2022 16:35:42 - INFO - __main__ - Loaded 12792 examples from test data
05/31/2022 16:43:56 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-24uptasks/singletask-tab_fact/tab_fact_16_21_0.2_8_predictions.txt
05/31/2022 16:43:57 - INFO - __main__ - Classification-F1 on test data: 0.0606
05/31/2022 16:43:57 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.2, bsz=8, dev_performance=0.4920634920634921, test_performance=0.06056889697663746
05/31/2022 16:43:57 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.5, bsz=8 ...
05/31/2022 16:43:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 16:43:58 - INFO - __main__ - Printing 3 examples
05/31/2022 16:43:58 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/31/2022 16:43:58 - INFO - __main__ - ['refuted']
05/31/2022 16:43:58 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/31/2022 16:43:58 - INFO - __main__ - ['refuted']
05/31/2022 16:43:58 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/31/2022 16:43:58 - INFO - __main__ - ['refuted']
05/31/2022 16:43:58 - INFO - __main__ - Tokenizing Input ...
05/31/2022 16:43:58 - INFO - __main__ - Tokenizing Output ...
05/31/2022 16:43:58 - INFO - __main__ - Loaded 32 examples from train data
05/31/2022 16:43:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/31/2022 16:43:58 - INFO - __main__ - Printing 3 examples
05/31/2022 16:43:58 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
05/31/2022 16:43:58 - INFO - __main__ - ['refuted']
05/31/2022 16:43:58 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#josé maría olazábal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
05/31/2022 16:43:58 - INFO - __main__ - ['refuted']
05/31/2022 16:43:58 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aimé anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar janković#6 april 2009#7th [n] 
05/31/2022 16:43:58 - INFO - __main__ - ['refuted']
05/31/2022 16:43:58 - INFO - __main__ - Tokenizing Input ...
05/31/2022 16:43:58 - INFO - __main__ - Tokenizing Output ...
05/31/2022 16:43:58 - INFO - __main__ - Loaded 32 examples from dev data
05/31/2022 16:44:15 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 16:44:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 16:44:15 - INFO - __main__ - Starting training!
05/31/2022 16:44:20 - INFO - __main__ - Step 10 Global step 10 Train loss 2.71 on epoch=4
05/31/2022 16:44:25 - INFO - __main__ - Step 20 Global step 20 Train loss 0.56 on epoch=9
05/31/2022 16:44:29 - INFO - __main__ - Step 30 Global step 30 Train loss 0.33 on epoch=14
05/31/2022 16:44:34 - INFO - __main__ - Step 40 Global step 40 Train loss 0.34 on epoch=19
05/31/2022 16:44:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.29 on epoch=24
05/31/2022 16:44:39 - INFO - __main__ - Global step 50 Train loss 0.85 Classification-F1 0.3333333333333333 on epoch=24
05/31/2022 16:44:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
05/31/2022 16:44:44 - INFO - __main__ - Step 60 Global step 60 Train loss 0.27 on epoch=29
05/31/2022 16:44:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.22 on epoch=34
05/31/2022 16:44:53 - INFO - __main__ - Step 80 Global step 80 Train loss 0.24 on epoch=39
05/31/2022 16:44:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.26 on epoch=44
05/31/2022 16:45:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.23 on epoch=49
05/31/2022 16:45:03 - INFO - __main__ - Global step 100 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=49
05/31/2022 16:45:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.22 on epoch=54
05/31/2022 16:45:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.23 on epoch=59
05/31/2022 16:45:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.20 on epoch=64
05/31/2022 16:45:21 - INFO - __main__ - Step 140 Global step 140 Train loss 0.20 on epoch=69
05/31/2022 16:45:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.24 on epoch=74
05/31/2022 16:45:27 - INFO - __main__ - Global step 150 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=74
05/31/2022 16:45:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.26 on epoch=79
05/31/2022 16:45:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.21 on epoch=84
05/31/2022 16:45:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.22 on epoch=89
05/31/2022 16:45:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.20 on epoch=94
05/31/2022 16:45:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.21 on epoch=99
05/31/2022 16:45:50 - INFO - __main__ - Global step 200 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=99
05/31/2022 16:45:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.18 on epoch=104
05/31/2022 16:45:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.22 on epoch=109
05/31/2022 16:46:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.18 on epoch=114
05/31/2022 16:46:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.20 on epoch=119
05/31/2022 16:46:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.18 on epoch=124
05/31/2022 16:46:14 - INFO - __main__ - Global step 250 Train loss 0.19 Classification-F1 0.3992490613266583 on epoch=124
05/31/2022 16:46:14 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=124, global_step=250
05/31/2022 16:46:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.18 on epoch=129
05/31/2022 16:46:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.16 on epoch=134
05/31/2022 16:46:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.17 on epoch=139
05/31/2022 16:46:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.17 on epoch=144
05/31/2022 16:46:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.16 on epoch=149
05/31/2022 16:46:38 - INFO - __main__ - Global step 300 Train loss 0.17 Classification-F1 0.49090909090909085 on epoch=149
05/31/2022 16:46:38 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.49090909090909085 on epoch=149, global_step=300
05/31/2022 16:46:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.15 on epoch=154
05/31/2022 16:46:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.17 on epoch=159
05/31/2022 16:46:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.17 on epoch=164
05/31/2022 16:46:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.13 on epoch=169
05/31/2022 16:47:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.16 on epoch=174
05/31/2022 16:47:01 - INFO - __main__ - Global step 350 Train loss 0.15 Classification-F1 0.4980392156862745 on epoch=174
05/31/2022 16:47:01 - INFO - __main__ - Saving model with best Classification-F1: 0.49090909090909085 -> 0.4980392156862745 on epoch=174, global_step=350
05/31/2022 16:47:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.11 on epoch=179
05/31/2022 16:47:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.14 on epoch=184
05/31/2022 16:47:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.12 on epoch=189
05/31/2022 16:47:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.15 on epoch=194
05/31/2022 16:47:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.16 on epoch=199
05/31/2022 16:47:25 - INFO - __main__ - Global step 400 Train loss 0.14 Classification-F1 0.39999999999999997 on epoch=199
05/31/2022 16:47:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=204
05/31/2022 16:47:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.13 on epoch=209
05/31/2022 16:47:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.13 on epoch=214
05/31/2022 16:47:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.13 on epoch=219
05/31/2022 16:47:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.10 on epoch=224
05/31/2022 16:47:49 - INFO - __main__ - Global step 450 Train loss 0.12 Classification-F1 0.29964912280701755 on epoch=224
05/31/2022 16:47:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.09 on epoch=229
05/31/2022 16:47:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.10 on epoch=234
05/31/2022 16:48:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.09 on epoch=239
05/31/2022 16:48:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.13 on epoch=244
05/31/2022 16:48:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.08 on epoch=249
05/31/2022 16:48:13 - INFO - __main__ - Global step 500 Train loss 0.10 Classification-F1 0.464039408866995 on epoch=249
05/31/2022 16:48:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.07 on epoch=254
05/31/2022 16:48:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.05 on epoch=259
05/31/2022 16:48:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.06 on epoch=264
05/31/2022 16:48:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=269
05/31/2022 16:48:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=274
05/31/2022 16:48:37 - INFO - __main__ - Global step 550 Train loss 0.06 Classification-F1 0.40566959921798634 on epoch=274
05/31/2022 16:48:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=279
05/31/2022 16:48:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=284
05/31/2022 16:48:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
05/31/2022 16:48:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.07 on epoch=294
05/31/2022 16:48:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=299
05/31/2022 16:49:00 - INFO - __main__ - Global step 600 Train loss 0.05 Classification-F1 0.39999999999999997 on epoch=299
05/31/2022 16:49:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.08 on epoch=304
05/31/2022 16:49:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.03 on epoch=309
05/31/2022 16:49:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
05/31/2022 16:49:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
05/31/2022 16:49:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=324
05/31/2022 16:49:24 - INFO - __main__ - Global step 650 Train loss 0.05 Classification-F1 0.34310850439882695 on epoch=324
05/31/2022 16:49:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
05/31/2022 16:49:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
05/31/2022 16:49:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
05/31/2022 16:49:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=344
05/31/2022 16:49:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
05/31/2022 16:49:47 - INFO - __main__ - Global step 700 Train loss 0.03 Classification-F1 0.2554385964912281 on epoch=349
05/31/2022 16:49:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=354
05/31/2022 16:49:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
05/31/2022 16:50:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
05/31/2022 16:50:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
05/31/2022 16:50:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
05/31/2022 16:50:11 - INFO - __main__ - Global step 750 Train loss 0.03 Classification-F1 0.2901234567901234 on epoch=374
05/31/2022 16:50:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
05/31/2022 16:50:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
05/31/2022 16:50:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
05/31/2022 16:50:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
05/31/2022 16:50:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=399
05/31/2022 16:50:34 - INFO - __main__ - Global step 800 Train loss 0.02 Classification-F1 0.4817813765182186 on epoch=399
05/31/2022 16:50:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=404
05/31/2022 16:50:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
05/31/2022 16:50:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
05/31/2022 16:50:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
05/31/2022 16:50:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
05/31/2022 16:50:58 - INFO - __main__ - Global step 850 Train loss 0.02 Classification-F1 0.4554554554554554 on epoch=424
05/31/2022 16:51:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
05/31/2022 16:51:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
05/31/2022 16:51:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
05/31/2022 16:51:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
05/31/2022 16:51:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
05/31/2022 16:51:21 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.40566959921798634 on epoch=449
05/31/2022 16:51:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
05/31/2022 16:51:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
05/31/2022 16:51:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
05/31/2022 16:51:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
05/31/2022 16:51:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
05/31/2022 16:51:45 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.25427350427350426 on epoch=474
05/31/2022 16:51:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
05/31/2022 16:51:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
05/31/2022 16:51:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
05/31/2022 16:52:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
05/31/2022 16:52:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
05/31/2022 16:52:09 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.3650793650793651 on epoch=499
05/31/2022 16:52:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
05/31/2022 16:52:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
05/31/2022 16:52:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
05/31/2022 16:52:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
05/31/2022 16:52:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
05/31/2022 16:52:32 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.30158730158730157 on epoch=524
05/31/2022 16:52:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
05/31/2022 16:52:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
05/31/2022 16:52:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
05/31/2022 16:52:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
05/31/2022 16:52:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
05/31/2022 16:52:56 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.3764102564102564 on epoch=549
05/31/2022 16:53:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
05/31/2022 16:53:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
05/31/2022 16:53:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
05/31/2022 16:53:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
05/31/2022 16:53:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
05/31/2022 16:53:19 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.25219743069641654 on epoch=574
05/31/2022 16:53:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
05/31/2022 16:53:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
05/31/2022 16:53:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
05/31/2022 16:53:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
05/31/2022 16:53:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
05/31/2022 16:53:43 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.24691358024691357 on epoch=599
05/31/2022 16:53:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
05/31/2022 16:53:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
05/31/2022 16:53:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
05/31/2022 16:54:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
05/31/2022 16:54:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
05/31/2022 16:54:07 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.22857142857142856 on epoch=624
05/31/2022 16:54:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
