05/17/2022 15:35:33 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-base-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-cls2cls/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-base', cuda='2,3')
05/17/2022 15:35:33 - INFO - __main__ - models/T5-base-ft-cls2cls/singletask-emo
05/17/2022 15:35:33 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-base-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-cls2cls/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-base', cuda='2,3')
05/17/2022 15:35:33 - INFO - __main__ - models/T5-base-ft-cls2cls/singletask-emo
05/17/2022 15:35:35 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/17/2022 15:35:35 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/17/2022 15:35:35 - INFO - __main__ - args.device: cuda:0
05/17/2022 15:35:35 - INFO - __main__ - Using 2 gpus
05/17/2022 15:35:35 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/17/2022 15:35:35 - INFO - __main__ - args.device: cuda:1
05/17/2022 15:35:35 - INFO - __main__ - Using 2 gpus
05/17/2022 15:35:35 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/17/2022 15:35:40 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.0005, bsz=8 ...
05/17/2022 15:35:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:35:41 - INFO - __main__ - Printing 3 examples
05/17/2022 15:35:41 - INFO - __main__ -  [emo] how cause yes am listening
05/17/2022 15:35:41 - INFO - __main__ - ['others']
05/17/2022 15:35:41 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/17/2022 15:35:41 - INFO - __main__ - ['others']
05/17/2022 15:35:41 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/17/2022 15:35:41 - INFO - __main__ - ['others']
05/17/2022 15:35:41 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:35:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:35:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:35:41 - INFO - __main__ - Printing 3 examples
05/17/2022 15:35:41 - INFO - __main__ -  [emo] how cause yes am listening
05/17/2022 15:35:41 - INFO - __main__ - ['others']
05/17/2022 15:35:41 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/17/2022 15:35:41 - INFO - __main__ - ['others']
05/17/2022 15:35:41 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/17/2022 15:35:41 - INFO - __main__ - ['others']
05/17/2022 15:35:41 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:35:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:35:41 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 15:35:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:35:41 - INFO - __main__ - Printing 3 examples
05/17/2022 15:35:41 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/17/2022 15:35:41 - INFO - __main__ - ['others']
05/17/2022 15:35:41 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/17/2022 15:35:41 - INFO - __main__ - ['others']
05/17/2022 15:35:41 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/17/2022 15:35:41 - INFO - __main__ - ['others']
05/17/2022 15:35:41 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:35:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:35:41 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 15:35:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:35:41 - INFO - __main__ - Printing 3 examples
05/17/2022 15:35:41 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/17/2022 15:35:41 - INFO - __main__ - ['others']
05/17/2022 15:35:41 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/17/2022 15:35:41 - INFO - __main__ - ['others']
05/17/2022 15:35:41 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/17/2022 15:35:41 - INFO - __main__ - ['others']
05/17/2022 15:35:41 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:35:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:35:41 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 15:35:41 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 15:35:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 15:35:47 - INFO - __main__ - Starting training!
05/17/2022 15:35:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 15:35:47 - INFO - __main__ - Starting training!
05/17/2022 15:35:52 - INFO - __main__ - Step 10 Global step 10 Train loss 20.773205 on epoch=2
05/17/2022 15:35:54 - INFO - __main__ - Step 20 Global step 20 Train loss 17.638365 on epoch=4
05/17/2022 15:35:57 - INFO - __main__ - Step 30 Global step 30 Train loss 8.968684 on epoch=7
05/17/2022 15:36:00 - INFO - __main__ - Step 40 Global step 40 Train loss 7.063629 on epoch=9
05/17/2022 15:36:03 - INFO - __main__ - Step 50 Global step 50 Train loss 5.807273 on epoch=12
05/17/2022 15:36:03 - INFO - __main__ - Global step 50 Train loss 12.050232 Classification-F1 0.10126582278481013 on epoch=12
05/17/2022 15:36:07 - INFO - __main__ - Step 60 Global step 60 Train loss 4.096446 on epoch=14
05/17/2022 15:36:09 - INFO - __main__ - Step 70 Global step 70 Train loss 2.909000 on epoch=17
05/17/2022 15:36:12 - INFO - __main__ - Step 80 Global step 80 Train loss 2.786402 on epoch=19
05/17/2022 15:36:15 - INFO - __main__ - Step 90 Global step 90 Train loss 2.467590 on epoch=22
05/17/2022 15:36:18 - INFO - __main__ - Step 100 Global step 100 Train loss 2.923608 on epoch=24
05/17/2022 15:36:18 - INFO - __main__ - Global step 100 Train loss 3.036609 Classification-F1 0.3689679158334058 on epoch=24
05/17/2022 15:36:22 - INFO - __main__ - Step 110 Global step 110 Train loss 2.543712 on epoch=27
05/17/2022 15:36:25 - INFO - __main__ - Step 120 Global step 120 Train loss 2.142263 on epoch=29
05/17/2022 15:36:27 - INFO - __main__ - Step 130 Global step 130 Train loss 2.037886 on epoch=32
05/17/2022 15:36:30 - INFO - __main__ - Step 140 Global step 140 Train loss 1.990813 on epoch=34
05/17/2022 15:36:33 - INFO - __main__ - Step 150 Global step 150 Train loss 1.948400 on epoch=37
05/17/2022 15:36:33 - INFO - __main__ - Global step 150 Train loss 2.132615 Classification-F1 0.26200351935646055 on epoch=37
05/17/2022 15:36:36 - INFO - __main__ - Step 160 Global step 160 Train loss 1.599749 on epoch=39
05/17/2022 15:36:38 - INFO - __main__ - Step 170 Global step 170 Train loss 1.579688 on epoch=42
05/17/2022 15:36:41 - INFO - __main__ - Step 180 Global step 180 Train loss 1.146824 on epoch=44
05/17/2022 15:36:44 - INFO - __main__ - Step 190 Global step 190 Train loss 1.371812 on epoch=47
05/17/2022 15:36:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.952161 on epoch=49
05/17/2022 15:36:47 - INFO - __main__ - Global step 200 Train loss 1.330047 Classification-F1 0.4088120254620753 on epoch=49
05/17/2022 15:36:50 - INFO - __main__ - Step 210 Global step 210 Train loss 1.576630 on epoch=52
05/17/2022 15:36:53 - INFO - __main__ - Step 220 Global step 220 Train loss 1.295952 on epoch=54
05/17/2022 15:36:55 - INFO - __main__ - Step 230 Global step 230 Train loss 1.614467 on epoch=57
05/17/2022 15:36:58 - INFO - __main__ - Step 240 Global step 240 Train loss 1.324346 on epoch=59
05/17/2022 15:37:01 - INFO - __main__ - Step 250 Global step 250 Train loss 1.188974 on epoch=62
05/17/2022 15:37:01 - INFO - __main__ - Global step 250 Train loss 1.400074 Classification-F1 0.42345938375350145 on epoch=62
05/17/2022 15:37:04 - INFO - __main__ - Step 260 Global step 260 Train loss 0.892118 on epoch=64
05/17/2022 15:37:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.935370 on epoch=67
05/17/2022 15:37:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.600462 on epoch=69
05/17/2022 15:37:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.968125 on epoch=72
05/17/2022 15:37:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.957228 on epoch=74
05/17/2022 15:37:16 - INFO - __main__ - Global step 300 Train loss 0.870661 Classification-F1 0.43804758459930876 on epoch=74
05/17/2022 15:37:19 - INFO - __main__ - Step 310 Global step 310 Train loss 1.025436 on epoch=77
05/17/2022 15:37:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.944736 on epoch=79
05/17/2022 15:37:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.881859 on epoch=82
05/17/2022 15:37:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.787918 on epoch=84
05/17/2022 15:37:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.643052 on epoch=87
05/17/2022 15:37:30 - INFO - __main__ - Global step 350 Train loss 0.856600 Classification-F1 0.6720238095238095 on epoch=87
05/17/2022 15:37:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.803563 on epoch=89
05/17/2022 15:37:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.987229 on epoch=92
05/17/2022 15:37:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.656693 on epoch=94
05/17/2022 15:37:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.836231 on epoch=97
05/17/2022 15:37:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.821148 on epoch=99
05/17/2022 15:37:45 - INFO - __main__ - Global step 400 Train loss 0.820973 Classification-F1 0.4869431643625192 on epoch=99
05/17/2022 15:37:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.741621 on epoch=102
05/17/2022 15:37:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.867610 on epoch=104
05/17/2022 15:37:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.940896 on epoch=107
05/17/2022 15:37:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.879731 on epoch=109
05/17/2022 15:37:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.918757 on epoch=112
05/17/2022 15:37:59 - INFO - __main__ - Global step 450 Train loss 0.869723 Classification-F1 0.4710115088163868 on epoch=112
05/17/2022 15:38:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.855686 on epoch=114
05/17/2022 15:38:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.871980 on epoch=117
05/17/2022 15:38:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.848426 on epoch=119
05/17/2022 15:38:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.839291 on epoch=122
05/17/2022 15:38:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.897363 on epoch=124
05/17/2022 15:38:13 - INFO - __main__ - Global step 500 Train loss 0.862549 Classification-F1 0.3439950980392157 on epoch=124
05/17/2022 15:38:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.940459 on epoch=127
05/17/2022 15:38:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.902441 on epoch=129
05/17/2022 15:38:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.950671 on epoch=132
05/17/2022 15:38:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.817526 on epoch=134
05/17/2022 15:38:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.871708 on epoch=137
05/17/2022 15:38:27 - INFO - __main__ - Global step 550 Train loss 0.896561 Classification-F1 0.3924117513740155 on epoch=137
05/17/2022 15:38:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.858088 on epoch=139
05/17/2022 15:38:32 - INFO - __main__ - Step 570 Global step 570 Train loss 1.084537 on epoch=142
05/17/2022 15:38:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.944616 on epoch=144
05/17/2022 15:38:38 - INFO - __main__ - Step 590 Global step 590 Train loss 1.011671 on epoch=147
05/17/2022 15:38:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.839114 on epoch=149
05/17/2022 15:38:41 - INFO - __main__ - Global step 600 Train loss 0.947605 Classification-F1 0.353828197945845 on epoch=149
05/17/2022 15:38:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.813112 on epoch=152
05/17/2022 15:38:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.891355 on epoch=154
05/17/2022 15:38:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.703770 on epoch=157
05/17/2022 15:38:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.815002 on epoch=159
05/17/2022 15:38:55 - INFO - __main__ - Step 650 Global step 650 Train loss 1.124581 on epoch=162
05/17/2022 15:38:55 - INFO - __main__ - Global step 650 Train loss 0.869564 Classification-F1 0.4647783251231527 on epoch=162
05/17/2022 15:38:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.867812 on epoch=164
05/17/2022 15:39:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.825773 on epoch=167
05/17/2022 15:39:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.857322 on epoch=169
05/17/2022 15:39:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.785881 on epoch=172
05/17/2022 15:39:09 - INFO - __main__ - Step 700 Global step 700 Train loss 1.135136 on epoch=174
05/17/2022 15:39:09 - INFO - __main__ - Global step 700 Train loss 0.894385 Classification-F1 0.2758714596949891 on epoch=174
05/17/2022 15:39:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.831414 on epoch=177
05/17/2022 15:39:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.847032 on epoch=179
05/17/2022 15:39:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.781777 on epoch=182
05/17/2022 15:39:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.863185 on epoch=184
05/17/2022 15:39:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.872231 on epoch=187
05/17/2022 15:39:23 - INFO - __main__ - Global step 750 Train loss 0.839128 Classification-F1 0.46234633153237803 on epoch=187
05/17/2022 15:39:26 - INFO - __main__ - Step 760 Global step 760 Train loss 1.954647 on epoch=189
05/17/2022 15:39:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.903745 on epoch=192
05/17/2022 15:39:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.642732 on epoch=194
05/17/2022 15:39:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.644615 on epoch=197
05/17/2022 15:39:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.578815 on epoch=199
05/17/2022 15:39:38 - INFO - __main__ - Global step 800 Train loss 0.944911 Classification-F1 0.43039215686274507 on epoch=199
05/17/2022 15:39:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.727308 on epoch=202
05/17/2022 15:39:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.782233 on epoch=204
05/17/2022 15:39:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.958131 on epoch=207
05/17/2022 15:39:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.690746 on epoch=209
05/17/2022 15:39:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.659354 on epoch=212
05/17/2022 15:39:52 - INFO - __main__ - Global step 850 Train loss 0.763554 Classification-F1 0.4620394528779253 on epoch=212
05/17/2022 15:39:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.696317 on epoch=214
05/17/2022 15:39:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.703953 on epoch=217
05/17/2022 15:40:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.764919 on epoch=219
05/17/2022 15:40:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.826507 on epoch=222
05/17/2022 15:40:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.678240 on epoch=224
05/17/2022 15:40:05 - INFO - __main__ - Global step 900 Train loss 0.733987 Classification-F1 0.3516852770885029 on epoch=224
05/17/2022 15:40:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.747932 on epoch=227
05/17/2022 15:40:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.770914 on epoch=229
05/17/2022 15:40:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.786116 on epoch=232
05/17/2022 15:40:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.683630 on epoch=234
05/17/2022 15:40:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.736343 on epoch=237
05/17/2022 15:40:20 - INFO - __main__ - Global step 950 Train loss 0.744987 Classification-F1 0.4370494417862839 on epoch=237
05/17/2022 15:40:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.566048 on epoch=239
05/17/2022 15:40:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.702408 on epoch=242
05/17/2022 15:40:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.677222 on epoch=244
05/17/2022 15:40:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.635359 on epoch=247
05/17/2022 15:40:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.692134 on epoch=249
05/17/2022 15:40:34 - INFO - __main__ - Global step 1000 Train loss 0.654634 Classification-F1 0.4626018170426065 on epoch=249
05/17/2022 15:40:34 - INFO - __main__ - save last model!
05/17/2022 15:40:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:40:35 - INFO - __main__ - Printing 3 examples
05/17/2022 15:40:35 - INFO - __main__ -  [emo] how cause yes am listening
05/17/2022 15:40:35 - INFO - __main__ - ['others']
05/17/2022 15:40:35 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/17/2022 15:40:35 - INFO - __main__ - ['others']
05/17/2022 15:40:35 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/17/2022 15:40:35 - INFO - __main__ - ['others']
05/17/2022 15:40:35 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:40:35 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:40:35 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 15:40:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:40:35 - INFO - __main__ - Printing 3 examples
05/17/2022 15:40:35 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/17/2022 15:40:35 - INFO - __main__ - ['others']
05/17/2022 15:40:35 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/17/2022 15:40:35 - INFO - __main__ - ['others']
05/17/2022 15:40:35 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/17/2022 15:40:35 - INFO - __main__ - ['others']
05/17/2022 15:40:35 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:40:35 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:40:35 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 15:40:38 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 15:40:38 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 15:40:38 - INFO - __main__ - Printing 3 examples
05/17/2022 15:40:38 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 15:40:38 - INFO - __main__ - ['others']
05/17/2022 15:40:38 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 15:40:38 - INFO - __main__ - ['others']
05/17/2022 15:40:38 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 15:40:38 - INFO - __main__ - ['others']
05/17/2022 15:40:38 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:40:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 15:40:40 - INFO - __main__ - Starting training!
05/17/2022 15:40:40 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:40:47 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 15:41:17 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_100_0.0005_8_predictions.txt
05/17/2022 15:41:17 - INFO - __main__ - Classification-F1 on test data: 0.3672
05/17/2022 15:41:17 - INFO - __main__ - prefix=emo_16_100, lr=0.0005, bsz=8, dev_performance=0.6720238095238095, test_performance=0.3672400477796371
05/17/2022 15:41:17 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.0003, bsz=8 ...
05/17/2022 15:41:18 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:41:18 - INFO - __main__ - Printing 3 examples
05/17/2022 15:41:18 - INFO - __main__ -  [emo] how cause yes am listening
05/17/2022 15:41:18 - INFO - __main__ - ['others']
05/17/2022 15:41:18 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/17/2022 15:41:18 - INFO - __main__ - ['others']
05/17/2022 15:41:18 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/17/2022 15:41:18 - INFO - __main__ - ['others']
05/17/2022 15:41:18 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:41:18 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:41:18 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 15:41:18 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:41:18 - INFO - __main__ - Printing 3 examples
05/17/2022 15:41:18 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/17/2022 15:41:18 - INFO - __main__ - ['others']
05/17/2022 15:41:18 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/17/2022 15:41:18 - INFO - __main__ - ['others']
05/17/2022 15:41:18 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/17/2022 15:41:18 - INFO - __main__ - ['others']
05/17/2022 15:41:18 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:41:18 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:41:18 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 15:41:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 15:41:23 - INFO - __main__ - Starting training!
05/17/2022 15:41:25 - INFO - __main__ - Step 10 Global step 10 Train loss 20.435167 on epoch=2
05/17/2022 15:41:27 - INFO - __main__ - Step 20 Global step 20 Train loss 16.308514 on epoch=4
05/17/2022 15:41:31 - INFO - __main__ - Step 30 Global step 30 Train loss 11.064670 on epoch=7
05/17/2022 15:41:33 - INFO - __main__ - Step 40 Global step 40 Train loss 8.008345 on epoch=9
05/17/2022 15:41:36 - INFO - __main__ - Step 50 Global step 50 Train loss 7.598494 on epoch=12
05/17/2022 15:41:41 - INFO - __main__ - Global step 50 Train loss 12.683039 Classification-F1 0.024156371012354787 on epoch=12
05/17/2022 15:41:45 - INFO - __main__ - Step 60 Global step 60 Train loss 5.452511 on epoch=14
05/17/2022 15:41:47 - INFO - __main__ - Step 70 Global step 70 Train loss 5.493804 on epoch=17
05/17/2022 15:41:50 - INFO - __main__ - Step 80 Global step 80 Train loss 3.623557 on epoch=19
05/17/2022 15:41:53 - INFO - __main__ - Step 90 Global step 90 Train loss 3.649139 on epoch=22
05/17/2022 15:41:56 - INFO - __main__ - Step 100 Global step 100 Train loss 3.424002 on epoch=24
05/17/2022 15:41:56 - INFO - __main__ - Global step 100 Train loss 4.328603 Classification-F1 0.1 on epoch=24
05/17/2022 15:41:59 - INFO - __main__ - Step 110 Global step 110 Train loss 3.268026 on epoch=27
05/17/2022 15:42:02 - INFO - __main__ - Step 120 Global step 120 Train loss 2.669455 on epoch=29
05/17/2022 15:42:05 - INFO - __main__ - Step 130 Global step 130 Train loss 3.265578 on epoch=32
05/17/2022 15:42:08 - INFO - __main__ - Step 140 Global step 140 Train loss 2.939521 on epoch=34
05/17/2022 15:42:10 - INFO - __main__ - Step 150 Global step 150 Train loss 2.548535 on epoch=37
05/17/2022 15:42:11 - INFO - __main__ - Global step 150 Train loss 2.938223 Classification-F1 0.19549039373814042 on epoch=37
05/17/2022 15:42:14 - INFO - __main__ - Step 160 Global step 160 Train loss 2.276872 on epoch=39
05/17/2022 15:42:17 - INFO - __main__ - Step 170 Global step 170 Train loss 2.465389 on epoch=42
05/17/2022 15:42:20 - INFO - __main__ - Step 180 Global step 180 Train loss 2.216686 on epoch=44
05/17/2022 15:42:23 - INFO - __main__ - Step 190 Global step 190 Train loss 1.934215 on epoch=47
05/17/2022 15:42:25 - INFO - __main__ - Step 200 Global step 200 Train loss 1.993251 on epoch=49
05/17/2022 15:42:26 - INFO - __main__ - Global step 200 Train loss 2.177283 Classification-F1 0.17121826861001552 on epoch=49
05/17/2022 15:42:29 - INFO - __main__ - Step 210 Global step 210 Train loss 1.390764 on epoch=52
05/17/2022 15:42:32 - INFO - __main__ - Step 220 Global step 220 Train loss 1.267628 on epoch=54
05/17/2022 15:42:35 - INFO - __main__ - Step 230 Global step 230 Train loss 1.194098 on epoch=57
05/17/2022 15:42:38 - INFO - __main__ - Step 240 Global step 240 Train loss 1.221617 on epoch=59
05/17/2022 15:42:40 - INFO - __main__ - Step 250 Global step 250 Train loss 1.380869 on epoch=62
05/17/2022 15:42:41 - INFO - __main__ - Global step 250 Train loss 1.290995 Classification-F1 0.3165635131152372 on epoch=62
05/17/2022 15:42:44 - INFO - __main__ - Step 260 Global step 260 Train loss 1.699731 on epoch=64
05/17/2022 15:42:47 - INFO - __main__ - Step 270 Global step 270 Train loss 1.635709 on epoch=67
05/17/2022 15:42:49 - INFO - __main__ - Step 280 Global step 280 Train loss 1.341788 on epoch=69
05/17/2022 15:42:52 - INFO - __main__ - Step 290 Global step 290 Train loss 1.002973 on epoch=72
05/17/2022 15:42:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.938586 on epoch=74
05/17/2022 15:42:55 - INFO - __main__ - Global step 300 Train loss 1.323758 Classification-F1 0.39166666666666666 on epoch=74
05/17/2022 15:42:59 - INFO - __main__ - Step 310 Global step 310 Train loss 1.128104 on epoch=77
05/17/2022 15:43:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.968421 on epoch=79
05/17/2022 15:43:04 - INFO - __main__ - Step 330 Global step 330 Train loss 1.157192 on epoch=82
05/17/2022 15:43:07 - INFO - __main__ - Step 340 Global step 340 Train loss 1.008624 on epoch=84
05/17/2022 15:43:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.971791 on epoch=87
05/17/2022 15:43:10 - INFO - __main__ - Global step 350 Train loss 1.046826 Classification-F1 0.3427725531173807 on epoch=87
05/17/2022 15:43:13 - INFO - __main__ - Step 360 Global step 360 Train loss 1.115648 on epoch=89
05/17/2022 15:43:16 - INFO - __main__ - Step 370 Global step 370 Train loss 1.282645 on epoch=92
05/17/2022 15:43:19 - INFO - __main__ - Step 380 Global step 380 Train loss 1.130568 on epoch=94
05/17/2022 15:43:21 - INFO - __main__ - Step 390 Global step 390 Train loss 1.008206 on epoch=97
05/17/2022 15:43:24 - INFO - __main__ - Step 400 Global step 400 Train loss 1.103175 on epoch=99
05/17/2022 15:43:25 - INFO - __main__ - Global step 400 Train loss 1.128048 Classification-F1 0.26938405797101445 on epoch=99
05/17/2022 15:43:27 - INFO - __main__ - Step 410 Global step 410 Train loss 1.203130 on epoch=102
05/17/2022 15:43:30 - INFO - __main__ - Step 420 Global step 420 Train loss 2.459415 on epoch=104
05/17/2022 15:43:33 - INFO - __main__ - Step 430 Global step 430 Train loss 1.697370 on epoch=107
05/17/2022 15:43:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.958219 on epoch=109
05/17/2022 15:43:39 - INFO - __main__ - Step 450 Global step 450 Train loss 1.103411 on epoch=112
05/17/2022 15:43:39 - INFO - __main__ - Global step 450 Train loss 1.484309 Classification-F1 0.263536409516943 on epoch=112
05/17/2022 15:43:42 - INFO - __main__ - Step 460 Global step 460 Train loss 1.394055 on epoch=114
05/17/2022 15:43:45 - INFO - __main__ - Step 470 Global step 470 Train loss 1.199902 on epoch=117
05/17/2022 15:43:47 - INFO - __main__ - Step 480 Global step 480 Train loss 1.061445 on epoch=119
05/17/2022 15:43:50 - INFO - __main__ - Step 490 Global step 490 Train loss 1.025277 on epoch=122
05/17/2022 15:43:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.949494 on epoch=124
05/17/2022 15:43:53 - INFO - __main__ - Global step 500 Train loss 1.126034 Classification-F1 0.3574660633484163 on epoch=124
05/17/2022 15:43:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.981453 on epoch=127
05/17/2022 15:43:59 - INFO - __main__ - Step 520 Global step 520 Train loss 1.121495 on epoch=129
05/17/2022 15:44:01 - INFO - __main__ - Step 530 Global step 530 Train loss 1.047223 on epoch=132
05/17/2022 15:44:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.905312 on epoch=134
05/17/2022 15:44:07 - INFO - __main__ - Step 550 Global step 550 Train loss 1.179888 on epoch=137
05/17/2022 15:44:08 - INFO - __main__ - Global step 550 Train loss 1.047074 Classification-F1 0.31874891887216744 on epoch=137
05/17/2022 15:44:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.976114 on epoch=139
05/17/2022 15:44:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.988243 on epoch=142
05/17/2022 15:44:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.972224 on epoch=144
05/17/2022 15:44:19 - INFO - __main__ - Step 590 Global step 590 Train loss 1.082803 on epoch=147
05/17/2022 15:44:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.965419 on epoch=149
05/17/2022 15:44:22 - INFO - __main__ - Global step 600 Train loss 0.996960 Classification-F1 0.1 on epoch=149
05/17/2022 15:44:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.977553 on epoch=152
05/17/2022 15:44:27 - INFO - __main__ - Step 620 Global step 620 Train loss 1.073538 on epoch=154
05/17/2022 15:44:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.879903 on epoch=157
05/17/2022 15:44:33 - INFO - __main__ - Step 640 Global step 640 Train loss 1.045228 on epoch=159
05/17/2022 15:44:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.992473 on epoch=162
05/17/2022 15:44:36 - INFO - __main__ - Global step 650 Train loss 0.993739 Classification-F1 0.09493670886075949 on epoch=162
05/17/2022 15:44:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.942459 on epoch=164
05/17/2022 15:44:41 - INFO - __main__ - Step 670 Global step 670 Train loss 1.004328 on epoch=167
05/17/2022 15:44:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.951362 on epoch=169
05/17/2022 15:44:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.883623 on epoch=172
05/17/2022 15:44:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.878224 on epoch=174
05/17/2022 15:44:50 - INFO - __main__ - Global step 700 Train loss 0.931999 Classification-F1 0.1715909090909091 on epoch=174
05/17/2022 15:44:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.928336 on epoch=177
05/17/2022 15:44:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.827314 on epoch=179
05/17/2022 15:44:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.843218 on epoch=182
05/17/2022 15:45:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.767862 on epoch=184
05/17/2022 15:45:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.875381 on epoch=187
05/17/2022 15:45:03 - INFO - __main__ - Global step 750 Train loss 0.848422 Classification-F1 0.13067758749069247 on epoch=187
05/17/2022 15:45:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.862911 on epoch=189
05/17/2022 15:45:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.886455 on epoch=192
05/17/2022 15:45:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.767799 on epoch=194
05/17/2022 15:45:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.780071 on epoch=197
05/17/2022 15:45:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.873814 on epoch=199
05/17/2022 15:45:18 - INFO - __main__ - Global step 800 Train loss 0.834210 Classification-F1 0.18553782810087494 on epoch=199
05/17/2022 15:45:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.828898 on epoch=202
05/17/2022 15:45:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.776028 on epoch=204
05/17/2022 15:45:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.801151 on epoch=207
05/17/2022 15:45:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.794572 on epoch=209
05/17/2022 15:45:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.914632 on epoch=212
05/17/2022 15:45:31 - INFO - __main__ - Global step 850 Train loss 0.823056 Classification-F1 0.22027972027972026 on epoch=212
05/17/2022 15:45:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.833477 on epoch=214
05/17/2022 15:45:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.808599 on epoch=217
05/17/2022 15:45:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.771296 on epoch=219
05/17/2022 15:45:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.852666 on epoch=222
05/17/2022 15:45:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.910130 on epoch=224
05/17/2022 15:45:45 - INFO - __main__ - Global step 900 Train loss 0.835234 Classification-F1 0.306551116333725 on epoch=224
05/17/2022 15:45:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.762564 on epoch=227
05/17/2022 15:45:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.850705 on epoch=229
05/17/2022 15:45:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.698893 on epoch=232
05/17/2022 15:45:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.884143 on epoch=234
05/17/2022 15:45:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.745389 on epoch=237
05/17/2022 15:45:59 - INFO - __main__ - Global step 950 Train loss 0.788339 Classification-F1 0.20984848484848484 on epoch=237
05/17/2022 15:46:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.844979 on epoch=239
05/17/2022 15:46:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.826276 on epoch=242
05/17/2022 15:46:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.835455 on epoch=244
05/17/2022 15:46:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.768502 on epoch=247
05/17/2022 15:46:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.795982 on epoch=249
05/17/2022 15:46:13 - INFO - __main__ - Global step 1000 Train loss 0.814239 Classification-F1 0.3700510855683269 on epoch=249
05/17/2022 15:46:13 - INFO - __main__ - save last model!
05/17/2022 15:46:13 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:46:13 - INFO - __main__ - Printing 3 examples
05/17/2022 15:46:13 - INFO - __main__ -  [emo] how cause yes am listening
05/17/2022 15:46:13 - INFO - __main__ - ['others']
05/17/2022 15:46:13 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/17/2022 15:46:13 - INFO - __main__ - ['others']
05/17/2022 15:46:13 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/17/2022 15:46:13 - INFO - __main__ - ['others']
05/17/2022 15:46:13 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:46:13 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:46:13 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 15:46:13 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:46:13 - INFO - __main__ - Printing 3 examples
05/17/2022 15:46:13 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/17/2022 15:46:13 - INFO - __main__ - ['others']
05/17/2022 15:46:13 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/17/2022 15:46:13 - INFO - __main__ - ['others']
05/17/2022 15:46:13 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/17/2022 15:46:13 - INFO - __main__ - ['others']
05/17/2022 15:46:13 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:46:14 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:46:14 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 15:46:16 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 15:46:16 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 15:46:16 - INFO - __main__ - Printing 3 examples
05/17/2022 15:46:16 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 15:46:16 - INFO - __main__ - ['others']
05/17/2022 15:46:16 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 15:46:16 - INFO - __main__ - ['others']
05/17/2022 15:46:16 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 15:46:16 - INFO - __main__ - ['others']
05/17/2022 15:46:16 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:46:18 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:46:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 15:46:18 - INFO - __main__ - Starting training!
05/17/2022 15:46:23 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 15:46:52 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_100_0.0003_8_predictions.txt
05/17/2022 15:46:52 - INFO - __main__ - Classification-F1 on test data: 0.1194
05/17/2022 15:46:52 - INFO - __main__ - prefix=emo_16_100, lr=0.0003, bsz=8, dev_performance=0.39166666666666666, test_performance=0.11942601056055605
05/17/2022 15:46:52 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.0002, bsz=8 ...
05/17/2022 15:46:53 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:46:53 - INFO - __main__ - Printing 3 examples
05/17/2022 15:46:53 - INFO - __main__ -  [emo] how cause yes am listening
05/17/2022 15:46:53 - INFO - __main__ - ['others']
05/17/2022 15:46:53 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/17/2022 15:46:53 - INFO - __main__ - ['others']
05/17/2022 15:46:53 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/17/2022 15:46:53 - INFO - __main__ - ['others']
05/17/2022 15:46:53 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:46:53 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:46:53 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 15:46:53 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:46:53 - INFO - __main__ - Printing 3 examples
05/17/2022 15:46:53 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/17/2022 15:46:53 - INFO - __main__ - ['others']
05/17/2022 15:46:53 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/17/2022 15:46:53 - INFO - __main__ - ['others']
05/17/2022 15:46:53 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/17/2022 15:46:53 - INFO - __main__ - ['others']
05/17/2022 15:46:53 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:46:53 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:46:53 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 15:46:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 15:46:58 - INFO - __main__ - Starting training!
05/17/2022 15:47:00 - INFO - __main__ - Step 10 Global step 10 Train loss 21.215363 on epoch=2
05/17/2022 15:47:02 - INFO - __main__ - Step 20 Global step 20 Train loss 19.331234 on epoch=4
05/17/2022 15:47:05 - INFO - __main__ - Step 30 Global step 30 Train loss 13.659596 on epoch=7
05/17/2022 15:47:08 - INFO - __main__ - Step 40 Global step 40 Train loss 10.569978 on epoch=9
05/17/2022 15:47:10 - INFO - __main__ - Step 50 Global step 50 Train loss 8.206144 on epoch=12
05/17/2022 15:47:17 - INFO - __main__ - Global step 50 Train loss 14.596462 Classification-F1 0.001755926251097454 on epoch=12
05/17/2022 15:47:21 - INFO - __main__ - Step 60 Global step 60 Train loss 6.757800 on epoch=14
05/17/2022 15:47:23 - INFO - __main__ - Step 70 Global step 70 Train loss 6.683671 on epoch=17
05/17/2022 15:47:26 - INFO - __main__ - Step 80 Global step 80 Train loss 5.125577 on epoch=19
05/17/2022 15:47:29 - INFO - __main__ - Step 90 Global step 90 Train loss 4.968767 on epoch=22
05/17/2022 15:47:31 - INFO - __main__ - Step 100 Global step 100 Train loss 5.473411 on epoch=24
05/17/2022 15:47:32 - INFO - __main__ - Global step 100 Train loss 5.801846 Classification-F1 0.1 on epoch=24
05/17/2022 15:47:35 - INFO - __main__ - Step 110 Global step 110 Train loss 3.898135 on epoch=27
05/17/2022 15:47:38 - INFO - __main__ - Step 120 Global step 120 Train loss 3.842667 on epoch=29
05/17/2022 15:47:41 - INFO - __main__ - Step 130 Global step 130 Train loss 3.830017 on epoch=32
05/17/2022 15:47:44 - INFO - __main__ - Step 140 Global step 140 Train loss 3.775826 on epoch=34
05/17/2022 15:47:46 - INFO - __main__ - Step 150 Global step 150 Train loss 3.141131 on epoch=37
05/17/2022 15:47:47 - INFO - __main__ - Global step 150 Train loss 3.697555 Classification-F1 0.1 on epoch=37
05/17/2022 15:47:49 - INFO - __main__ - Step 160 Global step 160 Train loss 2.193615 on epoch=39
05/17/2022 15:47:52 - INFO - __main__ - Step 170 Global step 170 Train loss 2.666981 on epoch=42
05/17/2022 15:47:55 - INFO - __main__ - Step 180 Global step 180 Train loss 2.526781 on epoch=44
05/17/2022 15:47:58 - INFO - __main__ - Step 190 Global step 190 Train loss 2.292360 on epoch=47
05/17/2022 15:48:00 - INFO - __main__ - Step 200 Global step 200 Train loss 3.127834 on epoch=49
05/17/2022 15:48:01 - INFO - __main__ - Global step 200 Train loss 2.561514 Classification-F1 0.23810046406622348 on epoch=49
05/17/2022 15:48:04 - INFO - __main__ - Step 210 Global step 210 Train loss 2.836284 on epoch=52
05/17/2022 15:48:07 - INFO - __main__ - Step 220 Global step 220 Train loss 2.195101 on epoch=54
05/17/2022 15:48:09 - INFO - __main__ - Step 230 Global step 230 Train loss 1.783296 on epoch=57
05/17/2022 15:48:12 - INFO - __main__ - Step 240 Global step 240 Train loss 2.007967 on epoch=59
05/17/2022 15:48:15 - INFO - __main__ - Step 250 Global step 250 Train loss 1.973439 on epoch=62
05/17/2022 15:48:15 - INFO - __main__ - Global step 250 Train loss 2.159217 Classification-F1 0.45533671134357634 on epoch=62
05/17/2022 15:48:18 - INFO - __main__ - Step 260 Global step 260 Train loss 1.789279 on epoch=64
05/17/2022 15:48:21 - INFO - __main__ - Step 270 Global step 270 Train loss 1.991658 on epoch=67
05/17/2022 15:48:24 - INFO - __main__ - Step 280 Global step 280 Train loss 1.644307 on epoch=69
05/17/2022 15:48:26 - INFO - __main__ - Step 290 Global step 290 Train loss 1.721527 on epoch=72
05/17/2022 15:48:29 - INFO - __main__ - Step 300 Global step 300 Train loss 1.862239 on epoch=74
05/17/2022 15:48:30 - INFO - __main__ - Global step 300 Train loss 1.801802 Classification-F1 0.4574162679425838 on epoch=74
05/17/2022 15:48:33 - INFO - __main__ - Step 310 Global step 310 Train loss 1.343722 on epoch=77
05/17/2022 15:48:36 - INFO - __main__ - Step 320 Global step 320 Train loss 1.295940 on epoch=79
05/17/2022 15:48:39 - INFO - __main__ - Step 330 Global step 330 Train loss 1.707178 on epoch=82
05/17/2022 15:48:41 - INFO - __main__ - Step 340 Global step 340 Train loss 1.550031 on epoch=84
05/17/2022 15:48:44 - INFO - __main__ - Step 350 Global step 350 Train loss 1.367387 on epoch=87
05/17/2022 15:48:45 - INFO - __main__ - Global step 350 Train loss 1.452852 Classification-F1 0.46857829670329676 on epoch=87
05/17/2022 15:48:48 - INFO - __main__ - Step 360 Global step 360 Train loss 1.500673 on epoch=89
05/17/2022 15:48:50 - INFO - __main__ - Step 370 Global step 370 Train loss 1.377626 on epoch=92
05/17/2022 15:48:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.980018 on epoch=94
05/17/2022 15:48:56 - INFO - __main__ - Step 390 Global step 390 Train loss 1.224490 on epoch=97
05/17/2022 15:48:59 - INFO - __main__ - Step 400 Global step 400 Train loss 1.333172 on epoch=99
05/17/2022 15:48:59 - INFO - __main__ - Global step 400 Train loss 1.283196 Classification-F1 0.49871299871299873 on epoch=99
05/17/2022 15:49:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.725817 on epoch=102
05/17/2022 15:49:05 - INFO - __main__ - Step 420 Global step 420 Train loss 1.178893 on epoch=104
05/17/2022 15:49:08 - INFO - __main__ - Step 430 Global step 430 Train loss 1.227191 on epoch=107
05/17/2022 15:49:10 - INFO - __main__ - Step 440 Global step 440 Train loss 1.305588 on epoch=109
05/17/2022 15:49:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.865556 on epoch=112
05/17/2022 15:49:14 - INFO - __main__ - Global step 450 Train loss 1.060609 Classification-F1 0.5357142857142857 on epoch=112
05/17/2022 15:49:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.953958 on epoch=114
05/17/2022 15:49:20 - INFO - __main__ - Step 470 Global step 470 Train loss 1.197086 on epoch=117
05/17/2022 15:49:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.812393 on epoch=119
05/17/2022 15:49:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.938927 on epoch=122
05/17/2022 15:49:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.788455 on epoch=124
05/17/2022 15:49:28 - INFO - __main__ - Global step 500 Train loss 0.938164 Classification-F1 0.5470704948646125 on epoch=124
05/17/2022 15:49:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.793105 on epoch=127
05/17/2022 15:49:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.763177 on epoch=129
05/17/2022 15:49:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.897743 on epoch=132
05/17/2022 15:49:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.781257 on epoch=134
05/17/2022 15:49:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.767345 on epoch=137
05/17/2022 15:49:43 - INFO - __main__ - Global step 550 Train loss 0.800525 Classification-F1 0.6956890659250005 on epoch=137
05/17/2022 15:49:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.740485 on epoch=139
05/17/2022 15:49:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.629077 on epoch=142
05/17/2022 15:49:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.894552 on epoch=144
05/17/2022 15:49:55 - INFO - __main__ - Step 590 Global step 590 Train loss 1.383237 on epoch=147
05/17/2022 15:49:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.880542 on epoch=149
05/17/2022 15:49:58 - INFO - __main__ - Global step 600 Train loss 0.905579 Classification-F1 0.4967144563918758 on epoch=149
05/17/2022 15:50:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.937278 on epoch=152
05/17/2022 15:50:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.800519 on epoch=154
05/17/2022 15:50:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.761558 on epoch=157
05/17/2022 15:50:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.772851 on epoch=159
05/17/2022 15:50:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.610741 on epoch=162
05/17/2022 15:50:12 - INFO - __main__ - Global step 650 Train loss 0.776590 Classification-F1 0.6170435653936372 on epoch=162
05/17/2022 15:50:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.493485 on epoch=164
05/17/2022 15:50:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.672298 on epoch=167
05/17/2022 15:50:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.564774 on epoch=169
05/17/2022 15:50:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.652070 on epoch=172
05/17/2022 15:50:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.457420 on epoch=174
05/17/2022 15:50:27 - INFO - __main__ - Global step 700 Train loss 0.568009 Classification-F1 0.7073328482496564 on epoch=174
05/17/2022 15:50:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.458969 on epoch=177
05/17/2022 15:50:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.496220 on epoch=179
05/17/2022 15:50:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.628055 on epoch=182
05/17/2022 15:50:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.437961 on epoch=184
05/17/2022 15:50:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.384391 on epoch=187
05/17/2022 15:50:41 - INFO - __main__ - Global step 750 Train loss 0.481119 Classification-F1 0.7417238667238667 on epoch=187
05/17/2022 15:50:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.543544 on epoch=189
05/17/2022 15:50:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.406882 on epoch=192
05/17/2022 15:50:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.383038 on epoch=194
05/17/2022 15:50:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.550818 on epoch=197
05/17/2022 15:50:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.283262 on epoch=199
05/17/2022 15:50:56 - INFO - __main__ - Global step 800 Train loss 0.433509 Classification-F1 0.7220429450030968 on epoch=199
05/17/2022 15:50:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.422970 on epoch=202
05/17/2022 15:51:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.319852 on epoch=204
05/17/2022 15:51:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.444568 on epoch=207
05/17/2022 15:51:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.367215 on epoch=209
05/17/2022 15:51:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.562926 on epoch=212
05/17/2022 15:51:10 - INFO - __main__ - Global step 850 Train loss 0.423506 Classification-F1 0.7368939304423175 on epoch=212
05/17/2022 15:51:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.413248 on epoch=214
05/17/2022 15:51:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.535912 on epoch=217
05/17/2022 15:51:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.483377 on epoch=219
05/17/2022 15:51:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.311038 on epoch=222
05/17/2022 15:51:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.527386 on epoch=224
05/17/2022 15:51:24 - INFO - __main__ - Global step 900 Train loss 0.454192 Classification-F1 0.7181235166388931 on epoch=224
05/17/2022 15:51:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.545596 on epoch=227
05/17/2022 15:51:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.484429 on epoch=229
05/17/2022 15:51:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.307622 on epoch=232
05/17/2022 15:51:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.404869 on epoch=234
05/17/2022 15:51:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.647474 on epoch=237
05/17/2022 15:51:39 - INFO - __main__ - Global step 950 Train loss 0.477998 Classification-F1 0.7338850957364282 on epoch=237
05/17/2022 15:51:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.356177 on epoch=239
05/17/2022 15:51:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.387155 on epoch=242
05/17/2022 15:51:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.332013 on epoch=244
05/17/2022 15:51:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.436656 on epoch=247
05/17/2022 15:51:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.262828 on epoch=249
05/17/2022 15:51:53 - INFO - __main__ - Global step 1000 Train loss 0.354966 Classification-F1 0.7211822660098522 on epoch=249
05/17/2022 15:51:53 - INFO - __main__ - save last model!
05/17/2022 15:51:53 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:51:53 - INFO - __main__ - Printing 3 examples
05/17/2022 15:51:53 - INFO - __main__ -  [emo] how cause yes am listening
05/17/2022 15:51:53 - INFO - __main__ - ['others']
05/17/2022 15:51:53 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/17/2022 15:51:53 - INFO - __main__ - ['others']
05/17/2022 15:51:53 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/17/2022 15:51:53 - INFO - __main__ - ['others']
05/17/2022 15:51:53 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:51:53 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:51:54 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 15:51:54 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:51:54 - INFO - __main__ - Printing 3 examples
05/17/2022 15:51:54 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/17/2022 15:51:54 - INFO - __main__ - ['others']
05/17/2022 15:51:54 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/17/2022 15:51:54 - INFO - __main__ - ['others']
05/17/2022 15:51:54 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/17/2022 15:51:54 - INFO - __main__ - ['others']
05/17/2022 15:51:54 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:51:54 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:51:54 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 15:51:56 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 15:51:56 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 15:51:56 - INFO - __main__ - Printing 3 examples
05/17/2022 15:51:56 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 15:51:56 - INFO - __main__ - ['others']
05/17/2022 15:51:56 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 15:51:56 - INFO - __main__ - ['others']
05/17/2022 15:51:56 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 15:51:56 - INFO - __main__ - ['others']
05/17/2022 15:51:56 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:51:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 15:51:57 - INFO - __main__ - Starting training!
05/17/2022 15:51:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:52:04 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 15:52:32 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_100_0.0002_8_predictions.txt
05/17/2022 15:52:32 - INFO - __main__ - Classification-F1 on test data: 0.4291
05/17/2022 15:52:32 - INFO - __main__ - prefix=emo_16_100, lr=0.0002, bsz=8, dev_performance=0.7417238667238667, test_performance=0.4291332251677281
05/17/2022 15:52:33 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.0001, bsz=8 ...
05/17/2022 15:52:33 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:52:33 - INFO - __main__ - Printing 3 examples
05/17/2022 15:52:33 - INFO - __main__ -  [emo] how cause yes am listening
05/17/2022 15:52:33 - INFO - __main__ - ['others']
05/17/2022 15:52:33 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/17/2022 15:52:33 - INFO - __main__ - ['others']
05/17/2022 15:52:33 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/17/2022 15:52:33 - INFO - __main__ - ['others']
05/17/2022 15:52:33 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:52:33 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:52:34 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 15:52:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:52:34 - INFO - __main__ - Printing 3 examples
05/17/2022 15:52:34 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/17/2022 15:52:34 - INFO - __main__ - ['others']
05/17/2022 15:52:34 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/17/2022 15:52:34 - INFO - __main__ - ['others']
05/17/2022 15:52:34 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/17/2022 15:52:34 - INFO - __main__ - ['others']
05/17/2022 15:52:34 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:52:34 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:52:34 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 15:52:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 15:52:38 - INFO - __main__ - Starting training!
05/17/2022 15:52:41 - INFO - __main__ - Step 10 Global step 10 Train loss 20.721483 on epoch=2
05/17/2022 15:52:43 - INFO - __main__ - Step 20 Global step 20 Train loss 19.300869 on epoch=4
05/17/2022 15:52:46 - INFO - __main__ - Step 30 Global step 30 Train loss 16.478638 on epoch=7
05/17/2022 15:52:49 - INFO - __main__ - Step 40 Global step 40 Train loss 13.865007 on epoch=9
05/17/2022 15:52:51 - INFO - __main__ - Step 50 Global step 50 Train loss 12.114893 on epoch=12
05/17/2022 15:53:05 - INFO - __main__ - Global step 50 Train loss 16.496180 Classification-F1 0.0 on epoch=12
05/17/2022 15:53:08 - INFO - __main__ - Step 60 Global step 60 Train loss 10.440310 on epoch=14
05/17/2022 15:53:11 - INFO - __main__ - Step 70 Global step 70 Train loss 9.332706 on epoch=17
05/17/2022 15:53:14 - INFO - __main__ - Step 80 Global step 80 Train loss 7.881997 on epoch=19
05/17/2022 15:53:16 - INFO - __main__ - Step 90 Global step 90 Train loss 7.784186 on epoch=22
05/17/2022 15:53:19 - INFO - __main__ - Step 100 Global step 100 Train loss 7.253451 on epoch=24
05/17/2022 15:53:24 - INFO - __main__ - Global step 100 Train loss 8.538530 Classification-F1 0.0 on epoch=24
05/17/2022 15:53:27 - INFO - __main__ - Step 110 Global step 110 Train loss 6.797396 on epoch=27
05/17/2022 15:53:30 - INFO - __main__ - Step 120 Global step 120 Train loss 6.691205 on epoch=29
05/17/2022 15:53:33 - INFO - __main__ - Step 130 Global step 130 Train loss 5.785422 on epoch=32
05/17/2022 15:53:35 - INFO - __main__ - Step 140 Global step 140 Train loss 6.009923 on epoch=34
05/17/2022 15:53:38 - INFO - __main__ - Step 150 Global step 150 Train loss 6.154008 on epoch=37
05/17/2022 15:53:40 - INFO - __main__ - Global step 150 Train loss 6.287591 Classification-F1 0.06837606837606837 on epoch=37
05/17/2022 15:53:43 - INFO - __main__ - Step 160 Global step 160 Train loss 4.861047 on epoch=39
05/17/2022 15:53:46 - INFO - __main__ - Step 170 Global step 170 Train loss 5.255890 on epoch=42
05/17/2022 15:53:49 - INFO - __main__ - Step 180 Global step 180 Train loss 4.897020 on epoch=44
05/17/2022 15:53:51 - INFO - __main__ - Step 190 Global step 190 Train loss 4.163675 on epoch=47
05/17/2022 15:53:54 - INFO - __main__ - Step 200 Global step 200 Train loss 4.323564 on epoch=49
05/17/2022 15:53:55 - INFO - __main__ - Global step 200 Train loss 4.700239 Classification-F1 0.1 on epoch=49
05/17/2022 15:53:58 - INFO - __main__ - Step 210 Global step 210 Train loss 3.709153 on epoch=52
05/17/2022 15:54:00 - INFO - __main__ - Step 220 Global step 220 Train loss 3.053333 on epoch=54
05/17/2022 15:54:03 - INFO - __main__ - Step 230 Global step 230 Train loss 3.009451 on epoch=57
05/17/2022 15:54:06 - INFO - __main__ - Step 240 Global step 240 Train loss 3.109648 on epoch=59
05/17/2022 15:54:09 - INFO - __main__ - Step 250 Global step 250 Train loss 2.965311 on epoch=62
05/17/2022 15:54:09 - INFO - __main__ - Global step 250 Train loss 3.169379 Classification-F1 0.1 on epoch=62
05/17/2022 15:54:12 - INFO - __main__ - Step 260 Global step 260 Train loss 3.264874 on epoch=64
05/17/2022 15:54:15 - INFO - __main__ - Step 270 Global step 270 Train loss 3.773873 on epoch=67
05/17/2022 15:54:17 - INFO - __main__ - Step 280 Global step 280 Train loss 2.622799 on epoch=69
05/17/2022 15:54:20 - INFO - __main__ - Step 290 Global step 290 Train loss 2.651724 on epoch=72
05/17/2022 15:54:23 - INFO - __main__ - Step 300 Global step 300 Train loss 3.093233 on epoch=74
05/17/2022 15:54:24 - INFO - __main__ - Global step 300 Train loss 3.081301 Classification-F1 0.2626262626262626 on epoch=74
05/17/2022 15:54:27 - INFO - __main__ - Step 310 Global step 310 Train loss 2.024891 on epoch=77
05/17/2022 15:54:29 - INFO - __main__ - Step 320 Global step 320 Train loss 2.645102 on epoch=79
05/17/2022 15:54:32 - INFO - __main__ - Step 330 Global step 330 Train loss 2.946944 on epoch=82
05/17/2022 15:54:35 - INFO - __main__ - Step 340 Global step 340 Train loss 2.817090 on epoch=84
05/17/2022 15:54:38 - INFO - __main__ - Step 350 Global step 350 Train loss 1.921262 on epoch=87
05/17/2022 15:54:38 - INFO - __main__ - Global step 350 Train loss 2.471058 Classification-F1 0.39748677248677244 on epoch=87
05/17/2022 15:54:41 - INFO - __main__ - Step 360 Global step 360 Train loss 2.263600 on epoch=89
05/17/2022 15:54:44 - INFO - __main__ - Step 370 Global step 370 Train loss 1.883753 on epoch=92
05/17/2022 15:54:46 - INFO - __main__ - Step 380 Global step 380 Train loss 2.393303 on epoch=94
05/17/2022 15:54:49 - INFO - __main__ - Step 390 Global step 390 Train loss 2.255341 on epoch=97
05/17/2022 15:54:51 - INFO - __main__ - Step 400 Global step 400 Train loss 2.143155 on epoch=99
05/17/2022 15:54:52 - INFO - __main__ - Global step 400 Train loss 2.187830 Classification-F1 0.38791643139469234 on epoch=99
05/17/2022 15:54:54 - INFO - __main__ - Step 410 Global step 410 Train loss 2.232460 on epoch=102
05/17/2022 15:54:57 - INFO - __main__ - Step 420 Global step 420 Train loss 1.713602 on epoch=104
05/17/2022 15:54:59 - INFO - __main__ - Step 430 Global step 430 Train loss 1.747358 on epoch=107
05/17/2022 15:55:02 - INFO - __main__ - Step 440 Global step 440 Train loss 2.126690 on epoch=109
05/17/2022 15:55:04 - INFO - __main__ - Step 450 Global step 450 Train loss 2.137556 on epoch=112
05/17/2022 15:55:05 - INFO - __main__ - Global step 450 Train loss 1.991533 Classification-F1 0.3841634276416885 on epoch=112
05/17/2022 15:55:07 - INFO - __main__ - Step 460 Global step 460 Train loss 1.834779 on epoch=114
05/17/2022 15:55:10 - INFO - __main__ - Step 470 Global step 470 Train loss 1.844631 on epoch=117
05/17/2022 15:55:12 - INFO - __main__ - Step 480 Global step 480 Train loss 1.762990 on epoch=119
05/17/2022 15:55:15 - INFO - __main__ - Step 490 Global step 490 Train loss 1.968361 on epoch=122
05/17/2022 15:55:17 - INFO - __main__ - Step 500 Global step 500 Train loss 1.429521 on epoch=124
05/17/2022 15:55:18 - INFO - __main__ - Global step 500 Train loss 1.768056 Classification-F1 0.4533088235294118 on epoch=124
05/17/2022 15:55:20 - INFO - __main__ - Step 510 Global step 510 Train loss 1.503485 on epoch=127
05/17/2022 15:55:23 - INFO - __main__ - Step 520 Global step 520 Train loss 1.608736 on epoch=129
05/17/2022 15:55:26 - INFO - __main__ - Step 530 Global step 530 Train loss 1.630434 on epoch=132
05/17/2022 15:55:28 - INFO - __main__ - Step 540 Global step 540 Train loss 1.589711 on epoch=134
05/17/2022 15:55:30 - INFO - __main__ - Step 550 Global step 550 Train loss 1.369395 on epoch=137
05/17/2022 15:55:31 - INFO - __main__ - Global step 550 Train loss 1.540352 Classification-F1 0.4254807692307692 on epoch=137
05/17/2022 15:55:33 - INFO - __main__ - Step 560 Global step 560 Train loss 1.339755 on epoch=139
05/17/2022 15:55:36 - INFO - __main__ - Step 570 Global step 570 Train loss 1.467793 on epoch=142
05/17/2022 15:55:38 - INFO - __main__ - Step 580 Global step 580 Train loss 1.392898 on epoch=144
05/17/2022 15:55:41 - INFO - __main__ - Step 590 Global step 590 Train loss 1.659306 on epoch=147
05/17/2022 15:55:43 - INFO - __main__ - Step 600 Global step 600 Train loss 1.179471 on epoch=149
05/17/2022 15:55:44 - INFO - __main__ - Global step 600 Train loss 1.407845 Classification-F1 0.4262544802867384 on epoch=149
05/17/2022 15:55:46 - INFO - __main__ - Step 610 Global step 610 Train loss 1.224091 on epoch=152
05/17/2022 15:55:49 - INFO - __main__ - Step 620 Global step 620 Train loss 1.336662 on epoch=154
05/17/2022 15:55:51 - INFO - __main__ - Step 630 Global step 630 Train loss 1.309911 on epoch=157
05/17/2022 15:55:54 - INFO - __main__ - Step 640 Global step 640 Train loss 1.224236 on epoch=159
05/17/2022 15:55:56 - INFO - __main__ - Step 650 Global step 650 Train loss 1.148695 on epoch=162
05/17/2022 15:55:57 - INFO - __main__ - Global step 650 Train loss 1.248719 Classification-F1 0.48755026953024727 on epoch=162
05/17/2022 15:55:59 - INFO - __main__ - Step 660 Global step 660 Train loss 1.515351 on epoch=164
05/17/2022 15:56:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.934876 on epoch=167
05/17/2022 15:56:04 - INFO - __main__ - Step 680 Global step 680 Train loss 1.139882 on epoch=169
05/17/2022 15:56:07 - INFO - __main__ - Step 690 Global step 690 Train loss 1.545576 on epoch=172
05/17/2022 15:56:09 - INFO - __main__ - Step 700 Global step 700 Train loss 1.194959 on epoch=174
05/17/2022 15:56:10 - INFO - __main__ - Global step 700 Train loss 1.266129 Classification-F1 0.4581699346405229 on epoch=174
05/17/2022 15:56:12 - INFO - __main__ - Step 710 Global step 710 Train loss 1.177396 on epoch=177
05/17/2022 15:56:15 - INFO - __main__ - Step 720 Global step 720 Train loss 1.146474 on epoch=179
05/17/2022 15:56:17 - INFO - __main__ - Step 730 Global step 730 Train loss 1.185977 on epoch=182
05/17/2022 15:56:20 - INFO - __main__ - Step 740 Global step 740 Train loss 1.201215 on epoch=184
05/17/2022 15:56:22 - INFO - __main__ - Step 750 Global step 750 Train loss 1.155599 on epoch=187
05/17/2022 15:56:22 - INFO - __main__ - Global step 750 Train loss 1.173332 Classification-F1 0.4411562840647584 on epoch=187
05/17/2022 15:56:25 - INFO - __main__ - Step 760 Global step 760 Train loss 1.079163 on epoch=189
05/17/2022 15:56:27 - INFO - __main__ - Step 770 Global step 770 Train loss 1.418798 on epoch=192
05/17/2022 15:56:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.967273 on epoch=194
05/17/2022 15:56:33 - INFO - __main__ - Step 790 Global step 790 Train loss 1.673682 on epoch=197
05/17/2022 15:56:35 - INFO - __main__ - Step 800 Global step 800 Train loss 1.200320 on epoch=199
05/17/2022 15:56:35 - INFO - __main__ - Global step 800 Train loss 1.267847 Classification-F1 0.46965018565565764 on epoch=199
05/17/2022 15:56:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.978510 on epoch=202
05/17/2022 15:56:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.933404 on epoch=204
05/17/2022 15:56:43 - INFO - __main__ - Step 830 Global step 830 Train loss 1.024719 on epoch=207
05/17/2022 15:56:45 - INFO - __main__ - Step 840 Global step 840 Train loss 1.195983 on epoch=209
05/17/2022 15:56:48 - INFO - __main__ - Step 850 Global step 850 Train loss 1.131726 on epoch=212
05/17/2022 15:56:48 - INFO - __main__ - Global step 850 Train loss 1.052869 Classification-F1 0.47357204588309243 on epoch=212
05/17/2022 15:56:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.944359 on epoch=214
05/17/2022 15:56:53 - INFO - __main__ - Step 870 Global step 870 Train loss 1.075794 on epoch=217
05/17/2022 15:56:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.927264 on epoch=219
05/17/2022 15:56:58 - INFO - __main__ - Step 890 Global step 890 Train loss 1.275324 on epoch=222
05/17/2022 15:57:01 - INFO - __main__ - Step 900 Global step 900 Train loss 1.307589 on epoch=224
05/17/2022 15:57:01 - INFO - __main__ - Global step 900 Train loss 1.106066 Classification-F1 0.5098039215686274 on epoch=224
05/17/2022 15:57:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.992643 on epoch=227
05/17/2022 15:57:06 - INFO - __main__ - Step 920 Global step 920 Train loss 1.022516 on epoch=229
05/17/2022 15:57:09 - INFO - __main__ - Step 930 Global step 930 Train loss 1.021445 on epoch=232
05/17/2022 15:57:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.875732 on epoch=234
05/17/2022 15:57:14 - INFO - __main__ - Step 950 Global step 950 Train loss 1.183577 on epoch=237
05/17/2022 15:57:14 - INFO - __main__ - Global step 950 Train loss 1.019182 Classification-F1 0.4833333333333334 on epoch=237
05/17/2022 15:57:17 - INFO - __main__ - Step 960 Global step 960 Train loss 1.120902 on epoch=239
05/17/2022 15:57:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.912191 on epoch=242
05/17/2022 15:57:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.966840 on epoch=244
05/17/2022 15:57:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.913659 on epoch=247
05/17/2022 15:57:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.079958 on epoch=249
05/17/2022 15:57:27 - INFO - __main__ - Global step 1000 Train loss 0.998710 Classification-F1 0.5116892911010558 on epoch=249
05/17/2022 15:57:27 - INFO - __main__ - save last model!
05/17/2022 15:57:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:57:28 - INFO - __main__ - Printing 3 examples
05/17/2022 15:57:28 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/17/2022 15:57:28 - INFO - __main__ - ['others']
05/17/2022 15:57:28 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/17/2022 15:57:28 - INFO - __main__ - ['others']
05/17/2022 15:57:28 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/17/2022 15:57:28 - INFO - __main__ - ['others']
05/17/2022 15:57:28 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:57:28 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:57:28 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 15:57:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:57:28 - INFO - __main__ - Printing 3 examples
05/17/2022 15:57:28 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/17/2022 15:57:28 - INFO - __main__ - ['others']
05/17/2022 15:57:28 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/17/2022 15:57:28 - INFO - __main__ - ['others']
05/17/2022 15:57:28 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/17/2022 15:57:28 - INFO - __main__ - ['others']
05/17/2022 15:57:28 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:57:28 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:57:28 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 15:57:30 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 15:57:30 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 15:57:30 - INFO - __main__ - Printing 3 examples
05/17/2022 15:57:30 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 15:57:30 - INFO - __main__ - ['others']
05/17/2022 15:57:30 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 15:57:30 - INFO - __main__ - ['others']
05/17/2022 15:57:30 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 15:57:30 - INFO - __main__ - ['others']
05/17/2022 15:57:30 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:57:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 15:57:32 - INFO - __main__ - Starting training!
05/17/2022 15:57:32 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:57:38 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 15:58:06 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_100_0.0001_8_predictions.txt
05/17/2022 15:58:06 - INFO - __main__ - Classification-F1 on test data: 0.1707
05/17/2022 15:58:06 - INFO - __main__ - prefix=emo_16_100, lr=0.0001, bsz=8, dev_performance=0.5116892911010558, test_performance=0.17065471296367846
05/17/2022 15:58:06 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.0005, bsz=8 ...
05/17/2022 15:58:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:58:07 - INFO - __main__ - Printing 3 examples
05/17/2022 15:58:07 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/17/2022 15:58:07 - INFO - __main__ - ['others']
05/17/2022 15:58:07 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/17/2022 15:58:07 - INFO - __main__ - ['others']
05/17/2022 15:58:07 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/17/2022 15:58:07 - INFO - __main__ - ['others']
05/17/2022 15:58:07 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:58:07 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:58:08 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 15:58:08 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 15:58:08 - INFO - __main__ - Printing 3 examples
05/17/2022 15:58:08 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/17/2022 15:58:08 - INFO - __main__ - ['others']
05/17/2022 15:58:08 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/17/2022 15:58:08 - INFO - __main__ - ['others']
05/17/2022 15:58:08 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/17/2022 15:58:08 - INFO - __main__ - ['others']
05/17/2022 15:58:08 - INFO - __main__ - Tokenizing Input ...
05/17/2022 15:58:08 - INFO - __main__ - Tokenizing Output ...
05/17/2022 15:58:08 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 15:58:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 15:58:13 - INFO - __main__ - Starting training!
05/17/2022 15:58:15 - INFO - __main__ - Step 10 Global step 10 Train loss 21.327835 on epoch=2
05/17/2022 15:58:17 - INFO - __main__ - Step 20 Global step 20 Train loss 17.050486 on epoch=4
05/17/2022 15:58:20 - INFO - __main__ - Step 30 Global step 30 Train loss 9.559040 on epoch=7
05/17/2022 15:58:22 - INFO - __main__ - Step 40 Global step 40 Train loss 6.202925 on epoch=9
05/17/2022 15:58:24 - INFO - __main__ - Step 50 Global step 50 Train loss 4.045063 on epoch=12
05/17/2022 15:58:25 - INFO - __main__ - Global step 50 Train loss 11.637071 Classification-F1 0.1 on epoch=12
05/17/2022 15:58:27 - INFO - __main__ - Step 60 Global step 60 Train loss 3.901897 on epoch=14
05/17/2022 15:58:30 - INFO - __main__ - Step 70 Global step 70 Train loss 3.051726 on epoch=17
05/17/2022 15:58:32 - INFO - __main__ - Step 80 Global step 80 Train loss 2.809185 on epoch=19
05/17/2022 15:58:35 - INFO - __main__ - Step 90 Global step 90 Train loss 1.739009 on epoch=22
05/17/2022 15:58:37 - INFO - __main__ - Step 100 Global step 100 Train loss 1.707343 on epoch=24
05/17/2022 15:58:38 - INFO - __main__ - Global step 100 Train loss 2.641832 Classification-F1 0.2707317073170732 on epoch=24
05/17/2022 15:58:41 - INFO - __main__ - Step 110 Global step 110 Train loss 1.466683 on epoch=27
05/17/2022 15:58:43 - INFO - __main__ - Step 120 Global step 120 Train loss 1.469977 on epoch=29
05/17/2022 15:58:45 - INFO - __main__ - Step 130 Global step 130 Train loss 1.061475 on epoch=32
05/17/2022 15:58:48 - INFO - __main__ - Step 140 Global step 140 Train loss 1.392798 on epoch=34
05/17/2022 15:58:50 - INFO - __main__ - Step 150 Global step 150 Train loss 1.005913 on epoch=37
05/17/2022 15:58:51 - INFO - __main__ - Global step 150 Train loss 1.279369 Classification-F1 0.4633477633477633 on epoch=37
05/17/2022 15:58:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.726553 on epoch=39
05/17/2022 15:58:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.866399 on epoch=42
05/17/2022 15:58:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.418827 on epoch=44
05/17/2022 15:59:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.759198 on epoch=47
05/17/2022 15:59:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.473426 on epoch=49
05/17/2022 15:59:04 - INFO - __main__ - Global step 200 Train loss 0.648881 Classification-F1 0.6730047182349814 on epoch=49
05/17/2022 15:59:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.466594 on epoch=52
05/17/2022 15:59:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.620345 on epoch=54
05/17/2022 15:59:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.386712 on epoch=57
05/17/2022 15:59:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.276276 on epoch=59
05/17/2022 15:59:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.353574 on epoch=62
05/17/2022 15:59:17 - INFO - __main__ - Global step 250 Train loss 0.420700 Classification-F1 0.7148809523809525 on epoch=62
05/17/2022 15:59:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.337012 on epoch=64
05/17/2022 15:59:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.235732 on epoch=67
05/17/2022 15:59:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.133045 on epoch=69
05/17/2022 15:59:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.153516 on epoch=72
05/17/2022 15:59:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.078050 on epoch=74
05/17/2022 15:59:30 - INFO - __main__ - Global step 300 Train loss 0.187471 Classification-F1 0.7198467945171798 on epoch=74
05/17/2022 15:59:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.128524 on epoch=77
05/17/2022 15:59:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.137314 on epoch=79
05/17/2022 15:59:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.021345 on epoch=82
05/17/2022 15:59:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.146406 on epoch=84
05/17/2022 15:59:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.115867 on epoch=87
05/17/2022 15:59:43 - INFO - __main__ - Global step 350 Train loss 0.109891 Classification-F1 0.7460842377416349 on epoch=87
05/17/2022 15:59:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.229494 on epoch=89
05/17/2022 15:59:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.012431 on epoch=92
05/17/2022 15:59:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.087836 on epoch=94
05/17/2022 15:59:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.131234 on epoch=97
05/17/2022 15:59:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.095849 on epoch=99
05/17/2022 15:59:56 - INFO - __main__ - Global step 400 Train loss 0.111369 Classification-F1 0.7459677419354839 on epoch=99
05/17/2022 15:59:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.080989 on epoch=102
05/17/2022 16:00:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.034234 on epoch=104
05/17/2022 16:00:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.008278 on epoch=107
05/17/2022 16:00:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.033529 on epoch=109
05/17/2022 16:00:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.066071 on epoch=112
05/17/2022 16:00:09 - INFO - __main__ - Global step 450 Train loss 0.044620 Classification-F1 0.6847947454844008 on epoch=112
05/17/2022 16:00:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.198947 on epoch=114
05/17/2022 16:00:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.075660 on epoch=117
05/17/2022 16:00:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.017640 on epoch=119
05/17/2022 16:00:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.095227 on epoch=122
05/17/2022 16:00:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.027044 on epoch=124
05/17/2022 16:00:21 - INFO - __main__ - Global step 500 Train loss 0.082904 Classification-F1 0.38815881345592723 on epoch=124
05/17/2022 16:00:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.020038 on epoch=127
05/17/2022 16:00:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.074995 on epoch=129
05/17/2022 16:00:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.083782 on epoch=132
05/17/2022 16:00:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.006138 on epoch=134
05/17/2022 16:00:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.004983 on epoch=137
05/17/2022 16:00:34 - INFO - __main__ - Global step 550 Train loss 0.037987 Classification-F1 0.7027168234064786 on epoch=137
05/17/2022 16:00:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.021340 on epoch=139
05/17/2022 16:00:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.054960 on epoch=142
05/17/2022 16:00:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.087821 on epoch=144
05/17/2022 16:00:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.015612 on epoch=147
05/17/2022 16:00:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.065861 on epoch=149
05/17/2022 16:00:47 - INFO - __main__ - Global step 600 Train loss 0.049119 Classification-F1 0.6439903846153846 on epoch=149
05/17/2022 16:00:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000343 on epoch=152
05/17/2022 16:00:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.006892 on epoch=154
05/17/2022 16:00:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.008693 on epoch=157
05/17/2022 16:00:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.009799 on epoch=159
05/17/2022 16:00:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.007433 on epoch=162
05/17/2022 16:01:00 - INFO - __main__ - Global step 650 Train loss 0.006632 Classification-F1 0.7323241906733368 on epoch=162
05/17/2022 16:01:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.006960 on epoch=164
05/17/2022 16:01:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.002587 on epoch=167
05/17/2022 16:01:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.020273 on epoch=169
05/17/2022 16:01:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.002304 on epoch=172
05/17/2022 16:01:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000243 on epoch=174
05/17/2022 16:01:12 - INFO - __main__ - Global step 700 Train loss 0.006474 Classification-F1 0.6986764193660745 on epoch=174
05/17/2022 16:01:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.001961 on epoch=177
05/17/2022 16:01:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000358 on epoch=179
05/17/2022 16:01:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000863 on epoch=182
05/17/2022 16:01:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.067286 on epoch=184
05/17/2022 16:01:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.022321 on epoch=187
05/17/2022 16:01:25 - INFO - __main__ - Global step 750 Train loss 0.018558 Classification-F1 0.6851075509584964 on epoch=187
05/17/2022 16:01:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.004750 on epoch=189
05/17/2022 16:01:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.002009 on epoch=192
05/17/2022 16:01:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000082 on epoch=194
05/17/2022 16:01:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.007404 on epoch=197
05/17/2022 16:01:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.003721 on epoch=199
05/17/2022 16:01:38 - INFO - __main__ - Global step 800 Train loss 0.003593 Classification-F1 0.701489276062331 on epoch=199
05/17/2022 16:01:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.012251 on epoch=202
05/17/2022 16:01:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.006832 on epoch=204
05/17/2022 16:01:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.001398 on epoch=207
05/17/2022 16:01:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000327 on epoch=209
05/17/2022 16:01:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.014801 on epoch=212
05/17/2022 16:01:51 - INFO - __main__ - Global step 850 Train loss 0.007122 Classification-F1 0.7121848739495799 on epoch=212
05/17/2022 16:01:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000085 on epoch=214
05/17/2022 16:01:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000099 on epoch=217
05/17/2022 16:01:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.022674 on epoch=219
05/17/2022 16:02:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.035874 on epoch=222
05/17/2022 16:02:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.003297 on epoch=224
05/17/2022 16:02:03 - INFO - __main__ - Global step 900 Train loss 0.012406 Classification-F1 0.7189033566708107 on epoch=224
05/17/2022 16:02:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000018 on epoch=227
05/17/2022 16:02:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000010 on epoch=229
05/17/2022 16:02:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.003736 on epoch=232
05/17/2022 16:02:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000360 on epoch=234
05/17/2022 16:02:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.002656 on epoch=237
05/17/2022 16:02:16 - INFO - __main__ - Global step 950 Train loss 0.001356 Classification-F1 0.749229127134725 on epoch=237
05/17/2022 16:02:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000092 on epoch=239
05/17/2022 16:02:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.019084 on epoch=242
05/17/2022 16:02:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000019 on epoch=244
05/17/2022 16:02:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.016258 on epoch=247
05/17/2022 16:02:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.001065 on epoch=249
05/17/2022 16:02:29 - INFO - __main__ - Global step 1000 Train loss 0.007304 Classification-F1 0.6986764193660745 on epoch=249
05/17/2022 16:02:29 - INFO - __main__ - save last model!
05/17/2022 16:02:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:02:30 - INFO - __main__ - Printing 3 examples
05/17/2022 16:02:30 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/17/2022 16:02:30 - INFO - __main__ - ['others']
05/17/2022 16:02:30 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/17/2022 16:02:30 - INFO - __main__ - ['others']
05/17/2022 16:02:30 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/17/2022 16:02:30 - INFO - __main__ - ['others']
05/17/2022 16:02:30 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:02:30 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:02:30 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:02:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:02:30 - INFO - __main__ - Printing 3 examples
05/17/2022 16:02:30 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/17/2022 16:02:30 - INFO - __main__ - ['others']
05/17/2022 16:02:30 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/17/2022 16:02:30 - INFO - __main__ - ['others']
05/17/2022 16:02:30 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/17/2022 16:02:30 - INFO - __main__ - ['others']
05/17/2022 16:02:30 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:02:30 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:02:30 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:02:32 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 16:02:32 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 16:02:32 - INFO - __main__ - Printing 3 examples
05/17/2022 16:02:32 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 16:02:32 - INFO - __main__ - ['others']
05/17/2022 16:02:32 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 16:02:32 - INFO - __main__ - ['others']
05/17/2022 16:02:32 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 16:02:32 - INFO - __main__ - ['others']
05/17/2022 16:02:32 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:02:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:02:34 - INFO - __main__ - Starting training!
05/17/2022 16:02:34 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:02:40 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 16:03:14 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_13_0.0005_8_predictions.txt
05/17/2022 16:03:14 - INFO - __main__ - Classification-F1 on test data: 0.1013
05/17/2022 16:03:15 - INFO - __main__ - prefix=emo_16_13, lr=0.0005, bsz=8, dev_performance=0.749229127134725, test_performance=0.10125974548429764
05/17/2022 16:03:15 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.0003, bsz=8 ...
05/17/2022 16:03:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:03:16 - INFO - __main__ - Printing 3 examples
05/17/2022 16:03:16 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/17/2022 16:03:16 - INFO - __main__ - ['others']
05/17/2022 16:03:16 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/17/2022 16:03:16 - INFO - __main__ - ['others']
05/17/2022 16:03:16 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/17/2022 16:03:16 - INFO - __main__ - ['others']
05/17/2022 16:03:16 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:03:16 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:03:16 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:03:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:03:16 - INFO - __main__ - Printing 3 examples
05/17/2022 16:03:16 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/17/2022 16:03:16 - INFO - __main__ - ['others']
05/17/2022 16:03:16 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/17/2022 16:03:16 - INFO - __main__ - ['others']
05/17/2022 16:03:16 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/17/2022 16:03:16 - INFO - __main__ - ['others']
05/17/2022 16:03:16 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:03:16 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:03:16 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:03:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:03:20 - INFO - __main__ - Starting training!
05/17/2022 16:03:22 - INFO - __main__ - Step 10 Global step 10 Train loss 20.158590 on epoch=2
05/17/2022 16:03:24 - INFO - __main__ - Step 20 Global step 20 Train loss 19.423309 on epoch=4
05/17/2022 16:03:27 - INFO - __main__ - Step 30 Global step 30 Train loss 12.317191 on epoch=7
05/17/2022 16:03:30 - INFO - __main__ - Step 40 Global step 40 Train loss 11.367667 on epoch=9
05/17/2022 16:03:32 - INFO - __main__ - Step 50 Global step 50 Train loss 7.201444 on epoch=12
05/17/2022 16:03:37 - INFO - __main__ - Global step 50 Train loss 14.093639 Classification-F1 0.026297109232709098 on epoch=12
05/17/2022 16:03:40 - INFO - __main__ - Step 60 Global step 60 Train loss 5.747188 on epoch=14
05/17/2022 16:03:42 - INFO - __main__ - Step 70 Global step 70 Train loss 4.567018 on epoch=17
05/17/2022 16:03:45 - INFO - __main__ - Step 80 Global step 80 Train loss 4.822685 on epoch=19
05/17/2022 16:03:48 - INFO - __main__ - Step 90 Global step 90 Train loss 3.892616 on epoch=22
05/17/2022 16:03:50 - INFO - __main__ - Step 100 Global step 100 Train loss 3.531338 on epoch=24
05/17/2022 16:03:50 - INFO - __main__ - Global step 100 Train loss 4.512169 Classification-F1 0.20238095238095238 on epoch=24
05/17/2022 16:03:53 - INFO - __main__ - Step 110 Global step 110 Train loss 2.489327 on epoch=27
05/17/2022 16:03:56 - INFO - __main__ - Step 120 Global step 120 Train loss 2.558974 on epoch=29
05/17/2022 16:03:58 - INFO - __main__ - Step 130 Global step 130 Train loss 3.070582 on epoch=32
05/17/2022 16:04:01 - INFO - __main__ - Step 140 Global step 140 Train loss 2.099866 on epoch=34
05/17/2022 16:04:03 - INFO - __main__ - Step 150 Global step 150 Train loss 1.830421 on epoch=37
05/17/2022 16:04:04 - INFO - __main__ - Global step 150 Train loss 2.409834 Classification-F1 0.4240232935279375 on epoch=37
05/17/2022 16:04:07 - INFO - __main__ - Step 160 Global step 160 Train loss 2.057958 on epoch=39
05/17/2022 16:04:09 - INFO - __main__ - Step 170 Global step 170 Train loss 1.669603 on epoch=42
05/17/2022 16:04:12 - INFO - __main__ - Step 180 Global step 180 Train loss 2.021516 on epoch=44
05/17/2022 16:04:14 - INFO - __main__ - Step 190 Global step 190 Train loss 1.552194 on epoch=47
05/17/2022 16:04:17 - INFO - __main__ - Step 200 Global step 200 Train loss 1.213409 on epoch=49
05/17/2022 16:04:17 - INFO - __main__ - Global step 200 Train loss 1.702936 Classification-F1 0.4888800283537126 on epoch=49
05/17/2022 16:04:20 - INFO - __main__ - Step 210 Global step 210 Train loss 1.242527 on epoch=52
05/17/2022 16:04:22 - INFO - __main__ - Step 220 Global step 220 Train loss 1.379086 on epoch=54
05/17/2022 16:04:25 - INFO - __main__ - Step 230 Global step 230 Train loss 2.848055 on epoch=57
05/17/2022 16:04:27 - INFO - __main__ - Step 240 Global step 240 Train loss 1.518437 on epoch=59
05/17/2022 16:04:30 - INFO - __main__ - Step 250 Global step 250 Train loss 1.298012 on epoch=62
05/17/2022 16:04:30 - INFO - __main__ - Global step 250 Train loss 1.657223 Classification-F1 0.42520054616828806 on epoch=62
05/17/2022 16:04:33 - INFO - __main__ - Step 260 Global step 260 Train loss 1.046851 on epoch=64
05/17/2022 16:04:35 - INFO - __main__ - Step 270 Global step 270 Train loss 1.170516 on epoch=67
05/17/2022 16:04:38 - INFO - __main__ - Step 280 Global step 280 Train loss 1.033053 on epoch=69
05/17/2022 16:04:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.991174 on epoch=72
05/17/2022 16:04:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.959011 on epoch=74
05/17/2022 16:04:43 - INFO - __main__ - Global step 300 Train loss 1.040121 Classification-F1 0.38209083717250686 on epoch=74
05/17/2022 16:04:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.930609 on epoch=77
05/17/2022 16:04:48 - INFO - __main__ - Step 320 Global step 320 Train loss 1.084875 on epoch=79
05/17/2022 16:04:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.848558 on epoch=82
05/17/2022 16:04:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.952638 on epoch=84
05/17/2022 16:04:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.842330 on epoch=87
05/17/2022 16:04:56 - INFO - __main__ - Global step 350 Train loss 0.931802 Classification-F1 0.49848484848484853 on epoch=87
05/17/2022 16:04:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.767769 on epoch=89
05/17/2022 16:05:01 - INFO - __main__ - Step 370 Global step 370 Train loss 1.012820 on epoch=92
05/17/2022 16:05:04 - INFO - __main__ - Step 380 Global step 380 Train loss 1.132791 on epoch=94
05/17/2022 16:05:06 - INFO - __main__ - Step 390 Global step 390 Train loss 1.194067 on epoch=97
05/17/2022 16:05:09 - INFO - __main__ - Step 400 Global step 400 Train loss 1.117913 on epoch=99
05/17/2022 16:05:09 - INFO - __main__ - Global step 400 Train loss 1.045072 Classification-F1 0.4207010343917249 on epoch=99
05/17/2022 16:05:12 - INFO - __main__ - Step 410 Global step 410 Train loss 1.421387 on epoch=102
05/17/2022 16:05:14 - INFO - __main__ - Step 420 Global step 420 Train loss 1.165648 on epoch=104
05/17/2022 16:05:17 - INFO - __main__ - Step 430 Global step 430 Train loss 1.118224 on epoch=107
05/17/2022 16:05:19 - INFO - __main__ - Step 440 Global step 440 Train loss 1.567163 on epoch=109
05/17/2022 16:05:22 - INFO - __main__ - Step 450 Global step 450 Train loss 1.070030 on epoch=112
05/17/2022 16:05:22 - INFO - __main__ - Global step 450 Train loss 1.268490 Classification-F1 0.41894338380513496 on epoch=112
05/17/2022 16:05:25 - INFO - __main__ - Step 460 Global step 460 Train loss 1.040246 on epoch=114
05/17/2022 16:05:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.984013 on epoch=117
05/17/2022 16:05:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.996506 on epoch=119
05/17/2022 16:05:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.710159 on epoch=122
05/17/2022 16:05:35 - INFO - __main__ - Step 500 Global step 500 Train loss 1.004793 on epoch=124
05/17/2022 16:05:35 - INFO - __main__ - Global step 500 Train loss 0.947143 Classification-F1 0.27561294540699577 on epoch=124
05/17/2022 16:05:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.933588 on epoch=127
05/17/2022 16:05:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.760797 on epoch=129
05/17/2022 16:05:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.997263 on epoch=132
05/17/2022 16:05:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.895424 on epoch=134
05/17/2022 16:05:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.790614 on epoch=137
05/17/2022 16:05:48 - INFO - __main__ - Global step 550 Train loss 0.875537 Classification-F1 0.463772659124983 on epoch=137
05/17/2022 16:05:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.756618 on epoch=139
05/17/2022 16:05:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.852355 on epoch=142
05/17/2022 16:05:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.881687 on epoch=144
05/17/2022 16:05:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.719306 on epoch=147
05/17/2022 16:06:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.719303 on epoch=149
05/17/2022 16:06:01 - INFO - __main__ - Global step 600 Train loss 0.785854 Classification-F1 0.4784764108072379 on epoch=149
05/17/2022 16:06:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.727858 on epoch=152
05/17/2022 16:06:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.672543 on epoch=154
05/17/2022 16:06:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.589878 on epoch=157
05/17/2022 16:06:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.713803 on epoch=159
05/17/2022 16:06:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.726501 on epoch=162
05/17/2022 16:06:13 - INFO - __main__ - Global step 650 Train loss 0.686117 Classification-F1 0.5558333333333334 on epoch=162
05/17/2022 16:06:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.707215 on epoch=164
05/17/2022 16:06:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.680814 on epoch=167
05/17/2022 16:06:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.645981 on epoch=169
05/17/2022 16:06:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.727842 on epoch=172
05/17/2022 16:06:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.680662 on epoch=174
05/17/2022 16:06:27 - INFO - __main__ - Global step 700 Train loss 0.688503 Classification-F1 0.5074525745257452 on epoch=174
05/17/2022 16:06:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.657548 on epoch=177
05/17/2022 16:06:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.672488 on epoch=179
05/17/2022 16:06:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.627680 on epoch=182
05/17/2022 16:06:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.650017 on epoch=184
05/17/2022 16:06:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.586650 on epoch=187
05/17/2022 16:06:40 - INFO - __main__ - Global step 750 Train loss 0.638877 Classification-F1 0.6394345238095238 on epoch=187
05/17/2022 16:06:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.632378 on epoch=189
05/17/2022 16:06:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.535688 on epoch=192
05/17/2022 16:06:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.729487 on epoch=194
05/17/2022 16:06:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.691528 on epoch=197
05/17/2022 16:06:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.686924 on epoch=199
05/17/2022 16:06:53 - INFO - __main__ - Global step 800 Train loss 0.655201 Classification-F1 0.5840062111801242 on epoch=199
05/17/2022 16:06:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.517742 on epoch=202
05/17/2022 16:06:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.612353 on epoch=204
05/17/2022 16:07:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.623937 on epoch=207
05/17/2022 16:07:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.556239 on epoch=209
05/17/2022 16:07:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.535628 on epoch=212
05/17/2022 16:07:06 - INFO - __main__ - Global step 850 Train loss 0.569180 Classification-F1 0.6096804740957966 on epoch=212
05/17/2022 16:07:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.530412 on epoch=214
05/17/2022 16:07:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.687709 on epoch=217
05/17/2022 16:07:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.677516 on epoch=219
05/17/2022 16:07:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.562711 on epoch=222
05/17/2022 16:07:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.587016 on epoch=224
05/17/2022 16:07:18 - INFO - __main__ - Global step 900 Train loss 0.609073 Classification-F1 0.5670707070707071 on epoch=224
05/17/2022 16:07:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.461659 on epoch=227
05/17/2022 16:07:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.450654 on epoch=229
05/17/2022 16:07:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.488063 on epoch=232
05/17/2022 16:07:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.362330 on epoch=234
05/17/2022 16:07:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.543383 on epoch=237
05/17/2022 16:07:31 - INFO - __main__ - Global step 950 Train loss 0.461218 Classification-F1 0.6879164590858139 on epoch=237
05/17/2022 16:07:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.377543 on epoch=239
05/17/2022 16:07:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.543179 on epoch=242
05/17/2022 16:07:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.527849 on epoch=244
05/17/2022 16:07:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.418463 on epoch=247
05/17/2022 16:07:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.531445 on epoch=249
05/17/2022 16:07:44 - INFO - __main__ - Global step 1000 Train loss 0.479696 Classification-F1 0.6907444067370537 on epoch=249
05/17/2022 16:07:45 - INFO - __main__ - save last model!
05/17/2022 16:07:45 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:07:45 - INFO - __main__ - Printing 3 examples
05/17/2022 16:07:45 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/17/2022 16:07:45 - INFO - __main__ - ['others']
05/17/2022 16:07:45 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/17/2022 16:07:45 - INFO - __main__ - ['others']
05/17/2022 16:07:45 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/17/2022 16:07:45 - INFO - __main__ - ['others']
05/17/2022 16:07:45 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:07:45 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:07:45 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:07:45 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:07:45 - INFO - __main__ - Printing 3 examples
05/17/2022 16:07:45 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/17/2022 16:07:45 - INFO - __main__ - ['others']
05/17/2022 16:07:45 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/17/2022 16:07:45 - INFO - __main__ - ['others']
05/17/2022 16:07:45 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/17/2022 16:07:45 - INFO - __main__ - ['others']
05/17/2022 16:07:45 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:07:45 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:07:45 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:07:47 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 16:07:48 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 16:07:48 - INFO - __main__ - Printing 3 examples
05/17/2022 16:07:48 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 16:07:48 - INFO - __main__ - ['others']
05/17/2022 16:07:48 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 16:07:48 - INFO - __main__ - ['others']
05/17/2022 16:07:48 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 16:07:48 - INFO - __main__ - ['others']
05/17/2022 16:07:48 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:07:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:07:49 - INFO - __main__ - Starting training!
05/17/2022 16:07:50 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:07:55 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 16:08:24 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_13_0.0003_8_predictions.txt
05/17/2022 16:08:24 - INFO - __main__ - Classification-F1 on test data: 0.3074
05/17/2022 16:08:24 - INFO - __main__ - prefix=emo_16_13, lr=0.0003, bsz=8, dev_performance=0.6907444067370537, test_performance=0.3073665469563368
05/17/2022 16:08:24 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.0002, bsz=8 ...
05/17/2022 16:08:25 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:08:25 - INFO - __main__ - Printing 3 examples
05/17/2022 16:08:25 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/17/2022 16:08:25 - INFO - __main__ - ['others']
05/17/2022 16:08:25 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/17/2022 16:08:25 - INFO - __main__ - ['others']
05/17/2022 16:08:25 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/17/2022 16:08:25 - INFO - __main__ - ['others']
05/17/2022 16:08:25 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:08:25 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:08:25 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:08:25 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:08:25 - INFO - __main__ - Printing 3 examples
05/17/2022 16:08:25 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/17/2022 16:08:25 - INFO - __main__ - ['others']
05/17/2022 16:08:25 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/17/2022 16:08:25 - INFO - __main__ - ['others']
05/17/2022 16:08:25 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/17/2022 16:08:25 - INFO - __main__ - ['others']
05/17/2022 16:08:25 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:08:25 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:08:25 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:08:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:08:29 - INFO - __main__ - Starting training!
05/17/2022 16:08:31 - INFO - __main__ - Step 10 Global step 10 Train loss 20.136141 on epoch=2
05/17/2022 16:08:33 - INFO - __main__ - Step 20 Global step 20 Train loss 19.112782 on epoch=4
05/17/2022 16:08:36 - INFO - __main__ - Step 30 Global step 30 Train loss 13.202579 on epoch=7
05/17/2022 16:08:38 - INFO - __main__ - Step 40 Global step 40 Train loss 11.497574 on epoch=9
05/17/2022 16:08:41 - INFO - __main__ - Step 50 Global step 50 Train loss 8.257957 on epoch=12
05/17/2022 16:08:52 - INFO - __main__ - Global step 50 Train loss 14.441406 Classification-F1 0.0 on epoch=12
05/17/2022 16:08:54 - INFO - __main__ - Step 60 Global step 60 Train loss 8.040622 on epoch=14
05/17/2022 16:08:57 - INFO - __main__ - Step 70 Global step 70 Train loss 7.091964 on epoch=17
05/17/2022 16:08:59 - INFO - __main__ - Step 80 Global step 80 Train loss 5.996793 on epoch=19
05/17/2022 16:09:02 - INFO - __main__ - Step 90 Global step 90 Train loss 4.779410 on epoch=22
05/17/2022 16:09:04 - INFO - __main__ - Step 100 Global step 100 Train loss 4.506227 on epoch=24
05/17/2022 16:09:05 - INFO - __main__ - Global step 100 Train loss 6.083003 Classification-F1 0.0974025974025974 on epoch=24
05/17/2022 16:09:08 - INFO - __main__ - Step 110 Global step 110 Train loss 4.991711 on epoch=27
05/17/2022 16:09:11 - INFO - __main__ - Step 120 Global step 120 Train loss 3.544822 on epoch=29
05/17/2022 16:09:13 - INFO - __main__ - Step 130 Global step 130 Train loss 3.246340 on epoch=32
05/17/2022 16:09:15 - INFO - __main__ - Step 140 Global step 140 Train loss 3.452197 on epoch=34
05/17/2022 16:09:18 - INFO - __main__ - Step 150 Global step 150 Train loss 2.665246 on epoch=37
05/17/2022 16:09:18 - INFO - __main__ - Global step 150 Train loss 3.580063 Classification-F1 0.1 on epoch=37
05/17/2022 16:09:21 - INFO - __main__ - Step 160 Global step 160 Train loss 3.124903 on epoch=39
05/17/2022 16:09:24 - INFO - __main__ - Step 170 Global step 170 Train loss 2.098857 on epoch=42
05/17/2022 16:09:26 - INFO - __main__ - Step 180 Global step 180 Train loss 2.989688 on epoch=44
05/17/2022 16:09:29 - INFO - __main__ - Step 190 Global step 190 Train loss 2.146470 on epoch=47
05/17/2022 16:09:31 - INFO - __main__ - Step 200 Global step 200 Train loss 2.533447 on epoch=49
05/17/2022 16:09:32 - INFO - __main__ - Global step 200 Train loss 2.578673 Classification-F1 0.11714285714285715 on epoch=49
05/17/2022 16:09:35 - INFO - __main__ - Step 210 Global step 210 Train loss 2.935793 on epoch=52
05/17/2022 16:09:37 - INFO - __main__ - Step 220 Global step 220 Train loss 2.984482 on epoch=54
05/17/2022 16:09:40 - INFO - __main__ - Step 230 Global step 230 Train loss 2.016507 on epoch=57
05/17/2022 16:09:42 - INFO - __main__ - Step 240 Global step 240 Train loss 1.846950 on epoch=59
05/17/2022 16:09:45 - INFO - __main__ - Step 250 Global step 250 Train loss 2.498783 on epoch=62
05/17/2022 16:09:45 - INFO - __main__ - Global step 250 Train loss 2.456503 Classification-F1 0.2857396870554765 on epoch=62
05/17/2022 16:09:48 - INFO - __main__ - Step 260 Global step 260 Train loss 2.014960 on epoch=64
05/17/2022 16:09:50 - INFO - __main__ - Step 270 Global step 270 Train loss 1.846507 on epoch=67
05/17/2022 16:09:53 - INFO - __main__ - Step 280 Global step 280 Train loss 2.051133 on epoch=69
05/17/2022 16:09:55 - INFO - __main__ - Step 290 Global step 290 Train loss 1.660800 on epoch=72
05/17/2022 16:09:58 - INFO - __main__ - Step 300 Global step 300 Train loss 1.475627 on epoch=74
05/17/2022 16:09:58 - INFO - __main__ - Global step 300 Train loss 1.809805 Classification-F1 0.4696594427244582 on epoch=74
05/17/2022 16:10:01 - INFO - __main__ - Step 310 Global step 310 Train loss 1.515870 on epoch=77
05/17/2022 16:10:03 - INFO - __main__ - Step 320 Global step 320 Train loss 1.374184 on epoch=79
05/17/2022 16:10:06 - INFO - __main__ - Step 330 Global step 330 Train loss 1.916609 on epoch=82
05/17/2022 16:10:08 - INFO - __main__ - Step 340 Global step 340 Train loss 1.632866 on epoch=84
05/17/2022 16:10:11 - INFO - __main__ - Step 350 Global step 350 Train loss 1.206679 on epoch=87
05/17/2022 16:10:11 - INFO - __main__ - Global step 350 Train loss 1.529241 Classification-F1 0.47073211875843457 on epoch=87
05/17/2022 16:10:14 - INFO - __main__ - Step 360 Global step 360 Train loss 1.188719 on epoch=89
05/17/2022 16:10:17 - INFO - __main__ - Step 370 Global step 370 Train loss 1.195963 on epoch=92
05/17/2022 16:10:19 - INFO - __main__ - Step 380 Global step 380 Train loss 1.048637 on epoch=94
05/17/2022 16:10:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.985637 on epoch=97
05/17/2022 16:10:24 - INFO - __main__ - Step 400 Global step 400 Train loss 2.172214 on epoch=99
05/17/2022 16:10:24 - INFO - __main__ - Global step 400 Train loss 1.318234 Classification-F1 0.29004329004329005 on epoch=99
05/17/2022 16:10:27 - INFO - __main__ - Step 410 Global step 410 Train loss 2.526440 on epoch=102
05/17/2022 16:10:29 - INFO - __main__ - Step 420 Global step 420 Train loss 2.139121 on epoch=104
05/17/2022 16:10:32 - INFO - __main__ - Step 430 Global step 430 Train loss 1.178687 on epoch=107
05/17/2022 16:10:35 - INFO - __main__ - Step 440 Global step 440 Train loss 1.313464 on epoch=109
05/17/2022 16:10:37 - INFO - __main__ - Step 450 Global step 450 Train loss 1.115827 on epoch=112
05/17/2022 16:10:37 - INFO - __main__ - Global step 450 Train loss 1.654708 Classification-F1 0.5944318181818181 on epoch=112
05/17/2022 16:10:40 - INFO - __main__ - Step 460 Global step 460 Train loss 1.040487 on epoch=114
05/17/2022 16:10:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.980560 on epoch=117
05/17/2022 16:10:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.829027 on epoch=119
05/17/2022 16:10:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.757257 on epoch=122
05/17/2022 16:10:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.832944 on epoch=124
05/17/2022 16:10:51 - INFO - __main__ - Global step 500 Train loss 0.888055 Classification-F1 0.46817988064791133 on epoch=124
05/17/2022 16:10:53 - INFO - __main__ - Step 510 Global step 510 Train loss 1.009231 on epoch=127
05/17/2022 16:10:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.794153 on epoch=129
05/17/2022 16:10:58 - INFO - __main__ - Step 530 Global step 530 Train loss 1.003692 on epoch=132
05/17/2022 16:11:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.824439 on epoch=134
05/17/2022 16:11:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.610390 on epoch=137
05/17/2022 16:11:04 - INFO - __main__ - Global step 550 Train loss 0.848381 Classification-F1 0.6247529644268774 on epoch=137
05/17/2022 16:11:07 - INFO - __main__ - Step 560 Global step 560 Train loss 1.205930 on epoch=139
05/17/2022 16:11:09 - INFO - __main__ - Step 570 Global step 570 Train loss 1.085955 on epoch=142
05/17/2022 16:11:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.915512 on epoch=144
05/17/2022 16:11:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.657123 on epoch=147
05/17/2022 16:11:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.858448 on epoch=149
05/17/2022 16:11:17 - INFO - __main__ - Global step 600 Train loss 0.944594 Classification-F1 0.5109207737148913 on epoch=149
05/17/2022 16:11:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.879410 on epoch=152
05/17/2022 16:11:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.798223 on epoch=154
05/17/2022 16:11:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.797328 on epoch=157
05/17/2022 16:11:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.833001 on epoch=159
05/17/2022 16:11:30 - INFO - __main__ - Step 650 Global step 650 Train loss 1.198427 on epoch=162
05/17/2022 16:11:30 - INFO - __main__ - Global step 650 Train loss 0.901278 Classification-F1 0.6381240213687023 on epoch=162
05/17/2022 16:11:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.721214 on epoch=164
05/17/2022 16:11:35 - INFO - __main__ - Step 670 Global step 670 Train loss 1.014831 on epoch=167
05/17/2022 16:11:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.696429 on epoch=169
05/17/2022 16:11:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.726166 on epoch=172
05/17/2022 16:11:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.824396 on epoch=174
05/17/2022 16:11:43 - INFO - __main__ - Global step 700 Train loss 0.796607 Classification-F1 0.5203629431570609 on epoch=174
05/17/2022 16:11:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.784428 on epoch=177
05/17/2022 16:11:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.684424 on epoch=179
05/17/2022 16:11:50 - INFO - __main__ - Step 730 Global step 730 Train loss 0.915492 on epoch=182
05/17/2022 16:11:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.698387 on epoch=184
05/17/2022 16:11:55 - INFO - __main__ - Step 750 Global step 750 Train loss 0.868414 on epoch=187
05/17/2022 16:11:56 - INFO - __main__ - Global step 750 Train loss 0.790229 Classification-F1 0.6124811146550276 on epoch=187
05/17/2022 16:11:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.623251 on epoch=189
05/17/2022 16:12:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.660279 on epoch=192
05/17/2022 16:12:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.642686 on epoch=194
05/17/2022 16:12:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.646586 on epoch=197
05/17/2022 16:12:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.603009 on epoch=199
05/17/2022 16:12:09 - INFO - __main__ - Global step 800 Train loss 0.635162 Classification-F1 0.6588861588861589 on epoch=199
05/17/2022 16:12:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.703793 on epoch=202
05/17/2022 16:12:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.670942 on epoch=204
05/17/2022 16:12:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.874964 on epoch=207
05/17/2022 16:12:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.701773 on epoch=209
05/17/2022 16:12:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.701056 on epoch=212
05/17/2022 16:12:22 - INFO - __main__ - Global step 850 Train loss 0.730506 Classification-F1 0.6938263001677636 on epoch=212
05/17/2022 16:12:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.417154 on epoch=214
05/17/2022 16:12:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.542544 on epoch=217
05/17/2022 16:12:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.615873 on epoch=219
05/17/2022 16:12:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.678552 on epoch=222
05/17/2022 16:12:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.782866 on epoch=224
05/17/2022 16:12:35 - INFO - __main__ - Global step 900 Train loss 0.607398 Classification-F1 0.6774044795783927 on epoch=224
05/17/2022 16:12:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.426380 on epoch=227
05/17/2022 16:12:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.643790 on epoch=229
05/17/2022 16:12:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.752656 on epoch=232
05/17/2022 16:12:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.568900 on epoch=234
05/17/2022 16:12:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.509057 on epoch=237
05/17/2022 16:12:48 - INFO - __main__ - Global step 950 Train loss 0.580156 Classification-F1 0.7004900332225914 on epoch=237
05/17/2022 16:12:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.600342 on epoch=239
05/17/2022 16:12:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.512768 on epoch=242
05/17/2022 16:12:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.457771 on epoch=244
05/17/2022 16:12:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.604194 on epoch=247
05/17/2022 16:13:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.383960 on epoch=249
05/17/2022 16:13:01 - INFO - __main__ - Global step 1000 Train loss 0.511807 Classification-F1 0.7404804804804805 on epoch=249
05/17/2022 16:13:02 - INFO - __main__ - save last model!
05/17/2022 16:13:02 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:13:02 - INFO - __main__ - Printing 3 examples
05/17/2022 16:13:02 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/17/2022 16:13:02 - INFO - __main__ - ['others']
05/17/2022 16:13:02 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/17/2022 16:13:02 - INFO - __main__ - ['others']
05/17/2022 16:13:02 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/17/2022 16:13:02 - INFO - __main__ - ['others']
05/17/2022 16:13:02 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:13:02 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:13:02 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:13:02 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:13:02 - INFO - __main__ - Printing 3 examples
05/17/2022 16:13:02 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/17/2022 16:13:02 - INFO - __main__ - ['others']
05/17/2022 16:13:02 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/17/2022 16:13:02 - INFO - __main__ - ['others']
05/17/2022 16:13:02 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/17/2022 16:13:02 - INFO - __main__ - ['others']
05/17/2022 16:13:02 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:13:02 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:13:02 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:13:04 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 16:13:05 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 16:13:05 - INFO - __main__ - Printing 3 examples
05/17/2022 16:13:05 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 16:13:05 - INFO - __main__ - ['others']
05/17/2022 16:13:05 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 16:13:05 - INFO - __main__ - ['others']
05/17/2022 16:13:05 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 16:13:05 - INFO - __main__ - ['others']
05/17/2022 16:13:05 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:13:07 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:13:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:13:07 - INFO - __main__ - Starting training!
05/17/2022 16:13:12 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 16:13:41 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_13_0.0002_8_predictions.txt
05/17/2022 16:13:41 - INFO - __main__ - Classification-F1 on test data: 0.3554
05/17/2022 16:13:41 - INFO - __main__ - prefix=emo_16_13, lr=0.0002, bsz=8, dev_performance=0.7404804804804805, test_performance=0.35538308729168494
05/17/2022 16:13:41 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.0001, bsz=8 ...
05/17/2022 16:13:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:13:42 - INFO - __main__ - Printing 3 examples
05/17/2022 16:13:42 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/17/2022 16:13:42 - INFO - __main__ - ['others']
05/17/2022 16:13:42 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/17/2022 16:13:42 - INFO - __main__ - ['others']
05/17/2022 16:13:42 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/17/2022 16:13:42 - INFO - __main__ - ['others']
05/17/2022 16:13:42 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:13:42 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:13:42 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:13:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:13:42 - INFO - __main__ - Printing 3 examples
05/17/2022 16:13:42 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/17/2022 16:13:42 - INFO - __main__ - ['others']
05/17/2022 16:13:42 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/17/2022 16:13:42 - INFO - __main__ - ['others']
05/17/2022 16:13:42 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/17/2022 16:13:42 - INFO - __main__ - ['others']
05/17/2022 16:13:42 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:13:42 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:13:42 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:13:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:13:46 - INFO - __main__ - Starting training!
05/17/2022 16:13:49 - INFO - __main__ - Step 10 Global step 10 Train loss 20.745197 on epoch=2
05/17/2022 16:13:51 - INFO - __main__ - Step 20 Global step 20 Train loss 20.005768 on epoch=4
05/17/2022 16:13:54 - INFO - __main__ - Step 30 Global step 30 Train loss 16.872931 on epoch=7
05/17/2022 16:13:56 - INFO - __main__ - Step 40 Global step 40 Train loss 13.166553 on epoch=9
05/17/2022 16:13:59 - INFO - __main__ - Step 50 Global step 50 Train loss 12.861676 on epoch=12
05/17/2022 16:14:11 - INFO - __main__ - Global step 50 Train loss 16.730427 Classification-F1 0.0 on epoch=12
05/17/2022 16:14:14 - INFO - __main__ - Step 60 Global step 60 Train loss 11.449907 on epoch=14
05/17/2022 16:14:17 - INFO - __main__ - Step 70 Global step 70 Train loss 9.162210 on epoch=17
05/17/2022 16:14:19 - INFO - __main__ - Step 80 Global step 80 Train loss 8.621845 on epoch=19
05/17/2022 16:14:22 - INFO - __main__ - Step 90 Global step 90 Train loss 8.787184 on epoch=22
05/17/2022 16:14:24 - INFO - __main__ - Step 100 Global step 100 Train loss 7.725720 on epoch=24
05/17/2022 16:14:33 - INFO - __main__ - Global step 100 Train loss 9.149373 Classification-F1 0.001755926251097454 on epoch=24
05/17/2022 16:14:37 - INFO - __main__ - Step 110 Global step 110 Train loss 7.868188 on epoch=27
05/17/2022 16:14:39 - INFO - __main__ - Step 120 Global step 120 Train loss 7.290758 on epoch=29
05/17/2022 16:14:42 - INFO - __main__ - Step 130 Global step 130 Train loss 5.475997 on epoch=32
05/17/2022 16:14:44 - INFO - __main__ - Step 140 Global step 140 Train loss 6.563271 on epoch=34
05/17/2022 16:14:47 - INFO - __main__ - Step 150 Global step 150 Train loss 5.548301 on epoch=37
05/17/2022 16:14:49 - INFO - __main__ - Global step 150 Train loss 6.549303 Classification-F1 0.06222222222222222 on epoch=37
05/17/2022 16:14:52 - INFO - __main__ - Step 160 Global step 160 Train loss 5.684021 on epoch=39
05/17/2022 16:14:54 - INFO - __main__ - Step 170 Global step 170 Train loss 6.113121 on epoch=42
05/17/2022 16:14:57 - INFO - __main__ - Step 180 Global step 180 Train loss 5.058249 on epoch=44
05/17/2022 16:14:59 - INFO - __main__ - Step 190 Global step 190 Train loss 4.836064 on epoch=47
05/17/2022 16:15:02 - INFO - __main__ - Step 200 Global step 200 Train loss 3.858514 on epoch=49
05/17/2022 16:15:02 - INFO - __main__ - Global step 200 Train loss 5.109993 Classification-F1 0.13067758749069247 on epoch=49
05/17/2022 16:15:05 - INFO - __main__ - Step 210 Global step 210 Train loss 4.856731 on epoch=52
05/17/2022 16:15:08 - INFO - __main__ - Step 220 Global step 220 Train loss 3.744133 on epoch=54
05/17/2022 16:15:10 - INFO - __main__ - Step 230 Global step 230 Train loss 3.521597 on epoch=57
05/17/2022 16:15:13 - INFO - __main__ - Step 240 Global step 240 Train loss 3.127650 on epoch=59
05/17/2022 16:15:15 - INFO - __main__ - Step 250 Global step 250 Train loss 4.226402 on epoch=62
05/17/2022 16:15:15 - INFO - __main__ - Global step 250 Train loss 3.895303 Classification-F1 0.1542857142857143 on epoch=62
05/17/2022 16:15:18 - INFO - __main__ - Step 260 Global step 260 Train loss 3.565670 on epoch=64
05/17/2022 16:15:21 - INFO - __main__ - Step 270 Global step 270 Train loss 3.269500 on epoch=67
05/17/2022 16:15:23 - INFO - __main__ - Step 280 Global step 280 Train loss 2.837326 on epoch=69
05/17/2022 16:15:26 - INFO - __main__ - Step 290 Global step 290 Train loss 2.686574 on epoch=72
05/17/2022 16:15:29 - INFO - __main__ - Step 300 Global step 300 Train loss 3.345623 on epoch=74
05/17/2022 16:15:29 - INFO - __main__ - Global step 300 Train loss 3.140939 Classification-F1 0.20714285714285716 on epoch=74
05/17/2022 16:15:32 - INFO - __main__ - Step 310 Global step 310 Train loss 3.389279 on epoch=77
05/17/2022 16:15:34 - INFO - __main__ - Step 320 Global step 320 Train loss 2.416405 on epoch=79
05/17/2022 16:15:37 - INFO - __main__ - Step 330 Global step 330 Train loss 2.240852 on epoch=82
05/17/2022 16:15:39 - INFO - __main__ - Step 340 Global step 340 Train loss 2.801862 on epoch=84
05/17/2022 16:15:42 - INFO - __main__ - Step 350 Global step 350 Train loss 3.332551 on epoch=87
05/17/2022 16:15:42 - INFO - __main__ - Global step 350 Train loss 2.836190 Classification-F1 0.3889010989010989 on epoch=87
05/17/2022 16:15:45 - INFO - __main__ - Step 360 Global step 360 Train loss 2.207777 on epoch=89
05/17/2022 16:15:48 - INFO - __main__ - Step 370 Global step 370 Train loss 2.420427 on epoch=92
05/17/2022 16:15:50 - INFO - __main__ - Step 380 Global step 380 Train loss 2.891585 on epoch=94
05/17/2022 16:15:53 - INFO - __main__ - Step 390 Global step 390 Train loss 2.234562 on epoch=97
05/17/2022 16:15:55 - INFO - __main__ - Step 400 Global step 400 Train loss 2.094809 on epoch=99
05/17/2022 16:15:56 - INFO - __main__ - Global step 400 Train loss 2.369832 Classification-F1 0.2755050505050505 on epoch=99
05/17/2022 16:15:58 - INFO - __main__ - Step 410 Global step 410 Train loss 2.101324 on epoch=102
05/17/2022 16:16:01 - INFO - __main__ - Step 420 Global step 420 Train loss 1.865271 on epoch=104
05/17/2022 16:16:03 - INFO - __main__ - Step 430 Global step 430 Train loss 2.179641 on epoch=107
05/17/2022 16:16:06 - INFO - __main__ - Step 440 Global step 440 Train loss 2.707781 on epoch=109
05/17/2022 16:16:08 - INFO - __main__ - Step 450 Global step 450 Train loss 2.419664 on epoch=112
05/17/2022 16:16:09 - INFO - __main__ - Global step 450 Train loss 2.254736 Classification-F1 0.4091431556948798 on epoch=112
05/17/2022 16:16:12 - INFO - __main__ - Step 460 Global step 460 Train loss 1.591369 on epoch=114
05/17/2022 16:16:14 - INFO - __main__ - Step 470 Global step 470 Train loss 1.827389 on epoch=117
05/17/2022 16:16:17 - INFO - __main__ - Step 480 Global step 480 Train loss 1.924816 on epoch=119
05/17/2022 16:16:19 - INFO - __main__ - Step 490 Global step 490 Train loss 1.991480 on epoch=122
05/17/2022 16:16:22 - INFO - __main__ - Step 500 Global step 500 Train loss 2.067889 on epoch=124
05/17/2022 16:16:22 - INFO - __main__ - Global step 500 Train loss 1.880589 Classification-F1 0.4415498065764023 on epoch=124
05/17/2022 16:16:25 - INFO - __main__ - Step 510 Global step 510 Train loss 1.866911 on epoch=127
05/17/2022 16:16:28 - INFO - __main__ - Step 520 Global step 520 Train loss 1.904857 on epoch=129
05/17/2022 16:16:30 - INFO - __main__ - Step 530 Global step 530 Train loss 1.414631 on epoch=132
05/17/2022 16:16:33 - INFO - __main__ - Step 540 Global step 540 Train loss 1.269825 on epoch=134
05/17/2022 16:16:35 - INFO - __main__ - Step 550 Global step 550 Train loss 1.577759 on epoch=137
05/17/2022 16:16:36 - INFO - __main__ - Global step 550 Train loss 1.606796 Classification-F1 0.43425202248731665 on epoch=137
05/17/2022 16:16:38 - INFO - __main__ - Step 560 Global step 560 Train loss 1.499776 on epoch=139
05/17/2022 16:16:41 - INFO - __main__ - Step 570 Global step 570 Train loss 1.596789 on epoch=142
05/17/2022 16:16:43 - INFO - __main__ - Step 580 Global step 580 Train loss 1.430746 on epoch=144
05/17/2022 16:16:46 - INFO - __main__ - Step 590 Global step 590 Train loss 1.685303 on epoch=147
05/17/2022 16:16:48 - INFO - __main__ - Step 600 Global step 600 Train loss 1.697818 on epoch=149
05/17/2022 16:16:49 - INFO - __main__ - Global step 600 Train loss 1.582086 Classification-F1 0.4690611664295875 on epoch=149
05/17/2022 16:16:51 - INFO - __main__ - Step 610 Global step 610 Train loss 1.411817 on epoch=152
05/17/2022 16:16:54 - INFO - __main__ - Step 620 Global step 620 Train loss 1.284889 on epoch=154
05/17/2022 16:16:56 - INFO - __main__ - Step 630 Global step 630 Train loss 1.506964 on epoch=157
05/17/2022 16:16:59 - INFO - __main__ - Step 640 Global step 640 Train loss 1.830166 on epoch=159
05/17/2022 16:17:02 - INFO - __main__ - Step 650 Global step 650 Train loss 1.219932 on epoch=162
05/17/2022 16:17:02 - INFO - __main__ - Global step 650 Train loss 1.450754 Classification-F1 0.48905334566759173 on epoch=162
05/17/2022 16:17:05 - INFO - __main__ - Step 660 Global step 660 Train loss 1.239304 on epoch=164
05/17/2022 16:17:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.870483 on epoch=167
05/17/2022 16:17:10 - INFO - __main__ - Step 680 Global step 680 Train loss 1.220901 on epoch=169
05/17/2022 16:17:12 - INFO - __main__ - Step 690 Global step 690 Train loss 1.254915 on epoch=172
05/17/2022 16:17:15 - INFO - __main__ - Step 700 Global step 700 Train loss 1.145851 on epoch=174
05/17/2022 16:17:15 - INFO - __main__ - Global step 700 Train loss 1.146291 Classification-F1 0.5453815672610508 on epoch=174
05/17/2022 16:17:18 - INFO - __main__ - Step 710 Global step 710 Train loss 1.580920 on epoch=177
05/17/2022 16:17:21 - INFO - __main__ - Step 720 Global step 720 Train loss 1.184256 on epoch=179
05/17/2022 16:17:23 - INFO - __main__ - Step 730 Global step 730 Train loss 1.117772 on epoch=182
05/17/2022 16:17:26 - INFO - __main__ - Step 740 Global step 740 Train loss 1.117629 on epoch=184
05/17/2022 16:17:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.983416 on epoch=187
05/17/2022 16:17:29 - INFO - __main__ - Global step 750 Train loss 1.196799 Classification-F1 0.49238496808829285 on epoch=187
05/17/2022 16:17:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.863977 on epoch=189
05/17/2022 16:17:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.833620 on epoch=192
05/17/2022 16:17:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.911423 on epoch=194
05/17/2022 16:17:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.885334 on epoch=197
05/17/2022 16:17:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.985181 on epoch=199
05/17/2022 16:17:42 - INFO - __main__ - Global step 800 Train loss 0.895907 Classification-F1 0.6138888888888889 on epoch=199
05/17/2022 16:17:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.965968 on epoch=202
05/17/2022 16:17:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.846297 on epoch=204
05/17/2022 16:17:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.894318 on epoch=207
05/17/2022 16:17:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.899630 on epoch=209
05/17/2022 16:17:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.606598 on epoch=212
05/17/2022 16:17:55 - INFO - __main__ - Global step 850 Train loss 0.842562 Classification-F1 0.6138888888888889 on epoch=212
05/17/2022 16:17:58 - INFO - __main__ - Step 860 Global step 860 Train loss 1.131906 on epoch=214
05/17/2022 16:18:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.576446 on epoch=217
05/17/2022 16:18:03 - INFO - __main__ - Step 880 Global step 880 Train loss 1.017884 on epoch=219
05/17/2022 16:18:05 - INFO - __main__ - Step 890 Global step 890 Train loss 1.148176 on epoch=222
05/17/2022 16:18:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.745477 on epoch=224
05/17/2022 16:18:08 - INFO - __main__ - Global step 900 Train loss 0.923978 Classification-F1 0.6981695840391493 on epoch=224
05/17/2022 16:18:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.667140 on epoch=227
05/17/2022 16:18:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.496084 on epoch=229
05/17/2022 16:18:16 - INFO - __main__ - Step 930 Global step 930 Train loss 1.046445 on epoch=232
05/17/2022 16:18:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.969410 on epoch=234
05/17/2022 16:18:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.858455 on epoch=237
05/17/2022 16:18:21 - INFO - __main__ - Global step 950 Train loss 0.807507 Classification-F1 0.7310912698412699 on epoch=237
05/17/2022 16:18:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.757196 on epoch=239
05/17/2022 16:18:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.671384 on epoch=242
05/17/2022 16:18:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.918571 on epoch=244
05/17/2022 16:18:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.996780 on epoch=247
05/17/2022 16:18:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.731314 on epoch=249
05/17/2022 16:18:35 - INFO - __main__ - Global step 1000 Train loss 0.815049 Classification-F1 0.7101889338731443 on epoch=249
05/17/2022 16:18:35 - INFO - __main__ - save last model!
05/17/2022 16:18:36 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:18:36 - INFO - __main__ - Printing 3 examples
05/17/2022 16:18:36 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/17/2022 16:18:36 - INFO - __main__ - ['sad']
05/17/2022 16:18:36 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/17/2022 16:18:36 - INFO - __main__ - ['sad']
05/17/2022 16:18:36 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/17/2022 16:18:36 - INFO - __main__ - ['sad']
05/17/2022 16:18:36 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:18:36 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:18:36 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:18:36 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:18:36 - INFO - __main__ - Printing 3 examples
05/17/2022 16:18:36 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/17/2022 16:18:36 - INFO - __main__ - ['sad']
05/17/2022 16:18:36 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/17/2022 16:18:36 - INFO - __main__ - ['sad']
05/17/2022 16:18:36 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/17/2022 16:18:36 - INFO - __main__ - ['sad']
05/17/2022 16:18:36 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:18:36 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:18:36 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:18:37 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 16:18:38 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 16:18:38 - INFO - __main__ - Printing 3 examples
05/17/2022 16:18:38 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 16:18:38 - INFO - __main__ - ['others']
05/17/2022 16:18:38 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 16:18:38 - INFO - __main__ - ['others']
05/17/2022 16:18:38 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 16:18:38 - INFO - __main__ - ['others']
05/17/2022 16:18:38 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:18:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:18:40 - INFO - __main__ - Starting training!
05/17/2022 16:18:40 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:18:45 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 16:19:13 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_13_0.0001_8_predictions.txt
05/17/2022 16:19:13 - INFO - __main__ - Classification-F1 on test data: 0.3575
05/17/2022 16:19:14 - INFO - __main__ - prefix=emo_16_13, lr=0.0001, bsz=8, dev_performance=0.7310912698412699, test_performance=0.35753057997407134
05/17/2022 16:19:14 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.0005, bsz=8 ...
05/17/2022 16:19:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:19:15 - INFO - __main__ - Printing 3 examples
05/17/2022 16:19:15 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/17/2022 16:19:15 - INFO - __main__ - ['sad']
05/17/2022 16:19:15 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/17/2022 16:19:15 - INFO - __main__ - ['sad']
05/17/2022 16:19:15 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/17/2022 16:19:15 - INFO - __main__ - ['sad']
05/17/2022 16:19:15 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:19:15 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:19:15 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:19:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:19:15 - INFO - __main__ - Printing 3 examples
05/17/2022 16:19:15 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/17/2022 16:19:15 - INFO - __main__ - ['sad']
05/17/2022 16:19:15 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/17/2022 16:19:15 - INFO - __main__ - ['sad']
05/17/2022 16:19:15 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/17/2022 16:19:15 - INFO - __main__ - ['sad']
05/17/2022 16:19:15 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:19:15 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:19:15 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:19:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:19:19 - INFO - __main__ - Starting training!
05/17/2022 16:19:22 - INFO - __main__ - Step 10 Global step 10 Train loss 19.595755 on epoch=2
05/17/2022 16:19:24 - INFO - __main__ - Step 20 Global step 20 Train loss 16.125452 on epoch=4
05/17/2022 16:19:27 - INFO - __main__ - Step 30 Global step 30 Train loss 8.682070 on epoch=7
05/17/2022 16:19:30 - INFO - __main__ - Step 40 Global step 40 Train loss 5.515838 on epoch=9
05/17/2022 16:19:32 - INFO - __main__ - Step 50 Global step 50 Train loss 5.093097 on epoch=12
05/17/2022 16:19:33 - INFO - __main__ - Global step 50 Train loss 11.002442 Classification-F1 0.1 on epoch=12
05/17/2022 16:19:36 - INFO - __main__ - Step 60 Global step 60 Train loss 4.271138 on epoch=14
05/17/2022 16:19:38 - INFO - __main__ - Step 70 Global step 70 Train loss 2.954668 on epoch=17
05/17/2022 16:19:41 - INFO - __main__ - Step 80 Global step 80 Train loss 2.701829 on epoch=19
05/17/2022 16:19:43 - INFO - __main__ - Step 90 Global step 90 Train loss 2.909255 on epoch=22
05/17/2022 16:19:46 - INFO - __main__ - Step 100 Global step 100 Train loss 2.296267 on epoch=24
05/17/2022 16:19:46 - INFO - __main__ - Global step 100 Train loss 3.026631 Classification-F1 0.25317460317460316 on epoch=24
05/17/2022 16:19:49 - INFO - __main__ - Step 110 Global step 110 Train loss 1.837661 on epoch=27
05/17/2022 16:19:52 - INFO - __main__ - Step 120 Global step 120 Train loss 2.020002 on epoch=29
05/17/2022 16:19:54 - INFO - __main__ - Step 130 Global step 130 Train loss 1.970633 on epoch=32
05/17/2022 16:19:57 - INFO - __main__ - Step 140 Global step 140 Train loss 1.592895 on epoch=34
05/17/2022 16:19:59 - INFO - __main__ - Step 150 Global step 150 Train loss 1.297703 on epoch=37
05/17/2022 16:19:59 - INFO - __main__ - Global step 150 Train loss 1.743779 Classification-F1 0.20980302336234538 on epoch=37
05/17/2022 16:20:02 - INFO - __main__ - Step 160 Global step 160 Train loss 1.488550 on epoch=39
05/17/2022 16:20:04 - INFO - __main__ - Step 170 Global step 170 Train loss 2.848735 on epoch=42
05/17/2022 16:20:07 - INFO - __main__ - Step 180 Global step 180 Train loss 1.798934 on epoch=44
05/17/2022 16:20:09 - INFO - __main__ - Step 190 Global step 190 Train loss 1.442849 on epoch=47
05/17/2022 16:20:12 - INFO - __main__ - Step 200 Global step 200 Train loss 1.204566 on epoch=49
05/17/2022 16:20:12 - INFO - __main__ - Global step 200 Train loss 1.756727 Classification-F1 0.27810254536597245 on epoch=49
05/17/2022 16:20:15 - INFO - __main__ - Step 210 Global step 210 Train loss 1.358337 on epoch=52
05/17/2022 16:20:17 - INFO - __main__ - Step 220 Global step 220 Train loss 1.255404 on epoch=54
05/17/2022 16:20:20 - INFO - __main__ - Step 230 Global step 230 Train loss 1.101726 on epoch=57
05/17/2022 16:20:22 - INFO - __main__ - Step 240 Global step 240 Train loss 1.124528 on epoch=59
05/17/2022 16:20:25 - INFO - __main__ - Step 250 Global step 250 Train loss 1.243303 on epoch=62
05/17/2022 16:20:25 - INFO - __main__ - Global step 250 Train loss 1.216660 Classification-F1 0.30237154150197626 on epoch=62
05/17/2022 16:20:28 - INFO - __main__ - Step 260 Global step 260 Train loss 1.053195 on epoch=64
05/17/2022 16:20:30 - INFO - __main__ - Step 270 Global step 270 Train loss 1.059820 on epoch=67
05/17/2022 16:20:33 - INFO - __main__ - Step 280 Global step 280 Train loss 1.462590 on epoch=69
05/17/2022 16:20:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.970435 on epoch=72
05/17/2022 16:20:38 - INFO - __main__ - Step 300 Global step 300 Train loss 1.202934 on epoch=74
05/17/2022 16:20:38 - INFO - __main__ - Global step 300 Train loss 1.149795 Classification-F1 0.26071761416589 on epoch=74
05/17/2022 16:20:41 - INFO - __main__ - Step 310 Global step 310 Train loss 1.379455 on epoch=77
05/17/2022 16:20:43 - INFO - __main__ - Step 320 Global step 320 Train loss 1.119295 on epoch=79
05/17/2022 16:20:46 - INFO - __main__ - Step 330 Global step 330 Train loss 1.135418 on epoch=82
05/17/2022 16:20:48 - INFO - __main__ - Step 340 Global step 340 Train loss 1.259973 on epoch=84
05/17/2022 16:20:51 - INFO - __main__ - Step 350 Global step 350 Train loss 1.253059 on epoch=87
05/17/2022 16:20:51 - INFO - __main__ - Global step 350 Train loss 1.229440 Classification-F1 0.18941176470588234 on epoch=87
05/17/2022 16:20:54 - INFO - __main__ - Step 360 Global step 360 Train loss 1.200672 on epoch=89
05/17/2022 16:20:56 - INFO - __main__ - Step 370 Global step 370 Train loss 1.020725 on epoch=92
05/17/2022 16:20:59 - INFO - __main__ - Step 380 Global step 380 Train loss 1.038571 on epoch=94
05/17/2022 16:21:01 - INFO - __main__ - Step 390 Global step 390 Train loss 1.015445 on epoch=97
05/17/2022 16:21:04 - INFO - __main__ - Step 400 Global step 400 Train loss 1.019051 on epoch=99
05/17/2022 16:21:04 - INFO - __main__ - Global step 400 Train loss 1.058893 Classification-F1 0.19106247150022798 on epoch=99
05/17/2022 16:21:06 - INFO - __main__ - Step 410 Global step 410 Train loss 1.202170 on epoch=102
05/17/2022 16:21:09 - INFO - __main__ - Step 420 Global step 420 Train loss 1.159808 on epoch=104
05/17/2022 16:21:11 - INFO - __main__ - Step 430 Global step 430 Train loss 1.174572 on epoch=107
05/17/2022 16:21:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.877500 on epoch=109
05/17/2022 16:21:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.943252 on epoch=112
05/17/2022 16:21:17 - INFO - __main__ - Global step 450 Train loss 1.071460 Classification-F1 0.2426734134514021 on epoch=112
05/17/2022 16:21:19 - INFO - __main__ - Step 460 Global step 460 Train loss 1.137324 on epoch=114
05/17/2022 16:21:22 - INFO - __main__ - Step 470 Global step 470 Train loss 1.091649 on epoch=117
05/17/2022 16:21:24 - INFO - __main__ - Step 480 Global step 480 Train loss 1.058876 on epoch=119
05/17/2022 16:21:27 - INFO - __main__ - Step 490 Global step 490 Train loss 1.018118 on epoch=122
05/17/2022 16:21:29 - INFO - __main__ - Step 500 Global step 500 Train loss 1.017445 on epoch=124
05/17/2022 16:21:29 - INFO - __main__ - Global step 500 Train loss 1.064682 Classification-F1 0.29880959574596655 on epoch=124
05/17/2022 16:21:32 - INFO - __main__ - Step 510 Global step 510 Train loss 1.065821 on epoch=127
05/17/2022 16:21:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.939026 on epoch=129
05/17/2022 16:21:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.892234 on epoch=132
05/17/2022 16:21:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.999079 on epoch=134
05/17/2022 16:21:42 - INFO - __main__ - Step 550 Global step 550 Train loss 1.011129 on epoch=137
05/17/2022 16:21:42 - INFO - __main__ - Global step 550 Train loss 0.981458 Classification-F1 0.21137935423649706 on epoch=137
05/17/2022 16:21:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.842779 on epoch=139
05/17/2022 16:21:47 - INFO - __main__ - Step 570 Global step 570 Train loss 1.421068 on epoch=142
05/17/2022 16:21:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.905648 on epoch=144
05/17/2022 16:21:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.987873 on epoch=147
05/17/2022 16:21:55 - INFO - __main__ - Step 600 Global step 600 Train loss 1.167585 on epoch=149
05/17/2022 16:21:55 - INFO - __main__ - Global step 600 Train loss 1.064991 Classification-F1 0.2945670628183362 on epoch=149
05/17/2022 16:21:58 - INFO - __main__ - Step 610 Global step 610 Train loss 1.116258 on epoch=152
05/17/2022 16:22:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.888193 on epoch=154
05/17/2022 16:22:02 - INFO - __main__ - Step 630 Global step 630 Train loss 1.120790 on epoch=157
05/17/2022 16:22:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.901163 on epoch=159
05/17/2022 16:22:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.930577 on epoch=162
05/17/2022 16:22:08 - INFO - __main__ - Global step 650 Train loss 0.991396 Classification-F1 0.35389610389610393 on epoch=162
05/17/2022 16:22:11 - INFO - __main__ - Step 660 Global step 660 Train loss 1.007664 on epoch=164
05/17/2022 16:22:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.831962 on epoch=167
05/17/2022 16:22:15 - INFO - __main__ - Step 680 Global step 680 Train loss 1.118447 on epoch=169
05/17/2022 16:22:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.792035 on epoch=172
05/17/2022 16:22:20 - INFO - __main__ - Step 700 Global step 700 Train loss 1.007657 on epoch=174
05/17/2022 16:22:21 - INFO - __main__ - Global step 700 Train loss 0.951553 Classification-F1 0.3145945945945946 on epoch=174
05/17/2022 16:22:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.932590 on epoch=177
05/17/2022 16:22:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.833943 on epoch=179
05/17/2022 16:22:28 - INFO - __main__ - Step 730 Global step 730 Train loss 1.068699 on epoch=182
05/17/2022 16:22:31 - INFO - __main__ - Step 740 Global step 740 Train loss 1.109457 on epoch=184
05/17/2022 16:22:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.754887 on epoch=187
05/17/2022 16:22:34 - INFO - __main__ - Global step 750 Train loss 0.939915 Classification-F1 0.26037972289213596 on epoch=187
05/17/2022 16:22:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.885051 on epoch=189
05/17/2022 16:22:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.978812 on epoch=192
05/17/2022 16:22:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.967915 on epoch=194
05/17/2022 16:22:43 - INFO - __main__ - Step 790 Global step 790 Train loss 1.020996 on epoch=197
05/17/2022 16:22:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.806530 on epoch=199
05/17/2022 16:22:46 - INFO - __main__ - Global step 800 Train loss 0.931861 Classification-F1 0.2532705389848247 on epoch=199
05/17/2022 16:22:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.927435 on epoch=202
05/17/2022 16:22:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.984757 on epoch=204
05/17/2022 16:22:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.906090 on epoch=207
05/17/2022 16:22:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.801765 on epoch=209
05/17/2022 16:22:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.813042 on epoch=212
05/17/2022 16:22:59 - INFO - __main__ - Global step 850 Train loss 0.886617 Classification-F1 0.24678063445057657 on epoch=212
05/17/2022 16:23:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.870552 on epoch=214
05/17/2022 16:23:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.951147 on epoch=217
05/17/2022 16:23:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.923367 on epoch=219
05/17/2022 16:23:09 - INFO - __main__ - Step 890 Global step 890 Train loss 1.029209 on epoch=222
05/17/2022 16:23:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.795875 on epoch=224
05/17/2022 16:23:12 - INFO - __main__ - Global step 900 Train loss 0.914030 Classification-F1 0.20402046783625732 on epoch=224
05/17/2022 16:23:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.882432 on epoch=227
05/17/2022 16:23:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.869390 on epoch=229
05/17/2022 16:23:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.940828 on epoch=232
05/17/2022 16:23:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.848656 on epoch=234
05/17/2022 16:23:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.878035 on epoch=237
05/17/2022 16:23:25 - INFO - __main__ - Global step 950 Train loss 0.883868 Classification-F1 0.31375 on epoch=237
05/17/2022 16:23:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.886232 on epoch=239
05/17/2022 16:23:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.849728 on epoch=242
05/17/2022 16:23:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.809089 on epoch=244
05/17/2022 16:23:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.819003 on epoch=247
05/17/2022 16:23:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.786666 on epoch=249
05/17/2022 16:23:38 - INFO - __main__ - Global step 1000 Train loss 0.830144 Classification-F1 0.2368814192343604 on epoch=249
05/17/2022 16:23:38 - INFO - __main__ - save last model!
05/17/2022 16:23:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:23:38 - INFO - __main__ - Printing 3 examples
05/17/2022 16:23:38 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/17/2022 16:23:38 - INFO - __main__ - ['sad']
05/17/2022 16:23:38 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/17/2022 16:23:38 - INFO - __main__ - ['sad']
05/17/2022 16:23:38 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/17/2022 16:23:38 - INFO - __main__ - ['sad']
05/17/2022 16:23:38 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:23:38 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:23:39 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:23:39 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:23:39 - INFO - __main__ - Printing 3 examples
05/17/2022 16:23:39 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/17/2022 16:23:39 - INFO - __main__ - ['sad']
05/17/2022 16:23:39 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/17/2022 16:23:39 - INFO - __main__ - ['sad']
05/17/2022 16:23:39 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/17/2022 16:23:39 - INFO - __main__ - ['sad']
05/17/2022 16:23:39 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:23:39 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:23:39 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:23:40 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 16:23:40 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 16:23:40 - INFO - __main__ - Printing 3 examples
05/17/2022 16:23:40 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 16:23:40 - INFO - __main__ - ['others']
05/17/2022 16:23:40 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 16:23:40 - INFO - __main__ - ['others']
05/17/2022 16:23:40 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 16:23:40 - INFO - __main__ - ['others']
05/17/2022 16:23:40 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:23:42 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:23:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:23:43 - INFO - __main__ - Starting training!
05/17/2022 16:23:48 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 16:24:16 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_21_0.0005_8_predictions.txt
05/17/2022 16:24:16 - INFO - __main__ - Classification-F1 on test data: 0.0962
05/17/2022 16:24:16 - INFO - __main__ - prefix=emo_16_21, lr=0.0005, bsz=8, dev_performance=0.35389610389610393, test_performance=0.09624342095279476
05/17/2022 16:24:16 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.0003, bsz=8 ...
05/17/2022 16:24:17 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:24:17 - INFO - __main__ - Printing 3 examples
05/17/2022 16:24:17 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/17/2022 16:24:17 - INFO - __main__ - ['sad']
05/17/2022 16:24:17 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/17/2022 16:24:17 - INFO - __main__ - ['sad']
05/17/2022 16:24:17 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/17/2022 16:24:17 - INFO - __main__ - ['sad']
05/17/2022 16:24:17 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:24:17 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:24:17 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:24:17 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:24:17 - INFO - __main__ - Printing 3 examples
05/17/2022 16:24:17 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/17/2022 16:24:17 - INFO - __main__ - ['sad']
05/17/2022 16:24:17 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/17/2022 16:24:17 - INFO - __main__ - ['sad']
05/17/2022 16:24:17 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/17/2022 16:24:17 - INFO - __main__ - ['sad']
05/17/2022 16:24:17 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:24:17 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:24:17 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:24:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:24:21 - INFO - __main__ - Starting training!
05/17/2022 16:24:23 - INFO - __main__ - Step 10 Global step 10 Train loss 19.143833 on epoch=2
05/17/2022 16:24:25 - INFO - __main__ - Step 20 Global step 20 Train loss 18.624968 on epoch=4
05/17/2022 16:24:27 - INFO - __main__ - Step 30 Global step 30 Train loss 12.065682 on epoch=7
05/17/2022 16:24:30 - INFO - __main__ - Step 40 Global step 40 Train loss 8.479048 on epoch=9
05/17/2022 16:24:32 - INFO - __main__ - Step 50 Global step 50 Train loss 7.024104 on epoch=12
05/17/2022 16:24:36 - INFO - __main__ - Global step 50 Train loss 13.067527 Classification-F1 0.012000000000000002 on epoch=12
05/17/2022 16:24:39 - INFO - __main__ - Step 60 Global step 60 Train loss 5.859340 on epoch=14
05/17/2022 16:24:42 - INFO - __main__ - Step 70 Global step 70 Train loss 4.406659 on epoch=17
05/17/2022 16:24:44 - INFO - __main__ - Step 80 Global step 80 Train loss 4.487922 on epoch=19
05/17/2022 16:24:47 - INFO - __main__ - Step 90 Global step 90 Train loss 3.843697 on epoch=22
05/17/2022 16:24:49 - INFO - __main__ - Step 100 Global step 100 Train loss 3.602828 on epoch=24
05/17/2022 16:24:50 - INFO - __main__ - Global step 100 Train loss 4.440089 Classification-F1 0.13067758749069247 on epoch=24
05/17/2022 16:24:53 - INFO - __main__ - Step 110 Global step 110 Train loss 3.567965 on epoch=27
05/17/2022 16:24:55 - INFO - __main__ - Step 120 Global step 120 Train loss 3.262013 on epoch=29
05/17/2022 16:24:57 - INFO - __main__ - Step 130 Global step 130 Train loss 2.616422 on epoch=32
05/17/2022 16:25:00 - INFO - __main__ - Step 140 Global step 140 Train loss 2.822494 on epoch=34
05/17/2022 16:25:02 - INFO - __main__ - Step 150 Global step 150 Train loss 2.809663 on epoch=37
05/17/2022 16:25:03 - INFO - __main__ - Global step 150 Train loss 3.015711 Classification-F1 0.25453560371517026 on epoch=37
05/17/2022 16:25:06 - INFO - __main__ - Step 160 Global step 160 Train loss 2.383608 on epoch=39
05/17/2022 16:25:08 - INFO - __main__ - Step 170 Global step 170 Train loss 2.218632 on epoch=42
05/17/2022 16:25:11 - INFO - __main__ - Step 180 Global step 180 Train loss 2.239555 on epoch=44
05/17/2022 16:25:13 - INFO - __main__ - Step 190 Global step 190 Train loss 1.733498 on epoch=47
05/17/2022 16:25:16 - INFO - __main__ - Step 200 Global step 200 Train loss 1.983592 on epoch=49
05/17/2022 16:25:16 - INFO - __main__ - Global step 200 Train loss 2.111777 Classification-F1 0.3880460238264942 on epoch=49
05/17/2022 16:25:19 - INFO - __main__ - Step 210 Global step 210 Train loss 1.785389 on epoch=52
05/17/2022 16:25:21 - INFO - __main__ - Step 220 Global step 220 Train loss 1.594245 on epoch=54
05/17/2022 16:25:24 - INFO - __main__ - Step 230 Global step 230 Train loss 1.233540 on epoch=57
05/17/2022 16:25:26 - INFO - __main__ - Step 240 Global step 240 Train loss 1.475545 on epoch=59
05/17/2022 16:25:29 - INFO - __main__ - Step 250 Global step 250 Train loss 1.482279 on epoch=62
05/17/2022 16:25:29 - INFO - __main__ - Global step 250 Train loss 1.514200 Classification-F1 0.458982683982684 on epoch=62
05/17/2022 16:25:32 - INFO - __main__ - Step 260 Global step 260 Train loss 1.597797 on epoch=64
05/17/2022 16:25:35 - INFO - __main__ - Step 270 Global step 270 Train loss 1.356998 on epoch=67
05/17/2022 16:25:37 - INFO - __main__ - Step 280 Global step 280 Train loss 1.331105 on epoch=69
05/17/2022 16:25:40 - INFO - __main__ - Step 290 Global step 290 Train loss 1.316127 on epoch=72
05/17/2022 16:25:42 - INFO - __main__ - Step 300 Global step 300 Train loss 1.202385 on epoch=74
05/17/2022 16:25:43 - INFO - __main__ - Global step 300 Train loss 1.360883 Classification-F1 0.4508013381569223 on epoch=74
05/17/2022 16:25:45 - INFO - __main__ - Step 310 Global step 310 Train loss 1.936191 on epoch=77
05/17/2022 16:25:48 - INFO - __main__ - Step 320 Global step 320 Train loss 1.784618 on epoch=79
05/17/2022 16:25:50 - INFO - __main__ - Step 330 Global step 330 Train loss 1.550932 on epoch=82
05/17/2022 16:25:53 - INFO - __main__ - Step 340 Global step 340 Train loss 1.416973 on epoch=84
05/17/2022 16:25:55 - INFO - __main__ - Step 350 Global step 350 Train loss 1.298873 on epoch=87
05/17/2022 16:25:56 - INFO - __main__ - Global step 350 Train loss 1.597517 Classification-F1 0.4751514461984965 on epoch=87
05/17/2022 16:25:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.922142 on epoch=89
05/17/2022 16:26:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.959774 on epoch=92
05/17/2022 16:26:03 - INFO - __main__ - Step 380 Global step 380 Train loss 1.142037 on epoch=94
05/17/2022 16:26:06 - INFO - __main__ - Step 390 Global step 390 Train loss 1.306827 on epoch=97
05/17/2022 16:26:08 - INFO - __main__ - Step 400 Global step 400 Train loss 1.023155 on epoch=99
05/17/2022 16:26:09 - INFO - __main__ - Global step 400 Train loss 1.070787 Classification-F1 0.4243873564899847 on epoch=99
05/17/2022 16:26:11 - INFO - __main__ - Step 410 Global step 410 Train loss 1.154386 on epoch=102
05/17/2022 16:26:14 - INFO - __main__ - Step 420 Global step 420 Train loss 1.223383 on epoch=104
05/17/2022 16:26:16 - INFO - __main__ - Step 430 Global step 430 Train loss 1.279390 on epoch=107
05/17/2022 16:26:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.968341 on epoch=109
05/17/2022 16:26:21 - INFO - __main__ - Step 450 Global step 450 Train loss 1.190924 on epoch=112
05/17/2022 16:26:22 - INFO - __main__ - Global step 450 Train loss 1.163285 Classification-F1 0.4904115492350787 on epoch=112
05/17/2022 16:26:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.950591 on epoch=114
05/17/2022 16:26:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.983050 on epoch=117
05/17/2022 16:26:30 - INFO - __main__ - Step 480 Global step 480 Train loss 1.159677 on epoch=119
05/17/2022 16:26:32 - INFO - __main__ - Step 490 Global step 490 Train loss 1.081913 on epoch=122
05/17/2022 16:26:35 - INFO - __main__ - Step 500 Global step 500 Train loss 1.045704 on epoch=124
05/17/2022 16:26:35 - INFO - __main__ - Global step 500 Train loss 1.044187 Classification-F1 0.5254399439458862 on epoch=124
05/17/2022 16:26:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.858740 on epoch=127
05/17/2022 16:26:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.988567 on epoch=129
05/17/2022 16:26:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.986759 on epoch=132
05/17/2022 16:26:45 - INFO - __main__ - Step 540 Global step 540 Train loss 1.079918 on epoch=134
05/17/2022 16:26:48 - INFO - __main__ - Step 550 Global step 550 Train loss 1.128338 on epoch=137
05/17/2022 16:26:48 - INFO - __main__ - Global step 550 Train loss 1.008464 Classification-F1 0.515441577941578 on epoch=137
05/17/2022 16:26:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.791778 on epoch=139
05/17/2022 16:26:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.895636 on epoch=142
05/17/2022 16:26:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.745071 on epoch=144
05/17/2022 16:26:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.873028 on epoch=147
05/17/2022 16:27:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.844604 on epoch=149
05/17/2022 16:27:01 - INFO - __main__ - Global step 600 Train loss 0.830023 Classification-F1 0.5398110661268556 on epoch=149
05/17/2022 16:27:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.962660 on epoch=152
05/17/2022 16:27:07 - INFO - __main__ - Step 620 Global step 620 Train loss 1.001684 on epoch=154
05/17/2022 16:27:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.900864 on epoch=157
05/17/2022 16:27:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.874627 on epoch=159
05/17/2022 16:27:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.786748 on epoch=162
05/17/2022 16:27:14 - INFO - __main__ - Global step 650 Train loss 0.905317 Classification-F1 0.5695566796208336 on epoch=162
05/17/2022 16:27:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.871998 on epoch=164
05/17/2022 16:27:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.830916 on epoch=167
05/17/2022 16:27:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.653077 on epoch=169
05/17/2022 16:27:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.867846 on epoch=172
05/17/2022 16:27:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.849327 on epoch=174
05/17/2022 16:27:28 - INFO - __main__ - Global step 700 Train loss 0.814633 Classification-F1 0.49035702282213905 on epoch=174
05/17/2022 16:27:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.910213 on epoch=177
05/17/2022 16:27:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.816722 on epoch=179
05/17/2022 16:27:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.833058 on epoch=182
05/17/2022 16:27:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.689816 on epoch=184
05/17/2022 16:27:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.696447 on epoch=187
05/17/2022 16:27:40 - INFO - __main__ - Global step 750 Train loss 0.789251 Classification-F1 0.48712185668368313 on epoch=187
05/17/2022 16:27:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.592616 on epoch=189
05/17/2022 16:27:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.864324 on epoch=192
05/17/2022 16:27:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.559343 on epoch=194
05/17/2022 16:27:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.893907 on epoch=197
05/17/2022 16:27:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.747942 on epoch=199
05/17/2022 16:27:54 - INFO - __main__ - Global step 800 Train loss 0.731626 Classification-F1 0.5108199765836328 on epoch=199
05/17/2022 16:27:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.800647 on epoch=202
05/17/2022 16:27:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.646667 on epoch=204
05/17/2022 16:28:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.594392 on epoch=207
05/17/2022 16:28:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.671640 on epoch=209
05/17/2022 16:28:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.535332 on epoch=212
05/17/2022 16:28:07 - INFO - __main__ - Global step 850 Train loss 0.649735 Classification-F1 0.6062544326241135 on epoch=212
05/17/2022 16:28:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.597369 on epoch=214
05/17/2022 16:28:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.536081 on epoch=217
05/17/2022 16:28:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.576029 on epoch=219
05/17/2022 16:28:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.849547 on epoch=222
05/17/2022 16:28:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.717382 on epoch=224
05/17/2022 16:28:20 - INFO - __main__ - Global step 900 Train loss 0.655281 Classification-F1 0.6027131782945736 on epoch=224
05/17/2022 16:28:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.729792 on epoch=227
05/17/2022 16:28:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.725929 on epoch=229
05/17/2022 16:28:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.756136 on epoch=232
05/17/2022 16:28:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.630794 on epoch=234
05/17/2022 16:28:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.637106 on epoch=237
05/17/2022 16:28:33 - INFO - __main__ - Global step 950 Train loss 0.695951 Classification-F1 0.6367886178861788 on epoch=237
05/17/2022 16:28:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.749809 on epoch=239
05/17/2022 16:28:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.711063 on epoch=242
05/17/2022 16:28:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.581397 on epoch=244
05/17/2022 16:28:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.830538 on epoch=247
05/17/2022 16:28:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.544927 on epoch=249
05/17/2022 16:28:46 - INFO - __main__ - Global step 1000 Train loss 0.683547 Classification-F1 0.6308148446785011 on epoch=249
05/17/2022 16:28:46 - INFO - __main__ - save last model!
05/17/2022 16:28:47 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:28:47 - INFO - __main__ - Printing 3 examples
05/17/2022 16:28:47 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/17/2022 16:28:47 - INFO - __main__ - ['sad']
05/17/2022 16:28:47 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/17/2022 16:28:47 - INFO - __main__ - ['sad']
05/17/2022 16:28:47 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/17/2022 16:28:47 - INFO - __main__ - ['sad']
05/17/2022 16:28:47 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:28:47 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:28:47 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:28:47 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:28:47 - INFO - __main__ - Printing 3 examples
05/17/2022 16:28:47 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/17/2022 16:28:47 - INFO - __main__ - ['sad']
05/17/2022 16:28:47 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/17/2022 16:28:47 - INFO - __main__ - ['sad']
05/17/2022 16:28:47 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/17/2022 16:28:47 - INFO - __main__ - ['sad']
05/17/2022 16:28:47 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:28:47 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:28:47 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:28:49 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 16:28:49 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 16:28:49 - INFO - __main__ - Printing 3 examples
05/17/2022 16:28:49 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 16:28:49 - INFO - __main__ - ['others']
05/17/2022 16:28:49 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 16:28:49 - INFO - __main__ - ['others']
05/17/2022 16:28:49 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 16:28:49 - INFO - __main__ - ['others']
05/17/2022 16:28:49 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:28:51 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:28:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:28:51 - INFO - __main__ - Starting training!
05/17/2022 16:28:57 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 16:29:25 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_21_0.0003_8_predictions.txt
05/17/2022 16:29:25 - INFO - __main__ - Classification-F1 on test data: 0.2801
05/17/2022 16:29:25 - INFO - __main__ - prefix=emo_16_21, lr=0.0003, bsz=8, dev_performance=0.6367886178861788, test_performance=0.2801058667883809
05/17/2022 16:29:25 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.0002, bsz=8 ...
05/17/2022 16:29:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:29:26 - INFO - __main__ - Printing 3 examples
05/17/2022 16:29:26 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/17/2022 16:29:26 - INFO - __main__ - ['sad']
05/17/2022 16:29:26 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/17/2022 16:29:26 - INFO - __main__ - ['sad']
05/17/2022 16:29:26 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/17/2022 16:29:26 - INFO - __main__ - ['sad']
05/17/2022 16:29:26 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:29:26 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:29:26 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:29:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:29:26 - INFO - __main__ - Printing 3 examples
05/17/2022 16:29:26 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/17/2022 16:29:26 - INFO - __main__ - ['sad']
05/17/2022 16:29:26 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/17/2022 16:29:26 - INFO - __main__ - ['sad']
05/17/2022 16:29:26 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/17/2022 16:29:26 - INFO - __main__ - ['sad']
05/17/2022 16:29:26 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:29:26 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:29:26 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:29:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:29:30 - INFO - __main__ - Starting training!
05/17/2022 16:29:32 - INFO - __main__ - Step 10 Global step 10 Train loss 20.100163 on epoch=2
05/17/2022 16:29:35 - INFO - __main__ - Step 20 Global step 20 Train loss 19.629200 on epoch=4
05/17/2022 16:29:37 - INFO - __main__ - Step 30 Global step 30 Train loss 13.670237 on epoch=7
05/17/2022 16:29:40 - INFO - __main__ - Step 40 Global step 40 Train loss 9.059789 on epoch=9
05/17/2022 16:29:42 - INFO - __main__ - Step 50 Global step 50 Train loss 7.338145 on epoch=12
05/17/2022 16:29:49 - INFO - __main__ - Global step 50 Train loss 13.959508 Classification-F1 0.0 on epoch=12
05/17/2022 16:29:52 - INFO - __main__ - Step 60 Global step 60 Train loss 7.369569 on epoch=14
05/17/2022 16:29:54 - INFO - __main__ - Step 70 Global step 70 Train loss 5.025889 on epoch=17
05/17/2022 16:29:57 - INFO - __main__ - Step 80 Global step 80 Train loss 5.380620 on epoch=19
05/17/2022 16:29:59 - INFO - __main__ - Step 90 Global step 90 Train loss 5.298578 on epoch=22
05/17/2022 16:30:02 - INFO - __main__ - Step 100 Global step 100 Train loss 4.455369 on epoch=24
05/17/2022 16:30:02 - INFO - __main__ - Global step 100 Train loss 5.506005 Classification-F1 0.13067758749069247 on epoch=24
05/17/2022 16:30:05 - INFO - __main__ - Step 110 Global step 110 Train loss 4.369081 on epoch=27
05/17/2022 16:30:08 - INFO - __main__ - Step 120 Global step 120 Train loss 2.806982 on epoch=29
05/17/2022 16:30:10 - INFO - __main__ - Step 130 Global step 130 Train loss 3.024277 on epoch=32
05/17/2022 16:30:13 - INFO - __main__ - Step 140 Global step 140 Train loss 2.950846 on epoch=34
05/17/2022 16:30:15 - INFO - __main__ - Step 150 Global step 150 Train loss 3.145061 on epoch=37
05/17/2022 16:30:16 - INFO - __main__ - Global step 150 Train loss 3.259250 Classification-F1 0.29677287581699346 on epoch=37
05/17/2022 16:30:19 - INFO - __main__ - Step 160 Global step 160 Train loss 2.680719 on epoch=39
05/17/2022 16:30:21 - INFO - __main__ - Step 170 Global step 170 Train loss 3.056741 on epoch=42
05/17/2022 16:30:24 - INFO - __main__ - Step 180 Global step 180 Train loss 2.567475 on epoch=44
05/17/2022 16:30:26 - INFO - __main__ - Step 190 Global step 190 Train loss 1.885105 on epoch=47
05/17/2022 16:30:29 - INFO - __main__ - Step 200 Global step 200 Train loss 1.578506 on epoch=49
05/17/2022 16:30:29 - INFO - __main__ - Global step 200 Train loss 2.353709 Classification-F1 0.40875420875420876 on epoch=49
05/17/2022 16:30:32 - INFO - __main__ - Step 210 Global step 210 Train loss 2.053296 on epoch=52
05/17/2022 16:30:34 - INFO - __main__ - Step 220 Global step 220 Train loss 1.686476 on epoch=54
05/17/2022 16:30:37 - INFO - __main__ - Step 230 Global step 230 Train loss 1.877031 on epoch=57
05/17/2022 16:30:39 - INFO - __main__ - Step 240 Global step 240 Train loss 1.352020 on epoch=59
05/17/2022 16:30:42 - INFO - __main__ - Step 250 Global step 250 Train loss 2.609302 on epoch=62
05/17/2022 16:30:42 - INFO - __main__ - Global step 250 Train loss 1.915625 Classification-F1 0.5041666666666667 on epoch=62
05/17/2022 16:30:45 - INFO - __main__ - Step 260 Global step 260 Train loss 1.483849 on epoch=64
05/17/2022 16:30:48 - INFO - __main__ - Step 270 Global step 270 Train loss 1.279468 on epoch=67
05/17/2022 16:30:50 - INFO - __main__ - Step 280 Global step 280 Train loss 1.148498 on epoch=69
05/17/2022 16:30:53 - INFO - __main__ - Step 290 Global step 290 Train loss 1.447327 on epoch=72
05/17/2022 16:30:55 - INFO - __main__ - Step 300 Global step 300 Train loss 1.084869 on epoch=74
05/17/2022 16:30:56 - INFO - __main__ - Global step 300 Train loss 1.288802 Classification-F1 0.3560337676279705 on epoch=74
05/17/2022 16:30:58 - INFO - __main__ - Step 310 Global step 310 Train loss 1.511087 on epoch=77
05/17/2022 16:31:01 - INFO - __main__ - Step 320 Global step 320 Train loss 1.499586 on epoch=79
05/17/2022 16:31:03 - INFO - __main__ - Step 330 Global step 330 Train loss 1.292119 on epoch=82
05/17/2022 16:31:06 - INFO - __main__ - Step 340 Global step 340 Train loss 1.163925 on epoch=84
05/17/2022 16:31:08 - INFO - __main__ - Step 350 Global step 350 Train loss 1.418771 on epoch=87
05/17/2022 16:31:08 - INFO - __main__ - Global step 350 Train loss 1.377098 Classification-F1 0.5162123912123912 on epoch=87
05/17/2022 16:31:11 - INFO - __main__ - Step 360 Global step 360 Train loss 1.259712 on epoch=89
05/17/2022 16:31:14 - INFO - __main__ - Step 370 Global step 370 Train loss 1.402653 on epoch=92
05/17/2022 16:31:16 - INFO - __main__ - Step 380 Global step 380 Train loss 1.110514 on epoch=94
05/17/2022 16:31:19 - INFO - __main__ - Step 390 Global step 390 Train loss 1.153371 on epoch=97
05/17/2022 16:31:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.864107 on epoch=99
05/17/2022 16:31:22 - INFO - __main__ - Global step 400 Train loss 1.158071 Classification-F1 0.5367923420249001 on epoch=99
05/17/2022 16:31:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.923108 on epoch=102
05/17/2022 16:31:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.842492 on epoch=104
05/17/2022 16:31:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.964268 on epoch=107
05/17/2022 16:31:32 - INFO - __main__ - Step 440 Global step 440 Train loss 1.097848 on epoch=109
05/17/2022 16:31:35 - INFO - __main__ - Step 450 Global step 450 Train loss 1.217229 on epoch=112
05/17/2022 16:31:35 - INFO - __main__ - Global step 450 Train loss 1.008989 Classification-F1 0.5613860115714819 on epoch=112
05/17/2022 16:31:38 - INFO - __main__ - Step 460 Global step 460 Train loss 1.187210 on epoch=114
05/17/2022 16:31:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.997128 on epoch=117
05/17/2022 16:31:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.828092 on epoch=119
05/17/2022 16:31:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.816583 on epoch=122
05/17/2022 16:31:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.781717 on epoch=124
05/17/2022 16:31:48 - INFO - __main__ - Global step 500 Train loss 0.922146 Classification-F1 0.6303370049829531 on epoch=124
05/17/2022 16:31:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.776398 on epoch=127
05/17/2022 16:31:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.701622 on epoch=129
05/17/2022 16:31:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.783487 on epoch=132
05/17/2022 16:31:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.768452 on epoch=134
05/17/2022 16:32:01 - INFO - __main__ - Step 550 Global step 550 Train loss 1.148015 on epoch=137
05/17/2022 16:32:01 - INFO - __main__ - Global step 550 Train loss 0.835595 Classification-F1 0.6550894720706874 on epoch=137
05/17/2022 16:32:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.862154 on epoch=139
05/17/2022 16:32:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.754632 on epoch=142
05/17/2022 16:32:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.669875 on epoch=144
05/17/2022 16:32:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.489407 on epoch=147
05/17/2022 16:32:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.501974 on epoch=149
05/17/2022 16:32:14 - INFO - __main__ - Global step 600 Train loss 0.655608 Classification-F1 0.6994824016563147 on epoch=149
05/17/2022 16:32:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.596336 on epoch=152
05/17/2022 16:32:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.534039 on epoch=154
05/17/2022 16:32:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.600553 on epoch=157
05/17/2022 16:32:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.625915 on epoch=159
05/17/2022 16:32:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.596628 on epoch=162
05/17/2022 16:32:28 - INFO - __main__ - Global step 650 Train loss 0.590694 Classification-F1 0.670477114074675 on epoch=162
05/17/2022 16:32:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.657367 on epoch=164
05/17/2022 16:32:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.628239 on epoch=167
05/17/2022 16:32:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.578091 on epoch=169
05/17/2022 16:32:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.504765 on epoch=172
05/17/2022 16:32:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.389935 on epoch=174
05/17/2022 16:32:41 - INFO - __main__ - Global step 700 Train loss 0.551679 Classification-F1 0.6633500907694456 on epoch=174
05/17/2022 16:32:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.765813 on epoch=177
05/17/2022 16:32:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.558071 on epoch=179
05/17/2022 16:32:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.464288 on epoch=182
05/17/2022 16:32:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.377592 on epoch=184
05/17/2022 16:32:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.569866 on epoch=187
05/17/2022 16:32:54 - INFO - __main__ - Global step 750 Train loss 0.547126 Classification-F1 0.720343137254902 on epoch=187
05/17/2022 16:32:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.574777 on epoch=189
05/17/2022 16:32:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.471907 on epoch=192
05/17/2022 16:33:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.440054 on epoch=194
05/17/2022 16:33:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.400138 on epoch=197
05/17/2022 16:33:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.334817 on epoch=199
05/17/2022 16:33:07 - INFO - __main__ - Global step 800 Train loss 0.444339 Classification-F1 0.7208333333333333 on epoch=199
05/17/2022 16:33:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.429302 on epoch=202
05/17/2022 16:33:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.220161 on epoch=204
05/17/2022 16:33:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.349655 on epoch=207
05/17/2022 16:33:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.268181 on epoch=209
05/17/2022 16:33:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.359188 on epoch=212
05/17/2022 16:33:20 - INFO - __main__ - Global step 850 Train loss 0.325298 Classification-F1 0.6695152423788105 on epoch=212
05/17/2022 16:33:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.315140 on epoch=214
05/17/2022 16:33:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.297918 on epoch=217
05/17/2022 16:33:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.230461 on epoch=219
05/17/2022 16:33:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.122599 on epoch=222
05/17/2022 16:33:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.407559 on epoch=224
05/17/2022 16:33:33 - INFO - __main__ - Global step 900 Train loss 0.274735 Classification-F1 0.6478387746259907 on epoch=224
05/17/2022 16:33:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.145988 on epoch=227
05/17/2022 16:33:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.280541 on epoch=229
05/17/2022 16:33:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.239354 on epoch=232
05/17/2022 16:33:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.323456 on epoch=234
05/17/2022 16:33:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.183145 on epoch=237
05/17/2022 16:33:46 - INFO - __main__ - Global step 950 Train loss 0.234497 Classification-F1 0.6482123189019741 on epoch=237
05/17/2022 16:33:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.259252 on epoch=239
05/17/2022 16:33:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.124086 on epoch=242
05/17/2022 16:33:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.204642 on epoch=244
05/17/2022 16:33:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.318765 on epoch=247
05/17/2022 16:33:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.294517 on epoch=249
05/17/2022 16:33:59 - INFO - __main__ - Global step 1000 Train loss 0.240252 Classification-F1 0.6624514966740576 on epoch=249
05/17/2022 16:33:59 - INFO - __main__ - save last model!
05/17/2022 16:33:59 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:33:59 - INFO - __main__ - Printing 3 examples
05/17/2022 16:33:59 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/17/2022 16:33:59 - INFO - __main__ - ['sad']
05/17/2022 16:33:59 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/17/2022 16:33:59 - INFO - __main__ - ['sad']
05/17/2022 16:33:59 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/17/2022 16:33:59 - INFO - __main__ - ['sad']
05/17/2022 16:33:59 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:33:59 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:33:59 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:33:59 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:33:59 - INFO - __main__ - Printing 3 examples
05/17/2022 16:33:59 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/17/2022 16:33:59 - INFO - __main__ - ['sad']
05/17/2022 16:33:59 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/17/2022 16:33:59 - INFO - __main__ - ['sad']
05/17/2022 16:33:59 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/17/2022 16:33:59 - INFO - __main__ - ['sad']
05/17/2022 16:33:59 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:33:59 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:33:59 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:34:01 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 16:34:01 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 16:34:01 - INFO - __main__ - Printing 3 examples
05/17/2022 16:34:01 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 16:34:01 - INFO - __main__ - ['others']
05/17/2022 16:34:01 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 16:34:01 - INFO - __main__ - ['others']
05/17/2022 16:34:01 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 16:34:01 - INFO - __main__ - ['others']
05/17/2022 16:34:01 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:34:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:34:03 - INFO - __main__ - Starting training!
05/17/2022 16:34:04 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:34:09 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 16:34:37 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_21_0.0002_8_predictions.txt
05/17/2022 16:34:37 - INFO - __main__ - Classification-F1 on test data: 0.3653
05/17/2022 16:34:37 - INFO - __main__ - prefix=emo_16_21, lr=0.0002, bsz=8, dev_performance=0.7208333333333333, test_performance=0.36529631320209033
05/17/2022 16:34:37 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.0001, bsz=8 ...
05/17/2022 16:34:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:34:38 - INFO - __main__ - Printing 3 examples
05/17/2022 16:34:38 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/17/2022 16:34:38 - INFO - __main__ - ['sad']
05/17/2022 16:34:38 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/17/2022 16:34:38 - INFO - __main__ - ['sad']
05/17/2022 16:34:38 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/17/2022 16:34:38 - INFO - __main__ - ['sad']
05/17/2022 16:34:38 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:34:38 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:34:38 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:34:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:34:38 - INFO - __main__ - Printing 3 examples
05/17/2022 16:34:38 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/17/2022 16:34:38 - INFO - __main__ - ['sad']
05/17/2022 16:34:38 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/17/2022 16:34:38 - INFO - __main__ - ['sad']
05/17/2022 16:34:38 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/17/2022 16:34:38 - INFO - __main__ - ['sad']
05/17/2022 16:34:38 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:34:38 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:34:39 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:34:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:34:42 - INFO - __main__ - Starting training!
05/17/2022 16:34:44 - INFO - __main__ - Step 10 Global step 10 Train loss 19.755779 on epoch=2
05/17/2022 16:34:47 - INFO - __main__ - Step 20 Global step 20 Train loss 19.241074 on epoch=4
05/17/2022 16:34:49 - INFO - __main__ - Step 30 Global step 30 Train loss 15.643690 on epoch=7
05/17/2022 16:34:52 - INFO - __main__ - Step 40 Global step 40 Train loss 13.955988 on epoch=9
05/17/2022 16:34:54 - INFO - __main__ - Step 50 Global step 50 Train loss 11.749243 on epoch=12
05/17/2022 16:35:06 - INFO - __main__ - Global step 50 Train loss 16.069155 Classification-F1 0.0 on epoch=12
05/17/2022 16:35:09 - INFO - __main__ - Step 60 Global step 60 Train loss 10.142225 on epoch=14
05/17/2022 16:35:12 - INFO - __main__ - Step 70 Global step 70 Train loss 8.623484 on epoch=17
05/17/2022 16:35:14 - INFO - __main__ - Step 80 Global step 80 Train loss 9.197367 on epoch=19
05/17/2022 16:35:17 - INFO - __main__ - Step 90 Global step 90 Train loss 8.327868 on epoch=22
05/17/2022 16:35:19 - INFO - __main__ - Step 100 Global step 100 Train loss 7.411546 on epoch=24
05/17/2022 16:35:27 - INFO - __main__ - Global step 100 Train loss 8.740498 Classification-F1 0.0 on epoch=24
05/17/2022 16:35:30 - INFO - __main__ - Step 110 Global step 110 Train loss 8.034338 on epoch=27
05/17/2022 16:35:32 - INFO - __main__ - Step 120 Global step 120 Train loss 6.488782 on epoch=29
05/17/2022 16:35:35 - INFO - __main__ - Step 130 Global step 130 Train loss 6.402215 on epoch=32
05/17/2022 16:35:37 - INFO - __main__ - Step 140 Global step 140 Train loss 5.443925 on epoch=34
05/17/2022 16:35:40 - INFO - __main__ - Step 150 Global step 150 Train loss 5.412324 on epoch=37
05/17/2022 16:35:41 - INFO - __main__ - Global step 150 Train loss 6.356317 Classification-F1 0.06837606837606837 on epoch=37
05/17/2022 16:35:44 - INFO - __main__ - Step 160 Global step 160 Train loss 5.170585 on epoch=39
05/17/2022 16:35:46 - INFO - __main__ - Step 170 Global step 170 Train loss 4.212664 on epoch=42
05/17/2022 16:35:49 - INFO - __main__ - Step 180 Global step 180 Train loss 5.009474 on epoch=44
05/17/2022 16:35:51 - INFO - __main__ - Step 190 Global step 190 Train loss 4.227974 on epoch=47
05/17/2022 16:35:54 - INFO - __main__ - Step 200 Global step 200 Train loss 4.567096 on epoch=49
05/17/2022 16:35:54 - INFO - __main__ - Global step 200 Train loss 4.637559 Classification-F1 0.1 on epoch=49
05/17/2022 16:35:57 - INFO - __main__ - Step 210 Global step 210 Train loss 3.770324 on epoch=52
05/17/2022 16:36:00 - INFO - __main__ - Step 220 Global step 220 Train loss 3.499950 on epoch=54
05/17/2022 16:36:02 - INFO - __main__ - Step 230 Global step 230 Train loss 3.325100 on epoch=57
05/17/2022 16:36:05 - INFO - __main__ - Step 240 Global step 240 Train loss 3.688211 on epoch=59
05/17/2022 16:36:07 - INFO - __main__ - Step 250 Global step 250 Train loss 3.585313 on epoch=62
05/17/2022 16:36:08 - INFO - __main__ - Global step 250 Train loss 3.573780 Classification-F1 0.1 on epoch=62
05/17/2022 16:36:10 - INFO - __main__ - Step 260 Global step 260 Train loss 2.848369 on epoch=64
05/17/2022 16:36:13 - INFO - __main__ - Step 270 Global step 270 Train loss 3.211064 on epoch=67
05/17/2022 16:36:15 - INFO - __main__ - Step 280 Global step 280 Train loss 3.380081 on epoch=69
05/17/2022 16:36:18 - INFO - __main__ - Step 290 Global step 290 Train loss 3.419387 on epoch=72
05/17/2022 16:36:20 - INFO - __main__ - Step 300 Global step 300 Train loss 2.679830 on epoch=74
05/17/2022 16:36:21 - INFO - __main__ - Global step 300 Train loss 3.107746 Classification-F1 0.1581196581196581 on epoch=74
05/17/2022 16:36:24 - INFO - __main__ - Step 310 Global step 310 Train loss 3.222391 on epoch=77
05/17/2022 16:36:26 - INFO - __main__ - Step 320 Global step 320 Train loss 2.966119 on epoch=79
05/17/2022 16:36:29 - INFO - __main__ - Step 330 Global step 330 Train loss 2.941418 on epoch=82
05/17/2022 16:36:31 - INFO - __main__ - Step 340 Global step 340 Train loss 2.636261 on epoch=84
05/17/2022 16:36:34 - INFO - __main__ - Step 350 Global step 350 Train loss 3.219789 on epoch=87
05/17/2022 16:36:34 - INFO - __main__ - Global step 350 Train loss 2.997195 Classification-F1 0.2510177984237173 on epoch=87
05/17/2022 16:36:37 - INFO - __main__ - Step 360 Global step 360 Train loss 2.822792 on epoch=89
05/17/2022 16:36:39 - INFO - __main__ - Step 370 Global step 370 Train loss 2.707325 on epoch=92
05/17/2022 16:36:42 - INFO - __main__ - Step 380 Global step 380 Train loss 2.638124 on epoch=94
05/17/2022 16:36:44 - INFO - __main__ - Step 390 Global step 390 Train loss 2.280257 on epoch=97
05/17/2022 16:36:47 - INFO - __main__ - Step 400 Global step 400 Train loss 2.754313 on epoch=99
05/17/2022 16:36:47 - INFO - __main__ - Global step 400 Train loss 2.640562 Classification-F1 0.2558061821219716 on epoch=99
05/17/2022 16:36:50 - INFO - __main__ - Step 410 Global step 410 Train loss 2.478922 on epoch=102
05/17/2022 16:36:53 - INFO - __main__ - Step 420 Global step 420 Train loss 2.431525 on epoch=104
05/17/2022 16:36:55 - INFO - __main__ - Step 430 Global step 430 Train loss 1.857792 on epoch=107
05/17/2022 16:36:58 - INFO - __main__ - Step 440 Global step 440 Train loss 2.642199 on epoch=109
05/17/2022 16:37:00 - INFO - __main__ - Step 450 Global step 450 Train loss 2.352322 on epoch=112
05/17/2022 16:37:01 - INFO - __main__ - Global step 450 Train loss 2.352552 Classification-F1 0.26366366366366367 on epoch=112
05/17/2022 16:37:03 - INFO - __main__ - Step 460 Global step 460 Train loss 2.723093 on epoch=114
05/17/2022 16:37:06 - INFO - __main__ - Step 470 Global step 470 Train loss 2.285610 on epoch=117
05/17/2022 16:37:08 - INFO - __main__ - Step 480 Global step 480 Train loss 2.412215 on epoch=119
05/17/2022 16:37:11 - INFO - __main__ - Step 490 Global step 490 Train loss 1.694419 on epoch=122
05/17/2022 16:37:14 - INFO - __main__ - Step 500 Global step 500 Train loss 2.073453 on epoch=124
05/17/2022 16:37:14 - INFO - __main__ - Global step 500 Train loss 2.237758 Classification-F1 0.30525978839544443 on epoch=124
05/17/2022 16:37:17 - INFO - __main__ - Step 510 Global step 510 Train loss 2.000691 on epoch=127
05/17/2022 16:37:19 - INFO - __main__ - Step 520 Global step 520 Train loss 1.658429 on epoch=129
05/17/2022 16:37:22 - INFO - __main__ - Step 530 Global step 530 Train loss 1.818279 on epoch=132
05/17/2022 16:37:24 - INFO - __main__ - Step 540 Global step 540 Train loss 1.877867 on epoch=134
05/17/2022 16:37:27 - INFO - __main__ - Step 550 Global step 550 Train loss 2.057478 on epoch=137
05/17/2022 16:37:27 - INFO - __main__ - Global step 550 Train loss 1.882549 Classification-F1 0.3272519534129441 on epoch=137
05/17/2022 16:37:30 - INFO - __main__ - Step 560 Global step 560 Train loss 1.532252 on epoch=139
05/17/2022 16:37:33 - INFO - __main__ - Step 570 Global step 570 Train loss 1.484308 on epoch=142
05/17/2022 16:37:35 - INFO - __main__ - Step 580 Global step 580 Train loss 1.942111 on epoch=144
05/17/2022 16:37:38 - INFO - __main__ - Step 590 Global step 590 Train loss 1.771497 on epoch=147
05/17/2022 16:37:40 - INFO - __main__ - Step 600 Global step 600 Train loss 1.532775 on epoch=149
05/17/2022 16:37:41 - INFO - __main__ - Global step 600 Train loss 1.652589 Classification-F1 0.39290322580645165 on epoch=149
05/17/2022 16:37:43 - INFO - __main__ - Step 610 Global step 610 Train loss 1.578401 on epoch=152
05/17/2022 16:37:46 - INFO - __main__ - Step 620 Global step 620 Train loss 1.671585 on epoch=154
05/17/2022 16:37:49 - INFO - __main__ - Step 630 Global step 630 Train loss 1.830785 on epoch=157
05/17/2022 16:37:51 - INFO - __main__ - Step 640 Global step 640 Train loss 1.666192 on epoch=159
05/17/2022 16:37:54 - INFO - __main__ - Step 650 Global step 650 Train loss 1.546575 on epoch=162
05/17/2022 16:37:54 - INFO - __main__ - Global step 650 Train loss 1.658707 Classification-F1 0.41314814814814815 on epoch=162
05/17/2022 16:37:57 - INFO - __main__ - Step 660 Global step 660 Train loss 1.579933 on epoch=164
05/17/2022 16:37:59 - INFO - __main__ - Step 670 Global step 670 Train loss 1.370762 on epoch=167
05/17/2022 16:38:02 - INFO - __main__ - Step 680 Global step 680 Train loss 1.790149 on epoch=169
05/17/2022 16:38:04 - INFO - __main__ - Step 690 Global step 690 Train loss 1.360026 on epoch=172
05/17/2022 16:38:07 - INFO - __main__ - Step 700 Global step 700 Train loss 1.110333 on epoch=174
05/17/2022 16:38:07 - INFO - __main__ - Global step 700 Train loss 1.442241 Classification-F1 0.4455498502353445 on epoch=174
05/17/2022 16:38:10 - INFO - __main__ - Step 710 Global step 710 Train loss 1.575413 on epoch=177
05/17/2022 16:38:13 - INFO - __main__ - Step 720 Global step 720 Train loss 1.419692 on epoch=179
05/17/2022 16:38:15 - INFO - __main__ - Step 730 Global step 730 Train loss 1.504499 on epoch=182
05/17/2022 16:38:18 - INFO - __main__ - Step 740 Global step 740 Train loss 1.563395 on epoch=184
05/17/2022 16:38:20 - INFO - __main__ - Step 750 Global step 750 Train loss 1.627927 on epoch=187
05/17/2022 16:38:21 - INFO - __main__ - Global step 750 Train loss 1.538185 Classification-F1 0.42261904761904767 on epoch=187
05/17/2022 16:38:23 - INFO - __main__ - Step 760 Global step 760 Train loss 1.188854 on epoch=189
05/17/2022 16:38:26 - INFO - __main__ - Step 770 Global step 770 Train loss 1.249391 on epoch=192
05/17/2022 16:38:28 - INFO - __main__ - Step 780 Global step 780 Train loss 1.399119 on epoch=194
05/17/2022 16:38:31 - INFO - __main__ - Step 790 Global step 790 Train loss 1.205890 on epoch=197
05/17/2022 16:38:33 - INFO - __main__ - Step 800 Global step 800 Train loss 1.008931 on epoch=199
05/17/2022 16:38:34 - INFO - __main__ - Global step 800 Train loss 1.210437 Classification-F1 0.45483641536273123 on epoch=199
05/17/2022 16:38:36 - INFO - __main__ - Step 810 Global step 810 Train loss 1.474193 on epoch=202
05/17/2022 16:38:39 - INFO - __main__ - Step 820 Global step 820 Train loss 1.234307 on epoch=204
05/17/2022 16:38:42 - INFO - __main__ - Step 830 Global step 830 Train loss 1.137627 on epoch=207
05/17/2022 16:38:44 - INFO - __main__ - Step 840 Global step 840 Train loss 1.530260 on epoch=209
05/17/2022 16:38:47 - INFO - __main__ - Step 850 Global step 850 Train loss 1.110270 on epoch=212
05/17/2022 16:38:47 - INFO - __main__ - Global step 850 Train loss 1.297331 Classification-F1 0.4335649461145774 on epoch=212
05/17/2022 16:38:50 - INFO - __main__ - Step 860 Global step 860 Train loss 1.196779 on epoch=214
05/17/2022 16:38:52 - INFO - __main__ - Step 870 Global step 870 Train loss 1.248519 on epoch=217
05/17/2022 16:38:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.975803 on epoch=219
05/17/2022 16:38:57 - INFO - __main__ - Step 890 Global step 890 Train loss 1.223896 on epoch=222
05/17/2022 16:39:00 - INFO - __main__ - Step 900 Global step 900 Train loss 1.016754 on epoch=224
05/17/2022 16:39:00 - INFO - __main__ - Global step 900 Train loss 1.132350 Classification-F1 0.5080781287677839 on epoch=224
05/17/2022 16:39:03 - INFO - __main__ - Step 910 Global step 910 Train loss 1.388710 on epoch=227
05/17/2022 16:39:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.880289 on epoch=229
05/17/2022 16:39:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.851288 on epoch=232
05/17/2022 16:39:10 - INFO - __main__ - Step 940 Global step 940 Train loss 1.260135 on epoch=234
05/17/2022 16:39:13 - INFO - __main__ - Step 950 Global step 950 Train loss 1.258577 on epoch=237
05/17/2022 16:39:13 - INFO - __main__ - Global step 950 Train loss 1.127800 Classification-F1 0.446519261736653 on epoch=237
05/17/2022 16:39:16 - INFO - __main__ - Step 960 Global step 960 Train loss 1.024476 on epoch=239
05/17/2022 16:39:18 - INFO - __main__ - Step 970 Global step 970 Train loss 1.112403 on epoch=242
05/17/2022 16:39:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.875463 on epoch=244
05/17/2022 16:39:23 - INFO - __main__ - Step 990 Global step 990 Train loss 1.273531 on epoch=247
05/17/2022 16:39:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.745035 on epoch=249
05/17/2022 16:39:26 - INFO - __main__ - Global step 1000 Train loss 1.006182 Classification-F1 0.4746753246753247 on epoch=249
05/17/2022 16:39:26 - INFO - __main__ - save last model!
05/17/2022 16:39:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:39:27 - INFO - __main__ - Printing 3 examples
05/17/2022 16:39:27 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/17/2022 16:39:27 - INFO - __main__ - ['happy']
05/17/2022 16:39:27 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/17/2022 16:39:27 - INFO - __main__ - ['happy']
05/17/2022 16:39:27 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/17/2022 16:39:27 - INFO - __main__ - ['happy']
05/17/2022 16:39:27 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:39:27 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:39:27 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:39:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:39:27 - INFO - __main__ - Printing 3 examples
05/17/2022 16:39:27 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/17/2022 16:39:27 - INFO - __main__ - ['happy']
05/17/2022 16:39:27 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/17/2022 16:39:27 - INFO - __main__ - ['happy']
05/17/2022 16:39:27 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/17/2022 16:39:27 - INFO - __main__ - ['happy']
05/17/2022 16:39:27 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:39:27 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:39:27 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:39:29 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 16:39:29 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 16:39:29 - INFO - __main__ - Printing 3 examples
05/17/2022 16:39:29 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 16:39:29 - INFO - __main__ - ['others']
05/17/2022 16:39:29 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 16:39:29 - INFO - __main__ - ['others']
05/17/2022 16:39:29 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 16:39:29 - INFO - __main__ - ['others']
05/17/2022 16:39:29 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:39:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:39:31 - INFO - __main__ - Starting training!
05/17/2022 16:39:31 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:39:37 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 16:40:05 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_21_0.0001_8_predictions.txt
05/17/2022 16:40:05 - INFO - __main__ - Classification-F1 on test data: 0.1487
05/17/2022 16:40:05 - INFO - __main__ - prefix=emo_16_21, lr=0.0001, bsz=8, dev_performance=0.5080781287677839, test_performance=0.14871698132510858
05/17/2022 16:40:05 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.0005, bsz=8 ...
05/17/2022 16:40:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:40:06 - INFO - __main__ - Printing 3 examples
05/17/2022 16:40:06 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/17/2022 16:40:06 - INFO - __main__ - ['happy']
05/17/2022 16:40:06 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/17/2022 16:40:06 - INFO - __main__ - ['happy']
05/17/2022 16:40:06 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/17/2022 16:40:06 - INFO - __main__ - ['happy']
05/17/2022 16:40:06 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:40:06 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:40:06 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:40:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:40:06 - INFO - __main__ - Printing 3 examples
05/17/2022 16:40:06 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/17/2022 16:40:06 - INFO - __main__ - ['happy']
05/17/2022 16:40:06 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/17/2022 16:40:06 - INFO - __main__ - ['happy']
05/17/2022 16:40:06 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/17/2022 16:40:06 - INFO - __main__ - ['happy']
05/17/2022 16:40:06 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:40:06 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:40:07 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:40:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:40:10 - INFO - __main__ - Starting training!
05/17/2022 16:40:12 - INFO - __main__ - Step 10 Global step 10 Train loss 20.264221 on epoch=2
05/17/2022 16:40:15 - INFO - __main__ - Step 20 Global step 20 Train loss 14.392705 on epoch=4
05/17/2022 16:40:17 - INFO - __main__ - Step 30 Global step 30 Train loss 8.331667 on epoch=7
05/17/2022 16:40:20 - INFO - __main__ - Step 40 Global step 40 Train loss 5.445749 on epoch=9
05/17/2022 16:40:22 - INFO - __main__ - Step 50 Global step 50 Train loss 4.892551 on epoch=12
05/17/2022 16:40:22 - INFO - __main__ - Global step 50 Train loss 10.665380 Classification-F1 0.18026315789473685 on epoch=12
05/17/2022 16:40:25 - INFO - __main__ - Step 60 Global step 60 Train loss 3.349233 on epoch=14
05/17/2022 16:40:28 - INFO - __main__ - Step 70 Global step 70 Train loss 2.992596 on epoch=17
05/17/2022 16:40:30 - INFO - __main__ - Step 80 Global step 80 Train loss 2.858829 on epoch=19
05/17/2022 16:40:33 - INFO - __main__ - Step 90 Global step 90 Train loss 1.935583 on epoch=22
05/17/2022 16:40:35 - INFO - __main__ - Step 100 Global step 100 Train loss 1.629264 on epoch=24
05/17/2022 16:40:36 - INFO - __main__ - Global step 100 Train loss 2.553101 Classification-F1 0.48352490421455935 on epoch=24
05/17/2022 16:40:39 - INFO - __main__ - Step 110 Global step 110 Train loss 1.535683 on epoch=27
05/17/2022 16:40:41 - INFO - __main__ - Step 120 Global step 120 Train loss 1.191207 on epoch=29
05/17/2022 16:40:44 - INFO - __main__ - Step 130 Global step 130 Train loss 1.173822 on epoch=32
05/17/2022 16:40:46 - INFO - __main__ - Step 140 Global step 140 Train loss 1.251526 on epoch=34
05/17/2022 16:40:49 - INFO - __main__ - Step 150 Global step 150 Train loss 1.025765 on epoch=37
05/17/2022 16:40:49 - INFO - __main__ - Global step 150 Train loss 1.235600 Classification-F1 0.4749128919860627 on epoch=37
05/17/2022 16:40:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.662288 on epoch=39
05/17/2022 16:40:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.667425 on epoch=42
05/17/2022 16:40:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.902063 on epoch=44
05/17/2022 16:40:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.558931 on epoch=47
05/17/2022 16:41:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.705760 on epoch=49
05/17/2022 16:41:02 - INFO - __main__ - Global step 200 Train loss 0.699294 Classification-F1 0.6722221045496907 on epoch=49
05/17/2022 16:41:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.492723 on epoch=52
05/17/2022 16:41:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.529641 on epoch=54
05/17/2022 16:41:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.823445 on epoch=57
05/17/2022 16:41:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.332671 on epoch=59
05/17/2022 16:41:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.513666 on epoch=62
05/17/2022 16:41:15 - INFO - __main__ - Global step 250 Train loss 0.538429 Classification-F1 0.5863095238095238 on epoch=62
05/17/2022 16:41:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.338648 on epoch=64
05/17/2022 16:41:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.388402 on epoch=67
05/17/2022 16:41:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.130006 on epoch=69
05/17/2022 16:41:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.234143 on epoch=72
05/17/2022 16:41:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.241856 on epoch=74
05/17/2022 16:41:28 - INFO - __main__ - Global step 300 Train loss 0.266611 Classification-F1 0.6952838827838828 on epoch=74
05/17/2022 16:41:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.247113 on epoch=77
05/17/2022 16:41:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.090027 on epoch=79
05/17/2022 16:41:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.243122 on epoch=82
05/17/2022 16:41:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.195717 on epoch=84
05/17/2022 16:41:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.126308 on epoch=87
05/17/2022 16:41:41 - INFO - __main__ - Global step 350 Train loss 0.180457 Classification-F1 0.6263250595257509 on epoch=87
05/17/2022 16:41:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.019596 on epoch=89
05/17/2022 16:41:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.251808 on epoch=92
05/17/2022 16:41:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.133756 on epoch=94
05/17/2022 16:41:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.090180 on epoch=97
05/17/2022 16:41:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.193352 on epoch=99
05/17/2022 16:41:54 - INFO - __main__ - Global step 400 Train loss 0.137739 Classification-F1 0.7484106569027315 on epoch=99
05/17/2022 16:41:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.079012 on epoch=102
05/17/2022 16:41:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.060731 on epoch=104
05/17/2022 16:42:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.110782 on epoch=107
05/17/2022 16:42:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.100743 on epoch=109
05/17/2022 16:42:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.152835 on epoch=112
05/17/2022 16:42:07 - INFO - __main__ - Global step 450 Train loss 0.100821 Classification-F1 0.6512820512820513 on epoch=112
05/17/2022 16:42:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.357737 on epoch=114
05/17/2022 16:42:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.723699 on epoch=117
05/17/2022 16:42:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.652755 on epoch=119
05/17/2022 16:42:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.612207 on epoch=122
05/17/2022 16:42:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.805656 on epoch=124
05/17/2022 16:42:20 - INFO - __main__ - Global step 500 Train loss 0.630411 Classification-F1 0.5643551850448403 on epoch=124
05/17/2022 16:42:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.839459 on epoch=127
05/17/2022 16:42:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.977317 on epoch=129
05/17/2022 16:42:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.873265 on epoch=132
05/17/2022 16:42:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.980525 on epoch=134
05/17/2022 16:42:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.737887 on epoch=137
05/17/2022 16:42:33 - INFO - __main__ - Global step 550 Train loss 0.881690 Classification-F1 0.58495979036776 on epoch=137
05/17/2022 16:42:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.525554 on epoch=139
05/17/2022 16:42:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.743736 on epoch=142
05/17/2022 16:42:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.509744 on epoch=144
05/17/2022 16:42:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.255283 on epoch=147
05/17/2022 16:42:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.387336 on epoch=149
05/17/2022 16:42:46 - INFO - __main__ - Global step 600 Train loss 0.484331 Classification-F1 0.6409252100936157 on epoch=149
05/17/2022 16:42:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.236844 on epoch=152
05/17/2022 16:42:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.323434 on epoch=154
05/17/2022 16:42:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.388870 on epoch=157
05/17/2022 16:42:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.499382 on epoch=159
05/17/2022 16:42:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.336826 on epoch=162
05/17/2022 16:42:59 - INFO - __main__ - Global step 650 Train loss 0.357071 Classification-F1 0.5982448923625394 on epoch=162
05/17/2022 16:43:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.370708 on epoch=164
05/17/2022 16:43:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.286771 on epoch=167
05/17/2022 16:43:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.323509 on epoch=169
05/17/2022 16:43:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.202150 on epoch=172
05/17/2022 16:43:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.227996 on epoch=174
05/17/2022 16:43:11 - INFO - __main__ - Global step 700 Train loss 0.282227 Classification-F1 0.7311688311688312 on epoch=174
05/17/2022 16:43:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.238122 on epoch=177
05/17/2022 16:43:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.279879 on epoch=179
05/17/2022 16:43:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.333896 on epoch=182
05/17/2022 16:43:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.312711 on epoch=184
05/17/2022 16:43:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.158537 on epoch=187
05/17/2022 16:43:24 - INFO - __main__ - Global step 750 Train loss 0.264629 Classification-F1 0.668595825426945 on epoch=187
05/17/2022 16:43:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.254015 on epoch=189
05/17/2022 16:43:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.091273 on epoch=192
05/17/2022 16:43:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.083430 on epoch=194
05/17/2022 16:43:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.150847 on epoch=197
05/17/2022 16:43:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.125498 on epoch=199
05/17/2022 16:43:37 - INFO - __main__ - Global step 800 Train loss 0.141013 Classification-F1 0.7018558378958824 on epoch=199
05/17/2022 16:43:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.087563 on epoch=202
05/17/2022 16:43:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.047447 on epoch=204
05/17/2022 16:43:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.125231 on epoch=207
05/17/2022 16:43:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.138863 on epoch=209
05/17/2022 16:43:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.108718 on epoch=212
05/17/2022 16:43:50 - INFO - __main__ - Global step 850 Train loss 0.101564 Classification-F1 0.6759816333092195 on epoch=212
05/17/2022 16:43:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.024934 on epoch=214
05/17/2022 16:43:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.030803 on epoch=217
05/17/2022 16:43:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.271214 on epoch=219
05/17/2022 16:44:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.064350 on epoch=222
05/17/2022 16:44:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.045808 on epoch=224
05/17/2022 16:44:03 - INFO - __main__ - Global step 900 Train loss 0.087422 Classification-F1 0.5325924075924076 on epoch=224
05/17/2022 16:44:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.058006 on epoch=227
05/17/2022 16:44:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.077948 on epoch=229
05/17/2022 16:44:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.105898 on epoch=232
05/17/2022 16:44:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.022756 on epoch=234
05/17/2022 16:44:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.129310 on epoch=237
05/17/2022 16:44:15 - INFO - __main__ - Global step 950 Train loss 0.078784 Classification-F1 0.6440590659340659 on epoch=237
05/17/2022 16:44:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.130607 on epoch=239
05/17/2022 16:44:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.089322 on epoch=242
05/17/2022 16:44:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.009588 on epoch=244
05/17/2022 16:44:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.114843 on epoch=247
05/17/2022 16:44:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.099221 on epoch=249
05/17/2022 16:44:28 - INFO - __main__ - Global step 1000 Train loss 0.088716 Classification-F1 0.695234242109242 on epoch=249
05/17/2022 16:44:28 - INFO - __main__ - save last model!
05/17/2022 16:44:29 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:44:29 - INFO - __main__ - Printing 3 examples
05/17/2022 16:44:29 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/17/2022 16:44:29 - INFO - __main__ - ['happy']
05/17/2022 16:44:29 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/17/2022 16:44:29 - INFO - __main__ - ['happy']
05/17/2022 16:44:29 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/17/2022 16:44:29 - INFO - __main__ - ['happy']
05/17/2022 16:44:29 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:44:29 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:44:29 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:44:29 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:44:29 - INFO - __main__ - Printing 3 examples
05/17/2022 16:44:29 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/17/2022 16:44:29 - INFO - __main__ - ['happy']
05/17/2022 16:44:29 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/17/2022 16:44:29 - INFO - __main__ - ['happy']
05/17/2022 16:44:29 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/17/2022 16:44:29 - INFO - __main__ - ['happy']
05/17/2022 16:44:29 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:44:29 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:44:29 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:44:31 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 16:44:31 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 16:44:31 - INFO - __main__ - Printing 3 examples
05/17/2022 16:44:31 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 16:44:31 - INFO - __main__ - ['others']
05/17/2022 16:44:31 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 16:44:31 - INFO - __main__ - ['others']
05/17/2022 16:44:31 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 16:44:31 - INFO - __main__ - ['others']
05/17/2022 16:44:31 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:44:33 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:44:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:44:33 - INFO - __main__ - Starting training!
05/17/2022 16:44:38 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 16:45:12 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_42_0.0005_8_predictions.txt
05/17/2022 16:45:12 - INFO - __main__ - Classification-F1 on test data: 0.3336
05/17/2022 16:45:12 - INFO - __main__ - prefix=emo_16_42, lr=0.0005, bsz=8, dev_performance=0.7484106569027315, test_performance=0.3335596887945225
05/17/2022 16:45:12 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.0003, bsz=8 ...
05/17/2022 16:45:13 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:45:13 - INFO - __main__ - Printing 3 examples
05/17/2022 16:45:13 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/17/2022 16:45:13 - INFO - __main__ - ['happy']
05/17/2022 16:45:13 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/17/2022 16:45:13 - INFO - __main__ - ['happy']
05/17/2022 16:45:13 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/17/2022 16:45:13 - INFO - __main__ - ['happy']
05/17/2022 16:45:13 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:45:13 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:45:13 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:45:13 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:45:13 - INFO - __main__ - Printing 3 examples
05/17/2022 16:45:13 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/17/2022 16:45:13 - INFO - __main__ - ['happy']
05/17/2022 16:45:13 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/17/2022 16:45:13 - INFO - __main__ - ['happy']
05/17/2022 16:45:13 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/17/2022 16:45:13 - INFO - __main__ - ['happy']
05/17/2022 16:45:13 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:45:13 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:45:13 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:45:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:45:17 - INFO - __main__ - Starting training!
05/17/2022 16:45:20 - INFO - __main__ - Step 10 Global step 10 Train loss 19.899857 on epoch=2
05/17/2022 16:45:23 - INFO - __main__ - Step 20 Global step 20 Train loss 16.787939 on epoch=4
05/17/2022 16:45:25 - INFO - __main__ - Step 30 Global step 30 Train loss 10.295536 on epoch=7
05/17/2022 16:45:28 - INFO - __main__ - Step 40 Global step 40 Train loss 6.969008 on epoch=9
05/17/2022 16:45:30 - INFO - __main__ - Step 50 Global step 50 Train loss 6.084836 on epoch=12
05/17/2022 16:45:31 - INFO - __main__ - Global step 50 Train loss 12.007435 Classification-F1 0.1 on epoch=12
05/17/2022 16:45:34 - INFO - __main__ - Step 60 Global step 60 Train loss 4.319016 on epoch=14
05/17/2022 16:45:36 - INFO - __main__ - Step 70 Global step 70 Train loss 4.798640 on epoch=17
05/17/2022 16:45:39 - INFO - __main__ - Step 80 Global step 80 Train loss 3.287187 on epoch=19
05/17/2022 16:45:41 - INFO - __main__ - Step 90 Global step 90 Train loss 3.081166 on epoch=22
05/17/2022 16:45:44 - INFO - __main__ - Step 100 Global step 100 Train loss 2.683737 on epoch=24
05/17/2022 16:45:44 - INFO - __main__ - Global step 100 Train loss 3.633950 Classification-F1 0.17243867243867242 on epoch=24
05/17/2022 16:45:47 - INFO - __main__ - Step 110 Global step 110 Train loss 1.976983 on epoch=27
05/17/2022 16:45:49 - INFO - __main__ - Step 120 Global step 120 Train loss 2.324547 on epoch=29
05/17/2022 16:45:52 - INFO - __main__ - Step 130 Global step 130 Train loss 2.060115 on epoch=32
05/17/2022 16:45:54 - INFO - __main__ - Step 140 Global step 140 Train loss 2.260827 on epoch=34
05/17/2022 16:45:57 - INFO - __main__ - Step 150 Global step 150 Train loss 1.639866 on epoch=37
05/17/2022 16:45:57 - INFO - __main__ - Global step 150 Train loss 2.052468 Classification-F1 0.42929670152206384 on epoch=37
05/17/2022 16:46:00 - INFO - __main__ - Step 160 Global step 160 Train loss 1.537729 on epoch=39
05/17/2022 16:46:03 - INFO - __main__ - Step 170 Global step 170 Train loss 1.243011 on epoch=42
05/17/2022 16:46:05 - INFO - __main__ - Step 180 Global step 180 Train loss 1.341743 on epoch=44
05/17/2022 16:46:08 - INFO - __main__ - Step 190 Global step 190 Train loss 1.385704 on epoch=47
05/17/2022 16:46:10 - INFO - __main__ - Step 200 Global step 200 Train loss 1.368000 on epoch=49
05/17/2022 16:46:11 - INFO - __main__ - Global step 200 Train loss 1.375237 Classification-F1 0.4703647416413374 on epoch=49
05/17/2022 16:46:13 - INFO - __main__ - Step 210 Global step 210 Train loss 1.174511 on epoch=52
05/17/2022 16:46:16 - INFO - __main__ - Step 220 Global step 220 Train loss 1.020285 on epoch=54
05/17/2022 16:46:18 - INFO - __main__ - Step 230 Global step 230 Train loss 1.013651 on epoch=57
05/17/2022 16:46:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.627067 on epoch=59
05/17/2022 16:46:23 - INFO - __main__ - Step 250 Global step 250 Train loss 1.110270 on epoch=62
05/17/2022 16:46:24 - INFO - __main__ - Global step 250 Train loss 0.989157 Classification-F1 0.4826203208556149 on epoch=62
05/17/2022 16:46:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.771990 on epoch=64
05/17/2022 16:46:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.771564 on epoch=67
05/17/2022 16:46:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.890303 on epoch=69
05/17/2022 16:46:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.729738 on epoch=72
05/17/2022 16:46:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.550329 on epoch=74
05/17/2022 16:46:37 - INFO - __main__ - Global step 300 Train loss 0.742785 Classification-F1 0.7258498254957736 on epoch=74
05/17/2022 16:46:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.613447 on epoch=77
05/17/2022 16:46:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.614617 on epoch=79
05/17/2022 16:46:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.488811 on epoch=82
05/17/2022 16:46:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.653293 on epoch=84
05/17/2022 16:46:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.846059 on epoch=87
05/17/2022 16:46:50 - INFO - __main__ - Global step 350 Train loss 0.643245 Classification-F1 0.6186909581646423 on epoch=87
05/17/2022 16:46:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.636116 on epoch=89
05/17/2022 16:46:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.625154 on epoch=92
05/17/2022 16:46:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.714906 on epoch=94
05/17/2022 16:47:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.711667 on epoch=97
05/17/2022 16:47:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.557728 on epoch=99
05/17/2022 16:47:03 - INFO - __main__ - Global step 400 Train loss 0.649114 Classification-F1 0.7203030303030303 on epoch=99
05/17/2022 16:47:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.619970 on epoch=102
05/17/2022 16:47:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.718934 on epoch=104
05/17/2022 16:47:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.744289 on epoch=107
05/17/2022 16:47:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.760734 on epoch=109
05/17/2022 16:47:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.649663 on epoch=112
05/17/2022 16:47:16 - INFO - __main__ - Global step 450 Train loss 0.698718 Classification-F1 0.7429292929292928 on epoch=112
05/17/2022 16:47:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.570399 on epoch=114
05/17/2022 16:47:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.667233 on epoch=117
05/17/2022 16:47:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.749744 on epoch=119
05/17/2022 16:47:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.559114 on epoch=122
05/17/2022 16:47:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.373697 on epoch=124
05/17/2022 16:47:29 - INFO - __main__ - Global step 500 Train loss 0.584037 Classification-F1 0.7434906597774245 on epoch=124
05/17/2022 16:47:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.438400 on epoch=127
05/17/2022 16:47:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.613051 on epoch=129
05/17/2022 16:47:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.497539 on epoch=132
05/17/2022 16:47:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.739623 on epoch=134
05/17/2022 16:47:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.380974 on epoch=137
05/17/2022 16:47:42 - INFO - __main__ - Global step 550 Train loss 0.533917 Classification-F1 0.7384219257620792 on epoch=137
05/17/2022 16:47:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.471132 on epoch=139
05/17/2022 16:47:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.573581 on epoch=142
05/17/2022 16:47:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.358701 on epoch=144
05/17/2022 16:47:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.295321 on epoch=147
05/17/2022 16:47:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.551124 on epoch=149
05/17/2022 16:47:55 - INFO - __main__ - Global step 600 Train loss 0.449972 Classification-F1 0.7227240896358543 on epoch=149
05/17/2022 16:47:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.451384 on epoch=152
05/17/2022 16:48:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.422384 on epoch=154
05/17/2022 16:48:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.281286 on epoch=157
05/17/2022 16:48:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.342226 on epoch=159
05/17/2022 16:48:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.377341 on epoch=162
05/17/2022 16:48:08 - INFO - __main__ - Global step 650 Train loss 0.374924 Classification-F1 0.7096774193548387 on epoch=162
05/17/2022 16:48:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.409169 on epoch=164
05/17/2022 16:48:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.306992 on epoch=167
05/17/2022 16:48:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.423739 on epoch=169
05/17/2022 16:48:19 - INFO - __main__ - Step 690 Global step 690 Train loss 0.297422 on epoch=172
05/17/2022 16:48:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.180333 on epoch=174
05/17/2022 16:48:21 - INFO - __main__ - Global step 700 Train loss 0.323531 Classification-F1 0.6909893949459904 on epoch=174
05/17/2022 16:48:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.259609 on epoch=177
05/17/2022 16:48:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.168213 on epoch=179
05/17/2022 16:48:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.190686 on epoch=182
05/17/2022 16:48:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.354582 on epoch=184
05/17/2022 16:48:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.264005 on epoch=187
05/17/2022 16:48:34 - INFO - __main__ - Global step 750 Train loss 0.247419 Classification-F1 0.7582800982800982 on epoch=187
05/17/2022 16:48:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.378650 on epoch=189
05/17/2022 16:48:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.282116 on epoch=192
05/17/2022 16:48:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.397241 on epoch=194
05/17/2022 16:48:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.210786 on epoch=197
05/17/2022 16:48:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.160122 on epoch=199
05/17/2022 16:48:48 - INFO - __main__ - Global step 800 Train loss 0.285783 Classification-F1 0.7471169557566617 on epoch=199
05/17/2022 16:48:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.274325 on epoch=202
05/17/2022 16:48:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.493573 on epoch=204
05/17/2022 16:48:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.113353 on epoch=207
05/17/2022 16:48:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.324281 on epoch=209
05/17/2022 16:49:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.226409 on epoch=212
05/17/2022 16:49:01 - INFO - __main__ - Global step 850 Train loss 0.286388 Classification-F1 0.7372474747474748 on epoch=212
05/17/2022 16:49:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.072166 on epoch=214
05/17/2022 16:49:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.187006 on epoch=217
05/17/2022 16:49:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.107639 on epoch=219
05/17/2022 16:49:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.171394 on epoch=222
05/17/2022 16:49:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.068786 on epoch=224
05/17/2022 16:49:14 - INFO - __main__ - Global step 900 Train loss 0.121398 Classification-F1 0.7223039215686274 on epoch=224
05/17/2022 16:49:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.088748 on epoch=227
05/17/2022 16:49:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.306961 on epoch=229
05/17/2022 16:49:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.126014 on epoch=232
05/17/2022 16:49:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.109137 on epoch=234
05/17/2022 16:49:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.174963 on epoch=237
05/17/2022 16:49:27 - INFO - __main__ - Global step 950 Train loss 0.161165 Classification-F1 0.7618228381096028 on epoch=237
05/17/2022 16:49:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.150055 on epoch=239
05/17/2022 16:49:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.213326 on epoch=242
05/17/2022 16:49:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.228093 on epoch=244
05/17/2022 16:49:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.300556 on epoch=247
05/17/2022 16:49:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.312964 on epoch=249
05/17/2022 16:49:40 - INFO - __main__ - Global step 1000 Train loss 0.240999 Classification-F1 0.7476304945054946 on epoch=249
05/17/2022 16:49:40 - INFO - __main__ - save last model!
05/17/2022 16:49:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:49:41 - INFO - __main__ - Printing 3 examples
05/17/2022 16:49:41 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/17/2022 16:49:41 - INFO - __main__ - ['happy']
05/17/2022 16:49:41 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/17/2022 16:49:41 - INFO - __main__ - ['happy']
05/17/2022 16:49:41 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/17/2022 16:49:41 - INFO - __main__ - ['happy']
05/17/2022 16:49:41 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:49:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:49:41 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:49:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:49:41 - INFO - __main__ - Printing 3 examples
05/17/2022 16:49:41 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/17/2022 16:49:41 - INFO - __main__ - ['happy']
05/17/2022 16:49:41 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/17/2022 16:49:41 - INFO - __main__ - ['happy']
05/17/2022 16:49:41 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/17/2022 16:49:41 - INFO - __main__ - ['happy']
05/17/2022 16:49:41 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:49:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:49:41 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:49:43 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 16:49:43 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 16:49:43 - INFO - __main__ - Printing 3 examples
05/17/2022 16:49:43 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 16:49:43 - INFO - __main__ - ['others']
05/17/2022 16:49:43 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 16:49:43 - INFO - __main__ - ['others']
05/17/2022 16:49:43 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 16:49:43 - INFO - __main__ - ['others']
05/17/2022 16:49:43 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:49:45 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:49:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:49:45 - INFO - __main__ - Starting training!
05/17/2022 16:49:50 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 16:50:23 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_42_0.0003_8_predictions.txt
05/17/2022 16:50:23 - INFO - __main__ - Classification-F1 on test data: 0.4081
05/17/2022 16:50:23 - INFO - __main__ - prefix=emo_16_42, lr=0.0003, bsz=8, dev_performance=0.7618228381096028, test_performance=0.4081037935712178
05/17/2022 16:50:23 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.0002, bsz=8 ...
05/17/2022 16:50:24 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:50:24 - INFO - __main__ - Printing 3 examples
05/17/2022 16:50:24 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/17/2022 16:50:24 - INFO - __main__ - ['happy']
05/17/2022 16:50:24 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/17/2022 16:50:24 - INFO - __main__ - ['happy']
05/17/2022 16:50:24 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/17/2022 16:50:24 - INFO - __main__ - ['happy']
05/17/2022 16:50:24 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:50:24 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:50:24 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:50:24 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:50:24 - INFO - __main__ - Printing 3 examples
05/17/2022 16:50:24 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/17/2022 16:50:24 - INFO - __main__ - ['happy']
05/17/2022 16:50:24 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/17/2022 16:50:24 - INFO - __main__ - ['happy']
05/17/2022 16:50:24 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/17/2022 16:50:24 - INFO - __main__ - ['happy']
05/17/2022 16:50:24 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:50:24 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:50:24 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:50:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:50:28 - INFO - __main__ - Starting training!
05/17/2022 16:50:30 - INFO - __main__ - Step 10 Global step 10 Train loss 20.680241 on epoch=2
05/17/2022 16:50:32 - INFO - __main__ - Step 20 Global step 20 Train loss 18.689068 on epoch=4
05/17/2022 16:50:35 - INFO - __main__ - Step 30 Global step 30 Train loss 13.497808 on epoch=7
05/17/2022 16:50:37 - INFO - __main__ - Step 40 Global step 40 Train loss 11.195722 on epoch=9
05/17/2022 16:50:40 - INFO - __main__ - Step 50 Global step 50 Train loss 9.482862 on epoch=12
05/17/2022 16:50:48 - INFO - __main__ - Global step 50 Train loss 14.709141 Classification-F1 0.0 on epoch=12
05/17/2022 16:50:51 - INFO - __main__ - Step 60 Global step 60 Train loss 8.708636 on epoch=14
05/17/2022 16:50:54 - INFO - __main__ - Step 70 Global step 70 Train loss 7.608450 on epoch=17
05/17/2022 16:50:56 - INFO - __main__ - Step 80 Global step 80 Train loss 5.965449 on epoch=19
05/17/2022 16:50:59 - INFO - __main__ - Step 90 Global step 90 Train loss 4.535589 on epoch=22
05/17/2022 16:51:01 - INFO - __main__ - Step 100 Global step 100 Train loss 5.233850 on epoch=24
05/17/2022 16:51:02 - INFO - __main__ - Global step 100 Train loss 6.410395 Classification-F1 0.1 on epoch=24
05/17/2022 16:51:05 - INFO - __main__ - Step 110 Global step 110 Train loss 4.530776 on epoch=27
05/17/2022 16:51:07 - INFO - __main__ - Step 120 Global step 120 Train loss 4.028597 on epoch=29
05/17/2022 16:51:10 - INFO - __main__ - Step 130 Global step 130 Train loss 3.745440 on epoch=32
05/17/2022 16:51:12 - INFO - __main__ - Step 140 Global step 140 Train loss 2.953146 on epoch=34
05/17/2022 16:51:15 - INFO - __main__ - Step 150 Global step 150 Train loss 2.991273 on epoch=37
05/17/2022 16:51:15 - INFO - __main__ - Global step 150 Train loss 3.649846 Classification-F1 0.13067758749069247 on epoch=37
05/17/2022 16:51:18 - INFO - __main__ - Step 160 Global step 160 Train loss 3.344471 on epoch=39
05/17/2022 16:51:21 - INFO - __main__ - Step 170 Global step 170 Train loss 3.414401 on epoch=42
05/17/2022 16:51:23 - INFO - __main__ - Step 180 Global step 180 Train loss 3.246522 on epoch=44
05/17/2022 16:51:26 - INFO - __main__ - Step 190 Global step 190 Train loss 2.689209 on epoch=47
05/17/2022 16:51:28 - INFO - __main__ - Step 200 Global step 200 Train loss 2.765782 on epoch=49
05/17/2022 16:51:29 - INFO - __main__ - Global step 200 Train loss 3.092077 Classification-F1 0.22024332120908063 on epoch=49
05/17/2022 16:51:32 - INFO - __main__ - Step 210 Global step 210 Train loss 1.993679 on epoch=52
05/17/2022 16:51:34 - INFO - __main__ - Step 220 Global step 220 Train loss 2.362598 on epoch=54
05/17/2022 16:51:37 - INFO - __main__ - Step 230 Global step 230 Train loss 1.853674 on epoch=57
05/17/2022 16:51:39 - INFO - __main__ - Step 240 Global step 240 Train loss 1.943171 on epoch=59
05/17/2022 16:51:42 - INFO - __main__ - Step 250 Global step 250 Train loss 1.737037 on epoch=62
05/17/2022 16:51:42 - INFO - __main__ - Global step 250 Train loss 1.978032 Classification-F1 0.3372241086587436 on epoch=62
05/17/2022 16:51:45 - INFO - __main__ - Step 260 Global step 260 Train loss 2.038245 on epoch=64
05/17/2022 16:51:48 - INFO - __main__ - Step 270 Global step 270 Train loss 1.557081 on epoch=67
05/17/2022 16:51:50 - INFO - __main__ - Step 280 Global step 280 Train loss 1.561959 on epoch=69
05/17/2022 16:51:53 - INFO - __main__ - Step 290 Global step 290 Train loss 1.662646 on epoch=72
05/17/2022 16:51:55 - INFO - __main__ - Step 300 Global step 300 Train loss 1.403406 on epoch=74
05/17/2022 16:51:55 - INFO - __main__ - Global step 300 Train loss 1.644667 Classification-F1 0.33928571428571425 on epoch=74
05/17/2022 16:51:58 - INFO - __main__ - Step 310 Global step 310 Train loss 1.483329 on epoch=77
05/17/2022 16:52:01 - INFO - __main__ - Step 320 Global step 320 Train loss 2.130394 on epoch=79
05/17/2022 16:52:03 - INFO - __main__ - Step 330 Global step 330 Train loss 1.238886 on epoch=82
05/17/2022 16:52:06 - INFO - __main__ - Step 340 Global step 340 Train loss 1.072766 on epoch=84
05/17/2022 16:52:08 - INFO - __main__ - Step 350 Global step 350 Train loss 1.270607 on epoch=87
05/17/2022 16:52:09 - INFO - __main__ - Global step 350 Train loss 1.439196 Classification-F1 0.43606143578334905 on epoch=87
05/17/2022 16:52:11 - INFO - __main__ - Step 360 Global step 360 Train loss 1.398726 on epoch=89
05/17/2022 16:52:14 - INFO - __main__ - Step 370 Global step 370 Train loss 1.619930 on epoch=92
05/17/2022 16:52:16 - INFO - __main__ - Step 380 Global step 380 Train loss 1.813373 on epoch=94
05/17/2022 16:52:19 - INFO - __main__ - Step 390 Global step 390 Train loss 1.564719 on epoch=97
05/17/2022 16:52:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.993951 on epoch=99
05/17/2022 16:52:22 - INFO - __main__ - Global step 400 Train loss 1.478140 Classification-F1 0.45798097251585623 on epoch=99
05/17/2022 16:52:25 - INFO - __main__ - Step 410 Global step 410 Train loss 1.265782 on epoch=102
05/17/2022 16:52:27 - INFO - __main__ - Step 420 Global step 420 Train loss 1.198493 on epoch=104
05/17/2022 16:52:30 - INFO - __main__ - Step 430 Global step 430 Train loss 1.170331 on epoch=107
05/17/2022 16:52:32 - INFO - __main__ - Step 440 Global step 440 Train loss 1.364983 on epoch=109
05/17/2022 16:52:35 - INFO - __main__ - Step 450 Global step 450 Train loss 1.496066 on epoch=112
05/17/2022 16:52:35 - INFO - __main__ - Global step 450 Train loss 1.299131 Classification-F1 0.4177905308464849 on epoch=112
05/17/2022 16:52:37 - INFO - __main__ - Step 460 Global step 460 Train loss 1.023139 on epoch=114
05/17/2022 16:52:40 - INFO - __main__ - Step 470 Global step 470 Train loss 1.099441 on epoch=117
05/17/2022 16:52:42 - INFO - __main__ - Step 480 Global step 480 Train loss 1.055507 on epoch=119
05/17/2022 16:52:45 - INFO - __main__ - Step 490 Global step 490 Train loss 1.099863 on epoch=122
05/17/2022 16:52:47 - INFO - __main__ - Step 500 Global step 500 Train loss 1.208381 on epoch=124
05/17/2022 16:52:48 - INFO - __main__ - Global step 500 Train loss 1.097266 Classification-F1 0.41477272727272724 on epoch=124
05/17/2022 16:52:50 - INFO - __main__ - Step 510 Global step 510 Train loss 1.071180 on epoch=127
05/17/2022 16:52:53 - INFO - __main__ - Step 520 Global step 520 Train loss 1.199249 on epoch=129
05/17/2022 16:52:55 - INFO - __main__ - Step 530 Global step 530 Train loss 1.143944 on epoch=132
05/17/2022 16:52:58 - INFO - __main__ - Step 540 Global step 540 Train loss 1.065669 on epoch=134
05/17/2022 16:53:00 - INFO - __main__ - Step 550 Global step 550 Train loss 1.068905 on epoch=137
05/17/2022 16:53:01 - INFO - __main__ - Global step 550 Train loss 1.109789 Classification-F1 0.4462962962962963 on epoch=137
05/17/2022 16:53:03 - INFO - __main__ - Step 560 Global step 560 Train loss 1.140610 on epoch=139
05/17/2022 16:53:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.892184 on epoch=142
05/17/2022 16:53:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.957243 on epoch=144
05/17/2022 16:53:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.976828 on epoch=147
05/17/2022 16:53:13 - INFO - __main__ - Step 600 Global step 600 Train loss 1.000285 on epoch=149
05/17/2022 16:53:14 - INFO - __main__ - Global step 600 Train loss 0.993430 Classification-F1 0.45783730158730157 on epoch=149
05/17/2022 16:53:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.982860 on epoch=152
05/17/2022 16:53:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.937752 on epoch=154
05/17/2022 16:53:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.884956 on epoch=157
05/17/2022 16:53:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.860174 on epoch=159
05/17/2022 16:53:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.848169 on epoch=162
05/17/2022 16:53:26 - INFO - __main__ - Global step 650 Train loss 0.902782 Classification-F1 0.525417332702009 on epoch=162
05/17/2022 16:53:29 - INFO - __main__ - Step 660 Global step 660 Train loss 1.068629 on epoch=164
05/17/2022 16:53:32 - INFO - __main__ - Step 670 Global step 670 Train loss 1.196029 on epoch=167
05/17/2022 16:53:34 - INFO - __main__ - Step 680 Global step 680 Train loss 1.225886 on epoch=169
05/17/2022 16:53:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.978881 on epoch=172
05/17/2022 16:53:39 - INFO - __main__ - Step 700 Global step 700 Train loss 1.042733 on epoch=174
05/17/2022 16:53:40 - INFO - __main__ - Global step 700 Train loss 1.102432 Classification-F1 0.5983973236861555 on epoch=174
05/17/2022 16:53:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.924284 on epoch=177
05/17/2022 16:53:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.954143 on epoch=179
05/17/2022 16:53:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.837274 on epoch=182
05/17/2022 16:53:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.857286 on epoch=184
05/17/2022 16:53:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.979522 on epoch=187
05/17/2022 16:53:53 - INFO - __main__ - Global step 750 Train loss 0.910502 Classification-F1 0.5358884671136515 on epoch=187
05/17/2022 16:53:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.954017 on epoch=189
05/17/2022 16:53:58 - INFO - __main__ - Step 770 Global step 770 Train loss 1.006067 on epoch=192
05/17/2022 16:54:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.867332 on epoch=194
05/17/2022 16:54:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.752617 on epoch=197
05/17/2022 16:54:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.924726 on epoch=199
05/17/2022 16:54:05 - INFO - __main__ - Global step 800 Train loss 0.900952 Classification-F1 0.5095254010695187 on epoch=199
05/17/2022 16:54:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.880057 on epoch=202
05/17/2022 16:54:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.880612 on epoch=204
05/17/2022 16:54:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.859031 on epoch=207
05/17/2022 16:54:15 - INFO - __main__ - Step 840 Global step 840 Train loss 1.016181 on epoch=209
05/17/2022 16:54:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.840222 on epoch=212
05/17/2022 16:54:18 - INFO - __main__ - Global step 850 Train loss 0.895221 Classification-F1 0.5986207381556219 on epoch=212
05/17/2022 16:54:21 - INFO - __main__ - Step 860 Global step 860 Train loss 1.050134 on epoch=214
05/17/2022 16:54:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.623213 on epoch=217
05/17/2022 16:54:26 - INFO - __main__ - Step 880 Global step 880 Train loss 1.184518 on epoch=219
05/17/2022 16:54:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.801039 on epoch=222
05/17/2022 16:54:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.679953 on epoch=224
05/17/2022 16:54:32 - INFO - __main__ - Global step 900 Train loss 0.867772 Classification-F1 0.7342792248864355 on epoch=224
05/17/2022 16:54:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.854639 on epoch=227
05/17/2022 16:54:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.890742 on epoch=229
05/17/2022 16:54:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.808521 on epoch=232
05/17/2022 16:54:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.833986 on epoch=234
05/17/2022 16:54:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.780688 on epoch=237
05/17/2022 16:54:46 - INFO - __main__ - Global step 950 Train loss 0.833715 Classification-F1 0.7455598455598456 on epoch=237
05/17/2022 16:54:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.840491 on epoch=239
05/17/2022 16:54:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.820023 on epoch=242
05/17/2022 16:54:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.756180 on epoch=244
05/17/2022 16:54:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.976732 on epoch=247
05/17/2022 16:54:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.863522 on epoch=249
05/17/2022 16:54:59 - INFO - __main__ - Global step 1000 Train loss 0.851390 Classification-F1 0.7181372549019608 on epoch=249
05/17/2022 16:54:59 - INFO - __main__ - save last model!
05/17/2022 16:55:00 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:55:00 - INFO - __main__ - Printing 3 examples
05/17/2022 16:55:00 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/17/2022 16:55:00 - INFO - __main__ - ['happy']
05/17/2022 16:55:00 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/17/2022 16:55:00 - INFO - __main__ - ['happy']
05/17/2022 16:55:00 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/17/2022 16:55:00 - INFO - __main__ - ['happy']
05/17/2022 16:55:00 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:55:00 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:55:00 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:55:00 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:55:00 - INFO - __main__ - Printing 3 examples
05/17/2022 16:55:00 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/17/2022 16:55:00 - INFO - __main__ - ['happy']
05/17/2022 16:55:00 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/17/2022 16:55:00 - INFO - __main__ - ['happy']
05/17/2022 16:55:00 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/17/2022 16:55:00 - INFO - __main__ - ['happy']
05/17/2022 16:55:00 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:55:00 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:55:00 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:55:02 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 16:55:02 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 16:55:02 - INFO - __main__ - Printing 3 examples
05/17/2022 16:55:02 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 16:55:02 - INFO - __main__ - ['others']
05/17/2022 16:55:02 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 16:55:02 - INFO - __main__ - ['others']
05/17/2022 16:55:02 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 16:55:02 - INFO - __main__ - ['others']
05/17/2022 16:55:02 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:55:04 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:55:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:55:04 - INFO - __main__ - Starting training!
05/17/2022 16:55:09 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 16:55:38 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_42_0.0002_8_predictions.txt
05/17/2022 16:55:38 - INFO - __main__ - Classification-F1 on test data: 0.4322
05/17/2022 16:55:38 - INFO - __main__ - prefix=emo_16_42, lr=0.0002, bsz=8, dev_performance=0.7455598455598456, test_performance=0.4322034975712033
05/17/2022 16:55:38 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.0001, bsz=8 ...
05/17/2022 16:55:39 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:55:39 - INFO - __main__ - Printing 3 examples
05/17/2022 16:55:39 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/17/2022 16:55:39 - INFO - __main__ - ['happy']
05/17/2022 16:55:39 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/17/2022 16:55:39 - INFO - __main__ - ['happy']
05/17/2022 16:55:39 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/17/2022 16:55:39 - INFO - __main__ - ['happy']
05/17/2022 16:55:39 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:55:39 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:55:39 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 16:55:39 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 16:55:39 - INFO - __main__ - Printing 3 examples
05/17/2022 16:55:39 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/17/2022 16:55:39 - INFO - __main__ - ['happy']
05/17/2022 16:55:39 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/17/2022 16:55:39 - INFO - __main__ - ['happy']
05/17/2022 16:55:39 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/17/2022 16:55:39 - INFO - __main__ - ['happy']
05/17/2022 16:55:39 - INFO - __main__ - Tokenizing Input ...
05/17/2022 16:55:39 - INFO - __main__ - Tokenizing Output ...
05/17/2022 16:55:39 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 16:55:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 16:55:42 - INFO - __main__ - Starting training!
05/17/2022 16:55:44 - INFO - __main__ - Step 10 Global step 10 Train loss 20.188116 on epoch=2
05/17/2022 16:55:47 - INFO - __main__ - Step 20 Global step 20 Train loss 20.459711 on epoch=4
05/17/2022 16:55:49 - INFO - __main__ - Step 30 Global step 30 Train loss 17.155773 on epoch=7
05/17/2022 16:55:51 - INFO - __main__ - Step 40 Global step 40 Train loss 13.394621 on epoch=9
05/17/2022 16:55:54 - INFO - __main__ - Step 50 Global step 50 Train loss 11.963054 on epoch=12
05/17/2022 16:56:05 - INFO - __main__ - Global step 50 Train loss 16.632254 Classification-F1 0.0 on epoch=12
05/17/2022 16:56:08 - INFO - __main__ - Step 60 Global step 60 Train loss 10.558160 on epoch=14
05/17/2022 16:56:10 - INFO - __main__ - Step 70 Global step 70 Train loss 10.766806 on epoch=17
05/17/2022 16:56:13 - INFO - __main__ - Step 80 Global step 80 Train loss 8.602217 on epoch=19
05/17/2022 16:56:15 - INFO - __main__ - Step 90 Global step 90 Train loss 9.393921 on epoch=22
05/17/2022 16:56:17 - INFO - __main__ - Step 100 Global step 100 Train loss 8.014681 on epoch=24
05/17/2022 16:56:24 - INFO - __main__ - Global step 100 Train loss 9.467156 Classification-F1 0.0 on epoch=24
05/17/2022 16:56:27 - INFO - __main__ - Step 110 Global step 110 Train loss 6.987501 on epoch=27
05/17/2022 16:56:29 - INFO - __main__ - Step 120 Global step 120 Train loss 6.369607 on epoch=29
05/17/2022 16:56:32 - INFO - __main__ - Step 130 Global step 130 Train loss 6.424760 on epoch=32
05/17/2022 16:56:34 - INFO - __main__ - Step 140 Global step 140 Train loss 6.873754 on epoch=34
05/17/2022 16:56:37 - INFO - __main__ - Step 150 Global step 150 Train loss 5.086124 on epoch=37
05/17/2022 16:56:37 - INFO - __main__ - Global step 150 Train loss 6.348350 Classification-F1 0.06926406926406926 on epoch=37
05/17/2022 16:56:40 - INFO - __main__ - Step 160 Global step 160 Train loss 5.950022 on epoch=39
05/17/2022 16:56:43 - INFO - __main__ - Step 170 Global step 170 Train loss 4.501969 on epoch=42
05/17/2022 16:56:45 - INFO - __main__ - Step 180 Global step 180 Train loss 4.598905 on epoch=44
05/17/2022 16:56:48 - INFO - __main__ - Step 190 Global step 190 Train loss 4.608664 on epoch=47
05/17/2022 16:56:50 - INFO - __main__ - Step 200 Global step 200 Train loss 3.842213 on epoch=49
05/17/2022 16:56:51 - INFO - __main__ - Global step 200 Train loss 4.700355 Classification-F1 0.1 on epoch=49
05/17/2022 16:56:54 - INFO - __main__ - Step 210 Global step 210 Train loss 3.754418 on epoch=52
05/17/2022 16:56:56 - INFO - __main__ - Step 220 Global step 220 Train loss 3.861505 on epoch=54
05/17/2022 16:56:59 - INFO - __main__ - Step 230 Global step 230 Train loss 3.785038 on epoch=57
05/17/2022 16:57:01 - INFO - __main__ - Step 240 Global step 240 Train loss 3.589116 on epoch=59
05/17/2022 16:57:04 - INFO - __main__ - Step 250 Global step 250 Train loss 4.237741 on epoch=62
05/17/2022 16:57:04 - INFO - __main__ - Global step 250 Train loss 3.845564 Classification-F1 0.1 on epoch=62
05/17/2022 16:57:06 - INFO - __main__ - Step 260 Global step 260 Train loss 4.205920 on epoch=64
05/17/2022 16:57:09 - INFO - __main__ - Step 270 Global step 270 Train loss 2.943230 on epoch=67
05/17/2022 16:57:11 - INFO - __main__ - Step 280 Global step 280 Train loss 3.380414 on epoch=69
05/17/2022 16:57:14 - INFO - __main__ - Step 290 Global step 290 Train loss 3.237811 on epoch=72
05/17/2022 16:57:16 - INFO - __main__ - Step 300 Global step 300 Train loss 3.686842 on epoch=74
05/17/2022 16:57:17 - INFO - __main__ - Global step 300 Train loss 3.490843 Classification-F1 0.1 on epoch=74
05/17/2022 16:57:19 - INFO - __main__ - Step 310 Global step 310 Train loss 3.251497 on epoch=77
05/17/2022 16:57:21 - INFO - __main__ - Step 320 Global step 320 Train loss 2.982701 on epoch=79
05/17/2022 16:57:24 - INFO - __main__ - Step 330 Global step 330 Train loss 2.399809 on epoch=82
05/17/2022 16:57:26 - INFO - __main__ - Step 340 Global step 340 Train loss 2.613891 on epoch=84
05/17/2022 16:57:29 - INFO - __main__ - Step 350 Global step 350 Train loss 3.071323 on epoch=87
05/17/2022 16:57:29 - INFO - __main__ - Global step 350 Train loss 2.863844 Classification-F1 0.19318181818181818 on epoch=87
05/17/2022 16:57:32 - INFO - __main__ - Step 360 Global step 360 Train loss 2.249923 on epoch=89
05/17/2022 16:57:34 - INFO - __main__ - Step 370 Global step 370 Train loss 3.409645 on epoch=92
05/17/2022 16:57:37 - INFO - __main__ - Step 380 Global step 380 Train loss 2.920430 on epoch=94
05/17/2022 16:57:39 - INFO - __main__ - Step 390 Global step 390 Train loss 3.008410 on epoch=97
05/17/2022 16:57:42 - INFO - __main__ - Step 400 Global step 400 Train loss 2.917170 on epoch=99
05/17/2022 16:57:42 - INFO - __main__ - Global step 400 Train loss 2.901115 Classification-F1 0.343228353903627 on epoch=99
05/17/2022 16:57:45 - INFO - __main__ - Step 410 Global step 410 Train loss 2.473569 on epoch=102
05/17/2022 16:57:47 - INFO - __main__ - Step 420 Global step 420 Train loss 2.458812 on epoch=104
05/17/2022 16:57:50 - INFO - __main__ - Step 430 Global step 430 Train loss 1.994996 on epoch=107
05/17/2022 16:57:52 - INFO - __main__ - Step 440 Global step 440 Train loss 2.196882 on epoch=109
05/17/2022 16:57:54 - INFO - __main__ - Step 450 Global step 450 Train loss 2.037521 on epoch=112
05/17/2022 16:57:55 - INFO - __main__ - Global step 450 Train loss 2.232356 Classification-F1 0.3630337026563441 on epoch=112
05/17/2022 16:57:58 - INFO - __main__ - Step 460 Global step 460 Train loss 1.985961 on epoch=114
05/17/2022 16:58:00 - INFO - __main__ - Step 470 Global step 470 Train loss 2.028961 on epoch=117
05/17/2022 16:58:02 - INFO - __main__ - Step 480 Global step 480 Train loss 1.921823 on epoch=119
05/17/2022 16:58:05 - INFO - __main__ - Step 490 Global step 490 Train loss 1.820828 on epoch=122
05/17/2022 16:58:07 - INFO - __main__ - Step 500 Global step 500 Train loss 2.334344 on epoch=124
05/17/2022 16:58:08 - INFO - __main__ - Global step 500 Train loss 2.018384 Classification-F1 0.35844155844155845 on epoch=124
05/17/2022 16:58:10 - INFO - __main__ - Step 510 Global step 510 Train loss 1.361304 on epoch=127
05/17/2022 16:58:12 - INFO - __main__ - Step 520 Global step 520 Train loss 1.710670 on epoch=129
05/17/2022 16:58:15 - INFO - __main__ - Step 530 Global step 530 Train loss 1.692276 on epoch=132
05/17/2022 16:58:17 - INFO - __main__ - Step 540 Global step 540 Train loss 1.379794 on epoch=134
05/17/2022 16:58:20 - INFO - __main__ - Step 550 Global step 550 Train loss 1.477528 on epoch=137
05/17/2022 16:58:20 - INFO - __main__ - Global step 550 Train loss 1.524314 Classification-F1 0.42183359830418654 on epoch=137
05/17/2022 16:58:23 - INFO - __main__ - Step 560 Global step 560 Train loss 1.717372 on epoch=139
05/17/2022 16:58:25 - INFO - __main__ - Step 570 Global step 570 Train loss 1.399165 on epoch=142
05/17/2022 16:58:28 - INFO - __main__ - Step 580 Global step 580 Train loss 1.616998 on epoch=144
05/17/2022 16:58:30 - INFO - __main__ - Step 590 Global step 590 Train loss 1.605499 on epoch=147
05/17/2022 16:58:33 - INFO - __main__ - Step 600 Global step 600 Train loss 1.624048 on epoch=149
05/17/2022 16:58:33 - INFO - __main__ - Global step 600 Train loss 1.592616 Classification-F1 0.42538126361655776 on epoch=149
05/17/2022 16:58:36 - INFO - __main__ - Step 610 Global step 610 Train loss 1.520099 on epoch=152
05/17/2022 16:58:38 - INFO - __main__ - Step 620 Global step 620 Train loss 1.525340 on epoch=154
05/17/2022 16:58:41 - INFO - __main__ - Step 630 Global step 630 Train loss 1.715015 on epoch=157
05/17/2022 16:58:43 - INFO - __main__ - Step 640 Global step 640 Train loss 1.368395 on epoch=159
05/17/2022 16:58:46 - INFO - __main__ - Step 650 Global step 650 Train loss 1.358964 on epoch=162
05/17/2022 16:58:46 - INFO - __main__ - Global step 650 Train loss 1.497563 Classification-F1 0.48598484848484846 on epoch=162
05/17/2022 16:58:49 - INFO - __main__ - Step 660 Global step 660 Train loss 1.075009 on epoch=164
05/17/2022 16:58:51 - INFO - __main__ - Step 670 Global step 670 Train loss 1.146414 on epoch=167
05/17/2022 16:58:54 - INFO - __main__ - Step 680 Global step 680 Train loss 1.137140 on epoch=169
05/17/2022 16:58:56 - INFO - __main__ - Step 690 Global step 690 Train loss 1.159340 on epoch=172
05/17/2022 16:58:59 - INFO - __main__ - Step 700 Global step 700 Train loss 1.569936 on epoch=174
05/17/2022 16:58:59 - INFO - __main__ - Global step 700 Train loss 1.217568 Classification-F1 0.5071428571428571 on epoch=174
05/17/2022 16:59:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.992634 on epoch=177
05/17/2022 16:59:04 - INFO - __main__ - Step 720 Global step 720 Train loss 1.427206 on epoch=179
05/17/2022 16:59:07 - INFO - __main__ - Step 730 Global step 730 Train loss 1.181202 on epoch=182
05/17/2022 16:59:09 - INFO - __main__ - Step 740 Global step 740 Train loss 1.194461 on epoch=184
05/17/2022 16:59:12 - INFO - __main__ - Step 750 Global step 750 Train loss 1.240501 on epoch=187
05/17/2022 16:59:12 - INFO - __main__ - Global step 750 Train loss 1.207201 Classification-F1 0.4868374868374868 on epoch=187
05/17/2022 16:59:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.995322 on epoch=189
05/17/2022 16:59:17 - INFO - __main__ - Step 770 Global step 770 Train loss 1.092723 on epoch=192
05/17/2022 16:59:20 - INFO - __main__ - Step 780 Global step 780 Train loss 1.061576 on epoch=194
05/17/2022 16:59:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.927470 on epoch=197
05/17/2022 16:59:24 - INFO - __main__ - Step 800 Global step 800 Train loss 1.163476 on epoch=199
05/17/2022 16:59:25 - INFO - __main__ - Global step 800 Train loss 1.048114 Classification-F1 0.4830956625074272 on epoch=199
05/17/2022 16:59:27 - INFO - __main__ - Step 810 Global step 810 Train loss 1.086941 on epoch=202
05/17/2022 16:59:30 - INFO - __main__ - Step 820 Global step 820 Train loss 1.335453 on epoch=204
05/17/2022 16:59:32 - INFO - __main__ - Step 830 Global step 830 Train loss 1.374597 on epoch=207
05/17/2022 16:59:35 - INFO - __main__ - Step 840 Global step 840 Train loss 1.131267 on epoch=209
05/17/2022 16:59:37 - INFO - __main__ - Step 850 Global step 850 Train loss 1.063754 on epoch=212
05/17/2022 16:59:37 - INFO - __main__ - Global step 850 Train loss 1.198402 Classification-F1 0.5106811145510836 on epoch=212
05/17/2022 16:59:40 - INFO - __main__ - Step 860 Global step 860 Train loss 1.199295 on epoch=214
05/17/2022 16:59:43 - INFO - __main__ - Step 870 Global step 870 Train loss 1.031544 on epoch=217
05/17/2022 16:59:45 - INFO - __main__ - Step 880 Global step 880 Train loss 1.303052 on epoch=219
05/17/2022 16:59:47 - INFO - __main__ - Step 890 Global step 890 Train loss 1.108434 on epoch=222
05/17/2022 16:59:50 - INFO - __main__ - Step 900 Global step 900 Train loss 1.101313 on epoch=224
05/17/2022 16:59:50 - INFO - __main__ - Global step 900 Train loss 1.148728 Classification-F1 0.537719298245614 on epoch=224
05/17/2022 16:59:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.909345 on epoch=227
05/17/2022 16:59:55 - INFO - __main__ - Step 920 Global step 920 Train loss 1.044569 on epoch=229
05/17/2022 16:59:58 - INFO - __main__ - Step 930 Global step 930 Train loss 1.065129 on epoch=232
05/17/2022 17:00:00 - INFO - __main__ - Step 940 Global step 940 Train loss 1.252564 on epoch=234
05/17/2022 17:00:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.974529 on epoch=237
05/17/2022 17:00:03 - INFO - __main__ - Global step 950 Train loss 1.049227 Classification-F1 0.5222886762360446 on epoch=237
05/17/2022 17:00:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.908524 on epoch=239
05/17/2022 17:00:08 - INFO - __main__ - Step 970 Global step 970 Train loss 1.066595 on epoch=242
05/17/2022 17:00:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.948404 on epoch=244
05/17/2022 17:00:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.902706 on epoch=247
05/17/2022 17:00:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.999560 on epoch=249
05/17/2022 17:00:16 - INFO - __main__ - Global step 1000 Train loss 0.965158 Classification-F1 0.49335874335874336 on epoch=249
05/17/2022 17:00:16 - INFO - __main__ - save last model!
05/17/2022 17:00:17 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 17:00:17 - INFO - __main__ - Printing 3 examples
05/17/2022 17:00:17 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/17/2022 17:00:17 - INFO - __main__ - ['others']
05/17/2022 17:00:17 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/17/2022 17:00:17 - INFO - __main__ - ['others']
05/17/2022 17:00:17 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/17/2022 17:00:17 - INFO - __main__ - ['others']
05/17/2022 17:00:17 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:00:17 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:00:17 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 17:00:17 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 17:00:17 - INFO - __main__ - Printing 3 examples
05/17/2022 17:00:17 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/17/2022 17:00:17 - INFO - __main__ - ['others']
05/17/2022 17:00:17 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/17/2022 17:00:17 - INFO - __main__ - ['others']
05/17/2022 17:00:17 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/17/2022 17:00:17 - INFO - __main__ - ['others']
05/17/2022 17:00:17 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:00:17 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:00:17 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 17:00:18 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 17:00:19 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 17:00:19 - INFO - __main__ - Printing 3 examples
05/17/2022 17:00:19 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 17:00:19 - INFO - __main__ - ['others']
05/17/2022 17:00:19 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 17:00:19 - INFO - __main__ - ['others']
05/17/2022 17:00:19 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 17:00:19 - INFO - __main__ - ['others']
05/17/2022 17:00:19 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:00:21 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:00:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 17:00:21 - INFO - __main__ - Starting training!
05/17/2022 17:00:26 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 17:00:54 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_42_0.0001_8_predictions.txt
05/17/2022 17:00:54 - INFO - __main__ - Classification-F1 on test data: 0.1529
05/17/2022 17:00:55 - INFO - __main__ - prefix=emo_16_42, lr=0.0001, bsz=8, dev_performance=0.537719298245614, test_performance=0.1528975225354412
05/17/2022 17:00:55 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.0005, bsz=8 ...
05/17/2022 17:00:55 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 17:00:55 - INFO - __main__ - Printing 3 examples
05/17/2022 17:00:55 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/17/2022 17:00:55 - INFO - __main__ - ['others']
05/17/2022 17:00:55 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/17/2022 17:00:55 - INFO - __main__ - ['others']
05/17/2022 17:00:55 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/17/2022 17:00:55 - INFO - __main__ - ['others']
05/17/2022 17:00:55 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:00:55 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:00:56 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 17:00:56 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 17:00:56 - INFO - __main__ - Printing 3 examples
05/17/2022 17:00:56 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/17/2022 17:00:56 - INFO - __main__ - ['others']
05/17/2022 17:00:56 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/17/2022 17:00:56 - INFO - __main__ - ['others']
05/17/2022 17:00:56 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/17/2022 17:00:56 - INFO - __main__ - ['others']
05/17/2022 17:00:56 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:00:56 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:00:56 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 17:00:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 17:00:59 - INFO - __main__ - Starting training!
05/17/2022 17:01:01 - INFO - __main__ - Step 10 Global step 10 Train loss 19.736971 on epoch=2
05/17/2022 17:01:04 - INFO - __main__ - Step 20 Global step 20 Train loss 13.522745 on epoch=4
05/17/2022 17:01:06 - INFO - __main__ - Step 30 Global step 30 Train loss 7.024580 on epoch=7
05/17/2022 17:01:08 - INFO - __main__ - Step 40 Global step 40 Train loss 5.755816 on epoch=9
05/17/2022 17:01:11 - INFO - __main__ - Step 50 Global step 50 Train loss 3.938964 on epoch=12
05/17/2022 17:01:11 - INFO - __main__ - Global step 50 Train loss 9.995815 Classification-F1 0.1 on epoch=12
05/17/2022 17:01:14 - INFO - __main__ - Step 60 Global step 60 Train loss 3.491011 on epoch=14
05/17/2022 17:01:16 - INFO - __main__ - Step 70 Global step 70 Train loss 2.525465 on epoch=17
05/17/2022 17:01:19 - INFO - __main__ - Step 80 Global step 80 Train loss 2.481082 on epoch=19
05/17/2022 17:01:21 - INFO - __main__ - Step 90 Global step 90 Train loss 2.469923 on epoch=22
05/17/2022 17:01:24 - INFO - __main__ - Step 100 Global step 100 Train loss 2.270663 on epoch=24
05/17/2022 17:01:24 - INFO - __main__ - Global step 100 Train loss 2.647629 Classification-F1 0.3541027710519236 on epoch=24
05/17/2022 17:01:27 - INFO - __main__ - Step 110 Global step 110 Train loss 1.399655 on epoch=27
05/17/2022 17:01:29 - INFO - __main__ - Step 120 Global step 120 Train loss 1.146433 on epoch=29
05/17/2022 17:01:32 - INFO - __main__ - Step 130 Global step 130 Train loss 1.496571 on epoch=32
05/17/2022 17:01:34 - INFO - __main__ - Step 140 Global step 140 Train loss 1.372790 on epoch=34
05/17/2022 17:01:37 - INFO - __main__ - Step 150 Global step 150 Train loss 1.021240 on epoch=37
05/17/2022 17:01:37 - INFO - __main__ - Global step 150 Train loss 1.287338 Classification-F1 0.6824592074592074 on epoch=37
05/17/2022 17:01:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.832503 on epoch=39
05/17/2022 17:01:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.970364 on epoch=42
05/17/2022 17:01:45 - INFO - __main__ - Step 180 Global step 180 Train loss 1.026563 on epoch=44
05/17/2022 17:01:47 - INFO - __main__ - Step 190 Global step 190 Train loss 1.261707 on epoch=47
05/17/2022 17:01:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.838137 on epoch=49
05/17/2022 17:01:50 - INFO - __main__ - Global step 200 Train loss 0.985855 Classification-F1 0.6302724660876138 on epoch=49
05/17/2022 17:01:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.849985 on epoch=52
05/17/2022 17:01:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.771126 on epoch=54
05/17/2022 17:01:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.951964 on epoch=57
05/17/2022 17:02:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.940963 on epoch=59
05/17/2022 17:02:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.896752 on epoch=62
05/17/2022 17:02:03 - INFO - __main__ - Global step 250 Train loss 0.882158 Classification-F1 0.6698329448329449 on epoch=62
05/17/2022 17:02:05 - INFO - __main__ - Step 260 Global step 260 Train loss 1.052186 on epoch=64
05/17/2022 17:02:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.744363 on epoch=67
05/17/2022 17:02:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.590314 on epoch=69
05/17/2022 17:02:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.667233 on epoch=72
05/17/2022 17:02:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.567460 on epoch=74
05/17/2022 17:02:15 - INFO - __main__ - Global step 300 Train loss 0.724311 Classification-F1 0.5859199280251912 on epoch=74
05/17/2022 17:02:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.495514 on epoch=77
05/17/2022 17:02:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.740189 on epoch=79
05/17/2022 17:02:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.645764 on epoch=82
05/17/2022 17:02:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.896815 on epoch=84
05/17/2022 17:02:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.885171 on epoch=87
05/17/2022 17:02:28 - INFO - __main__ - Global step 350 Train loss 0.732691 Classification-F1 0.6515087451530402 on epoch=87
05/17/2022 17:02:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.616605 on epoch=89
05/17/2022 17:02:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.814271 on epoch=92
05/17/2022 17:02:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.663735 on epoch=94
05/17/2022 17:02:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.632074 on epoch=97
05/17/2022 17:02:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.728465 on epoch=99
05/17/2022 17:02:41 - INFO - __main__ - Global step 400 Train loss 0.691030 Classification-F1 0.6360511326543936 on epoch=99
05/17/2022 17:02:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.538413 on epoch=102
05/17/2022 17:02:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.546857 on epoch=104
05/17/2022 17:02:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.655587 on epoch=107
05/17/2022 17:02:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.748998 on epoch=109
05/17/2022 17:02:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.854687 on epoch=112
05/17/2022 17:02:53 - INFO - __main__ - Global step 450 Train loss 0.668908 Classification-F1 0.7072029242731575 on epoch=112
05/17/2022 17:02:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.771636 on epoch=114
05/17/2022 17:02:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.910666 on epoch=117
05/17/2022 17:03:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.699909 on epoch=119
05/17/2022 17:03:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.613345 on epoch=122
05/17/2022 17:03:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.564697 on epoch=124
05/17/2022 17:03:07 - INFO - __main__ - Global step 500 Train loss 0.712051 Classification-F1 0.5236957282913165 on epoch=124
05/17/2022 17:03:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.716833 on epoch=127
05/17/2022 17:03:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.618839 on epoch=129
05/17/2022 17:03:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.778302 on epoch=132
05/17/2022 17:03:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.604818 on epoch=134
05/17/2022 17:03:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.601016 on epoch=137
05/17/2022 17:03:19 - INFO - __main__ - Global step 550 Train loss 0.663961 Classification-F1 0.6796423578751165 on epoch=137
05/17/2022 17:03:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.590153 on epoch=139
05/17/2022 17:03:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.551554 on epoch=142
05/17/2022 17:03:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.648984 on epoch=144
05/17/2022 17:03:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.298573 on epoch=147
05/17/2022 17:03:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.343003 on epoch=149
05/17/2022 17:03:32 - INFO - __main__ - Global step 600 Train loss 0.486453 Classification-F1 0.6934995737425406 on epoch=149
05/17/2022 17:03:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.428537 on epoch=152
05/17/2022 17:03:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.257205 on epoch=154
05/17/2022 17:03:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.306596 on epoch=157
05/17/2022 17:03:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.222915 on epoch=159
05/17/2022 17:03:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.212433 on epoch=162
05/17/2022 17:03:44 - INFO - __main__ - Global step 650 Train loss 0.285537 Classification-F1 0.7645405669599218 on epoch=162
05/17/2022 17:03:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.307462 on epoch=164
05/17/2022 17:03:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.178304 on epoch=167
05/17/2022 17:03:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.197698 on epoch=169
05/17/2022 17:03:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.117445 on epoch=172
05/17/2022 17:03:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.098542 on epoch=174
05/17/2022 17:03:57 - INFO - __main__ - Global step 700 Train loss 0.179890 Classification-F1 0.7558324028912264 on epoch=174
05/17/2022 17:04:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.150162 on epoch=177
05/17/2022 17:04:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.164202 on epoch=179
05/17/2022 17:04:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.221016 on epoch=182
05/17/2022 17:04:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.140818 on epoch=184
05/17/2022 17:04:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.078510 on epoch=187
05/17/2022 17:04:10 - INFO - __main__ - Global step 750 Train loss 0.150942 Classification-F1 0.7252309577677225 on epoch=187
05/17/2022 17:04:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.149463 on epoch=189
05/17/2022 17:04:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.090933 on epoch=192
05/17/2022 17:04:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.041779 on epoch=194
05/17/2022 17:04:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.183359 on epoch=197
05/17/2022 17:04:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.146283 on epoch=199
05/17/2022 17:04:23 - INFO - __main__ - Global step 800 Train loss 0.122364 Classification-F1 0.7097652681144142 on epoch=199
05/17/2022 17:04:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.151235 on epoch=202
05/17/2022 17:04:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.090045 on epoch=204
05/17/2022 17:04:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.133029 on epoch=207
05/17/2022 17:04:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.188133 on epoch=209
05/17/2022 17:04:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.184254 on epoch=212
05/17/2022 17:04:35 - INFO - __main__ - Global step 850 Train loss 0.149339 Classification-F1 0.7440159934941718 on epoch=212
05/17/2022 17:04:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.084493 on epoch=214
05/17/2022 17:04:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.077679 on epoch=217
05/17/2022 17:04:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.209689 on epoch=219
05/17/2022 17:04:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.126164 on epoch=222
05/17/2022 17:04:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.127438 on epoch=224
05/17/2022 17:04:48 - INFO - __main__ - Global step 900 Train loss 0.125093 Classification-F1 0.7601450257522364 on epoch=224
05/17/2022 17:04:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.097802 on epoch=227
05/17/2022 17:04:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.139331 on epoch=229
05/17/2022 17:04:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.056322 on epoch=232
05/17/2022 17:04:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.108906 on epoch=234
05/17/2022 17:05:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.127257 on epoch=237
05/17/2022 17:05:01 - INFO - __main__ - Global step 950 Train loss 0.105924 Classification-F1 0.7111996187363834 on epoch=237
05/17/2022 17:05:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.054392 on epoch=239
05/17/2022 17:05:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.056505 on epoch=242
05/17/2022 17:05:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.037612 on epoch=244
05/17/2022 17:05:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.032587 on epoch=247
05/17/2022 17:05:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.058651 on epoch=249
05/17/2022 17:05:14 - INFO - __main__ - Global step 1000 Train loss 0.047949 Classification-F1 0.7447129807530252 on epoch=249
05/17/2022 17:05:14 - INFO - __main__ - save last model!
05/17/2022 17:05:14 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 17:05:14 - INFO - __main__ - Printing 3 examples
05/17/2022 17:05:14 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/17/2022 17:05:14 - INFO - __main__ - ['others']
05/17/2022 17:05:14 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/17/2022 17:05:14 - INFO - __main__ - ['others']
05/17/2022 17:05:14 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/17/2022 17:05:14 - INFO - __main__ - ['others']
05/17/2022 17:05:14 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:05:14 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:05:15 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 17:05:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 17:05:15 - INFO - __main__ - Printing 3 examples
05/17/2022 17:05:15 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/17/2022 17:05:15 - INFO - __main__ - ['others']
05/17/2022 17:05:15 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/17/2022 17:05:15 - INFO - __main__ - ['others']
05/17/2022 17:05:15 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/17/2022 17:05:15 - INFO - __main__ - ['others']
05/17/2022 17:05:15 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:05:15 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:05:15 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 17:05:16 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 17:05:17 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 17:05:17 - INFO - __main__ - Printing 3 examples
05/17/2022 17:05:17 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 17:05:17 - INFO - __main__ - ['others']
05/17/2022 17:05:17 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 17:05:17 - INFO - __main__ - ['others']
05/17/2022 17:05:17 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 17:05:17 - INFO - __main__ - ['others']
05/17/2022 17:05:17 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:05:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 17:05:18 - INFO - __main__ - Starting training!
05/17/2022 17:05:19 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:05:24 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 17:05:52 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_87_0.0005_8_predictions.txt
05/17/2022 17:05:53 - INFO - __main__ - Classification-F1 on test data: 0.4266
05/17/2022 17:05:53 - INFO - __main__ - prefix=emo_16_87, lr=0.0005, bsz=8, dev_performance=0.7645405669599218, test_performance=0.4266098395438303
05/17/2022 17:05:53 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.0003, bsz=8 ...
05/17/2022 17:05:54 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 17:05:54 - INFO - __main__ - Printing 3 examples
05/17/2022 17:05:54 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/17/2022 17:05:54 - INFO - __main__ - ['others']
05/17/2022 17:05:54 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/17/2022 17:05:54 - INFO - __main__ - ['others']
05/17/2022 17:05:54 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/17/2022 17:05:54 - INFO - __main__ - ['others']
05/17/2022 17:05:54 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:05:54 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:05:54 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 17:05:54 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 17:05:54 - INFO - __main__ - Printing 3 examples
05/17/2022 17:05:54 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/17/2022 17:05:54 - INFO - __main__ - ['others']
05/17/2022 17:05:54 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/17/2022 17:05:54 - INFO - __main__ - ['others']
05/17/2022 17:05:54 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/17/2022 17:05:54 - INFO - __main__ - ['others']
05/17/2022 17:05:54 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:05:54 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:05:54 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 17:05:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 17:05:58 - INFO - __main__ - Starting training!
05/17/2022 17:06:00 - INFO - __main__ - Step 10 Global step 10 Train loss 21.232431 on epoch=2
05/17/2022 17:06:02 - INFO - __main__ - Step 20 Global step 20 Train loss 18.306307 on epoch=4
05/17/2022 17:06:05 - INFO - __main__ - Step 30 Global step 30 Train loss 10.778108 on epoch=7
05/17/2022 17:06:07 - INFO - __main__ - Step 40 Global step 40 Train loss 10.043188 on epoch=9
05/17/2022 17:06:10 - INFO - __main__ - Step 50 Global step 50 Train loss 6.270121 on epoch=12
05/17/2022 17:06:15 - INFO - __main__ - Global step 50 Train loss 13.326032 Classification-F1 0.0014792899408284025 on epoch=12
05/17/2022 17:06:18 - INFO - __main__ - Step 60 Global step 60 Train loss 6.755998 on epoch=14
05/17/2022 17:06:20 - INFO - __main__ - Step 70 Global step 70 Train loss 4.803172 on epoch=17
05/17/2022 17:06:23 - INFO - __main__ - Step 80 Global step 80 Train loss 3.686452 on epoch=19
05/17/2022 17:06:25 - INFO - __main__ - Step 90 Global step 90 Train loss 3.447287 on epoch=22
05/17/2022 17:06:28 - INFO - __main__ - Step 100 Global step 100 Train loss 2.998932 on epoch=24
05/17/2022 17:06:28 - INFO - __main__ - Global step 100 Train loss 4.338369 Classification-F1 0.13067758749069247 on epoch=24
05/17/2022 17:06:31 - INFO - __main__ - Step 110 Global step 110 Train loss 2.878742 on epoch=27
05/17/2022 17:06:33 - INFO - __main__ - Step 120 Global step 120 Train loss 2.049046 on epoch=29
05/17/2022 17:06:36 - INFO - __main__ - Step 130 Global step 130 Train loss 2.380112 on epoch=32
05/17/2022 17:06:38 - INFO - __main__ - Step 140 Global step 140 Train loss 2.190035 on epoch=34
05/17/2022 17:06:41 - INFO - __main__ - Step 150 Global step 150 Train loss 1.666915 on epoch=37
05/17/2022 17:06:41 - INFO - __main__ - Global step 150 Train loss 2.232970 Classification-F1 0.36199215965787596 on epoch=37
05/17/2022 17:06:44 - INFO - __main__ - Step 160 Global step 160 Train loss 2.539189 on epoch=39
05/17/2022 17:06:46 - INFO - __main__ - Step 170 Global step 170 Train loss 1.623471 on epoch=42
05/17/2022 17:06:49 - INFO - __main__ - Step 180 Global step 180 Train loss 1.855997 on epoch=44
05/17/2022 17:06:51 - INFO - __main__ - Step 190 Global step 190 Train loss 1.213165 on epoch=47
05/17/2022 17:06:54 - INFO - __main__ - Step 200 Global step 200 Train loss 1.528245 on epoch=49
05/17/2022 17:06:54 - INFO - __main__ - Global step 200 Train loss 1.752014 Classification-F1 0.4886496265172736 on epoch=49
05/17/2022 17:06:57 - INFO - __main__ - Step 210 Global step 210 Train loss 1.413488 on epoch=52
05/17/2022 17:07:00 - INFO - __main__ - Step 220 Global step 220 Train loss 1.184772 on epoch=54
05/17/2022 17:07:03 - INFO - __main__ - Step 230 Global step 230 Train loss 1.216046 on epoch=57
05/17/2022 17:07:05 - INFO - __main__ - Step 240 Global step 240 Train loss 1.294843 on epoch=59
05/17/2022 17:07:08 - INFO - __main__ - Step 250 Global step 250 Train loss 1.218281 on epoch=62
05/17/2022 17:07:08 - INFO - __main__ - Global step 250 Train loss 1.265486 Classification-F1 0.6396026831785346 on epoch=62
05/17/2022 17:07:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.524447 on epoch=64
05/17/2022 17:07:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.755071 on epoch=67
05/17/2022 17:07:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.831779 on epoch=69
05/17/2022 17:07:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.767410 on epoch=72
05/17/2022 17:07:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.789809 on epoch=74
05/17/2022 17:07:21 - INFO - __main__ - Global step 300 Train loss 0.733703 Classification-F1 0.6086083679833679 on epoch=74
05/17/2022 17:07:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.491948 on epoch=77
05/17/2022 17:07:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.824879 on epoch=79
05/17/2022 17:07:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.632454 on epoch=82
05/17/2022 17:07:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.637731 on epoch=84
05/17/2022 17:07:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.610705 on epoch=87
05/17/2022 17:07:34 - INFO - __main__ - Global step 350 Train loss 0.639543 Classification-F1 0.6795093795093794 on epoch=87
05/17/2022 17:07:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.583748 on epoch=89
05/17/2022 17:07:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.544670 on epoch=92
05/17/2022 17:07:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.356200 on epoch=94
05/17/2022 17:07:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.647723 on epoch=97
05/17/2022 17:07:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.633238 on epoch=99
05/17/2022 17:07:47 - INFO - __main__ - Global step 400 Train loss 0.553116 Classification-F1 0.6308558558558558 on epoch=99
05/17/2022 17:07:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.532766 on epoch=102
05/17/2022 17:07:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.559285 on epoch=104
05/17/2022 17:07:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.418977 on epoch=107
05/17/2022 17:07:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.526117 on epoch=109
05/17/2022 17:08:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.409038 on epoch=112
05/17/2022 17:08:00 - INFO - __main__ - Global step 450 Train loss 0.489237 Classification-F1 0.736638655462185 on epoch=112
05/17/2022 17:08:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.342286 on epoch=114
05/17/2022 17:08:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.284193 on epoch=117
05/17/2022 17:08:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.470620 on epoch=119
05/17/2022 17:08:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.483785 on epoch=122
05/17/2022 17:08:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.492757 on epoch=124
05/17/2022 17:08:14 - INFO - __main__ - Global step 500 Train loss 0.414728 Classification-F1 0.6558558558558558 on epoch=124
05/17/2022 17:08:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.419809 on epoch=127
05/17/2022 17:08:19 - INFO - __main__ - Step 520 Global step 520 Train loss 1.029251 on epoch=129
05/17/2022 17:08:21 - INFO - __main__ - Step 530 Global step 530 Train loss 1.174029 on epoch=132
05/17/2022 17:08:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.519481 on epoch=134
05/17/2022 17:08:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.381812 on epoch=137
05/17/2022 17:08:26 - INFO - __main__ - Global step 550 Train loss 0.704876 Classification-F1 0.7904804804804805 on epoch=137
05/17/2022 17:08:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.384470 on epoch=139
05/17/2022 17:08:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.373137 on epoch=142
05/17/2022 17:08:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.207784 on epoch=144
05/17/2022 17:08:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.290279 on epoch=147
05/17/2022 17:08:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.125024 on epoch=149
05/17/2022 17:08:40 - INFO - __main__ - Global step 600 Train loss 0.276139 Classification-F1 0.7215432824128476 on epoch=149
05/17/2022 17:08:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.325276 on epoch=152
05/17/2022 17:08:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.241823 on epoch=154
05/17/2022 17:08:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.178045 on epoch=157
05/17/2022 17:08:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.291337 on epoch=159
05/17/2022 17:08:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.182360 on epoch=162
05/17/2022 17:08:53 - INFO - __main__ - Global step 650 Train loss 0.243768 Classification-F1 0.7741891891891891 on epoch=162
05/17/2022 17:08:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.192884 on epoch=164
05/17/2022 17:08:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.206367 on epoch=167
05/17/2022 17:09:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.171828 on epoch=169
05/17/2022 17:09:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.169324 on epoch=172
05/17/2022 17:09:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.243195 on epoch=174
05/17/2022 17:09:06 - INFO - __main__ - Global step 700 Train loss 0.196720 Classification-F1 0.6859903381642511 on epoch=174
05/17/2022 17:09:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.139962 on epoch=177
05/17/2022 17:09:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.233864 on epoch=179
05/17/2022 17:09:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.155839 on epoch=182
05/17/2022 17:09:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.154621 on epoch=184
05/17/2022 17:09:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.065830 on epoch=187
05/17/2022 17:09:19 - INFO - __main__ - Global step 750 Train loss 0.150024 Classification-F1 0.6457108829774364 on epoch=187
05/17/2022 17:09:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.213679 on epoch=189
05/17/2022 17:09:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.240698 on epoch=192
05/17/2022 17:09:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.093774 on epoch=194
05/17/2022 17:09:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.111670 on epoch=197
05/17/2022 17:09:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.129637 on epoch=199
05/17/2022 17:09:31 - INFO - __main__ - Global step 800 Train loss 0.157891 Classification-F1 0.739519761697181 on epoch=199
05/17/2022 17:09:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.081556 on epoch=202
05/17/2022 17:09:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.082180 on epoch=204
05/17/2022 17:09:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.075290 on epoch=207
05/17/2022 17:09:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.211163 on epoch=209
05/17/2022 17:09:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.106323 on epoch=212
05/17/2022 17:09:44 - INFO - __main__ - Global step 850 Train loss 0.111302 Classification-F1 0.7673611111111112 on epoch=212
05/17/2022 17:09:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.104486 on epoch=214
05/17/2022 17:09:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.110281 on epoch=217
05/17/2022 17:09:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.057271 on epoch=219
05/17/2022 17:09:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.029600 on epoch=222
05/17/2022 17:09:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.127194 on epoch=224
05/17/2022 17:09:57 - INFO - __main__ - Global step 900 Train loss 0.085766 Classification-F1 0.6928096545743605 on epoch=224
05/17/2022 17:10:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.095698 on epoch=227
05/17/2022 17:10:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.046395 on epoch=229
05/17/2022 17:10:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.045527 on epoch=232
05/17/2022 17:10:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.106064 on epoch=234
05/17/2022 17:10:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.030559 on epoch=237
05/17/2022 17:10:10 - INFO - __main__ - Global step 950 Train loss 0.064849 Classification-F1 0.7721212121212122 on epoch=237
05/17/2022 17:10:13 - INFO - __main__ - Step 960 Global step 960 Train loss 0.020303 on epoch=239
05/17/2022 17:10:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.075400 on epoch=242
05/17/2022 17:10:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.072006 on epoch=244
05/17/2022 17:10:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.047109 on epoch=247
05/17/2022 17:10:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.040909 on epoch=249
05/17/2022 17:10:23 - INFO - __main__ - Global step 1000 Train loss 0.051145 Classification-F1 0.7242782555282555 on epoch=249
05/17/2022 17:10:23 - INFO - __main__ - save last model!
05/17/2022 17:10:24 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 17:10:24 - INFO - __main__ - Printing 3 examples
05/17/2022 17:10:24 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/17/2022 17:10:24 - INFO - __main__ - ['others']
05/17/2022 17:10:24 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/17/2022 17:10:24 - INFO - __main__ - ['others']
05/17/2022 17:10:24 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/17/2022 17:10:24 - INFO - __main__ - ['others']
05/17/2022 17:10:24 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:10:24 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:10:24 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 17:10:24 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 17:10:24 - INFO - __main__ - Printing 3 examples
05/17/2022 17:10:24 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/17/2022 17:10:24 - INFO - __main__ - ['others']
05/17/2022 17:10:24 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/17/2022 17:10:24 - INFO - __main__ - ['others']
05/17/2022 17:10:24 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/17/2022 17:10:24 - INFO - __main__ - ['others']
05/17/2022 17:10:24 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:10:24 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:10:24 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 17:10:26 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 17:10:26 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 17:10:26 - INFO - __main__ - Printing 3 examples
05/17/2022 17:10:26 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 17:10:26 - INFO - __main__ - ['others']
05/17/2022 17:10:26 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 17:10:26 - INFO - __main__ - ['others']
05/17/2022 17:10:26 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 17:10:26 - INFO - __main__ - ['others']
05/17/2022 17:10:26 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:10:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 17:10:28 - INFO - __main__ - Starting training!
05/17/2022 17:10:28 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:10:34 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 17:11:03 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_87_0.0003_8_predictions.txt
05/17/2022 17:11:03 - INFO - __main__ - Classification-F1 on test data: 0.4593
05/17/2022 17:11:03 - INFO - __main__ - prefix=emo_16_87, lr=0.0003, bsz=8, dev_performance=0.7904804804804805, test_performance=0.45932463797624035
05/17/2022 17:11:03 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.0002, bsz=8 ...
05/17/2022 17:11:04 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 17:11:04 - INFO - __main__ - Printing 3 examples
05/17/2022 17:11:04 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/17/2022 17:11:04 - INFO - __main__ - ['others']
05/17/2022 17:11:04 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/17/2022 17:11:04 - INFO - __main__ - ['others']
05/17/2022 17:11:04 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/17/2022 17:11:04 - INFO - __main__ - ['others']
05/17/2022 17:11:04 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:11:04 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:11:04 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 17:11:04 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 17:11:04 - INFO - __main__ - Printing 3 examples
05/17/2022 17:11:04 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/17/2022 17:11:04 - INFO - __main__ - ['others']
05/17/2022 17:11:04 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/17/2022 17:11:04 - INFO - __main__ - ['others']
05/17/2022 17:11:04 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/17/2022 17:11:04 - INFO - __main__ - ['others']
05/17/2022 17:11:04 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:11:04 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:11:04 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 17:11:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 17:11:08 - INFO - __main__ - Starting training!
05/17/2022 17:11:09 - INFO - __main__ - Step 10 Global step 10 Train loss 19.732557 on epoch=2
05/17/2022 17:11:12 - INFO - __main__ - Step 20 Global step 20 Train loss 19.679234 on epoch=4
05/17/2022 17:11:14 - INFO - __main__ - Step 30 Global step 30 Train loss 13.481415 on epoch=7
05/17/2022 17:11:16 - INFO - __main__ - Step 40 Global step 40 Train loss 10.151956 on epoch=9
05/17/2022 17:11:19 - INFO - __main__ - Step 50 Global step 50 Train loss 9.041439 on epoch=12
05/17/2022 17:11:27 - INFO - __main__ - Global step 50 Train loss 14.417320 Classification-F1 0.0 on epoch=12
05/17/2022 17:11:29 - INFO - __main__ - Step 60 Global step 60 Train loss 7.662802 on epoch=14
05/17/2022 17:11:32 - INFO - __main__ - Step 70 Global step 70 Train loss 6.638319 on epoch=17
05/17/2022 17:11:34 - INFO - __main__ - Step 80 Global step 80 Train loss 5.148648 on epoch=19
05/17/2022 17:11:37 - INFO - __main__ - Step 90 Global step 90 Train loss 5.359299 on epoch=22
05/17/2022 17:11:39 - INFO - __main__ - Step 100 Global step 100 Train loss 5.511099 on epoch=24
05/17/2022 17:11:39 - INFO - __main__ - Global step 100 Train loss 6.064034 Classification-F1 0.1 on epoch=24
05/17/2022 17:11:42 - INFO - __main__ - Step 110 Global step 110 Train loss 3.184741 on epoch=27
05/17/2022 17:11:45 - INFO - __main__ - Step 120 Global step 120 Train loss 4.093537 on epoch=29
05/17/2022 17:11:47 - INFO - __main__ - Step 130 Global step 130 Train loss 3.492687 on epoch=32
05/17/2022 17:11:50 - INFO - __main__ - Step 140 Global step 140 Train loss 3.201585 on epoch=34
05/17/2022 17:11:52 - INFO - __main__ - Step 150 Global step 150 Train loss 2.108763 on epoch=37
05/17/2022 17:11:52 - INFO - __main__ - Global step 150 Train loss 3.216263 Classification-F1 0.1 on epoch=37
05/17/2022 17:11:55 - INFO - __main__ - Step 160 Global step 160 Train loss 3.289022 on epoch=39
05/17/2022 17:11:57 - INFO - __main__ - Step 170 Global step 170 Train loss 3.303216 on epoch=42
05/17/2022 17:12:00 - INFO - __main__ - Step 180 Global step 180 Train loss 2.519246 on epoch=44
05/17/2022 17:12:02 - INFO - __main__ - Step 190 Global step 190 Train loss 1.947860 on epoch=47
05/17/2022 17:12:05 - INFO - __main__ - Step 200 Global step 200 Train loss 2.716340 on epoch=49
05/17/2022 17:12:05 - INFO - __main__ - Global step 200 Train loss 2.755137 Classification-F1 0.31958776250972254 on epoch=49
05/17/2022 17:12:08 - INFO - __main__ - Step 210 Global step 210 Train loss 2.305012 on epoch=52
05/17/2022 17:12:10 - INFO - __main__ - Step 220 Global step 220 Train loss 2.005485 on epoch=54
05/17/2022 17:12:13 - INFO - __main__ - Step 230 Global step 230 Train loss 2.436990 on epoch=57
05/17/2022 17:12:15 - INFO - __main__ - Step 240 Global step 240 Train loss 2.173930 on epoch=59
05/17/2022 17:12:18 - INFO - __main__ - Step 250 Global step 250 Train loss 1.928057 on epoch=62
05/17/2022 17:12:18 - INFO - __main__ - Global step 250 Train loss 2.169895 Classification-F1 0.4403929403929404 on epoch=62
05/17/2022 17:12:21 - INFO - __main__ - Step 260 Global step 260 Train loss 1.880750 on epoch=64
05/17/2022 17:12:23 - INFO - __main__ - Step 270 Global step 270 Train loss 1.904463 on epoch=67
05/17/2022 17:12:26 - INFO - __main__ - Step 280 Global step 280 Train loss 1.272152 on epoch=69
05/17/2022 17:12:28 - INFO - __main__ - Step 290 Global step 290 Train loss 1.373799 on epoch=72
05/17/2022 17:12:31 - INFO - __main__ - Step 300 Global step 300 Train loss 1.088235 on epoch=74
05/17/2022 17:12:31 - INFO - __main__ - Global step 300 Train loss 1.503880 Classification-F1 0.549807576604448 on epoch=74
05/17/2022 17:12:34 - INFO - __main__ - Step 310 Global step 310 Train loss 1.655835 on epoch=77
05/17/2022 17:12:36 - INFO - __main__ - Step 320 Global step 320 Train loss 1.756117 on epoch=79
05/17/2022 17:12:39 - INFO - __main__ - Step 330 Global step 330 Train loss 1.786130 on epoch=82
05/17/2022 17:12:41 - INFO - __main__ - Step 340 Global step 340 Train loss 1.259457 on epoch=84
05/17/2022 17:12:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.931179 on epoch=87
05/17/2022 17:12:44 - INFO - __main__ - Global step 350 Train loss 1.477744 Classification-F1 0.5552336392119674 on epoch=87
05/17/2022 17:12:47 - INFO - __main__ - Step 360 Global step 360 Train loss 1.542276 on epoch=89
05/17/2022 17:12:49 - INFO - __main__ - Step 370 Global step 370 Train loss 1.684364 on epoch=92
05/17/2022 17:12:52 - INFO - __main__ - Step 380 Global step 380 Train loss 1.107174 on epoch=94
05/17/2022 17:12:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.917991 on epoch=97
05/17/2022 17:12:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.888163 on epoch=99
05/17/2022 17:12:57 - INFO - __main__ - Global step 400 Train loss 1.227993 Classification-F1 0.5680245381858285 on epoch=99
05/17/2022 17:13:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.946626 on epoch=102
05/17/2022 17:13:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.874961 on epoch=104
05/17/2022 17:13:05 - INFO - __main__ - Step 430 Global step 430 Train loss 1.247030 on epoch=107
05/17/2022 17:13:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.807333 on epoch=109
05/17/2022 17:13:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.864573 on epoch=112
05/17/2022 17:13:10 - INFO - __main__ - Global step 450 Train loss 0.948105 Classification-F1 0.6120414673046252 on epoch=112
05/17/2022 17:13:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.744264 on epoch=114
05/17/2022 17:13:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.940925 on epoch=117
05/17/2022 17:13:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.690314 on epoch=119
05/17/2022 17:13:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.710353 on epoch=122
05/17/2022 17:13:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.730009 on epoch=124
05/17/2022 17:13:24 - INFO - __main__ - Global step 500 Train loss 0.763173 Classification-F1 0.5839263076105181 on epoch=124
05/17/2022 17:13:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.674365 on epoch=127
05/17/2022 17:13:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.573164 on epoch=129
05/17/2022 17:13:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.708767 on epoch=132
05/17/2022 17:13:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.550526 on epoch=134
05/17/2022 17:13:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.701598 on epoch=137
05/17/2022 17:13:36 - INFO - __main__ - Global step 550 Train loss 0.641684 Classification-F1 0.5835889256941889 on epoch=137
05/17/2022 17:13:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.535890 on epoch=139
05/17/2022 17:13:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.774242 on epoch=142
05/17/2022 17:13:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.624461 on epoch=144
05/17/2022 17:13:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.501402 on epoch=147
05/17/2022 17:13:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.549546 on epoch=149
05/17/2022 17:13:49 - INFO - __main__ - Global step 600 Train loss 0.597108 Classification-F1 0.568816974247841 on epoch=149
05/17/2022 17:13:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.537877 on epoch=152
05/17/2022 17:13:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.632932 on epoch=154
05/17/2022 17:13:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.417367 on epoch=157
05/17/2022 17:13:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.537404 on epoch=159
05/17/2022 17:14:02 - INFO - __main__ - Step 650 Global step 650 Train loss 0.471380 on epoch=162
05/17/2022 17:14:02 - INFO - __main__ - Global step 650 Train loss 0.519392 Classification-F1 0.6623756360598465 on epoch=162
05/17/2022 17:14:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.588970 on epoch=164
05/17/2022 17:14:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.690749 on epoch=167
05/17/2022 17:14:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.589263 on epoch=169
05/17/2022 17:14:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.405943 on epoch=172
05/17/2022 17:14:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.429251 on epoch=174
05/17/2022 17:14:16 - INFO - __main__ - Global step 700 Train loss 0.540835 Classification-F1 0.6713629507747154 on epoch=174
05/17/2022 17:14:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.391248 on epoch=177
05/17/2022 17:14:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.375243 on epoch=179
05/17/2022 17:14:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.397821 on epoch=182
05/17/2022 17:14:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.525810 on epoch=184
05/17/2022 17:14:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.432974 on epoch=187
05/17/2022 17:14:29 - INFO - __main__ - Global step 750 Train loss 0.424619 Classification-F1 0.7263194444444444 on epoch=187
05/17/2022 17:14:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.277049 on epoch=189
05/17/2022 17:14:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.302545 on epoch=192
05/17/2022 17:14:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.442522 on epoch=194
05/17/2022 17:14:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.395417 on epoch=197
05/17/2022 17:14:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.411229 on epoch=199
05/17/2022 17:14:42 - INFO - __main__ - Global step 800 Train loss 0.365752 Classification-F1 0.7412443693693693 on epoch=199
05/17/2022 17:14:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.391852 on epoch=202
05/17/2022 17:14:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.366756 on epoch=204
05/17/2022 17:14:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.325464 on epoch=207
05/17/2022 17:14:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.316992 on epoch=209
05/17/2022 17:14:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.244030 on epoch=212
05/17/2022 17:14:55 - INFO - __main__ - Global step 850 Train loss 0.329019 Classification-F1 0.7270865583075335 on epoch=212
05/17/2022 17:14:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.541865 on epoch=214
05/17/2022 17:15:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.521105 on epoch=217
05/17/2022 17:15:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.384777 on epoch=219
05/17/2022 17:15:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.651712 on epoch=222
05/17/2022 17:15:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.782868 on epoch=224
05/17/2022 17:15:08 - INFO - __main__ - Global step 900 Train loss 0.576465 Classification-F1 0.7094384707287933 on epoch=224
05/17/2022 17:15:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.604878 on epoch=227
05/17/2022 17:15:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.666953 on epoch=229
05/17/2022 17:15:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.294143 on epoch=232
05/17/2022 17:15:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.399804 on epoch=234
05/17/2022 17:15:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.262428 on epoch=237
05/17/2022 17:15:21 - INFO - __main__ - Global step 950 Train loss 0.445641 Classification-F1 0.7769100743745773 on epoch=237
05/17/2022 17:15:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.475954 on epoch=239
05/17/2022 17:15:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.410005 on epoch=242
05/17/2022 17:15:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.355972 on epoch=244
05/17/2022 17:15:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.269908 on epoch=247
05/17/2022 17:15:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.309281 on epoch=249
05/17/2022 17:15:34 - INFO - __main__ - Global step 1000 Train loss 0.364224 Classification-F1 0.7743347338935574 on epoch=249
05/17/2022 17:15:34 - INFO - __main__ - save last model!
05/17/2022 17:15:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 17:15:35 - INFO - __main__ - Printing 3 examples
05/17/2022 17:15:35 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/17/2022 17:15:35 - INFO - __main__ - ['others']
05/17/2022 17:15:35 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/17/2022 17:15:35 - INFO - __main__ - ['others']
05/17/2022 17:15:35 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/17/2022 17:15:35 - INFO - __main__ - ['others']
05/17/2022 17:15:35 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:15:35 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:15:35 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 17:15:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 17:15:35 - INFO - __main__ - Printing 3 examples
05/17/2022 17:15:35 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/17/2022 17:15:35 - INFO - __main__ - ['others']
05/17/2022 17:15:35 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/17/2022 17:15:35 - INFO - __main__ - ['others']
05/17/2022 17:15:35 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/17/2022 17:15:35 - INFO - __main__ - ['others']
05/17/2022 17:15:35 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:15:35 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:15:35 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 17:15:37 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 17:15:37 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 17:15:37 - INFO - __main__ - Printing 3 examples
05/17/2022 17:15:37 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 17:15:37 - INFO - __main__ - ['others']
05/17/2022 17:15:37 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 17:15:37 - INFO - __main__ - ['others']
05/17/2022 17:15:37 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 17:15:37 - INFO - __main__ - ['others']
05/17/2022 17:15:37 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:15:39 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:15:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 17:15:39 - INFO - __main__ - Starting training!
05/17/2022 17:15:44 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 17:16:13 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_87_0.0002_8_predictions.txt
05/17/2022 17:16:13 - INFO - __main__ - Classification-F1 on test data: 0.5334
05/17/2022 17:16:13 - INFO - __main__ - prefix=emo_16_87, lr=0.0002, bsz=8, dev_performance=0.7769100743745773, test_performance=0.5334462112668948
05/17/2022 17:16:13 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.0001, bsz=8 ...
05/17/2022 17:16:14 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 17:16:14 - INFO - __main__ - Printing 3 examples
05/17/2022 17:16:14 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/17/2022 17:16:14 - INFO - __main__ - ['others']
05/17/2022 17:16:14 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/17/2022 17:16:14 - INFO - __main__ - ['others']
05/17/2022 17:16:14 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/17/2022 17:16:14 - INFO - __main__ - ['others']
05/17/2022 17:16:14 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:16:14 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:16:14 - INFO - __main__ - Loaded 64 examples from train data
05/17/2022 17:16:14 - INFO - __main__ - Start tokenizing ... 64 instances
05/17/2022 17:16:14 - INFO - __main__ - Printing 3 examples
05/17/2022 17:16:14 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/17/2022 17:16:14 - INFO - __main__ - ['others']
05/17/2022 17:16:14 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/17/2022 17:16:14 - INFO - __main__ - ['others']
05/17/2022 17:16:14 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/17/2022 17:16:14 - INFO - __main__ - ['others']
05/17/2022 17:16:14 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:16:14 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:16:14 - INFO - __main__ - Loaded 64 examples from dev data
05/17/2022 17:16:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 17:16:18 - INFO - __main__ - Starting training!
05/17/2022 17:16:21 - INFO - __main__ - Step 10 Global step 10 Train loss 19.387754 on epoch=2
05/17/2022 17:16:24 - INFO - __main__ - Step 20 Global step 20 Train loss 20.286346 on epoch=4
05/17/2022 17:16:26 - INFO - __main__ - Step 30 Global step 30 Train loss 16.444752 on epoch=7
05/17/2022 17:16:29 - INFO - __main__ - Step 40 Global step 40 Train loss 13.456749 on epoch=9
05/17/2022 17:16:31 - INFO - __main__ - Step 50 Global step 50 Train loss 11.632437 on epoch=12
05/17/2022 17:16:44 - INFO - __main__ - Global step 50 Train loss 16.241610 Classification-F1 0.0 on epoch=12
05/17/2022 17:16:46 - INFO - __main__ - Step 60 Global step 60 Train loss 10.621569 on epoch=14
05/17/2022 17:16:49 - INFO - __main__ - Step 70 Global step 70 Train loss 8.574507 on epoch=17
05/17/2022 17:16:51 - INFO - __main__ - Step 80 Global step 80 Train loss 8.394670 on epoch=19
05/17/2022 17:16:54 - INFO - __main__ - Step 90 Global step 90 Train loss 7.822206 on epoch=22
05/17/2022 17:16:56 - INFO - __main__ - Step 100 Global step 100 Train loss 6.169219 on epoch=24
05/17/2022 17:17:03 - INFO - __main__ - Global step 100 Train loss 8.316434 Classification-F1 0.0 on epoch=24
05/17/2022 17:17:05 - INFO - __main__ - Step 110 Global step 110 Train loss 6.878873 on epoch=27
05/17/2022 17:17:08 - INFO - __main__ - Step 120 Global step 120 Train loss 6.439587 on epoch=29
05/17/2022 17:17:10 - INFO - __main__ - Step 130 Global step 130 Train loss 5.196949 on epoch=32
05/17/2022 17:17:13 - INFO - __main__ - Step 140 Global step 140 Train loss 6.091989 on epoch=34
05/17/2022 17:17:15 - INFO - __main__ - Step 150 Global step 150 Train loss 5.321081 on epoch=37
05/17/2022 17:17:18 - INFO - __main__ - Global step 150 Train loss 5.985696 Classification-F1 0.052631578947368425 on epoch=37
05/17/2022 17:17:21 - INFO - __main__ - Step 160 Global step 160 Train loss 5.910367 on epoch=39
05/17/2022 17:17:23 - INFO - __main__ - Step 170 Global step 170 Train loss 5.454767 on epoch=42
05/17/2022 17:17:26 - INFO - __main__ - Step 180 Global step 180 Train loss 3.738188 on epoch=44
05/17/2022 17:17:29 - INFO - __main__ - Step 190 Global step 190 Train loss 4.136756 on epoch=47
05/17/2022 17:17:31 - INFO - __main__ - Step 200 Global step 200 Train loss 4.208047 on epoch=49
05/17/2022 17:17:32 - INFO - __main__ - Global step 200 Train loss 4.689625 Classification-F1 0.1 on epoch=49
05/17/2022 17:17:35 - INFO - __main__ - Step 210 Global step 210 Train loss 3.826185 on epoch=52
05/17/2022 17:17:37 - INFO - __main__ - Step 220 Global step 220 Train loss 3.897552 on epoch=54
05/17/2022 17:17:40 - INFO - __main__ - Step 230 Global step 230 Train loss 2.804317 on epoch=57
05/17/2022 17:17:42 - INFO - __main__ - Step 240 Global step 240 Train loss 3.068241 on epoch=59
05/17/2022 17:17:45 - INFO - __main__ - Step 250 Global step 250 Train loss 2.532011 on epoch=62
05/17/2022 17:17:45 - INFO - __main__ - Global step 250 Train loss 3.225661 Classification-F1 0.10126582278481013 on epoch=62
05/17/2022 17:17:48 - INFO - __main__ - Step 260 Global step 260 Train loss 2.705453 on epoch=64
05/17/2022 17:17:51 - INFO - __main__ - Step 270 Global step 270 Train loss 2.535949 on epoch=67
05/17/2022 17:17:53 - INFO - __main__ - Step 280 Global step 280 Train loss 3.154717 on epoch=69
05/17/2022 17:17:56 - INFO - __main__ - Step 290 Global step 290 Train loss 2.501947 on epoch=72
05/17/2022 17:17:58 - INFO - __main__ - Step 300 Global step 300 Train loss 2.874803 on epoch=74
05/17/2022 17:17:59 - INFO - __main__ - Global step 300 Train loss 2.754574 Classification-F1 0.28476972184222527 on epoch=74
05/17/2022 17:18:02 - INFO - __main__ - Step 310 Global step 310 Train loss 2.246427 on epoch=77
05/17/2022 17:18:04 - INFO - __main__ - Step 320 Global step 320 Train loss 2.602578 on epoch=79
05/17/2022 17:18:07 - INFO - __main__ - Step 330 Global step 330 Train loss 2.766542 on epoch=82
05/17/2022 17:18:09 - INFO - __main__ - Step 340 Global step 340 Train loss 2.146891 on epoch=84
05/17/2022 17:18:12 - INFO - __main__ - Step 350 Global step 350 Train loss 2.729198 on epoch=87
05/17/2022 17:18:12 - INFO - __main__ - Global step 350 Train loss 2.498327 Classification-F1 0.3968818322401749 on epoch=87
05/17/2022 17:18:15 - INFO - __main__ - Step 360 Global step 360 Train loss 2.299375 on epoch=89
05/17/2022 17:18:17 - INFO - __main__ - Step 370 Global step 370 Train loss 2.111692 on epoch=92
05/17/2022 17:18:20 - INFO - __main__ - Step 380 Global step 380 Train loss 2.767159 on epoch=94
05/17/2022 17:18:22 - INFO - __main__ - Step 390 Global step 390 Train loss 2.433788 on epoch=97
05/17/2022 17:18:25 - INFO - __main__ - Step 400 Global step 400 Train loss 1.889946 on epoch=99
05/17/2022 17:18:25 - INFO - __main__ - Global step 400 Train loss 2.300392 Classification-F1 0.3220649895178197 on epoch=99
05/17/2022 17:18:28 - INFO - __main__ - Step 410 Global step 410 Train loss 2.168242 on epoch=102
05/17/2022 17:18:30 - INFO - __main__ - Step 420 Global step 420 Train loss 2.078489 on epoch=104
05/17/2022 17:18:33 - INFO - __main__ - Step 430 Global step 430 Train loss 2.115144 on epoch=107
05/17/2022 17:18:35 - INFO - __main__ - Step 440 Global step 440 Train loss 1.952581 on epoch=109
05/17/2022 17:18:38 - INFO - __main__ - Step 450 Global step 450 Train loss 1.939342 on epoch=112
05/17/2022 17:18:38 - INFO - __main__ - Global step 450 Train loss 2.050760 Classification-F1 0.35034013605442177 on epoch=112
05/17/2022 17:18:41 - INFO - __main__ - Step 460 Global step 460 Train loss 1.641681 on epoch=114
05/17/2022 17:18:43 - INFO - __main__ - Step 470 Global step 470 Train loss 1.800286 on epoch=117
05/17/2022 17:18:46 - INFO - __main__ - Step 480 Global step 480 Train loss 1.889125 on epoch=119
05/17/2022 17:18:48 - INFO - __main__ - Step 490 Global step 490 Train loss 1.586793 on epoch=122
05/17/2022 17:18:51 - INFO - __main__ - Step 500 Global step 500 Train loss 2.065584 on epoch=124
05/17/2022 17:18:51 - INFO - __main__ - Global step 500 Train loss 1.796694 Classification-F1 0.33979229989868287 on epoch=124
05/17/2022 17:18:54 - INFO - __main__ - Step 510 Global step 510 Train loss 1.867655 on epoch=127
05/17/2022 17:18:56 - INFO - __main__ - Step 520 Global step 520 Train loss 1.550464 on epoch=129
05/17/2022 17:18:59 - INFO - __main__ - Step 530 Global step 530 Train loss 1.617828 on epoch=132
05/17/2022 17:19:01 - INFO - __main__ - Step 540 Global step 540 Train loss 1.371469 on epoch=134
05/17/2022 17:19:04 - INFO - __main__ - Step 550 Global step 550 Train loss 2.102063 on epoch=137
05/17/2022 17:19:04 - INFO - __main__ - Global step 550 Train loss 1.701896 Classification-F1 0.44248441182108267 on epoch=137
05/17/2022 17:19:07 - INFO - __main__ - Step 560 Global step 560 Train loss 1.459978 on epoch=139
05/17/2022 17:19:09 - INFO - __main__ - Step 570 Global step 570 Train loss 1.640768 on epoch=142
05/17/2022 17:19:12 - INFO - __main__ - Step 580 Global step 580 Train loss 1.235617 on epoch=144
05/17/2022 17:19:14 - INFO - __main__ - Step 590 Global step 590 Train loss 1.377176 on epoch=147
05/17/2022 17:19:17 - INFO - __main__ - Step 600 Global step 600 Train loss 1.173108 on epoch=149
05/17/2022 17:19:17 - INFO - __main__ - Global step 600 Train loss 1.377330 Classification-F1 0.43677181580407387 on epoch=149
05/17/2022 17:19:20 - INFO - __main__ - Step 610 Global step 610 Train loss 1.299426 on epoch=152
05/17/2022 17:19:22 - INFO - __main__ - Step 620 Global step 620 Train loss 1.829477 on epoch=154
05/17/2022 17:19:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.979989 on epoch=157
05/17/2022 17:19:27 - INFO - __main__ - Step 640 Global step 640 Train loss 1.082058 on epoch=159
05/17/2022 17:19:30 - INFO - __main__ - Step 650 Global step 650 Train loss 1.404869 on epoch=162
05/17/2022 17:19:30 - INFO - __main__ - Global step 650 Train loss 1.319164 Classification-F1 0.4625 on epoch=162
05/17/2022 17:19:33 - INFO - __main__ - Step 660 Global step 660 Train loss 1.108610 on epoch=164
05/17/2022 17:19:35 - INFO - __main__ - Step 670 Global step 670 Train loss 1.769799 on epoch=167
05/17/2022 17:19:38 - INFO - __main__ - Step 680 Global step 680 Train loss 1.178465 on epoch=169
05/17/2022 17:19:40 - INFO - __main__ - Step 690 Global step 690 Train loss 1.096216 on epoch=172
05/17/2022 17:19:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.950878 on epoch=174
05/17/2022 17:19:43 - INFO - __main__ - Global step 700 Train loss 1.220793 Classification-F1 0.44167187825724413 on epoch=174
05/17/2022 17:19:46 - INFO - __main__ - Step 710 Global step 710 Train loss 1.037631 on epoch=177
05/17/2022 17:19:48 - INFO - __main__ - Step 720 Global step 720 Train loss 1.495732 on epoch=179
05/17/2022 17:19:51 - INFO - __main__ - Step 730 Global step 730 Train loss 1.126567 on epoch=182
05/17/2022 17:19:53 - INFO - __main__ - Step 740 Global step 740 Train loss 1.311848 on epoch=184
05/17/2022 17:19:56 - INFO - __main__ - Step 750 Global step 750 Train loss 1.387041 on epoch=187
05/17/2022 17:19:56 - INFO - __main__ - Global step 750 Train loss 1.271764 Classification-F1 0.4805382305382305 on epoch=187
05/17/2022 17:19:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.893157 on epoch=189
05/17/2022 17:20:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.996687 on epoch=192
05/17/2022 17:20:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.988108 on epoch=194
05/17/2022 17:20:07 - INFO - __main__ - Step 790 Global step 790 Train loss 1.135987 on epoch=197
05/17/2022 17:20:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.979054 on epoch=199
05/17/2022 17:20:09 - INFO - __main__ - Global step 800 Train loss 0.998599 Classification-F1 0.48690476190476195 on epoch=199
05/17/2022 17:20:12 - INFO - __main__ - Step 810 Global step 810 Train loss 1.244316 on epoch=202
05/17/2022 17:20:15 - INFO - __main__ - Step 820 Global step 820 Train loss 1.046694 on epoch=204
05/17/2022 17:20:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.909466 on epoch=207
05/17/2022 17:20:20 - INFO - __main__ - Step 840 Global step 840 Train loss 1.084273 on epoch=209
05/17/2022 17:20:22 - INFO - __main__ - Step 850 Global step 850 Train loss 1.034420 on epoch=212
05/17/2022 17:20:23 - INFO - __main__ - Global step 850 Train loss 1.063834 Classification-F1 0.4701082598235766 on epoch=212
05/17/2022 17:20:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.824809 on epoch=214
05/17/2022 17:20:28 - INFO - __main__ - Step 870 Global step 870 Train loss 1.019218 on epoch=217
05/17/2022 17:20:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.772072 on epoch=219
05/17/2022 17:20:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.895025 on epoch=222
05/17/2022 17:20:35 - INFO - __main__ - Step 900 Global step 900 Train loss 1.056899 on epoch=224
05/17/2022 17:20:35 - INFO - __main__ - Global step 900 Train loss 0.913605 Classification-F1 0.46941678520625885 on epoch=224
05/17/2022 17:20:38 - INFO - __main__ - Step 910 Global step 910 Train loss 1.094845 on epoch=227
05/17/2022 17:20:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.874569 on epoch=229
05/17/2022 17:20:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.876580 on epoch=232
05/17/2022 17:20:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.645046 on epoch=234
05/17/2022 17:20:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.993797 on epoch=237
05/17/2022 17:20:48 - INFO - __main__ - Global step 950 Train loss 0.896968 Classification-F1 0.4950772200772201 on epoch=237
05/17/2022 17:20:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.943350 on epoch=239
05/17/2022 17:20:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.982511 on epoch=242
05/17/2022 17:20:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.727781 on epoch=244
05/17/2022 17:21:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.947518 on epoch=247
05/17/2022 17:21:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.157110 on epoch=249
05/17/2022 17:21:03 - INFO - __main__ - Global step 1000 Train loss 0.951654 Classification-F1 0.49346620652216067 on epoch=249
05/17/2022 17:21:03 - INFO - __main__ - save last model!
05/17/2022 17:21:05 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 17:21:05 - INFO - __main__ - Start tokenizing ... 5509 instances
05/17/2022 17:21:05 - INFO - __main__ - Printing 3 examples
05/17/2022 17:21:05 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/17/2022 17:21:05 - INFO - __main__ - ['others']
05/17/2022 17:21:05 - INFO - __main__ -  [emo] what you like very little things ok
05/17/2022 17:21:05 - INFO - __main__ - ['others']
05/17/2022 17:21:05 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/17/2022 17:21:05 - INFO - __main__ - ['others']
05/17/2022 17:21:05 - INFO - __main__ - Tokenizing Input ...
05/17/2022 17:21:08 - INFO - __main__ - Tokenizing Output ...
05/17/2022 17:21:13 - INFO - __main__ - Loaded 5509 examples from test data
05/17/2022 17:21:42 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_87_0.0001_8_predictions.txt
05/17/2022 17:21:42 - INFO - __main__ - Classification-F1 on test data: 0.1590
05/17/2022 17:21:42 - INFO - __main__ - prefix=emo_16_87, lr=0.0001, bsz=8, dev_performance=0.4950772200772201, test_performance=0.15903312063988745
05/21/2022 12:52:53 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-base-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-cls2cls/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-base', cuda='2,3')
05/21/2022 12:52:53 - INFO - __main__ - models/T5-base-ft-cls2cls/singletask-emo
05/21/2022 12:52:54 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-base-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-cls2cls/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-base', cuda='2,3')
05/21/2022 12:52:54 - INFO - __main__ - models/T5-base-ft-cls2cls/singletask-emo
05/21/2022 12:52:55 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 12:52:55 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 12:52:55 - INFO - __main__ - args.device: cuda:0
05/21/2022 12:52:55 - INFO - __main__ - Using 2 gpus
05/21/2022 12:52:55 - INFO - __main__ - args.device: cuda:1
05/21/2022 12:52:55 - INFO - __main__ - Using 2 gpus
05/21/2022 12:52:55 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/21/2022 12:52:55 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/21/2022 12:53:01 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.0005, bsz=8 ...
05/21/2022 12:53:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 12:53:01 - INFO - __main__ - Printing 3 examples
05/21/2022 12:53:01 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 12:53:01 - INFO - __main__ - ['others']
05/21/2022 12:53:01 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 12:53:01 - INFO - __main__ - ['others']
05/21/2022 12:53:01 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 12:53:01 - INFO - __main__ - ['others']
05/21/2022 12:53:01 - INFO - __main__ - Tokenizing Input ...
05/21/2022 12:53:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 12:53:01 - INFO - __main__ - Printing 3 examples
05/21/2022 12:53:01 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 12:53:01 - INFO - __main__ - ['others']
05/21/2022 12:53:01 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 12:53:01 - INFO - __main__ - ['others']
05/21/2022 12:53:01 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 12:53:01 - INFO - __main__ - ['others']
05/21/2022 12:53:01 - INFO - __main__ - Tokenizing Input ...
05/21/2022 12:53:02 - INFO - __main__ - Tokenizing Output ...
05/21/2022 12:53:02 - INFO - __main__ - Tokenizing Output ...
05/21/2022 12:53:02 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 12:53:02 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 12:53:02 - INFO - __main__ - Printing 3 examples
05/21/2022 12:53:02 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 12:53:02 - INFO - __main__ - ['others']
05/21/2022 12:53:02 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 12:53:02 - INFO - __main__ - ['others']
05/21/2022 12:53:02 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 12:53:02 - INFO - __main__ - ['others']
05/21/2022 12:53:02 - INFO - __main__ - Tokenizing Input ...
05/21/2022 12:53:02 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 12:53:02 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 12:53:02 - INFO - __main__ - Printing 3 examples
05/21/2022 12:53:02 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 12:53:02 - INFO - __main__ - ['others']
05/21/2022 12:53:02 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 12:53:02 - INFO - __main__ - ['others']
05/21/2022 12:53:02 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 12:53:02 - INFO - __main__ - ['others']
05/21/2022 12:53:02 - INFO - __main__ - Tokenizing Input ...
05/21/2022 12:53:02 - INFO - __main__ - Tokenizing Output ...
05/21/2022 12:53:02 - INFO - __main__ - Tokenizing Output ...
05/21/2022 12:53:02 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 12:53:02 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 12:53:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 12:53:06 - INFO - __main__ - Starting training!
05/21/2022 12:53:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 12:53:06 - INFO - __main__ - Starting training!
05/21/2022 12:53:09 - INFO - __main__ - Step 10 Global step 10 Train loss 20.773205 on epoch=2
05/21/2022 12:53:11 - INFO - __main__ - Step 20 Global step 20 Train loss 17.638365 on epoch=4
05/21/2022 12:53:14 - INFO - __main__ - Step 30 Global step 30 Train loss 8.968684 on epoch=7
05/21/2022 12:53:17 - INFO - __main__ - Step 40 Global step 40 Train loss 7.063629 on epoch=9
05/21/2022 12:53:20 - INFO - __main__ - Step 50 Global step 50 Train loss 5.807273 on epoch=12
05/21/2022 12:53:20 - INFO - __main__ - Global step 50 Train loss 12.050232 Classification-F1 0.10126582278481013 on epoch=12
05/21/2022 12:53:23 - INFO - __main__ - Step 60 Global step 60 Train loss 4.096446 on epoch=14
05/21/2022 12:53:26 - INFO - __main__ - Step 70 Global step 70 Train loss 2.909000 on epoch=17
05/21/2022 12:53:29 - INFO - __main__ - Step 80 Global step 80 Train loss 2.786402 on epoch=19
05/21/2022 12:53:32 - INFO - __main__ - Step 90 Global step 90 Train loss 2.467590 on epoch=22
05/21/2022 12:53:35 - INFO - __main__ - Step 100 Global step 100 Train loss 2.923608 on epoch=24
05/21/2022 12:53:35 - INFO - __main__ - Global step 100 Train loss 3.036609 Classification-F1 0.3689679158334058 on epoch=24
05/21/2022 12:53:39 - INFO - __main__ - Step 110 Global step 110 Train loss 2.543712 on epoch=27
05/21/2022 12:53:41 - INFO - __main__ - Step 120 Global step 120 Train loss 2.142263 on epoch=29
05/21/2022 12:53:44 - INFO - __main__ - Step 130 Global step 130 Train loss 2.037886 on epoch=32
05/21/2022 12:53:47 - INFO - __main__ - Step 140 Global step 140 Train loss 1.990813 on epoch=34
05/21/2022 12:53:50 - INFO - __main__ - Step 150 Global step 150 Train loss 1.948400 on epoch=37
05/21/2022 12:53:50 - INFO - __main__ - Global step 150 Train loss 2.132615 Classification-F1 0.26200351935646055 on epoch=37
05/21/2022 12:53:53 - INFO - __main__ - Step 160 Global step 160 Train loss 1.599749 on epoch=39
05/21/2022 12:53:56 - INFO - __main__ - Step 170 Global step 170 Train loss 1.579688 on epoch=42
05/21/2022 12:53:58 - INFO - __main__ - Step 180 Global step 180 Train loss 1.146824 on epoch=44
05/21/2022 12:54:01 - INFO - __main__ - Step 190 Global step 190 Train loss 1.371812 on epoch=47
05/21/2022 12:54:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.952161 on epoch=49
05/21/2022 12:54:04 - INFO - __main__ - Global step 200 Train loss 1.330047 Classification-F1 0.4088120254620753 on epoch=49
05/21/2022 12:54:07 - INFO - __main__ - Step 210 Global step 210 Train loss 1.576630 on epoch=52
05/21/2022 12:54:11 - INFO - __main__ - Step 220 Global step 220 Train loss 1.295952 on epoch=54
05/21/2022 12:54:13 - INFO - __main__ - Step 230 Global step 230 Train loss 1.614467 on epoch=57
05/21/2022 12:54:16 - INFO - __main__ - Step 240 Global step 240 Train loss 1.324346 on epoch=59
05/21/2022 12:54:19 - INFO - __main__ - Step 250 Global step 250 Train loss 1.188974 on epoch=62
05/21/2022 12:54:20 - INFO - __main__ - Global step 250 Train loss 1.400074 Classification-F1 0.42345938375350145 on epoch=62
05/21/2022 12:54:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.892118 on epoch=64
05/21/2022 12:54:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.935370 on epoch=67
05/21/2022 12:54:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.600462 on epoch=69
05/21/2022 12:54:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.968125 on epoch=72
05/21/2022 12:54:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.957228 on epoch=74
05/21/2022 12:54:34 - INFO - __main__ - Global step 300 Train loss 0.870661 Classification-F1 0.43804758459930876 on epoch=74
05/21/2022 12:54:37 - INFO - __main__ - Step 310 Global step 310 Train loss 1.025436 on epoch=77
05/21/2022 12:54:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.944736 on epoch=79
05/21/2022 12:54:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.881859 on epoch=82
05/21/2022 12:54:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.787918 on epoch=84
05/21/2022 12:54:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.643052 on epoch=87
05/21/2022 12:54:49 - INFO - __main__ - Global step 350 Train loss 0.856600 Classification-F1 0.6720238095238095 on epoch=87
05/21/2022 12:54:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.803563 on epoch=89
05/21/2022 12:54:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.987229 on epoch=92
05/21/2022 12:54:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.656693 on epoch=94
05/21/2022 12:55:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.836231 on epoch=97
05/21/2022 12:55:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.821148 on epoch=99
05/21/2022 12:55:03 - INFO - __main__ - Global step 400 Train loss 0.820973 Classification-F1 0.4869431643625192 on epoch=99
05/21/2022 12:55:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.741621 on epoch=102
05/21/2022 12:55:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.867610 on epoch=104
05/21/2022 12:55:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.940896 on epoch=107
05/21/2022 12:55:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.879731 on epoch=109
05/21/2022 12:55:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.918757 on epoch=112
05/21/2022 12:55:17 - INFO - __main__ - Global step 450 Train loss 0.869723 Classification-F1 0.4710115088163868 on epoch=112
05/21/2022 12:55:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.855686 on epoch=114
05/21/2022 12:55:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.871980 on epoch=117
05/21/2022 12:55:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.848426 on epoch=119
05/21/2022 12:55:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.839291 on epoch=122
05/21/2022 12:55:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.897363 on epoch=124
05/21/2022 12:55:32 - INFO - __main__ - Global step 500 Train loss 0.862549 Classification-F1 0.3439950980392157 on epoch=124
05/21/2022 12:55:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.940459 on epoch=127
05/21/2022 12:55:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.902441 on epoch=129
05/21/2022 12:55:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.950671 on epoch=132
05/21/2022 12:55:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.817526 on epoch=134
05/21/2022 12:55:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.871708 on epoch=137
05/21/2022 12:55:46 - INFO - __main__ - Global step 550 Train loss 0.896561 Classification-F1 0.3924117513740155 on epoch=137
05/21/2022 12:55:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.858088 on epoch=139
05/21/2022 12:55:51 - INFO - __main__ - Step 570 Global step 570 Train loss 1.084537 on epoch=142
05/21/2022 12:55:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.944616 on epoch=144
05/21/2022 12:55:57 - INFO - __main__ - Step 590 Global step 590 Train loss 1.011671 on epoch=147
05/21/2022 12:55:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.839114 on epoch=149
05/21/2022 12:56:00 - INFO - __main__ - Global step 600 Train loss 0.947605 Classification-F1 0.353828197945845 on epoch=149
05/21/2022 12:56:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.813112 on epoch=152
05/21/2022 12:56:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.891355 on epoch=154
05/21/2022 12:56:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.703770 on epoch=157
05/21/2022 12:56:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.815002 on epoch=159
05/21/2022 12:56:14 - INFO - __main__ - Step 650 Global step 650 Train loss 1.124581 on epoch=162
05/21/2022 12:56:14 - INFO - __main__ - Global step 650 Train loss 0.869564 Classification-F1 0.4647783251231527 on epoch=162
05/21/2022 12:56:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.867812 on epoch=164
05/21/2022 12:56:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.825773 on epoch=167
05/21/2022 12:56:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.857322 on epoch=169
05/21/2022 12:56:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.785881 on epoch=172
05/21/2022 12:56:28 - INFO - __main__ - Step 700 Global step 700 Train loss 1.135136 on epoch=174
05/21/2022 12:56:28 - INFO - __main__ - Global step 700 Train loss 0.894385 Classification-F1 0.2758714596949891 on epoch=174
05/21/2022 12:56:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.831414 on epoch=177
05/21/2022 12:56:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.847032 on epoch=179
05/21/2022 12:56:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.781777 on epoch=182
05/21/2022 12:56:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.863185 on epoch=184
05/21/2022 12:56:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.872231 on epoch=187
05/21/2022 12:56:43 - INFO - __main__ - Global step 750 Train loss 0.839128 Classification-F1 0.46234633153237803 on epoch=187
05/21/2022 12:56:46 - INFO - __main__ - Step 760 Global step 760 Train loss 1.954647 on epoch=189
05/21/2022 12:56:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.903745 on epoch=192
05/21/2022 12:56:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.642732 on epoch=194
05/21/2022 12:56:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.644615 on epoch=197
05/21/2022 12:56:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.578815 on epoch=199
05/21/2022 12:56:59 - INFO - __main__ - Global step 800 Train loss 0.944911 Classification-F1 0.43039215686274507 on epoch=199
05/21/2022 12:57:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.727308 on epoch=202
05/21/2022 12:57:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.782233 on epoch=204
05/21/2022 12:57:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.958131 on epoch=207
05/21/2022 12:57:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.690746 on epoch=209
05/21/2022 12:57:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.659354 on epoch=212
05/21/2022 12:57:14 - INFO - __main__ - Global step 850 Train loss 0.763554 Classification-F1 0.4620394528779253 on epoch=212
05/21/2022 12:57:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.696317 on epoch=214
05/21/2022 12:57:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.703953 on epoch=217
05/21/2022 12:57:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.764919 on epoch=219
05/21/2022 12:57:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.826507 on epoch=222
05/21/2022 12:57:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.678240 on epoch=224
05/21/2022 12:57:28 - INFO - __main__ - Global step 900 Train loss 0.733987 Classification-F1 0.3516852770885029 on epoch=224
05/21/2022 12:57:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.747932 on epoch=227
05/21/2022 12:57:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.770914 on epoch=229
05/21/2022 12:57:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.786116 on epoch=232
05/21/2022 12:57:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.683630 on epoch=234
05/21/2022 12:57:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.736343 on epoch=237
05/21/2022 12:57:42 - INFO - __main__ - Global step 950 Train loss 0.744987 Classification-F1 0.4370494417862839 on epoch=237
05/21/2022 12:57:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.566048 on epoch=239
05/21/2022 12:57:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.702408 on epoch=242
05/21/2022 12:57:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.677222 on epoch=244
05/21/2022 12:57:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.635359 on epoch=247
05/21/2022 12:57:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.692134 on epoch=249
05/21/2022 12:57:57 - INFO - __main__ - Global step 1000 Train loss 0.654634 Classification-F1 0.4626018170426065 on epoch=249
05/21/2022 12:57:57 - INFO - __main__ - save last model!
05/21/2022 12:57:58 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 12:57:58 - INFO - __main__ - Printing 3 examples
05/21/2022 12:57:58 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 12:57:58 - INFO - __main__ - ['others']
05/21/2022 12:57:58 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 12:57:58 - INFO - __main__ - ['others']
05/21/2022 12:57:58 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 12:57:58 - INFO - __main__ - ['others']
05/21/2022 12:57:58 - INFO - __main__ - Tokenizing Input ...
05/21/2022 12:57:58 - INFO - __main__ - Tokenizing Output ...
05/21/2022 12:57:58 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 12:57:58 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 12:57:58 - INFO - __main__ - Printing 3 examples
05/21/2022 12:57:58 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 12:57:58 - INFO - __main__ - ['others']
05/21/2022 12:57:58 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 12:57:58 - INFO - __main__ - ['others']
05/21/2022 12:57:58 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 12:57:58 - INFO - __main__ - ['others']
05/21/2022 12:57:58 - INFO - __main__ - Tokenizing Input ...
05/21/2022 12:57:58 - INFO - __main__ - Tokenizing Output ...
05/21/2022 12:57:58 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 12:58:01 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 12:58:01 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 12:58:01 - INFO - __main__ - Printing 3 examples
05/21/2022 12:58:01 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 12:58:01 - INFO - __main__ - ['others']
05/21/2022 12:58:01 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 12:58:01 - INFO - __main__ - ['others']
05/21/2022 12:58:01 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 12:58:01 - INFO - __main__ - ['others']
05/21/2022 12:58:01 - INFO - __main__ - Tokenizing Input ...
05/21/2022 12:58:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 12:58:04 - INFO - __main__ - Starting training!
05/21/2022 12:58:05 - INFO - __main__ - Tokenizing Output ...
05/21/2022 12:58:12 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 12:58:41 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_100_0.0005_8_predictions.txt
05/21/2022 12:58:42 - INFO - __main__ - Classification-F1 on test data: 0.3672
05/21/2022 12:58:42 - INFO - __main__ - prefix=emo_16_100, lr=0.0005, bsz=8, dev_performance=0.6720238095238095, test_performance=0.3672400477796371
05/21/2022 12:58:42 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.0003, bsz=8 ...
05/21/2022 12:58:43 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 12:58:43 - INFO - __main__ - Printing 3 examples
05/21/2022 12:58:43 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 12:58:43 - INFO - __main__ - ['others']
05/21/2022 12:58:43 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 12:58:43 - INFO - __main__ - ['others']
05/21/2022 12:58:43 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 12:58:43 - INFO - __main__ - ['others']
05/21/2022 12:58:43 - INFO - __main__ - Tokenizing Input ...
05/21/2022 12:58:43 - INFO - __main__ - Tokenizing Output ...
05/21/2022 12:58:43 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 12:58:43 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 12:58:43 - INFO - __main__ - Printing 3 examples
05/21/2022 12:58:43 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 12:58:43 - INFO - __main__ - ['others']
05/21/2022 12:58:43 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 12:58:43 - INFO - __main__ - ['others']
05/21/2022 12:58:43 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 12:58:43 - INFO - __main__ - ['others']
05/21/2022 12:58:43 - INFO - __main__ - Tokenizing Input ...
05/21/2022 12:58:43 - INFO - __main__ - Tokenizing Output ...
05/21/2022 12:58:43 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 12:58:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 12:58:48 - INFO - __main__ - Starting training!
05/21/2022 12:58:52 - INFO - __main__ - Step 10 Global step 10 Train loss 20.435167 on epoch=2
05/21/2022 12:58:54 - INFO - __main__ - Step 20 Global step 20 Train loss 16.308514 on epoch=4
05/21/2022 12:58:57 - INFO - __main__ - Step 30 Global step 30 Train loss 11.064670 on epoch=7
05/21/2022 12:58:59 - INFO - __main__ - Step 40 Global step 40 Train loss 8.008345 on epoch=9
05/21/2022 12:59:02 - INFO - __main__ - Step 50 Global step 50 Train loss 7.598494 on epoch=12
05/21/2022 12:59:08 - INFO - __main__ - Global step 50 Train loss 12.683039 Classification-F1 0.024156371012354787 on epoch=12
05/21/2022 12:59:11 - INFO - __main__ - Step 60 Global step 60 Train loss 5.452511 on epoch=14
05/21/2022 12:59:14 - INFO - __main__ - Step 70 Global step 70 Train loss 5.493804 on epoch=17
05/21/2022 12:59:17 - INFO - __main__ - Step 80 Global step 80 Train loss 3.623557 on epoch=19
05/21/2022 12:59:20 - INFO - __main__ - Step 90 Global step 90 Train loss 3.649139 on epoch=22
05/21/2022 12:59:23 - INFO - __main__ - Step 100 Global step 100 Train loss 3.424002 on epoch=24
05/21/2022 12:59:24 - INFO - __main__ - Global step 100 Train loss 4.328603 Classification-F1 0.1 on epoch=24
05/21/2022 12:59:27 - INFO - __main__ - Step 110 Global step 110 Train loss 3.268026 on epoch=27
05/21/2022 12:59:30 - INFO - __main__ - Step 120 Global step 120 Train loss 2.669455 on epoch=29
05/21/2022 12:59:33 - INFO - __main__ - Step 130 Global step 130 Train loss 3.265578 on epoch=32
05/21/2022 12:59:35 - INFO - __main__ - Step 140 Global step 140 Train loss 2.939521 on epoch=34
05/21/2022 12:59:38 - INFO - __main__ - Step 150 Global step 150 Train loss 2.548535 on epoch=37
05/21/2022 12:59:38 - INFO - __main__ - Global step 150 Train loss 2.938223 Classification-F1 0.19549039373814042 on epoch=37
05/21/2022 12:59:41 - INFO - __main__ - Step 160 Global step 160 Train loss 2.276872 on epoch=39
05/21/2022 12:59:45 - INFO - __main__ - Step 170 Global step 170 Train loss 2.465389 on epoch=42
05/21/2022 12:59:47 - INFO - __main__ - Step 180 Global step 180 Train loss 2.216686 on epoch=44
05/21/2022 12:59:50 - INFO - __main__ - Step 190 Global step 190 Train loss 1.934215 on epoch=47
05/21/2022 12:59:53 - INFO - __main__ - Step 200 Global step 200 Train loss 1.993251 on epoch=49
05/21/2022 12:59:53 - INFO - __main__ - Global step 200 Train loss 2.177283 Classification-F1 0.17121826861001552 on epoch=49
05/21/2022 12:59:56 - INFO - __main__ - Step 210 Global step 210 Train loss 1.390764 on epoch=52
05/21/2022 12:59:58 - INFO - __main__ - Step 220 Global step 220 Train loss 1.267628 on epoch=54
05/21/2022 13:00:02 - INFO - __main__ - Step 230 Global step 230 Train loss 1.194098 on epoch=57
05/21/2022 13:00:05 - INFO - __main__ - Step 240 Global step 240 Train loss 1.221617 on epoch=59
05/21/2022 13:00:08 - INFO - __main__ - Step 250 Global step 250 Train loss 1.380869 on epoch=62
05/21/2022 13:00:08 - INFO - __main__ - Global step 250 Train loss 1.290995 Classification-F1 0.3165635131152372 on epoch=62
05/21/2022 13:00:12 - INFO - __main__ - Step 260 Global step 260 Train loss 1.699731 on epoch=64
05/21/2022 13:00:14 - INFO - __main__ - Step 270 Global step 270 Train loss 1.635709 on epoch=67
05/21/2022 13:00:17 - INFO - __main__ - Step 280 Global step 280 Train loss 1.341788 on epoch=69
05/21/2022 13:00:20 - INFO - __main__ - Step 290 Global step 290 Train loss 1.002973 on epoch=72
05/21/2022 13:00:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.938586 on epoch=74
05/21/2022 13:00:23 - INFO - __main__ - Global step 300 Train loss 1.323758 Classification-F1 0.39166666666666666 on epoch=74
05/21/2022 13:00:26 - INFO - __main__ - Step 310 Global step 310 Train loss 1.128104 on epoch=77
05/21/2022 13:00:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.968421 on epoch=79
05/21/2022 13:00:32 - INFO - __main__ - Step 330 Global step 330 Train loss 1.157192 on epoch=82
05/21/2022 13:00:34 - INFO - __main__ - Step 340 Global step 340 Train loss 1.008624 on epoch=84
05/21/2022 13:00:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.971791 on epoch=87
05/21/2022 13:00:37 - INFO - __main__ - Global step 350 Train loss 1.046826 Classification-F1 0.3427725531173807 on epoch=87
05/21/2022 13:00:40 - INFO - __main__ - Step 360 Global step 360 Train loss 1.115648 on epoch=89
05/21/2022 13:00:42 - INFO - __main__ - Step 370 Global step 370 Train loss 1.282645 on epoch=92
05/21/2022 13:00:45 - INFO - __main__ - Step 380 Global step 380 Train loss 1.130568 on epoch=94
05/21/2022 13:00:48 - INFO - __main__ - Step 390 Global step 390 Train loss 1.008206 on epoch=97
05/21/2022 13:00:51 - INFO - __main__ - Step 400 Global step 400 Train loss 1.103175 on epoch=99
05/21/2022 13:00:52 - INFO - __main__ - Global step 400 Train loss 1.128048 Classification-F1 0.26938405797101445 on epoch=99
05/21/2022 13:00:55 - INFO - __main__ - Step 410 Global step 410 Train loss 1.203130 on epoch=102
05/21/2022 13:00:58 - INFO - __main__ - Step 420 Global step 420 Train loss 2.459415 on epoch=104
05/21/2022 13:01:00 - INFO - __main__ - Step 430 Global step 430 Train loss 1.697370 on epoch=107
05/21/2022 13:01:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.958219 on epoch=109
05/21/2022 13:01:06 - INFO - __main__ - Step 450 Global step 450 Train loss 1.103411 on epoch=112
05/21/2022 13:01:06 - INFO - __main__ - Global step 450 Train loss 1.484309 Classification-F1 0.263536409516943 on epoch=112
05/21/2022 13:01:09 - INFO - __main__ - Step 460 Global step 460 Train loss 1.394055 on epoch=114
05/21/2022 13:01:12 - INFO - __main__ - Step 470 Global step 470 Train loss 1.199902 on epoch=117
05/21/2022 13:01:14 - INFO - __main__ - Step 480 Global step 480 Train loss 1.061445 on epoch=119
05/21/2022 13:01:17 - INFO - __main__ - Step 490 Global step 490 Train loss 1.025277 on epoch=122
05/21/2022 13:01:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.949494 on epoch=124
05/21/2022 13:01:20 - INFO - __main__ - Global step 500 Train loss 1.126034 Classification-F1 0.3574660633484163 on epoch=124
05/21/2022 13:01:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.981453 on epoch=127
05/21/2022 13:01:25 - INFO - __main__ - Step 520 Global step 520 Train loss 1.121495 on epoch=129
05/21/2022 13:01:28 - INFO - __main__ - Step 530 Global step 530 Train loss 1.047223 on epoch=132
05/21/2022 13:01:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.905312 on epoch=134
05/21/2022 13:01:34 - INFO - __main__ - Step 550 Global step 550 Train loss 1.179888 on epoch=137
05/21/2022 13:01:34 - INFO - __main__ - Global step 550 Train loss 1.047074 Classification-F1 0.31874891887216744 on epoch=137
05/21/2022 13:01:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.976114 on epoch=139
05/21/2022 13:01:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.988243 on epoch=142
05/21/2022 13:01:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.972224 on epoch=144
05/21/2022 13:01:45 - INFO - __main__ - Step 590 Global step 590 Train loss 1.082803 on epoch=147
05/21/2022 13:01:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.965419 on epoch=149
05/21/2022 13:01:48 - INFO - __main__ - Global step 600 Train loss 0.996960 Classification-F1 0.1 on epoch=149
05/21/2022 13:01:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.977553 on epoch=152
05/21/2022 13:01:53 - INFO - __main__ - Step 620 Global step 620 Train loss 1.073538 on epoch=154
05/21/2022 13:01:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.879903 on epoch=157
05/21/2022 13:01:58 - INFO - __main__ - Step 640 Global step 640 Train loss 1.045228 on epoch=159
05/21/2022 13:02:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.992473 on epoch=162
05/21/2022 13:02:01 - INFO - __main__ - Global step 650 Train loss 0.993739 Classification-F1 0.09493670886075949 on epoch=162
05/21/2022 13:02:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.942459 on epoch=164
05/21/2022 13:02:06 - INFO - __main__ - Step 670 Global step 670 Train loss 1.004328 on epoch=167
05/21/2022 13:02:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.951362 on epoch=169
05/21/2022 13:02:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.883623 on epoch=172
05/21/2022 13:02:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.878224 on epoch=174
05/21/2022 13:02:14 - INFO - __main__ - Global step 700 Train loss 0.931999 Classification-F1 0.1715909090909091 on epoch=174
05/21/2022 13:02:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.928336 on epoch=177
05/21/2022 13:02:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.827314 on epoch=179
05/21/2022 13:02:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.843218 on epoch=182
05/21/2022 13:02:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.767862 on epoch=184
05/21/2022 13:02:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.875381 on epoch=187
05/21/2022 13:02:27 - INFO - __main__ - Global step 750 Train loss 0.848422 Classification-F1 0.13067758749069247 on epoch=187
05/21/2022 13:02:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.862911 on epoch=189
05/21/2022 13:02:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.886455 on epoch=192
05/21/2022 13:02:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.767799 on epoch=194
05/21/2022 13:02:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.780071 on epoch=197
05/21/2022 13:02:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.873814 on epoch=199
05/21/2022 13:02:40 - INFO - __main__ - Global step 800 Train loss 0.834210 Classification-F1 0.18553782810087494 on epoch=199
05/21/2022 13:02:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.828898 on epoch=202
05/21/2022 13:02:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.776028 on epoch=204
05/21/2022 13:02:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.801151 on epoch=207
05/21/2022 13:02:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.794572 on epoch=209
05/21/2022 13:02:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.914632 on epoch=212
05/21/2022 13:02:53 - INFO - __main__ - Global step 850 Train loss 0.823056 Classification-F1 0.22027972027972026 on epoch=212
05/21/2022 13:02:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.833477 on epoch=214
05/21/2022 13:02:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.808599 on epoch=217
05/21/2022 13:03:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.771296 on epoch=219
05/21/2022 13:03:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.852666 on epoch=222
05/21/2022 13:03:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.910130 on epoch=224
05/21/2022 13:03:06 - INFO - __main__ - Global step 900 Train loss 0.835234 Classification-F1 0.306551116333725 on epoch=224
05/21/2022 13:03:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.762564 on epoch=227
05/21/2022 13:03:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.850705 on epoch=229
05/21/2022 13:03:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.698893 on epoch=232
05/21/2022 13:03:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.884143 on epoch=234
05/21/2022 13:03:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.745389 on epoch=237
05/21/2022 13:03:19 - INFO - __main__ - Global step 950 Train loss 0.788339 Classification-F1 0.20984848484848484 on epoch=237
05/21/2022 13:03:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.844979 on epoch=239
05/21/2022 13:03:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.826276 on epoch=242
05/21/2022 13:03:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.835455 on epoch=244
05/21/2022 13:03:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.768502 on epoch=247
05/21/2022 13:03:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.795982 on epoch=249
05/21/2022 13:03:32 - INFO - __main__ - Global step 1000 Train loss 0.814239 Classification-F1 0.3700510855683269 on epoch=249
05/21/2022 13:03:32 - INFO - __main__ - save last model!
05/21/2022 13:03:32 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:03:32 - INFO - __main__ - Printing 3 examples
05/21/2022 13:03:32 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 13:03:32 - INFO - __main__ - ['others']
05/21/2022 13:03:32 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 13:03:32 - INFO - __main__ - ['others']
05/21/2022 13:03:32 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 13:03:32 - INFO - __main__ - ['others']
05/21/2022 13:03:32 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:03:33 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:03:33 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:03:33 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:03:33 - INFO - __main__ - Printing 3 examples
05/21/2022 13:03:33 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 13:03:33 - INFO - __main__ - ['others']
05/21/2022 13:03:33 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 13:03:33 - INFO - __main__ - ['others']
05/21/2022 13:03:33 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 13:03:33 - INFO - __main__ - ['others']
05/21/2022 13:03:33 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:03:33 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:03:33 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:03:34 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 13:03:35 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 13:03:35 - INFO - __main__ - Printing 3 examples
05/21/2022 13:03:35 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 13:03:35 - INFO - __main__ - ['others']
05/21/2022 13:03:35 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 13:03:35 - INFO - __main__ - ['others']
05/21/2022 13:03:35 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 13:03:35 - INFO - __main__ - ['others']
05/21/2022 13:03:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:03:37 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:03:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:03:37 - INFO - __main__ - Starting training!
05/21/2022 13:03:42 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 13:04:11 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_100_0.0003_8_predictions.txt
05/21/2022 13:04:11 - INFO - __main__ - Classification-F1 on test data: 0.1194
05/21/2022 13:04:11 - INFO - __main__ - prefix=emo_16_100, lr=0.0003, bsz=8, dev_performance=0.39166666666666666, test_performance=0.11942601056055605
05/21/2022 13:04:11 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.0002, bsz=8 ...
05/21/2022 13:04:12 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:04:12 - INFO - __main__ - Printing 3 examples
05/21/2022 13:04:12 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 13:04:12 - INFO - __main__ - ['others']
05/21/2022 13:04:12 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 13:04:12 - INFO - __main__ - ['others']
05/21/2022 13:04:12 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 13:04:12 - INFO - __main__ - ['others']
05/21/2022 13:04:12 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:04:12 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:04:12 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:04:12 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:04:12 - INFO - __main__ - Printing 3 examples
05/21/2022 13:04:12 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 13:04:12 - INFO - __main__ - ['others']
05/21/2022 13:04:12 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 13:04:12 - INFO - __main__ - ['others']
05/21/2022 13:04:12 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 13:04:12 - INFO - __main__ - ['others']
05/21/2022 13:04:12 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:04:12 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:04:12 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:04:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:04:16 - INFO - __main__ - Starting training!
05/21/2022 13:04:19 - INFO - __main__ - Step 10 Global step 10 Train loss 21.215363 on epoch=2
05/21/2022 13:04:22 - INFO - __main__ - Step 20 Global step 20 Train loss 19.331234 on epoch=4
05/21/2022 13:04:25 - INFO - __main__ - Step 30 Global step 30 Train loss 13.659596 on epoch=7
05/21/2022 13:04:27 - INFO - __main__ - Step 40 Global step 40 Train loss 10.569978 on epoch=9
05/21/2022 13:04:30 - INFO - __main__ - Step 50 Global step 50 Train loss 8.206144 on epoch=12
05/21/2022 13:04:36 - INFO - __main__ - Global step 50 Train loss 14.596462 Classification-F1 0.001755926251097454 on epoch=12
05/21/2022 13:04:39 - INFO - __main__ - Step 60 Global step 60 Train loss 6.757800 on epoch=14
05/21/2022 13:04:41 - INFO - __main__ - Step 70 Global step 70 Train loss 6.683671 on epoch=17
05/21/2022 13:04:44 - INFO - __main__ - Step 80 Global step 80 Train loss 5.125577 on epoch=19
05/21/2022 13:04:46 - INFO - __main__ - Step 90 Global step 90 Train loss 4.968767 on epoch=22
05/21/2022 13:04:49 - INFO - __main__ - Step 100 Global step 100 Train loss 5.473411 on epoch=24
05/21/2022 13:04:49 - INFO - __main__ - Global step 100 Train loss 5.801846 Classification-F1 0.1 on epoch=24
05/21/2022 13:04:52 - INFO - __main__ - Step 110 Global step 110 Train loss 3.898135 on epoch=27
05/21/2022 13:04:55 - INFO - __main__ - Step 120 Global step 120 Train loss 3.842667 on epoch=29
05/21/2022 13:04:58 - INFO - __main__ - Step 130 Global step 130 Train loss 3.830017 on epoch=32
05/21/2022 13:05:00 - INFO - __main__ - Step 140 Global step 140 Train loss 3.775826 on epoch=34
05/21/2022 13:05:03 - INFO - __main__ - Step 150 Global step 150 Train loss 3.141131 on epoch=37
05/21/2022 13:05:03 - INFO - __main__ - Global step 150 Train loss 3.697555 Classification-F1 0.1 on epoch=37
05/21/2022 13:05:06 - INFO - __main__ - Step 160 Global step 160 Train loss 2.193615 on epoch=39
05/21/2022 13:05:08 - INFO - __main__ - Step 170 Global step 170 Train loss 2.666981 on epoch=42
05/21/2022 13:05:11 - INFO - __main__ - Step 180 Global step 180 Train loss 2.526781 on epoch=44
05/21/2022 13:05:13 - INFO - __main__ - Step 190 Global step 190 Train loss 2.292360 on epoch=47
05/21/2022 13:05:16 - INFO - __main__ - Step 200 Global step 200 Train loss 3.127834 on epoch=49
05/21/2022 13:05:16 - INFO - __main__ - Global step 200 Train loss 2.561514 Classification-F1 0.23810046406622348 on epoch=49
05/21/2022 13:05:19 - INFO - __main__ - Step 210 Global step 210 Train loss 2.836284 on epoch=52
05/21/2022 13:05:21 - INFO - __main__ - Step 220 Global step 220 Train loss 2.195101 on epoch=54
05/21/2022 13:05:24 - INFO - __main__ - Step 230 Global step 230 Train loss 1.783296 on epoch=57
05/21/2022 13:05:26 - INFO - __main__ - Step 240 Global step 240 Train loss 2.007967 on epoch=59
05/21/2022 13:05:29 - INFO - __main__ - Step 250 Global step 250 Train loss 1.973439 on epoch=62
05/21/2022 13:05:29 - INFO - __main__ - Global step 250 Train loss 2.159217 Classification-F1 0.45533671134357634 on epoch=62
05/21/2022 13:05:32 - INFO - __main__ - Step 260 Global step 260 Train loss 1.789279 on epoch=64
05/21/2022 13:05:35 - INFO - __main__ - Step 270 Global step 270 Train loss 1.991658 on epoch=67
05/21/2022 13:05:37 - INFO - __main__ - Step 280 Global step 280 Train loss 1.644307 on epoch=69
05/21/2022 13:05:40 - INFO - __main__ - Step 290 Global step 290 Train loss 1.721527 on epoch=72
05/21/2022 13:05:42 - INFO - __main__ - Step 300 Global step 300 Train loss 1.862239 on epoch=74
05/21/2022 13:05:43 - INFO - __main__ - Global step 300 Train loss 1.801802 Classification-F1 0.4574162679425838 on epoch=74
05/21/2022 13:05:46 - INFO - __main__ - Step 310 Global step 310 Train loss 1.343722 on epoch=77
05/21/2022 13:05:48 - INFO - __main__ - Step 320 Global step 320 Train loss 1.295940 on epoch=79
05/21/2022 13:05:51 - INFO - __main__ - Step 330 Global step 330 Train loss 1.707178 on epoch=82
05/21/2022 13:05:53 - INFO - __main__ - Step 340 Global step 340 Train loss 1.550031 on epoch=84
05/21/2022 13:05:56 - INFO - __main__ - Step 350 Global step 350 Train loss 1.367387 on epoch=87
05/21/2022 13:05:56 - INFO - __main__ - Global step 350 Train loss 1.452852 Classification-F1 0.46857829670329676 on epoch=87
05/21/2022 13:05:59 - INFO - __main__ - Step 360 Global step 360 Train loss 1.500673 on epoch=89
05/21/2022 13:06:02 - INFO - __main__ - Step 370 Global step 370 Train loss 1.377626 on epoch=92
05/21/2022 13:06:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.980018 on epoch=94
05/21/2022 13:06:07 - INFO - __main__ - Step 390 Global step 390 Train loss 1.224490 on epoch=97
05/21/2022 13:06:09 - INFO - __main__ - Step 400 Global step 400 Train loss 1.333172 on epoch=99
05/21/2022 13:06:09 - INFO - __main__ - Global step 400 Train loss 1.283196 Classification-F1 0.49871299871299873 on epoch=99
05/21/2022 13:06:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.725817 on epoch=102
05/21/2022 13:06:15 - INFO - __main__ - Step 420 Global step 420 Train loss 1.178893 on epoch=104
05/21/2022 13:06:17 - INFO - __main__ - Step 430 Global step 430 Train loss 1.227191 on epoch=107
05/21/2022 13:06:20 - INFO - __main__ - Step 440 Global step 440 Train loss 1.305588 on epoch=109
05/21/2022 13:06:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.865556 on epoch=112
05/21/2022 13:06:23 - INFO - __main__ - Global step 450 Train loss 1.060609 Classification-F1 0.5357142857142857 on epoch=112
05/21/2022 13:06:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.953958 on epoch=114
05/21/2022 13:06:28 - INFO - __main__ - Step 470 Global step 470 Train loss 1.197086 on epoch=117
05/21/2022 13:06:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.812393 on epoch=119
05/21/2022 13:06:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.938927 on epoch=122
05/21/2022 13:06:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.788455 on epoch=124
05/21/2022 13:06:36 - INFO - __main__ - Global step 500 Train loss 0.938164 Classification-F1 0.5470704948646125 on epoch=124
05/21/2022 13:06:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.793105 on epoch=127
05/21/2022 13:06:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.763177 on epoch=129
05/21/2022 13:06:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.897743 on epoch=132
05/21/2022 13:06:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.781257 on epoch=134
05/21/2022 13:06:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.767345 on epoch=137
05/21/2022 13:06:50 - INFO - __main__ - Global step 550 Train loss 0.800525 Classification-F1 0.6956890659250005 on epoch=137
05/21/2022 13:06:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.740485 on epoch=139
05/21/2022 13:06:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.629077 on epoch=142
05/21/2022 13:06:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.894552 on epoch=144
05/21/2022 13:07:00 - INFO - __main__ - Step 590 Global step 590 Train loss 1.383237 on epoch=147
05/21/2022 13:07:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.880542 on epoch=149
05/21/2022 13:07:03 - INFO - __main__ - Global step 600 Train loss 0.905579 Classification-F1 0.4967144563918758 on epoch=149
05/21/2022 13:07:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.937278 on epoch=152
05/21/2022 13:07:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.800519 on epoch=154
05/21/2022 13:07:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.761558 on epoch=157
05/21/2022 13:07:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.772851 on epoch=159
05/21/2022 13:07:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.610741 on epoch=162
05/21/2022 13:07:16 - INFO - __main__ - Global step 650 Train loss 0.776590 Classification-F1 0.6170435653936372 on epoch=162
05/21/2022 13:07:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.493485 on epoch=164
05/21/2022 13:07:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.672298 on epoch=167
05/21/2022 13:07:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.564774 on epoch=169
05/21/2022 13:07:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.652070 on epoch=172
05/21/2022 13:07:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.457420 on epoch=174
05/21/2022 13:07:29 - INFO - __main__ - Global step 700 Train loss 0.568009 Classification-F1 0.7073328482496564 on epoch=174
05/21/2022 13:07:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.458969 on epoch=177
05/21/2022 13:07:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.496220 on epoch=179
05/21/2022 13:07:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.628055 on epoch=182
05/21/2022 13:07:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.437961 on epoch=184
05/21/2022 13:07:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.384391 on epoch=187
05/21/2022 13:07:42 - INFO - __main__ - Global step 750 Train loss 0.481119 Classification-F1 0.7417238667238667 on epoch=187
05/21/2022 13:07:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.543544 on epoch=189
05/21/2022 13:07:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.406882 on epoch=192
05/21/2022 13:07:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.383038 on epoch=194
05/21/2022 13:07:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.550818 on epoch=197
05/21/2022 13:07:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.283262 on epoch=199
05/21/2022 13:07:56 - INFO - __main__ - Global step 800 Train loss 0.433509 Classification-F1 0.7220429450030968 on epoch=199
05/21/2022 13:07:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.422970 on epoch=202
05/21/2022 13:08:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.319852 on epoch=204
05/21/2022 13:08:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.444568 on epoch=207
05/21/2022 13:08:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.367215 on epoch=209
05/21/2022 13:08:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.562926 on epoch=212
05/21/2022 13:08:09 - INFO - __main__ - Global step 850 Train loss 0.423506 Classification-F1 0.7368939304423175 on epoch=212
05/21/2022 13:08:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.413248 on epoch=214
05/21/2022 13:08:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.535912 on epoch=217
05/21/2022 13:08:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.483377 on epoch=219
05/21/2022 13:08:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.311038 on epoch=222
05/21/2022 13:08:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.527386 on epoch=224
05/21/2022 13:08:22 - INFO - __main__ - Global step 900 Train loss 0.454192 Classification-F1 0.7181235166388931 on epoch=224
05/21/2022 13:08:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.545596 on epoch=227
05/21/2022 13:08:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.484429 on epoch=229
05/21/2022 13:08:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.307622 on epoch=232
05/21/2022 13:08:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.404869 on epoch=234
05/21/2022 13:08:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.647474 on epoch=237
05/21/2022 13:08:35 - INFO - __main__ - Global step 950 Train loss 0.477998 Classification-F1 0.7338850957364282 on epoch=237
05/21/2022 13:08:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.356177 on epoch=239
05/21/2022 13:08:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.387155 on epoch=242
05/21/2022 13:08:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.332013 on epoch=244
05/21/2022 13:08:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.436656 on epoch=247
05/21/2022 13:08:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.262828 on epoch=249
05/21/2022 13:08:48 - INFO - __main__ - Global step 1000 Train loss 0.354966 Classification-F1 0.7211822660098522 on epoch=249
05/21/2022 13:08:48 - INFO - __main__ - save last model!
05/21/2022 13:08:49 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:08:49 - INFO - __main__ - Printing 3 examples
05/21/2022 13:08:49 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 13:08:49 - INFO - __main__ - ['others']
05/21/2022 13:08:49 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 13:08:49 - INFO - __main__ - ['others']
05/21/2022 13:08:49 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 13:08:49 - INFO - __main__ - ['others']
05/21/2022 13:08:49 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:08:49 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:08:49 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:08:49 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:08:49 - INFO - __main__ - Printing 3 examples
05/21/2022 13:08:49 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 13:08:49 - INFO - __main__ - ['others']
05/21/2022 13:08:49 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 13:08:49 - INFO - __main__ - ['others']
05/21/2022 13:08:49 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 13:08:49 - INFO - __main__ - ['others']
05/21/2022 13:08:49 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:08:49 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:08:49 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:08:50 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 13:08:51 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 13:08:51 - INFO - __main__ - Printing 3 examples
05/21/2022 13:08:51 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 13:08:51 - INFO - __main__ - ['others']
05/21/2022 13:08:51 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 13:08:51 - INFO - __main__ - ['others']
05/21/2022 13:08:51 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 13:08:51 - INFO - __main__ - ['others']
05/21/2022 13:08:51 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:08:53 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:08:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:08:53 - INFO - __main__ - Starting training!
05/21/2022 13:08:58 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 13:09:26 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_100_0.0002_8_predictions.txt
05/21/2022 13:09:26 - INFO - __main__ - Classification-F1 on test data: 0.4291
05/21/2022 13:09:26 - INFO - __main__ - prefix=emo_16_100, lr=0.0002, bsz=8, dev_performance=0.7417238667238667, test_performance=0.4291332251677281
05/21/2022 13:09:26 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.0001, bsz=8 ...
05/21/2022 13:09:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:09:27 - INFO - __main__ - Printing 3 examples
05/21/2022 13:09:27 - INFO - __main__ -  [emo] how cause yes am listening
05/21/2022 13:09:27 - INFO - __main__ - ['others']
05/21/2022 13:09:27 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/21/2022 13:09:27 - INFO - __main__ - ['others']
05/21/2022 13:09:27 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/21/2022 13:09:27 - INFO - __main__ - ['others']
05/21/2022 13:09:27 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:09:27 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:09:27 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:09:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:09:27 - INFO - __main__ - Printing 3 examples
05/21/2022 13:09:27 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/21/2022 13:09:27 - INFO - __main__ - ['others']
05/21/2022 13:09:27 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/21/2022 13:09:27 - INFO - __main__ - ['others']
05/21/2022 13:09:27 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/21/2022 13:09:27 - INFO - __main__ - ['others']
05/21/2022 13:09:27 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:09:27 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:09:27 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:09:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:09:31 - INFO - __main__ - Starting training!
05/21/2022 13:09:33 - INFO - __main__ - Step 10 Global step 10 Train loss 20.721483 on epoch=2
05/21/2022 13:09:36 - INFO - __main__ - Step 20 Global step 20 Train loss 19.300869 on epoch=4
05/21/2022 13:09:38 - INFO - __main__ - Step 30 Global step 30 Train loss 16.478638 on epoch=7
05/21/2022 13:09:41 - INFO - __main__ - Step 40 Global step 40 Train loss 13.865007 on epoch=9
05/21/2022 13:09:43 - INFO - __main__ - Step 50 Global step 50 Train loss 12.114893 on epoch=12
05/21/2022 13:09:57 - INFO - __main__ - Global step 50 Train loss 16.496180 Classification-F1 0.0 on epoch=12
05/21/2022 13:10:00 - INFO - __main__ - Step 60 Global step 60 Train loss 10.440310 on epoch=14
05/21/2022 13:10:02 - INFO - __main__ - Step 70 Global step 70 Train loss 9.332706 on epoch=17
05/21/2022 13:10:05 - INFO - __main__ - Step 80 Global step 80 Train loss 7.881997 on epoch=19
05/21/2022 13:10:07 - INFO - __main__ - Step 90 Global step 90 Train loss 7.784186 on epoch=22
05/21/2022 13:10:10 - INFO - __main__ - Step 100 Global step 100 Train loss 7.253451 on epoch=24
05/21/2022 13:10:15 - INFO - __main__ - Global step 100 Train loss 8.538530 Classification-F1 0.0 on epoch=24
05/21/2022 13:10:18 - INFO - __main__ - Step 110 Global step 110 Train loss 6.797396 on epoch=27
05/21/2022 13:10:20 - INFO - __main__ - Step 120 Global step 120 Train loss 6.691205 on epoch=29
05/21/2022 13:10:23 - INFO - __main__ - Step 130 Global step 130 Train loss 5.785422 on epoch=32
05/21/2022 13:10:25 - INFO - __main__ - Step 140 Global step 140 Train loss 6.009923 on epoch=34
05/21/2022 13:10:28 - INFO - __main__ - Step 150 Global step 150 Train loss 6.154008 on epoch=37
05/21/2022 13:10:29 - INFO - __main__ - Global step 150 Train loss 6.287591 Classification-F1 0.06837606837606837 on epoch=37
05/21/2022 13:10:32 - INFO - __main__ - Step 160 Global step 160 Train loss 4.861047 on epoch=39
05/21/2022 13:10:35 - INFO - __main__ - Step 170 Global step 170 Train loss 5.255890 on epoch=42
05/21/2022 13:10:37 - INFO - __main__ - Step 180 Global step 180 Train loss 4.897020 on epoch=44
05/21/2022 13:10:40 - INFO - __main__ - Step 190 Global step 190 Train loss 4.163675 on epoch=47
05/21/2022 13:10:42 - INFO - __main__ - Step 200 Global step 200 Train loss 4.323564 on epoch=49
05/21/2022 13:10:43 - INFO - __main__ - Global step 200 Train loss 4.700239 Classification-F1 0.1 on epoch=49
05/21/2022 13:10:45 - INFO - __main__ - Step 210 Global step 210 Train loss 3.709153 on epoch=52
05/21/2022 13:10:48 - INFO - __main__ - Step 220 Global step 220 Train loss 3.053333 on epoch=54
05/21/2022 13:10:51 - INFO - __main__ - Step 230 Global step 230 Train loss 3.009451 on epoch=57
05/21/2022 13:10:53 - INFO - __main__ - Step 240 Global step 240 Train loss 3.109648 on epoch=59
05/21/2022 13:10:56 - INFO - __main__ - Step 250 Global step 250 Train loss 2.965311 on epoch=62
05/21/2022 13:10:56 - INFO - __main__ - Global step 250 Train loss 3.169379 Classification-F1 0.1 on epoch=62
05/21/2022 13:10:58 - INFO - __main__ - Step 260 Global step 260 Train loss 3.264874 on epoch=64
05/21/2022 13:11:01 - INFO - __main__ - Step 270 Global step 270 Train loss 3.773873 on epoch=67
05/21/2022 13:11:03 - INFO - __main__ - Step 280 Global step 280 Train loss 2.622799 on epoch=69
05/21/2022 13:11:06 - INFO - __main__ - Step 290 Global step 290 Train loss 2.651724 on epoch=72
05/21/2022 13:11:08 - INFO - __main__ - Step 300 Global step 300 Train loss 3.093233 on epoch=74
05/21/2022 13:11:09 - INFO - __main__ - Global step 300 Train loss 3.081301 Classification-F1 0.2626262626262626 on epoch=74
05/21/2022 13:11:11 - INFO - __main__ - Step 310 Global step 310 Train loss 2.024891 on epoch=77
05/21/2022 13:11:14 - INFO - __main__ - Step 320 Global step 320 Train loss 2.645102 on epoch=79
05/21/2022 13:11:16 - INFO - __main__ - Step 330 Global step 330 Train loss 2.946944 on epoch=82
05/21/2022 13:11:19 - INFO - __main__ - Step 340 Global step 340 Train loss 2.817090 on epoch=84
05/21/2022 13:11:21 - INFO - __main__ - Step 350 Global step 350 Train loss 1.921262 on epoch=87
05/21/2022 13:11:22 - INFO - __main__ - Global step 350 Train loss 2.471058 Classification-F1 0.39748677248677244 on epoch=87
05/21/2022 13:11:25 - INFO - __main__ - Step 360 Global step 360 Train loss 2.263600 on epoch=89
05/21/2022 13:11:27 - INFO - __main__ - Step 370 Global step 370 Train loss 1.883753 on epoch=92
05/21/2022 13:11:30 - INFO - __main__ - Step 380 Global step 380 Train loss 2.393303 on epoch=94
05/21/2022 13:11:32 - INFO - __main__ - Step 390 Global step 390 Train loss 2.255341 on epoch=97
05/21/2022 13:11:35 - INFO - __main__ - Step 400 Global step 400 Train loss 2.143155 on epoch=99
05/21/2022 13:11:35 - INFO - __main__ - Global step 400 Train loss 2.187830 Classification-F1 0.38791643139469234 on epoch=99
05/21/2022 13:11:37 - INFO - __main__ - Step 410 Global step 410 Train loss 2.232460 on epoch=102
05/21/2022 13:11:40 - INFO - __main__ - Step 420 Global step 420 Train loss 1.713602 on epoch=104
05/21/2022 13:11:42 - INFO - __main__ - Step 430 Global step 430 Train loss 1.747358 on epoch=107
05/21/2022 13:11:45 - INFO - __main__ - Step 440 Global step 440 Train loss 2.126690 on epoch=109
05/21/2022 13:11:48 - INFO - __main__ - Step 450 Global step 450 Train loss 2.137556 on epoch=112
05/21/2022 13:11:48 - INFO - __main__ - Global step 450 Train loss 1.991533 Classification-F1 0.3841634276416885 on epoch=112
05/21/2022 13:11:50 - INFO - __main__ - Step 460 Global step 460 Train loss 1.834779 on epoch=114
05/21/2022 13:11:53 - INFO - __main__ - Step 470 Global step 470 Train loss 1.844631 on epoch=117
05/21/2022 13:11:55 - INFO - __main__ - Step 480 Global step 480 Train loss 1.762990 on epoch=119
05/21/2022 13:11:58 - INFO - __main__ - Step 490 Global step 490 Train loss 1.968361 on epoch=122
05/21/2022 13:12:00 - INFO - __main__ - Step 500 Global step 500 Train loss 1.429521 on epoch=124
05/21/2022 13:12:01 - INFO - __main__ - Global step 500 Train loss 1.768056 Classification-F1 0.4533088235294118 on epoch=124
05/21/2022 13:12:03 - INFO - __main__ - Step 510 Global step 510 Train loss 1.503485 on epoch=127
05/21/2022 13:12:06 - INFO - __main__ - Step 520 Global step 520 Train loss 1.608736 on epoch=129
05/21/2022 13:12:09 - INFO - __main__ - Step 530 Global step 530 Train loss 1.630434 on epoch=132
05/21/2022 13:12:11 - INFO - __main__ - Step 540 Global step 540 Train loss 1.589711 on epoch=134
05/21/2022 13:12:13 - INFO - __main__ - Step 550 Global step 550 Train loss 1.369395 on epoch=137
05/21/2022 13:12:14 - INFO - __main__ - Global step 550 Train loss 1.540352 Classification-F1 0.4254807692307692 on epoch=137
05/21/2022 13:12:16 - INFO - __main__ - Step 560 Global step 560 Train loss 1.339755 on epoch=139
05/21/2022 13:12:19 - INFO - __main__ - Step 570 Global step 570 Train loss 1.467793 on epoch=142
05/21/2022 13:12:21 - INFO - __main__ - Step 580 Global step 580 Train loss 1.392898 on epoch=144
05/21/2022 13:12:24 - INFO - __main__ - Step 590 Global step 590 Train loss 1.659306 on epoch=147
05/21/2022 13:12:26 - INFO - __main__ - Step 600 Global step 600 Train loss 1.179471 on epoch=149
05/21/2022 13:12:27 - INFO - __main__ - Global step 600 Train loss 1.407845 Classification-F1 0.4262544802867384 on epoch=149
05/21/2022 13:12:29 - INFO - __main__ - Step 610 Global step 610 Train loss 1.224091 on epoch=152
05/21/2022 13:12:32 - INFO - __main__ - Step 620 Global step 620 Train loss 1.336662 on epoch=154
05/21/2022 13:12:34 - INFO - __main__ - Step 630 Global step 630 Train loss 1.309911 on epoch=157
05/21/2022 13:12:37 - INFO - __main__ - Step 640 Global step 640 Train loss 1.224236 on epoch=159
05/21/2022 13:12:39 - INFO - __main__ - Step 650 Global step 650 Train loss 1.148695 on epoch=162
05/21/2022 13:12:40 - INFO - __main__ - Global step 650 Train loss 1.248719 Classification-F1 0.48755026953024727 on epoch=162
05/21/2022 13:12:42 - INFO - __main__ - Step 660 Global step 660 Train loss 1.515351 on epoch=164
05/21/2022 13:12:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.934876 on epoch=167
05/21/2022 13:12:47 - INFO - __main__ - Step 680 Global step 680 Train loss 1.139882 on epoch=169
05/21/2022 13:12:50 - INFO - __main__ - Step 690 Global step 690 Train loss 1.545576 on epoch=172
05/21/2022 13:12:52 - INFO - __main__ - Step 700 Global step 700 Train loss 1.194959 on epoch=174
05/21/2022 13:12:53 - INFO - __main__ - Global step 700 Train loss 1.266129 Classification-F1 0.4581699346405229 on epoch=174
05/21/2022 13:12:55 - INFO - __main__ - Step 710 Global step 710 Train loss 1.177396 on epoch=177
05/21/2022 13:12:58 - INFO - __main__ - Step 720 Global step 720 Train loss 1.146474 on epoch=179
05/21/2022 13:13:00 - INFO - __main__ - Step 730 Global step 730 Train loss 1.185977 on epoch=182
05/21/2022 13:13:03 - INFO - __main__ - Step 740 Global step 740 Train loss 1.201215 on epoch=184
05/21/2022 13:13:05 - INFO - __main__ - Step 750 Global step 750 Train loss 1.155599 on epoch=187
05/21/2022 13:13:06 - INFO - __main__ - Global step 750 Train loss 1.173332 Classification-F1 0.4411562840647584 on epoch=187
05/21/2022 13:13:08 - INFO - __main__ - Step 760 Global step 760 Train loss 1.079163 on epoch=189
05/21/2022 13:13:11 - INFO - __main__ - Step 770 Global step 770 Train loss 1.418798 on epoch=192
05/21/2022 13:13:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.967273 on epoch=194
05/21/2022 13:13:16 - INFO - __main__ - Step 790 Global step 790 Train loss 1.673682 on epoch=197
05/21/2022 13:13:18 - INFO - __main__ - Step 800 Global step 800 Train loss 1.200320 on epoch=199
05/21/2022 13:13:19 - INFO - __main__ - Global step 800 Train loss 1.267847 Classification-F1 0.46965018565565764 on epoch=199
05/21/2022 13:13:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.978510 on epoch=202
05/21/2022 13:13:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.933404 on epoch=204
05/21/2022 13:13:26 - INFO - __main__ - Step 830 Global step 830 Train loss 1.024719 on epoch=207
05/21/2022 13:13:29 - INFO - __main__ - Step 840 Global step 840 Train loss 1.195983 on epoch=209
05/21/2022 13:13:31 - INFO - __main__ - Step 850 Global step 850 Train loss 1.131726 on epoch=212
05/21/2022 13:13:31 - INFO - __main__ - Global step 850 Train loss 1.052869 Classification-F1 0.47357204588309243 on epoch=212
05/21/2022 13:13:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.944359 on epoch=214
05/21/2022 13:13:36 - INFO - __main__ - Step 870 Global step 870 Train loss 1.075794 on epoch=217
05/21/2022 13:13:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.927264 on epoch=219
05/21/2022 13:13:41 - INFO - __main__ - Step 890 Global step 890 Train loss 1.275324 on epoch=222
05/21/2022 13:13:44 - INFO - __main__ - Step 900 Global step 900 Train loss 1.307589 on epoch=224
05/21/2022 13:13:44 - INFO - __main__ - Global step 900 Train loss 1.106066 Classification-F1 0.5098039215686274 on epoch=224
05/21/2022 13:13:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.992643 on epoch=227
05/21/2022 13:13:50 - INFO - __main__ - Step 920 Global step 920 Train loss 1.022516 on epoch=229
05/21/2022 13:13:52 - INFO - __main__ - Step 930 Global step 930 Train loss 1.021445 on epoch=232
05/21/2022 13:13:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.875732 on epoch=234
05/21/2022 13:13:57 - INFO - __main__ - Step 950 Global step 950 Train loss 1.183577 on epoch=237
05/21/2022 13:13:57 - INFO - __main__ - Global step 950 Train loss 1.019182 Classification-F1 0.4833333333333334 on epoch=237
05/21/2022 13:14:00 - INFO - __main__ - Step 960 Global step 960 Train loss 1.120902 on epoch=239
05/21/2022 13:14:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.912191 on epoch=242
05/21/2022 13:14:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.966840 on epoch=244
05/21/2022 13:14:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.913659 on epoch=247
05/21/2022 13:14:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.079958 on epoch=249
05/21/2022 13:14:10 - INFO - __main__ - Global step 1000 Train loss 0.998710 Classification-F1 0.5116892911010558 on epoch=249
05/21/2022 13:14:11 - INFO - __main__ - save last model!
05/21/2022 13:14:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:14:11 - INFO - __main__ - Printing 3 examples
05/21/2022 13:14:11 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 13:14:11 - INFO - __main__ - ['others']
05/21/2022 13:14:11 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 13:14:11 - INFO - __main__ - ['others']
05/21/2022 13:14:11 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 13:14:11 - INFO - __main__ - ['others']
05/21/2022 13:14:11 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:14:11 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:14:11 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:14:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:14:11 - INFO - __main__ - Printing 3 examples
05/21/2022 13:14:11 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/21/2022 13:14:11 - INFO - __main__ - ['others']
05/21/2022 13:14:11 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/21/2022 13:14:11 - INFO - __main__ - ['others']
05/21/2022 13:14:11 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/21/2022 13:14:11 - INFO - __main__ - ['others']
05/21/2022 13:14:11 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:14:11 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:14:11 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:14:13 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 13:14:13 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 13:14:13 - INFO - __main__ - Printing 3 examples
05/21/2022 13:14:13 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 13:14:13 - INFO - __main__ - ['others']
05/21/2022 13:14:13 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 13:14:13 - INFO - __main__ - ['others']
05/21/2022 13:14:13 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 13:14:13 - INFO - __main__ - ['others']
05/21/2022 13:14:13 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:14:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:14:15 - INFO - __main__ - Starting training!
05/21/2022 13:14:16 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:14:21 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 13:14:51 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_100_0.0001_8_predictions.txt
05/21/2022 13:14:51 - INFO - __main__ - Classification-F1 on test data: 0.1707
05/21/2022 13:14:51 - INFO - __main__ - prefix=emo_16_100, lr=0.0001, bsz=8, dev_performance=0.5116892911010558, test_performance=0.17065471296367846
05/21/2022 13:14:51 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.0005, bsz=8 ...
05/21/2022 13:14:52 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:14:52 - INFO - __main__ - Printing 3 examples
05/21/2022 13:14:52 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 13:14:52 - INFO - __main__ - ['others']
05/21/2022 13:14:52 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 13:14:52 - INFO - __main__ - ['others']
05/21/2022 13:14:52 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 13:14:52 - INFO - __main__ - ['others']
05/21/2022 13:14:52 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:14:52 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:14:52 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:14:52 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:14:52 - INFO - __main__ - Printing 3 examples
05/21/2022 13:14:52 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/21/2022 13:14:52 - INFO - __main__ - ['others']
05/21/2022 13:14:52 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/21/2022 13:14:52 - INFO - __main__ - ['others']
05/21/2022 13:14:52 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/21/2022 13:14:52 - INFO - __main__ - ['others']
05/21/2022 13:14:52 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:14:52 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:14:52 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:14:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:14:56 - INFO - __main__ - Starting training!
05/21/2022 13:14:58 - INFO - __main__ - Step 10 Global step 10 Train loss 21.327835 on epoch=2
05/21/2022 13:15:01 - INFO - __main__ - Step 20 Global step 20 Train loss 17.050486 on epoch=4
05/21/2022 13:15:03 - INFO - __main__ - Step 30 Global step 30 Train loss 9.559040 on epoch=7
05/21/2022 13:15:06 - INFO - __main__ - Step 40 Global step 40 Train loss 6.202925 on epoch=9
05/21/2022 13:15:08 - INFO - __main__ - Step 50 Global step 50 Train loss 4.045063 on epoch=12
05/21/2022 13:15:09 - INFO - __main__ - Global step 50 Train loss 11.637071 Classification-F1 0.1 on epoch=12
05/21/2022 13:15:12 - INFO - __main__ - Step 60 Global step 60 Train loss 3.901897 on epoch=14
05/21/2022 13:15:14 - INFO - __main__ - Step 70 Global step 70 Train loss 3.051726 on epoch=17
05/21/2022 13:15:17 - INFO - __main__ - Step 80 Global step 80 Train loss 2.809185 on epoch=19
05/21/2022 13:15:19 - INFO - __main__ - Step 90 Global step 90 Train loss 1.739009 on epoch=22
05/21/2022 13:15:22 - INFO - __main__ - Step 100 Global step 100 Train loss 1.707343 on epoch=24
05/21/2022 13:15:22 - INFO - __main__ - Global step 100 Train loss 2.641832 Classification-F1 0.2707317073170732 on epoch=24
05/21/2022 13:15:25 - INFO - __main__ - Step 110 Global step 110 Train loss 1.466683 on epoch=27
05/21/2022 13:15:28 - INFO - __main__ - Step 120 Global step 120 Train loss 1.469977 on epoch=29
05/21/2022 13:15:30 - INFO - __main__ - Step 130 Global step 130 Train loss 1.061475 on epoch=32
05/21/2022 13:15:33 - INFO - __main__ - Step 140 Global step 140 Train loss 1.392798 on epoch=34
05/21/2022 13:15:36 - INFO - __main__ - Step 150 Global step 150 Train loss 1.005913 on epoch=37
05/21/2022 13:15:36 - INFO - __main__ - Global step 150 Train loss 1.279369 Classification-F1 0.4633477633477633 on epoch=37
05/21/2022 13:15:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.726553 on epoch=39
05/21/2022 13:15:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.866399 on epoch=42
05/21/2022 13:15:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.418827 on epoch=44
05/21/2022 13:15:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.759198 on epoch=47
05/21/2022 13:15:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.473426 on epoch=49
05/21/2022 13:15:50 - INFO - __main__ - Global step 200 Train loss 0.648881 Classification-F1 0.6730047182349814 on epoch=49
05/21/2022 13:15:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.466594 on epoch=52
05/21/2022 13:15:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.620345 on epoch=54
05/21/2022 13:15:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.386712 on epoch=57
05/21/2022 13:16:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.276276 on epoch=59
05/21/2022 13:16:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.353574 on epoch=62
05/21/2022 13:16:03 - INFO - __main__ - Global step 250 Train loss 0.420700 Classification-F1 0.7148809523809525 on epoch=62
05/21/2022 13:16:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.337012 on epoch=64
05/21/2022 13:16:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.235732 on epoch=67
05/21/2022 13:16:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.133045 on epoch=69
05/21/2022 13:16:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.153516 on epoch=72
05/21/2022 13:16:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.078050 on epoch=74
05/21/2022 13:16:17 - INFO - __main__ - Global step 300 Train loss 0.187471 Classification-F1 0.7198467945171798 on epoch=74
05/21/2022 13:16:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.128524 on epoch=77
05/21/2022 13:16:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.137314 on epoch=79
05/21/2022 13:16:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.021345 on epoch=82
05/21/2022 13:16:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.146406 on epoch=84
05/21/2022 13:16:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.115867 on epoch=87
05/21/2022 13:16:31 - INFO - __main__ - Global step 350 Train loss 0.109891 Classification-F1 0.7460842377416349 on epoch=87
05/21/2022 13:16:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.229494 on epoch=89
05/21/2022 13:16:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.012431 on epoch=92
05/21/2022 13:16:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.087836 on epoch=94
05/21/2022 13:16:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.131234 on epoch=97
05/21/2022 13:16:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.095849 on epoch=99
05/21/2022 13:16:44 - INFO - __main__ - Global step 400 Train loss 0.111369 Classification-F1 0.7459677419354839 on epoch=99
05/21/2022 13:16:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.080989 on epoch=102
05/21/2022 13:16:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.034234 on epoch=104
05/21/2022 13:16:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.008278 on epoch=107
05/21/2022 13:16:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.033529 on epoch=109
05/21/2022 13:16:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.066071 on epoch=112
05/21/2022 13:16:58 - INFO - __main__ - Global step 450 Train loss 0.044620 Classification-F1 0.6847947454844008 on epoch=112
05/21/2022 13:17:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.198947 on epoch=114
05/21/2022 13:17:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.075660 on epoch=117
05/21/2022 13:17:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.017640 on epoch=119
05/21/2022 13:17:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.095227 on epoch=122
05/21/2022 13:17:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.027044 on epoch=124
05/21/2022 13:17:11 - INFO - __main__ - Global step 500 Train loss 0.082904 Classification-F1 0.38815881345592723 on epoch=124
05/21/2022 13:17:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.020038 on epoch=127
05/21/2022 13:17:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.074995 on epoch=129
05/21/2022 13:17:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.083782 on epoch=132
05/21/2022 13:17:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.006138 on epoch=134
05/21/2022 13:17:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.004983 on epoch=137
05/21/2022 13:17:24 - INFO - __main__ - Global step 550 Train loss 0.037987 Classification-F1 0.7027168234064786 on epoch=137
05/21/2022 13:17:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.021340 on epoch=139
05/21/2022 13:17:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.054960 on epoch=142
05/21/2022 13:17:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.087821 on epoch=144
05/21/2022 13:17:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.015612 on epoch=147
05/21/2022 13:17:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.065861 on epoch=149
05/21/2022 13:17:38 - INFO - __main__ - Global step 600 Train loss 0.049119 Classification-F1 0.6439903846153846 on epoch=149
05/21/2022 13:17:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000343 on epoch=152
05/21/2022 13:17:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.006892 on epoch=154
05/21/2022 13:17:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.008693 on epoch=157
05/21/2022 13:17:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.009799 on epoch=159
05/21/2022 13:17:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.007433 on epoch=162
05/21/2022 13:17:51 - INFO - __main__ - Global step 650 Train loss 0.006632 Classification-F1 0.7323241906733368 on epoch=162
05/21/2022 13:17:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.006960 on epoch=164
05/21/2022 13:17:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.002587 on epoch=167
05/21/2022 13:17:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.020273 on epoch=169
05/21/2022 13:18:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.002304 on epoch=172
05/21/2022 13:18:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000243 on epoch=174
05/21/2022 13:18:05 - INFO - __main__ - Global step 700 Train loss 0.006474 Classification-F1 0.6986764193660745 on epoch=174
05/21/2022 13:18:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.001961 on epoch=177
05/21/2022 13:18:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000358 on epoch=179
05/21/2022 13:18:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000863 on epoch=182
05/21/2022 13:18:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.067286 on epoch=184
05/21/2022 13:18:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.022321 on epoch=187
05/21/2022 13:18:18 - INFO - __main__ - Global step 750 Train loss 0.018558 Classification-F1 0.6851075509584964 on epoch=187
05/21/2022 13:18:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.004750 on epoch=189
05/21/2022 13:18:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.002009 on epoch=192
05/21/2022 13:18:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000082 on epoch=194
05/21/2022 13:18:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.007404 on epoch=197
05/21/2022 13:18:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.003721 on epoch=199
05/21/2022 13:18:31 - INFO - __main__ - Global step 800 Train loss 0.003593 Classification-F1 0.701489276062331 on epoch=199
05/21/2022 13:18:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.012251 on epoch=202
05/21/2022 13:18:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.006832 on epoch=204
05/21/2022 13:18:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.001398 on epoch=207
05/21/2022 13:18:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000327 on epoch=209
05/21/2022 13:18:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.014801 on epoch=212
05/21/2022 13:18:45 - INFO - __main__ - Global step 850 Train loss 0.007122 Classification-F1 0.7121848739495799 on epoch=212
05/21/2022 13:18:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000085 on epoch=214
05/21/2022 13:18:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000099 on epoch=217
05/21/2022 13:18:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.022674 on epoch=219
05/21/2022 13:18:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.035874 on epoch=222
05/21/2022 13:18:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.003297 on epoch=224
05/21/2022 13:18:58 - INFO - __main__ - Global step 900 Train loss 0.012406 Classification-F1 0.7189033566708107 on epoch=224
05/21/2022 13:19:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000018 on epoch=227
05/21/2022 13:19:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000010 on epoch=229
05/21/2022 13:19:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.003736 on epoch=232
05/21/2022 13:19:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000360 on epoch=234
05/21/2022 13:19:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.002656 on epoch=237
05/21/2022 13:19:11 - INFO - __main__ - Global step 950 Train loss 0.001356 Classification-F1 0.749229127134725 on epoch=237
05/21/2022 13:19:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000092 on epoch=239
05/21/2022 13:19:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.019084 on epoch=242
05/21/2022 13:19:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000019 on epoch=244
05/21/2022 13:19:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.016258 on epoch=247
05/21/2022 13:19:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.001065 on epoch=249
05/21/2022 13:19:25 - INFO - __main__ - Global step 1000 Train loss 0.007304 Classification-F1 0.6986764193660745 on epoch=249
05/21/2022 13:19:25 - INFO - __main__ - save last model!
05/21/2022 13:19:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:19:26 - INFO - __main__ - Printing 3 examples
05/21/2022 13:19:26 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 13:19:26 - INFO - __main__ - ['others']
05/21/2022 13:19:26 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 13:19:26 - INFO - __main__ - ['others']
05/21/2022 13:19:26 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 13:19:26 - INFO - __main__ - ['others']
05/21/2022 13:19:26 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:19:26 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:19:26 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:19:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:19:26 - INFO - __main__ - Printing 3 examples
05/21/2022 13:19:26 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/21/2022 13:19:26 - INFO - __main__ - ['others']
05/21/2022 13:19:26 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/21/2022 13:19:26 - INFO - __main__ - ['others']
05/21/2022 13:19:26 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/21/2022 13:19:26 - INFO - __main__ - ['others']
05/21/2022 13:19:26 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:19:26 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:19:26 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:19:28 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 13:19:28 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 13:19:28 - INFO - __main__ - Printing 3 examples
05/21/2022 13:19:28 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 13:19:28 - INFO - __main__ - ['others']
05/21/2022 13:19:28 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 13:19:28 - INFO - __main__ - ['others']
05/21/2022 13:19:28 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 13:19:28 - INFO - __main__ - ['others']
05/21/2022 13:19:28 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:19:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:19:30 - INFO - __main__ - Starting training!
05/21/2022 13:19:30 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:19:35 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 13:20:09 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_13_0.0005_8_predictions.txt
05/21/2022 13:20:09 - INFO - __main__ - Classification-F1 on test data: 0.1013
05/21/2022 13:20:10 - INFO - __main__ - prefix=emo_16_13, lr=0.0005, bsz=8, dev_performance=0.749229127134725, test_performance=0.10125974548429764
05/21/2022 13:20:10 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.0003, bsz=8 ...
05/21/2022 13:20:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:20:11 - INFO - __main__ - Printing 3 examples
05/21/2022 13:20:11 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 13:20:11 - INFO - __main__ - ['others']
05/21/2022 13:20:11 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 13:20:11 - INFO - __main__ - ['others']
05/21/2022 13:20:11 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 13:20:11 - INFO - __main__ - ['others']
05/21/2022 13:20:11 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:20:11 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:20:11 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:20:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:20:11 - INFO - __main__ - Printing 3 examples
05/21/2022 13:20:11 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/21/2022 13:20:11 - INFO - __main__ - ['others']
05/21/2022 13:20:11 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/21/2022 13:20:11 - INFO - __main__ - ['others']
05/21/2022 13:20:11 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/21/2022 13:20:11 - INFO - __main__ - ['others']
05/21/2022 13:20:11 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:20:11 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:20:11 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:20:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:20:15 - INFO - __main__ - Starting training!
05/21/2022 13:20:17 - INFO - __main__ - Step 10 Global step 10 Train loss 20.158590 on epoch=2
05/21/2022 13:20:19 - INFO - __main__ - Step 20 Global step 20 Train loss 19.423309 on epoch=4
05/21/2022 13:20:22 - INFO - __main__ - Step 30 Global step 30 Train loss 12.317191 on epoch=7
05/21/2022 13:20:24 - INFO - __main__ - Step 40 Global step 40 Train loss 11.367667 on epoch=9
05/21/2022 13:20:27 - INFO - __main__ - Step 50 Global step 50 Train loss 7.201444 on epoch=12
05/21/2022 13:20:32 - INFO - __main__ - Global step 50 Train loss 14.093639 Classification-F1 0.026297109232709098 on epoch=12
05/21/2022 13:20:34 - INFO - __main__ - Step 60 Global step 60 Train loss 5.747188 on epoch=14
05/21/2022 13:20:37 - INFO - __main__ - Step 70 Global step 70 Train loss 4.567018 on epoch=17
05/21/2022 13:20:39 - INFO - __main__ - Step 80 Global step 80 Train loss 4.822685 on epoch=19
05/21/2022 13:20:42 - INFO - __main__ - Step 90 Global step 90 Train loss 3.892616 on epoch=22
05/21/2022 13:20:44 - INFO - __main__ - Step 100 Global step 100 Train loss 3.531338 on epoch=24
05/21/2022 13:20:45 - INFO - __main__ - Global step 100 Train loss 4.512169 Classification-F1 0.20238095238095238 on epoch=24
05/21/2022 13:20:48 - INFO - __main__ - Step 110 Global step 110 Train loss 2.489327 on epoch=27
05/21/2022 13:20:50 - INFO - __main__ - Step 120 Global step 120 Train loss 2.558974 on epoch=29
05/21/2022 13:20:53 - INFO - __main__ - Step 130 Global step 130 Train loss 3.070582 on epoch=32
05/21/2022 13:20:55 - INFO - __main__ - Step 140 Global step 140 Train loss 2.099866 on epoch=34
05/21/2022 13:20:57 - INFO - __main__ - Step 150 Global step 150 Train loss 1.830421 on epoch=37
05/21/2022 13:20:58 - INFO - __main__ - Global step 150 Train loss 2.409834 Classification-F1 0.4240232935279375 on epoch=37
05/21/2022 13:21:01 - INFO - __main__ - Step 160 Global step 160 Train loss 2.057958 on epoch=39
05/21/2022 13:21:03 - INFO - __main__ - Step 170 Global step 170 Train loss 1.669603 on epoch=42
05/21/2022 13:21:06 - INFO - __main__ - Step 180 Global step 180 Train loss 2.021516 on epoch=44
05/21/2022 13:21:08 - INFO - __main__ - Step 190 Global step 190 Train loss 1.552194 on epoch=47
05/21/2022 13:21:11 - INFO - __main__ - Step 200 Global step 200 Train loss 1.213409 on epoch=49
05/21/2022 13:21:11 - INFO - __main__ - Global step 200 Train loss 1.702936 Classification-F1 0.4888800283537126 on epoch=49
05/21/2022 13:21:14 - INFO - __main__ - Step 210 Global step 210 Train loss 1.242527 on epoch=52
05/21/2022 13:21:16 - INFO - __main__ - Step 220 Global step 220 Train loss 1.379086 on epoch=54
05/21/2022 13:21:19 - INFO - __main__ - Step 230 Global step 230 Train loss 2.848055 on epoch=57
05/21/2022 13:21:21 - INFO - __main__ - Step 240 Global step 240 Train loss 1.518437 on epoch=59
05/21/2022 13:21:24 - INFO - __main__ - Step 250 Global step 250 Train loss 1.298012 on epoch=62
05/21/2022 13:21:24 - INFO - __main__ - Global step 250 Train loss 1.657223 Classification-F1 0.42520054616828806 on epoch=62
05/21/2022 13:21:26 - INFO - __main__ - Step 260 Global step 260 Train loss 1.046851 on epoch=64
05/21/2022 13:21:29 - INFO - __main__ - Step 270 Global step 270 Train loss 1.170516 on epoch=67
05/21/2022 13:21:31 - INFO - __main__ - Step 280 Global step 280 Train loss 1.033053 on epoch=69
05/21/2022 13:21:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.991174 on epoch=72
05/21/2022 13:21:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.959011 on epoch=74
05/21/2022 13:21:37 - INFO - __main__ - Global step 300 Train loss 1.040121 Classification-F1 0.38209083717250686 on epoch=74
05/21/2022 13:21:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.930609 on epoch=77
05/21/2022 13:21:42 - INFO - __main__ - Step 320 Global step 320 Train loss 1.084875 on epoch=79
05/21/2022 13:21:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.848558 on epoch=82
05/21/2022 13:21:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.952638 on epoch=84
05/21/2022 13:21:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.842330 on epoch=87
05/21/2022 13:21:49 - INFO - __main__ - Global step 350 Train loss 0.931802 Classification-F1 0.49848484848484853 on epoch=87
05/21/2022 13:21:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.767769 on epoch=89
05/21/2022 13:21:55 - INFO - __main__ - Step 370 Global step 370 Train loss 1.012820 on epoch=92
05/21/2022 13:21:57 - INFO - __main__ - Step 380 Global step 380 Train loss 1.132791 on epoch=94
05/21/2022 13:22:00 - INFO - __main__ - Step 390 Global step 390 Train loss 1.194067 on epoch=97
05/21/2022 13:22:02 - INFO - __main__ - Step 400 Global step 400 Train loss 1.117913 on epoch=99
05/21/2022 13:22:03 - INFO - __main__ - Global step 400 Train loss 1.045072 Classification-F1 0.4207010343917249 on epoch=99
05/21/2022 13:22:05 - INFO - __main__ - Step 410 Global step 410 Train loss 1.421387 on epoch=102
05/21/2022 13:22:08 - INFO - __main__ - Step 420 Global step 420 Train loss 1.165648 on epoch=104
05/21/2022 13:22:10 - INFO - __main__ - Step 430 Global step 430 Train loss 1.118224 on epoch=107
05/21/2022 13:22:13 - INFO - __main__ - Step 440 Global step 440 Train loss 1.567163 on epoch=109
05/21/2022 13:22:15 - INFO - __main__ - Step 450 Global step 450 Train loss 1.070030 on epoch=112
05/21/2022 13:22:16 - INFO - __main__ - Global step 450 Train loss 1.268490 Classification-F1 0.41894338380513496 on epoch=112
05/21/2022 13:22:18 - INFO - __main__ - Step 460 Global step 460 Train loss 1.040246 on epoch=114
05/21/2022 13:22:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.984013 on epoch=117
05/21/2022 13:22:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.996506 on epoch=119
05/21/2022 13:22:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.710159 on epoch=122
05/21/2022 13:22:28 - INFO - __main__ - Step 500 Global step 500 Train loss 1.004793 on epoch=124
05/21/2022 13:22:28 - INFO - __main__ - Global step 500 Train loss 0.947143 Classification-F1 0.27561294540699577 on epoch=124
05/21/2022 13:22:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.933588 on epoch=127
05/21/2022 13:22:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.760797 on epoch=129
05/21/2022 13:22:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.997263 on epoch=132
05/21/2022 13:22:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.895424 on epoch=134
05/21/2022 13:22:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.790614 on epoch=137
05/21/2022 13:22:41 - INFO - __main__ - Global step 550 Train loss 0.875537 Classification-F1 0.463772659124983 on epoch=137
05/21/2022 13:22:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.756618 on epoch=139
05/21/2022 13:22:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.852355 on epoch=142
05/21/2022 13:22:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.881687 on epoch=144
05/21/2022 13:22:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.719306 on epoch=147
05/21/2022 13:22:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.719303 on epoch=149
05/21/2022 13:22:54 - INFO - __main__ - Global step 600 Train loss 0.785854 Classification-F1 0.4784764108072379 on epoch=149
05/21/2022 13:22:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.727858 on epoch=152
05/21/2022 13:22:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.672543 on epoch=154
05/21/2022 13:23:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.589878 on epoch=157
05/21/2022 13:23:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.713803 on epoch=159
05/21/2022 13:23:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.726501 on epoch=162
05/21/2022 13:23:07 - INFO - __main__ - Global step 650 Train loss 0.686117 Classification-F1 0.5558333333333334 on epoch=162
05/21/2022 13:23:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.707215 on epoch=164
05/21/2022 13:23:12 - INFO - __main__ - Step 670 Global step 670 Train loss 0.680814 on epoch=167
05/21/2022 13:23:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.645981 on epoch=169
05/21/2022 13:23:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.727842 on epoch=172
05/21/2022 13:23:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.680662 on epoch=174
05/21/2022 13:23:20 - INFO - __main__ - Global step 700 Train loss 0.688503 Classification-F1 0.5074525745257452 on epoch=174
05/21/2022 13:23:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.657548 on epoch=177
05/21/2022 13:23:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.672488 on epoch=179
05/21/2022 13:23:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.627680 on epoch=182
05/21/2022 13:23:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.650017 on epoch=184
05/21/2022 13:23:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.586650 on epoch=187
05/21/2022 13:23:33 - INFO - __main__ - Global step 750 Train loss 0.638877 Classification-F1 0.6394345238095238 on epoch=187
05/21/2022 13:23:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.632378 on epoch=189
05/21/2022 13:23:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.535688 on epoch=192
05/21/2022 13:23:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.729487 on epoch=194
05/21/2022 13:23:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.691528 on epoch=197
05/21/2022 13:23:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.686924 on epoch=199
05/21/2022 13:23:46 - INFO - __main__ - Global step 800 Train loss 0.655201 Classification-F1 0.5840062111801242 on epoch=199
05/21/2022 13:23:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.517742 on epoch=202
05/21/2022 13:23:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.612353 on epoch=204
05/21/2022 13:23:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.623937 on epoch=207
05/21/2022 13:23:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.556239 on epoch=209
05/21/2022 13:23:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.535628 on epoch=212
05/21/2022 13:23:59 - INFO - __main__ - Global step 850 Train loss 0.569180 Classification-F1 0.6096804740957966 on epoch=212
05/21/2022 13:24:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.530412 on epoch=214
05/21/2022 13:24:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.687709 on epoch=217
05/21/2022 13:24:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.677516 on epoch=219
05/21/2022 13:24:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.562711 on epoch=222
05/21/2022 13:24:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.587016 on epoch=224
05/21/2022 13:24:11 - INFO - __main__ - Global step 900 Train loss 0.609073 Classification-F1 0.5670707070707071 on epoch=224
05/21/2022 13:24:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.461659 on epoch=227
05/21/2022 13:24:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.450654 on epoch=229
05/21/2022 13:24:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.488063 on epoch=232
05/21/2022 13:24:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.362330 on epoch=234
05/21/2022 13:24:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.543383 on epoch=237
05/21/2022 13:24:24 - INFO - __main__ - Global step 950 Train loss 0.461218 Classification-F1 0.6879164590858139 on epoch=237
05/21/2022 13:24:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.377543 on epoch=239
05/21/2022 13:24:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.543179 on epoch=242
05/21/2022 13:24:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.527849 on epoch=244
05/21/2022 13:24:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.418463 on epoch=247
05/21/2022 13:24:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.531445 on epoch=249
05/21/2022 13:24:37 - INFO - __main__ - Global step 1000 Train loss 0.479696 Classification-F1 0.6907444067370537 on epoch=249
05/21/2022 13:24:38 - INFO - __main__ - save last model!
05/21/2022 13:24:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:24:38 - INFO - __main__ - Printing 3 examples
05/21/2022 13:24:38 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 13:24:38 - INFO - __main__ - ['others']
05/21/2022 13:24:38 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 13:24:38 - INFO - __main__ - ['others']
05/21/2022 13:24:38 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 13:24:38 - INFO - __main__ - ['others']
05/21/2022 13:24:38 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:24:38 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:24:38 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:24:38 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:24:38 - INFO - __main__ - Printing 3 examples
05/21/2022 13:24:38 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/21/2022 13:24:38 - INFO - __main__ - ['others']
05/21/2022 13:24:38 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/21/2022 13:24:38 - INFO - __main__ - ['others']
05/21/2022 13:24:38 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/21/2022 13:24:38 - INFO - __main__ - ['others']
05/21/2022 13:24:38 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:24:38 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:24:38 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:24:40 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 13:24:40 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 13:24:40 - INFO - __main__ - Printing 3 examples
05/21/2022 13:24:40 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 13:24:40 - INFO - __main__ - ['others']
05/21/2022 13:24:40 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 13:24:40 - INFO - __main__ - ['others']
05/21/2022 13:24:40 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 13:24:40 - INFO - __main__ - ['others']
05/21/2022 13:24:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:24:43 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:24:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:24:43 - INFO - __main__ - Starting training!
05/21/2022 13:24:48 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 13:25:17 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_13_0.0003_8_predictions.txt
05/21/2022 13:25:17 - INFO - __main__ - Classification-F1 on test data: 0.3074
05/21/2022 13:25:17 - INFO - __main__ - prefix=emo_16_13, lr=0.0003, bsz=8, dev_performance=0.6907444067370537, test_performance=0.3073665469563368
05/21/2022 13:25:17 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.0002, bsz=8 ...
05/21/2022 13:25:18 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:25:18 - INFO - __main__ - Printing 3 examples
05/21/2022 13:25:18 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 13:25:18 - INFO - __main__ - ['others']
05/21/2022 13:25:18 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 13:25:18 - INFO - __main__ - ['others']
05/21/2022 13:25:18 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 13:25:18 - INFO - __main__ - ['others']
05/21/2022 13:25:18 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:25:18 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:25:18 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:25:18 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:25:18 - INFO - __main__ - Printing 3 examples
05/21/2022 13:25:18 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/21/2022 13:25:18 - INFO - __main__ - ['others']
05/21/2022 13:25:18 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/21/2022 13:25:18 - INFO - __main__ - ['others']
05/21/2022 13:25:18 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/21/2022 13:25:18 - INFO - __main__ - ['others']
05/21/2022 13:25:18 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:25:18 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:25:18 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:25:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:25:22 - INFO - __main__ - Starting training!
05/21/2022 13:25:24 - INFO - __main__ - Step 10 Global step 10 Train loss 20.136141 on epoch=2
05/21/2022 13:25:27 - INFO - __main__ - Step 20 Global step 20 Train loss 19.112782 on epoch=4
05/21/2022 13:25:29 - INFO - __main__ - Step 30 Global step 30 Train loss 13.202579 on epoch=7
05/21/2022 13:25:32 - INFO - __main__ - Step 40 Global step 40 Train loss 11.497574 on epoch=9
05/21/2022 13:25:34 - INFO - __main__ - Step 50 Global step 50 Train loss 8.257957 on epoch=12
05/21/2022 13:25:43 - INFO - __main__ - Global step 50 Train loss 14.441406 Classification-F1 0.0 on epoch=12
05/21/2022 13:25:46 - INFO - __main__ - Step 60 Global step 60 Train loss 8.040622 on epoch=14
05/21/2022 13:25:48 - INFO - __main__ - Step 70 Global step 70 Train loss 7.091964 on epoch=17
05/21/2022 13:25:51 - INFO - __main__ - Step 80 Global step 80 Train loss 5.996793 on epoch=19
05/21/2022 13:25:53 - INFO - __main__ - Step 90 Global step 90 Train loss 4.779410 on epoch=22
05/21/2022 13:25:56 - INFO - __main__ - Step 100 Global step 100 Train loss 4.506227 on epoch=24
05/21/2022 13:25:56 - INFO - __main__ - Global step 100 Train loss 6.083003 Classification-F1 0.0974025974025974 on epoch=24
05/21/2022 13:25:59 - INFO - __main__ - Step 110 Global step 110 Train loss 4.991711 on epoch=27
05/21/2022 13:26:02 - INFO - __main__ - Step 120 Global step 120 Train loss 3.544822 on epoch=29
05/21/2022 13:26:04 - INFO - __main__ - Step 130 Global step 130 Train loss 3.246340 on epoch=32
05/21/2022 13:26:07 - INFO - __main__ - Step 140 Global step 140 Train loss 3.452197 on epoch=34
05/21/2022 13:26:09 - INFO - __main__ - Step 150 Global step 150 Train loss 2.665246 on epoch=37
05/21/2022 13:26:10 - INFO - __main__ - Global step 150 Train loss 3.580063 Classification-F1 0.1 on epoch=37
05/21/2022 13:26:12 - INFO - __main__ - Step 160 Global step 160 Train loss 3.124903 on epoch=39
05/21/2022 13:26:15 - INFO - __main__ - Step 170 Global step 170 Train loss 2.098857 on epoch=42
05/21/2022 13:26:18 - INFO - __main__ - Step 180 Global step 180 Train loss 2.989688 on epoch=44
05/21/2022 13:26:20 - INFO - __main__ - Step 190 Global step 190 Train loss 2.146470 on epoch=47
05/21/2022 13:26:23 - INFO - __main__ - Step 200 Global step 200 Train loss 2.533447 on epoch=49
05/21/2022 13:26:23 - INFO - __main__ - Global step 200 Train loss 2.578673 Classification-F1 0.11714285714285715 on epoch=49
05/21/2022 13:26:26 - INFO - __main__ - Step 210 Global step 210 Train loss 2.935793 on epoch=52
05/21/2022 13:26:28 - INFO - __main__ - Step 220 Global step 220 Train loss 2.984482 on epoch=54
05/21/2022 13:26:31 - INFO - __main__ - Step 230 Global step 230 Train loss 2.016507 on epoch=57
05/21/2022 13:26:33 - INFO - __main__ - Step 240 Global step 240 Train loss 1.846950 on epoch=59
05/21/2022 13:26:36 - INFO - __main__ - Step 250 Global step 250 Train loss 2.498783 on epoch=62
05/21/2022 13:26:36 - INFO - __main__ - Global step 250 Train loss 2.456503 Classification-F1 0.2857396870554765 on epoch=62
05/21/2022 13:26:39 - INFO - __main__ - Step 260 Global step 260 Train loss 2.014960 on epoch=64
05/21/2022 13:26:42 - INFO - __main__ - Step 270 Global step 270 Train loss 1.846507 on epoch=67
05/21/2022 13:26:44 - INFO - __main__ - Step 280 Global step 280 Train loss 2.051133 on epoch=69
05/21/2022 13:26:47 - INFO - __main__ - Step 290 Global step 290 Train loss 1.660800 on epoch=72
05/21/2022 13:26:49 - INFO - __main__ - Step 300 Global step 300 Train loss 1.475627 on epoch=74
05/21/2022 13:26:50 - INFO - __main__ - Global step 300 Train loss 1.809805 Classification-F1 0.4696594427244582 on epoch=74
05/21/2022 13:26:52 - INFO - __main__ - Step 310 Global step 310 Train loss 1.515870 on epoch=77
05/21/2022 13:26:55 - INFO - __main__ - Step 320 Global step 320 Train loss 1.374184 on epoch=79
05/21/2022 13:26:57 - INFO - __main__ - Step 330 Global step 330 Train loss 1.916609 on epoch=82
05/21/2022 13:27:00 - INFO - __main__ - Step 340 Global step 340 Train loss 1.632866 on epoch=84
05/21/2022 13:27:02 - INFO - __main__ - Step 350 Global step 350 Train loss 1.206679 on epoch=87
05/21/2022 13:27:03 - INFO - __main__ - Global step 350 Train loss 1.529241 Classification-F1 0.47073211875843457 on epoch=87
05/21/2022 13:27:06 - INFO - __main__ - Step 360 Global step 360 Train loss 1.188719 on epoch=89
05/21/2022 13:27:08 - INFO - __main__ - Step 370 Global step 370 Train loss 1.195963 on epoch=92
05/21/2022 13:27:11 - INFO - __main__ - Step 380 Global step 380 Train loss 1.048637 on epoch=94
05/21/2022 13:27:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.985637 on epoch=97
05/21/2022 13:27:16 - INFO - __main__ - Step 400 Global step 400 Train loss 2.172214 on epoch=99
05/21/2022 13:27:16 - INFO - __main__ - Global step 400 Train loss 1.318234 Classification-F1 0.29004329004329005 on epoch=99
05/21/2022 13:27:19 - INFO - __main__ - Step 410 Global step 410 Train loss 2.526440 on epoch=102
05/21/2022 13:27:21 - INFO - __main__ - Step 420 Global step 420 Train loss 2.139121 on epoch=104
05/21/2022 13:27:24 - INFO - __main__ - Step 430 Global step 430 Train loss 1.178687 on epoch=107
05/21/2022 13:27:26 - INFO - __main__ - Step 440 Global step 440 Train loss 1.313464 on epoch=109
05/21/2022 13:27:29 - INFO - __main__ - Step 450 Global step 450 Train loss 1.115827 on epoch=112
05/21/2022 13:27:29 - INFO - __main__ - Global step 450 Train loss 1.654708 Classification-F1 0.5944318181818181 on epoch=112
05/21/2022 13:27:32 - INFO - __main__ - Step 460 Global step 460 Train loss 1.040487 on epoch=114
05/21/2022 13:27:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.980560 on epoch=117
05/21/2022 13:27:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.829027 on epoch=119
05/21/2022 13:27:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.757257 on epoch=122
05/21/2022 13:27:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.832944 on epoch=124
05/21/2022 13:27:42 - INFO - __main__ - Global step 500 Train loss 0.888055 Classification-F1 0.46817988064791133 on epoch=124
05/21/2022 13:27:45 - INFO - __main__ - Step 510 Global step 510 Train loss 1.009231 on epoch=127
05/21/2022 13:27:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.794153 on epoch=129
05/21/2022 13:27:50 - INFO - __main__ - Step 530 Global step 530 Train loss 1.003692 on epoch=132
05/21/2022 13:27:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.824439 on epoch=134
05/21/2022 13:27:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.610390 on epoch=137
05/21/2022 13:27:55 - INFO - __main__ - Global step 550 Train loss 0.848381 Classification-F1 0.6247529644268774 on epoch=137
05/21/2022 13:27:58 - INFO - __main__ - Step 560 Global step 560 Train loss 1.205930 on epoch=139
05/21/2022 13:28:01 - INFO - __main__ - Step 570 Global step 570 Train loss 1.085955 on epoch=142
05/21/2022 13:28:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.915512 on epoch=144
05/21/2022 13:28:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.657123 on epoch=147
05/21/2022 13:28:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.858448 on epoch=149
05/21/2022 13:28:09 - INFO - __main__ - Global step 600 Train loss 0.944594 Classification-F1 0.5109207737148913 on epoch=149
05/21/2022 13:28:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.879410 on epoch=152
05/21/2022 13:28:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.798223 on epoch=154
05/21/2022 13:28:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.797328 on epoch=157
05/21/2022 13:28:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.833001 on epoch=159
05/21/2022 13:28:21 - INFO - __main__ - Step 650 Global step 650 Train loss 1.198427 on epoch=162
05/21/2022 13:28:22 - INFO - __main__ - Global step 650 Train loss 0.901278 Classification-F1 0.6381240213687023 on epoch=162
05/21/2022 13:28:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.721214 on epoch=164
05/21/2022 13:28:27 - INFO - __main__ - Step 670 Global step 670 Train loss 1.014831 on epoch=167
05/21/2022 13:28:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.696429 on epoch=169
05/21/2022 13:28:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.726166 on epoch=172
05/21/2022 13:28:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.824396 on epoch=174
05/21/2022 13:28:35 - INFO - __main__ - Global step 700 Train loss 0.796607 Classification-F1 0.5203629431570609 on epoch=174
05/21/2022 13:28:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.784428 on epoch=177
05/21/2022 13:28:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.684424 on epoch=179
05/21/2022 13:28:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.915492 on epoch=182
05/21/2022 13:28:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.698387 on epoch=184
05/21/2022 13:28:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.868414 on epoch=187
05/21/2022 13:28:48 - INFO - __main__ - Global step 750 Train loss 0.790229 Classification-F1 0.6124811146550276 on epoch=187
05/21/2022 13:28:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.623251 on epoch=189
05/21/2022 13:28:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.660279 on epoch=192
05/21/2022 13:28:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.642686 on epoch=194
05/21/2022 13:28:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.646586 on epoch=197
05/21/2022 13:29:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.603009 on epoch=199
05/21/2022 13:29:01 - INFO - __main__ - Global step 800 Train loss 0.635162 Classification-F1 0.6588861588861589 on epoch=199
05/21/2022 13:29:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.703793 on epoch=202
05/21/2022 13:29:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.670942 on epoch=204
05/21/2022 13:29:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.874964 on epoch=207
05/21/2022 13:29:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.701773 on epoch=209
05/21/2022 13:29:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.701056 on epoch=212
05/21/2022 13:29:14 - INFO - __main__ - Global step 850 Train loss 0.730506 Classification-F1 0.6938263001677636 on epoch=212
05/21/2022 13:29:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.417154 on epoch=214
05/21/2022 13:29:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.542544 on epoch=217
05/21/2022 13:29:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.615873 on epoch=219
05/21/2022 13:29:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.678552 on epoch=222
05/21/2022 13:29:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.782866 on epoch=224
05/21/2022 13:29:28 - INFO - __main__ - Global step 900 Train loss 0.607398 Classification-F1 0.6774044795783927 on epoch=224
05/21/2022 13:29:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.426380 on epoch=227
05/21/2022 13:29:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.643790 on epoch=229
05/21/2022 13:29:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.752656 on epoch=232
05/21/2022 13:29:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.568900 on epoch=234
05/21/2022 13:29:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.509057 on epoch=237
05/21/2022 13:29:41 - INFO - __main__ - Global step 950 Train loss 0.580156 Classification-F1 0.7004900332225914 on epoch=237
05/21/2022 13:29:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.600342 on epoch=239
05/21/2022 13:29:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.512768 on epoch=242
05/21/2022 13:29:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.457771 on epoch=244
05/21/2022 13:29:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.604194 on epoch=247
05/21/2022 13:29:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.383960 on epoch=249
05/21/2022 13:29:55 - INFO - __main__ - Global step 1000 Train loss 0.511807 Classification-F1 0.7404804804804805 on epoch=249
05/21/2022 13:29:55 - INFO - __main__ - save last model!
05/21/2022 13:29:55 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:29:55 - INFO - __main__ - Printing 3 examples
05/21/2022 13:29:55 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 13:29:55 - INFO - __main__ - ['others']
05/21/2022 13:29:55 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 13:29:55 - INFO - __main__ - ['others']
05/21/2022 13:29:55 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 13:29:55 - INFO - __main__ - ['others']
05/21/2022 13:29:55 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:29:55 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:29:55 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:29:55 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:29:55 - INFO - __main__ - Printing 3 examples
05/21/2022 13:29:55 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/21/2022 13:29:55 - INFO - __main__ - ['others']
05/21/2022 13:29:55 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/21/2022 13:29:55 - INFO - __main__ - ['others']
05/21/2022 13:29:55 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/21/2022 13:29:55 - INFO - __main__ - ['others']
05/21/2022 13:29:55 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:29:55 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:29:55 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:29:57 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 13:29:58 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 13:29:58 - INFO - __main__ - Printing 3 examples
05/21/2022 13:29:58 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 13:29:58 - INFO - __main__ - ['others']
05/21/2022 13:29:58 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 13:29:58 - INFO - __main__ - ['others']
05/21/2022 13:29:58 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 13:29:58 - INFO - __main__ - ['others']
05/21/2022 13:29:58 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:30:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:30:00 - INFO - __main__ - Starting training!
05/21/2022 13:30:00 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:30:05 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 13:30:34 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_13_0.0002_8_predictions.txt
05/21/2022 13:30:34 - INFO - __main__ - Classification-F1 on test data: 0.3554
05/21/2022 13:30:34 - INFO - __main__ - prefix=emo_16_13, lr=0.0002, bsz=8, dev_performance=0.7404804804804805, test_performance=0.35538308729168494
05/21/2022 13:30:34 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.0001, bsz=8 ...
05/21/2022 13:30:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:30:35 - INFO - __main__ - Printing 3 examples
05/21/2022 13:30:35 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/21/2022 13:30:35 - INFO - __main__ - ['others']
05/21/2022 13:30:35 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/21/2022 13:30:35 - INFO - __main__ - ['others']
05/21/2022 13:30:35 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/21/2022 13:30:35 - INFO - __main__ - ['others']
05/21/2022 13:30:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:30:35 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:30:35 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:30:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:30:35 - INFO - __main__ - Printing 3 examples
05/21/2022 13:30:35 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/21/2022 13:30:35 - INFO - __main__ - ['others']
05/21/2022 13:30:35 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/21/2022 13:30:35 - INFO - __main__ - ['others']
05/21/2022 13:30:35 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/21/2022 13:30:35 - INFO - __main__ - ['others']
05/21/2022 13:30:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:30:35 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:30:35 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:30:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:30:39 - INFO - __main__ - Starting training!
05/21/2022 13:30:41 - INFO - __main__ - Step 10 Global step 10 Train loss 20.745197 on epoch=2
05/21/2022 13:30:44 - INFO - __main__ - Step 20 Global step 20 Train loss 20.005768 on epoch=4
05/21/2022 13:30:46 - INFO - __main__ - Step 30 Global step 30 Train loss 16.872931 on epoch=7
05/21/2022 13:30:49 - INFO - __main__ - Step 40 Global step 40 Train loss 13.166553 on epoch=9
05/21/2022 13:30:51 - INFO - __main__ - Step 50 Global step 50 Train loss 12.861676 on epoch=12
05/21/2022 13:31:04 - INFO - __main__ - Global step 50 Train loss 16.730427 Classification-F1 0.0 on epoch=12
05/21/2022 13:31:07 - INFO - __main__ - Step 60 Global step 60 Train loss 11.449907 on epoch=14
05/21/2022 13:31:09 - INFO - __main__ - Step 70 Global step 70 Train loss 9.162210 on epoch=17
05/21/2022 13:31:12 - INFO - __main__ - Step 80 Global step 80 Train loss 8.621845 on epoch=19
05/21/2022 13:31:14 - INFO - __main__ - Step 90 Global step 90 Train loss 8.787184 on epoch=22
05/21/2022 13:31:17 - INFO - __main__ - Step 100 Global step 100 Train loss 7.725720 on epoch=24
05/21/2022 13:31:26 - INFO - __main__ - Global step 100 Train loss 9.149373 Classification-F1 0.001755926251097454 on epoch=24
05/21/2022 13:31:29 - INFO - __main__ - Step 110 Global step 110 Train loss 7.868188 on epoch=27
05/21/2022 13:31:31 - INFO - __main__ - Step 120 Global step 120 Train loss 7.290758 on epoch=29
05/21/2022 13:31:34 - INFO - __main__ - Step 130 Global step 130 Train loss 5.475997 on epoch=32
05/21/2022 13:31:36 - INFO - __main__ - Step 140 Global step 140 Train loss 6.563271 on epoch=34
05/21/2022 13:31:39 - INFO - __main__ - Step 150 Global step 150 Train loss 5.548301 on epoch=37
05/21/2022 13:31:41 - INFO - __main__ - Global step 150 Train loss 6.549303 Classification-F1 0.06222222222222222 on epoch=37
05/21/2022 13:31:44 - INFO - __main__ - Step 160 Global step 160 Train loss 5.684021 on epoch=39
05/21/2022 13:31:47 - INFO - __main__ - Step 170 Global step 170 Train loss 6.113121 on epoch=42
05/21/2022 13:31:49 - INFO - __main__ - Step 180 Global step 180 Train loss 5.058249 on epoch=44
05/21/2022 13:31:52 - INFO - __main__ - Step 190 Global step 190 Train loss 4.836064 on epoch=47
05/21/2022 13:31:54 - INFO - __main__ - Step 200 Global step 200 Train loss 3.858514 on epoch=49
05/21/2022 13:31:55 - INFO - __main__ - Global step 200 Train loss 5.109993 Classification-F1 0.13067758749069247 on epoch=49
05/21/2022 13:31:57 - INFO - __main__ - Step 210 Global step 210 Train loss 4.856731 on epoch=52
05/21/2022 13:32:00 - INFO - __main__ - Step 220 Global step 220 Train loss 3.744133 on epoch=54
05/21/2022 13:32:03 - INFO - __main__ - Step 230 Global step 230 Train loss 3.521597 on epoch=57
05/21/2022 13:32:05 - INFO - __main__ - Step 240 Global step 240 Train loss 3.127650 on epoch=59
05/21/2022 13:32:08 - INFO - __main__ - Step 250 Global step 250 Train loss 4.226402 on epoch=62
05/21/2022 13:32:08 - INFO - __main__ - Global step 250 Train loss 3.895303 Classification-F1 0.1542857142857143 on epoch=62
05/21/2022 13:32:11 - INFO - __main__ - Step 260 Global step 260 Train loss 3.565670 on epoch=64
05/21/2022 13:32:13 - INFO - __main__ - Step 270 Global step 270 Train loss 3.269500 on epoch=67
05/21/2022 13:32:16 - INFO - __main__ - Step 280 Global step 280 Train loss 2.837326 on epoch=69
05/21/2022 13:32:19 - INFO - __main__ - Step 290 Global step 290 Train loss 2.686574 on epoch=72
05/21/2022 13:32:21 - INFO - __main__ - Step 300 Global step 300 Train loss 3.345623 on epoch=74
05/21/2022 13:32:21 - INFO - __main__ - Global step 300 Train loss 3.140939 Classification-F1 0.20714285714285716 on epoch=74
05/21/2022 13:32:24 - INFO - __main__ - Step 310 Global step 310 Train loss 3.389279 on epoch=77
05/21/2022 13:32:27 - INFO - __main__ - Step 320 Global step 320 Train loss 2.416405 on epoch=79
05/21/2022 13:32:29 - INFO - __main__ - Step 330 Global step 330 Train loss 2.240852 on epoch=82
05/21/2022 13:32:32 - INFO - __main__ - Step 340 Global step 340 Train loss 2.801862 on epoch=84
05/21/2022 13:32:34 - INFO - __main__ - Step 350 Global step 350 Train loss 3.332551 on epoch=87
05/21/2022 13:32:35 - INFO - __main__ - Global step 350 Train loss 2.836190 Classification-F1 0.3889010989010989 on epoch=87
05/21/2022 13:32:38 - INFO - __main__ - Step 360 Global step 360 Train loss 2.207777 on epoch=89
05/21/2022 13:32:40 - INFO - __main__ - Step 370 Global step 370 Train loss 2.420427 on epoch=92
05/21/2022 13:32:43 - INFO - __main__ - Step 380 Global step 380 Train loss 2.891585 on epoch=94
05/21/2022 13:32:45 - INFO - __main__ - Step 390 Global step 390 Train loss 2.234562 on epoch=97
05/21/2022 13:32:48 - INFO - __main__ - Step 400 Global step 400 Train loss 2.094809 on epoch=99
05/21/2022 13:32:48 - INFO - __main__ - Global step 400 Train loss 2.369832 Classification-F1 0.2755050505050505 on epoch=99
05/21/2022 13:32:51 - INFO - __main__ - Step 410 Global step 410 Train loss 2.101324 on epoch=102
05/21/2022 13:32:53 - INFO - __main__ - Step 420 Global step 420 Train loss 1.865271 on epoch=104
05/21/2022 13:32:56 - INFO - __main__ - Step 430 Global step 430 Train loss 2.179641 on epoch=107
05/21/2022 13:32:58 - INFO - __main__ - Step 440 Global step 440 Train loss 2.707781 on epoch=109
05/21/2022 13:33:01 - INFO - __main__ - Step 450 Global step 450 Train loss 2.419664 on epoch=112
05/21/2022 13:33:01 - INFO - __main__ - Global step 450 Train loss 2.254736 Classification-F1 0.4091431556948798 on epoch=112
05/21/2022 13:33:04 - INFO - __main__ - Step 460 Global step 460 Train loss 1.591369 on epoch=114
05/21/2022 13:33:07 - INFO - __main__ - Step 470 Global step 470 Train loss 1.827389 on epoch=117
05/21/2022 13:33:09 - INFO - __main__ - Step 480 Global step 480 Train loss 1.924816 on epoch=119
05/21/2022 13:33:12 - INFO - __main__ - Step 490 Global step 490 Train loss 1.991480 on epoch=122
05/21/2022 13:33:14 - INFO - __main__ - Step 500 Global step 500 Train loss 2.067889 on epoch=124
05/21/2022 13:33:15 - INFO - __main__ - Global step 500 Train loss 1.880589 Classification-F1 0.4415498065764023 on epoch=124
05/21/2022 13:33:18 - INFO - __main__ - Step 510 Global step 510 Train loss 1.866911 on epoch=127
05/21/2022 13:33:20 - INFO - __main__ - Step 520 Global step 520 Train loss 1.904857 on epoch=129
05/21/2022 13:33:23 - INFO - __main__ - Step 530 Global step 530 Train loss 1.414631 on epoch=132
05/21/2022 13:33:25 - INFO - __main__ - Step 540 Global step 540 Train loss 1.269825 on epoch=134
05/21/2022 13:33:28 - INFO - __main__ - Step 550 Global step 550 Train loss 1.577759 on epoch=137
05/21/2022 13:33:28 - INFO - __main__ - Global step 550 Train loss 1.606796 Classification-F1 0.43425202248731665 on epoch=137
05/21/2022 13:33:31 - INFO - __main__ - Step 560 Global step 560 Train loss 1.499776 on epoch=139
05/21/2022 13:33:33 - INFO - __main__ - Step 570 Global step 570 Train loss 1.596789 on epoch=142
05/21/2022 13:33:36 - INFO - __main__ - Step 580 Global step 580 Train loss 1.430746 on epoch=144
05/21/2022 13:33:38 - INFO - __main__ - Step 590 Global step 590 Train loss 1.685303 on epoch=147
05/21/2022 13:33:41 - INFO - __main__ - Step 600 Global step 600 Train loss 1.697818 on epoch=149
05/21/2022 13:33:41 - INFO - __main__ - Global step 600 Train loss 1.582086 Classification-F1 0.4690611664295875 on epoch=149
05/21/2022 13:33:44 - INFO - __main__ - Step 610 Global step 610 Train loss 1.411817 on epoch=152
05/21/2022 13:33:46 - INFO - __main__ - Step 620 Global step 620 Train loss 1.284889 on epoch=154
05/21/2022 13:33:49 - INFO - __main__ - Step 630 Global step 630 Train loss 1.506964 on epoch=157
05/21/2022 13:33:52 - INFO - __main__ - Step 640 Global step 640 Train loss 1.830166 on epoch=159
05/21/2022 13:33:54 - INFO - __main__ - Step 650 Global step 650 Train loss 1.219932 on epoch=162
05/21/2022 13:33:54 - INFO - __main__ - Global step 650 Train loss 1.450754 Classification-F1 0.48905334566759173 on epoch=162
05/21/2022 13:33:57 - INFO - __main__ - Step 660 Global step 660 Train loss 1.239304 on epoch=164
05/21/2022 13:34:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.870483 on epoch=167
05/21/2022 13:34:02 - INFO - __main__ - Step 680 Global step 680 Train loss 1.220901 on epoch=169
05/21/2022 13:34:05 - INFO - __main__ - Step 690 Global step 690 Train loss 1.254915 on epoch=172
05/21/2022 13:34:07 - INFO - __main__ - Step 700 Global step 700 Train loss 1.145851 on epoch=174
05/21/2022 13:34:08 - INFO - __main__ - Global step 700 Train loss 1.146291 Classification-F1 0.5453815672610508 on epoch=174
05/21/2022 13:34:11 - INFO - __main__ - Step 710 Global step 710 Train loss 1.580920 on epoch=177
05/21/2022 13:34:13 - INFO - __main__ - Step 720 Global step 720 Train loss 1.184256 on epoch=179
05/21/2022 13:34:16 - INFO - __main__ - Step 730 Global step 730 Train loss 1.117772 on epoch=182
05/21/2022 13:34:18 - INFO - __main__ - Step 740 Global step 740 Train loss 1.117629 on epoch=184
05/21/2022 13:34:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.983416 on epoch=187
05/21/2022 13:34:21 - INFO - __main__ - Global step 750 Train loss 1.196799 Classification-F1 0.49238496808829285 on epoch=187
05/21/2022 13:34:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.863977 on epoch=189
05/21/2022 13:34:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.833620 on epoch=192
05/21/2022 13:34:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.911423 on epoch=194
05/21/2022 13:34:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.885334 on epoch=197
05/21/2022 13:34:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.985181 on epoch=199
05/21/2022 13:34:34 - INFO - __main__ - Global step 800 Train loss 0.895907 Classification-F1 0.6138888888888889 on epoch=199
05/21/2022 13:34:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.965968 on epoch=202
05/21/2022 13:34:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.846297 on epoch=204
05/21/2022 13:34:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.894318 on epoch=207
05/21/2022 13:34:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.899630 on epoch=209
05/21/2022 13:34:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.606598 on epoch=212
05/21/2022 13:34:48 - INFO - __main__ - Global step 850 Train loss 0.842562 Classification-F1 0.6138888888888889 on epoch=212
05/21/2022 13:34:50 - INFO - __main__ - Step 860 Global step 860 Train loss 1.131906 on epoch=214
05/21/2022 13:34:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.576446 on epoch=217
05/21/2022 13:34:55 - INFO - __main__ - Step 880 Global step 880 Train loss 1.017884 on epoch=219
05/21/2022 13:34:58 - INFO - __main__ - Step 890 Global step 890 Train loss 1.148176 on epoch=222
05/21/2022 13:35:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.745477 on epoch=224
05/21/2022 13:35:01 - INFO - __main__ - Global step 900 Train loss 0.923978 Classification-F1 0.6981695840391493 on epoch=224
05/21/2022 13:35:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.667140 on epoch=227
05/21/2022 13:35:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.496084 on epoch=229
05/21/2022 13:35:09 - INFO - __main__ - Step 930 Global step 930 Train loss 1.046445 on epoch=232
05/21/2022 13:35:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.969410 on epoch=234
05/21/2022 13:35:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.858455 on epoch=237
05/21/2022 13:35:14 - INFO - __main__ - Global step 950 Train loss 0.807507 Classification-F1 0.7310912698412699 on epoch=237
05/21/2022 13:35:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.757196 on epoch=239
05/21/2022 13:35:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.671384 on epoch=242
05/21/2022 13:35:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.918571 on epoch=244
05/21/2022 13:35:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.996780 on epoch=247
05/21/2022 13:35:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.731314 on epoch=249
05/21/2022 13:35:28 - INFO - __main__ - Global step 1000 Train loss 0.815049 Classification-F1 0.7101889338731443 on epoch=249
05/21/2022 13:35:28 - INFO - __main__ - save last model!
05/21/2022 13:35:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:35:28 - INFO - __main__ - Printing 3 examples
05/21/2022 13:35:28 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 13:35:28 - INFO - __main__ - ['sad']
05/21/2022 13:35:28 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 13:35:28 - INFO - __main__ - ['sad']
05/21/2022 13:35:28 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 13:35:28 - INFO - __main__ - ['sad']
05/21/2022 13:35:28 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:35:28 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:35:28 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:35:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:35:28 - INFO - __main__ - Printing 3 examples
05/21/2022 13:35:28 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/21/2022 13:35:28 - INFO - __main__ - ['sad']
05/21/2022 13:35:28 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/21/2022 13:35:28 - INFO - __main__ - ['sad']
05/21/2022 13:35:28 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/21/2022 13:35:28 - INFO - __main__ - ['sad']
05/21/2022 13:35:28 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:35:28 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:35:28 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:35:30 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 13:35:30 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 13:35:30 - INFO - __main__ - Printing 3 examples
05/21/2022 13:35:30 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 13:35:30 - INFO - __main__ - ['others']
05/21/2022 13:35:30 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 13:35:30 - INFO - __main__ - ['others']
05/21/2022 13:35:30 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 13:35:30 - INFO - __main__ - ['others']
05/21/2022 13:35:30 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:35:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:35:32 - INFO - __main__ - Starting training!
05/21/2022 13:35:32 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:35:38 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 13:36:06 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_13_0.0001_8_predictions.txt
05/21/2022 13:36:06 - INFO - __main__ - Classification-F1 on test data: 0.3575
05/21/2022 13:36:06 - INFO - __main__ - prefix=emo_16_13, lr=0.0001, bsz=8, dev_performance=0.7310912698412699, test_performance=0.35753057997407134
05/21/2022 13:36:06 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.0005, bsz=8 ...
05/21/2022 13:36:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:36:07 - INFO - __main__ - Printing 3 examples
05/21/2022 13:36:07 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 13:36:07 - INFO - __main__ - ['sad']
05/21/2022 13:36:07 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 13:36:07 - INFO - __main__ - ['sad']
05/21/2022 13:36:07 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 13:36:07 - INFO - __main__ - ['sad']
05/21/2022 13:36:07 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:36:07 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:36:07 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:36:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:36:07 - INFO - __main__ - Printing 3 examples
05/21/2022 13:36:07 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/21/2022 13:36:07 - INFO - __main__ - ['sad']
05/21/2022 13:36:07 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/21/2022 13:36:07 - INFO - __main__ - ['sad']
05/21/2022 13:36:07 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/21/2022 13:36:07 - INFO - __main__ - ['sad']
05/21/2022 13:36:07 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:36:07 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:36:07 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:36:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:36:11 - INFO - __main__ - Starting training!
05/21/2022 13:36:13 - INFO - __main__ - Step 10 Global step 10 Train loss 19.595755 on epoch=2
05/21/2022 13:36:15 - INFO - __main__ - Step 20 Global step 20 Train loss 16.125452 on epoch=4
05/21/2022 13:36:18 - INFO - __main__ - Step 30 Global step 30 Train loss 8.682070 on epoch=7
05/21/2022 13:36:20 - INFO - __main__ - Step 40 Global step 40 Train loss 5.515838 on epoch=9
05/21/2022 13:36:23 - INFO - __main__ - Step 50 Global step 50 Train loss 5.093097 on epoch=12
05/21/2022 13:36:24 - INFO - __main__ - Global step 50 Train loss 11.002442 Classification-F1 0.1 on epoch=12
05/21/2022 13:36:27 - INFO - __main__ - Step 60 Global step 60 Train loss 4.271138 on epoch=14
05/21/2022 13:36:29 - INFO - __main__ - Step 70 Global step 70 Train loss 2.954668 on epoch=17
05/21/2022 13:36:32 - INFO - __main__ - Step 80 Global step 80 Train loss 2.701829 on epoch=19
05/21/2022 13:36:34 - INFO - __main__ - Step 90 Global step 90 Train loss 2.909255 on epoch=22
05/21/2022 13:36:37 - INFO - __main__ - Step 100 Global step 100 Train loss 2.296267 on epoch=24
05/21/2022 13:36:37 - INFO - __main__ - Global step 100 Train loss 3.026631 Classification-F1 0.25317460317460316 on epoch=24
05/21/2022 13:36:40 - INFO - __main__ - Step 110 Global step 110 Train loss 1.837661 on epoch=27
05/21/2022 13:36:43 - INFO - __main__ - Step 120 Global step 120 Train loss 2.020002 on epoch=29
05/21/2022 13:36:45 - INFO - __main__ - Step 130 Global step 130 Train loss 1.970633 on epoch=32
05/21/2022 13:36:48 - INFO - __main__ - Step 140 Global step 140 Train loss 1.592895 on epoch=34
05/21/2022 13:36:50 - INFO - __main__ - Step 150 Global step 150 Train loss 1.297703 on epoch=37
05/21/2022 13:36:51 - INFO - __main__ - Global step 150 Train loss 1.743779 Classification-F1 0.20980302336234538 on epoch=37
05/21/2022 13:36:53 - INFO - __main__ - Step 160 Global step 160 Train loss 1.488550 on epoch=39
05/21/2022 13:36:56 - INFO - __main__ - Step 170 Global step 170 Train loss 2.848735 on epoch=42
05/21/2022 13:36:58 - INFO - __main__ - Step 180 Global step 180 Train loss 1.798934 on epoch=44
05/21/2022 13:37:01 - INFO - __main__ - Step 190 Global step 190 Train loss 1.442849 on epoch=47
05/21/2022 13:37:03 - INFO - __main__ - Step 200 Global step 200 Train loss 1.204566 on epoch=49
05/21/2022 13:37:03 - INFO - __main__ - Global step 200 Train loss 1.756727 Classification-F1 0.27810254536597245 on epoch=49
05/21/2022 13:37:06 - INFO - __main__ - Step 210 Global step 210 Train loss 1.358337 on epoch=52
05/21/2022 13:37:09 - INFO - __main__ - Step 220 Global step 220 Train loss 1.255404 on epoch=54
05/21/2022 13:37:11 - INFO - __main__ - Step 230 Global step 230 Train loss 1.101726 on epoch=57
05/21/2022 13:37:14 - INFO - __main__ - Step 240 Global step 240 Train loss 1.124528 on epoch=59
05/21/2022 13:37:16 - INFO - __main__ - Step 250 Global step 250 Train loss 1.243303 on epoch=62
05/21/2022 13:37:17 - INFO - __main__ - Global step 250 Train loss 1.216660 Classification-F1 0.30237154150197626 on epoch=62
05/21/2022 13:37:20 - INFO - __main__ - Step 260 Global step 260 Train loss 1.053195 on epoch=64
05/21/2022 13:37:22 - INFO - __main__ - Step 270 Global step 270 Train loss 1.059820 on epoch=67
05/21/2022 13:37:25 - INFO - __main__ - Step 280 Global step 280 Train loss 1.462590 on epoch=69
05/21/2022 13:37:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.970435 on epoch=72
05/21/2022 13:37:30 - INFO - __main__ - Step 300 Global step 300 Train loss 1.202934 on epoch=74
05/21/2022 13:37:30 - INFO - __main__ - Global step 300 Train loss 1.149795 Classification-F1 0.26071761416589 on epoch=74
05/21/2022 13:37:32 - INFO - __main__ - Step 310 Global step 310 Train loss 1.379455 on epoch=77
05/21/2022 13:37:35 - INFO - __main__ - Step 320 Global step 320 Train loss 1.119295 on epoch=79
05/21/2022 13:37:37 - INFO - __main__ - Step 330 Global step 330 Train loss 1.135418 on epoch=82
05/21/2022 13:37:40 - INFO - __main__ - Step 340 Global step 340 Train loss 1.259973 on epoch=84
05/21/2022 13:37:42 - INFO - __main__ - Step 350 Global step 350 Train loss 1.253059 on epoch=87
05/21/2022 13:37:43 - INFO - __main__ - Global step 350 Train loss 1.229440 Classification-F1 0.18941176470588234 on epoch=87
05/21/2022 13:37:45 - INFO - __main__ - Step 360 Global step 360 Train loss 1.200672 on epoch=89
05/21/2022 13:37:48 - INFO - __main__ - Step 370 Global step 370 Train loss 1.020725 on epoch=92
05/21/2022 13:37:50 - INFO - __main__ - Step 380 Global step 380 Train loss 1.038571 on epoch=94
05/21/2022 13:37:53 - INFO - __main__ - Step 390 Global step 390 Train loss 1.015445 on epoch=97
05/21/2022 13:37:55 - INFO - __main__ - Step 400 Global step 400 Train loss 1.019051 on epoch=99
05/21/2022 13:37:55 - INFO - __main__ - Global step 400 Train loss 1.058893 Classification-F1 0.19106247150022798 on epoch=99
05/21/2022 13:37:58 - INFO - __main__ - Step 410 Global step 410 Train loss 1.202170 on epoch=102
05/21/2022 13:38:00 - INFO - __main__ - Step 420 Global step 420 Train loss 1.159808 on epoch=104
05/21/2022 13:38:03 - INFO - __main__ - Step 430 Global step 430 Train loss 1.174572 on epoch=107
05/21/2022 13:38:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.877500 on epoch=109
05/21/2022 13:38:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.943252 on epoch=112
05/21/2022 13:38:08 - INFO - __main__ - Global step 450 Train loss 1.071460 Classification-F1 0.2426734134514021 on epoch=112
05/21/2022 13:38:11 - INFO - __main__ - Step 460 Global step 460 Train loss 1.137324 on epoch=114
05/21/2022 13:38:13 - INFO - __main__ - Step 470 Global step 470 Train loss 1.091649 on epoch=117
05/21/2022 13:38:16 - INFO - __main__ - Step 480 Global step 480 Train loss 1.058876 on epoch=119
05/21/2022 13:38:18 - INFO - __main__ - Step 490 Global step 490 Train loss 1.018118 on epoch=122
05/21/2022 13:38:21 - INFO - __main__ - Step 500 Global step 500 Train loss 1.017445 on epoch=124
05/21/2022 13:38:21 - INFO - __main__ - Global step 500 Train loss 1.064682 Classification-F1 0.29880959574596655 on epoch=124
05/21/2022 13:38:24 - INFO - __main__ - Step 510 Global step 510 Train loss 1.065821 on epoch=127
05/21/2022 13:38:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.939026 on epoch=129
05/21/2022 13:38:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.892234 on epoch=132
05/21/2022 13:38:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.999079 on epoch=134
05/21/2022 13:38:34 - INFO - __main__ - Step 550 Global step 550 Train loss 1.011129 on epoch=137
05/21/2022 13:38:34 - INFO - __main__ - Global step 550 Train loss 0.981458 Classification-F1 0.21137935423649706 on epoch=137
05/21/2022 13:38:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.842779 on epoch=139
05/21/2022 13:38:39 - INFO - __main__ - Step 570 Global step 570 Train loss 1.421068 on epoch=142
05/21/2022 13:38:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.905648 on epoch=144
05/21/2022 13:38:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.987873 on epoch=147
05/21/2022 13:38:46 - INFO - __main__ - Step 600 Global step 600 Train loss 1.167585 on epoch=149
05/21/2022 13:38:47 - INFO - __main__ - Global step 600 Train loss 1.064991 Classification-F1 0.2945670628183362 on epoch=149
05/21/2022 13:38:49 - INFO - __main__ - Step 610 Global step 610 Train loss 1.116258 on epoch=152
05/21/2022 13:38:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.888193 on epoch=154
05/21/2022 13:38:54 - INFO - __main__ - Step 630 Global step 630 Train loss 1.120790 on epoch=157
05/21/2022 13:38:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.901163 on epoch=159
05/21/2022 13:38:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.930577 on epoch=162
05/21/2022 13:38:59 - INFO - __main__ - Global step 650 Train loss 0.991396 Classification-F1 0.35389610389610393 on epoch=162
05/21/2022 13:39:02 - INFO - __main__ - Step 660 Global step 660 Train loss 1.007664 on epoch=164
05/21/2022 13:39:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.831962 on epoch=167
05/21/2022 13:39:07 - INFO - __main__ - Step 680 Global step 680 Train loss 1.118447 on epoch=169
05/21/2022 13:39:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.792035 on epoch=172
05/21/2022 13:39:12 - INFO - __main__ - Step 700 Global step 700 Train loss 1.007657 on epoch=174
05/21/2022 13:39:13 - INFO - __main__ - Global step 700 Train loss 0.951553 Classification-F1 0.3145945945945946 on epoch=174
05/21/2022 13:39:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.932590 on epoch=177
05/21/2022 13:39:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.833943 on epoch=179
05/21/2022 13:39:20 - INFO - __main__ - Step 730 Global step 730 Train loss 1.068699 on epoch=182
05/21/2022 13:39:23 - INFO - __main__ - Step 740 Global step 740 Train loss 1.109457 on epoch=184
05/21/2022 13:39:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.754887 on epoch=187
05/21/2022 13:39:26 - INFO - __main__ - Global step 750 Train loss 0.939915 Classification-F1 0.26037972289213596 on epoch=187
05/21/2022 13:39:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.885051 on epoch=189
05/21/2022 13:39:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.978812 on epoch=192
05/21/2022 13:39:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.967915 on epoch=194
05/21/2022 13:39:35 - INFO - __main__ - Step 790 Global step 790 Train loss 1.020996 on epoch=197
05/21/2022 13:39:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.806530 on epoch=199
05/21/2022 13:39:38 - INFO - __main__ - Global step 800 Train loss 0.931861 Classification-F1 0.2532705389848247 on epoch=199
05/21/2022 13:39:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.927435 on epoch=202
05/21/2022 13:39:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.984757 on epoch=204
05/21/2022 13:39:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.906090 on epoch=207
05/21/2022 13:39:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.801765 on epoch=209
05/21/2022 13:39:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.813042 on epoch=212
05/21/2022 13:39:51 - INFO - __main__ - Global step 850 Train loss 0.886617 Classification-F1 0.24678063445057657 on epoch=212
05/21/2022 13:39:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.870552 on epoch=214
05/21/2022 13:39:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.951147 on epoch=217
05/21/2022 13:39:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.923367 on epoch=219
05/21/2022 13:40:01 - INFO - __main__ - Step 890 Global step 890 Train loss 1.029209 on epoch=222
05/21/2022 13:40:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.795875 on epoch=224
05/21/2022 13:40:04 - INFO - __main__ - Global step 900 Train loss 0.914030 Classification-F1 0.20402046783625732 on epoch=224
05/21/2022 13:40:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.882432 on epoch=227
05/21/2022 13:40:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.869390 on epoch=229
05/21/2022 13:40:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.940828 on epoch=232
05/21/2022 13:40:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.848656 on epoch=234
05/21/2022 13:40:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.878035 on epoch=237
05/21/2022 13:40:17 - INFO - __main__ - Global step 950 Train loss 0.883868 Classification-F1 0.31375 on epoch=237
05/21/2022 13:40:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.886232 on epoch=239
05/21/2022 13:40:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.849728 on epoch=242
05/21/2022 13:40:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.809089 on epoch=244
05/21/2022 13:40:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.819003 on epoch=247
05/21/2022 13:40:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.786666 on epoch=249
05/21/2022 13:40:29 - INFO - __main__ - Global step 1000 Train loss 0.830144 Classification-F1 0.2368814192343604 on epoch=249
05/21/2022 13:40:29 - INFO - __main__ - save last model!
05/21/2022 13:40:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:40:30 - INFO - __main__ - Printing 3 examples
05/21/2022 13:40:30 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 13:40:30 - INFO - __main__ - ['sad']
05/21/2022 13:40:30 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 13:40:30 - INFO - __main__ - ['sad']
05/21/2022 13:40:30 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 13:40:30 - INFO - __main__ - ['sad']
05/21/2022 13:40:30 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:40:30 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:40:30 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:40:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:40:30 - INFO - __main__ - Printing 3 examples
05/21/2022 13:40:30 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/21/2022 13:40:30 - INFO - __main__ - ['sad']
05/21/2022 13:40:30 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/21/2022 13:40:30 - INFO - __main__ - ['sad']
05/21/2022 13:40:30 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/21/2022 13:40:30 - INFO - __main__ - ['sad']
05/21/2022 13:40:30 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:40:30 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:40:30 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:40:32 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 13:40:32 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 13:40:32 - INFO - __main__ - Printing 3 examples
05/21/2022 13:40:32 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 13:40:32 - INFO - __main__ - ['others']
05/21/2022 13:40:32 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 13:40:32 - INFO - __main__ - ['others']
05/21/2022 13:40:32 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 13:40:32 - INFO - __main__ - ['others']
05/21/2022 13:40:32 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:40:34 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:40:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:40:35 - INFO - __main__ - Starting training!
05/21/2022 13:40:40 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 13:41:08 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_21_0.0005_8_predictions.txt
05/21/2022 13:41:08 - INFO - __main__ - Classification-F1 on test data: 0.0962
05/21/2022 13:41:08 - INFO - __main__ - prefix=emo_16_21, lr=0.0005, bsz=8, dev_performance=0.35389610389610393, test_performance=0.09624342095279476
05/21/2022 13:41:08 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.0003, bsz=8 ...
05/21/2022 13:41:09 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:41:09 - INFO - __main__ - Printing 3 examples
05/21/2022 13:41:09 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 13:41:09 - INFO - __main__ - ['sad']
05/21/2022 13:41:09 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 13:41:09 - INFO - __main__ - ['sad']
05/21/2022 13:41:09 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 13:41:09 - INFO - __main__ - ['sad']
05/21/2022 13:41:09 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:41:09 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:41:09 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:41:09 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:41:09 - INFO - __main__ - Printing 3 examples
05/21/2022 13:41:09 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/21/2022 13:41:09 - INFO - __main__ - ['sad']
05/21/2022 13:41:09 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/21/2022 13:41:09 - INFO - __main__ - ['sad']
05/21/2022 13:41:09 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/21/2022 13:41:09 - INFO - __main__ - ['sad']
05/21/2022 13:41:09 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:41:09 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:41:09 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:41:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:41:13 - INFO - __main__ - Starting training!
05/21/2022 13:41:15 - INFO - __main__ - Step 10 Global step 10 Train loss 19.143833 on epoch=2
05/21/2022 13:41:17 - INFO - __main__ - Step 20 Global step 20 Train loss 18.624968 on epoch=4
05/21/2022 13:41:20 - INFO - __main__ - Step 30 Global step 30 Train loss 12.065682 on epoch=7
05/21/2022 13:41:22 - INFO - __main__ - Step 40 Global step 40 Train loss 8.479048 on epoch=9
05/21/2022 13:41:25 - INFO - __main__ - Step 50 Global step 50 Train loss 7.024104 on epoch=12
05/21/2022 13:41:29 - INFO - __main__ - Global step 50 Train loss 13.067527 Classification-F1 0.012000000000000002 on epoch=12
05/21/2022 13:41:32 - INFO - __main__ - Step 60 Global step 60 Train loss 5.859340 on epoch=14
05/21/2022 13:41:34 - INFO - __main__ - Step 70 Global step 70 Train loss 4.406659 on epoch=17
05/21/2022 13:41:37 - INFO - __main__ - Step 80 Global step 80 Train loss 4.487922 on epoch=19
05/21/2022 13:41:39 - INFO - __main__ - Step 90 Global step 90 Train loss 3.843697 on epoch=22
05/21/2022 13:41:42 - INFO - __main__ - Step 100 Global step 100 Train loss 3.602828 on epoch=24
05/21/2022 13:41:42 - INFO - __main__ - Global step 100 Train loss 4.440089 Classification-F1 0.13067758749069247 on epoch=24
05/21/2022 13:41:45 - INFO - __main__ - Step 110 Global step 110 Train loss 3.567965 on epoch=27
05/21/2022 13:41:48 - INFO - __main__ - Step 120 Global step 120 Train loss 3.262013 on epoch=29
05/21/2022 13:41:50 - INFO - __main__ - Step 130 Global step 130 Train loss 2.616422 on epoch=32
05/21/2022 13:41:53 - INFO - __main__ - Step 140 Global step 140 Train loss 2.822494 on epoch=34
05/21/2022 13:41:56 - INFO - __main__ - Step 150 Global step 150 Train loss 2.809663 on epoch=37
05/21/2022 13:41:56 - INFO - __main__ - Global step 150 Train loss 3.015711 Classification-F1 0.25453560371517026 on epoch=37
05/21/2022 13:41:59 - INFO - __main__ - Step 160 Global step 160 Train loss 2.383608 on epoch=39
05/21/2022 13:42:01 - INFO - __main__ - Step 170 Global step 170 Train loss 2.218632 on epoch=42
05/21/2022 13:42:04 - INFO - __main__ - Step 180 Global step 180 Train loss 2.239555 on epoch=44
05/21/2022 13:42:06 - INFO - __main__ - Step 190 Global step 190 Train loss 1.733498 on epoch=47
05/21/2022 13:42:09 - INFO - __main__ - Step 200 Global step 200 Train loss 1.983592 on epoch=49
05/21/2022 13:42:09 - INFO - __main__ - Global step 200 Train loss 2.111777 Classification-F1 0.3880460238264942 on epoch=49
05/21/2022 13:42:12 - INFO - __main__ - Step 210 Global step 210 Train loss 1.785389 on epoch=52
05/21/2022 13:42:15 - INFO - __main__ - Step 220 Global step 220 Train loss 1.594245 on epoch=54
05/21/2022 13:42:18 - INFO - __main__ - Step 230 Global step 230 Train loss 1.233540 on epoch=57
05/21/2022 13:42:20 - INFO - __main__ - Step 240 Global step 240 Train loss 1.475545 on epoch=59
05/21/2022 13:42:23 - INFO - __main__ - Step 250 Global step 250 Train loss 1.482279 on epoch=62
05/21/2022 13:42:23 - INFO - __main__ - Global step 250 Train loss 1.514200 Classification-F1 0.458982683982684 on epoch=62
05/21/2022 13:42:26 - INFO - __main__ - Step 260 Global step 260 Train loss 1.597797 on epoch=64
05/21/2022 13:42:29 - INFO - __main__ - Step 270 Global step 270 Train loss 1.356998 on epoch=67
05/21/2022 13:42:31 - INFO - __main__ - Step 280 Global step 280 Train loss 1.331105 on epoch=69
05/21/2022 13:42:34 - INFO - __main__ - Step 290 Global step 290 Train loss 1.316127 on epoch=72
05/21/2022 13:42:36 - INFO - __main__ - Step 300 Global step 300 Train loss 1.202385 on epoch=74
05/21/2022 13:42:37 - INFO - __main__ - Global step 300 Train loss 1.360883 Classification-F1 0.4508013381569223 on epoch=74
05/21/2022 13:42:39 - INFO - __main__ - Step 310 Global step 310 Train loss 1.936191 on epoch=77
05/21/2022 13:42:42 - INFO - __main__ - Step 320 Global step 320 Train loss 1.784618 on epoch=79
05/21/2022 13:42:44 - INFO - __main__ - Step 330 Global step 330 Train loss 1.550932 on epoch=82
05/21/2022 13:42:47 - INFO - __main__ - Step 340 Global step 340 Train loss 1.416973 on epoch=84
05/21/2022 13:42:50 - INFO - __main__ - Step 350 Global step 350 Train loss 1.298873 on epoch=87
05/21/2022 13:42:50 - INFO - __main__ - Global step 350 Train loss 1.597517 Classification-F1 0.4751514461984965 on epoch=87
05/21/2022 13:42:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.922142 on epoch=89
05/21/2022 13:42:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.959774 on epoch=92
05/21/2022 13:42:58 - INFO - __main__ - Step 380 Global step 380 Train loss 1.142037 on epoch=94
05/21/2022 13:43:01 - INFO - __main__ - Step 390 Global step 390 Train loss 1.306827 on epoch=97
05/21/2022 13:43:03 - INFO - __main__ - Step 400 Global step 400 Train loss 1.023155 on epoch=99
05/21/2022 13:43:03 - INFO - __main__ - Global step 400 Train loss 1.070787 Classification-F1 0.4243873564899847 on epoch=99
05/21/2022 13:43:06 - INFO - __main__ - Step 410 Global step 410 Train loss 1.154386 on epoch=102
05/21/2022 13:43:08 - INFO - __main__ - Step 420 Global step 420 Train loss 1.223383 on epoch=104
05/21/2022 13:43:11 - INFO - __main__ - Step 430 Global step 430 Train loss 1.279390 on epoch=107
05/21/2022 13:43:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.968341 on epoch=109
05/21/2022 13:43:16 - INFO - __main__ - Step 450 Global step 450 Train loss 1.190924 on epoch=112
05/21/2022 13:43:16 - INFO - __main__ - Global step 450 Train loss 1.163285 Classification-F1 0.4904115492350787 on epoch=112
05/21/2022 13:43:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.950591 on epoch=114
05/21/2022 13:43:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.983050 on epoch=117
05/21/2022 13:43:24 - INFO - __main__ - Step 480 Global step 480 Train loss 1.159677 on epoch=119
05/21/2022 13:43:27 - INFO - __main__ - Step 490 Global step 490 Train loss 1.081913 on epoch=122
05/21/2022 13:43:29 - INFO - __main__ - Step 500 Global step 500 Train loss 1.045704 on epoch=124
05/21/2022 13:43:30 - INFO - __main__ - Global step 500 Train loss 1.044187 Classification-F1 0.5254399439458862 on epoch=124
05/21/2022 13:43:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.858740 on epoch=127
05/21/2022 13:43:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.988567 on epoch=129
05/21/2022 13:43:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.986759 on epoch=132
05/21/2022 13:43:40 - INFO - __main__ - Step 540 Global step 540 Train loss 1.079918 on epoch=134
05/21/2022 13:43:42 - INFO - __main__ - Step 550 Global step 550 Train loss 1.128338 on epoch=137
05/21/2022 13:43:43 - INFO - __main__ - Global step 550 Train loss 1.008464 Classification-F1 0.515441577941578 on epoch=137
05/21/2022 13:43:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.791778 on epoch=139
05/21/2022 13:43:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.895636 on epoch=142
05/21/2022 13:43:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.745071 on epoch=144
05/21/2022 13:43:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.873028 on epoch=147
05/21/2022 13:43:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.844604 on epoch=149
05/21/2022 13:43:56 - INFO - __main__ - Global step 600 Train loss 0.830023 Classification-F1 0.5398110661268556 on epoch=149
05/21/2022 13:43:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.962660 on epoch=152
05/21/2022 13:44:01 - INFO - __main__ - Step 620 Global step 620 Train loss 1.001684 on epoch=154
05/21/2022 13:44:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.900864 on epoch=157
05/21/2022 13:44:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.874627 on epoch=159
05/21/2022 13:44:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.786748 on epoch=162
05/21/2022 13:44:09 - INFO - __main__ - Global step 650 Train loss 0.905317 Classification-F1 0.5695566796208336 on epoch=162
05/21/2022 13:44:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.871998 on epoch=164
05/21/2022 13:44:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.830916 on epoch=167
05/21/2022 13:44:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.653077 on epoch=169
05/21/2022 13:44:19 - INFO - __main__ - Step 690 Global step 690 Train loss 0.867846 on epoch=172
05/21/2022 13:44:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.849327 on epoch=174
05/21/2022 13:44:22 - INFO - __main__ - Global step 700 Train loss 0.814633 Classification-F1 0.49035702282213905 on epoch=174
05/21/2022 13:44:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.910213 on epoch=177
05/21/2022 13:44:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.816722 on epoch=179
05/21/2022 13:44:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.833058 on epoch=182
05/21/2022 13:44:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.689816 on epoch=184
05/21/2022 13:44:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.696447 on epoch=187
05/21/2022 13:44:35 - INFO - __main__ - Global step 750 Train loss 0.789251 Classification-F1 0.48712185668368313 on epoch=187
05/21/2022 13:44:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.592616 on epoch=189
05/21/2022 13:44:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.864324 on epoch=192
05/21/2022 13:44:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.559343 on epoch=194
05/21/2022 13:44:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.893907 on epoch=197
05/21/2022 13:44:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.747942 on epoch=199
05/21/2022 13:44:47 - INFO - __main__ - Global step 800 Train loss 0.731626 Classification-F1 0.5108199765836328 on epoch=199
05/21/2022 13:44:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.800647 on epoch=202
05/21/2022 13:44:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.646667 on epoch=204
05/21/2022 13:44:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.594392 on epoch=207
05/21/2022 13:44:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.671640 on epoch=209
05/21/2022 13:45:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.535332 on epoch=212
05/21/2022 13:45:00 - INFO - __main__ - Global step 850 Train loss 0.649735 Classification-F1 0.6062544326241135 on epoch=212
05/21/2022 13:45:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.597369 on epoch=214
05/21/2022 13:45:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.536081 on epoch=217
05/21/2022 13:45:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.576029 on epoch=219
05/21/2022 13:45:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.849547 on epoch=222
05/21/2022 13:45:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.717382 on epoch=224
05/21/2022 13:45:13 - INFO - __main__ - Global step 900 Train loss 0.655281 Classification-F1 0.6027131782945736 on epoch=224
05/21/2022 13:45:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.729792 on epoch=227
05/21/2022 13:45:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.725929 on epoch=229
05/21/2022 13:45:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.756136 on epoch=232
05/21/2022 13:45:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.630794 on epoch=234
05/21/2022 13:45:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.637106 on epoch=237
05/21/2022 13:45:26 - INFO - __main__ - Global step 950 Train loss 0.695951 Classification-F1 0.6367886178861788 on epoch=237
05/21/2022 13:45:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.749809 on epoch=239
05/21/2022 13:45:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.711063 on epoch=242
05/21/2022 13:45:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.581397 on epoch=244
05/21/2022 13:45:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.830538 on epoch=247
05/21/2022 13:45:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.544927 on epoch=249
05/21/2022 13:45:39 - INFO - __main__ - Global step 1000 Train loss 0.683547 Classification-F1 0.6308148446785011 on epoch=249
05/21/2022 13:45:39 - INFO - __main__ - save last model!
05/21/2022 13:45:40 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:45:40 - INFO - __main__ - Printing 3 examples
05/21/2022 13:45:40 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 13:45:40 - INFO - __main__ - ['sad']
05/21/2022 13:45:40 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 13:45:40 - INFO - __main__ - ['sad']
05/21/2022 13:45:40 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 13:45:40 - INFO - __main__ - ['sad']
05/21/2022 13:45:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:45:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:45:40 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:45:40 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:45:40 - INFO - __main__ - Printing 3 examples
05/21/2022 13:45:40 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/21/2022 13:45:40 - INFO - __main__ - ['sad']
05/21/2022 13:45:40 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/21/2022 13:45:40 - INFO - __main__ - ['sad']
05/21/2022 13:45:40 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/21/2022 13:45:40 - INFO - __main__ - ['sad']
05/21/2022 13:45:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:45:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:45:40 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:45:42 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 13:45:42 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 13:45:42 - INFO - __main__ - Printing 3 examples
05/21/2022 13:45:42 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 13:45:42 - INFO - __main__ - ['others']
05/21/2022 13:45:42 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 13:45:42 - INFO - __main__ - ['others']
05/21/2022 13:45:42 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 13:45:42 - INFO - __main__ - ['others']
05/21/2022 13:45:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:45:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:45:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:45:44 - INFO - __main__ - Starting training!
05/21/2022 13:45:50 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 13:46:18 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_21_0.0003_8_predictions.txt
05/21/2022 13:46:18 - INFO - __main__ - Classification-F1 on test data: 0.2801
05/21/2022 13:46:18 - INFO - __main__ - prefix=emo_16_21, lr=0.0003, bsz=8, dev_performance=0.6367886178861788, test_performance=0.2801058667883809
05/21/2022 13:46:18 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.0002, bsz=8 ...
05/21/2022 13:46:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:46:19 - INFO - __main__ - Printing 3 examples
05/21/2022 13:46:19 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 13:46:19 - INFO - __main__ - ['sad']
05/21/2022 13:46:19 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 13:46:19 - INFO - __main__ - ['sad']
05/21/2022 13:46:19 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 13:46:19 - INFO - __main__ - ['sad']
05/21/2022 13:46:19 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:46:19 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:46:19 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:46:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:46:19 - INFO - __main__ - Printing 3 examples
05/21/2022 13:46:19 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/21/2022 13:46:19 - INFO - __main__ - ['sad']
05/21/2022 13:46:19 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/21/2022 13:46:19 - INFO - __main__ - ['sad']
05/21/2022 13:46:19 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/21/2022 13:46:19 - INFO - __main__ - ['sad']
05/21/2022 13:46:19 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:46:19 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:46:19 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:46:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:46:23 - INFO - __main__ - Starting training!
05/21/2022 13:46:25 - INFO - __main__ - Step 10 Global step 10 Train loss 20.100163 on epoch=2
05/21/2022 13:46:27 - INFO - __main__ - Step 20 Global step 20 Train loss 19.629200 on epoch=4
05/21/2022 13:46:30 - INFO - __main__ - Step 30 Global step 30 Train loss 13.670237 on epoch=7
05/21/2022 13:46:33 - INFO - __main__ - Step 40 Global step 40 Train loss 9.059789 on epoch=9
05/21/2022 13:46:35 - INFO - __main__ - Step 50 Global step 50 Train loss 7.338145 on epoch=12
05/21/2022 13:46:41 - INFO - __main__ - Global step 50 Train loss 13.959508 Classification-F1 0.0 on epoch=12
05/21/2022 13:46:44 - INFO - __main__ - Step 60 Global step 60 Train loss 7.369569 on epoch=14
05/21/2022 13:46:46 - INFO - __main__ - Step 70 Global step 70 Train loss 5.025889 on epoch=17
05/21/2022 13:46:49 - INFO - __main__ - Step 80 Global step 80 Train loss 5.380620 on epoch=19
05/21/2022 13:46:52 - INFO - __main__ - Step 90 Global step 90 Train loss 5.298578 on epoch=22
05/21/2022 13:46:54 - INFO - __main__ - Step 100 Global step 100 Train loss 4.455369 on epoch=24
05/21/2022 13:46:54 - INFO - __main__ - Global step 100 Train loss 5.506005 Classification-F1 0.13067758749069247 on epoch=24
05/21/2022 13:46:57 - INFO - __main__ - Step 110 Global step 110 Train loss 4.369081 on epoch=27
05/21/2022 13:47:00 - INFO - __main__ - Step 120 Global step 120 Train loss 2.806982 on epoch=29
05/21/2022 13:47:03 - INFO - __main__ - Step 130 Global step 130 Train loss 3.024277 on epoch=32
05/21/2022 13:47:05 - INFO - __main__ - Step 140 Global step 140 Train loss 2.950846 on epoch=34
05/21/2022 13:47:08 - INFO - __main__ - Step 150 Global step 150 Train loss 3.145061 on epoch=37
05/21/2022 13:47:08 - INFO - __main__ - Global step 150 Train loss 3.259250 Classification-F1 0.29677287581699346 on epoch=37
05/21/2022 13:47:11 - INFO - __main__ - Step 160 Global step 160 Train loss 2.680719 on epoch=39
05/21/2022 13:47:13 - INFO - __main__ - Step 170 Global step 170 Train loss 3.056741 on epoch=42
05/21/2022 13:47:16 - INFO - __main__ - Step 180 Global step 180 Train loss 2.567475 on epoch=44
05/21/2022 13:47:19 - INFO - __main__ - Step 190 Global step 190 Train loss 1.885105 on epoch=47
05/21/2022 13:47:21 - INFO - __main__ - Step 200 Global step 200 Train loss 1.578506 on epoch=49
05/21/2022 13:47:21 - INFO - __main__ - Global step 200 Train loss 2.353709 Classification-F1 0.40875420875420876 on epoch=49
05/21/2022 13:47:24 - INFO - __main__ - Step 210 Global step 210 Train loss 2.053296 on epoch=52
05/21/2022 13:47:27 - INFO - __main__ - Step 220 Global step 220 Train loss 1.686476 on epoch=54
05/21/2022 13:47:29 - INFO - __main__ - Step 230 Global step 230 Train loss 1.877031 on epoch=57
05/21/2022 13:47:32 - INFO - __main__ - Step 240 Global step 240 Train loss 1.352020 on epoch=59
05/21/2022 13:47:35 - INFO - __main__ - Step 250 Global step 250 Train loss 2.609302 on epoch=62
05/21/2022 13:47:35 - INFO - __main__ - Global step 250 Train loss 1.915625 Classification-F1 0.5041666666666667 on epoch=62
05/21/2022 13:47:38 - INFO - __main__ - Step 260 Global step 260 Train loss 1.483849 on epoch=64
05/21/2022 13:47:40 - INFO - __main__ - Step 270 Global step 270 Train loss 1.279468 on epoch=67
05/21/2022 13:47:43 - INFO - __main__ - Step 280 Global step 280 Train loss 1.148498 on epoch=69
05/21/2022 13:47:45 - INFO - __main__ - Step 290 Global step 290 Train loss 1.447327 on epoch=72
05/21/2022 13:47:48 - INFO - __main__ - Step 300 Global step 300 Train loss 1.084869 on epoch=74
05/21/2022 13:47:48 - INFO - __main__ - Global step 300 Train loss 1.288802 Classification-F1 0.3560337676279705 on epoch=74
05/21/2022 13:47:51 - INFO - __main__ - Step 310 Global step 310 Train loss 1.511087 on epoch=77
05/21/2022 13:47:53 - INFO - __main__ - Step 320 Global step 320 Train loss 1.499586 on epoch=79
05/21/2022 13:47:56 - INFO - __main__ - Step 330 Global step 330 Train loss 1.292119 on epoch=82
05/21/2022 13:47:58 - INFO - __main__ - Step 340 Global step 340 Train loss 1.163925 on epoch=84
05/21/2022 13:48:01 - INFO - __main__ - Step 350 Global step 350 Train loss 1.418771 on epoch=87
05/21/2022 13:48:01 - INFO - __main__ - Global step 350 Train loss 1.377098 Classification-F1 0.5162123912123912 on epoch=87
05/21/2022 13:48:04 - INFO - __main__ - Step 360 Global step 360 Train loss 1.259712 on epoch=89
05/21/2022 13:48:07 - INFO - __main__ - Step 370 Global step 370 Train loss 1.402653 on epoch=92
05/21/2022 13:48:09 - INFO - __main__ - Step 380 Global step 380 Train loss 1.110514 on epoch=94
05/21/2022 13:48:12 - INFO - __main__ - Step 390 Global step 390 Train loss 1.153371 on epoch=97
05/21/2022 13:48:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.864107 on epoch=99
05/21/2022 13:48:15 - INFO - __main__ - Global step 400 Train loss 1.158071 Classification-F1 0.5367923420249001 on epoch=99
05/21/2022 13:48:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.923108 on epoch=102
05/21/2022 13:48:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.842492 on epoch=104
05/21/2022 13:48:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.964268 on epoch=107
05/21/2022 13:48:25 - INFO - __main__ - Step 440 Global step 440 Train loss 1.097848 on epoch=109
05/21/2022 13:48:28 - INFO - __main__ - Step 450 Global step 450 Train loss 1.217229 on epoch=112
05/21/2022 13:48:28 - INFO - __main__ - Global step 450 Train loss 1.008989 Classification-F1 0.5613860115714819 on epoch=112
05/21/2022 13:48:31 - INFO - __main__ - Step 460 Global step 460 Train loss 1.187210 on epoch=114
05/21/2022 13:48:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.997128 on epoch=117
05/21/2022 13:48:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.828092 on epoch=119
05/21/2022 13:48:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.816583 on epoch=122
05/21/2022 13:48:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.781717 on epoch=124
05/21/2022 13:48:42 - INFO - __main__ - Global step 500 Train loss 0.922146 Classification-F1 0.6303370049829531 on epoch=124
05/21/2022 13:48:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.776398 on epoch=127
05/21/2022 13:48:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.701622 on epoch=129
05/21/2022 13:48:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.783487 on epoch=132
05/21/2022 13:48:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.768452 on epoch=134
05/21/2022 13:48:55 - INFO - __main__ - Step 550 Global step 550 Train loss 1.148015 on epoch=137
05/21/2022 13:48:55 - INFO - __main__ - Global step 550 Train loss 0.835595 Classification-F1 0.6550894720706874 on epoch=137
05/21/2022 13:48:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.862154 on epoch=139
05/21/2022 13:49:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.754632 on epoch=142
05/21/2022 13:49:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.669875 on epoch=144
05/21/2022 13:49:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.489407 on epoch=147
05/21/2022 13:49:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.501974 on epoch=149
05/21/2022 13:49:08 - INFO - __main__ - Global step 600 Train loss 0.655608 Classification-F1 0.6994824016563147 on epoch=149
05/21/2022 13:49:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.596336 on epoch=152
05/21/2022 13:49:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.534039 on epoch=154
05/21/2022 13:49:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.600553 on epoch=157
05/21/2022 13:49:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.625915 on epoch=159
05/21/2022 13:49:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.596628 on epoch=162
05/21/2022 13:49:22 - INFO - __main__ - Global step 650 Train loss 0.590694 Classification-F1 0.670477114074675 on epoch=162
05/21/2022 13:49:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.657367 on epoch=164
05/21/2022 13:49:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.628239 on epoch=167
05/21/2022 13:49:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.578091 on epoch=169
05/21/2022 13:49:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.504765 on epoch=172
05/21/2022 13:49:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.389935 on epoch=174
05/21/2022 13:49:35 - INFO - __main__ - Global step 700 Train loss 0.551679 Classification-F1 0.6633500907694456 on epoch=174
05/21/2022 13:49:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.765813 on epoch=177
05/21/2022 13:49:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.558071 on epoch=179
05/21/2022 13:49:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.464288 on epoch=182
05/21/2022 13:49:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.377592 on epoch=184
05/21/2022 13:49:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.569866 on epoch=187
05/21/2022 13:49:48 - INFO - __main__ - Global step 750 Train loss 0.547126 Classification-F1 0.720343137254902 on epoch=187
05/21/2022 13:49:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.574777 on epoch=189
05/21/2022 13:49:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.471907 on epoch=192
05/21/2022 13:49:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.440054 on epoch=194
05/21/2022 13:49:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.400138 on epoch=197
05/21/2022 13:50:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.334817 on epoch=199
05/21/2022 13:50:02 - INFO - __main__ - Global step 800 Train loss 0.444339 Classification-F1 0.7208333333333333 on epoch=199
05/21/2022 13:50:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.429302 on epoch=202
05/21/2022 13:50:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.220161 on epoch=204
05/21/2022 13:50:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.349655 on epoch=207
05/21/2022 13:50:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.268181 on epoch=209
05/21/2022 13:50:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.359188 on epoch=212
05/21/2022 13:50:15 - INFO - __main__ - Global step 850 Train loss 0.325298 Classification-F1 0.6695152423788105 on epoch=212
05/21/2022 13:50:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.315140 on epoch=214
05/21/2022 13:50:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.297918 on epoch=217
05/21/2022 13:50:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.230461 on epoch=219
05/21/2022 13:50:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.122599 on epoch=222
05/21/2022 13:50:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.407559 on epoch=224
05/21/2022 13:50:28 - INFO - __main__ - Global step 900 Train loss 0.274735 Classification-F1 0.6478387746259907 on epoch=224
05/21/2022 13:50:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.145988 on epoch=227
05/21/2022 13:50:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.280541 on epoch=229
05/21/2022 13:50:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.239354 on epoch=232
05/21/2022 13:50:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.323456 on epoch=234
05/21/2022 13:50:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.183145 on epoch=237
05/21/2022 13:50:41 - INFO - __main__ - Global step 950 Train loss 0.234497 Classification-F1 0.6482123189019741 on epoch=237
05/21/2022 13:50:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.259252 on epoch=239
05/21/2022 13:50:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.124086 on epoch=242
05/21/2022 13:50:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.204642 on epoch=244
05/21/2022 13:50:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.318765 on epoch=247
05/21/2022 13:50:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.294517 on epoch=249
05/21/2022 13:50:55 - INFO - __main__ - Global step 1000 Train loss 0.240252 Classification-F1 0.6624514966740576 on epoch=249
05/21/2022 13:50:55 - INFO - __main__ - save last model!
05/21/2022 13:50:55 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:50:55 - INFO - __main__ - Printing 3 examples
05/21/2022 13:50:55 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 13:50:55 - INFO - __main__ - ['sad']
05/21/2022 13:50:55 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 13:50:55 - INFO - __main__ - ['sad']
05/21/2022 13:50:55 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 13:50:55 - INFO - __main__ - ['sad']
05/21/2022 13:50:55 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:50:55 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:50:55 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:50:55 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:50:55 - INFO - __main__ - Printing 3 examples
05/21/2022 13:50:55 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/21/2022 13:50:55 - INFO - __main__ - ['sad']
05/21/2022 13:50:55 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/21/2022 13:50:55 - INFO - __main__ - ['sad']
05/21/2022 13:50:55 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/21/2022 13:50:55 - INFO - __main__ - ['sad']
05/21/2022 13:50:55 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:50:55 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:50:55 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:50:57 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 13:50:57 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 13:50:57 - INFO - __main__ - Printing 3 examples
05/21/2022 13:50:57 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 13:50:57 - INFO - __main__ - ['others']
05/21/2022 13:50:57 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 13:50:57 - INFO - __main__ - ['others']
05/21/2022 13:50:57 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 13:50:57 - INFO - __main__ - ['others']
05/21/2022 13:50:57 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:50:59 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:51:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:51:00 - INFO - __main__ - Starting training!
05/21/2022 13:51:05 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 13:51:33 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_21_0.0002_8_predictions.txt
05/21/2022 13:51:33 - INFO - __main__ - Classification-F1 on test data: 0.3653
05/21/2022 13:51:33 - INFO - __main__ - prefix=emo_16_21, lr=0.0002, bsz=8, dev_performance=0.7208333333333333, test_performance=0.36529631320209033
05/21/2022 13:51:33 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.0001, bsz=8 ...
05/21/2022 13:51:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:51:34 - INFO - __main__ - Printing 3 examples
05/21/2022 13:51:34 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/21/2022 13:51:34 - INFO - __main__ - ['sad']
05/21/2022 13:51:34 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/21/2022 13:51:34 - INFO - __main__ - ['sad']
05/21/2022 13:51:34 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/21/2022 13:51:34 - INFO - __main__ - ['sad']
05/21/2022 13:51:34 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:51:34 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:51:34 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:51:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:51:34 - INFO - __main__ - Printing 3 examples
05/21/2022 13:51:34 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/21/2022 13:51:34 - INFO - __main__ - ['sad']
05/21/2022 13:51:34 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/21/2022 13:51:34 - INFO - __main__ - ['sad']
05/21/2022 13:51:34 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/21/2022 13:51:34 - INFO - __main__ - ['sad']
05/21/2022 13:51:34 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:51:34 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:51:34 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:51:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:51:38 - INFO - __main__ - Starting training!
05/21/2022 13:51:40 - INFO - __main__ - Step 10 Global step 10 Train loss 19.755779 on epoch=2
05/21/2022 13:51:42 - INFO - __main__ - Step 20 Global step 20 Train loss 19.241074 on epoch=4
05/21/2022 13:51:45 - INFO - __main__ - Step 30 Global step 30 Train loss 15.643690 on epoch=7
05/21/2022 13:51:47 - INFO - __main__ - Step 40 Global step 40 Train loss 13.955988 on epoch=9
05/21/2022 13:51:50 - INFO - __main__ - Step 50 Global step 50 Train loss 11.749243 on epoch=12
05/21/2022 13:52:02 - INFO - __main__ - Global step 50 Train loss 16.069155 Classification-F1 0.0 on epoch=12
05/21/2022 13:52:05 - INFO - __main__ - Step 60 Global step 60 Train loss 10.142225 on epoch=14
05/21/2022 13:52:07 - INFO - __main__ - Step 70 Global step 70 Train loss 8.623484 on epoch=17
05/21/2022 13:52:10 - INFO - __main__ - Step 80 Global step 80 Train loss 9.197367 on epoch=19
05/21/2022 13:52:13 - INFO - __main__ - Step 90 Global step 90 Train loss 8.327868 on epoch=22
05/21/2022 13:52:15 - INFO - __main__ - Step 100 Global step 100 Train loss 7.411546 on epoch=24
05/21/2022 13:52:23 - INFO - __main__ - Global step 100 Train loss 8.740498 Classification-F1 0.0 on epoch=24
05/21/2022 13:52:26 - INFO - __main__ - Step 110 Global step 110 Train loss 8.034338 on epoch=27
05/21/2022 13:52:28 - INFO - __main__ - Step 120 Global step 120 Train loss 6.488782 on epoch=29
05/21/2022 13:52:31 - INFO - __main__ - Step 130 Global step 130 Train loss 6.402215 on epoch=32
05/21/2022 13:52:33 - INFO - __main__ - Step 140 Global step 140 Train loss 5.443925 on epoch=34
05/21/2022 13:52:36 - INFO - __main__ - Step 150 Global step 150 Train loss 5.412324 on epoch=37
05/21/2022 13:52:37 - INFO - __main__ - Global step 150 Train loss 6.356317 Classification-F1 0.06837606837606837 on epoch=37
05/21/2022 13:52:40 - INFO - __main__ - Step 160 Global step 160 Train loss 5.170585 on epoch=39
05/21/2022 13:52:42 - INFO - __main__ - Step 170 Global step 170 Train loss 4.212664 on epoch=42
05/21/2022 13:52:45 - INFO - __main__ - Step 180 Global step 180 Train loss 5.009474 on epoch=44
05/21/2022 13:52:47 - INFO - __main__ - Step 190 Global step 190 Train loss 4.227974 on epoch=47
05/21/2022 13:52:50 - INFO - __main__ - Step 200 Global step 200 Train loss 4.567096 on epoch=49
05/21/2022 13:52:50 - INFO - __main__ - Global step 200 Train loss 4.637559 Classification-F1 0.1 on epoch=49
05/21/2022 13:52:53 - INFO - __main__ - Step 210 Global step 210 Train loss 3.770324 on epoch=52
05/21/2022 13:52:56 - INFO - __main__ - Step 220 Global step 220 Train loss 3.499950 on epoch=54
05/21/2022 13:52:58 - INFO - __main__ - Step 230 Global step 230 Train loss 3.325100 on epoch=57
05/21/2022 13:53:01 - INFO - __main__ - Step 240 Global step 240 Train loss 3.688211 on epoch=59
05/21/2022 13:53:03 - INFO - __main__ - Step 250 Global step 250 Train loss 3.585313 on epoch=62
05/21/2022 13:53:04 - INFO - __main__ - Global step 250 Train loss 3.573780 Classification-F1 0.1 on epoch=62
05/21/2022 13:53:06 - INFO - __main__ - Step 260 Global step 260 Train loss 2.848369 on epoch=64
05/21/2022 13:53:09 - INFO - __main__ - Step 270 Global step 270 Train loss 3.211064 on epoch=67
05/21/2022 13:53:12 - INFO - __main__ - Step 280 Global step 280 Train loss 3.380081 on epoch=69
05/21/2022 13:53:14 - INFO - __main__ - Step 290 Global step 290 Train loss 3.419387 on epoch=72
05/21/2022 13:53:17 - INFO - __main__ - Step 300 Global step 300 Train loss 2.679830 on epoch=74
05/21/2022 13:53:17 - INFO - __main__ - Global step 300 Train loss 3.107746 Classification-F1 0.1581196581196581 on epoch=74
05/21/2022 13:53:20 - INFO - __main__ - Step 310 Global step 310 Train loss 3.222391 on epoch=77
05/21/2022 13:53:22 - INFO - __main__ - Step 320 Global step 320 Train loss 2.966119 on epoch=79
05/21/2022 13:53:25 - INFO - __main__ - Step 330 Global step 330 Train loss 2.941418 on epoch=82
05/21/2022 13:53:27 - INFO - __main__ - Step 340 Global step 340 Train loss 2.636261 on epoch=84
05/21/2022 13:53:30 - INFO - __main__ - Step 350 Global step 350 Train loss 3.219789 on epoch=87
05/21/2022 13:53:30 - INFO - __main__ - Global step 350 Train loss 2.997195 Classification-F1 0.2510177984237173 on epoch=87
05/21/2022 13:53:33 - INFO - __main__ - Step 360 Global step 360 Train loss 2.822792 on epoch=89
05/21/2022 13:53:36 - INFO - __main__ - Step 370 Global step 370 Train loss 2.707325 on epoch=92
05/21/2022 13:53:38 - INFO - __main__ - Step 380 Global step 380 Train loss 2.638124 on epoch=94
05/21/2022 13:53:41 - INFO - __main__ - Step 390 Global step 390 Train loss 2.280257 on epoch=97
05/21/2022 13:53:44 - INFO - __main__ - Step 400 Global step 400 Train loss 2.754313 on epoch=99
05/21/2022 13:53:44 - INFO - __main__ - Global step 400 Train loss 2.640562 Classification-F1 0.2558061821219716 on epoch=99
05/21/2022 13:53:47 - INFO - __main__ - Step 410 Global step 410 Train loss 2.478922 on epoch=102
05/21/2022 13:53:49 - INFO - __main__ - Step 420 Global step 420 Train loss 2.431525 on epoch=104
05/21/2022 13:53:52 - INFO - __main__ - Step 430 Global step 430 Train loss 1.857792 on epoch=107
05/21/2022 13:53:54 - INFO - __main__ - Step 440 Global step 440 Train loss 2.642199 on epoch=109
05/21/2022 13:53:57 - INFO - __main__ - Step 450 Global step 450 Train loss 2.352322 on epoch=112
05/21/2022 13:53:57 - INFO - __main__ - Global step 450 Train loss 2.352552 Classification-F1 0.26366366366366367 on epoch=112
05/21/2022 13:54:00 - INFO - __main__ - Step 460 Global step 460 Train loss 2.723093 on epoch=114
05/21/2022 13:54:03 - INFO - __main__ - Step 470 Global step 470 Train loss 2.285610 on epoch=117
05/21/2022 13:54:05 - INFO - __main__ - Step 480 Global step 480 Train loss 2.412215 on epoch=119
05/21/2022 13:54:08 - INFO - __main__ - Step 490 Global step 490 Train loss 1.694419 on epoch=122
05/21/2022 13:54:10 - INFO - __main__ - Step 500 Global step 500 Train loss 2.073453 on epoch=124
05/21/2022 13:54:11 - INFO - __main__ - Global step 500 Train loss 2.237758 Classification-F1 0.30525978839544443 on epoch=124
05/21/2022 13:54:14 - INFO - __main__ - Step 510 Global step 510 Train loss 2.000691 on epoch=127
05/21/2022 13:54:16 - INFO - __main__ - Step 520 Global step 520 Train loss 1.658429 on epoch=129
05/21/2022 13:54:19 - INFO - __main__ - Step 530 Global step 530 Train loss 1.818279 on epoch=132
05/21/2022 13:54:21 - INFO - __main__ - Step 540 Global step 540 Train loss 1.877867 on epoch=134
05/21/2022 13:54:24 - INFO - __main__ - Step 550 Global step 550 Train loss 2.057478 on epoch=137
05/21/2022 13:54:24 - INFO - __main__ - Global step 550 Train loss 1.882549 Classification-F1 0.3272519534129441 on epoch=137
05/21/2022 13:54:27 - INFO - __main__ - Step 560 Global step 560 Train loss 1.532252 on epoch=139
05/21/2022 13:54:30 - INFO - __main__ - Step 570 Global step 570 Train loss 1.484308 on epoch=142
05/21/2022 13:54:32 - INFO - __main__ - Step 580 Global step 580 Train loss 1.942111 on epoch=144
05/21/2022 13:54:35 - INFO - __main__ - Step 590 Global step 590 Train loss 1.771497 on epoch=147
05/21/2022 13:54:37 - INFO - __main__ - Step 600 Global step 600 Train loss 1.532775 on epoch=149
05/21/2022 13:54:38 - INFO - __main__ - Global step 600 Train loss 1.652589 Classification-F1 0.39290322580645165 on epoch=149
05/21/2022 13:54:41 - INFO - __main__ - Step 610 Global step 610 Train loss 1.578401 on epoch=152
05/21/2022 13:54:43 - INFO - __main__ - Step 620 Global step 620 Train loss 1.671585 on epoch=154
05/21/2022 13:54:46 - INFO - __main__ - Step 630 Global step 630 Train loss 1.830785 on epoch=157
05/21/2022 13:54:48 - INFO - __main__ - Step 640 Global step 640 Train loss 1.666192 on epoch=159
05/21/2022 13:54:51 - INFO - __main__ - Step 650 Global step 650 Train loss 1.546575 on epoch=162
05/21/2022 13:54:51 - INFO - __main__ - Global step 650 Train loss 1.658707 Classification-F1 0.41314814814814815 on epoch=162
05/21/2022 13:54:54 - INFO - __main__ - Step 660 Global step 660 Train loss 1.579933 on epoch=164
05/21/2022 13:54:57 - INFO - __main__ - Step 670 Global step 670 Train loss 1.370762 on epoch=167
05/21/2022 13:54:59 - INFO - __main__ - Step 680 Global step 680 Train loss 1.790149 on epoch=169
05/21/2022 13:55:02 - INFO - __main__ - Step 690 Global step 690 Train loss 1.360026 on epoch=172
05/21/2022 13:55:04 - INFO - __main__ - Step 700 Global step 700 Train loss 1.110333 on epoch=174
05/21/2022 13:55:05 - INFO - __main__ - Global step 700 Train loss 1.442241 Classification-F1 0.4455498502353445 on epoch=174
05/21/2022 13:55:08 - INFO - __main__ - Step 710 Global step 710 Train loss 1.575413 on epoch=177
05/21/2022 13:55:10 - INFO - __main__ - Step 720 Global step 720 Train loss 1.419692 on epoch=179
05/21/2022 13:55:13 - INFO - __main__ - Step 730 Global step 730 Train loss 1.504499 on epoch=182
05/21/2022 13:55:15 - INFO - __main__ - Step 740 Global step 740 Train loss 1.563395 on epoch=184
05/21/2022 13:55:18 - INFO - __main__ - Step 750 Global step 750 Train loss 1.627927 on epoch=187
05/21/2022 13:55:18 - INFO - __main__ - Global step 750 Train loss 1.538185 Classification-F1 0.42261904761904767 on epoch=187
05/21/2022 13:55:21 - INFO - __main__ - Step 760 Global step 760 Train loss 1.188854 on epoch=189
05/21/2022 13:55:23 - INFO - __main__ - Step 770 Global step 770 Train loss 1.249391 on epoch=192
05/21/2022 13:55:26 - INFO - __main__ - Step 780 Global step 780 Train loss 1.399119 on epoch=194
05/21/2022 13:55:29 - INFO - __main__ - Step 790 Global step 790 Train loss 1.205890 on epoch=197
05/21/2022 13:55:31 - INFO - __main__ - Step 800 Global step 800 Train loss 1.008931 on epoch=199
05/21/2022 13:55:31 - INFO - __main__ - Global step 800 Train loss 1.210437 Classification-F1 0.45483641536273123 on epoch=199
05/21/2022 13:55:34 - INFO - __main__ - Step 810 Global step 810 Train loss 1.474193 on epoch=202
05/21/2022 13:55:37 - INFO - __main__ - Step 820 Global step 820 Train loss 1.234307 on epoch=204
05/21/2022 13:55:40 - INFO - __main__ - Step 830 Global step 830 Train loss 1.137627 on epoch=207
05/21/2022 13:55:42 - INFO - __main__ - Step 840 Global step 840 Train loss 1.530260 on epoch=209
05/21/2022 13:55:45 - INFO - __main__ - Step 850 Global step 850 Train loss 1.110270 on epoch=212
05/21/2022 13:55:45 - INFO - __main__ - Global step 850 Train loss 1.297331 Classification-F1 0.4335649461145774 on epoch=212
05/21/2022 13:55:48 - INFO - __main__ - Step 860 Global step 860 Train loss 1.196779 on epoch=214
05/21/2022 13:55:50 - INFO - __main__ - Step 870 Global step 870 Train loss 1.248519 on epoch=217
05/21/2022 13:55:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.975803 on epoch=219
05/21/2022 13:55:55 - INFO - __main__ - Step 890 Global step 890 Train loss 1.223896 on epoch=222
05/21/2022 13:55:58 - INFO - __main__ - Step 900 Global step 900 Train loss 1.016754 on epoch=224
05/21/2022 13:55:58 - INFO - __main__ - Global step 900 Train loss 1.132350 Classification-F1 0.5080781287677839 on epoch=224
05/21/2022 13:56:01 - INFO - __main__ - Step 910 Global step 910 Train loss 1.388710 on epoch=227
05/21/2022 13:56:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.880289 on epoch=229
05/21/2022 13:56:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.851288 on epoch=232
05/21/2022 13:56:09 - INFO - __main__ - Step 940 Global step 940 Train loss 1.260135 on epoch=234
05/21/2022 13:56:11 - INFO - __main__ - Step 950 Global step 950 Train loss 1.258577 on epoch=237
05/21/2022 13:56:12 - INFO - __main__ - Global step 950 Train loss 1.127800 Classification-F1 0.446519261736653 on epoch=237
05/21/2022 13:56:14 - INFO - __main__ - Step 960 Global step 960 Train loss 1.024476 on epoch=239
05/21/2022 13:56:17 - INFO - __main__ - Step 970 Global step 970 Train loss 1.112403 on epoch=242
05/21/2022 13:56:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.875463 on epoch=244
05/21/2022 13:56:22 - INFO - __main__ - Step 990 Global step 990 Train loss 1.273531 on epoch=247
05/21/2022 13:56:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.745035 on epoch=249
05/21/2022 13:56:25 - INFO - __main__ - Global step 1000 Train loss 1.006182 Classification-F1 0.4746753246753247 on epoch=249
05/21/2022 13:56:25 - INFO - __main__ - save last model!
05/21/2022 13:56:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:56:26 - INFO - __main__ - Printing 3 examples
05/21/2022 13:56:26 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/21/2022 13:56:26 - INFO - __main__ - ['happy']
05/21/2022 13:56:26 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/21/2022 13:56:26 - INFO - __main__ - ['happy']
05/21/2022 13:56:26 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/21/2022 13:56:26 - INFO - __main__ - ['happy']
05/21/2022 13:56:26 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:56:26 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:56:26 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:56:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:56:26 - INFO - __main__ - Printing 3 examples
05/21/2022 13:56:26 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/21/2022 13:56:26 - INFO - __main__ - ['happy']
05/21/2022 13:56:26 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/21/2022 13:56:26 - INFO - __main__ - ['happy']
05/21/2022 13:56:26 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/21/2022 13:56:26 - INFO - __main__ - ['happy']
05/21/2022 13:56:26 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:56:26 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:56:26 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:56:28 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 13:56:28 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 13:56:28 - INFO - __main__ - Printing 3 examples
05/21/2022 13:56:28 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 13:56:28 - INFO - __main__ - ['others']
05/21/2022 13:56:28 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 13:56:28 - INFO - __main__ - ['others']
05/21/2022 13:56:28 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 13:56:28 - INFO - __main__ - ['others']
05/21/2022 13:56:28 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:56:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:56:30 - INFO - __main__ - Starting training!
05/21/2022 13:56:30 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:56:35 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 13:57:04 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_21_0.0001_8_predictions.txt
05/21/2022 13:57:04 - INFO - __main__ - Classification-F1 on test data: 0.1487
05/21/2022 13:57:04 - INFO - __main__ - prefix=emo_16_21, lr=0.0001, bsz=8, dev_performance=0.5080781287677839, test_performance=0.14871698132510858
05/21/2022 13:57:04 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.0005, bsz=8 ...
05/21/2022 13:57:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:57:05 - INFO - __main__ - Printing 3 examples
05/21/2022 13:57:05 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/21/2022 13:57:05 - INFO - __main__ - ['happy']
05/21/2022 13:57:05 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/21/2022 13:57:05 - INFO - __main__ - ['happy']
05/21/2022 13:57:05 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/21/2022 13:57:05 - INFO - __main__ - ['happy']
05/21/2022 13:57:05 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:57:05 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:57:05 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 13:57:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 13:57:05 - INFO - __main__ - Printing 3 examples
05/21/2022 13:57:05 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/21/2022 13:57:05 - INFO - __main__ - ['happy']
05/21/2022 13:57:05 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/21/2022 13:57:05 - INFO - __main__ - ['happy']
05/21/2022 13:57:05 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/21/2022 13:57:05 - INFO - __main__ - ['happy']
05/21/2022 13:57:05 - INFO - __main__ - Tokenizing Input ...
05/21/2022 13:57:05 - INFO - __main__ - Tokenizing Output ...
05/21/2022 13:57:05 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 13:57:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 13:57:09 - INFO - __main__ - Starting training!
05/21/2022 13:57:11 - INFO - __main__ - Step 10 Global step 10 Train loss 20.264221 on epoch=2
05/21/2022 13:57:13 - INFO - __main__ - Step 20 Global step 20 Train loss 14.392705 on epoch=4
05/21/2022 13:57:16 - INFO - __main__ - Step 30 Global step 30 Train loss 8.331667 on epoch=7
05/21/2022 13:57:18 - INFO - __main__ - Step 40 Global step 40 Train loss 5.445749 on epoch=9
05/21/2022 13:57:20 - INFO - __main__ - Step 50 Global step 50 Train loss 4.892551 on epoch=12
05/21/2022 13:57:21 - INFO - __main__ - Global step 50 Train loss 10.665380 Classification-F1 0.18026315789473685 on epoch=12
05/21/2022 13:57:24 - INFO - __main__ - Step 60 Global step 60 Train loss 3.349233 on epoch=14
05/21/2022 13:57:26 - INFO - __main__ - Step 70 Global step 70 Train loss 2.992596 on epoch=17
05/21/2022 13:57:29 - INFO - __main__ - Step 80 Global step 80 Train loss 2.858829 on epoch=19
05/21/2022 13:57:31 - INFO - __main__ - Step 90 Global step 90 Train loss 1.935583 on epoch=22
05/21/2022 13:57:33 - INFO - __main__ - Step 100 Global step 100 Train loss 1.629264 on epoch=24
05/21/2022 13:57:34 - INFO - __main__ - Global step 100 Train loss 2.553101 Classification-F1 0.48352490421455935 on epoch=24
05/21/2022 13:57:37 - INFO - __main__ - Step 110 Global step 110 Train loss 1.535683 on epoch=27
05/21/2022 13:57:39 - INFO - __main__ - Step 120 Global step 120 Train loss 1.191207 on epoch=29
05/21/2022 13:57:42 - INFO - __main__ - Step 130 Global step 130 Train loss 1.173822 on epoch=32
05/21/2022 13:57:44 - INFO - __main__ - Step 140 Global step 140 Train loss 1.251526 on epoch=34
05/21/2022 13:57:47 - INFO - __main__ - Step 150 Global step 150 Train loss 1.025765 on epoch=37
05/21/2022 13:57:47 - INFO - __main__ - Global step 150 Train loss 1.235600 Classification-F1 0.4749128919860627 on epoch=37
05/21/2022 13:57:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.662288 on epoch=39
05/21/2022 13:57:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.667425 on epoch=42
05/21/2022 13:57:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.902063 on epoch=44
05/21/2022 13:57:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.558931 on epoch=47
05/21/2022 13:57:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.705760 on epoch=49
05/21/2022 13:58:00 - INFO - __main__ - Global step 200 Train loss 0.699294 Classification-F1 0.6722221045496907 on epoch=49
05/21/2022 13:58:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.492723 on epoch=52
05/21/2022 13:58:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.529641 on epoch=54
05/21/2022 13:58:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.823445 on epoch=57
05/21/2022 13:58:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.332671 on epoch=59
05/21/2022 13:58:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.513666 on epoch=62
05/21/2022 13:58:13 - INFO - __main__ - Global step 250 Train loss 0.538429 Classification-F1 0.5863095238095238 on epoch=62
05/21/2022 13:58:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.338648 on epoch=64
05/21/2022 13:58:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.388402 on epoch=67
05/21/2022 13:58:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.130006 on epoch=69
05/21/2022 13:58:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.234143 on epoch=72
05/21/2022 13:58:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.241856 on epoch=74
05/21/2022 13:58:26 - INFO - __main__ - Global step 300 Train loss 0.266611 Classification-F1 0.6952838827838828 on epoch=74
05/21/2022 13:58:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.247113 on epoch=77
05/21/2022 13:58:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.090027 on epoch=79
05/21/2022 13:58:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.243122 on epoch=82
05/21/2022 13:58:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.195717 on epoch=84
05/21/2022 13:58:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.126308 on epoch=87
05/21/2022 13:58:39 - INFO - __main__ - Global step 350 Train loss 0.180457 Classification-F1 0.6263250595257509 on epoch=87
05/21/2022 13:58:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.019596 on epoch=89
05/21/2022 13:58:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.251808 on epoch=92
05/21/2022 13:58:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.133756 on epoch=94
05/21/2022 13:58:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.090180 on epoch=97
05/21/2022 13:58:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.193352 on epoch=99
05/21/2022 13:58:52 - INFO - __main__ - Global step 400 Train loss 0.137739 Classification-F1 0.7484106569027315 on epoch=99
05/21/2022 13:58:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.079012 on epoch=102
05/21/2022 13:58:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.060731 on epoch=104
05/21/2022 13:58:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.110782 on epoch=107
05/21/2022 13:59:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.100743 on epoch=109
05/21/2022 13:59:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.152835 on epoch=112
05/21/2022 13:59:05 - INFO - __main__ - Global step 450 Train loss 0.100821 Classification-F1 0.6512820512820513 on epoch=112
05/21/2022 13:59:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.357737 on epoch=114
05/21/2022 13:59:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.723699 on epoch=117
05/21/2022 13:59:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.652755 on epoch=119
05/21/2022 13:59:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.612207 on epoch=122
05/21/2022 13:59:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.805656 on epoch=124
05/21/2022 13:59:17 - INFO - __main__ - Global step 500 Train loss 0.630411 Classification-F1 0.5643551850448403 on epoch=124
05/21/2022 13:59:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.839459 on epoch=127
05/21/2022 13:59:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.977317 on epoch=129
05/21/2022 13:59:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.873265 on epoch=132
05/21/2022 13:59:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.980525 on epoch=134
05/21/2022 13:59:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.737887 on epoch=137
05/21/2022 13:59:30 - INFO - __main__ - Global step 550 Train loss 0.881690 Classification-F1 0.58495979036776 on epoch=137
05/21/2022 13:59:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.525554 on epoch=139
05/21/2022 13:59:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.743736 on epoch=142
05/21/2022 13:59:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.509744 on epoch=144
05/21/2022 13:59:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.255283 on epoch=147
05/21/2022 13:59:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.387336 on epoch=149
05/21/2022 13:59:43 - INFO - __main__ - Global step 600 Train loss 0.484331 Classification-F1 0.6409252100936157 on epoch=149
05/21/2022 13:59:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.236844 on epoch=152
05/21/2022 13:59:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.323434 on epoch=154
05/21/2022 13:59:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.388870 on epoch=157
05/21/2022 13:59:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.499382 on epoch=159
05/21/2022 13:59:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.336826 on epoch=162
05/21/2022 13:59:56 - INFO - __main__ - Global step 650 Train loss 0.357071 Classification-F1 0.5982448923625394 on epoch=162
05/21/2022 13:59:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.370708 on epoch=164
05/21/2022 14:00:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.286771 on epoch=167
05/21/2022 14:00:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.323509 on epoch=169
05/21/2022 14:00:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.202150 on epoch=172
05/21/2022 14:00:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.227996 on epoch=174
05/21/2022 14:00:08 - INFO - __main__ - Global step 700 Train loss 0.282227 Classification-F1 0.7311688311688312 on epoch=174
05/21/2022 14:00:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.238122 on epoch=177
05/21/2022 14:00:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.279879 on epoch=179
05/21/2022 14:00:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.333896 on epoch=182
05/21/2022 14:00:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.312711 on epoch=184
05/21/2022 14:00:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.158537 on epoch=187
05/21/2022 14:00:21 - INFO - __main__ - Global step 750 Train loss 0.264629 Classification-F1 0.668595825426945 on epoch=187
05/21/2022 14:00:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.254015 on epoch=189
05/21/2022 14:00:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.091273 on epoch=192
05/21/2022 14:00:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.083430 on epoch=194
05/21/2022 14:00:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.150847 on epoch=197
05/21/2022 14:00:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.125498 on epoch=199
05/21/2022 14:00:34 - INFO - __main__ - Global step 800 Train loss 0.141013 Classification-F1 0.7018558378958824 on epoch=199
05/21/2022 14:00:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.087563 on epoch=202
05/21/2022 14:00:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.047447 on epoch=204
05/21/2022 14:00:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.125231 on epoch=207
05/21/2022 14:00:44 - INFO - __main__ - Step 840 Global step 840 Train loss 0.138863 on epoch=209
05/21/2022 14:00:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.108718 on epoch=212
05/21/2022 14:00:47 - INFO - __main__ - Global step 850 Train loss 0.101564 Classification-F1 0.6759816333092195 on epoch=212
05/21/2022 14:00:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.024934 on epoch=214
05/21/2022 14:00:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.030803 on epoch=217
05/21/2022 14:00:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.271214 on epoch=219
05/21/2022 14:00:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.064350 on epoch=222
05/21/2022 14:00:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.045808 on epoch=224
05/21/2022 14:01:00 - INFO - __main__ - Global step 900 Train loss 0.087422 Classification-F1 0.5325924075924076 on epoch=224
05/21/2022 14:01:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.058006 on epoch=227
05/21/2022 14:01:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.077948 on epoch=229
05/21/2022 14:01:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.105898 on epoch=232
05/21/2022 14:01:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.022756 on epoch=234
05/21/2022 14:01:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.129310 on epoch=237
05/21/2022 14:01:13 - INFO - __main__ - Global step 950 Train loss 0.078784 Classification-F1 0.6440590659340659 on epoch=237
05/21/2022 14:01:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.130607 on epoch=239
05/21/2022 14:01:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.089322 on epoch=242
05/21/2022 14:01:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.009588 on epoch=244
05/21/2022 14:01:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.114843 on epoch=247
05/21/2022 14:01:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.099221 on epoch=249
05/21/2022 14:01:26 - INFO - __main__ - Global step 1000 Train loss 0.088716 Classification-F1 0.695234242109242 on epoch=249
05/21/2022 14:01:26 - INFO - __main__ - save last model!
05/21/2022 14:01:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:01:26 - INFO - __main__ - Printing 3 examples
05/21/2022 14:01:26 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/21/2022 14:01:26 - INFO - __main__ - ['happy']
05/21/2022 14:01:26 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/21/2022 14:01:26 - INFO - __main__ - ['happy']
05/21/2022 14:01:26 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/21/2022 14:01:26 - INFO - __main__ - ['happy']
05/21/2022 14:01:26 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:01:26 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:01:26 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 14:01:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:01:26 - INFO - __main__ - Printing 3 examples
05/21/2022 14:01:26 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/21/2022 14:01:26 - INFO - __main__ - ['happy']
05/21/2022 14:01:26 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/21/2022 14:01:26 - INFO - __main__ - ['happy']
05/21/2022 14:01:26 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/21/2022 14:01:26 - INFO - __main__ - ['happy']
05/21/2022 14:01:26 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:01:26 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:01:26 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 14:01:28 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 14:01:28 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 14:01:28 - INFO - __main__ - Printing 3 examples
05/21/2022 14:01:28 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 14:01:28 - INFO - __main__ - ['others']
05/21/2022 14:01:28 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 14:01:28 - INFO - __main__ - ['others']
05/21/2022 14:01:28 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 14:01:28 - INFO - __main__ - ['others']
05/21/2022 14:01:28 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:01:30 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:01:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 14:01:31 - INFO - __main__ - Starting training!
05/21/2022 14:01:36 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 14:02:09 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_42_0.0005_8_predictions.txt
05/21/2022 14:02:09 - INFO - __main__ - Classification-F1 on test data: 0.3336
05/21/2022 14:02:09 - INFO - __main__ - prefix=emo_16_42, lr=0.0005, bsz=8, dev_performance=0.7484106569027315, test_performance=0.3335596887945225
05/21/2022 14:02:09 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.0003, bsz=8 ...
05/21/2022 14:02:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:02:10 - INFO - __main__ - Printing 3 examples
05/21/2022 14:02:10 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/21/2022 14:02:10 - INFO - __main__ - ['happy']
05/21/2022 14:02:10 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/21/2022 14:02:10 - INFO - __main__ - ['happy']
05/21/2022 14:02:10 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/21/2022 14:02:10 - INFO - __main__ - ['happy']
05/21/2022 14:02:10 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:02:10 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:02:10 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 14:02:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:02:10 - INFO - __main__ - Printing 3 examples
05/21/2022 14:02:10 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/21/2022 14:02:10 - INFO - __main__ - ['happy']
05/21/2022 14:02:10 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/21/2022 14:02:10 - INFO - __main__ - ['happy']
05/21/2022 14:02:10 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/21/2022 14:02:10 - INFO - __main__ - ['happy']
05/21/2022 14:02:10 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:02:10 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:02:10 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 14:02:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 14:02:14 - INFO - __main__ - Starting training!
05/21/2022 14:02:16 - INFO - __main__ - Step 10 Global step 10 Train loss 19.899857 on epoch=2
05/21/2022 14:02:19 - INFO - __main__ - Step 20 Global step 20 Train loss 16.787939 on epoch=4
05/21/2022 14:02:21 - INFO - __main__ - Step 30 Global step 30 Train loss 10.295536 on epoch=7
05/21/2022 14:02:24 - INFO - __main__ - Step 40 Global step 40 Train loss 6.969008 on epoch=9
05/21/2022 14:02:26 - INFO - __main__ - Step 50 Global step 50 Train loss 6.084836 on epoch=12
05/21/2022 14:02:27 - INFO - __main__ - Global step 50 Train loss 12.007435 Classification-F1 0.1 on epoch=12
05/21/2022 14:02:30 - INFO - __main__ - Step 60 Global step 60 Train loss 4.319016 on epoch=14
05/21/2022 14:02:32 - INFO - __main__ - Step 70 Global step 70 Train loss 4.798640 on epoch=17
05/21/2022 14:02:35 - INFO - __main__ - Step 80 Global step 80 Train loss 3.287187 on epoch=19
05/21/2022 14:02:38 - INFO - __main__ - Step 90 Global step 90 Train loss 3.081166 on epoch=22
05/21/2022 14:02:40 - INFO - __main__ - Step 100 Global step 100 Train loss 2.683737 on epoch=24
05/21/2022 14:02:40 - INFO - __main__ - Global step 100 Train loss 3.633950 Classification-F1 0.17243867243867242 on epoch=24
05/21/2022 14:02:43 - INFO - __main__ - Step 110 Global step 110 Train loss 1.976983 on epoch=27
05/21/2022 14:02:46 - INFO - __main__ - Step 120 Global step 120 Train loss 2.324547 on epoch=29
05/21/2022 14:02:48 - INFO - __main__ - Step 130 Global step 130 Train loss 2.060115 on epoch=32
05/21/2022 14:02:51 - INFO - __main__ - Step 140 Global step 140 Train loss 2.260827 on epoch=34
05/21/2022 14:02:53 - INFO - __main__ - Step 150 Global step 150 Train loss 1.639866 on epoch=37
05/21/2022 14:02:54 - INFO - __main__ - Global step 150 Train loss 2.052468 Classification-F1 0.42929670152206384 on epoch=37
05/21/2022 14:02:57 - INFO - __main__ - Step 160 Global step 160 Train loss 1.537729 on epoch=39
05/21/2022 14:02:59 - INFO - __main__ - Step 170 Global step 170 Train loss 1.243011 on epoch=42
05/21/2022 14:03:02 - INFO - __main__ - Step 180 Global step 180 Train loss 1.341743 on epoch=44
05/21/2022 14:03:04 - INFO - __main__ - Step 190 Global step 190 Train loss 1.385704 on epoch=47
05/21/2022 14:03:07 - INFO - __main__ - Step 200 Global step 200 Train loss 1.368000 on epoch=49
05/21/2022 14:03:07 - INFO - __main__ - Global step 200 Train loss 1.375237 Classification-F1 0.4703647416413374 on epoch=49
05/21/2022 14:03:10 - INFO - __main__ - Step 210 Global step 210 Train loss 1.174511 on epoch=52
05/21/2022 14:03:13 - INFO - __main__ - Step 220 Global step 220 Train loss 1.020285 on epoch=54
05/21/2022 14:03:15 - INFO - __main__ - Step 230 Global step 230 Train loss 1.013651 on epoch=57
05/21/2022 14:03:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.627067 on epoch=59
05/21/2022 14:03:20 - INFO - __main__ - Step 250 Global step 250 Train loss 1.110270 on epoch=62
05/21/2022 14:03:20 - INFO - __main__ - Global step 250 Train loss 0.989157 Classification-F1 0.4826203208556149 on epoch=62
05/21/2022 14:03:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.771990 on epoch=64
05/21/2022 14:03:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.771564 on epoch=67
05/21/2022 14:03:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.890303 on epoch=69
05/21/2022 14:03:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.729738 on epoch=72
05/21/2022 14:03:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.550329 on epoch=74
05/21/2022 14:03:34 - INFO - __main__ - Global step 300 Train loss 0.742785 Classification-F1 0.7258498254957736 on epoch=74
05/21/2022 14:03:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.613447 on epoch=77
05/21/2022 14:03:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.614617 on epoch=79
05/21/2022 14:03:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.488811 on epoch=82
05/21/2022 14:03:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.653293 on epoch=84
05/21/2022 14:03:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.846059 on epoch=87
05/21/2022 14:03:47 - INFO - __main__ - Global step 350 Train loss 0.643245 Classification-F1 0.6186909581646423 on epoch=87
05/21/2022 14:03:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.636116 on epoch=89
05/21/2022 14:03:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.625154 on epoch=92
05/21/2022 14:03:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.714906 on epoch=94
05/21/2022 14:03:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.711667 on epoch=97
05/21/2022 14:04:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.557728 on epoch=99
05/21/2022 14:04:00 - INFO - __main__ - Global step 400 Train loss 0.649114 Classification-F1 0.7203030303030303 on epoch=99
05/21/2022 14:04:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.619970 on epoch=102
05/21/2022 14:04:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.718934 on epoch=104
05/21/2022 14:04:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.744289 on epoch=107
05/21/2022 14:04:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.760734 on epoch=109
05/21/2022 14:04:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.649663 on epoch=112
05/21/2022 14:04:14 - INFO - __main__ - Global step 450 Train loss 0.698718 Classification-F1 0.7429292929292928 on epoch=112
05/21/2022 14:04:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.570399 on epoch=114
05/21/2022 14:04:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.667233 on epoch=117
05/21/2022 14:04:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.749744 on epoch=119
05/21/2022 14:04:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.559114 on epoch=122
05/21/2022 14:04:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.373697 on epoch=124
05/21/2022 14:04:27 - INFO - __main__ - Global step 500 Train loss 0.584037 Classification-F1 0.7434906597774245 on epoch=124
05/21/2022 14:04:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.438400 on epoch=127
05/21/2022 14:04:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.613051 on epoch=129
05/21/2022 14:04:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.497539 on epoch=132
05/21/2022 14:04:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.739623 on epoch=134
05/21/2022 14:04:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.380974 on epoch=137
05/21/2022 14:04:41 - INFO - __main__ - Global step 550 Train loss 0.533917 Classification-F1 0.7384219257620792 on epoch=137
05/21/2022 14:04:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.471132 on epoch=139
05/21/2022 14:04:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.573581 on epoch=142
05/21/2022 14:04:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.358701 on epoch=144
05/21/2022 14:04:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.295321 on epoch=147
05/21/2022 14:04:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.551124 on epoch=149
05/21/2022 14:04:54 - INFO - __main__ - Global step 600 Train loss 0.449972 Classification-F1 0.7227240896358543 on epoch=149
05/21/2022 14:04:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.451384 on epoch=152
05/21/2022 14:04:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.422384 on epoch=154
05/21/2022 14:05:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.281286 on epoch=157
05/21/2022 14:05:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.342226 on epoch=159
05/21/2022 14:05:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.377341 on epoch=162
05/21/2022 14:05:07 - INFO - __main__ - Global step 650 Train loss 0.374924 Classification-F1 0.7096774193548387 on epoch=162
05/21/2022 14:05:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.409169 on epoch=164
05/21/2022 14:05:12 - INFO - __main__ - Step 670 Global step 670 Train loss 0.306992 on epoch=167
05/21/2022 14:05:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.423739 on epoch=169
05/21/2022 14:05:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.297422 on epoch=172
05/21/2022 14:05:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.180333 on epoch=174
05/21/2022 14:05:20 - INFO - __main__ - Global step 700 Train loss 0.323531 Classification-F1 0.6909893949459904 on epoch=174
05/21/2022 14:05:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.259609 on epoch=177
05/21/2022 14:05:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.168213 on epoch=179
05/21/2022 14:05:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.190686 on epoch=182
05/21/2022 14:05:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.354582 on epoch=184
05/21/2022 14:05:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.264005 on epoch=187
05/21/2022 14:05:33 - INFO - __main__ - Global step 750 Train loss 0.247419 Classification-F1 0.7582800982800982 on epoch=187
05/21/2022 14:05:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.378650 on epoch=189
05/21/2022 14:05:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.282116 on epoch=192
05/21/2022 14:05:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.397241 on epoch=194
05/21/2022 14:05:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.210786 on epoch=197
05/21/2022 14:05:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.160122 on epoch=199
05/21/2022 14:05:47 - INFO - __main__ - Global step 800 Train loss 0.285783 Classification-F1 0.7471169557566617 on epoch=199
05/21/2022 14:05:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.274325 on epoch=202
05/21/2022 14:05:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.493573 on epoch=204
05/21/2022 14:05:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.113353 on epoch=207
05/21/2022 14:05:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.324281 on epoch=209
05/21/2022 14:06:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.226409 on epoch=212
05/21/2022 14:06:00 - INFO - __main__ - Global step 850 Train loss 0.286388 Classification-F1 0.7372474747474748 on epoch=212
05/21/2022 14:06:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.072166 on epoch=214
05/21/2022 14:06:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.187006 on epoch=217
05/21/2022 14:06:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.107639 on epoch=219
05/21/2022 14:06:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.171394 on epoch=222
05/21/2022 14:06:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.068786 on epoch=224
05/21/2022 14:06:13 - INFO - __main__ - Global step 900 Train loss 0.121398 Classification-F1 0.7223039215686274 on epoch=224
05/21/2022 14:06:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.088748 on epoch=227
05/21/2022 14:06:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.306961 on epoch=229
05/21/2022 14:06:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.126014 on epoch=232
05/21/2022 14:06:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.109137 on epoch=234
05/21/2022 14:06:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.174963 on epoch=237
05/21/2022 14:06:26 - INFO - __main__ - Global step 950 Train loss 0.161165 Classification-F1 0.7618228381096028 on epoch=237
05/21/2022 14:06:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.150055 on epoch=239
05/21/2022 14:06:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.213326 on epoch=242
05/21/2022 14:06:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.228093 on epoch=244
05/21/2022 14:06:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.300556 on epoch=247
05/21/2022 14:06:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.312964 on epoch=249
05/21/2022 14:06:40 - INFO - __main__ - Global step 1000 Train loss 0.240999 Classification-F1 0.7476304945054946 on epoch=249
05/21/2022 14:06:40 - INFO - __main__ - save last model!
05/21/2022 14:06:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:06:41 - INFO - __main__ - Printing 3 examples
05/21/2022 14:06:41 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/21/2022 14:06:41 - INFO - __main__ - ['happy']
05/21/2022 14:06:41 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/21/2022 14:06:41 - INFO - __main__ - ['happy']
05/21/2022 14:06:41 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/21/2022 14:06:41 - INFO - __main__ - ['happy']
05/21/2022 14:06:41 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:06:41 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:06:41 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 14:06:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:06:41 - INFO - __main__ - Printing 3 examples
05/21/2022 14:06:41 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/21/2022 14:06:41 - INFO - __main__ - ['happy']
05/21/2022 14:06:41 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/21/2022 14:06:41 - INFO - __main__ - ['happy']
05/21/2022 14:06:41 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/21/2022 14:06:41 - INFO - __main__ - ['happy']
05/21/2022 14:06:41 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:06:41 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:06:41 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 14:06:43 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 14:06:43 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 14:06:43 - INFO - __main__ - Printing 3 examples
05/21/2022 14:06:43 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 14:06:43 - INFO - __main__ - ['others']
05/21/2022 14:06:43 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 14:06:43 - INFO - __main__ - ['others']
05/21/2022 14:06:43 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 14:06:43 - INFO - __main__ - ['others']
05/21/2022 14:06:43 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:06:45 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:06:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 14:06:45 - INFO - __main__ - Starting training!
05/21/2022 14:06:50 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 14:07:22 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_42_0.0003_8_predictions.txt
05/21/2022 14:07:22 - INFO - __main__ - Classification-F1 on test data: 0.4081
05/21/2022 14:07:22 - INFO - __main__ - prefix=emo_16_42, lr=0.0003, bsz=8, dev_performance=0.7618228381096028, test_performance=0.4081037935712178
05/21/2022 14:07:22 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.0002, bsz=8 ...
05/21/2022 14:07:23 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:07:23 - INFO - __main__ - Printing 3 examples
05/21/2022 14:07:23 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/21/2022 14:07:23 - INFO - __main__ - ['happy']
05/21/2022 14:07:23 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/21/2022 14:07:23 - INFO - __main__ - ['happy']
05/21/2022 14:07:23 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/21/2022 14:07:23 - INFO - __main__ - ['happy']
05/21/2022 14:07:23 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:07:23 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:07:23 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 14:07:23 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:07:23 - INFO - __main__ - Printing 3 examples
05/21/2022 14:07:23 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/21/2022 14:07:23 - INFO - __main__ - ['happy']
05/21/2022 14:07:23 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/21/2022 14:07:23 - INFO - __main__ - ['happy']
05/21/2022 14:07:23 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/21/2022 14:07:23 - INFO - __main__ - ['happy']
05/21/2022 14:07:23 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:07:23 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:07:23 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 14:07:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 14:07:27 - INFO - __main__ - Starting training!
05/21/2022 14:07:29 - INFO - __main__ - Step 10 Global step 10 Train loss 20.680241 on epoch=2
05/21/2022 14:07:32 - INFO - __main__ - Step 20 Global step 20 Train loss 18.689068 on epoch=4
05/21/2022 14:07:34 - INFO - __main__ - Step 30 Global step 30 Train loss 13.497808 on epoch=7
05/21/2022 14:07:37 - INFO - __main__ - Step 40 Global step 40 Train loss 11.195722 on epoch=9
05/21/2022 14:07:39 - INFO - __main__ - Step 50 Global step 50 Train loss 9.482862 on epoch=12
05/21/2022 14:07:47 - INFO - __main__ - Global step 50 Train loss 14.709141 Classification-F1 0.0 on epoch=12
05/21/2022 14:07:50 - INFO - __main__ - Step 60 Global step 60 Train loss 8.708636 on epoch=14
05/21/2022 14:07:52 - INFO - __main__ - Step 70 Global step 70 Train loss 7.608450 on epoch=17
05/21/2022 14:07:55 - INFO - __main__ - Step 80 Global step 80 Train loss 5.965449 on epoch=19
05/21/2022 14:07:57 - INFO - __main__ - Step 90 Global step 90 Train loss 4.535589 on epoch=22
05/21/2022 14:08:00 - INFO - __main__ - Step 100 Global step 100 Train loss 5.233850 on epoch=24
05/21/2022 14:08:00 - INFO - __main__ - Global step 100 Train loss 6.410395 Classification-F1 0.1 on epoch=24
05/21/2022 14:08:03 - INFO - __main__ - Step 110 Global step 110 Train loss 4.530776 on epoch=27
05/21/2022 14:08:06 - INFO - __main__ - Step 120 Global step 120 Train loss 4.028597 on epoch=29
05/21/2022 14:08:08 - INFO - __main__ - Step 130 Global step 130 Train loss 3.745440 on epoch=32
05/21/2022 14:08:11 - INFO - __main__ - Step 140 Global step 140 Train loss 2.953146 on epoch=34
05/21/2022 14:08:13 - INFO - __main__ - Step 150 Global step 150 Train loss 2.991273 on epoch=37
05/21/2022 14:08:14 - INFO - __main__ - Global step 150 Train loss 3.649846 Classification-F1 0.13067758749069247 on epoch=37
05/21/2022 14:08:17 - INFO - __main__ - Step 160 Global step 160 Train loss 3.344471 on epoch=39
05/21/2022 14:08:19 - INFO - __main__ - Step 170 Global step 170 Train loss 3.414401 on epoch=42
05/21/2022 14:08:22 - INFO - __main__ - Step 180 Global step 180 Train loss 3.246522 on epoch=44
05/21/2022 14:08:24 - INFO - __main__ - Step 190 Global step 190 Train loss 2.689209 on epoch=47
05/21/2022 14:08:27 - INFO - __main__ - Step 200 Global step 200 Train loss 2.765782 on epoch=49
05/21/2022 14:08:27 - INFO - __main__ - Global step 200 Train loss 3.092077 Classification-F1 0.22024332120908063 on epoch=49
05/21/2022 14:08:30 - INFO - __main__ - Step 210 Global step 210 Train loss 1.993679 on epoch=52
05/21/2022 14:08:33 - INFO - __main__ - Step 220 Global step 220 Train loss 2.362598 on epoch=54
05/21/2022 14:08:35 - INFO - __main__ - Step 230 Global step 230 Train loss 1.853674 on epoch=57
05/21/2022 14:08:38 - INFO - __main__ - Step 240 Global step 240 Train loss 1.943171 on epoch=59
05/21/2022 14:08:41 - INFO - __main__ - Step 250 Global step 250 Train loss 1.737037 on epoch=62
05/21/2022 14:08:41 - INFO - __main__ - Global step 250 Train loss 1.978032 Classification-F1 0.3372241086587436 on epoch=62
05/21/2022 14:08:44 - INFO - __main__ - Step 260 Global step 260 Train loss 2.038245 on epoch=64
05/21/2022 14:08:46 - INFO - __main__ - Step 270 Global step 270 Train loss 1.557081 on epoch=67
05/21/2022 14:08:49 - INFO - __main__ - Step 280 Global step 280 Train loss 1.561959 on epoch=69
05/21/2022 14:08:52 - INFO - __main__ - Step 290 Global step 290 Train loss 1.662646 on epoch=72
05/21/2022 14:08:54 - INFO - __main__ - Step 300 Global step 300 Train loss 1.403406 on epoch=74
05/21/2022 14:08:55 - INFO - __main__ - Global step 300 Train loss 1.644667 Classification-F1 0.33928571428571425 on epoch=74
05/21/2022 14:08:57 - INFO - __main__ - Step 310 Global step 310 Train loss 1.483329 on epoch=77
05/21/2022 14:09:00 - INFO - __main__ - Step 320 Global step 320 Train loss 2.130394 on epoch=79
05/21/2022 14:09:03 - INFO - __main__ - Step 330 Global step 330 Train loss 1.238886 on epoch=82
05/21/2022 14:09:05 - INFO - __main__ - Step 340 Global step 340 Train loss 1.072766 on epoch=84
05/21/2022 14:09:08 - INFO - __main__ - Step 350 Global step 350 Train loss 1.270607 on epoch=87
05/21/2022 14:09:08 - INFO - __main__ - Global step 350 Train loss 1.439196 Classification-F1 0.43606143578334905 on epoch=87
05/21/2022 14:09:11 - INFO - __main__ - Step 360 Global step 360 Train loss 1.398726 on epoch=89
05/21/2022 14:09:14 - INFO - __main__ - Step 370 Global step 370 Train loss 1.619930 on epoch=92
05/21/2022 14:09:16 - INFO - __main__ - Step 380 Global step 380 Train loss 1.813373 on epoch=94
05/21/2022 14:09:19 - INFO - __main__ - Step 390 Global step 390 Train loss 1.564719 on epoch=97
05/21/2022 14:09:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.993951 on epoch=99
05/21/2022 14:09:22 - INFO - __main__ - Global step 400 Train loss 1.478140 Classification-F1 0.45798097251585623 on epoch=99
05/21/2022 14:09:25 - INFO - __main__ - Step 410 Global step 410 Train loss 1.265782 on epoch=102
05/21/2022 14:09:27 - INFO - __main__ - Step 420 Global step 420 Train loss 1.198493 on epoch=104
05/21/2022 14:09:30 - INFO - __main__ - Step 430 Global step 430 Train loss 1.170331 on epoch=107
05/21/2022 14:09:32 - INFO - __main__ - Step 440 Global step 440 Train loss 1.364983 on epoch=109
05/21/2022 14:09:35 - INFO - __main__ - Step 450 Global step 450 Train loss 1.496066 on epoch=112
05/21/2022 14:09:35 - INFO - __main__ - Global step 450 Train loss 1.299131 Classification-F1 0.4177905308464849 on epoch=112
05/21/2022 14:09:38 - INFO - __main__ - Step 460 Global step 460 Train loss 1.023139 on epoch=114
05/21/2022 14:09:40 - INFO - __main__ - Step 470 Global step 470 Train loss 1.099441 on epoch=117
05/21/2022 14:09:43 - INFO - __main__ - Step 480 Global step 480 Train loss 1.055507 on epoch=119
05/21/2022 14:09:46 - INFO - __main__ - Step 490 Global step 490 Train loss 1.099863 on epoch=122
05/21/2022 14:09:48 - INFO - __main__ - Step 500 Global step 500 Train loss 1.208381 on epoch=124
05/21/2022 14:09:49 - INFO - __main__ - Global step 500 Train loss 1.097266 Classification-F1 0.41477272727272724 on epoch=124
05/21/2022 14:09:51 - INFO - __main__ - Step 510 Global step 510 Train loss 1.071180 on epoch=127
05/21/2022 14:09:54 - INFO - __main__ - Step 520 Global step 520 Train loss 1.199249 on epoch=129
05/21/2022 14:09:56 - INFO - __main__ - Step 530 Global step 530 Train loss 1.143944 on epoch=132
05/21/2022 14:09:59 - INFO - __main__ - Step 540 Global step 540 Train loss 1.065669 on epoch=134
05/21/2022 14:10:01 - INFO - __main__ - Step 550 Global step 550 Train loss 1.068905 on epoch=137
05/21/2022 14:10:02 - INFO - __main__ - Global step 550 Train loss 1.109789 Classification-F1 0.4462962962962963 on epoch=137
05/21/2022 14:10:04 - INFO - __main__ - Step 560 Global step 560 Train loss 1.140610 on epoch=139
05/21/2022 14:10:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.892184 on epoch=142
05/21/2022 14:10:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.957243 on epoch=144
05/21/2022 14:10:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.976828 on epoch=147
05/21/2022 14:10:14 - INFO - __main__ - Step 600 Global step 600 Train loss 1.000285 on epoch=149
05/21/2022 14:10:15 - INFO - __main__ - Global step 600 Train loss 0.993430 Classification-F1 0.45783730158730157 on epoch=149
05/21/2022 14:10:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.982860 on epoch=152
05/21/2022 14:10:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.937752 on epoch=154
05/21/2022 14:10:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.884956 on epoch=157
05/21/2022 14:10:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.860174 on epoch=159
05/21/2022 14:10:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.848169 on epoch=162
05/21/2022 14:10:28 - INFO - __main__ - Global step 650 Train loss 0.902782 Classification-F1 0.525417332702009 on epoch=162
05/21/2022 14:10:31 - INFO - __main__ - Step 660 Global step 660 Train loss 1.068629 on epoch=164
05/21/2022 14:10:33 - INFO - __main__ - Step 670 Global step 670 Train loss 1.196029 on epoch=167
05/21/2022 14:10:36 - INFO - __main__ - Step 680 Global step 680 Train loss 1.225886 on epoch=169
05/21/2022 14:10:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.978881 on epoch=172
05/21/2022 14:10:41 - INFO - __main__ - Step 700 Global step 700 Train loss 1.042733 on epoch=174
05/21/2022 14:10:41 - INFO - __main__ - Global step 700 Train loss 1.102432 Classification-F1 0.5983973236861555 on epoch=174
05/21/2022 14:10:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.924284 on epoch=177
05/21/2022 14:10:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.954143 on epoch=179
05/21/2022 14:10:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.837274 on epoch=182
05/21/2022 14:10:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.857286 on epoch=184
05/21/2022 14:10:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.979522 on epoch=187
05/21/2022 14:10:54 - INFO - __main__ - Global step 750 Train loss 0.910502 Classification-F1 0.5358884671136515 on epoch=187
05/21/2022 14:10:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.954017 on epoch=189
05/21/2022 14:11:00 - INFO - __main__ - Step 770 Global step 770 Train loss 1.006067 on epoch=192
05/21/2022 14:11:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.867332 on epoch=194
05/21/2022 14:11:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.752617 on epoch=197
05/21/2022 14:11:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.924726 on epoch=199
05/21/2022 14:11:08 - INFO - __main__ - Global step 800 Train loss 0.900952 Classification-F1 0.5095254010695187 on epoch=199
05/21/2022 14:11:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.880057 on epoch=202
05/21/2022 14:11:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.880612 on epoch=204
05/21/2022 14:11:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.859031 on epoch=207
05/21/2022 14:11:18 - INFO - __main__ - Step 840 Global step 840 Train loss 1.016181 on epoch=209
05/21/2022 14:11:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.840222 on epoch=212
05/21/2022 14:11:21 - INFO - __main__ - Global step 850 Train loss 0.895221 Classification-F1 0.5986207381556219 on epoch=212
05/21/2022 14:11:24 - INFO - __main__ - Step 860 Global step 860 Train loss 1.050134 on epoch=214
05/21/2022 14:11:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.623213 on epoch=217
05/21/2022 14:11:29 - INFO - __main__ - Step 880 Global step 880 Train loss 1.184518 on epoch=219
05/21/2022 14:11:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.801039 on epoch=222
05/21/2022 14:11:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.679953 on epoch=224
05/21/2022 14:11:34 - INFO - __main__ - Global step 900 Train loss 0.867772 Classification-F1 0.7342792248864355 on epoch=224
05/21/2022 14:11:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.854639 on epoch=227
05/21/2022 14:11:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.890742 on epoch=229
05/21/2022 14:11:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.808521 on epoch=232
05/21/2022 14:11:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.833986 on epoch=234
05/21/2022 14:11:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.780688 on epoch=237
05/21/2022 14:11:48 - INFO - __main__ - Global step 950 Train loss 0.833715 Classification-F1 0.7455598455598456 on epoch=237
05/21/2022 14:11:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.840491 on epoch=239
05/21/2022 14:11:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.820023 on epoch=242
05/21/2022 14:11:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.756180 on epoch=244
05/21/2022 14:11:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.976732 on epoch=247
05/21/2022 14:12:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.863522 on epoch=249
05/21/2022 14:12:02 - INFO - __main__ - Global step 1000 Train loss 0.851390 Classification-F1 0.7181372549019608 on epoch=249
05/21/2022 14:12:02 - INFO - __main__ - save last model!
05/21/2022 14:12:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:12:03 - INFO - __main__ - Printing 3 examples
05/21/2022 14:12:03 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/21/2022 14:12:03 - INFO - __main__ - ['happy']
05/21/2022 14:12:03 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/21/2022 14:12:03 - INFO - __main__ - ['happy']
05/21/2022 14:12:03 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/21/2022 14:12:03 - INFO - __main__ - ['happy']
05/21/2022 14:12:03 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:12:03 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:12:03 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 14:12:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:12:03 - INFO - __main__ - Printing 3 examples
05/21/2022 14:12:03 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/21/2022 14:12:03 - INFO - __main__ - ['happy']
05/21/2022 14:12:03 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/21/2022 14:12:03 - INFO - __main__ - ['happy']
05/21/2022 14:12:03 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/21/2022 14:12:03 - INFO - __main__ - ['happy']
05/21/2022 14:12:03 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:12:03 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:12:03 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 14:12:05 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 14:12:05 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 14:12:05 - INFO - __main__ - Printing 3 examples
05/21/2022 14:12:05 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 14:12:05 - INFO - __main__ - ['others']
05/21/2022 14:12:05 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 14:12:05 - INFO - __main__ - ['others']
05/21/2022 14:12:05 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 14:12:05 - INFO - __main__ - ['others']
05/21/2022 14:12:05 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:12:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 14:12:07 - INFO - __main__ - Starting training!
05/21/2022 14:12:07 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:12:13 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 14:12:41 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_42_0.0002_8_predictions.txt
05/21/2022 14:12:41 - INFO - __main__ - Classification-F1 on test data: 0.4322
05/21/2022 14:12:41 - INFO - __main__ - prefix=emo_16_42, lr=0.0002, bsz=8, dev_performance=0.7455598455598456, test_performance=0.4322034975712033
05/21/2022 14:12:41 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.0001, bsz=8 ...
05/21/2022 14:12:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:12:42 - INFO - __main__ - Printing 3 examples
05/21/2022 14:12:42 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/21/2022 14:12:42 - INFO - __main__ - ['happy']
05/21/2022 14:12:42 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/21/2022 14:12:42 - INFO - __main__ - ['happy']
05/21/2022 14:12:42 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/21/2022 14:12:42 - INFO - __main__ - ['happy']
05/21/2022 14:12:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:12:42 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:12:42 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 14:12:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:12:42 - INFO - __main__ - Printing 3 examples
05/21/2022 14:12:42 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/21/2022 14:12:42 - INFO - __main__ - ['happy']
05/21/2022 14:12:42 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/21/2022 14:12:42 - INFO - __main__ - ['happy']
05/21/2022 14:12:42 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/21/2022 14:12:42 - INFO - __main__ - ['happy']
05/21/2022 14:12:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:12:42 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:12:42 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 14:12:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 14:12:46 - INFO - __main__ - Starting training!
05/21/2022 14:12:48 - INFO - __main__ - Step 10 Global step 10 Train loss 20.188116 on epoch=2
05/21/2022 14:12:50 - INFO - __main__ - Step 20 Global step 20 Train loss 20.459711 on epoch=4
05/21/2022 14:12:53 - INFO - __main__ - Step 30 Global step 30 Train loss 17.155773 on epoch=7
05/21/2022 14:12:55 - INFO - __main__ - Step 40 Global step 40 Train loss 13.394621 on epoch=9
05/21/2022 14:12:58 - INFO - __main__ - Step 50 Global step 50 Train loss 11.963054 on epoch=12
05/21/2022 14:13:09 - INFO - __main__ - Global step 50 Train loss 16.632254 Classification-F1 0.0 on epoch=12
05/21/2022 14:13:12 - INFO - __main__ - Step 60 Global step 60 Train loss 10.558160 on epoch=14
05/21/2022 14:13:14 - INFO - __main__ - Step 70 Global step 70 Train loss 10.766806 on epoch=17
05/21/2022 14:13:17 - INFO - __main__ - Step 80 Global step 80 Train loss 8.602217 on epoch=19
05/21/2022 14:13:20 - INFO - __main__ - Step 90 Global step 90 Train loss 9.393921 on epoch=22
05/21/2022 14:13:22 - INFO - __main__ - Step 100 Global step 100 Train loss 8.014681 on epoch=24
05/21/2022 14:13:29 - INFO - __main__ - Global step 100 Train loss 9.467156 Classification-F1 0.0 on epoch=24
05/21/2022 14:13:31 - INFO - __main__ - Step 110 Global step 110 Train loss 6.987501 on epoch=27
05/21/2022 14:13:34 - INFO - __main__ - Step 120 Global step 120 Train loss 6.369607 on epoch=29
05/21/2022 14:13:37 - INFO - __main__ - Step 130 Global step 130 Train loss 6.424760 on epoch=32
05/21/2022 14:13:39 - INFO - __main__ - Step 140 Global step 140 Train loss 6.873754 on epoch=34
05/21/2022 14:13:42 - INFO - __main__ - Step 150 Global step 150 Train loss 5.086124 on epoch=37
05/21/2022 14:13:43 - INFO - __main__ - Global step 150 Train loss 6.348350 Classification-F1 0.06926406926406926 on epoch=37
05/21/2022 14:13:45 - INFO - __main__ - Step 160 Global step 160 Train loss 5.950022 on epoch=39
05/21/2022 14:13:48 - INFO - __main__ - Step 170 Global step 170 Train loss 4.501969 on epoch=42
05/21/2022 14:13:51 - INFO - __main__ - Step 180 Global step 180 Train loss 4.598905 on epoch=44
05/21/2022 14:13:53 - INFO - __main__ - Step 190 Global step 190 Train loss 4.608664 on epoch=47
05/21/2022 14:13:56 - INFO - __main__ - Step 200 Global step 200 Train loss 3.842213 on epoch=49
05/21/2022 14:13:57 - INFO - __main__ - Global step 200 Train loss 4.700355 Classification-F1 0.1 on epoch=49
05/21/2022 14:14:00 - INFO - __main__ - Step 210 Global step 210 Train loss 3.754418 on epoch=52
05/21/2022 14:14:02 - INFO - __main__ - Step 220 Global step 220 Train loss 3.861505 on epoch=54
05/21/2022 14:14:05 - INFO - __main__ - Step 230 Global step 230 Train loss 3.785038 on epoch=57
05/21/2022 14:14:07 - INFO - __main__ - Step 240 Global step 240 Train loss 3.589116 on epoch=59
05/21/2022 14:14:10 - INFO - __main__ - Step 250 Global step 250 Train loss 4.237741 on epoch=62
05/21/2022 14:14:10 - INFO - __main__ - Global step 250 Train loss 3.845564 Classification-F1 0.1 on epoch=62
05/21/2022 14:14:13 - INFO - __main__ - Step 260 Global step 260 Train loss 4.205920 on epoch=64
05/21/2022 14:14:15 - INFO - __main__ - Step 270 Global step 270 Train loss 2.943230 on epoch=67
05/21/2022 14:14:18 - INFO - __main__ - Step 280 Global step 280 Train loss 3.380414 on epoch=69
05/21/2022 14:14:21 - INFO - __main__ - Step 290 Global step 290 Train loss 3.237811 on epoch=72
05/21/2022 14:14:23 - INFO - __main__ - Step 300 Global step 300 Train loss 3.686842 on epoch=74
05/21/2022 14:14:24 - INFO - __main__ - Global step 300 Train loss 3.490843 Classification-F1 0.1 on epoch=74
05/21/2022 14:14:26 - INFO - __main__ - Step 310 Global step 310 Train loss 3.251497 on epoch=77
05/21/2022 14:14:29 - INFO - __main__ - Step 320 Global step 320 Train loss 2.982701 on epoch=79
05/21/2022 14:14:31 - INFO - __main__ - Step 330 Global step 330 Train loss 2.399809 on epoch=82
05/21/2022 14:14:34 - INFO - __main__ - Step 340 Global step 340 Train loss 2.613891 on epoch=84
05/21/2022 14:14:36 - INFO - __main__ - Step 350 Global step 350 Train loss 3.071323 on epoch=87
05/21/2022 14:14:37 - INFO - __main__ - Global step 350 Train loss 2.863844 Classification-F1 0.19318181818181818 on epoch=87
05/21/2022 14:14:40 - INFO - __main__ - Step 360 Global step 360 Train loss 2.249923 on epoch=89
05/21/2022 14:14:42 - INFO - __main__ - Step 370 Global step 370 Train loss 3.409645 on epoch=92
05/21/2022 14:14:45 - INFO - __main__ - Step 380 Global step 380 Train loss 2.920430 on epoch=94
05/21/2022 14:14:47 - INFO - __main__ - Step 390 Global step 390 Train loss 3.008410 on epoch=97
05/21/2022 14:14:50 - INFO - __main__ - Step 400 Global step 400 Train loss 2.917170 on epoch=99
05/21/2022 14:14:50 - INFO - __main__ - Global step 400 Train loss 2.901115 Classification-F1 0.343228353903627 on epoch=99
05/21/2022 14:14:53 - INFO - __main__ - Step 410 Global step 410 Train loss 2.473569 on epoch=102
05/21/2022 14:14:55 - INFO - __main__ - Step 420 Global step 420 Train loss 2.458812 on epoch=104
05/21/2022 14:14:58 - INFO - __main__ - Step 430 Global step 430 Train loss 1.994996 on epoch=107
05/21/2022 14:15:01 - INFO - __main__ - Step 440 Global step 440 Train loss 2.196882 on epoch=109
05/21/2022 14:15:03 - INFO - __main__ - Step 450 Global step 450 Train loss 2.037521 on epoch=112
05/21/2022 14:15:03 - INFO - __main__ - Global step 450 Train loss 2.232356 Classification-F1 0.3630337026563441 on epoch=112
05/21/2022 14:15:06 - INFO - __main__ - Step 460 Global step 460 Train loss 1.985961 on epoch=114
05/21/2022 14:15:09 - INFO - __main__ - Step 470 Global step 470 Train loss 2.028961 on epoch=117
05/21/2022 14:15:11 - INFO - __main__ - Step 480 Global step 480 Train loss 1.921823 on epoch=119
05/21/2022 14:15:14 - INFO - __main__ - Step 490 Global step 490 Train loss 1.820828 on epoch=122
05/21/2022 14:15:17 - INFO - __main__ - Step 500 Global step 500 Train loss 2.334344 on epoch=124
05/21/2022 14:15:17 - INFO - __main__ - Global step 500 Train loss 2.018384 Classification-F1 0.35844155844155845 on epoch=124
05/21/2022 14:15:19 - INFO - __main__ - Step 510 Global step 510 Train loss 1.361304 on epoch=127
05/21/2022 14:15:22 - INFO - __main__ - Step 520 Global step 520 Train loss 1.710670 on epoch=129
05/21/2022 14:15:24 - INFO - __main__ - Step 530 Global step 530 Train loss 1.692276 on epoch=132
05/21/2022 14:15:27 - INFO - __main__ - Step 540 Global step 540 Train loss 1.379794 on epoch=134
05/21/2022 14:15:29 - INFO - __main__ - Step 550 Global step 550 Train loss 1.477528 on epoch=137
05/21/2022 14:15:30 - INFO - __main__ - Global step 550 Train loss 1.524314 Classification-F1 0.42183359830418654 on epoch=137
05/21/2022 14:15:33 - INFO - __main__ - Step 560 Global step 560 Train loss 1.717372 on epoch=139
05/21/2022 14:15:35 - INFO - __main__ - Step 570 Global step 570 Train loss 1.399165 on epoch=142
05/21/2022 14:15:38 - INFO - __main__ - Step 580 Global step 580 Train loss 1.616998 on epoch=144
05/21/2022 14:15:41 - INFO - __main__ - Step 590 Global step 590 Train loss 1.605499 on epoch=147
05/21/2022 14:15:43 - INFO - __main__ - Step 600 Global step 600 Train loss 1.624048 on epoch=149
05/21/2022 14:15:43 - INFO - __main__ - Global step 600 Train loss 1.592616 Classification-F1 0.42538126361655776 on epoch=149
05/21/2022 14:15:46 - INFO - __main__ - Step 610 Global step 610 Train loss 1.520099 on epoch=152
05/21/2022 14:15:49 - INFO - __main__ - Step 620 Global step 620 Train loss 1.525340 on epoch=154
05/21/2022 14:15:51 - INFO - __main__ - Step 630 Global step 630 Train loss 1.715015 on epoch=157
05/21/2022 14:15:54 - INFO - __main__ - Step 640 Global step 640 Train loss 1.368395 on epoch=159
05/21/2022 14:15:57 - INFO - __main__ - Step 650 Global step 650 Train loss 1.358964 on epoch=162
05/21/2022 14:15:57 - INFO - __main__ - Global step 650 Train loss 1.497563 Classification-F1 0.48598484848484846 on epoch=162
05/21/2022 14:16:00 - INFO - __main__ - Step 660 Global step 660 Train loss 1.075009 on epoch=164
05/21/2022 14:16:02 - INFO - __main__ - Step 670 Global step 670 Train loss 1.146414 on epoch=167
05/21/2022 14:16:05 - INFO - __main__ - Step 680 Global step 680 Train loss 1.137140 on epoch=169
05/21/2022 14:16:07 - INFO - __main__ - Step 690 Global step 690 Train loss 1.159340 on epoch=172
05/21/2022 14:16:10 - INFO - __main__ - Step 700 Global step 700 Train loss 1.569936 on epoch=174
05/21/2022 14:16:10 - INFO - __main__ - Global step 700 Train loss 1.217568 Classification-F1 0.5071428571428571 on epoch=174
05/21/2022 14:16:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.992634 on epoch=177
05/21/2022 14:16:16 - INFO - __main__ - Step 720 Global step 720 Train loss 1.427206 on epoch=179
05/21/2022 14:16:19 - INFO - __main__ - Step 730 Global step 730 Train loss 1.181202 on epoch=182
05/21/2022 14:16:21 - INFO - __main__ - Step 740 Global step 740 Train loss 1.194461 on epoch=184
05/21/2022 14:16:24 - INFO - __main__ - Step 750 Global step 750 Train loss 1.240501 on epoch=187
05/21/2022 14:16:24 - INFO - __main__ - Global step 750 Train loss 1.207201 Classification-F1 0.4868374868374868 on epoch=187
05/21/2022 14:16:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.995322 on epoch=189
05/21/2022 14:16:29 - INFO - __main__ - Step 770 Global step 770 Train loss 1.092723 on epoch=192
05/21/2022 14:16:32 - INFO - __main__ - Step 780 Global step 780 Train loss 1.061576 on epoch=194
05/21/2022 14:16:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.927470 on epoch=197
05/21/2022 14:16:37 - INFO - __main__ - Step 800 Global step 800 Train loss 1.163476 on epoch=199
05/21/2022 14:16:37 - INFO - __main__ - Global step 800 Train loss 1.048114 Classification-F1 0.4830956625074272 on epoch=199
05/21/2022 14:16:40 - INFO - __main__ - Step 810 Global step 810 Train loss 1.086941 on epoch=202
05/21/2022 14:16:42 - INFO - __main__ - Step 820 Global step 820 Train loss 1.335453 on epoch=204
05/21/2022 14:16:45 - INFO - __main__ - Step 830 Global step 830 Train loss 1.374597 on epoch=207
05/21/2022 14:16:47 - INFO - __main__ - Step 840 Global step 840 Train loss 1.131267 on epoch=209
05/21/2022 14:16:50 - INFO - __main__ - Step 850 Global step 850 Train loss 1.063754 on epoch=212
05/21/2022 14:16:50 - INFO - __main__ - Global step 850 Train loss 1.198402 Classification-F1 0.5106811145510836 on epoch=212
05/21/2022 14:16:53 - INFO - __main__ - Step 860 Global step 860 Train loss 1.199295 on epoch=214
05/21/2022 14:16:56 - INFO - __main__ - Step 870 Global step 870 Train loss 1.031544 on epoch=217
05/21/2022 14:16:58 - INFO - __main__ - Step 880 Global step 880 Train loss 1.303052 on epoch=219
05/21/2022 14:17:01 - INFO - __main__ - Step 890 Global step 890 Train loss 1.108434 on epoch=222
05/21/2022 14:17:04 - INFO - __main__ - Step 900 Global step 900 Train loss 1.101313 on epoch=224
05/21/2022 14:17:04 - INFO - __main__ - Global step 900 Train loss 1.148728 Classification-F1 0.537719298245614 on epoch=224
05/21/2022 14:17:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.909345 on epoch=227
05/21/2022 14:17:09 - INFO - __main__ - Step 920 Global step 920 Train loss 1.044569 on epoch=229
05/21/2022 14:17:12 - INFO - __main__ - Step 930 Global step 930 Train loss 1.065129 on epoch=232
05/21/2022 14:17:15 - INFO - __main__ - Step 940 Global step 940 Train loss 1.252564 on epoch=234
05/21/2022 14:17:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.974529 on epoch=237
05/21/2022 14:17:17 - INFO - __main__ - Global step 950 Train loss 1.049227 Classification-F1 0.5222886762360446 on epoch=237
05/21/2022 14:17:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.908524 on epoch=239
05/21/2022 14:17:23 - INFO - __main__ - Step 970 Global step 970 Train loss 1.066595 on epoch=242
05/21/2022 14:17:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.948404 on epoch=244
05/21/2022 14:17:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.902706 on epoch=247
05/21/2022 14:17:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.999560 on epoch=249
05/21/2022 14:17:31 - INFO - __main__ - Global step 1000 Train loss 0.965158 Classification-F1 0.49335874335874336 on epoch=249
05/21/2022 14:17:31 - INFO - __main__ - save last model!
05/21/2022 14:17:31 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:17:31 - INFO - __main__ - Printing 3 examples
05/21/2022 14:17:31 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/21/2022 14:17:31 - INFO - __main__ - ['others']
05/21/2022 14:17:31 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/21/2022 14:17:31 - INFO - __main__ - ['others']
05/21/2022 14:17:31 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/21/2022 14:17:31 - INFO - __main__ - ['others']
05/21/2022 14:17:31 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:17:31 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:17:31 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 14:17:31 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:17:31 - INFO - __main__ - Printing 3 examples
05/21/2022 14:17:31 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/21/2022 14:17:31 - INFO - __main__ - ['others']
05/21/2022 14:17:31 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/21/2022 14:17:31 - INFO - __main__ - ['others']
05/21/2022 14:17:31 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/21/2022 14:17:31 - INFO - __main__ - ['others']
05/21/2022 14:17:31 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:17:31 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:17:31 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 14:17:33 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 14:17:34 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 14:17:34 - INFO - __main__ - Printing 3 examples
05/21/2022 14:17:34 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 14:17:34 - INFO - __main__ - ['others']
05/21/2022 14:17:34 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 14:17:34 - INFO - __main__ - ['others']
05/21/2022 14:17:34 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 14:17:34 - INFO - __main__ - ['others']
05/21/2022 14:17:34 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:17:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 14:17:36 - INFO - __main__ - Starting training!
05/21/2022 14:17:36 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:17:41 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 14:18:09 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_42_0.0001_8_predictions.txt
05/21/2022 14:18:09 - INFO - __main__ - Classification-F1 on test data: 0.1529
05/21/2022 14:18:09 - INFO - __main__ - prefix=emo_16_42, lr=0.0001, bsz=8, dev_performance=0.537719298245614, test_performance=0.1528975225354412
05/21/2022 14:18:09 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.0005, bsz=8 ...
05/21/2022 14:18:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:18:10 - INFO - __main__ - Printing 3 examples
05/21/2022 14:18:10 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/21/2022 14:18:10 - INFO - __main__ - ['others']
05/21/2022 14:18:10 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/21/2022 14:18:10 - INFO - __main__ - ['others']
05/21/2022 14:18:10 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/21/2022 14:18:10 - INFO - __main__ - ['others']
05/21/2022 14:18:10 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:18:10 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:18:10 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 14:18:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:18:10 - INFO - __main__ - Printing 3 examples
05/21/2022 14:18:10 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/21/2022 14:18:10 - INFO - __main__ - ['others']
05/21/2022 14:18:10 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/21/2022 14:18:10 - INFO - __main__ - ['others']
05/21/2022 14:18:10 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/21/2022 14:18:10 - INFO - __main__ - ['others']
05/21/2022 14:18:10 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:18:10 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:18:10 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 14:18:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 14:18:14 - INFO - __main__ - Starting training!
05/21/2022 14:18:16 - INFO - __main__ - Step 10 Global step 10 Train loss 19.736971 on epoch=2
05/21/2022 14:18:19 - INFO - __main__ - Step 20 Global step 20 Train loss 13.522745 on epoch=4
05/21/2022 14:18:21 - INFO - __main__ - Step 30 Global step 30 Train loss 7.024580 on epoch=7
05/21/2022 14:18:24 - INFO - __main__ - Step 40 Global step 40 Train loss 5.755816 on epoch=9
05/21/2022 14:18:26 - INFO - __main__ - Step 50 Global step 50 Train loss 3.938964 on epoch=12
05/21/2022 14:18:27 - INFO - __main__ - Global step 50 Train loss 9.995815 Classification-F1 0.1 on epoch=12
05/21/2022 14:18:29 - INFO - __main__ - Step 60 Global step 60 Train loss 3.491011 on epoch=14
05/21/2022 14:18:32 - INFO - __main__ - Step 70 Global step 70 Train loss 2.525465 on epoch=17
05/21/2022 14:18:34 - INFO - __main__ - Step 80 Global step 80 Train loss 2.481082 on epoch=19
05/21/2022 14:18:37 - INFO - __main__ - Step 90 Global step 90 Train loss 2.469923 on epoch=22
05/21/2022 14:18:39 - INFO - __main__ - Step 100 Global step 100 Train loss 2.270663 on epoch=24
05/21/2022 14:18:40 - INFO - __main__ - Global step 100 Train loss 2.647629 Classification-F1 0.3541027710519236 on epoch=24
05/21/2022 14:18:43 - INFO - __main__ - Step 110 Global step 110 Train loss 1.399655 on epoch=27
05/21/2022 14:18:45 - INFO - __main__ - Step 120 Global step 120 Train loss 1.146433 on epoch=29
05/21/2022 14:18:47 - INFO - __main__ - Step 130 Global step 130 Train loss 1.496571 on epoch=32
05/21/2022 14:18:50 - INFO - __main__ - Step 140 Global step 140 Train loss 1.372790 on epoch=34
05/21/2022 14:18:52 - INFO - __main__ - Step 150 Global step 150 Train loss 1.021240 on epoch=37
05/21/2022 14:18:53 - INFO - __main__ - Global step 150 Train loss 1.287338 Classification-F1 0.6824592074592074 on epoch=37
05/21/2022 14:18:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.832503 on epoch=39
05/21/2022 14:18:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.970364 on epoch=42
05/21/2022 14:19:01 - INFO - __main__ - Step 180 Global step 180 Train loss 1.026563 on epoch=44
05/21/2022 14:19:03 - INFO - __main__ - Step 190 Global step 190 Train loss 1.261707 on epoch=47
05/21/2022 14:19:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.838137 on epoch=49
05/21/2022 14:19:06 - INFO - __main__ - Global step 200 Train loss 0.985855 Classification-F1 0.6302724660876138 on epoch=49
05/21/2022 14:19:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.849985 on epoch=52
05/21/2022 14:19:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.771126 on epoch=54
05/21/2022 14:19:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.951964 on epoch=57
05/21/2022 14:19:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.940963 on epoch=59
05/21/2022 14:19:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.896752 on epoch=62
05/21/2022 14:19:19 - INFO - __main__ - Global step 250 Train loss 0.882158 Classification-F1 0.6698329448329449 on epoch=62
05/21/2022 14:19:21 - INFO - __main__ - Step 260 Global step 260 Train loss 1.052186 on epoch=64
05/21/2022 14:19:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.744363 on epoch=67
05/21/2022 14:19:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.590314 on epoch=69
05/21/2022 14:19:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.667233 on epoch=72
05/21/2022 14:19:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.567460 on epoch=74
05/21/2022 14:19:32 - INFO - __main__ - Global step 300 Train loss 0.724311 Classification-F1 0.5859199280251912 on epoch=74
05/21/2022 14:19:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.495514 on epoch=77
05/21/2022 14:19:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.740189 on epoch=79
05/21/2022 14:19:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.645764 on epoch=82
05/21/2022 14:19:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.896815 on epoch=84
05/21/2022 14:19:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.885171 on epoch=87
05/21/2022 14:19:45 - INFO - __main__ - Global step 350 Train loss 0.732691 Classification-F1 0.6515087451530402 on epoch=87
05/21/2022 14:19:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.616605 on epoch=89
05/21/2022 14:19:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.814271 on epoch=92
05/21/2022 14:19:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.663735 on epoch=94
05/21/2022 14:19:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.632074 on epoch=97
05/21/2022 14:19:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.728465 on epoch=99
05/21/2022 14:19:57 - INFO - __main__ - Global step 400 Train loss 0.691030 Classification-F1 0.6360511326543936 on epoch=99
05/21/2022 14:20:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.538413 on epoch=102
05/21/2022 14:20:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.546857 on epoch=104
05/21/2022 14:20:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.655587 on epoch=107
05/21/2022 14:20:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.748998 on epoch=109
05/21/2022 14:20:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.854687 on epoch=112
05/21/2022 14:20:10 - INFO - __main__ - Global step 450 Train loss 0.668908 Classification-F1 0.7072029242731575 on epoch=112
05/21/2022 14:20:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.771636 on epoch=114
05/21/2022 14:20:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.910666 on epoch=117
05/21/2022 14:20:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.699909 on epoch=119
05/21/2022 14:20:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.613345 on epoch=122
05/21/2022 14:20:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.564697 on epoch=124
05/21/2022 14:20:24 - INFO - __main__ - Global step 500 Train loss 0.712051 Classification-F1 0.5236957282913165 on epoch=124
05/21/2022 14:20:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.716833 on epoch=127
05/21/2022 14:20:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.618839 on epoch=129
05/21/2022 14:20:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.778302 on epoch=132
05/21/2022 14:20:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.604818 on epoch=134
05/21/2022 14:20:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.601016 on epoch=137
05/21/2022 14:20:37 - INFO - __main__ - Global step 550 Train loss 0.663961 Classification-F1 0.6796423578751165 on epoch=137
05/21/2022 14:20:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.590153 on epoch=139
05/21/2022 14:20:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.551554 on epoch=142
05/21/2022 14:20:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.648984 on epoch=144
05/21/2022 14:20:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.298573 on epoch=147
05/21/2022 14:20:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.343003 on epoch=149
05/21/2022 14:20:50 - INFO - __main__ - Global step 600 Train loss 0.486453 Classification-F1 0.6934995737425406 on epoch=149
05/21/2022 14:20:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.428537 on epoch=152
05/21/2022 14:20:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.257205 on epoch=154
05/21/2022 14:20:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.306596 on epoch=157
05/21/2022 14:21:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.222915 on epoch=159
05/21/2022 14:21:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.212433 on epoch=162
05/21/2022 14:21:03 - INFO - __main__ - Global step 650 Train loss 0.285537 Classification-F1 0.7645405669599218 on epoch=162
05/21/2022 14:21:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.307462 on epoch=164
05/21/2022 14:21:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.178304 on epoch=167
05/21/2022 14:21:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.197698 on epoch=169
05/21/2022 14:21:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.117445 on epoch=172
05/21/2022 14:21:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.098542 on epoch=174
05/21/2022 14:21:16 - INFO - __main__ - Global step 700 Train loss 0.179890 Classification-F1 0.7558324028912264 on epoch=174
05/21/2022 14:21:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.150162 on epoch=177
05/21/2022 14:21:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.164202 on epoch=179
05/21/2022 14:21:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.221016 on epoch=182
05/21/2022 14:21:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.140818 on epoch=184
05/21/2022 14:21:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.078510 on epoch=187
05/21/2022 14:21:30 - INFO - __main__ - Global step 750 Train loss 0.150942 Classification-F1 0.7252309577677225 on epoch=187
05/21/2022 14:21:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.149463 on epoch=189
05/21/2022 14:21:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.090933 on epoch=192
05/21/2022 14:21:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.041779 on epoch=194
05/21/2022 14:21:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.183359 on epoch=197
05/21/2022 14:21:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.146283 on epoch=199
05/21/2022 14:21:43 - INFO - __main__ - Global step 800 Train loss 0.122364 Classification-F1 0.7097652681144142 on epoch=199
05/21/2022 14:21:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.151235 on epoch=202
05/21/2022 14:21:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.090045 on epoch=204
05/21/2022 14:21:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.133029 on epoch=207
05/21/2022 14:21:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.188133 on epoch=209
05/21/2022 14:21:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.184254 on epoch=212
05/21/2022 14:21:56 - INFO - __main__ - Global step 850 Train loss 0.149339 Classification-F1 0.7440159934941718 on epoch=212
05/21/2022 14:21:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.084493 on epoch=214
05/21/2022 14:22:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.077679 on epoch=217
05/21/2022 14:22:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.209689 on epoch=219
05/21/2022 14:22:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.126164 on epoch=222
05/21/2022 14:22:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.127438 on epoch=224
05/21/2022 14:22:09 - INFO - __main__ - Global step 900 Train loss 0.125093 Classification-F1 0.7601450257522364 on epoch=224
05/21/2022 14:22:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.097802 on epoch=227
05/21/2022 14:22:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.139331 on epoch=229
05/21/2022 14:22:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.056322 on epoch=232
05/21/2022 14:22:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.108906 on epoch=234
05/21/2022 14:22:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.127257 on epoch=237
05/21/2022 14:22:22 - INFO - __main__ - Global step 950 Train loss 0.105924 Classification-F1 0.7111996187363834 on epoch=237
05/21/2022 14:22:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.054392 on epoch=239
05/21/2022 14:22:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.056505 on epoch=242
05/21/2022 14:22:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.037612 on epoch=244
05/21/2022 14:22:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.032587 on epoch=247
05/21/2022 14:22:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.058651 on epoch=249
05/21/2022 14:22:35 - INFO - __main__ - Global step 1000 Train loss 0.047949 Classification-F1 0.7447129807530252 on epoch=249
05/21/2022 14:22:35 - INFO - __main__ - save last model!
05/21/2022 14:22:36 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:22:36 - INFO - __main__ - Printing 3 examples
05/21/2022 14:22:36 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/21/2022 14:22:36 - INFO - __main__ - ['others']
05/21/2022 14:22:36 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/21/2022 14:22:36 - INFO - __main__ - ['others']
05/21/2022 14:22:36 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/21/2022 14:22:36 - INFO - __main__ - ['others']
05/21/2022 14:22:36 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:22:36 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:22:36 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 14:22:36 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:22:36 - INFO - __main__ - Printing 3 examples
05/21/2022 14:22:36 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/21/2022 14:22:36 - INFO - __main__ - ['others']
05/21/2022 14:22:36 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/21/2022 14:22:36 - INFO - __main__ - ['others']
05/21/2022 14:22:36 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/21/2022 14:22:36 - INFO - __main__ - ['others']
05/21/2022 14:22:36 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:22:36 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:22:36 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 14:22:38 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 14:22:38 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 14:22:38 - INFO - __main__ - Printing 3 examples
05/21/2022 14:22:38 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 14:22:38 - INFO - __main__ - ['others']
05/21/2022 14:22:38 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 14:22:38 - INFO - __main__ - ['others']
05/21/2022 14:22:38 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 14:22:38 - INFO - __main__ - ['others']
05/21/2022 14:22:38 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:22:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 14:22:40 - INFO - __main__ - Starting training!
05/21/2022 14:22:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:22:46 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 14:23:14 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_87_0.0005_8_predictions.txt
05/21/2022 14:23:14 - INFO - __main__ - Classification-F1 on test data: 0.4266
05/21/2022 14:23:14 - INFO - __main__ - prefix=emo_16_87, lr=0.0005, bsz=8, dev_performance=0.7645405669599218, test_performance=0.4266098395438303
05/21/2022 14:23:14 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.0003, bsz=8 ...
05/21/2022 14:23:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:23:15 - INFO - __main__ - Printing 3 examples
05/21/2022 14:23:15 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/21/2022 14:23:15 - INFO - __main__ - ['others']
05/21/2022 14:23:15 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/21/2022 14:23:15 - INFO - __main__ - ['others']
05/21/2022 14:23:15 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/21/2022 14:23:15 - INFO - __main__ - ['others']
05/21/2022 14:23:15 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:23:15 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:23:15 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 14:23:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:23:15 - INFO - __main__ - Printing 3 examples
05/21/2022 14:23:15 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/21/2022 14:23:15 - INFO - __main__ - ['others']
05/21/2022 14:23:15 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/21/2022 14:23:15 - INFO - __main__ - ['others']
05/21/2022 14:23:15 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/21/2022 14:23:15 - INFO - __main__ - ['others']
05/21/2022 14:23:15 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:23:15 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:23:15 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 14:23:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 14:23:19 - INFO - __main__ - Starting training!
05/21/2022 14:23:21 - INFO - __main__ - Step 10 Global step 10 Train loss 21.232431 on epoch=2
05/21/2022 14:23:23 - INFO - __main__ - Step 20 Global step 20 Train loss 18.306307 on epoch=4
05/21/2022 14:23:26 - INFO - __main__ - Step 30 Global step 30 Train loss 10.778108 on epoch=7
05/21/2022 14:23:28 - INFO - __main__ - Step 40 Global step 40 Train loss 10.043188 on epoch=9
05/21/2022 14:23:31 - INFO - __main__ - Step 50 Global step 50 Train loss 6.270121 on epoch=12
05/21/2022 14:23:36 - INFO - __main__ - Global step 50 Train loss 13.326032 Classification-F1 0.0014792899408284025 on epoch=12
05/21/2022 14:23:39 - INFO - __main__ - Step 60 Global step 60 Train loss 6.755998 on epoch=14
05/21/2022 14:23:41 - INFO - __main__ - Step 70 Global step 70 Train loss 4.803172 on epoch=17
05/21/2022 14:23:44 - INFO - __main__ - Step 80 Global step 80 Train loss 3.686452 on epoch=19
05/21/2022 14:23:46 - INFO - __main__ - Step 90 Global step 90 Train loss 3.447287 on epoch=22
05/21/2022 14:23:49 - INFO - __main__ - Step 100 Global step 100 Train loss 2.998932 on epoch=24
05/21/2022 14:23:49 - INFO - __main__ - Global step 100 Train loss 4.338369 Classification-F1 0.13067758749069247 on epoch=24
05/21/2022 14:23:52 - INFO - __main__ - Step 110 Global step 110 Train loss 2.878742 on epoch=27
05/21/2022 14:23:55 - INFO - __main__ - Step 120 Global step 120 Train loss 2.049046 on epoch=29
05/21/2022 14:23:57 - INFO - __main__ - Step 130 Global step 130 Train loss 2.380112 on epoch=32
05/21/2022 14:24:00 - INFO - __main__ - Step 140 Global step 140 Train loss 2.190035 on epoch=34
05/21/2022 14:24:02 - INFO - __main__ - Step 150 Global step 150 Train loss 1.666915 on epoch=37
05/21/2022 14:24:03 - INFO - __main__ - Global step 150 Train loss 2.232970 Classification-F1 0.36199215965787596 on epoch=37
05/21/2022 14:24:05 - INFO - __main__ - Step 160 Global step 160 Train loss 2.539189 on epoch=39
05/21/2022 14:24:08 - INFO - __main__ - Step 170 Global step 170 Train loss 1.623471 on epoch=42
05/21/2022 14:24:10 - INFO - __main__ - Step 180 Global step 180 Train loss 1.855997 on epoch=44
05/21/2022 14:24:13 - INFO - __main__ - Step 190 Global step 190 Train loss 1.213165 on epoch=47
05/21/2022 14:24:16 - INFO - __main__ - Step 200 Global step 200 Train loss 1.528245 on epoch=49
05/21/2022 14:24:16 - INFO - __main__ - Global step 200 Train loss 1.752014 Classification-F1 0.4886496265172736 on epoch=49
05/21/2022 14:24:19 - INFO - __main__ - Step 210 Global step 210 Train loss 1.413488 on epoch=52
05/21/2022 14:24:21 - INFO - __main__ - Step 220 Global step 220 Train loss 1.184772 on epoch=54
05/21/2022 14:24:24 - INFO - __main__ - Step 230 Global step 230 Train loss 1.216046 on epoch=57
05/21/2022 14:24:26 - INFO - __main__ - Step 240 Global step 240 Train loss 1.294843 on epoch=59
05/21/2022 14:24:29 - INFO - __main__ - Step 250 Global step 250 Train loss 1.218281 on epoch=62
05/21/2022 14:24:29 - INFO - __main__ - Global step 250 Train loss 1.265486 Classification-F1 0.6396026831785346 on epoch=62
05/21/2022 14:24:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.524447 on epoch=64
05/21/2022 14:24:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.755071 on epoch=67
05/21/2022 14:24:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.831779 on epoch=69
05/21/2022 14:24:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.767410 on epoch=72
05/21/2022 14:24:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.789809 on epoch=74
05/21/2022 14:24:43 - INFO - __main__ - Global step 300 Train loss 0.733703 Classification-F1 0.6086083679833679 on epoch=74
05/21/2022 14:24:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.491948 on epoch=77
05/21/2022 14:24:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.824879 on epoch=79
05/21/2022 14:24:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.632454 on epoch=82
05/21/2022 14:24:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.637731 on epoch=84
05/21/2022 14:24:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.610705 on epoch=87
05/21/2022 14:24:56 - INFO - __main__ - Global step 350 Train loss 0.639543 Classification-F1 0.6795093795093794 on epoch=87
05/21/2022 14:24:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.583748 on epoch=89
05/21/2022 14:25:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.544670 on epoch=92
05/21/2022 14:25:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.356200 on epoch=94
05/21/2022 14:25:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.647723 on epoch=97
05/21/2022 14:25:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.633238 on epoch=99
05/21/2022 14:25:09 - INFO - __main__ - Global step 400 Train loss 0.553116 Classification-F1 0.6308558558558558 on epoch=99
05/21/2022 14:25:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.532766 on epoch=102
05/21/2022 14:25:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.559285 on epoch=104
05/21/2022 14:25:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.418977 on epoch=107
05/21/2022 14:25:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.526117 on epoch=109
05/21/2022 14:25:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.409038 on epoch=112
05/21/2022 14:25:22 - INFO - __main__ - Global step 450 Train loss 0.489237 Classification-F1 0.736638655462185 on epoch=112
05/21/2022 14:25:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.342286 on epoch=114
05/21/2022 14:25:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.284193 on epoch=117
05/21/2022 14:25:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.470620 on epoch=119
05/21/2022 14:25:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.483785 on epoch=122
05/21/2022 14:25:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.492757 on epoch=124
05/21/2022 14:25:35 - INFO - __main__ - Global step 500 Train loss 0.414728 Classification-F1 0.6558558558558558 on epoch=124
05/21/2022 14:25:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.419809 on epoch=127
05/21/2022 14:25:40 - INFO - __main__ - Step 520 Global step 520 Train loss 1.029251 on epoch=129
05/21/2022 14:25:43 - INFO - __main__ - Step 530 Global step 530 Train loss 1.174029 on epoch=132
05/21/2022 14:25:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.519481 on epoch=134
05/21/2022 14:25:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.381812 on epoch=137
05/21/2022 14:25:48 - INFO - __main__ - Global step 550 Train loss 0.704876 Classification-F1 0.7904804804804805 on epoch=137
05/21/2022 14:25:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.384470 on epoch=139
05/21/2022 14:25:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.373137 on epoch=142
05/21/2022 14:25:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.207784 on epoch=144
05/21/2022 14:25:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.290279 on epoch=147
05/21/2022 14:26:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.125024 on epoch=149
05/21/2022 14:26:02 - INFO - __main__ - Global step 600 Train loss 0.276139 Classification-F1 0.7215432824128476 on epoch=149
05/21/2022 14:26:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.325276 on epoch=152
05/21/2022 14:26:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.241823 on epoch=154
05/21/2022 14:26:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.178045 on epoch=157
05/21/2022 14:26:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.291337 on epoch=159
05/21/2022 14:26:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.182360 on epoch=162
05/21/2022 14:26:15 - INFO - __main__ - Global step 650 Train loss 0.243768 Classification-F1 0.7741891891891891 on epoch=162
05/21/2022 14:26:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.192884 on epoch=164
05/21/2022 14:26:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.206367 on epoch=167
05/21/2022 14:26:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.171828 on epoch=169
05/21/2022 14:26:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.169324 on epoch=172
05/21/2022 14:26:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.243195 on epoch=174
05/21/2022 14:26:28 - INFO - __main__ - Global step 700 Train loss 0.196720 Classification-F1 0.6859903381642511 on epoch=174
05/21/2022 14:26:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.139962 on epoch=177
05/21/2022 14:26:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.233864 on epoch=179
05/21/2022 14:26:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.155839 on epoch=182
05/21/2022 14:26:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.154621 on epoch=184
05/21/2022 14:26:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.065830 on epoch=187
05/21/2022 14:26:41 - INFO - __main__ - Global step 750 Train loss 0.150024 Classification-F1 0.6457108829774364 on epoch=187
05/21/2022 14:26:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.213679 on epoch=189
05/21/2022 14:26:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.240698 on epoch=192
05/21/2022 14:26:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.093774 on epoch=194
05/21/2022 14:26:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.111670 on epoch=197
05/21/2022 14:26:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.129637 on epoch=199
05/21/2022 14:26:54 - INFO - __main__ - Global step 800 Train loss 0.157891 Classification-F1 0.739519761697181 on epoch=199
05/21/2022 14:26:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.081556 on epoch=202
05/21/2022 14:27:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.082180 on epoch=204
05/21/2022 14:27:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.075290 on epoch=207
05/21/2022 14:27:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.211163 on epoch=209
05/21/2022 14:27:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.106323 on epoch=212
05/21/2022 14:27:08 - INFO - __main__ - Global step 850 Train loss 0.111302 Classification-F1 0.7673611111111112 on epoch=212
05/21/2022 14:27:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.104486 on epoch=214
05/21/2022 14:27:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.110281 on epoch=217
05/21/2022 14:27:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.057271 on epoch=219
05/21/2022 14:27:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.029600 on epoch=222
05/21/2022 14:27:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.127194 on epoch=224
05/21/2022 14:27:21 - INFO - __main__ - Global step 900 Train loss 0.085766 Classification-F1 0.6928096545743605 on epoch=224
05/21/2022 14:27:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.095698 on epoch=227
05/21/2022 14:27:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.046395 on epoch=229
05/21/2022 14:27:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.045527 on epoch=232
05/21/2022 14:27:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.106064 on epoch=234
05/21/2022 14:27:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.030559 on epoch=237
05/21/2022 14:27:34 - INFO - __main__ - Global step 950 Train loss 0.064849 Classification-F1 0.7721212121212122 on epoch=237
05/21/2022 14:27:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.020303 on epoch=239
05/21/2022 14:27:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.075400 on epoch=242
05/21/2022 14:27:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.072006 on epoch=244
05/21/2022 14:27:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.047109 on epoch=247
05/21/2022 14:27:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.040909 on epoch=249
05/21/2022 14:27:47 - INFO - __main__ - Global step 1000 Train loss 0.051145 Classification-F1 0.7242782555282555 on epoch=249
05/21/2022 14:27:47 - INFO - __main__ - save last model!
05/21/2022 14:27:48 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:27:48 - INFO - __main__ - Printing 3 examples
05/21/2022 14:27:48 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/21/2022 14:27:48 - INFO - __main__ - ['others']
05/21/2022 14:27:48 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/21/2022 14:27:48 - INFO - __main__ - ['others']
05/21/2022 14:27:48 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/21/2022 14:27:48 - INFO - __main__ - ['others']
05/21/2022 14:27:48 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:27:48 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:27:48 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 14:27:48 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:27:48 - INFO - __main__ - Printing 3 examples
05/21/2022 14:27:48 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/21/2022 14:27:48 - INFO - __main__ - ['others']
05/21/2022 14:27:48 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/21/2022 14:27:48 - INFO - __main__ - ['others']
05/21/2022 14:27:48 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/21/2022 14:27:48 - INFO - __main__ - ['others']
05/21/2022 14:27:48 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:27:48 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:27:48 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 14:27:49 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 14:27:50 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 14:27:50 - INFO - __main__ - Printing 3 examples
05/21/2022 14:27:50 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 14:27:50 - INFO - __main__ - ['others']
05/21/2022 14:27:50 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 14:27:50 - INFO - __main__ - ['others']
05/21/2022 14:27:50 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 14:27:50 - INFO - __main__ - ['others']
05/21/2022 14:27:50 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:27:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 14:27:51 - INFO - __main__ - Starting training!
05/21/2022 14:27:52 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:27:57 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 14:28:26 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_87_0.0003_8_predictions.txt
05/21/2022 14:28:26 - INFO - __main__ - Classification-F1 on test data: 0.4593
05/21/2022 14:28:26 - INFO - __main__ - prefix=emo_16_87, lr=0.0003, bsz=8, dev_performance=0.7904804804804805, test_performance=0.45932463797624035
05/21/2022 14:28:26 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.0002, bsz=8 ...
05/21/2022 14:28:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:28:27 - INFO - __main__ - Printing 3 examples
05/21/2022 14:28:27 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/21/2022 14:28:27 - INFO - __main__ - ['others']
05/21/2022 14:28:27 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/21/2022 14:28:27 - INFO - __main__ - ['others']
05/21/2022 14:28:27 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/21/2022 14:28:27 - INFO - __main__ - ['others']
05/21/2022 14:28:27 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:28:27 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:28:27 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 14:28:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:28:27 - INFO - __main__ - Printing 3 examples
05/21/2022 14:28:27 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/21/2022 14:28:27 - INFO - __main__ - ['others']
05/21/2022 14:28:27 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/21/2022 14:28:27 - INFO - __main__ - ['others']
05/21/2022 14:28:27 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/21/2022 14:28:27 - INFO - __main__ - ['others']
05/21/2022 14:28:27 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:28:27 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:28:27 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 14:28:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 14:28:31 - INFO - __main__ - Starting training!
05/21/2022 14:28:33 - INFO - __main__ - Step 10 Global step 10 Train loss 19.732557 on epoch=2
05/21/2022 14:28:36 - INFO - __main__ - Step 20 Global step 20 Train loss 19.679234 on epoch=4
05/21/2022 14:28:38 - INFO - __main__ - Step 30 Global step 30 Train loss 13.481415 on epoch=7
05/21/2022 14:28:41 - INFO - __main__ - Step 40 Global step 40 Train loss 10.151956 on epoch=9
05/21/2022 14:28:43 - INFO - __main__ - Step 50 Global step 50 Train loss 9.041439 on epoch=12
05/21/2022 14:28:50 - INFO - __main__ - Global step 50 Train loss 14.417320 Classification-F1 0.0 on epoch=12
05/21/2022 14:28:53 - INFO - __main__ - Step 60 Global step 60 Train loss 7.662802 on epoch=14
05/21/2022 14:28:55 - INFO - __main__ - Step 70 Global step 70 Train loss 6.638319 on epoch=17
05/21/2022 14:28:58 - INFO - __main__ - Step 80 Global step 80 Train loss 5.148648 on epoch=19
05/21/2022 14:29:01 - INFO - __main__ - Step 90 Global step 90 Train loss 5.359299 on epoch=22
05/21/2022 14:29:03 - INFO - __main__ - Step 100 Global step 100 Train loss 5.511099 on epoch=24
05/21/2022 14:29:03 - INFO - __main__ - Global step 100 Train loss 6.064034 Classification-F1 0.1 on epoch=24
05/21/2022 14:29:06 - INFO - __main__ - Step 110 Global step 110 Train loss 3.184741 on epoch=27
05/21/2022 14:29:09 - INFO - __main__ - Step 120 Global step 120 Train loss 4.093537 on epoch=29
05/21/2022 14:29:11 - INFO - __main__ - Step 130 Global step 130 Train loss 3.492687 on epoch=32
05/21/2022 14:29:14 - INFO - __main__ - Step 140 Global step 140 Train loss 3.201585 on epoch=34
05/21/2022 14:29:16 - INFO - __main__ - Step 150 Global step 150 Train loss 2.108763 on epoch=37
05/21/2022 14:29:17 - INFO - __main__ - Global step 150 Train loss 3.216263 Classification-F1 0.1 on epoch=37
05/21/2022 14:29:19 - INFO - __main__ - Step 160 Global step 160 Train loss 3.289022 on epoch=39
05/21/2022 14:29:22 - INFO - __main__ - Step 170 Global step 170 Train loss 3.303216 on epoch=42
05/21/2022 14:29:24 - INFO - __main__ - Step 180 Global step 180 Train loss 2.519246 on epoch=44
05/21/2022 14:29:27 - INFO - __main__ - Step 190 Global step 190 Train loss 1.947860 on epoch=47
05/21/2022 14:29:30 - INFO - __main__ - Step 200 Global step 200 Train loss 2.716340 on epoch=49
05/21/2022 14:29:30 - INFO - __main__ - Global step 200 Train loss 2.755137 Classification-F1 0.31958776250972254 on epoch=49
05/21/2022 14:29:33 - INFO - __main__ - Step 210 Global step 210 Train loss 2.305012 on epoch=52
05/21/2022 14:29:35 - INFO - __main__ - Step 220 Global step 220 Train loss 2.005485 on epoch=54
05/21/2022 14:29:38 - INFO - __main__ - Step 230 Global step 230 Train loss 2.436990 on epoch=57
05/21/2022 14:29:40 - INFO - __main__ - Step 240 Global step 240 Train loss 2.173930 on epoch=59
05/21/2022 14:29:43 - INFO - __main__ - Step 250 Global step 250 Train loss 1.928057 on epoch=62
05/21/2022 14:29:43 - INFO - __main__ - Global step 250 Train loss 2.169895 Classification-F1 0.4403929403929404 on epoch=62
05/21/2022 14:29:46 - INFO - __main__ - Step 260 Global step 260 Train loss 1.880750 on epoch=64
05/21/2022 14:29:49 - INFO - __main__ - Step 270 Global step 270 Train loss 1.904463 on epoch=67
05/21/2022 14:29:51 - INFO - __main__ - Step 280 Global step 280 Train loss 1.272152 on epoch=69
05/21/2022 14:29:54 - INFO - __main__ - Step 290 Global step 290 Train loss 1.373799 on epoch=72
05/21/2022 14:29:56 - INFO - __main__ - Step 300 Global step 300 Train loss 1.088235 on epoch=74
05/21/2022 14:29:57 - INFO - __main__ - Global step 300 Train loss 1.503880 Classification-F1 0.549807576604448 on epoch=74
05/21/2022 14:29:59 - INFO - __main__ - Step 310 Global step 310 Train loss 1.655835 on epoch=77
05/21/2022 14:30:02 - INFO - __main__ - Step 320 Global step 320 Train loss 1.756117 on epoch=79
05/21/2022 14:30:04 - INFO - __main__ - Step 330 Global step 330 Train loss 1.786130 on epoch=82
05/21/2022 14:30:07 - INFO - __main__ - Step 340 Global step 340 Train loss 1.259457 on epoch=84
05/21/2022 14:30:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.931179 on epoch=87
05/21/2022 14:30:10 - INFO - __main__ - Global step 350 Train loss 1.477744 Classification-F1 0.5552336392119674 on epoch=87
05/21/2022 14:30:13 - INFO - __main__ - Step 360 Global step 360 Train loss 1.542276 on epoch=89
05/21/2022 14:30:15 - INFO - __main__ - Step 370 Global step 370 Train loss 1.684364 on epoch=92
05/21/2022 14:30:18 - INFO - __main__ - Step 380 Global step 380 Train loss 1.107174 on epoch=94
05/21/2022 14:30:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.917991 on epoch=97
05/21/2022 14:30:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.888163 on epoch=99
05/21/2022 14:30:23 - INFO - __main__ - Global step 400 Train loss 1.227993 Classification-F1 0.5680245381858285 on epoch=99
05/21/2022 14:30:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.946626 on epoch=102
05/21/2022 14:30:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.874961 on epoch=104
05/21/2022 14:30:31 - INFO - __main__ - Step 430 Global step 430 Train loss 1.247030 on epoch=107
05/21/2022 14:30:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.807333 on epoch=109
05/21/2022 14:30:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.864573 on epoch=112
05/21/2022 14:30:37 - INFO - __main__ - Global step 450 Train loss 0.948105 Classification-F1 0.6120414673046252 on epoch=112
05/21/2022 14:30:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.744264 on epoch=114
05/21/2022 14:30:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.940925 on epoch=117
05/21/2022 14:30:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.690314 on epoch=119
05/21/2022 14:30:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.710353 on epoch=122
05/21/2022 14:30:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.730009 on epoch=124
05/21/2022 14:30:50 - INFO - __main__ - Global step 500 Train loss 0.763173 Classification-F1 0.5839263076105181 on epoch=124
05/21/2022 14:30:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.674365 on epoch=127
05/21/2022 14:30:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.573164 on epoch=129
05/21/2022 14:30:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.708767 on epoch=132
05/21/2022 14:31:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.550526 on epoch=134
05/21/2022 14:31:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.701598 on epoch=137
05/21/2022 14:31:03 - INFO - __main__ - Global step 550 Train loss 0.641684 Classification-F1 0.5835889256941889 on epoch=137
05/21/2022 14:31:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.535890 on epoch=139
05/21/2022 14:31:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.774242 on epoch=142
05/21/2022 14:31:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.624461 on epoch=144
05/21/2022 14:31:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.501402 on epoch=147
05/21/2022 14:31:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.549546 on epoch=149
05/21/2022 14:31:16 - INFO - __main__ - Global step 600 Train loss 0.597108 Classification-F1 0.568816974247841 on epoch=149
05/21/2022 14:31:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.537877 on epoch=152
05/21/2022 14:31:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.632932 on epoch=154
05/21/2022 14:31:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.417367 on epoch=157
05/21/2022 14:31:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.537404 on epoch=159
05/21/2022 14:31:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.471380 on epoch=162
05/21/2022 14:31:29 - INFO - __main__ - Global step 650 Train loss 0.519392 Classification-F1 0.6623756360598465 on epoch=162
05/21/2022 14:31:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.588970 on epoch=164
05/21/2022 14:31:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.690749 on epoch=167
05/21/2022 14:31:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.589263 on epoch=169
05/21/2022 14:31:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.405943 on epoch=172
05/21/2022 14:31:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.429251 on epoch=174
05/21/2022 14:31:43 - INFO - __main__ - Global step 700 Train loss 0.540835 Classification-F1 0.6713629507747154 on epoch=174
05/21/2022 14:31:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.391248 on epoch=177
05/21/2022 14:31:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.375243 on epoch=179
05/21/2022 14:31:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.397821 on epoch=182
05/21/2022 14:31:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.525810 on epoch=184
05/21/2022 14:31:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.432974 on epoch=187
05/21/2022 14:31:56 - INFO - __main__ - Global step 750 Train loss 0.424619 Classification-F1 0.7263194444444444 on epoch=187
05/21/2022 14:31:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.277049 on epoch=189
05/21/2022 14:32:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.302545 on epoch=192
05/21/2022 14:32:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.442522 on epoch=194
05/21/2022 14:32:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.395417 on epoch=197
05/21/2022 14:32:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.411229 on epoch=199
05/21/2022 14:32:09 - INFO - __main__ - Global step 800 Train loss 0.365752 Classification-F1 0.7412443693693693 on epoch=199
05/21/2022 14:32:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.391852 on epoch=202
05/21/2022 14:32:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.366756 on epoch=204
05/21/2022 14:32:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.325464 on epoch=207
05/21/2022 14:32:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.316992 on epoch=209
05/21/2022 14:32:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.244030 on epoch=212
05/21/2022 14:32:23 - INFO - __main__ - Global step 850 Train loss 0.329019 Classification-F1 0.7270865583075335 on epoch=212
05/21/2022 14:32:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.541865 on epoch=214
05/21/2022 14:32:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.521105 on epoch=217
05/21/2022 14:32:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.384777 on epoch=219
05/21/2022 14:32:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.651712 on epoch=222
05/21/2022 14:32:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.782868 on epoch=224
05/21/2022 14:32:36 - INFO - __main__ - Global step 900 Train loss 0.576465 Classification-F1 0.7094384707287933 on epoch=224
05/21/2022 14:32:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.604878 on epoch=227
05/21/2022 14:32:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.666953 on epoch=229
05/21/2022 14:32:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.294143 on epoch=232
05/21/2022 14:32:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.399804 on epoch=234
05/21/2022 14:32:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.262428 on epoch=237
05/21/2022 14:32:49 - INFO - __main__ - Global step 950 Train loss 0.445641 Classification-F1 0.7769100743745773 on epoch=237
05/21/2022 14:32:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.475954 on epoch=239
05/21/2022 14:32:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.410005 on epoch=242
05/21/2022 14:32:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.355972 on epoch=244
05/21/2022 14:32:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.269908 on epoch=247
05/21/2022 14:33:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.309281 on epoch=249
05/21/2022 14:33:02 - INFO - __main__ - Global step 1000 Train loss 0.364224 Classification-F1 0.7743347338935574 on epoch=249
05/21/2022 14:33:02 - INFO - __main__ - save last model!
05/21/2022 14:33:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:33:03 - INFO - __main__ - Printing 3 examples
05/21/2022 14:33:03 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/21/2022 14:33:03 - INFO - __main__ - ['others']
05/21/2022 14:33:03 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/21/2022 14:33:03 - INFO - __main__ - ['others']
05/21/2022 14:33:03 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/21/2022 14:33:03 - INFO - __main__ - ['others']
05/21/2022 14:33:03 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:33:03 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:33:03 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 14:33:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:33:03 - INFO - __main__ - Printing 3 examples
05/21/2022 14:33:03 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/21/2022 14:33:03 - INFO - __main__ - ['others']
05/21/2022 14:33:03 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/21/2022 14:33:03 - INFO - __main__ - ['others']
05/21/2022 14:33:03 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/21/2022 14:33:03 - INFO - __main__ - ['others']
05/21/2022 14:33:03 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:33:03 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:33:03 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 14:33:05 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 14:33:05 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 14:33:05 - INFO - __main__ - Printing 3 examples
05/21/2022 14:33:05 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 14:33:05 - INFO - __main__ - ['others']
05/21/2022 14:33:05 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 14:33:05 - INFO - __main__ - ['others']
05/21/2022 14:33:05 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 14:33:05 - INFO - __main__ - ['others']
05/21/2022 14:33:05 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:33:07 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:33:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 14:33:07 - INFO - __main__ - Starting training!
05/21/2022 14:33:12 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 14:33:40 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_87_0.0002_8_predictions.txt
05/21/2022 14:33:41 - INFO - __main__ - Classification-F1 on test data: 0.5334
05/21/2022 14:33:41 - INFO - __main__ - prefix=emo_16_87, lr=0.0002, bsz=8, dev_performance=0.7769100743745773, test_performance=0.5334462112668948
05/21/2022 14:33:41 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.0001, bsz=8 ...
05/21/2022 14:33:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:33:42 - INFO - __main__ - Printing 3 examples
05/21/2022 14:33:42 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/21/2022 14:33:42 - INFO - __main__ - ['others']
05/21/2022 14:33:42 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/21/2022 14:33:42 - INFO - __main__ - ['others']
05/21/2022 14:33:42 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/21/2022 14:33:42 - INFO - __main__ - ['others']
05/21/2022 14:33:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:33:42 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:33:42 - INFO - __main__ - Loaded 64 examples from train data
05/21/2022 14:33:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/21/2022 14:33:42 - INFO - __main__ - Printing 3 examples
05/21/2022 14:33:42 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/21/2022 14:33:42 - INFO - __main__ - ['others']
05/21/2022 14:33:42 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/21/2022 14:33:42 - INFO - __main__ - ['others']
05/21/2022 14:33:42 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/21/2022 14:33:42 - INFO - __main__ - ['others']
05/21/2022 14:33:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:33:42 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:33:42 - INFO - __main__ - Loaded 64 examples from dev data
05/21/2022 14:33:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 14:33:46 - INFO - __main__ - Starting training!
05/21/2022 14:33:48 - INFO - __main__ - Step 10 Global step 10 Train loss 19.387754 on epoch=2
05/21/2022 14:33:50 - INFO - __main__ - Step 20 Global step 20 Train loss 20.286346 on epoch=4
05/21/2022 14:33:53 - INFO - __main__ - Step 30 Global step 30 Train loss 16.444752 on epoch=7
05/21/2022 14:33:55 - INFO - __main__ - Step 40 Global step 40 Train loss 13.456749 on epoch=9
05/21/2022 14:33:58 - INFO - __main__ - Step 50 Global step 50 Train loss 11.632437 on epoch=12
05/21/2022 14:34:10 - INFO - __main__ - Global step 50 Train loss 16.241610 Classification-F1 0.0 on epoch=12
05/21/2022 14:34:13 - INFO - __main__ - Step 60 Global step 60 Train loss 10.621569 on epoch=14
05/21/2022 14:34:16 - INFO - __main__ - Step 70 Global step 70 Train loss 8.574507 on epoch=17
05/21/2022 14:34:18 - INFO - __main__ - Step 80 Global step 80 Train loss 8.394670 on epoch=19
05/21/2022 14:34:21 - INFO - __main__ - Step 90 Global step 90 Train loss 7.822206 on epoch=22
05/21/2022 14:34:23 - INFO - __main__ - Step 100 Global step 100 Train loss 6.169219 on epoch=24
05/21/2022 14:34:30 - INFO - __main__ - Global step 100 Train loss 8.316434 Classification-F1 0.0 on epoch=24
05/21/2022 14:34:33 - INFO - __main__ - Step 110 Global step 110 Train loss 6.878873 on epoch=27
05/21/2022 14:34:35 - INFO - __main__ - Step 120 Global step 120 Train loss 6.439587 on epoch=29
05/21/2022 14:34:38 - INFO - __main__ - Step 130 Global step 130 Train loss 5.196949 on epoch=32
05/21/2022 14:34:41 - INFO - __main__ - Step 140 Global step 140 Train loss 6.091989 on epoch=34
05/21/2022 14:34:43 - INFO - __main__ - Step 150 Global step 150 Train loss 5.321081 on epoch=37
05/21/2022 14:34:46 - INFO - __main__ - Global step 150 Train loss 5.985696 Classification-F1 0.052631578947368425 on epoch=37
05/21/2022 14:34:49 - INFO - __main__ - Step 160 Global step 160 Train loss 5.910367 on epoch=39
05/21/2022 14:34:51 - INFO - __main__ - Step 170 Global step 170 Train loss 5.454767 on epoch=42
05/21/2022 14:34:54 - INFO - __main__ - Step 180 Global step 180 Train loss 3.738188 on epoch=44
05/21/2022 14:34:57 - INFO - __main__ - Step 190 Global step 190 Train loss 4.136756 on epoch=47
05/21/2022 14:34:59 - INFO - __main__ - Step 200 Global step 200 Train loss 4.208047 on epoch=49
05/21/2022 14:35:00 - INFO - __main__ - Global step 200 Train loss 4.689625 Classification-F1 0.1 on epoch=49
05/21/2022 14:35:03 - INFO - __main__ - Step 210 Global step 210 Train loss 3.826185 on epoch=52
05/21/2022 14:35:06 - INFO - __main__ - Step 220 Global step 220 Train loss 3.897552 on epoch=54
05/21/2022 14:35:08 - INFO - __main__ - Step 230 Global step 230 Train loss 2.804317 on epoch=57
05/21/2022 14:35:11 - INFO - __main__ - Step 240 Global step 240 Train loss 3.068241 on epoch=59
05/21/2022 14:35:14 - INFO - __main__ - Step 250 Global step 250 Train loss 2.532011 on epoch=62
05/21/2022 14:35:14 - INFO - __main__ - Global step 250 Train loss 3.225661 Classification-F1 0.10126582278481013 on epoch=62
05/21/2022 14:35:17 - INFO - __main__ - Step 260 Global step 260 Train loss 2.705453 on epoch=64
05/21/2022 14:35:19 - INFO - __main__ - Step 270 Global step 270 Train loss 2.535949 on epoch=67
05/21/2022 14:35:22 - INFO - __main__ - Step 280 Global step 280 Train loss 3.154717 on epoch=69
05/21/2022 14:35:25 - INFO - __main__ - Step 290 Global step 290 Train loss 2.501947 on epoch=72
05/21/2022 14:35:27 - INFO - __main__ - Step 300 Global step 300 Train loss 2.874803 on epoch=74
05/21/2022 14:35:28 - INFO - __main__ - Global step 300 Train loss 2.754574 Classification-F1 0.28476972184222527 on epoch=74
05/21/2022 14:35:31 - INFO - __main__ - Step 310 Global step 310 Train loss 2.246427 on epoch=77
05/21/2022 14:35:33 - INFO - __main__ - Step 320 Global step 320 Train loss 2.602578 on epoch=79
05/21/2022 14:35:36 - INFO - __main__ - Step 330 Global step 330 Train loss 2.766542 on epoch=82
05/21/2022 14:35:38 - INFO - __main__ - Step 340 Global step 340 Train loss 2.146891 on epoch=84
05/21/2022 14:35:41 - INFO - __main__ - Step 350 Global step 350 Train loss 2.729198 on epoch=87
05/21/2022 14:35:41 - INFO - __main__ - Global step 350 Train loss 2.498327 Classification-F1 0.3968818322401749 on epoch=87
05/21/2022 14:35:44 - INFO - __main__ - Step 360 Global step 360 Train loss 2.299375 on epoch=89
05/21/2022 14:35:47 - INFO - __main__ - Step 370 Global step 370 Train loss 2.111692 on epoch=92
05/21/2022 14:35:50 - INFO - __main__ - Step 380 Global step 380 Train loss 2.767159 on epoch=94
05/21/2022 14:35:52 - INFO - __main__ - Step 390 Global step 390 Train loss 2.433788 on epoch=97
05/21/2022 14:35:55 - INFO - __main__ - Step 400 Global step 400 Train loss 1.889946 on epoch=99
05/21/2022 14:35:55 - INFO - __main__ - Global step 400 Train loss 2.300392 Classification-F1 0.3220649895178197 on epoch=99
05/21/2022 14:35:58 - INFO - __main__ - Step 410 Global step 410 Train loss 2.168242 on epoch=102
05/21/2022 14:36:00 - INFO - __main__ - Step 420 Global step 420 Train loss 2.078489 on epoch=104
05/21/2022 14:36:03 - INFO - __main__ - Step 430 Global step 430 Train loss 2.115144 on epoch=107
05/21/2022 14:36:06 - INFO - __main__ - Step 440 Global step 440 Train loss 1.952581 on epoch=109
05/21/2022 14:36:08 - INFO - __main__ - Step 450 Global step 450 Train loss 1.939342 on epoch=112
05/21/2022 14:36:09 - INFO - __main__ - Global step 450 Train loss 2.050760 Classification-F1 0.35034013605442177 on epoch=112
05/21/2022 14:36:11 - INFO - __main__ - Step 460 Global step 460 Train loss 1.641681 on epoch=114
05/21/2022 14:36:14 - INFO - __main__ - Step 470 Global step 470 Train loss 1.800286 on epoch=117
05/21/2022 14:36:16 - INFO - __main__ - Step 480 Global step 480 Train loss 1.889125 on epoch=119
05/21/2022 14:36:19 - INFO - __main__ - Step 490 Global step 490 Train loss 1.586793 on epoch=122
05/21/2022 14:36:22 - INFO - __main__ - Step 500 Global step 500 Train loss 2.065584 on epoch=124
05/21/2022 14:36:22 - INFO - __main__ - Global step 500 Train loss 1.796694 Classification-F1 0.33979229989868287 on epoch=124
05/21/2022 14:36:25 - INFO - __main__ - Step 510 Global step 510 Train loss 1.867655 on epoch=127
05/21/2022 14:36:27 - INFO - __main__ - Step 520 Global step 520 Train loss 1.550464 on epoch=129
05/21/2022 14:36:30 - INFO - __main__ - Step 530 Global step 530 Train loss 1.617828 on epoch=132
05/21/2022 14:36:33 - INFO - __main__ - Step 540 Global step 540 Train loss 1.371469 on epoch=134
05/21/2022 14:36:35 - INFO - __main__ - Step 550 Global step 550 Train loss 2.102063 on epoch=137
05/21/2022 14:36:36 - INFO - __main__ - Global step 550 Train loss 1.701896 Classification-F1 0.44248441182108267 on epoch=137
05/21/2022 14:36:39 - INFO - __main__ - Step 560 Global step 560 Train loss 1.459978 on epoch=139
05/21/2022 14:36:41 - INFO - __main__ - Step 570 Global step 570 Train loss 1.640768 on epoch=142
05/21/2022 14:36:44 - INFO - __main__ - Step 580 Global step 580 Train loss 1.235617 on epoch=144
05/21/2022 14:36:46 - INFO - __main__ - Step 590 Global step 590 Train loss 1.377176 on epoch=147
05/21/2022 14:36:49 - INFO - __main__ - Step 600 Global step 600 Train loss 1.173108 on epoch=149
05/21/2022 14:36:49 - INFO - __main__ - Global step 600 Train loss 1.377330 Classification-F1 0.43677181580407387 on epoch=149
05/21/2022 14:36:52 - INFO - __main__ - Step 610 Global step 610 Train loss 1.299426 on epoch=152
05/21/2022 14:36:55 - INFO - __main__ - Step 620 Global step 620 Train loss 1.829477 on epoch=154
05/21/2022 14:36:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.979989 on epoch=157
05/21/2022 14:37:00 - INFO - __main__ - Step 640 Global step 640 Train loss 1.082058 on epoch=159
05/21/2022 14:37:02 - INFO - __main__ - Step 650 Global step 650 Train loss 1.404869 on epoch=162
05/21/2022 14:37:03 - INFO - __main__ - Global step 650 Train loss 1.319164 Classification-F1 0.4625 on epoch=162
05/21/2022 14:37:06 - INFO - __main__ - Step 660 Global step 660 Train loss 1.108610 on epoch=164
05/21/2022 14:37:08 - INFO - __main__ - Step 670 Global step 670 Train loss 1.769799 on epoch=167
05/21/2022 14:37:11 - INFO - __main__ - Step 680 Global step 680 Train loss 1.178465 on epoch=169
05/21/2022 14:37:14 - INFO - __main__ - Step 690 Global step 690 Train loss 1.096216 on epoch=172
05/21/2022 14:37:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.950878 on epoch=174
05/21/2022 14:37:17 - INFO - __main__ - Global step 700 Train loss 1.220793 Classification-F1 0.44167187825724413 on epoch=174
05/21/2022 14:37:19 - INFO - __main__ - Step 710 Global step 710 Train loss 1.037631 on epoch=177
05/21/2022 14:37:22 - INFO - __main__ - Step 720 Global step 720 Train loss 1.495732 on epoch=179
05/21/2022 14:37:25 - INFO - __main__ - Step 730 Global step 730 Train loss 1.126567 on epoch=182
05/21/2022 14:37:27 - INFO - __main__ - Step 740 Global step 740 Train loss 1.311848 on epoch=184
05/21/2022 14:37:30 - INFO - __main__ - Step 750 Global step 750 Train loss 1.387041 on epoch=187
05/21/2022 14:37:30 - INFO - __main__ - Global step 750 Train loss 1.271764 Classification-F1 0.4805382305382305 on epoch=187
05/21/2022 14:37:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.893157 on epoch=189
05/21/2022 14:37:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.996687 on epoch=192
05/21/2022 14:37:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.988108 on epoch=194
05/21/2022 14:37:41 - INFO - __main__ - Step 790 Global step 790 Train loss 1.135987 on epoch=197
05/21/2022 14:37:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.979054 on epoch=199
05/21/2022 14:37:44 - INFO - __main__ - Global step 800 Train loss 0.998599 Classification-F1 0.48690476190476195 on epoch=199
05/21/2022 14:37:47 - INFO - __main__ - Step 810 Global step 810 Train loss 1.244316 on epoch=202
05/21/2022 14:37:50 - INFO - __main__ - Step 820 Global step 820 Train loss 1.046694 on epoch=204
05/21/2022 14:37:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.909466 on epoch=207
05/21/2022 14:37:55 - INFO - __main__ - Step 840 Global step 840 Train loss 1.084273 on epoch=209
05/21/2022 14:37:58 - INFO - __main__ - Step 850 Global step 850 Train loss 1.034420 on epoch=212
05/21/2022 14:37:58 - INFO - __main__ - Global step 850 Train loss 1.063834 Classification-F1 0.4701082598235766 on epoch=212
05/21/2022 14:38:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.824809 on epoch=214
05/21/2022 14:38:03 - INFO - __main__ - Step 870 Global step 870 Train loss 1.019218 on epoch=217
05/21/2022 14:38:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.772072 on epoch=219
05/21/2022 14:38:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.895025 on epoch=222
05/21/2022 14:38:11 - INFO - __main__ - Step 900 Global step 900 Train loss 1.056899 on epoch=224
05/21/2022 14:38:11 - INFO - __main__ - Global step 900 Train loss 0.913605 Classification-F1 0.46941678520625885 on epoch=224
05/21/2022 14:38:14 - INFO - __main__ - Step 910 Global step 910 Train loss 1.094845 on epoch=227
05/21/2022 14:38:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.874569 on epoch=229
05/21/2022 14:38:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.876580 on epoch=232
05/21/2022 14:38:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.645046 on epoch=234
05/21/2022 14:38:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.993797 on epoch=237
05/21/2022 14:38:25 - INFO - __main__ - Global step 950 Train loss 0.896968 Classification-F1 0.4950772200772201 on epoch=237
05/21/2022 14:38:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.943350 on epoch=239
05/21/2022 14:38:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.982511 on epoch=242
05/21/2022 14:38:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.727781 on epoch=244
05/21/2022 14:38:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.947518 on epoch=247
05/21/2022 14:38:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.157110 on epoch=249
05/21/2022 14:38:39 - INFO - __main__ - Global step 1000 Train loss 0.951654 Classification-F1 0.49346620652216067 on epoch=249
05/21/2022 14:38:39 - INFO - __main__ - save last model!
05/21/2022 14:38:41 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 14:38:42 - INFO - __main__ - Start tokenizing ... 5509 instances
05/21/2022 14:38:42 - INFO - __main__ - Printing 3 examples
05/21/2022 14:38:42 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/21/2022 14:38:42 - INFO - __main__ - ['others']
05/21/2022 14:38:42 - INFO - __main__ -  [emo] what you like very little things ok
05/21/2022 14:38:42 - INFO - __main__ - ['others']
05/21/2022 14:38:42 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/21/2022 14:38:42 - INFO - __main__ - ['others']
05/21/2022 14:38:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 14:38:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 14:38:49 - INFO - __main__ - Loaded 5509 examples from test data
05/21/2022 14:39:17 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-emo/emo_16_87_0.0001_8_predictions.txt
05/21/2022 14:39:17 - INFO - __main__ - Classification-F1 on test data: 0.1590
05/21/2022 14:39:17 - INFO - __main__ - prefix=emo_16_87, lr=0.0001, bsz=8, dev_performance=0.4950772200772201, test_performance=0.15903312063988745
