05/17/2022 19:45:25 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-base-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-cls2cls/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-base', cuda='2,3')
05/17/2022 19:45:25 - INFO - __main__ - models/T5-base-ft-cls2cls/singletask-amazon_polarity
05/17/2022 19:45:25 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-base-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-cls2cls/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-base', cuda='2,3')
05/17/2022 19:45:25 - INFO - __main__ - models/T5-base-ft-cls2cls/singletask-amazon_polarity
05/17/2022 19:45:27 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/17/2022 19:45:27 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/17/2022 19:45:27 - INFO - __main__ - args.device: cuda:0
05/17/2022 19:45:27 - INFO - __main__ - Using 2 gpus
05/17/2022 19:45:27 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
05/17/2022 19:45:27 - INFO - __main__ - args.device: cuda:1
05/17/2022 19:45:27 - INFO - __main__ - Using 2 gpus
05/17/2022 19:45:27 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
05/17/2022 19:45:31 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0005, bsz=8 ...
05/17/2022 19:45:32 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:45:32 - INFO - __main__ - Printing 3 examples
05/17/2022 19:45:32 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/17/2022 19:45:32 - INFO - __main__ - ['positive']
05/17/2022 19:45:32 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/17/2022 19:45:32 - INFO - __main__ - ['positive']
05/17/2022 19:45:32 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/17/2022 19:45:32 - INFO - __main__ - ['positive']
05/17/2022 19:45:32 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:45:32 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:45:32 - INFO - __main__ - Printing 3 examples
05/17/2022 19:45:32 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/17/2022 19:45:32 - INFO - __main__ - ['positive']
05/17/2022 19:45:32 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/17/2022 19:45:32 - INFO - __main__ - ['positive']
05/17/2022 19:45:32 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/17/2022 19:45:32 - INFO - __main__ - ['positive']
05/17/2022 19:45:32 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:45:32 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:45:32 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:45:32 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 19:45:32 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:45:32 - INFO - __main__ - Printing 3 examples
05/17/2022 19:45:32 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
05/17/2022 19:45:32 - INFO - __main__ - ['positive']
05/17/2022 19:45:32 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
05/17/2022 19:45:32 - INFO - __main__ - ['positive']
05/17/2022 19:45:32 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
05/17/2022 19:45:32 - INFO - __main__ - ['positive']
05/17/2022 19:45:32 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:45:32 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 19:45:32 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:45:32 - INFO - __main__ - Printing 3 examples
05/17/2022 19:45:32 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
05/17/2022 19:45:32 - INFO - __main__ - ['positive']
05/17/2022 19:45:32 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
05/17/2022 19:45:32 - INFO - __main__ - ['positive']
05/17/2022 19:45:32 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
05/17/2022 19:45:32 - INFO - __main__ - ['positive']
05/17/2022 19:45:32 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:45:32 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:45:32 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:45:32 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 19:45:32 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 19:45:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 19:45:37 - INFO - __main__ - Starting training!
05/17/2022 19:45:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 19:45:37 - INFO - __main__ - Starting training!
05/17/2022 19:45:40 - INFO - __main__ - Step 10 Global step 10 Train loss 23.661139 on epoch=4
05/17/2022 19:45:42 - INFO - __main__ - Step 20 Global step 20 Train loss 12.957950 on epoch=9
05/17/2022 19:45:44 - INFO - __main__ - Step 30 Global step 30 Train loss 6.977796 on epoch=14
05/17/2022 19:45:47 - INFO - __main__ - Step 40 Global step 40 Train loss 5.750542 on epoch=19
05/17/2022 19:45:49 - INFO - __main__ - Step 50 Global step 50 Train loss 2.891956 on epoch=24
05/17/2022 19:45:50 - INFO - __main__ - Global step 50 Train loss 10.447877 Classification-F1 0.3333333333333333 on epoch=24
05/17/2022 19:45:53 - INFO - __main__ - Step 60 Global step 60 Train loss 1.156420 on epoch=29
05/17/2022 19:45:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.757889 on epoch=34
05/17/2022 19:45:58 - INFO - __main__ - Step 80 Global step 80 Train loss 1.111740 on epoch=39
05/17/2022 19:46:00 - INFO - __main__ - Step 90 Global step 90 Train loss 1.792735 on epoch=44
05/17/2022 19:46:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.834326 on epoch=49
05/17/2022 19:46:03 - INFO - __main__ - Global step 100 Train loss 1.130622 Classification-F1 0.5835835835835835 on epoch=49
05/17/2022 19:46:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.669656 on epoch=54
05/17/2022 19:46:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.590878 on epoch=59
05/17/2022 19:46:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.419090 on epoch=64
05/17/2022 19:46:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.433872 on epoch=69
05/17/2022 19:46:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.563244 on epoch=74
05/17/2022 19:46:16 - INFO - __main__ - Global step 150 Train loss 0.535348 Classification-F1 0.3333333333333333 on epoch=74
05/17/2022 19:46:19 - INFO - __main__ - Step 160 Global step 160 Train loss 0.429037 on epoch=79
05/17/2022 19:46:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.549639 on epoch=84
05/17/2022 19:46:24 - INFO - __main__ - Step 180 Global step 180 Train loss 0.430576 on epoch=89
05/17/2022 19:46:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.449138 on epoch=94
05/17/2022 19:46:29 - INFO - __main__ - Step 200 Global step 200 Train loss 0.661719 on epoch=99
05/17/2022 19:46:29 - INFO - __main__ - Global step 200 Train loss 0.504022 Classification-F1 0.3333333333333333 on epoch=99
05/17/2022 19:46:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.517256 on epoch=104
05/17/2022 19:46:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.405508 on epoch=109
05/17/2022 19:46:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.450215 on epoch=114
05/17/2022 19:46:39 - INFO - __main__ - Step 240 Global step 240 Train loss 0.436179 on epoch=119
05/17/2022 19:46:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.407454 on epoch=124
05/17/2022 19:46:41 - INFO - __main__ - Global step 250 Train loss 0.443322 Classification-F1 0.3333333333333333 on epoch=124
05/17/2022 19:46:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.528839 on epoch=129
05/17/2022 19:46:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.461893 on epoch=134
05/17/2022 19:46:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.462175 on epoch=139
05/17/2022 19:46:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.438429 on epoch=144
05/17/2022 19:46:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.433231 on epoch=149
05/17/2022 19:46:54 - INFO - __main__ - Global step 300 Train loss 0.464913 Classification-F1 0.36374269005847953 on epoch=149
05/17/2022 19:46:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.370258 on epoch=154
05/17/2022 19:46:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.361674 on epoch=159
05/17/2022 19:47:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.369027 on epoch=164
05/17/2022 19:47:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.390920 on epoch=169
05/17/2022 19:47:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.376450 on epoch=174
05/17/2022 19:47:07 - INFO - __main__ - Global step 350 Train loss 0.373666 Classification-F1 0.41700404858299595 on epoch=174
05/17/2022 19:47:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.363572 on epoch=179
05/17/2022 19:47:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.384015 on epoch=184
05/17/2022 19:47:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.349541 on epoch=189
05/17/2022 19:47:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.375079 on epoch=194
05/17/2022 19:47:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.340723 on epoch=199
05/17/2022 19:47:20 - INFO - __main__ - Global step 400 Train loss 0.362586 Classification-F1 0.3333333333333333 on epoch=199
05/17/2022 19:47:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.355917 on epoch=204
05/17/2022 19:47:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.353619 on epoch=209
05/17/2022 19:47:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.363899 on epoch=214
05/17/2022 19:47:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.348104 on epoch=219
05/17/2022 19:47:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.374684 on epoch=224
05/17/2022 19:47:33 - INFO - __main__ - Global step 450 Train loss 0.359244 Classification-F1 0.3333333333333333 on epoch=224
05/17/2022 19:47:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.344305 on epoch=229
05/17/2022 19:47:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.350446 on epoch=234
05/17/2022 19:47:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.343447 on epoch=239
05/17/2022 19:47:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.346177 on epoch=244
05/17/2022 19:47:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.329744 on epoch=249
05/17/2022 19:47:46 - INFO - __main__ - Global step 500 Train loss 0.342824 Classification-F1 0.3191489361702127 on epoch=249
05/17/2022 19:47:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.351530 on epoch=254
05/17/2022 19:47:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.366867 on epoch=259
05/17/2022 19:47:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.344025 on epoch=264
05/17/2022 19:47:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.382259 on epoch=269
05/17/2022 19:47:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.346679 on epoch=274
05/17/2022 19:47:59 - INFO - __main__ - Global step 550 Train loss 0.358272 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 19:48:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.347637 on epoch=279
05/17/2022 19:48:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.348550 on epoch=284
05/17/2022 19:48:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.342637 on epoch=289
05/17/2022 19:48:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.357163 on epoch=294
05/17/2022 19:48:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.346524 on epoch=299
05/17/2022 19:48:11 - INFO - __main__ - Global step 600 Train loss 0.348502 Classification-F1 0.3454545454545454 on epoch=299
05/17/2022 19:48:11 - INFO - __main__ - save last model!
05/17/2022 19:48:12 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:48:12 - INFO - __main__ - Printing 3 examples
05/17/2022 19:48:12 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/17/2022 19:48:12 - INFO - __main__ - ['positive']
05/17/2022 19:48:12 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/17/2022 19:48:12 - INFO - __main__ - ['positive']
05/17/2022 19:48:12 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/17/2022 19:48:12 - INFO - __main__ - ['positive']
05/17/2022 19:48:12 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:48:12 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:48:12 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 19:48:12 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:48:12 - INFO - __main__ - Printing 3 examples
05/17/2022 19:48:12 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
05/17/2022 19:48:12 - INFO - __main__ - ['positive']
05/17/2022 19:48:12 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
05/17/2022 19:48:12 - INFO - __main__ - ['positive']
05/17/2022 19:48:12 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
05/17/2022 19:48:12 - INFO - __main__ - ['positive']
05/17/2022 19:48:12 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:48:12 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:48:13 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 19:48:14 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 19:48:14 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 19:48:14 - INFO - __main__ - Printing 3 examples
05/17/2022 19:48:14 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 19:48:14 - INFO - __main__ - ['negative']
05/17/2022 19:48:14 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 19:48:14 - INFO - __main__ - ['negative']
05/17/2022 19:48:14 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 19:48:14 - INFO - __main__ - ['negative']
05/17/2022 19:48:14 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:48:15 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:48:16 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 19:48:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 19:48:16 - INFO - __main__ - Starting training!
05/17/2022 19:48:23 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_100_0.0005_8_predictions.txt
05/17/2022 19:48:23 - INFO - __main__ - Classification-F1 on test data: 0.4792
05/17/2022 19:48:23 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0005, bsz=8, dev_performance=0.5835835835835835, test_performance=0.47919845387040994
05/17/2022 19:48:23 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0003, bsz=8 ...
05/17/2022 19:48:24 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:48:24 - INFO - __main__ - Printing 3 examples
05/17/2022 19:48:24 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/17/2022 19:48:24 - INFO - __main__ - ['positive']
05/17/2022 19:48:24 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/17/2022 19:48:24 - INFO - __main__ - ['positive']
05/17/2022 19:48:24 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/17/2022 19:48:24 - INFO - __main__ - ['positive']
05/17/2022 19:48:24 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:48:24 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:48:24 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 19:48:24 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:48:24 - INFO - __main__ - Printing 3 examples
05/17/2022 19:48:24 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
05/17/2022 19:48:24 - INFO - __main__ - ['positive']
05/17/2022 19:48:24 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
05/17/2022 19:48:24 - INFO - __main__ - ['positive']
05/17/2022 19:48:24 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
05/17/2022 19:48:24 - INFO - __main__ - ['positive']
05/17/2022 19:48:24 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:48:24 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:48:24 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 19:48:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 19:48:28 - INFO - __main__ - Starting training!
05/17/2022 19:48:30 - INFO - __main__ - Step 10 Global step 10 Train loss 23.343750 on epoch=4
05/17/2022 19:48:33 - INFO - __main__ - Step 20 Global step 20 Train loss 16.951300 on epoch=9
05/17/2022 19:48:35 - INFO - __main__ - Step 30 Global step 30 Train loss 13.714992 on epoch=14
05/17/2022 19:48:38 - INFO - __main__ - Step 40 Global step 40 Train loss 10.306339 on epoch=19
05/17/2022 19:48:40 - INFO - __main__ - Step 50 Global step 50 Train loss 7.203708 on epoch=24
05/17/2022 19:48:42 - INFO - __main__ - Global step 50 Train loss 14.304017 Classification-F1 0.10606060606060606 on epoch=24
05/17/2022 19:48:45 - INFO - __main__ - Step 60 Global step 60 Train loss 5.663486 on epoch=29
05/17/2022 19:48:47 - INFO - __main__ - Step 70 Global step 70 Train loss 3.164496 on epoch=34
05/17/2022 19:48:50 - INFO - __main__ - Step 80 Global step 80 Train loss 1.326816 on epoch=39
05/17/2022 19:48:52 - INFO - __main__ - Step 90 Global step 90 Train loss 1.947676 on epoch=44
05/17/2022 19:48:55 - INFO - __main__ - Step 100 Global step 100 Train loss 1.061263 on epoch=49
05/17/2022 19:48:55 - INFO - __main__ - Global step 100 Train loss 2.632747 Classification-F1 0.3333333333333333 on epoch=49
05/17/2022 19:48:58 - INFO - __main__ - Step 110 Global step 110 Train loss 0.561167 on epoch=54
05/17/2022 19:49:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.547699 on epoch=59
05/17/2022 19:49:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.547289 on epoch=64
05/17/2022 19:49:06 - INFO - __main__ - Step 140 Global step 140 Train loss 0.571439 on epoch=69
05/17/2022 19:49:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.586023 on epoch=74
05/17/2022 19:49:09 - INFO - __main__ - Global step 150 Train loss 0.562723 Classification-F1 0.4589371980676329 on epoch=74
05/17/2022 19:49:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.632440 on epoch=79
05/17/2022 19:49:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.499952 on epoch=84
05/17/2022 19:49:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.400522 on epoch=89
05/17/2022 19:49:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.520478 on epoch=94
05/17/2022 19:49:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.453114 on epoch=99
05/17/2022 19:49:22 - INFO - __main__ - Global step 200 Train loss 0.501301 Classification-F1 0.5333333333333333 on epoch=99
05/17/2022 19:49:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.490667 on epoch=104
05/17/2022 19:49:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.597295 on epoch=109
05/17/2022 19:49:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.550249 on epoch=114
05/17/2022 19:49:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.442752 on epoch=119
05/17/2022 19:49:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.459599 on epoch=124
05/17/2022 19:49:35 - INFO - __main__ - Global step 250 Train loss 0.508113 Classification-F1 0.5636363636363637 on epoch=124
05/17/2022 19:49:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.366902 on epoch=129
05/17/2022 19:49:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.462301 on epoch=134
05/17/2022 19:49:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.473623 on epoch=139
05/17/2022 19:49:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.375068 on epoch=144
05/17/2022 19:49:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.482325 on epoch=149
05/17/2022 19:49:48 - INFO - __main__ - Global step 300 Train loss 0.432044 Classification-F1 0.5636363636363637 on epoch=149
05/17/2022 19:49:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.425300 on epoch=154
05/17/2022 19:49:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.347335 on epoch=159
05/17/2022 19:49:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.403669 on epoch=164
05/17/2022 19:49:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.378408 on epoch=169
05/17/2022 19:50:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.421813 on epoch=174
05/17/2022 19:50:01 - INFO - __main__ - Global step 350 Train loss 0.395305 Classification-F1 0.805668016194332 on epoch=174
05/17/2022 19:50:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.351701 on epoch=179
05/17/2022 19:50:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.316351 on epoch=184
05/17/2022 19:50:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.418205 on epoch=189
05/17/2022 19:50:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.387913 on epoch=194
05/17/2022 19:50:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.392369 on epoch=199
05/17/2022 19:50:15 - INFO - __main__ - Global step 400 Train loss 0.373308 Classification-F1 0.7046153846153846 on epoch=199
05/17/2022 19:50:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.351762 on epoch=204
05/17/2022 19:50:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.327127 on epoch=209
05/17/2022 19:50:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.432403 on epoch=214
05/17/2022 19:50:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.337398 on epoch=219
05/17/2022 19:50:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.317020 on epoch=224
05/17/2022 19:50:27 - INFO - __main__ - Global step 450 Train loss 0.353142 Classification-F1 0.6862745098039216 on epoch=224
05/17/2022 19:50:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.350456 on epoch=229
05/17/2022 19:50:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.332441 on epoch=234
05/17/2022 19:50:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.297466 on epoch=239
05/17/2022 19:50:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.219415 on epoch=244
05/17/2022 19:50:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.449293 on epoch=249
05/17/2022 19:50:40 - INFO - __main__ - Global step 500 Train loss 0.329814 Classification-F1 0.8125 on epoch=249
05/17/2022 19:50:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.347977 on epoch=254
05/17/2022 19:50:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.306819 on epoch=259
05/17/2022 19:50:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.299589 on epoch=264
05/17/2022 19:50:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.337848 on epoch=269
05/17/2022 19:50:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.383960 on epoch=274
05/17/2022 19:50:53 - INFO - __main__ - Global step 550 Train loss 0.335239 Classification-F1 0.716256157635468 on epoch=274
05/17/2022 19:50:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.294643 on epoch=279
05/17/2022 19:50:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.297464 on epoch=284
05/17/2022 19:51:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.303745 on epoch=289
05/17/2022 19:51:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.337289 on epoch=294
05/17/2022 19:51:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.252847 on epoch=299
05/17/2022 19:51:06 - INFO - __main__ - Global step 600 Train loss 0.297198 Classification-F1 0.7757757757757757 on epoch=299
05/17/2022 19:51:06 - INFO - __main__ - save last model!
05/17/2022 19:51:07 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:51:07 - INFO - __main__ - Printing 3 examples
05/17/2022 19:51:07 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/17/2022 19:51:07 - INFO - __main__ - ['positive']
05/17/2022 19:51:07 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/17/2022 19:51:07 - INFO - __main__ - ['positive']
05/17/2022 19:51:07 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/17/2022 19:51:07 - INFO - __main__ - ['positive']
05/17/2022 19:51:07 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:51:07 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:51:07 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 19:51:07 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:51:07 - INFO - __main__ - Printing 3 examples
05/17/2022 19:51:07 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
05/17/2022 19:51:07 - INFO - __main__ - ['positive']
05/17/2022 19:51:07 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
05/17/2022 19:51:07 - INFO - __main__ - ['positive']
05/17/2022 19:51:07 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
05/17/2022 19:51:07 - INFO - __main__ - ['positive']
05/17/2022 19:51:07 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:51:07 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:51:07 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 19:51:09 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 19:51:09 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 19:51:09 - INFO - __main__ - Printing 3 examples
05/17/2022 19:51:09 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 19:51:09 - INFO - __main__ - ['negative']
05/17/2022 19:51:09 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 19:51:09 - INFO - __main__ - ['negative']
05/17/2022 19:51:09 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 19:51:09 - INFO - __main__ - ['negative']
05/17/2022 19:51:09 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:51:10 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:51:11 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 19:51:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 19:51:11 - INFO - __main__ - Starting training!
05/17/2022 19:51:17 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_100_0.0003_8_predictions.txt
05/17/2022 19:51:17 - INFO - __main__ - Classification-F1 on test data: 0.7046
05/17/2022 19:51:17 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0003, bsz=8, dev_performance=0.8125, test_performance=0.7045523063008743
05/17/2022 19:51:17 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0002, bsz=8 ...
05/17/2022 19:51:18 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:51:18 - INFO - __main__ - Printing 3 examples
05/17/2022 19:51:18 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/17/2022 19:51:18 - INFO - __main__ - ['positive']
05/17/2022 19:51:18 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/17/2022 19:51:18 - INFO - __main__ - ['positive']
05/17/2022 19:51:18 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/17/2022 19:51:18 - INFO - __main__ - ['positive']
05/17/2022 19:51:18 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:51:18 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:51:18 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 19:51:18 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:51:18 - INFO - __main__ - Printing 3 examples
05/17/2022 19:51:18 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
05/17/2022 19:51:18 - INFO - __main__ - ['positive']
05/17/2022 19:51:18 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
05/17/2022 19:51:18 - INFO - __main__ - ['positive']
05/17/2022 19:51:18 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
05/17/2022 19:51:18 - INFO - __main__ - ['positive']
05/17/2022 19:51:18 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:51:18 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:51:18 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 19:51:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 19:51:22 - INFO - __main__ - Starting training!
05/17/2022 19:51:24 - INFO - __main__ - Step 10 Global step 10 Train loss 23.025887 on epoch=4
05/17/2022 19:51:27 - INFO - __main__ - Step 20 Global step 20 Train loss 18.186964 on epoch=9
05/17/2022 19:51:29 - INFO - __main__ - Step 30 Global step 30 Train loss 14.659251 on epoch=14
05/17/2022 19:51:31 - INFO - __main__ - Step 40 Global step 40 Train loss 12.479498 on epoch=19
05/17/2022 19:51:34 - INFO - __main__ - Step 50 Global step 50 Train loss 9.964463 on epoch=24
05/17/2022 19:51:37 - INFO - __main__ - Global step 50 Train loss 15.663213 Classification-F1 0.0 on epoch=24
05/17/2022 19:51:39 - INFO - __main__ - Step 60 Global step 60 Train loss 6.776296 on epoch=29
05/17/2022 19:51:42 - INFO - __main__ - Step 70 Global step 70 Train loss 6.368814 on epoch=34
05/17/2022 19:51:44 - INFO - __main__ - Step 80 Global step 80 Train loss 4.949464 on epoch=39
05/17/2022 19:51:47 - INFO - __main__ - Step 90 Global step 90 Train loss 3.215489 on epoch=44
05/17/2022 19:51:49 - INFO - __main__ - Step 100 Global step 100 Train loss 2.306784 on epoch=49
05/17/2022 19:51:50 - INFO - __main__ - Global step 100 Train loss 4.723369 Classification-F1 0.3333333333333333 on epoch=49
05/17/2022 19:51:52 - INFO - __main__ - Step 110 Global step 110 Train loss 1.451317 on epoch=54
05/17/2022 19:51:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.981809 on epoch=59
05/17/2022 19:51:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.759899 on epoch=64
05/17/2022 19:52:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.663758 on epoch=69
05/17/2022 19:52:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.756769 on epoch=74
05/17/2022 19:52:02 - INFO - __main__ - Global step 150 Train loss 0.922710 Classification-F1 0.3333333333333333 on epoch=74
05/17/2022 19:52:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.745839 on epoch=79
05/17/2022 19:52:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.705602 on epoch=84
05/17/2022 19:52:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.622838 on epoch=89
05/17/2022 19:52:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.465309 on epoch=94
05/17/2022 19:52:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.533246 on epoch=99
05/17/2022 19:52:15 - INFO - __main__ - Global step 200 Train loss 0.614567 Classification-F1 0.4285714285714286 on epoch=99
05/17/2022 19:52:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.458323 on epoch=104
05/17/2022 19:52:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.583040 on epoch=109
05/17/2022 19:52:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.432174 on epoch=114
05/17/2022 19:52:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.707390 on epoch=119
05/17/2022 19:52:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.629417 on epoch=124
05/17/2022 19:52:28 - INFO - __main__ - Global step 250 Train loss 0.562069 Classification-F1 0.4980392156862745 on epoch=124
05/17/2022 19:52:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.518916 on epoch=129
05/17/2022 19:52:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.428617 on epoch=134
05/17/2022 19:52:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.532014 on epoch=139
05/17/2022 19:52:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.455975 on epoch=144
05/17/2022 19:52:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.597676 on epoch=149
05/17/2022 19:52:41 - INFO - __main__ - Global step 300 Train loss 0.506640 Classification-F1 0.3073593073593074 on epoch=149
05/17/2022 19:52:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.486172 on epoch=154
05/17/2022 19:52:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.484111 on epoch=159
05/17/2022 19:52:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.452814 on epoch=164
05/17/2022 19:52:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.570166 on epoch=169
05/17/2022 19:52:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.514866 on epoch=174
05/17/2022 19:52:53 - INFO - __main__ - Global step 350 Train loss 0.501626 Classification-F1 0.3333333333333333 on epoch=174
05/17/2022 19:52:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.489031 on epoch=179
05/17/2022 19:52:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.479337 on epoch=184
05/17/2022 19:53:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.472848 on epoch=189
05/17/2022 19:53:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.440612 on epoch=194
05/17/2022 19:53:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.429277 on epoch=199
05/17/2022 19:53:06 - INFO - __main__ - Global step 400 Train loss 0.462221 Classification-F1 0.3333333333333333 on epoch=199
05/17/2022 19:53:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.593375 on epoch=204
05/17/2022 19:53:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.457772 on epoch=209
05/17/2022 19:53:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.489369 on epoch=214
05/17/2022 19:53:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.378783 on epoch=219
05/17/2022 19:53:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.448385 on epoch=224
05/17/2022 19:53:18 - INFO - __main__ - Global step 450 Train loss 0.473537 Classification-F1 0.4920634920634921 on epoch=224
05/17/2022 19:53:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.472622 on epoch=229
05/17/2022 19:53:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.514958 on epoch=234
05/17/2022 19:53:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.462621 on epoch=239
05/17/2022 19:53:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.477887 on epoch=244
05/17/2022 19:53:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.389092 on epoch=249
05/17/2022 19:53:31 - INFO - __main__ - Global step 500 Train loss 0.463436 Classification-F1 0.3333333333333333 on epoch=249
05/17/2022 19:53:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.359210 on epoch=254
05/17/2022 19:53:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.433663 on epoch=259
05/17/2022 19:53:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.469312 on epoch=264
05/17/2022 19:53:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.438962 on epoch=269
05/17/2022 19:53:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.465420 on epoch=274
05/17/2022 19:53:44 - INFO - __main__ - Global step 550 Train loss 0.433313 Classification-F1 0.6476476476476476 on epoch=274
05/17/2022 19:53:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.370292 on epoch=279
05/17/2022 19:53:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.437483 on epoch=284
05/17/2022 19:53:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.408203 on epoch=289
05/17/2022 19:53:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.408618 on epoch=294
05/17/2022 19:53:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.446299 on epoch=299
05/17/2022 19:53:57 - INFO - __main__ - Global step 600 Train loss 0.414179 Classification-F1 0.5465587044534412 on epoch=299
05/17/2022 19:53:57 - INFO - __main__ - save last model!
05/17/2022 19:53:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:53:58 - INFO - __main__ - Printing 3 examples
05/17/2022 19:53:58 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/17/2022 19:53:58 - INFO - __main__ - ['positive']
05/17/2022 19:53:58 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/17/2022 19:53:58 - INFO - __main__ - ['positive']
05/17/2022 19:53:58 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/17/2022 19:53:58 - INFO - __main__ - ['positive']
05/17/2022 19:53:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:53:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:53:58 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 19:53:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:53:58 - INFO - __main__ - Printing 3 examples
05/17/2022 19:53:58 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
05/17/2022 19:53:58 - INFO - __main__ - ['positive']
05/17/2022 19:53:58 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
05/17/2022 19:53:58 - INFO - __main__ - ['positive']
05/17/2022 19:53:58 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
05/17/2022 19:53:58 - INFO - __main__ - ['positive']
05/17/2022 19:53:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:53:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:53:58 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 19:54:00 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 19:54:00 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 19:54:00 - INFO - __main__ - Printing 3 examples
05/17/2022 19:54:00 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 19:54:00 - INFO - __main__ - ['negative']
05/17/2022 19:54:00 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 19:54:00 - INFO - __main__ - ['negative']
05/17/2022 19:54:00 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 19:54:00 - INFO - __main__ - ['negative']
05/17/2022 19:54:00 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:54:00 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:54:01 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 19:54:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 19:54:02 - INFO - __main__ - Starting training!
05/17/2022 19:54:08 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_100_0.0002_8_predictions.txt
05/17/2022 19:54:08 - INFO - __main__ - Classification-F1 on test data: 0.6056
05/17/2022 19:54:08 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0002, bsz=8, dev_performance=0.6476476476476476, test_performance=0.6055974587023851
05/17/2022 19:54:08 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0001, bsz=8 ...
05/17/2022 19:54:09 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:54:09 - INFO - __main__ - Printing 3 examples
05/17/2022 19:54:09 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/17/2022 19:54:09 - INFO - __main__ - ['positive']
05/17/2022 19:54:09 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/17/2022 19:54:09 - INFO - __main__ - ['positive']
05/17/2022 19:54:09 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/17/2022 19:54:09 - INFO - __main__ - ['positive']
05/17/2022 19:54:09 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:54:09 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:54:09 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 19:54:09 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:54:09 - INFO - __main__ - Printing 3 examples
05/17/2022 19:54:09 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
05/17/2022 19:54:09 - INFO - __main__ - ['positive']
05/17/2022 19:54:09 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
05/17/2022 19:54:09 - INFO - __main__ - ['positive']
05/17/2022 19:54:09 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
05/17/2022 19:54:09 - INFO - __main__ - ['positive']
05/17/2022 19:54:09 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:54:09 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:54:09 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 19:54:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 19:54:13 - INFO - __main__ - Starting training!
05/17/2022 19:54:15 - INFO - __main__ - Step 10 Global step 10 Train loss 23.066504 on epoch=4
05/17/2022 19:54:17 - INFO - __main__ - Step 20 Global step 20 Train loss 21.329798 on epoch=9
05/17/2022 19:54:20 - INFO - __main__ - Step 30 Global step 30 Train loss 16.673851 on epoch=14
05/17/2022 19:54:22 - INFO - __main__ - Step 40 Global step 40 Train loss 16.442656 on epoch=19
05/17/2022 19:54:25 - INFO - __main__ - Step 50 Global step 50 Train loss 16.270634 on epoch=24
05/17/2022 19:54:30 - INFO - __main__ - Global step 50 Train loss 18.756689 Classification-F1 0.0 on epoch=24
05/17/2022 19:54:33 - INFO - __main__ - Step 60 Global step 60 Train loss 12.869272 on epoch=29
05/17/2022 19:54:35 - INFO - __main__ - Step 70 Global step 70 Train loss 12.795426 on epoch=34
05/17/2022 19:54:38 - INFO - __main__ - Step 80 Global step 80 Train loss 12.587499 on epoch=39
05/17/2022 19:54:40 - INFO - __main__ - Step 90 Global step 90 Train loss 11.425797 on epoch=44
05/17/2022 19:54:43 - INFO - __main__ - Step 100 Global step 100 Train loss 10.580706 on epoch=49
05/17/2022 19:54:46 - INFO - __main__ - Global step 100 Train loss 12.051740 Classification-F1 0.0 on epoch=49
05/17/2022 19:54:48 - INFO - __main__ - Step 110 Global step 110 Train loss 9.018617 on epoch=54
05/17/2022 19:54:51 - INFO - __main__ - Step 120 Global step 120 Train loss 8.547529 on epoch=59
05/17/2022 19:54:54 - INFO - __main__ - Step 130 Global step 130 Train loss 6.698777 on epoch=64
05/17/2022 19:54:56 - INFO - __main__ - Step 140 Global step 140 Train loss 5.294441 on epoch=69
05/17/2022 19:54:59 - INFO - __main__ - Step 150 Global step 150 Train loss 5.898695 on epoch=74
05/17/2022 19:54:59 - INFO - __main__ - Global step 150 Train loss 7.091611 Classification-F1 0.3333333333333333 on epoch=74
05/17/2022 19:55:02 - INFO - __main__ - Step 160 Global step 160 Train loss 4.311020 on epoch=79
05/17/2022 19:55:04 - INFO - __main__ - Step 170 Global step 170 Train loss 4.009510 on epoch=84
05/17/2022 19:55:07 - INFO - __main__ - Step 180 Global step 180 Train loss 3.299757 on epoch=89
05/17/2022 19:55:10 - INFO - __main__ - Step 190 Global step 190 Train loss 3.511825 on epoch=94
05/17/2022 19:55:12 - INFO - __main__ - Step 200 Global step 200 Train loss 2.145335 on epoch=99
05/17/2022 19:55:13 - INFO - __main__ - Global step 200 Train loss 3.455489 Classification-F1 0.3333333333333333 on epoch=99
05/17/2022 19:55:15 - INFO - __main__ - Step 210 Global step 210 Train loss 2.240998 on epoch=104
05/17/2022 19:55:18 - INFO - __main__ - Step 220 Global step 220 Train loss 1.598090 on epoch=109
05/17/2022 19:55:20 - INFO - __main__ - Step 230 Global step 230 Train loss 1.305165 on epoch=114
05/17/2022 19:55:23 - INFO - __main__ - Step 240 Global step 240 Train loss 2.051868 on epoch=119
05/17/2022 19:55:26 - INFO - __main__ - Step 250 Global step 250 Train loss 1.187603 on epoch=124
05/17/2022 19:55:26 - INFO - __main__ - Global step 250 Train loss 1.676745 Classification-F1 0.873015873015873 on epoch=124
05/17/2022 19:55:29 - INFO - __main__ - Step 260 Global step 260 Train loss 1.028265 on epoch=129
05/17/2022 19:55:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.570232 on epoch=134
05/17/2022 19:55:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.847506 on epoch=139
05/17/2022 19:55:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.494514 on epoch=144
05/17/2022 19:55:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.428189 on epoch=149
05/17/2022 19:55:39 - INFO - __main__ - Global step 300 Train loss 0.673741 Classification-F1 0.9372549019607843 on epoch=149
05/17/2022 19:55:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.579125 on epoch=154
05/17/2022 19:55:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.400153 on epoch=159
05/17/2022 19:55:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.499650 on epoch=164
05/17/2022 19:55:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.390685 on epoch=169
05/17/2022 19:55:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.540405 on epoch=174
05/17/2022 19:55:53 - INFO - __main__ - Global step 350 Train loss 0.482004 Classification-F1 0.9687194525904204 on epoch=174
05/17/2022 19:55:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.392907 on epoch=179
05/17/2022 19:55:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.331169 on epoch=184
05/17/2022 19:56:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.343094 on epoch=189
05/17/2022 19:56:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.405425 on epoch=194
05/17/2022 19:56:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.333904 on epoch=199
05/17/2022 19:56:06 - INFO - __main__ - Global step 400 Train loss 0.361300 Classification-F1 0.9687194525904204 on epoch=199
05/17/2022 19:56:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.469549 on epoch=204
05/17/2022 19:56:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.280571 on epoch=209
05/17/2022 19:56:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.295417 on epoch=214
05/17/2022 19:56:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.312856 on epoch=219
05/17/2022 19:56:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.194278 on epoch=224
05/17/2022 19:56:19 - INFO - __main__ - Global step 450 Train loss 0.310534 Classification-F1 0.9687194525904204 on epoch=224
05/17/2022 19:56:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.200797 on epoch=229
05/17/2022 19:56:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.146843 on epoch=234
05/17/2022 19:56:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.148945 on epoch=239
05/17/2022 19:56:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.144146 on epoch=244
05/17/2022 19:56:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.065328 on epoch=249
05/17/2022 19:56:32 - INFO - __main__ - Global step 500 Train loss 0.141212 Classification-F1 1.0 on epoch=249
05/17/2022 19:56:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.138846 on epoch=254
05/17/2022 19:56:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.104573 on epoch=259
05/17/2022 19:56:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.149553 on epoch=264
05/17/2022 19:56:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.104114 on epoch=269
05/17/2022 19:56:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.032218 on epoch=274
05/17/2022 19:56:46 - INFO - __main__ - Global step 550 Train loss 0.105861 Classification-F1 0.9687194525904204 on epoch=274
05/17/2022 19:56:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.075268 on epoch=279
05/17/2022 19:56:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.102348 on epoch=284
05/17/2022 19:56:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.052427 on epoch=289
05/17/2022 19:56:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.031870 on epoch=294
05/17/2022 19:56:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.037894 on epoch=299
05/17/2022 19:56:59 - INFO - __main__ - Global step 600 Train loss 0.059961 Classification-F1 1.0 on epoch=299
05/17/2022 19:56:59 - INFO - __main__ - save last model!
05/17/2022 19:57:00 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:57:00 - INFO - __main__ - Printing 3 examples
05/17/2022 19:57:00 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/17/2022 19:57:00 - INFO - __main__ - ['negative']
05/17/2022 19:57:00 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/17/2022 19:57:00 - INFO - __main__ - ['negative']
05/17/2022 19:57:00 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/17/2022 19:57:00 - INFO - __main__ - ['negative']
05/17/2022 19:57:00 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:57:00 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:57:00 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 19:57:00 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:57:00 - INFO - __main__ - Printing 3 examples
05/17/2022 19:57:00 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
05/17/2022 19:57:00 - INFO - __main__ - ['negative']
05/17/2022 19:57:00 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
05/17/2022 19:57:00 - INFO - __main__ - ['negative']
05/17/2022 19:57:00 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
05/17/2022 19:57:00 - INFO - __main__ - ['negative']
05/17/2022 19:57:00 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:57:00 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:57:00 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 19:57:01 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 19:57:01 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 19:57:01 - INFO - __main__ - Printing 3 examples
05/17/2022 19:57:01 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 19:57:01 - INFO - __main__ - ['negative']
05/17/2022 19:57:01 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 19:57:01 - INFO - __main__ - ['negative']
05/17/2022 19:57:01 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 19:57:01 - INFO - __main__ - ['negative']
05/17/2022 19:57:01 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:57:02 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:57:03 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 19:57:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 19:57:03 - INFO - __main__ - Starting training!
05/17/2022 19:57:10 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_100_0.0001_8_predictions.txt
05/17/2022 19:57:10 - INFO - __main__ - Classification-F1 on test data: 0.9229
05/17/2022 19:57:10 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0001, bsz=8, dev_performance=1.0, test_performance=0.922882704593687
05/17/2022 19:57:10 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0005, bsz=8 ...
05/17/2022 19:57:11 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:57:11 - INFO - __main__ - Printing 3 examples
05/17/2022 19:57:11 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/17/2022 19:57:11 - INFO - __main__ - ['negative']
05/17/2022 19:57:11 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/17/2022 19:57:11 - INFO - __main__ - ['negative']
05/17/2022 19:57:11 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/17/2022 19:57:11 - INFO - __main__ - ['negative']
05/17/2022 19:57:11 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:57:11 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:57:11 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 19:57:11 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:57:11 - INFO - __main__ - Printing 3 examples
05/17/2022 19:57:11 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
05/17/2022 19:57:11 - INFO - __main__ - ['negative']
05/17/2022 19:57:11 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
05/17/2022 19:57:11 - INFO - __main__ - ['negative']
05/17/2022 19:57:11 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
05/17/2022 19:57:11 - INFO - __main__ - ['negative']
05/17/2022 19:57:11 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:57:11 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:57:11 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 19:57:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 19:57:14 - INFO - __main__ - Starting training!
05/17/2022 19:57:16 - INFO - __main__ - Step 10 Global step 10 Train loss 20.977041 on epoch=4
05/17/2022 19:57:19 - INFO - __main__ - Step 20 Global step 20 Train loss 13.515381 on epoch=9
05/17/2022 19:57:21 - INFO - __main__ - Step 30 Global step 30 Train loss 6.956714 on epoch=14
05/17/2022 19:57:24 - INFO - __main__ - Step 40 Global step 40 Train loss 3.961160 on epoch=19
05/17/2022 19:57:26 - INFO - __main__ - Step 50 Global step 50 Train loss 1.341097 on epoch=24
05/17/2022 19:57:26 - INFO - __main__ - Global step 50 Train loss 9.350279 Classification-F1 0.3333333333333333 on epoch=24
05/17/2022 19:57:29 - INFO - __main__ - Step 60 Global step 60 Train loss 0.749617 on epoch=29
05/17/2022 19:57:32 - INFO - __main__ - Step 70 Global step 70 Train loss 0.619719 on epoch=34
05/17/2022 19:57:34 - INFO - __main__ - Step 80 Global step 80 Train loss 0.499908 on epoch=39
05/17/2022 19:57:37 - INFO - __main__ - Step 90 Global step 90 Train loss 0.390980 on epoch=44
05/17/2022 19:57:39 - INFO - __main__ - Step 100 Global step 100 Train loss 0.379992 on epoch=49
05/17/2022 19:57:39 - INFO - __main__ - Global step 100 Train loss 0.528043 Classification-F1 0.7810361681329424 on epoch=49
05/17/2022 19:57:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.484721 on epoch=54
05/17/2022 19:57:45 - INFO - __main__ - Step 120 Global step 120 Train loss 0.404455 on epoch=59
05/17/2022 19:57:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.375244 on epoch=64
05/17/2022 19:57:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.317886 on epoch=69
05/17/2022 19:57:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.342540 on epoch=74
05/17/2022 19:57:53 - INFO - __main__ - Global step 150 Train loss 0.384969 Classification-F1 0.7408906882591093 on epoch=74
05/17/2022 19:57:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.149154 on epoch=79
05/17/2022 19:57:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.067230 on epoch=84
05/17/2022 19:58:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.086326 on epoch=89
05/17/2022 19:58:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.104973 on epoch=94
05/17/2022 19:58:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.079982 on epoch=99
05/17/2022 19:58:06 - INFO - __main__ - Global step 200 Train loss 0.097533 Classification-F1 0.8125 on epoch=99
05/17/2022 19:58:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.050639 on epoch=104
05/17/2022 19:58:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.217225 on epoch=109
05/17/2022 19:58:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.059150 on epoch=114
05/17/2022 19:58:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.027804 on epoch=119
05/17/2022 19:58:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.026136 on epoch=124
05/17/2022 19:58:19 - INFO - __main__ - Global step 250 Train loss 0.076191 Classification-F1 0.8125 on epoch=124
05/17/2022 19:58:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.014625 on epoch=129
05/17/2022 19:58:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.017142 on epoch=134
05/17/2022 19:58:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.020084 on epoch=139
05/17/2022 19:58:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.003615 on epoch=144
05/17/2022 19:58:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.033052 on epoch=149
05/17/2022 19:58:32 - INFO - __main__ - Global step 300 Train loss 0.017704 Classification-F1 0.8125 on epoch=149
05/17/2022 19:58:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.035237 on epoch=154
05/17/2022 19:58:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.026470 on epoch=159
05/17/2022 19:58:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.014032 on epoch=164
05/17/2022 19:58:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.027457 on epoch=169
05/17/2022 19:58:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.053951 on epoch=174
05/17/2022 19:58:44 - INFO - __main__ - Global step 350 Train loss 0.031430 Classification-F1 0.7810361681329424 on epoch=174
05/17/2022 19:58:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.102712 on epoch=179
05/17/2022 19:58:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.060663 on epoch=184
05/17/2022 19:58:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.047482 on epoch=189
05/17/2022 19:58:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.263771 on epoch=194
05/17/2022 19:58:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.113289 on epoch=199
05/17/2022 19:58:57 - INFO - __main__ - Global step 400 Train loss 0.117583 Classification-F1 0.7117117117117117 on epoch=199
05/17/2022 19:59:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.039586 on epoch=204
05/17/2022 19:59:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.074541 on epoch=209
05/17/2022 19:59:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.026238 on epoch=214
05/17/2022 19:59:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.003384 on epoch=219
05/17/2022 19:59:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.007923 on epoch=224
05/17/2022 19:59:10 - INFO - __main__ - Global step 450 Train loss 0.030334 Classification-F1 0.8117647058823529 on epoch=224
05/17/2022 19:59:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.002415 on epoch=229
05/17/2022 19:59:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.009466 on epoch=234
05/17/2022 19:59:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.006555 on epoch=239
05/17/2022 19:59:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.004784 on epoch=244
05/17/2022 19:59:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.013992 on epoch=249
05/17/2022 19:59:23 - INFO - __main__ - Global step 500 Train loss 0.007442 Classification-F1 0.8125 on epoch=249
05/17/2022 19:59:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.120088 on epoch=254
05/17/2022 19:59:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.022304 on epoch=259
05/17/2022 19:59:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.060230 on epoch=264
05/17/2022 19:59:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.004352 on epoch=269
05/17/2022 19:59:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.014697 on epoch=274
05/17/2022 19:59:36 - INFO - __main__ - Global step 550 Train loss 0.044334 Classification-F1 0.716256157635468 on epoch=274
05/17/2022 19:59:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000782 on epoch=279
05/17/2022 19:59:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.002574 on epoch=284
05/17/2022 19:59:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.112930 on epoch=289
05/17/2022 19:59:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.029203 on epoch=294
05/17/2022 19:59:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.022611 on epoch=299
05/17/2022 19:59:49 - INFO - __main__ - Global step 600 Train loss 0.033620 Classification-F1 0.7810361681329424 on epoch=299
05/17/2022 19:59:49 - INFO - __main__ - save last model!
05/17/2022 19:59:50 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:59:50 - INFO - __main__ - Printing 3 examples
05/17/2022 19:59:50 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/17/2022 19:59:50 - INFO - __main__ - ['negative']
05/17/2022 19:59:50 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/17/2022 19:59:50 - INFO - __main__ - ['negative']
05/17/2022 19:59:50 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/17/2022 19:59:50 - INFO - __main__ - ['negative']
05/17/2022 19:59:50 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:59:50 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:59:50 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 19:59:50 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 19:59:50 - INFO - __main__ - Printing 3 examples
05/17/2022 19:59:50 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
05/17/2022 19:59:50 - INFO - __main__ - ['negative']
05/17/2022 19:59:50 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
05/17/2022 19:59:50 - INFO - __main__ - ['negative']
05/17/2022 19:59:50 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
05/17/2022 19:59:50 - INFO - __main__ - ['negative']
05/17/2022 19:59:50 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:59:50 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:59:50 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 19:59:51 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 19:59:51 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 19:59:51 - INFO - __main__ - Printing 3 examples
05/17/2022 19:59:51 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 19:59:51 - INFO - __main__ - ['negative']
05/17/2022 19:59:51 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 19:59:51 - INFO - __main__ - ['negative']
05/17/2022 19:59:51 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 19:59:51 - INFO - __main__ - ['negative']
05/17/2022 19:59:51 - INFO - __main__ - Tokenizing Input ...
05/17/2022 19:59:52 - INFO - __main__ - Tokenizing Output ...
05/17/2022 19:59:53 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 19:59:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 19:59:54 - INFO - __main__ - Starting training!
05/17/2022 20:00:00 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_13_0.0005_8_predictions.txt
05/17/2022 20:00:00 - INFO - __main__ - Classification-F1 on test data: 0.8676
05/17/2022 20:00:00 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0005, bsz=8, dev_performance=0.8125, test_performance=0.8676498965951673
05/17/2022 20:00:00 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0003, bsz=8 ...
05/17/2022 20:00:01 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:00:01 - INFO - __main__ - Printing 3 examples
05/17/2022 20:00:01 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/17/2022 20:00:01 - INFO - __main__ - ['negative']
05/17/2022 20:00:01 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/17/2022 20:00:01 - INFO - __main__ - ['negative']
05/17/2022 20:00:01 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/17/2022 20:00:01 - INFO - __main__ - ['negative']
05/17/2022 20:00:01 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:00:01 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:00:01 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:00:01 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:00:01 - INFO - __main__ - Printing 3 examples
05/17/2022 20:00:01 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
05/17/2022 20:00:01 - INFO - __main__ - ['negative']
05/17/2022 20:00:01 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
05/17/2022 20:00:01 - INFO - __main__ - ['negative']
05/17/2022 20:00:01 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
05/17/2022 20:00:01 - INFO - __main__ - ['negative']
05/17/2022 20:00:01 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:00:01 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:00:01 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:00:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:00:05 - INFO - __main__ - Starting training!
05/17/2022 20:00:07 - INFO - __main__ - Step 10 Global step 10 Train loss 23.547005 on epoch=4
05/17/2022 20:00:10 - INFO - __main__ - Step 20 Global step 20 Train loss 18.799934 on epoch=9
05/17/2022 20:00:12 - INFO - __main__ - Step 30 Global step 30 Train loss 13.827263 on epoch=14
05/17/2022 20:00:15 - INFO - __main__ - Step 40 Global step 40 Train loss 9.364819 on epoch=19
05/17/2022 20:00:18 - INFO - __main__ - Step 50 Global step 50 Train loss 6.536245 on epoch=24
05/17/2022 20:00:18 - INFO - __main__ - Global step 50 Train loss 14.415053 Classification-F1 0.3333333333333333 on epoch=24
05/17/2022 20:00:21 - INFO - __main__ - Step 60 Global step 60 Train loss 3.630085 on epoch=29
05/17/2022 20:00:23 - INFO - __main__ - Step 70 Global step 70 Train loss 2.447478 on epoch=34
05/17/2022 20:00:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.911344 on epoch=39
05/17/2022 20:00:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.853284 on epoch=44
05/17/2022 20:00:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.561617 on epoch=49
05/17/2022 20:00:31 - INFO - __main__ - Global step 100 Train loss 1.680762 Classification-F1 0.7810361681329424 on epoch=49
05/17/2022 20:00:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.400667 on epoch=54
05/17/2022 20:00:37 - INFO - __main__ - Step 120 Global step 120 Train loss 0.592836 on epoch=59
05/17/2022 20:00:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.658156 on epoch=64
05/17/2022 20:00:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.349616 on epoch=69
05/17/2022 20:00:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.342492 on epoch=74
05/17/2022 20:00:45 - INFO - __main__ - Global step 150 Train loss 0.468753 Classification-F1 0.716256157635468 on epoch=74
05/17/2022 20:00:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.354464 on epoch=79
05/17/2022 20:00:50 - INFO - __main__ - Step 170 Global step 170 Train loss 0.334074 on epoch=84
05/17/2022 20:00:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.273495 on epoch=89
05/17/2022 20:00:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.192571 on epoch=94
05/17/2022 20:00:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.182291 on epoch=99
05/17/2022 20:00:58 - INFO - __main__ - Global step 200 Train loss 0.267379 Classification-F1 0.8435972629521017 on epoch=99
05/17/2022 20:01:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.199136 on epoch=104
05/17/2022 20:01:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.183730 on epoch=109
05/17/2022 20:01:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.143301 on epoch=114
05/17/2022 20:01:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.076849 on epoch=119
05/17/2022 20:01:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.185766 on epoch=124
05/17/2022 20:01:11 - INFO - __main__ - Global step 250 Train loss 0.157756 Classification-F1 0.7810361681329424 on epoch=124
05/17/2022 20:01:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.129487 on epoch=129
05/17/2022 20:01:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.127215 on epoch=134
05/17/2022 20:01:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.135828 on epoch=139
05/17/2022 20:01:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.125952 on epoch=144
05/17/2022 20:01:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.100352 on epoch=149
05/17/2022 20:01:24 - INFO - __main__ - Global step 300 Train loss 0.123767 Classification-F1 0.8125 on epoch=149
05/17/2022 20:01:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.107041 on epoch=154
05/17/2022 20:01:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.070897 on epoch=159
05/17/2022 20:01:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.028891 on epoch=164
05/17/2022 20:01:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.048351 on epoch=169
05/17/2022 20:01:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.043975 on epoch=174
05/17/2022 20:01:38 - INFO - __main__ - Global step 350 Train loss 0.059831 Classification-F1 0.8125 on epoch=174
05/17/2022 20:01:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.021868 on epoch=179
05/17/2022 20:01:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.095236 on epoch=184
05/17/2022 20:01:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.174868 on epoch=189
05/17/2022 20:01:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.193512 on epoch=194
05/17/2022 20:01:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.387844 on epoch=199
05/17/2022 20:01:51 - INFO - __main__ - Global step 400 Train loss 0.174666 Classification-F1 0.6862745098039216 on epoch=199
05/17/2022 20:01:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.343682 on epoch=204
05/17/2022 20:01:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.401983 on epoch=209
05/17/2022 20:01:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.552725 on epoch=214
05/17/2022 20:02:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.356257 on epoch=219
05/17/2022 20:02:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.432890 on epoch=224
05/17/2022 20:02:05 - INFO - __main__ - Global step 450 Train loss 0.417507 Classification-F1 0.6666666666666667 on epoch=224
05/17/2022 20:02:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.404804 on epoch=229
05/17/2022 20:02:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.320311 on epoch=234
05/17/2022 20:02:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.347179 on epoch=239
05/17/2022 20:02:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.393870 on epoch=244
05/17/2022 20:02:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.354403 on epoch=249
05/17/2022 20:02:18 - INFO - __main__ - Global step 500 Train loss 0.364114 Classification-F1 0.6532019704433498 on epoch=249
05/17/2022 20:02:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.319167 on epoch=254
05/17/2022 20:02:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.424678 on epoch=259
05/17/2022 20:02:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.269028 on epoch=264
05/17/2022 20:02:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.212524 on epoch=269
05/17/2022 20:02:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.238810 on epoch=274
05/17/2022 20:02:32 - INFO - __main__ - Global step 550 Train loss 0.292841 Classification-F1 0.746031746031746 on epoch=274
05/17/2022 20:02:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.279962 on epoch=279
05/17/2022 20:02:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.304568 on epoch=284
05/17/2022 20:02:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.265362 on epoch=289
05/17/2022 20:02:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.318004 on epoch=294
05/17/2022 20:02:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.527317 on epoch=299
05/17/2022 20:02:46 - INFO - __main__ - Global step 600 Train loss 0.339043 Classification-F1 0.7117117117117117 on epoch=299
05/17/2022 20:02:46 - INFO - __main__ - save last model!
05/17/2022 20:02:47 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:02:47 - INFO - __main__ - Printing 3 examples
05/17/2022 20:02:47 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/17/2022 20:02:47 - INFO - __main__ - ['negative']
05/17/2022 20:02:47 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/17/2022 20:02:47 - INFO - __main__ - ['negative']
05/17/2022 20:02:47 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/17/2022 20:02:47 - INFO - __main__ - ['negative']
05/17/2022 20:02:47 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:02:47 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:02:47 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:02:47 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:02:47 - INFO - __main__ - Printing 3 examples
05/17/2022 20:02:47 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
05/17/2022 20:02:47 - INFO - __main__ - ['negative']
05/17/2022 20:02:47 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
05/17/2022 20:02:47 - INFO - __main__ - ['negative']
05/17/2022 20:02:47 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
05/17/2022 20:02:47 - INFO - __main__ - ['negative']
05/17/2022 20:02:47 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:02:47 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:02:47 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:02:48 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 20:02:48 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 20:02:48 - INFO - __main__ - Printing 3 examples
05/17/2022 20:02:48 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 20:02:48 - INFO - __main__ - ['negative']
05/17/2022 20:02:48 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 20:02:48 - INFO - __main__ - ['negative']
05/17/2022 20:02:48 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 20:02:48 - INFO - __main__ - ['negative']
05/17/2022 20:02:48 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:02:49 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:02:50 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 20:02:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:02:50 - INFO - __main__ - Starting training!
05/17/2022 20:02:57 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_13_0.0003_8_predictions.txt
05/17/2022 20:02:57 - INFO - __main__ - Classification-F1 on test data: 0.8985
05/17/2022 20:02:57 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0003, bsz=8, dev_performance=0.8435972629521017, test_performance=0.8985445665592845
05/17/2022 20:02:57 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0002, bsz=8 ...
05/17/2022 20:02:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:02:58 - INFO - __main__ - Printing 3 examples
05/17/2022 20:02:58 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/17/2022 20:02:58 - INFO - __main__ - ['negative']
05/17/2022 20:02:58 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/17/2022 20:02:58 - INFO - __main__ - ['negative']
05/17/2022 20:02:58 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/17/2022 20:02:58 - INFO - __main__ - ['negative']
05/17/2022 20:02:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:02:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:02:58 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:02:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:02:58 - INFO - __main__ - Printing 3 examples
05/17/2022 20:02:58 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
05/17/2022 20:02:58 - INFO - __main__ - ['negative']
05/17/2022 20:02:58 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
05/17/2022 20:02:58 - INFO - __main__ - ['negative']
05/17/2022 20:02:58 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
05/17/2022 20:02:58 - INFO - __main__ - ['negative']
05/17/2022 20:02:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:02:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:02:58 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:03:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:03:02 - INFO - __main__ - Starting training!
05/17/2022 20:03:04 - INFO - __main__ - Step 10 Global step 10 Train loss 22.959497 on epoch=4
05/17/2022 20:03:07 - INFO - __main__ - Step 20 Global step 20 Train loss 17.088507 on epoch=9
05/17/2022 20:03:09 - INFO - __main__ - Step 30 Global step 30 Train loss 15.666501 on epoch=14
05/17/2022 20:03:12 - INFO - __main__ - Step 40 Global step 40 Train loss 11.695167 on epoch=19
05/17/2022 20:03:14 - INFO - __main__ - Step 50 Global step 50 Train loss 11.379983 on epoch=24
05/17/2022 20:03:17 - INFO - __main__ - Global step 50 Train loss 15.757931 Classification-F1 0.0 on epoch=24
05/17/2022 20:03:20 - INFO - __main__ - Step 60 Global step 60 Train loss 6.961893 on epoch=29
05/17/2022 20:03:22 - INFO - __main__ - Step 70 Global step 70 Train loss 4.865077 on epoch=34
05/17/2022 20:03:25 - INFO - __main__ - Step 80 Global step 80 Train loss 3.076273 on epoch=39
05/17/2022 20:03:27 - INFO - __main__ - Step 90 Global step 90 Train loss 1.816796 on epoch=44
05/17/2022 20:03:30 - INFO - __main__ - Step 100 Global step 100 Train loss 1.132157 on epoch=49
05/17/2022 20:03:30 - INFO - __main__ - Global step 100 Train loss 3.570439 Classification-F1 0.3333333333333333 on epoch=49
05/17/2022 20:03:33 - INFO - __main__ - Step 110 Global step 110 Train loss 1.204790 on epoch=54
05/17/2022 20:03:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.621285 on epoch=59
05/17/2022 20:03:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.653022 on epoch=64
05/17/2022 20:03:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.647672 on epoch=69
05/17/2022 20:03:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.415842 on epoch=74
05/17/2022 20:03:43 - INFO - __main__ - Global step 150 Train loss 0.708522 Classification-F1 0.6761133603238867 on epoch=74
05/17/2022 20:03:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.489275 on epoch=79
05/17/2022 20:03:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.517498 on epoch=84
05/17/2022 20:03:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.464962 on epoch=89
05/17/2022 20:03:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.466067 on epoch=94
05/17/2022 20:03:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.430436 on epoch=99
05/17/2022 20:03:57 - INFO - __main__ - Global step 200 Train loss 0.473648 Classification-F1 0.6875 on epoch=99
05/17/2022 20:03:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.492223 on epoch=104
05/17/2022 20:04:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.363682 on epoch=109
05/17/2022 20:04:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.431613 on epoch=114
05/17/2022 20:04:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.428044 on epoch=119
05/17/2022 20:04:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.412170 on epoch=124
05/17/2022 20:04:10 - INFO - __main__ - Global step 250 Train loss 0.425546 Classification-F1 0.6267232237539766 on epoch=124
05/17/2022 20:04:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.321962 on epoch=129
05/17/2022 20:04:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.384095 on epoch=134
05/17/2022 20:04:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.241743 on epoch=139
05/17/2022 20:04:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.280151 on epoch=144
05/17/2022 20:04:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.323077 on epoch=149
05/17/2022 20:04:23 - INFO - __main__ - Global step 300 Train loss 0.310206 Classification-F1 0.7490196078431373 on epoch=149
05/17/2022 20:04:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.328733 on epoch=154
05/17/2022 20:04:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.416599 on epoch=159
05/17/2022 20:04:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.435625 on epoch=164
05/17/2022 20:04:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.402681 on epoch=169
05/17/2022 20:04:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.352044 on epoch=174
05/17/2022 20:04:36 - INFO - __main__ - Global step 350 Train loss 0.387136 Classification-F1 0.7810361681329424 on epoch=174
05/17/2022 20:04:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.323725 on epoch=179
05/17/2022 20:04:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.355365 on epoch=184
05/17/2022 20:04:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.325961 on epoch=189
05/17/2022 20:04:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.354560 on epoch=194
05/17/2022 20:04:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.503923 on epoch=199
05/17/2022 20:04:49 - INFO - __main__ - Global step 400 Train loss 0.372707 Classification-F1 0.41700404858299595 on epoch=199
05/17/2022 20:04:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.518618 on epoch=204
05/17/2022 20:04:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.527155 on epoch=209
05/17/2022 20:04:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.367273 on epoch=214
05/17/2022 20:04:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.399783 on epoch=219
05/17/2022 20:05:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.465199 on epoch=224
05/17/2022 20:05:02 - INFO - __main__ - Global step 450 Train loss 0.455605 Classification-F1 0.6875 on epoch=224
05/17/2022 20:05:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.428586 on epoch=229
05/17/2022 20:05:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.469929 on epoch=234
05/17/2022 20:05:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.522920 on epoch=239
05/17/2022 20:05:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.373445 on epoch=244
05/17/2022 20:05:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.335567 on epoch=249
05/17/2022 20:05:15 - INFO - __main__ - Global step 500 Train loss 0.426089 Classification-F1 0.7046153846153846 on epoch=249
05/17/2022 20:05:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.419518 on epoch=254
05/17/2022 20:05:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.382998 on epoch=259
05/17/2022 20:05:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.530730 on epoch=264
05/17/2022 20:05:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.369233 on epoch=269
05/17/2022 20:05:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.389460 on epoch=274
05/17/2022 20:05:27 - INFO - __main__ - Global step 550 Train loss 0.418388 Classification-F1 0.49090909090909085 on epoch=274
05/17/2022 20:05:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.314953 on epoch=279
05/17/2022 20:05:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.352878 on epoch=284
05/17/2022 20:05:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.344506 on epoch=289
05/17/2022 20:05:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.361180 on epoch=294
05/17/2022 20:05:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.334150 on epoch=299
05/17/2022 20:05:40 - INFO - __main__ - Global step 600 Train loss 0.341533 Classification-F1 0.5733333333333335 on epoch=299
05/17/2022 20:05:40 - INFO - __main__ - save last model!
05/17/2022 20:05:41 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:05:41 - INFO - __main__ - Printing 3 examples
05/17/2022 20:05:41 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/17/2022 20:05:41 - INFO - __main__ - ['negative']
05/17/2022 20:05:41 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/17/2022 20:05:41 - INFO - __main__ - ['negative']
05/17/2022 20:05:41 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/17/2022 20:05:41 - INFO - __main__ - ['negative']
05/17/2022 20:05:41 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:05:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:05:41 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:05:41 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:05:41 - INFO - __main__ - Printing 3 examples
05/17/2022 20:05:41 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
05/17/2022 20:05:41 - INFO - __main__ - ['negative']
05/17/2022 20:05:41 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
05/17/2022 20:05:41 - INFO - __main__ - ['negative']
05/17/2022 20:05:41 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
05/17/2022 20:05:41 - INFO - __main__ - ['negative']
05/17/2022 20:05:41 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:05:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:05:41 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:05:43 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 20:05:43 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 20:05:43 - INFO - __main__ - Printing 3 examples
05/17/2022 20:05:43 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 20:05:43 - INFO - __main__ - ['negative']
05/17/2022 20:05:43 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 20:05:43 - INFO - __main__ - ['negative']
05/17/2022 20:05:43 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 20:05:43 - INFO - __main__ - ['negative']
05/17/2022 20:05:43 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:05:44 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:05:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:05:45 - INFO - __main__ - Starting training!
05/17/2022 20:05:45 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 20:05:52 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_13_0.0002_8_predictions.txt
05/17/2022 20:05:52 - INFO - __main__ - Classification-F1 on test data: 0.8266
05/17/2022 20:05:52 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0002, bsz=8, dev_performance=0.7810361681329424, test_performance=0.8265954229256982
05/17/2022 20:05:52 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0001, bsz=8 ...
05/17/2022 20:05:53 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:05:53 - INFO - __main__ - Printing 3 examples
05/17/2022 20:05:53 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/17/2022 20:05:53 - INFO - __main__ - ['negative']
05/17/2022 20:05:53 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/17/2022 20:05:53 - INFO - __main__ - ['negative']
05/17/2022 20:05:53 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/17/2022 20:05:53 - INFO - __main__ - ['negative']
05/17/2022 20:05:53 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:05:53 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:05:53 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:05:53 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:05:53 - INFO - __main__ - Printing 3 examples
05/17/2022 20:05:53 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
05/17/2022 20:05:53 - INFO - __main__ - ['negative']
05/17/2022 20:05:53 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
05/17/2022 20:05:53 - INFO - __main__ - ['negative']
05/17/2022 20:05:53 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
05/17/2022 20:05:53 - INFO - __main__ - ['negative']
05/17/2022 20:05:53 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:05:53 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:05:53 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:05:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:05:57 - INFO - __main__ - Starting training!
05/17/2022 20:05:59 - INFO - __main__ - Step 10 Global step 10 Train loss 22.384211 on epoch=4
05/17/2022 20:06:02 - INFO - __main__ - Step 20 Global step 20 Train loss 19.902678 on epoch=9
05/17/2022 20:06:04 - INFO - __main__ - Step 30 Global step 30 Train loss 17.676655 on epoch=14
05/17/2022 20:06:07 - INFO - __main__ - Step 40 Global step 40 Train loss 15.222328 on epoch=19
05/17/2022 20:06:09 - INFO - __main__ - Step 50 Global step 50 Train loss 13.582125 on epoch=24
05/17/2022 20:06:12 - INFO - __main__ - Global step 50 Train loss 17.753599 Classification-F1 0.0 on epoch=24
05/17/2022 20:06:15 - INFO - __main__ - Step 60 Global step 60 Train loss 12.253461 on epoch=29
05/17/2022 20:06:17 - INFO - __main__ - Step 70 Global step 70 Train loss 11.388090 on epoch=34
05/17/2022 20:06:20 - INFO - __main__ - Step 80 Global step 80 Train loss 10.085814 on epoch=39
05/17/2022 20:06:22 - INFO - __main__ - Step 90 Global step 90 Train loss 8.321781 on epoch=44
05/17/2022 20:06:25 - INFO - __main__ - Step 100 Global step 100 Train loss 8.044699 on epoch=49
05/17/2022 20:06:27 - INFO - __main__ - Global step 100 Train loss 10.018769 Classification-F1 0.0 on epoch=49
05/17/2022 20:06:29 - INFO - __main__ - Step 110 Global step 110 Train loss 7.417597 on epoch=54
05/17/2022 20:06:32 - INFO - __main__ - Step 120 Global step 120 Train loss 5.060439 on epoch=59
05/17/2022 20:06:34 - INFO - __main__ - Step 130 Global step 130 Train loss 6.657136 on epoch=64
05/17/2022 20:06:37 - INFO - __main__ - Step 140 Global step 140 Train loss 5.098737 on epoch=69
05/17/2022 20:06:39 - INFO - __main__ - Step 150 Global step 150 Train loss 4.558659 on epoch=74
05/17/2022 20:06:40 - INFO - __main__ - Global step 150 Train loss 5.758514 Classification-F1 0.3333333333333333 on epoch=74
05/17/2022 20:06:43 - INFO - __main__ - Step 160 Global step 160 Train loss 3.122873 on epoch=79
05/17/2022 20:06:45 - INFO - __main__ - Step 170 Global step 170 Train loss 2.377743 on epoch=84
05/17/2022 20:06:48 - INFO - __main__ - Step 180 Global step 180 Train loss 2.613099 on epoch=89
05/17/2022 20:06:50 - INFO - __main__ - Step 190 Global step 190 Train loss 1.511306 on epoch=94
05/17/2022 20:06:53 - INFO - __main__ - Step 200 Global step 200 Train loss 1.647698 on epoch=99
05/17/2022 20:06:53 - INFO - __main__ - Global step 200 Train loss 2.254544 Classification-F1 0.3333333333333333 on epoch=99
05/17/2022 20:06:55 - INFO - __main__ - Step 210 Global step 210 Train loss 2.115567 on epoch=104
05/17/2022 20:06:58 - INFO - __main__ - Step 220 Global step 220 Train loss 1.557242 on epoch=109
05/17/2022 20:07:01 - INFO - __main__ - Step 230 Global step 230 Train loss 1.292981 on epoch=114
05/17/2022 20:07:03 - INFO - __main__ - Step 240 Global step 240 Train loss 1.175394 on epoch=119
05/17/2022 20:07:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.717857 on epoch=124
05/17/2022 20:07:06 - INFO - __main__ - Global step 250 Train loss 1.371808 Classification-F1 0.8435972629521017 on epoch=124
05/17/2022 20:07:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.861736 on epoch=129
05/17/2022 20:07:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.540782 on epoch=134
05/17/2022 20:07:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.467493 on epoch=139
05/17/2022 20:07:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.423702 on epoch=144
05/17/2022 20:07:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.397855 on epoch=149
05/17/2022 20:07:19 - INFO - __main__ - Global step 300 Train loss 0.538314 Classification-F1 0.9054187192118226 on epoch=149
05/17/2022 20:07:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.321003 on epoch=154
05/17/2022 20:07:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.236681 on epoch=159
05/17/2022 20:07:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.288165 on epoch=164
05/17/2022 20:07:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.237452 on epoch=169
05/17/2022 20:07:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.508934 on epoch=174
05/17/2022 20:07:32 - INFO - __main__ - Global step 350 Train loss 0.318447 Classification-F1 0.8745098039215686 on epoch=174
05/17/2022 20:07:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.249101 on epoch=179
05/17/2022 20:07:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.277386 on epoch=184
05/17/2022 20:07:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.197821 on epoch=189
05/17/2022 20:07:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.171572 on epoch=194
05/17/2022 20:07:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.104221 on epoch=199
05/17/2022 20:07:45 - INFO - __main__ - Global step 400 Train loss 0.200020 Classification-F1 0.8745098039215686 on epoch=199
05/17/2022 20:07:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.322926 on epoch=204
05/17/2022 20:07:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.198660 on epoch=209
05/17/2022 20:07:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.123564 on epoch=214
05/17/2022 20:07:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.115225 on epoch=219
05/17/2022 20:07:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.120990 on epoch=224
05/17/2022 20:07:58 - INFO - __main__ - Global step 450 Train loss 0.176273 Classification-F1 0.8745098039215686 on epoch=224
05/17/2022 20:08:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.085395 on epoch=229
05/17/2022 20:08:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.143340 on epoch=234
05/17/2022 20:08:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.221039 on epoch=239
05/17/2022 20:08:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.199914 on epoch=244
05/17/2022 20:08:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.199085 on epoch=249
05/17/2022 20:08:11 - INFO - __main__ - Global step 500 Train loss 0.169755 Classification-F1 0.8745098039215686 on epoch=249
05/17/2022 20:08:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.137350 on epoch=254
05/17/2022 20:08:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.121160 on epoch=259
05/17/2022 20:08:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.122803 on epoch=264
05/17/2022 20:08:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.106386 on epoch=269
05/17/2022 20:08:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.135321 on epoch=274
05/17/2022 20:08:24 - INFO - __main__ - Global step 550 Train loss 0.124604 Classification-F1 0.9375 on epoch=274
05/17/2022 20:08:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.116605 on epoch=279
05/17/2022 20:08:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.040996 on epoch=284
05/17/2022 20:08:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.103227 on epoch=289
05/17/2022 20:08:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.182302 on epoch=294
05/17/2022 20:08:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.243430 on epoch=299
05/17/2022 20:08:38 - INFO - __main__ - Global step 600 Train loss 0.137312 Classification-F1 0.875 on epoch=299
05/17/2022 20:08:38 - INFO - __main__ - save last model!
05/17/2022 20:08:38 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:08:38 - INFO - __main__ - Printing 3 examples
05/17/2022 20:08:38 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/17/2022 20:08:38 - INFO - __main__ - ['positive']
05/17/2022 20:08:38 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/17/2022 20:08:38 - INFO - __main__ - ['positive']
05/17/2022 20:08:38 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/17/2022 20:08:38 - INFO - __main__ - ['positive']
05/17/2022 20:08:38 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:08:38 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:08:38 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:08:38 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:08:38 - INFO - __main__ - Printing 3 examples
05/17/2022 20:08:38 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
05/17/2022 20:08:38 - INFO - __main__ - ['positive']
05/17/2022 20:08:38 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
05/17/2022 20:08:38 - INFO - __main__ - ['positive']
05/17/2022 20:08:38 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
05/17/2022 20:08:38 - INFO - __main__ - ['positive']
05/17/2022 20:08:38 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:08:38 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:08:39 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:08:40 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 20:08:41 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 20:08:41 - INFO - __main__ - Printing 3 examples
05/17/2022 20:08:41 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 20:08:41 - INFO - __main__ - ['negative']
05/17/2022 20:08:41 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 20:08:41 - INFO - __main__ - ['negative']
05/17/2022 20:08:41 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 20:08:41 - INFO - __main__ - ['negative']
05/17/2022 20:08:41 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:08:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:08:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:08:42 - INFO - __main__ - Starting training!
05/17/2022 20:08:42 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 20:08:49 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_13_0.0001_8_predictions.txt
05/17/2022 20:08:49 - INFO - __main__ - Classification-F1 on test data: 0.9055
05/17/2022 20:08:49 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0001, bsz=8, dev_performance=0.9375, test_performance=0.9054539017364297
05/17/2022 20:08:49 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0005, bsz=8 ...
05/17/2022 20:08:50 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:08:50 - INFO - __main__ - Printing 3 examples
05/17/2022 20:08:50 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/17/2022 20:08:50 - INFO - __main__ - ['positive']
05/17/2022 20:08:50 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/17/2022 20:08:50 - INFO - __main__ - ['positive']
05/17/2022 20:08:50 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/17/2022 20:08:50 - INFO - __main__ - ['positive']
05/17/2022 20:08:50 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:08:50 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:08:50 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:08:50 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:08:50 - INFO - __main__ - Printing 3 examples
05/17/2022 20:08:50 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
05/17/2022 20:08:50 - INFO - __main__ - ['positive']
05/17/2022 20:08:50 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
05/17/2022 20:08:50 - INFO - __main__ - ['positive']
05/17/2022 20:08:50 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
05/17/2022 20:08:50 - INFO - __main__ - ['positive']
05/17/2022 20:08:50 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:08:50 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:08:50 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:08:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:08:55 - INFO - __main__ - Starting training!
05/17/2022 20:08:57 - INFO - __main__ - Step 10 Global step 10 Train loss 23.253136 on epoch=4
05/17/2022 20:08:59 - INFO - __main__ - Step 20 Global step 20 Train loss 17.285347 on epoch=9
05/17/2022 20:09:02 - INFO - __main__ - Step 30 Global step 30 Train loss 9.006243 on epoch=14
05/17/2022 20:09:04 - INFO - __main__ - Step 40 Global step 40 Train loss 6.233538 on epoch=19
05/17/2022 20:09:07 - INFO - __main__ - Step 50 Global step 50 Train loss 2.832517 on epoch=24
05/17/2022 20:09:07 - INFO - __main__ - Global step 50 Train loss 11.722157 Classification-F1 0.3333333333333333 on epoch=24
05/17/2022 20:09:10 - INFO - __main__ - Step 60 Global step 60 Train loss 2.119040 on epoch=29
05/17/2022 20:09:12 - INFO - __main__ - Step 70 Global step 70 Train loss 0.718546 on epoch=34
05/17/2022 20:09:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.624286 on epoch=39
05/17/2022 20:09:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.639297 on epoch=44
05/17/2022 20:09:20 - INFO - __main__ - Step 100 Global step 100 Train loss 0.724010 on epoch=49
05/17/2022 20:09:20 - INFO - __main__ - Global step 100 Train loss 0.965036 Classification-F1 0.7333333333333334 on epoch=49
05/17/2022 20:09:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.662028 on epoch=54
05/17/2022 20:09:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.536835 on epoch=59
05/17/2022 20:09:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.428294 on epoch=64
05/17/2022 20:09:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.491666 on epoch=69
05/17/2022 20:09:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.500777 on epoch=74
05/17/2022 20:09:34 - INFO - __main__ - Global step 150 Train loss 0.523920 Classification-F1 0.5835835835835835 on epoch=74
05/17/2022 20:09:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.412771 on epoch=79
05/17/2022 20:09:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.558114 on epoch=84
05/17/2022 20:09:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.443564 on epoch=89
05/17/2022 20:09:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.517498 on epoch=94
05/17/2022 20:09:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.553436 on epoch=99
05/17/2022 20:09:46 - INFO - __main__ - Global step 200 Train loss 0.497077 Classification-F1 0.3992490613266583 on epoch=99
05/17/2022 20:09:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.426690 on epoch=104
05/17/2022 20:09:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.450987 on epoch=109
05/17/2022 20:09:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.460141 on epoch=114
05/17/2022 20:09:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.465242 on epoch=119
05/17/2022 20:09:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.421311 on epoch=124
05/17/2022 20:09:59 - INFO - __main__ - Global step 250 Train loss 0.444874 Classification-F1 0.49090909090909085 on epoch=124
05/17/2022 20:10:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.360743 on epoch=129
05/17/2022 20:10:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.405495 on epoch=134
05/17/2022 20:10:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.345838 on epoch=139
05/17/2022 20:10:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.354858 on epoch=144
05/17/2022 20:10:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.513463 on epoch=149
05/17/2022 20:10:12 - INFO - __main__ - Global step 300 Train loss 0.396079 Classification-F1 0.4909862142099682 on epoch=149
05/17/2022 20:10:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.482873 on epoch=154
05/17/2022 20:10:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.434238 on epoch=159
05/17/2022 20:10:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.410876 on epoch=164
05/17/2022 20:10:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.451521 on epoch=169
05/17/2022 20:10:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.461304 on epoch=174
05/17/2022 20:10:25 - INFO - __main__ - Global step 350 Train loss 0.448163 Classification-F1 0.3333333333333333 on epoch=174
05/17/2022 20:10:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.415657 on epoch=179
05/17/2022 20:10:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.443715 on epoch=184
05/17/2022 20:10:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.391612 on epoch=189
05/17/2022 20:10:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.494674 on epoch=194
05/17/2022 20:10:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.391322 on epoch=199
05/17/2022 20:10:38 - INFO - __main__ - Global step 400 Train loss 0.427396 Classification-F1 0.3522267206477733 on epoch=199
05/17/2022 20:10:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.381091 on epoch=204
05/17/2022 20:10:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.441488 on epoch=209
05/17/2022 20:10:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.357401 on epoch=214
05/17/2022 20:10:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.357809 on epoch=219
05/17/2022 20:10:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.380382 on epoch=224
05/17/2022 20:10:51 - INFO - __main__ - Global step 450 Train loss 0.383634 Classification-F1 0.4666666666666667 on epoch=224
05/17/2022 20:10:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.428285 on epoch=229
05/17/2022 20:10:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.412903 on epoch=234
05/17/2022 20:10:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.412757 on epoch=239
05/17/2022 20:11:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.351999 on epoch=244
05/17/2022 20:11:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.473198 on epoch=249
05/17/2022 20:11:03 - INFO - __main__ - Global step 500 Train loss 0.415828 Classification-F1 0.49090909090909085 on epoch=249
05/17/2022 20:11:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.347546 on epoch=254
05/17/2022 20:11:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.377974 on epoch=259
05/17/2022 20:11:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.318049 on epoch=264
05/17/2022 20:11:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.389285 on epoch=269
05/17/2022 20:11:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.383316 on epoch=274
05/17/2022 20:11:16 - INFO - __main__ - Global step 550 Train loss 0.363234 Classification-F1 0.3333333333333333 on epoch=274
05/17/2022 20:11:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.309052 on epoch=279
05/17/2022 20:11:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.377269 on epoch=284
05/17/2022 20:11:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.324798 on epoch=289
05/17/2022 20:11:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.326514 on epoch=294
05/17/2022 20:11:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.325387 on epoch=299
05/17/2022 20:11:29 - INFO - __main__ - Global step 600 Train loss 0.332604 Classification-F1 0.6113360323886641 on epoch=299
05/17/2022 20:11:29 - INFO - __main__ - save last model!
05/17/2022 20:11:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:11:30 - INFO - __main__ - Printing 3 examples
05/17/2022 20:11:30 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/17/2022 20:11:30 - INFO - __main__ - ['positive']
05/17/2022 20:11:30 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/17/2022 20:11:30 - INFO - __main__ - ['positive']
05/17/2022 20:11:30 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/17/2022 20:11:30 - INFO - __main__ - ['positive']
05/17/2022 20:11:30 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:11:30 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:11:30 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:11:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:11:30 - INFO - __main__ - Printing 3 examples
05/17/2022 20:11:30 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
05/17/2022 20:11:30 - INFO - __main__ - ['positive']
05/17/2022 20:11:30 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
05/17/2022 20:11:30 - INFO - __main__ - ['positive']
05/17/2022 20:11:30 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
05/17/2022 20:11:30 - INFO - __main__ - ['positive']
05/17/2022 20:11:30 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:11:30 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:11:30 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:11:32 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 20:11:32 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 20:11:32 - INFO - __main__ - Printing 3 examples
05/17/2022 20:11:32 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 20:11:32 - INFO - __main__ - ['negative']
05/17/2022 20:11:32 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 20:11:32 - INFO - __main__ - ['negative']
05/17/2022 20:11:32 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 20:11:32 - INFO - __main__ - ['negative']
05/17/2022 20:11:32 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:11:33 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:11:34 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 20:11:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:11:34 - INFO - __main__ - Starting training!
05/17/2022 20:11:40 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_21_0.0005_8_predictions.txt
05/17/2022 20:11:40 - INFO - __main__ - Classification-F1 on test data: 0.6672
05/17/2022 20:11:41 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0005, bsz=8, dev_performance=0.7333333333333334, test_performance=0.667237325463788
05/17/2022 20:11:41 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0003, bsz=8 ...
05/17/2022 20:11:41 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:11:41 - INFO - __main__ - Printing 3 examples
05/17/2022 20:11:41 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/17/2022 20:11:41 - INFO - __main__ - ['positive']
05/17/2022 20:11:41 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/17/2022 20:11:41 - INFO - __main__ - ['positive']
05/17/2022 20:11:41 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/17/2022 20:11:41 - INFO - __main__ - ['positive']
05/17/2022 20:11:41 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:11:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:11:42 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:11:42 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:11:42 - INFO - __main__ - Printing 3 examples
05/17/2022 20:11:42 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
05/17/2022 20:11:42 - INFO - __main__ - ['positive']
05/17/2022 20:11:42 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
05/17/2022 20:11:42 - INFO - __main__ - ['positive']
05/17/2022 20:11:42 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
05/17/2022 20:11:42 - INFO - __main__ - ['positive']
05/17/2022 20:11:42 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:11:42 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:11:42 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:11:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:11:46 - INFO - __main__ - Starting training!
05/17/2022 20:11:49 - INFO - __main__ - Step 10 Global step 10 Train loss 23.075850 on epoch=4
05/17/2022 20:11:52 - INFO - __main__ - Step 20 Global step 20 Train loss 15.001192 on epoch=9
05/17/2022 20:11:54 - INFO - __main__ - Step 30 Global step 30 Train loss 11.753180 on epoch=14
05/17/2022 20:11:56 - INFO - __main__ - Step 40 Global step 40 Train loss 9.253186 on epoch=19
05/17/2022 20:11:59 - INFO - __main__ - Step 50 Global step 50 Train loss 5.902024 on epoch=24
05/17/2022 20:11:59 - INFO - __main__ - Global step 50 Train loss 12.997087 Classification-F1 0.3333333333333333 on epoch=24
05/17/2022 20:12:02 - INFO - __main__ - Step 60 Global step 60 Train loss 3.771093 on epoch=29
05/17/2022 20:12:04 - INFO - __main__ - Step 70 Global step 70 Train loss 2.323153 on epoch=34
05/17/2022 20:12:07 - INFO - __main__ - Step 80 Global step 80 Train loss 1.243892 on epoch=39
05/17/2022 20:12:09 - INFO - __main__ - Step 90 Global step 90 Train loss 0.622413 on epoch=44
05/17/2022 20:12:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.803816 on epoch=49
05/17/2022 20:12:12 - INFO - __main__ - Global step 100 Train loss 1.752873 Classification-F1 0.9687194525904204 on epoch=49
05/17/2022 20:12:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.509007 on epoch=54
05/17/2022 20:12:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.513864 on epoch=59
05/17/2022 20:12:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.489448 on epoch=64
05/17/2022 20:12:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.368396 on epoch=69
05/17/2022 20:12:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.335294 on epoch=74
05/17/2022 20:12:25 - INFO - __main__ - Global step 150 Train loss 0.443202 Classification-F1 0.9372549019607843 on epoch=74
05/17/2022 20:12:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.477575 on epoch=79
05/17/2022 20:12:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.328307 on epoch=84
05/17/2022 20:12:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.337549 on epoch=89
05/17/2022 20:12:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.377245 on epoch=94
05/17/2022 20:12:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.369957 on epoch=99
05/17/2022 20:12:38 - INFO - __main__ - Global step 200 Train loss 0.378127 Classification-F1 0.9054187192118226 on epoch=99
05/17/2022 20:12:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.386210 on epoch=104
05/17/2022 20:12:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.302200 on epoch=109
05/17/2022 20:12:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.300163 on epoch=114
05/17/2022 20:12:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.370618 on epoch=119
05/17/2022 20:12:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.471800 on epoch=124
05/17/2022 20:12:50 - INFO - __main__ - Global step 250 Train loss 0.366198 Classification-F1 0.8095238095238095 on epoch=124
05/17/2022 20:12:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.380883 on epoch=129
05/17/2022 20:12:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.451796 on epoch=134
05/17/2022 20:12:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.370143 on epoch=139
05/17/2022 20:13:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.240117 on epoch=144
05/17/2022 20:13:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.326774 on epoch=149
05/17/2022 20:13:03 - INFO - __main__ - Global step 300 Train loss 0.353943 Classification-F1 0.9054187192118226 on epoch=149
05/17/2022 20:13:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.344938 on epoch=154
05/17/2022 20:13:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.367750 on epoch=159
05/17/2022 20:13:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.318263 on epoch=164
05/17/2022 20:13:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.326285 on epoch=169
05/17/2022 20:13:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.287097 on epoch=174
05/17/2022 20:13:16 - INFO - __main__ - Global step 350 Train loss 0.328867 Classification-F1 0.7490196078431373 on epoch=174
05/17/2022 20:13:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.266275 on epoch=179
05/17/2022 20:13:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.288699 on epoch=184
05/17/2022 20:13:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.188107 on epoch=189
05/17/2022 20:13:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.144709 on epoch=194
05/17/2022 20:13:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.141774 on epoch=199
05/17/2022 20:13:28 - INFO - __main__ - Global step 400 Train loss 0.205913 Classification-F1 0.8117647058823529 on epoch=199
05/17/2022 20:13:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.110848 on epoch=204
05/17/2022 20:13:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.176652 on epoch=209
05/17/2022 20:13:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.147033 on epoch=214
05/17/2022 20:13:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.201129 on epoch=219
05/17/2022 20:13:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.151801 on epoch=224
05/17/2022 20:13:41 - INFO - __main__ - Global step 450 Train loss 0.157493 Classification-F1 0.875 on epoch=224
05/17/2022 20:13:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.172355 on epoch=229
05/17/2022 20:13:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.231924 on epoch=234
05/17/2022 20:13:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.126590 on epoch=239
05/17/2022 20:13:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.129253 on epoch=244
05/17/2022 20:13:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.056857 on epoch=249
05/17/2022 20:13:53 - INFO - __main__ - Global step 500 Train loss 0.143396 Classification-F1 0.9372549019607843 on epoch=249
05/17/2022 20:13:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.039393 on epoch=254
05/17/2022 20:13:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.041361 on epoch=259
05/17/2022 20:14:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.028053 on epoch=264
05/17/2022 20:14:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.046831 on epoch=269
05/17/2022 20:14:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.014734 on epoch=274
05/17/2022 20:14:06 - INFO - __main__ - Global step 550 Train loss 0.034075 Classification-F1 0.9054187192118226 on epoch=274
05/17/2022 20:14:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.007099 on epoch=279
05/17/2022 20:14:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.020892 on epoch=284
05/17/2022 20:14:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.016450 on epoch=289
05/17/2022 20:14:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.010869 on epoch=294
05/17/2022 20:14:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.002383 on epoch=299
05/17/2022 20:14:19 - INFO - __main__ - Global step 600 Train loss 0.011539 Classification-F1 0.9372549019607843 on epoch=299
05/17/2022 20:14:19 - INFO - __main__ - save last model!
05/17/2022 20:14:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:14:20 - INFO - __main__ - Printing 3 examples
05/17/2022 20:14:20 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/17/2022 20:14:20 - INFO - __main__ - ['positive']
05/17/2022 20:14:20 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/17/2022 20:14:20 - INFO - __main__ - ['positive']
05/17/2022 20:14:20 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/17/2022 20:14:20 - INFO - __main__ - ['positive']
05/17/2022 20:14:20 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:14:20 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:14:20 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:14:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:14:20 - INFO - __main__ - Printing 3 examples
05/17/2022 20:14:20 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
05/17/2022 20:14:20 - INFO - __main__ - ['positive']
05/17/2022 20:14:20 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
05/17/2022 20:14:20 - INFO - __main__ - ['positive']
05/17/2022 20:14:20 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
05/17/2022 20:14:20 - INFO - __main__ - ['positive']
05/17/2022 20:14:20 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:14:20 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:14:20 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:14:21 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 20:14:21 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 20:14:21 - INFO - __main__ - Printing 3 examples
05/17/2022 20:14:21 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 20:14:21 - INFO - __main__ - ['negative']
05/17/2022 20:14:21 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 20:14:21 - INFO - __main__ - ['negative']
05/17/2022 20:14:21 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 20:14:21 - INFO - __main__ - ['negative']
05/17/2022 20:14:21 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:14:22 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:14:23 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 20:14:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:14:23 - INFO - __main__ - Starting training!
05/17/2022 20:14:30 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_21_0.0003_8_predictions.txt
05/17/2022 20:14:30 - INFO - __main__ - Classification-F1 on test data: 0.9320
05/17/2022 20:14:30 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0003, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9319997279989121
05/17/2022 20:14:30 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0002, bsz=8 ...
05/17/2022 20:14:31 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:14:31 - INFO - __main__ - Printing 3 examples
05/17/2022 20:14:31 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/17/2022 20:14:31 - INFO - __main__ - ['positive']
05/17/2022 20:14:31 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/17/2022 20:14:31 - INFO - __main__ - ['positive']
05/17/2022 20:14:31 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/17/2022 20:14:31 - INFO - __main__ - ['positive']
05/17/2022 20:14:31 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:14:31 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:14:31 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:14:31 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:14:31 - INFO - __main__ - Printing 3 examples
05/17/2022 20:14:31 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
05/17/2022 20:14:31 - INFO - __main__ - ['positive']
05/17/2022 20:14:31 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
05/17/2022 20:14:31 - INFO - __main__ - ['positive']
05/17/2022 20:14:31 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
05/17/2022 20:14:31 - INFO - __main__ - ['positive']
05/17/2022 20:14:31 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:14:31 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:14:31 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:14:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:14:35 - INFO - __main__ - Starting training!
05/17/2022 20:14:37 - INFO - __main__ - Step 10 Global step 10 Train loss 24.045040 on epoch=4
05/17/2022 20:14:40 - INFO - __main__ - Step 20 Global step 20 Train loss 19.661203 on epoch=9
05/17/2022 20:14:42 - INFO - __main__ - Step 30 Global step 30 Train loss 14.514160 on epoch=14
05/17/2022 20:14:45 - INFO - __main__ - Step 40 Global step 40 Train loss 12.023011 on epoch=19
05/17/2022 20:14:47 - INFO - __main__ - Step 50 Global step 50 Train loss 8.357065 on epoch=24
05/17/2022 20:14:51 - INFO - __main__ - Global step 50 Train loss 15.720096 Classification-F1 0.0 on epoch=24
05/17/2022 20:14:53 - INFO - __main__ - Step 60 Global step 60 Train loss 7.699440 on epoch=29
05/17/2022 20:14:56 - INFO - __main__ - Step 70 Global step 70 Train loss 6.204771 on epoch=34
05/17/2022 20:14:58 - INFO - __main__ - Step 80 Global step 80 Train loss 4.678150 on epoch=39
05/17/2022 20:15:01 - INFO - __main__ - Step 90 Global step 90 Train loss 3.621038 on epoch=44
05/17/2022 20:15:03 - INFO - __main__ - Step 100 Global step 100 Train loss 2.611035 on epoch=49
05/17/2022 20:15:03 - INFO - __main__ - Global step 100 Train loss 4.962887 Classification-F1 0.3333333333333333 on epoch=49
05/17/2022 20:15:06 - INFO - __main__ - Step 110 Global step 110 Train loss 2.406062 on epoch=54
05/17/2022 20:15:09 - INFO - __main__ - Step 120 Global step 120 Train loss 1.364316 on epoch=59
05/17/2022 20:15:11 - INFO - __main__ - Step 130 Global step 130 Train loss 1.130327 on epoch=64
05/17/2022 20:15:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.879169 on epoch=69
05/17/2022 20:15:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.939974 on epoch=74
05/17/2022 20:15:16 - INFO - __main__ - Global step 150 Train loss 1.343970 Classification-F1 0.9054187192118226 on epoch=74
05/17/2022 20:15:19 - INFO - __main__ - Step 160 Global step 160 Train loss 0.682717 on epoch=79
05/17/2022 20:15:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.496115 on epoch=84
05/17/2022 20:15:24 - INFO - __main__ - Step 180 Global step 180 Train loss 0.616383 on epoch=89
05/17/2022 20:15:27 - INFO - __main__ - Step 190 Global step 190 Train loss 0.550101 on epoch=94
05/17/2022 20:15:29 - INFO - __main__ - Step 200 Global step 200 Train loss 0.648756 on epoch=99
05/17/2022 20:15:30 - INFO - __main__ - Global step 200 Train loss 0.598815 Classification-F1 0.8398398398398398 on epoch=99
05/17/2022 20:15:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.462276 on epoch=104
05/17/2022 20:15:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.408158 on epoch=109
05/17/2022 20:15:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.453879 on epoch=114
05/17/2022 20:15:39 - INFO - __main__ - Step 240 Global step 240 Train loss 0.444928 on epoch=119
05/17/2022 20:15:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.417622 on epoch=124
05/17/2022 20:15:42 - INFO - __main__ - Global step 250 Train loss 0.437373 Classification-F1 0.805668016194332 on epoch=124
05/17/2022 20:15:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.404397 on epoch=129
05/17/2022 20:15:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.398747 on epoch=134
05/17/2022 20:15:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.423018 on epoch=139
05/17/2022 20:15:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.359888 on epoch=144
05/17/2022 20:15:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.475722 on epoch=149
05/17/2022 20:15:55 - INFO - __main__ - Global step 300 Train loss 0.412354 Classification-F1 0.9372549019607843 on epoch=149
05/17/2022 20:15:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.202138 on epoch=154
05/17/2022 20:16:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.388624 on epoch=159
05/17/2022 20:16:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.258332 on epoch=164
05/17/2022 20:16:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.254199 on epoch=169
05/17/2022 20:16:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.287523 on epoch=174
05/17/2022 20:16:08 - INFO - __main__ - Global step 350 Train loss 0.278163 Classification-F1 0.9687194525904204 on epoch=174
05/17/2022 20:16:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.157123 on epoch=179
05/17/2022 20:16:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.242332 on epoch=184
05/17/2022 20:16:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.249132 on epoch=189
05/17/2022 20:16:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.218095 on epoch=194
05/17/2022 20:16:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.133686 on epoch=199
05/17/2022 20:16:21 - INFO - __main__ - Global step 400 Train loss 0.200073 Classification-F1 0.9687194525904204 on epoch=199
05/17/2022 20:16:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.217790 on epoch=204
05/17/2022 20:16:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.137326 on epoch=209
05/17/2022 20:16:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.214168 on epoch=214
05/17/2022 20:16:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.194538 on epoch=219
05/17/2022 20:16:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.147789 on epoch=224
05/17/2022 20:16:33 - INFO - __main__ - Global step 450 Train loss 0.182322 Classification-F1 0.9372549019607843 on epoch=224
05/17/2022 20:16:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.093911 on epoch=229
05/17/2022 20:16:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.144808 on epoch=234
05/17/2022 20:16:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.089197 on epoch=239
05/17/2022 20:16:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.189413 on epoch=244
05/17/2022 20:16:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.049132 on epoch=249
05/17/2022 20:16:46 - INFO - __main__ - Global step 500 Train loss 0.113292 Classification-F1 0.9687194525904204 on epoch=249
05/17/2022 20:16:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.092419 on epoch=254
05/17/2022 20:16:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.147853 on epoch=259
05/17/2022 20:16:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.107623 on epoch=264
05/17/2022 20:16:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.097265 on epoch=269
05/17/2022 20:16:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.055877 on epoch=274
05/17/2022 20:16:59 - INFO - __main__ - Global step 550 Train loss 0.100207 Classification-F1 0.9372549019607843 on epoch=274
05/17/2022 20:17:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.075820 on epoch=279
05/17/2022 20:17:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.045705 on epoch=284
05/17/2022 20:17:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.197463 on epoch=289
05/17/2022 20:17:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.038309 on epoch=294
05/17/2022 20:17:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.041223 on epoch=299
05/17/2022 20:17:11 - INFO - __main__ - Global step 600 Train loss 0.079704 Classification-F1 0.9372549019607843 on epoch=299
05/17/2022 20:17:11 - INFO - __main__ - save last model!
05/17/2022 20:17:12 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:17:12 - INFO - __main__ - Printing 3 examples
05/17/2022 20:17:12 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/17/2022 20:17:12 - INFO - __main__ - ['positive']
05/17/2022 20:17:12 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/17/2022 20:17:12 - INFO - __main__ - ['positive']
05/17/2022 20:17:12 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/17/2022 20:17:12 - INFO - __main__ - ['positive']
05/17/2022 20:17:12 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:17:12 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:17:12 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:17:12 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:17:12 - INFO - __main__ - Printing 3 examples
05/17/2022 20:17:12 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
05/17/2022 20:17:12 - INFO - __main__ - ['positive']
05/17/2022 20:17:12 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
05/17/2022 20:17:12 - INFO - __main__ - ['positive']
05/17/2022 20:17:12 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
05/17/2022 20:17:12 - INFO - __main__ - ['positive']
05/17/2022 20:17:12 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:17:12 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:17:12 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:17:14 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 20:17:14 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 20:17:14 - INFO - __main__ - Printing 3 examples
05/17/2022 20:17:14 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 20:17:14 - INFO - __main__ - ['negative']
05/17/2022 20:17:14 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 20:17:14 - INFO - __main__ - ['negative']
05/17/2022 20:17:14 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 20:17:14 - INFO - __main__ - ['negative']
05/17/2022 20:17:14 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:17:15 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:17:16 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 20:17:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:17:17 - INFO - __main__ - Starting training!
05/17/2022 20:17:23 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_21_0.0002_8_predictions.txt
05/17/2022 20:17:23 - INFO - __main__ - Classification-F1 on test data: 0.9137
05/17/2022 20:17:23 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0002, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9137294555726758
05/17/2022 20:17:23 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0001, bsz=8 ...
05/17/2022 20:17:24 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:17:24 - INFO - __main__ - Printing 3 examples
05/17/2022 20:17:24 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/17/2022 20:17:24 - INFO - __main__ - ['positive']
05/17/2022 20:17:24 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/17/2022 20:17:24 - INFO - __main__ - ['positive']
05/17/2022 20:17:24 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/17/2022 20:17:24 - INFO - __main__ - ['positive']
05/17/2022 20:17:24 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:17:24 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:17:24 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:17:24 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:17:24 - INFO - __main__ - Printing 3 examples
05/17/2022 20:17:24 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
05/17/2022 20:17:24 - INFO - __main__ - ['positive']
05/17/2022 20:17:24 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
05/17/2022 20:17:24 - INFO - __main__ - ['positive']
05/17/2022 20:17:24 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
05/17/2022 20:17:24 - INFO - __main__ - ['positive']
05/17/2022 20:17:24 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:17:24 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:17:24 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:17:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:17:28 - INFO - __main__ - Starting training!
05/17/2022 20:17:30 - INFO - __main__ - Step 10 Global step 10 Train loss 23.104008 on epoch=4
05/17/2022 20:17:32 - INFO - __main__ - Step 20 Global step 20 Train loss 21.507864 on epoch=9
05/17/2022 20:17:35 - INFO - __main__ - Step 30 Global step 30 Train loss 17.235716 on epoch=14
05/17/2022 20:17:37 - INFO - __main__ - Step 40 Global step 40 Train loss 16.364555 on epoch=19
05/17/2022 20:17:40 - INFO - __main__ - Step 50 Global step 50 Train loss 14.133166 on epoch=24
05/17/2022 20:17:43 - INFO - __main__ - Global step 50 Train loss 18.469061 Classification-F1 0.0 on epoch=24
05/17/2022 20:17:46 - INFO - __main__ - Step 60 Global step 60 Train loss 11.753659 on epoch=29
05/17/2022 20:17:48 - INFO - __main__ - Step 70 Global step 70 Train loss 11.463918 on epoch=34
05/17/2022 20:17:51 - INFO - __main__ - Step 80 Global step 80 Train loss 11.613515 on epoch=39
05/17/2022 20:17:53 - INFO - __main__ - Step 90 Global step 90 Train loss 9.862890 on epoch=44
05/17/2022 20:17:56 - INFO - __main__ - Step 100 Global step 100 Train loss 8.322186 on epoch=49
05/17/2022 20:17:58 - INFO - __main__ - Global step 100 Train loss 10.603232 Classification-F1 0.0 on epoch=49
05/17/2022 20:18:01 - INFO - __main__ - Step 110 Global step 110 Train loss 8.450625 on epoch=54
05/17/2022 20:18:03 - INFO - __main__ - Step 120 Global step 120 Train loss 7.064596 on epoch=59
05/17/2022 20:18:06 - INFO - __main__ - Step 130 Global step 130 Train loss 6.820754 on epoch=64
05/17/2022 20:18:08 - INFO - __main__ - Step 140 Global step 140 Train loss 6.245649 on epoch=69
05/17/2022 20:18:11 - INFO - __main__ - Step 150 Global step 150 Train loss 4.841098 on epoch=74
05/17/2022 20:18:11 - INFO - __main__ - Global step 150 Train loss 6.684544 Classification-F1 0.3333333333333333 on epoch=74
05/17/2022 20:18:14 - INFO - __main__ - Step 160 Global step 160 Train loss 3.569440 on epoch=79
05/17/2022 20:18:16 - INFO - __main__ - Step 170 Global step 170 Train loss 3.195032 on epoch=84
05/17/2022 20:18:19 - INFO - __main__ - Step 180 Global step 180 Train loss 2.557412 on epoch=89
05/17/2022 20:18:21 - INFO - __main__ - Step 190 Global step 190 Train loss 3.054714 on epoch=94
05/17/2022 20:18:24 - INFO - __main__ - Step 200 Global step 200 Train loss 2.012224 on epoch=99
05/17/2022 20:18:24 - INFO - __main__ - Global step 200 Train loss 2.877764 Classification-F1 0.3333333333333333 on epoch=99
05/17/2022 20:18:27 - INFO - __main__ - Step 210 Global step 210 Train loss 2.177314 on epoch=104
05/17/2022 20:18:29 - INFO - __main__ - Step 220 Global step 220 Train loss 1.366000 on epoch=109
05/17/2022 20:18:32 - INFO - __main__ - Step 230 Global step 230 Train loss 1.097214 on epoch=114
05/17/2022 20:18:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.892595 on epoch=119
05/17/2022 20:18:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.808653 on epoch=124
05/17/2022 20:18:37 - INFO - __main__ - Global step 250 Train loss 1.268355 Classification-F1 0.9054187192118226 on epoch=124
05/17/2022 20:18:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.728233 on epoch=129
05/17/2022 20:18:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.661548 on epoch=134
05/17/2022 20:18:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.606353 on epoch=139
05/17/2022 20:18:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.671822 on epoch=144
05/17/2022 20:18:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.657460 on epoch=149
05/17/2022 20:18:50 - INFO - __main__ - Global step 300 Train loss 0.665083 Classification-F1 0.9372549019607843 on epoch=149
05/17/2022 20:18:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.510883 on epoch=154
05/17/2022 20:18:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.361170 on epoch=159
05/17/2022 20:18:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.773926 on epoch=164
05/17/2022 20:19:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.432750 on epoch=169
05/17/2022 20:19:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.372284 on epoch=174
05/17/2022 20:19:03 - INFO - __main__ - Global step 350 Train loss 0.490202 Classification-F1 0.873015873015873 on epoch=174
05/17/2022 20:19:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.463949 on epoch=179
05/17/2022 20:19:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.343288 on epoch=184
05/17/2022 20:19:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.346857 on epoch=189
05/17/2022 20:19:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.379962 on epoch=194
05/17/2022 20:19:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.528462 on epoch=199
05/17/2022 20:19:16 - INFO - __main__ - Global step 400 Train loss 0.412503 Classification-F1 0.9372549019607843 on epoch=199
05/17/2022 20:19:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.355990 on epoch=204
05/17/2022 20:19:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.447301 on epoch=209
05/17/2022 20:19:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.300874 on epoch=214
05/17/2022 20:19:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.316831 on epoch=219
05/17/2022 20:19:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.409797 on epoch=224
05/17/2022 20:19:28 - INFO - __main__ - Global step 450 Train loss 0.366159 Classification-F1 0.9372549019607843 on epoch=224
05/17/2022 20:19:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.319127 on epoch=229
05/17/2022 20:19:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.223502 on epoch=234
05/17/2022 20:19:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.297325 on epoch=239
05/17/2022 20:19:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.233650 on epoch=244
05/17/2022 20:19:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.222174 on epoch=249
05/17/2022 20:19:41 - INFO - __main__ - Global step 500 Train loss 0.259156 Classification-F1 0.9372549019607843 on epoch=249
05/17/2022 20:19:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.285676 on epoch=254
05/17/2022 20:19:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.181748 on epoch=259
05/17/2022 20:19:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.192369 on epoch=264
05/17/2022 20:19:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.210949 on epoch=269
05/17/2022 20:19:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.155525 on epoch=274
05/17/2022 20:19:54 - INFO - __main__ - Global step 550 Train loss 0.205253 Classification-F1 0.9372549019607843 on epoch=274
05/17/2022 20:19:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.213828 on epoch=279
05/17/2022 20:19:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.222191 on epoch=284
05/17/2022 20:20:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.218149 on epoch=289
05/17/2022 20:20:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.188681 on epoch=294
05/17/2022 20:20:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.259504 on epoch=299
05/17/2022 20:20:07 - INFO - __main__ - Global step 600 Train loss 0.220471 Classification-F1 0.9372549019607843 on epoch=299
05/17/2022 20:20:07 - INFO - __main__ - save last model!
05/17/2022 20:20:08 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:20:08 - INFO - __main__ - Printing 3 examples
05/17/2022 20:20:08 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/17/2022 20:20:08 - INFO - __main__ - ['negative']
05/17/2022 20:20:08 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/17/2022 20:20:08 - INFO - __main__ - ['negative']
05/17/2022 20:20:08 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/17/2022 20:20:08 - INFO - __main__ - ['negative']
05/17/2022 20:20:08 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:20:08 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:20:08 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:20:08 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:20:08 - INFO - __main__ - Printing 3 examples
05/17/2022 20:20:08 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
05/17/2022 20:20:08 - INFO - __main__ - ['negative']
05/17/2022 20:20:08 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
05/17/2022 20:20:08 - INFO - __main__ - ['negative']
05/17/2022 20:20:08 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
05/17/2022 20:20:08 - INFO - __main__ - ['negative']
05/17/2022 20:20:08 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:20:08 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:20:08 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:20:09 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 20:20:10 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 20:20:10 - INFO - __main__ - Printing 3 examples
05/17/2022 20:20:10 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 20:20:10 - INFO - __main__ - ['negative']
05/17/2022 20:20:10 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 20:20:10 - INFO - __main__ - ['negative']
05/17/2022 20:20:10 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 20:20:10 - INFO - __main__ - ['negative']
05/17/2022 20:20:10 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:20:10 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:20:11 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 20:20:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:20:12 - INFO - __main__ - Starting training!
05/17/2022 20:20:18 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_21_0.0001_8_predictions.txt
05/17/2022 20:20:18 - INFO - __main__ - Classification-F1 on test data: 0.8788
05/17/2022 20:20:18 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0001, bsz=8, dev_performance=0.9372549019607843, test_performance=0.8788363442501786
05/17/2022 20:20:18 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0005, bsz=8 ...
05/17/2022 20:20:19 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:20:19 - INFO - __main__ - Printing 3 examples
05/17/2022 20:20:19 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/17/2022 20:20:19 - INFO - __main__ - ['negative']
05/17/2022 20:20:19 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/17/2022 20:20:19 - INFO - __main__ - ['negative']
05/17/2022 20:20:19 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/17/2022 20:20:19 - INFO - __main__ - ['negative']
05/17/2022 20:20:19 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:20:19 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:20:19 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:20:19 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:20:19 - INFO - __main__ - Printing 3 examples
05/17/2022 20:20:19 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
05/17/2022 20:20:19 - INFO - __main__ - ['negative']
05/17/2022 20:20:19 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
05/17/2022 20:20:19 - INFO - __main__ - ['negative']
05/17/2022 20:20:19 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
05/17/2022 20:20:19 - INFO - __main__ - ['negative']
05/17/2022 20:20:19 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:20:19 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:20:19 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:20:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:20:23 - INFO - __main__ - Starting training!
05/17/2022 20:20:25 - INFO - __main__ - Step 10 Global step 10 Train loss 22.337944 on epoch=4
05/17/2022 20:20:28 - INFO - __main__ - Step 20 Global step 20 Train loss 15.356854 on epoch=9
05/17/2022 20:20:30 - INFO - __main__ - Step 30 Global step 30 Train loss 10.307205 on epoch=14
05/17/2022 20:20:33 - INFO - __main__ - Step 40 Global step 40 Train loss 7.150568 on epoch=19
05/17/2022 20:20:35 - INFO - __main__ - Step 50 Global step 50 Train loss 2.875314 on epoch=24
05/17/2022 20:20:35 - INFO - __main__ - Global step 50 Train loss 11.605577 Classification-F1 0.3333333333333333 on epoch=24
05/17/2022 20:20:38 - INFO - __main__ - Step 60 Global step 60 Train loss 1.627203 on epoch=29
05/17/2022 20:20:41 - INFO - __main__ - Step 70 Global step 70 Train loss 0.864053 on epoch=34
05/17/2022 20:20:43 - INFO - __main__ - Step 80 Global step 80 Train loss 0.722600 on epoch=39
05/17/2022 20:20:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.515666 on epoch=44
05/17/2022 20:20:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.452131 on epoch=49
05/17/2022 20:20:48 - INFO - __main__ - Global step 100 Train loss 0.836330 Classification-F1 0.5844155844155844 on epoch=49
05/17/2022 20:20:52 - INFO - __main__ - Step 110 Global step 110 Train loss 0.525319 on epoch=54
05/17/2022 20:20:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.581154 on epoch=59
05/17/2022 20:20:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.522317 on epoch=64
05/17/2022 20:20:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.540084 on epoch=69
05/17/2022 20:21:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.476987 on epoch=74
05/17/2022 20:21:02 - INFO - __main__ - Global step 150 Train loss 0.529172 Classification-F1 0.3191489361702127 on epoch=74
05/17/2022 20:21:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.490449 on epoch=79
05/17/2022 20:21:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.563897 on epoch=84
05/17/2022 20:21:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.509253 on epoch=89
05/17/2022 20:21:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.446416 on epoch=94
05/17/2022 20:21:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.506881 on epoch=99
05/17/2022 20:21:15 - INFO - __main__ - Global step 200 Train loss 0.503379 Classification-F1 0.3333333333333333 on epoch=99
05/17/2022 20:21:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.339115 on epoch=104
05/17/2022 20:21:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.411783 on epoch=109
05/17/2022 20:21:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.452038 on epoch=114
05/17/2022 20:21:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.470480 on epoch=119
05/17/2022 20:21:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.434519 on epoch=124
05/17/2022 20:21:27 - INFO - __main__ - Global step 250 Train loss 0.421587 Classification-F1 0.5636363636363637 on epoch=124
05/17/2022 20:21:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.482151 on epoch=129
05/17/2022 20:21:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.494278 on epoch=134
05/17/2022 20:21:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.457343 on epoch=139
05/17/2022 20:21:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.425744 on epoch=144
05/17/2022 20:21:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.400701 on epoch=149
05/17/2022 20:21:40 - INFO - __main__ - Global step 300 Train loss 0.452044 Classification-F1 0.3816425120772947 on epoch=149
05/17/2022 20:21:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.421994 on epoch=154
05/17/2022 20:21:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.436395 on epoch=159
05/17/2022 20:21:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.433824 on epoch=164
05/17/2022 20:21:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.370004 on epoch=169
05/17/2022 20:21:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.466582 on epoch=174
05/17/2022 20:21:53 - INFO - __main__ - Global step 350 Train loss 0.425760 Classification-F1 0.6825396825396826 on epoch=174
05/17/2022 20:21:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.343293 on epoch=179
05/17/2022 20:21:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.396725 on epoch=184
05/17/2022 20:22:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.338535 on epoch=189
05/17/2022 20:22:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.372665 on epoch=194
05/17/2022 20:22:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.402821 on epoch=199
05/17/2022 20:22:06 - INFO - __main__ - Global step 400 Train loss 0.370808 Classification-F1 0.5636363636363637 on epoch=199
05/17/2022 20:22:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.387883 on epoch=204
05/17/2022 20:22:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.325749 on epoch=209
05/17/2022 20:22:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.360252 on epoch=214
05/17/2022 20:22:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.385518 on epoch=219
05/17/2022 20:22:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.373026 on epoch=224
05/17/2022 20:22:19 - INFO - __main__ - Global step 450 Train loss 0.366486 Classification-F1 0.716256157635468 on epoch=224
05/17/2022 20:22:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.403867 on epoch=229
05/17/2022 20:22:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.388849 on epoch=234
05/17/2022 20:22:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.360272 on epoch=239
05/17/2022 20:22:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.412811 on epoch=244
05/17/2022 20:22:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.389336 on epoch=249
05/17/2022 20:22:32 - INFO - __main__ - Global step 500 Train loss 0.391027 Classification-F1 0.5076923076923077 on epoch=249
05/17/2022 20:22:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.336195 on epoch=254
05/17/2022 20:22:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.387607 on epoch=259
05/17/2022 20:22:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.412364 on epoch=264
05/17/2022 20:22:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.352906 on epoch=269
05/17/2022 20:22:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.335379 on epoch=274
05/17/2022 20:22:45 - INFO - __main__ - Global step 550 Train loss 0.364890 Classification-F1 0.5844155844155844 on epoch=274
05/17/2022 20:22:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.336188 on epoch=279
05/17/2022 20:22:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.359402 on epoch=284
05/17/2022 20:22:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.343791 on epoch=289
05/17/2022 20:22:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.330145 on epoch=294
05/17/2022 20:22:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.363235 on epoch=299
05/17/2022 20:22:57 - INFO - __main__ - Global step 600 Train loss 0.346552 Classification-F1 0.4589371980676329 on epoch=299
05/17/2022 20:22:57 - INFO - __main__ - save last model!
05/17/2022 20:22:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:22:58 - INFO - __main__ - Printing 3 examples
05/17/2022 20:22:58 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/17/2022 20:22:58 - INFO - __main__ - ['negative']
05/17/2022 20:22:58 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/17/2022 20:22:58 - INFO - __main__ - ['negative']
05/17/2022 20:22:58 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/17/2022 20:22:58 - INFO - __main__ - ['negative']
05/17/2022 20:22:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:22:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:22:58 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:22:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:22:58 - INFO - __main__ - Printing 3 examples
05/17/2022 20:22:58 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
05/17/2022 20:22:58 - INFO - __main__ - ['negative']
05/17/2022 20:22:58 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
05/17/2022 20:22:58 - INFO - __main__ - ['negative']
05/17/2022 20:22:58 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
05/17/2022 20:22:58 - INFO - __main__ - ['negative']
05/17/2022 20:22:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:22:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:22:58 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:23:00 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 20:23:00 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 20:23:00 - INFO - __main__ - Printing 3 examples
05/17/2022 20:23:00 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 20:23:00 - INFO - __main__ - ['negative']
05/17/2022 20:23:00 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 20:23:00 - INFO - __main__ - ['negative']
05/17/2022 20:23:00 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 20:23:00 - INFO - __main__ - ['negative']
05/17/2022 20:23:00 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:23:01 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:23:02 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 20:23:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:23:02 - INFO - __main__ - Starting training!
05/17/2022 20:23:08 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_42_0.0005_8_predictions.txt
05/17/2022 20:23:08 - INFO - __main__ - Classification-F1 on test data: 0.5702
05/17/2022 20:23:09 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0005, bsz=8, dev_performance=0.716256157635468, test_performance=0.5702397018186491
05/17/2022 20:23:09 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0003, bsz=8 ...
05/17/2022 20:23:10 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:23:10 - INFO - __main__ - Printing 3 examples
05/17/2022 20:23:10 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/17/2022 20:23:10 - INFO - __main__ - ['negative']
05/17/2022 20:23:10 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/17/2022 20:23:10 - INFO - __main__ - ['negative']
05/17/2022 20:23:10 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/17/2022 20:23:10 - INFO - __main__ - ['negative']
05/17/2022 20:23:10 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:23:10 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:23:10 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:23:10 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:23:10 - INFO - __main__ - Printing 3 examples
05/17/2022 20:23:10 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
05/17/2022 20:23:10 - INFO - __main__ - ['negative']
05/17/2022 20:23:10 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
05/17/2022 20:23:10 - INFO - __main__ - ['negative']
05/17/2022 20:23:10 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
05/17/2022 20:23:10 - INFO - __main__ - ['negative']
05/17/2022 20:23:10 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:23:10 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:23:10 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:23:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:23:13 - INFO - __main__ - Starting training!
05/17/2022 20:23:16 - INFO - __main__ - Step 10 Global step 10 Train loss 22.502666 on epoch=4
05/17/2022 20:23:18 - INFO - __main__ - Step 20 Global step 20 Train loss 18.069433 on epoch=9
05/17/2022 20:23:20 - INFO - __main__ - Step 30 Global step 30 Train loss 12.965147 on epoch=14
05/17/2022 20:23:23 - INFO - __main__ - Step 40 Global step 40 Train loss 8.695524 on epoch=19
05/17/2022 20:23:26 - INFO - __main__ - Step 50 Global step 50 Train loss 5.376731 on epoch=24
05/17/2022 20:23:27 - INFO - __main__ - Global step 50 Train loss 13.521900 Classification-F1 0.21276595744680848 on epoch=24
05/17/2022 20:23:30 - INFO - __main__ - Step 60 Global step 60 Train loss 3.178871 on epoch=29
05/17/2022 20:23:33 - INFO - __main__ - Step 70 Global step 70 Train loss 2.360881 on epoch=34
05/17/2022 20:23:35 - INFO - __main__ - Step 80 Global step 80 Train loss 1.288979 on epoch=39
05/17/2022 20:23:38 - INFO - __main__ - Step 90 Global step 90 Train loss 1.061392 on epoch=44
05/17/2022 20:23:40 - INFO - __main__ - Step 100 Global step 100 Train loss 0.717994 on epoch=49
05/17/2022 20:23:40 - INFO - __main__ - Global step 100 Train loss 1.721623 Classification-F1 0.8398398398398398 on epoch=49
05/17/2022 20:23:44 - INFO - __main__ - Step 110 Global step 110 Train loss 0.688074 on epoch=54
05/17/2022 20:23:46 - INFO - __main__ - Step 120 Global step 120 Train loss 0.410030 on epoch=59
05/17/2022 20:23:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.348011 on epoch=64
05/17/2022 20:23:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.435327 on epoch=69
05/17/2022 20:23:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.292934 on epoch=74
05/17/2022 20:23:54 - INFO - __main__ - Global step 150 Train loss 0.434875 Classification-F1 0.8398398398398398 on epoch=74
05/17/2022 20:23:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.385264 on epoch=79
05/17/2022 20:23:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.323589 on epoch=84
05/17/2022 20:24:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.176888 on epoch=89
05/17/2022 20:24:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.132750 on epoch=94
05/17/2022 20:24:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.165264 on epoch=99
05/17/2022 20:24:07 - INFO - __main__ - Global step 200 Train loss 0.236751 Classification-F1 0.9054187192118226 on epoch=99
05/17/2022 20:24:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.223244 on epoch=104
05/17/2022 20:24:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.193932 on epoch=109
05/17/2022 20:24:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.082371 on epoch=114
05/17/2022 20:24:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.235333 on epoch=119
05/17/2022 20:24:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.113017 on epoch=124
05/17/2022 20:24:20 - INFO - __main__ - Global step 250 Train loss 0.169579 Classification-F1 0.9054187192118226 on epoch=124
05/17/2022 20:24:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.044656 on epoch=129
05/17/2022 20:24:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.030367 on epoch=134
05/17/2022 20:24:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.094354 on epoch=139
05/17/2022 20:24:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.052986 on epoch=144
05/17/2022 20:24:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.038969 on epoch=149
05/17/2022 20:24:33 - INFO - __main__ - Global step 300 Train loss 0.052267 Classification-F1 0.9054187192118226 on epoch=149
05/17/2022 20:24:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.075542 on epoch=154
05/17/2022 20:24:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.053617 on epoch=159
05/17/2022 20:24:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.023118 on epoch=164
05/17/2022 20:24:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.073829 on epoch=169
05/17/2022 20:24:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.020520 on epoch=174
05/17/2022 20:24:46 - INFO - __main__ - Global step 350 Train loss 0.049325 Classification-F1 0.9054187192118226 on epoch=174
05/17/2022 20:24:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.039540 on epoch=179
05/17/2022 20:24:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.045694 on epoch=184
05/17/2022 20:24:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.034075 on epoch=189
05/17/2022 20:24:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.038417 on epoch=194
05/17/2022 20:24:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.016087 on epoch=199
05/17/2022 20:24:58 - INFO - __main__ - Global step 400 Train loss 0.034762 Classification-F1 0.9687194525904204 on epoch=199
05/17/2022 20:25:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.069252 on epoch=204
05/17/2022 20:25:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.038882 on epoch=209
05/17/2022 20:25:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.028560 on epoch=214
05/17/2022 20:25:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.037676 on epoch=219
05/17/2022 20:25:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.006560 on epoch=224
05/17/2022 20:25:12 - INFO - __main__ - Global step 450 Train loss 0.036186 Classification-F1 0.9372549019607843 on epoch=224
05/17/2022 20:25:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.037645 on epoch=229
05/17/2022 20:25:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.037255 on epoch=234
05/17/2022 20:25:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.063125 on epoch=239
05/17/2022 20:25:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.015179 on epoch=244
05/17/2022 20:25:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.017556 on epoch=249
05/17/2022 20:25:25 - INFO - __main__ - Global step 500 Train loss 0.034152 Classification-F1 0.9054187192118226 on epoch=249
05/17/2022 20:25:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.032587 on epoch=254
05/17/2022 20:25:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.008831 on epoch=259
05/17/2022 20:25:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.034066 on epoch=264
05/17/2022 20:25:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.077398 on epoch=269
05/17/2022 20:25:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.036592 on epoch=274
05/17/2022 20:25:38 - INFO - __main__ - Global step 550 Train loss 0.037895 Classification-F1 0.9372549019607843 on epoch=274
05/17/2022 20:25:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.017081 on epoch=279
05/17/2022 20:25:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.036186 on epoch=284
05/17/2022 20:25:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.016026 on epoch=289
05/17/2022 20:25:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.046679 on epoch=294
05/17/2022 20:25:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.047649 on epoch=299
05/17/2022 20:25:50 - INFO - __main__ - Global step 600 Train loss 0.032724 Classification-F1 0.9372549019607843 on epoch=299
05/17/2022 20:25:50 - INFO - __main__ - save last model!
05/17/2022 20:25:52 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:25:52 - INFO - __main__ - Printing 3 examples
05/17/2022 20:25:52 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/17/2022 20:25:52 - INFO - __main__ - ['negative']
05/17/2022 20:25:52 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/17/2022 20:25:52 - INFO - __main__ - ['negative']
05/17/2022 20:25:52 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/17/2022 20:25:52 - INFO - __main__ - ['negative']
05/17/2022 20:25:52 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:25:52 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:25:52 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:25:52 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:25:52 - INFO - __main__ - Printing 3 examples
05/17/2022 20:25:52 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
05/17/2022 20:25:52 - INFO - __main__ - ['negative']
05/17/2022 20:25:52 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
05/17/2022 20:25:52 - INFO - __main__ - ['negative']
05/17/2022 20:25:52 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
05/17/2022 20:25:52 - INFO - __main__ - ['negative']
05/17/2022 20:25:52 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:25:52 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:25:52 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:25:53 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 20:25:53 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 20:25:53 - INFO - __main__ - Printing 3 examples
05/17/2022 20:25:53 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 20:25:53 - INFO - __main__ - ['negative']
05/17/2022 20:25:53 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 20:25:53 - INFO - __main__ - ['negative']
05/17/2022 20:25:53 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 20:25:53 - INFO - __main__ - ['negative']
05/17/2022 20:25:53 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:25:54 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:25:55 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 20:25:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:25:56 - INFO - __main__ - Starting training!
05/17/2022 20:26:01 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_42_0.0003_8_predictions.txt
05/17/2022 20:26:01 - INFO - __main__ - Classification-F1 on test data: 0.9219
05/17/2022 20:26:02 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0003, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9219472363317602
05/17/2022 20:26:02 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0002, bsz=8 ...
05/17/2022 20:26:02 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:26:02 - INFO - __main__ - Printing 3 examples
05/17/2022 20:26:02 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/17/2022 20:26:02 - INFO - __main__ - ['negative']
05/17/2022 20:26:02 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/17/2022 20:26:02 - INFO - __main__ - ['negative']
05/17/2022 20:26:02 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/17/2022 20:26:02 - INFO - __main__ - ['negative']
05/17/2022 20:26:02 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:26:02 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:26:03 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:26:03 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:26:03 - INFO - __main__ - Printing 3 examples
05/17/2022 20:26:03 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
05/17/2022 20:26:03 - INFO - __main__ - ['negative']
05/17/2022 20:26:03 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
05/17/2022 20:26:03 - INFO - __main__ - ['negative']
05/17/2022 20:26:03 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
05/17/2022 20:26:03 - INFO - __main__ - ['negative']
05/17/2022 20:26:03 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:26:03 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:26:03 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:26:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:26:07 - INFO - __main__ - Starting training!
05/17/2022 20:26:09 - INFO - __main__ - Step 10 Global step 10 Train loss 24.064505 on epoch=4
05/17/2022 20:26:11 - INFO - __main__ - Step 20 Global step 20 Train loss 18.720013 on epoch=9
05/17/2022 20:26:14 - INFO - __main__ - Step 30 Global step 30 Train loss 13.290155 on epoch=14
05/17/2022 20:26:16 - INFO - __main__ - Step 40 Global step 40 Train loss 10.528700 on epoch=19
05/17/2022 20:26:19 - INFO - __main__ - Step 50 Global step 50 Train loss 7.390696 on epoch=24
05/17/2022 20:26:22 - INFO - __main__ - Global step 50 Train loss 14.798812 Classification-F1 0.0 on epoch=24
05/17/2022 20:26:25 - INFO - __main__ - Step 60 Global step 60 Train loss 6.962415 on epoch=29
05/17/2022 20:26:28 - INFO - __main__ - Step 70 Global step 70 Train loss 5.632655 on epoch=34
05/17/2022 20:26:30 - INFO - __main__ - Step 80 Global step 80 Train loss 2.861608 on epoch=39
05/17/2022 20:26:33 - INFO - __main__ - Step 90 Global step 90 Train loss 1.639277 on epoch=44
05/17/2022 20:26:35 - INFO - __main__ - Step 100 Global step 100 Train loss 1.506745 on epoch=49
05/17/2022 20:26:35 - INFO - __main__ - Global step 100 Train loss 3.720540 Classification-F1 0.6101882613510521 on epoch=49
05/17/2022 20:26:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.702399 on epoch=54
05/17/2022 20:26:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.650741 on epoch=59
05/17/2022 20:26:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.479301 on epoch=64
05/17/2022 20:26:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.530710 on epoch=69
05/17/2022 20:26:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.387099 on epoch=74
05/17/2022 20:26:49 - INFO - __main__ - Global step 150 Train loss 0.550050 Classification-F1 0.8095238095238095 on epoch=74
05/17/2022 20:26:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.366801 on epoch=79
05/17/2022 20:26:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.470313 on epoch=84
05/17/2022 20:26:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.272424 on epoch=89
05/17/2022 20:26:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.360355 on epoch=94
05/17/2022 20:27:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.309588 on epoch=99
05/17/2022 20:27:02 - INFO - __main__ - Global step 200 Train loss 0.355896 Classification-F1 0.7702564102564102 on epoch=99
05/17/2022 20:27:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.281285 on epoch=104
05/17/2022 20:27:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.310519 on epoch=109
05/17/2022 20:27:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.202286 on epoch=114
05/17/2022 20:27:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.148536 on epoch=119
05/17/2022 20:27:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.216410 on epoch=124
05/17/2022 20:27:15 - INFO - __main__ - Global step 250 Train loss 0.231807 Classification-F1 0.805668016194332 on epoch=124
05/17/2022 20:27:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.157885 on epoch=129
05/17/2022 20:27:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.131938 on epoch=134
05/17/2022 20:27:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.202626 on epoch=139
05/17/2022 20:27:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.099636 on epoch=144
05/17/2022 20:27:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.121854 on epoch=149
05/17/2022 20:27:27 - INFO - __main__ - Global step 300 Train loss 0.142788 Classification-F1 0.873015873015873 on epoch=149
05/17/2022 20:27:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.095947 on epoch=154
05/17/2022 20:27:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.106509 on epoch=159
05/17/2022 20:27:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.104975 on epoch=164
05/17/2022 20:27:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.076311 on epoch=169
05/17/2022 20:27:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.141726 on epoch=174
05/17/2022 20:27:41 - INFO - __main__ - Global step 350 Train loss 0.105093 Classification-F1 0.873015873015873 on epoch=174
05/17/2022 20:27:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.100957 on epoch=179
05/17/2022 20:27:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.101457 on epoch=184
05/17/2022 20:27:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.183997 on epoch=189
05/17/2022 20:27:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.195775 on epoch=194
05/17/2022 20:27:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.128620 on epoch=199
05/17/2022 20:27:54 - INFO - __main__ - Global step 400 Train loss 0.142161 Classification-F1 0.9372549019607843 on epoch=199
05/17/2022 20:27:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.106055 on epoch=204
05/17/2022 20:27:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.269904 on epoch=209
05/17/2022 20:28:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.213726 on epoch=214
05/17/2022 20:28:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.274055 on epoch=219
05/17/2022 20:28:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.203523 on epoch=224
05/17/2022 20:28:07 - INFO - __main__ - Global step 450 Train loss 0.213453 Classification-F1 0.9054187192118226 on epoch=224
05/17/2022 20:28:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.148058 on epoch=229
05/17/2022 20:28:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.116368 on epoch=234
05/17/2022 20:28:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.125820 on epoch=239
05/17/2022 20:28:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.239583 on epoch=244
05/17/2022 20:28:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.244844 on epoch=249
05/17/2022 20:28:19 - INFO - __main__ - Global step 500 Train loss 0.174935 Classification-F1 0.9372549019607843 on epoch=249
05/17/2022 20:28:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.101245 on epoch=254
05/17/2022 20:28:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.139134 on epoch=259
05/17/2022 20:28:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.156940 on epoch=264
05/17/2022 20:28:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.342352 on epoch=269
05/17/2022 20:28:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.264275 on epoch=274
05/17/2022 20:28:32 - INFO - __main__ - Global step 550 Train loss 0.200789 Classification-F1 0.9372549019607843 on epoch=274
05/17/2022 20:28:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.306130 on epoch=279
05/17/2022 20:28:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.286041 on epoch=284
05/17/2022 20:28:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.299344 on epoch=289
05/17/2022 20:28:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.329087 on epoch=294
05/17/2022 20:28:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.384885 on epoch=299
05/17/2022 20:28:45 - INFO - __main__ - Global step 600 Train loss 0.321098 Classification-F1 0.9054187192118226 on epoch=299
05/17/2022 20:28:45 - INFO - __main__ - save last model!
05/17/2022 20:28:46 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:28:46 - INFO - __main__ - Printing 3 examples
05/17/2022 20:28:46 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/17/2022 20:28:46 - INFO - __main__ - ['negative']
05/17/2022 20:28:46 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/17/2022 20:28:46 - INFO - __main__ - ['negative']
05/17/2022 20:28:46 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/17/2022 20:28:46 - INFO - __main__ - ['negative']
05/17/2022 20:28:46 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:28:46 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:28:46 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:28:46 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:28:46 - INFO - __main__ - Printing 3 examples
05/17/2022 20:28:46 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
05/17/2022 20:28:46 - INFO - __main__ - ['negative']
05/17/2022 20:28:46 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
05/17/2022 20:28:46 - INFO - __main__ - ['negative']
05/17/2022 20:28:46 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
05/17/2022 20:28:46 - INFO - __main__ - ['negative']
05/17/2022 20:28:46 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:28:46 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:28:46 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:28:48 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 20:28:48 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 20:28:48 - INFO - __main__ - Printing 3 examples
05/17/2022 20:28:48 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 20:28:48 - INFO - __main__ - ['negative']
05/17/2022 20:28:48 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 20:28:48 - INFO - __main__ - ['negative']
05/17/2022 20:28:48 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 20:28:48 - INFO - __main__ - ['negative']
05/17/2022 20:28:48 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:28:48 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:28:49 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 20:28:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:28:50 - INFO - __main__ - Starting training!
05/17/2022 20:28:57 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_42_0.0002_8_predictions.txt
05/17/2022 20:28:57 - INFO - __main__ - Classification-F1 on test data: 0.9209
05/17/2022 20:28:57 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0002, bsz=8, dev_performance=0.9372549019607843, test_performance=0.920903106305224
05/17/2022 20:28:57 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0001, bsz=8 ...
05/17/2022 20:28:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:28:58 - INFO - __main__ - Printing 3 examples
05/17/2022 20:28:58 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/17/2022 20:28:58 - INFO - __main__ - ['negative']
05/17/2022 20:28:58 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/17/2022 20:28:58 - INFO - __main__ - ['negative']
05/17/2022 20:28:58 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/17/2022 20:28:58 - INFO - __main__ - ['negative']
05/17/2022 20:28:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:28:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:28:58 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:28:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:28:58 - INFO - __main__ - Printing 3 examples
05/17/2022 20:28:58 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
05/17/2022 20:28:58 - INFO - __main__ - ['negative']
05/17/2022 20:28:58 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
05/17/2022 20:28:58 - INFO - __main__ - ['negative']
05/17/2022 20:28:58 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
05/17/2022 20:28:58 - INFO - __main__ - ['negative']
05/17/2022 20:28:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:28:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:28:58 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:29:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:29:02 - INFO - __main__ - Starting training!
05/17/2022 20:29:04 - INFO - __main__ - Step 10 Global step 10 Train loss 23.080467 on epoch=4
05/17/2022 20:29:07 - INFO - __main__ - Step 20 Global step 20 Train loss 20.416803 on epoch=9
05/17/2022 20:29:09 - INFO - __main__ - Step 30 Global step 30 Train loss 16.402710 on epoch=14
05/17/2022 20:29:12 - INFO - __main__ - Step 40 Global step 40 Train loss 14.290489 on epoch=19
05/17/2022 20:29:14 - INFO - __main__ - Step 50 Global step 50 Train loss 13.570227 on epoch=24
05/17/2022 20:29:20 - INFO - __main__ - Global step 50 Train loss 17.552139 Classification-F1 0.0 on epoch=24
05/17/2022 20:29:23 - INFO - __main__ - Step 60 Global step 60 Train loss 12.655910 on epoch=29
05/17/2022 20:29:25 - INFO - __main__ - Step 70 Global step 70 Train loss 9.575312 on epoch=34
05/17/2022 20:29:28 - INFO - __main__ - Step 80 Global step 80 Train loss 9.976664 on epoch=39
05/17/2022 20:29:30 - INFO - __main__ - Step 90 Global step 90 Train loss 7.592167 on epoch=44
05/17/2022 20:29:33 - INFO - __main__ - Step 100 Global step 100 Train loss 7.178473 on epoch=49
05/17/2022 20:29:36 - INFO - __main__ - Global step 100 Train loss 9.395705 Classification-F1 0.0 on epoch=49
05/17/2022 20:29:38 - INFO - __main__ - Step 110 Global step 110 Train loss 7.424104 on epoch=54
05/17/2022 20:29:41 - INFO - __main__ - Step 120 Global step 120 Train loss 5.164761 on epoch=59
05/17/2022 20:29:43 - INFO - __main__ - Step 130 Global step 130 Train loss 4.963780 on epoch=64
05/17/2022 20:29:46 - INFO - __main__ - Step 140 Global step 140 Train loss 3.609797 on epoch=69
05/17/2022 20:29:48 - INFO - __main__ - Step 150 Global step 150 Train loss 3.577845 on epoch=74
05/17/2022 20:29:49 - INFO - __main__ - Global step 150 Train loss 4.948057 Classification-F1 0.3333333333333333 on epoch=74
05/17/2022 20:29:52 - INFO - __main__ - Step 160 Global step 160 Train loss 2.537570 on epoch=79
05/17/2022 20:29:54 - INFO - __main__ - Step 170 Global step 170 Train loss 1.930163 on epoch=84
05/17/2022 20:29:57 - INFO - __main__ - Step 180 Global step 180 Train loss 2.298062 on epoch=89
05/17/2022 20:29:59 - INFO - __main__ - Step 190 Global step 190 Train loss 1.418861 on epoch=94
05/17/2022 20:30:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.837788 on epoch=99
05/17/2022 20:30:02 - INFO - __main__ - Global step 200 Train loss 1.804489 Classification-F1 0.7333333333333334 on epoch=99
05/17/2022 20:30:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.669911 on epoch=104
05/17/2022 20:30:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.905622 on epoch=109
05/17/2022 20:30:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.499725 on epoch=114
05/17/2022 20:30:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.461260 on epoch=119
05/17/2022 20:30:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.448899 on epoch=124
05/17/2022 20:30:15 - INFO - __main__ - Global step 250 Train loss 0.597083 Classification-F1 0.7333333333333334 on epoch=124
05/17/2022 20:30:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.612382 on epoch=129
05/17/2022 20:30:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.423664 on epoch=134
05/17/2022 20:30:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.383242 on epoch=139
05/17/2022 20:30:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.417458 on epoch=144
05/17/2022 20:30:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.281632 on epoch=149
05/17/2022 20:30:28 - INFO - __main__ - Global step 300 Train loss 0.423676 Classification-F1 0.805668016194332 on epoch=149
05/17/2022 20:30:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.452146 on epoch=154
05/17/2022 20:30:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.404379 on epoch=159
05/17/2022 20:30:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.226232 on epoch=164
05/17/2022 20:30:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.229576 on epoch=169
05/17/2022 20:30:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.405813 on epoch=174
05/17/2022 20:30:42 - INFO - __main__ - Global step 350 Train loss 0.343629 Classification-F1 0.873015873015873 on epoch=174
05/17/2022 20:30:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.280334 on epoch=179
05/17/2022 20:30:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.279462 on epoch=184
05/17/2022 20:30:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.187734 on epoch=189
05/17/2022 20:30:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.198403 on epoch=194
05/17/2022 20:30:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.353076 on epoch=199
05/17/2022 20:30:55 - INFO - __main__ - Global step 400 Train loss 0.259802 Classification-F1 0.873015873015873 on epoch=199
05/17/2022 20:30:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.149269 on epoch=204
05/17/2022 20:31:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.204869 on epoch=209
05/17/2022 20:31:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.110231 on epoch=214
05/17/2022 20:31:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.125313 on epoch=219
05/17/2022 20:31:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.109720 on epoch=224
05/17/2022 20:31:08 - INFO - __main__ - Global step 450 Train loss 0.139881 Classification-F1 0.9054187192118226 on epoch=224
05/17/2022 20:31:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.175363 on epoch=229
05/17/2022 20:31:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.205845 on epoch=234
05/17/2022 20:31:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.179358 on epoch=239
05/17/2022 20:31:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.135808 on epoch=244
05/17/2022 20:31:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.064266 on epoch=249
05/17/2022 20:31:21 - INFO - __main__ - Global step 500 Train loss 0.152128 Classification-F1 0.9054187192118226 on epoch=249
05/17/2022 20:31:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.076097 on epoch=254
05/17/2022 20:31:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.144290 on epoch=259
05/17/2022 20:31:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.086630 on epoch=264
05/17/2022 20:31:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.177044 on epoch=269
05/17/2022 20:31:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.092625 on epoch=274
05/17/2022 20:31:34 - INFO - __main__ - Global step 550 Train loss 0.115337 Classification-F1 0.9372549019607843 on epoch=274
05/17/2022 20:31:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.090488 on epoch=279
05/17/2022 20:31:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.050974 on epoch=284
05/17/2022 20:31:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.082687 on epoch=289
05/17/2022 20:31:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.072531 on epoch=294
05/17/2022 20:31:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.144913 on epoch=299
05/17/2022 20:31:48 - INFO - __main__ - Global step 600 Train loss 0.088319 Classification-F1 0.873015873015873 on epoch=299
05/17/2022 20:31:48 - INFO - __main__ - save last model!
05/17/2022 20:31:49 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:31:49 - INFO - __main__ - Printing 3 examples
05/17/2022 20:31:49 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/17/2022 20:31:49 - INFO - __main__ - ['negative']
05/17/2022 20:31:49 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/17/2022 20:31:49 - INFO - __main__ - ['negative']
05/17/2022 20:31:49 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/17/2022 20:31:49 - INFO - __main__ - ['negative']
05/17/2022 20:31:49 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:31:49 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:31:49 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:31:49 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:31:49 - INFO - __main__ - Printing 3 examples
05/17/2022 20:31:49 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
05/17/2022 20:31:49 - INFO - __main__ - ['negative']
05/17/2022 20:31:49 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
05/17/2022 20:31:49 - INFO - __main__ - ['negative']
05/17/2022 20:31:49 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
05/17/2022 20:31:49 - INFO - __main__ - ['negative']
05/17/2022 20:31:49 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:31:49 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:31:49 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:31:51 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 20:31:51 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 20:31:51 - INFO - __main__ - Printing 3 examples
05/17/2022 20:31:51 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 20:31:51 - INFO - __main__ - ['negative']
05/17/2022 20:31:51 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 20:31:51 - INFO - __main__ - ['negative']
05/17/2022 20:31:51 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 20:31:51 - INFO - __main__ - ['negative']
05/17/2022 20:31:51 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:31:51 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:31:52 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 20:31:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:31:53 - INFO - __main__ - Starting training!
05/17/2022 20:31:59 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_42_0.0001_8_predictions.txt
05/17/2022 20:31:59 - INFO - __main__ - Classification-F1 on test data: 0.9047
05/17/2022 20:31:59 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0001, bsz=8, dev_performance=0.9372549019607843, test_performance=0.9046681498295568
05/17/2022 20:31:59 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0005, bsz=8 ...
05/17/2022 20:32:00 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:32:00 - INFO - __main__ - Printing 3 examples
05/17/2022 20:32:00 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/17/2022 20:32:00 - INFO - __main__ - ['negative']
05/17/2022 20:32:00 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/17/2022 20:32:00 - INFO - __main__ - ['negative']
05/17/2022 20:32:00 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/17/2022 20:32:00 - INFO - __main__ - ['negative']
05/17/2022 20:32:00 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:32:00 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:32:00 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:32:00 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:32:00 - INFO - __main__ - Printing 3 examples
05/17/2022 20:32:00 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
05/17/2022 20:32:00 - INFO - __main__ - ['negative']
05/17/2022 20:32:00 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
05/17/2022 20:32:00 - INFO - __main__ - ['negative']
05/17/2022 20:32:00 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
05/17/2022 20:32:00 - INFO - __main__ - ['negative']
05/17/2022 20:32:00 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:32:00 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:32:00 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:32:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:32:05 - INFO - __main__ - Starting training!
05/17/2022 20:32:07 - INFO - __main__ - Step 10 Global step 10 Train loss 23.061754 on epoch=4
05/17/2022 20:32:10 - INFO - __main__ - Step 20 Global step 20 Train loss 14.489832 on epoch=9
05/17/2022 20:32:12 - INFO - __main__ - Step 30 Global step 30 Train loss 7.925420 on epoch=14
05/17/2022 20:32:15 - INFO - __main__ - Step 40 Global step 40 Train loss 3.252630 on epoch=19
05/17/2022 20:32:17 - INFO - __main__ - Step 50 Global step 50 Train loss 3.232647 on epoch=24
05/17/2022 20:32:17 - INFO - __main__ - Global step 50 Train loss 10.392457 Classification-F1 0.6536796536796536 on epoch=24
05/17/2022 20:32:20 - INFO - __main__ - Step 60 Global step 60 Train loss 1.152914 on epoch=29
05/17/2022 20:32:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.579387 on epoch=34
05/17/2022 20:32:25 - INFO - __main__ - Step 80 Global step 80 Train loss 0.542897 on epoch=39
05/17/2022 20:32:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.443260 on epoch=44
05/17/2022 20:32:30 - INFO - __main__ - Step 100 Global step 100 Train loss 0.463908 on epoch=49
05/17/2022 20:32:30 - INFO - __main__ - Global step 100 Train loss 0.636473 Classification-F1 0.9687194525904204 on epoch=49
05/17/2022 20:32:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.368172 on epoch=54
05/17/2022 20:32:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.252658 on epoch=59
05/17/2022 20:32:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.228909 on epoch=64
05/17/2022 20:32:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.153645 on epoch=69
05/17/2022 20:32:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.074982 on epoch=74
05/17/2022 20:32:44 - INFO - __main__ - Global step 150 Train loss 0.215673 Classification-F1 0.9375 on epoch=74
05/17/2022 20:32:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.081963 on epoch=79
05/17/2022 20:32:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.036408 on epoch=84
05/17/2022 20:32:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.117529 on epoch=89
05/17/2022 20:32:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.022578 on epoch=94
05/17/2022 20:32:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.404472 on epoch=99
05/17/2022 20:32:57 - INFO - __main__ - Global step 200 Train loss 0.132590 Classification-F1 0.8745098039215686 on epoch=99
05/17/2022 20:32:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.058105 on epoch=104
05/17/2022 20:33:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.013744 on epoch=109
05/17/2022 20:33:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.019843 on epoch=114
05/17/2022 20:33:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.003990 on epoch=119
05/17/2022 20:33:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.056236 on epoch=124
05/17/2022 20:33:09 - INFO - __main__ - Global step 250 Train loss 0.030384 Classification-F1 0.873015873015873 on epoch=124
05/17/2022 20:33:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.063865 on epoch=129
05/17/2022 20:33:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.020403 on epoch=134
05/17/2022 20:33:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.001314 on epoch=139
05/17/2022 20:33:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.027714 on epoch=144
05/17/2022 20:33:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.178012 on epoch=149
05/17/2022 20:33:22 - INFO - __main__ - Global step 300 Train loss 0.058262 Classification-F1 0.875 on epoch=149
05/17/2022 20:33:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.015233 on epoch=154
05/17/2022 20:33:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.008720 on epoch=159
05/17/2022 20:33:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.022905 on epoch=164
05/17/2022 20:33:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.005772 on epoch=169
05/17/2022 20:33:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.029042 on epoch=174
05/17/2022 20:33:35 - INFO - __main__ - Global step 350 Train loss 0.016335 Classification-F1 0.873015873015873 on epoch=174
05/17/2022 20:33:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.027106 on epoch=179
05/17/2022 20:33:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001169 on epoch=184
05/17/2022 20:33:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.052844 on epoch=189
05/17/2022 20:33:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.431270 on epoch=194
05/17/2022 20:33:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.218317 on epoch=199
05/17/2022 20:33:48 - INFO - __main__ - Global step 400 Train loss 0.146141 Classification-F1 0.873015873015873 on epoch=199
05/17/2022 20:33:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.227784 on epoch=204
05/17/2022 20:33:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.097021 on epoch=209
05/17/2022 20:33:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.348648 on epoch=214
05/17/2022 20:33:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.324151 on epoch=219
05/17/2022 20:34:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.395312 on epoch=224
05/17/2022 20:34:00 - INFO - __main__ - Global step 450 Train loss 0.278583 Classification-F1 0.746031746031746 on epoch=224
05/17/2022 20:34:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.448788 on epoch=229
05/17/2022 20:34:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.380909 on epoch=234
05/17/2022 20:34:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.399365 on epoch=239
05/17/2022 20:34:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.308831 on epoch=244
05/17/2022 20:34:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.244376 on epoch=249
05/17/2022 20:34:13 - INFO - __main__ - Global step 500 Train loss 0.356454 Classification-F1 0.7117117117117117 on epoch=249
05/17/2022 20:34:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.309813 on epoch=254
05/17/2022 20:34:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.228840 on epoch=259
05/17/2022 20:34:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.207017 on epoch=264
05/17/2022 20:34:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.229130 on epoch=269
05/17/2022 20:34:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.252780 on epoch=274
05/17/2022 20:34:26 - INFO - __main__ - Global step 550 Train loss 0.245516 Classification-F1 0.8745098039215686 on epoch=274
05/17/2022 20:34:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.348186 on epoch=279
05/17/2022 20:34:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.199025 on epoch=284
05/17/2022 20:34:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.324809 on epoch=289
05/17/2022 20:34:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.197676 on epoch=294
05/17/2022 20:34:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.215486 on epoch=299
05/17/2022 20:34:38 - INFO - __main__ - Global step 600 Train loss 0.257036 Classification-F1 0.7046153846153846 on epoch=299
05/17/2022 20:34:38 - INFO - __main__ - save last model!
05/17/2022 20:34:39 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:34:39 - INFO - __main__ - Printing 3 examples
05/17/2022 20:34:39 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/17/2022 20:34:39 - INFO - __main__ - ['negative']
05/17/2022 20:34:39 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/17/2022 20:34:39 - INFO - __main__ - ['negative']
05/17/2022 20:34:39 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/17/2022 20:34:39 - INFO - __main__ - ['negative']
05/17/2022 20:34:39 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:34:39 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:34:39 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:34:39 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:34:39 - INFO - __main__ - Printing 3 examples
05/17/2022 20:34:39 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
05/17/2022 20:34:39 - INFO - __main__ - ['negative']
05/17/2022 20:34:39 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
05/17/2022 20:34:39 - INFO - __main__ - ['negative']
05/17/2022 20:34:39 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
05/17/2022 20:34:39 - INFO - __main__ - ['negative']
05/17/2022 20:34:39 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:34:39 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:34:39 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:34:42 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 20:34:42 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 20:34:42 - INFO - __main__ - Printing 3 examples
05/17/2022 20:34:42 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 20:34:42 - INFO - __main__ - ['negative']
05/17/2022 20:34:42 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 20:34:42 - INFO - __main__ - ['negative']
05/17/2022 20:34:42 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 20:34:42 - INFO - __main__ - ['negative']
05/17/2022 20:34:42 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:34:42 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:34:43 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 20:34:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:34:44 - INFO - __main__ - Starting training!
05/17/2022 20:34:51 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_87_0.0005_8_predictions.txt
05/17/2022 20:34:51 - INFO - __main__ - Classification-F1 on test data: 0.8495
05/17/2022 20:34:51 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0005, bsz=8, dev_performance=0.9687194525904204, test_performance=0.8495347777994664
05/17/2022 20:34:51 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0003, bsz=8 ...
05/17/2022 20:34:52 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:34:52 - INFO - __main__ - Printing 3 examples
05/17/2022 20:34:52 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/17/2022 20:34:52 - INFO - __main__ - ['negative']
05/17/2022 20:34:52 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/17/2022 20:34:52 - INFO - __main__ - ['negative']
05/17/2022 20:34:52 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/17/2022 20:34:52 - INFO - __main__ - ['negative']
05/17/2022 20:34:52 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:34:52 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:34:52 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:34:52 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:34:52 - INFO - __main__ - Printing 3 examples
05/17/2022 20:34:52 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
05/17/2022 20:34:52 - INFO - __main__ - ['negative']
05/17/2022 20:34:52 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
05/17/2022 20:34:52 - INFO - __main__ - ['negative']
05/17/2022 20:34:52 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
05/17/2022 20:34:52 - INFO - __main__ - ['negative']
05/17/2022 20:34:52 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:34:52 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:34:52 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:34:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:34:56 - INFO - __main__ - Starting training!
05/17/2022 20:34:58 - INFO - __main__ - Step 10 Global step 10 Train loss 24.265236 on epoch=4
05/17/2022 20:35:00 - INFO - __main__ - Step 20 Global step 20 Train loss 16.066587 on epoch=9
05/17/2022 20:35:02 - INFO - __main__ - Step 30 Global step 30 Train loss 12.182312 on epoch=14
05/17/2022 20:35:05 - INFO - __main__ - Step 40 Global step 40 Train loss 6.715188 on epoch=19
05/17/2022 20:35:08 - INFO - __main__ - Step 50 Global step 50 Train loss 5.518987 on epoch=24
05/17/2022 20:35:08 - INFO - __main__ - Global step 50 Train loss 12.949662 Classification-F1 0.3333333333333333 on epoch=24
05/17/2022 20:35:11 - INFO - __main__ - Step 60 Global step 60 Train loss 4.087318 on epoch=29
05/17/2022 20:35:13 - INFO - __main__ - Step 70 Global step 70 Train loss 1.455246 on epoch=34
05/17/2022 20:35:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.915719 on epoch=39
05/17/2022 20:35:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.790845 on epoch=44
05/17/2022 20:35:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.680356 on epoch=49
05/17/2022 20:35:21 - INFO - __main__ - Global step 100 Train loss 1.585897 Classification-F1 0.3333333333333333 on epoch=49
05/17/2022 20:35:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.489878 on epoch=54
05/17/2022 20:35:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.721565 on epoch=59
05/17/2022 20:35:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.582968 on epoch=64
05/17/2022 20:35:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.557389 on epoch=69
05/17/2022 20:35:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.402826 on epoch=74
05/17/2022 20:35:34 - INFO - __main__ - Global step 150 Train loss 0.550925 Classification-F1 0.7757757757757757 on epoch=74
05/17/2022 20:35:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.423569 on epoch=79
05/17/2022 20:35:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.503054 on epoch=84
05/17/2022 20:35:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.377649 on epoch=89
05/17/2022 20:35:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.465672 on epoch=94
05/17/2022 20:35:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.625819 on epoch=99
05/17/2022 20:35:47 - INFO - __main__ - Global step 200 Train loss 0.479153 Classification-F1 0.9372549019607843 on epoch=99
05/17/2022 20:35:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.541317 on epoch=104
05/17/2022 20:35:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.357812 on epoch=109
05/17/2022 20:35:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.357524 on epoch=114
05/17/2022 20:35:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.342800 on epoch=119
05/17/2022 20:36:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.328275 on epoch=124
05/17/2022 20:36:01 - INFO - __main__ - Global step 250 Train loss 0.385546 Classification-F1 0.906158357771261 on epoch=124
05/17/2022 20:36:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.317272 on epoch=129
05/17/2022 20:36:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.247968 on epoch=134
05/17/2022 20:36:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.228915 on epoch=139
05/17/2022 20:36:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.296713 on epoch=144
05/17/2022 20:36:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.452892 on epoch=149
05/17/2022 20:36:14 - INFO - __main__ - Global step 300 Train loss 0.308752 Classification-F1 0.7702564102564102 on epoch=149
05/17/2022 20:36:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.323392 on epoch=154
05/17/2022 20:36:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.276614 on epoch=159
05/17/2022 20:36:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.279414 on epoch=164
05/17/2022 20:36:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.355095 on epoch=169
05/17/2022 20:36:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.252283 on epoch=174
05/17/2022 20:36:27 - INFO - __main__ - Global step 350 Train loss 0.297359 Classification-F1 0.873015873015873 on epoch=174
05/17/2022 20:36:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.324579 on epoch=179
05/17/2022 20:36:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.202201 on epoch=184
05/17/2022 20:36:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.210660 on epoch=189
05/17/2022 20:36:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.215593 on epoch=194
05/17/2022 20:36:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.198482 on epoch=199
05/17/2022 20:36:40 - INFO - __main__ - Global step 400 Train loss 0.230303 Classification-F1 0.9372549019607843 on epoch=199
05/17/2022 20:36:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.246374 on epoch=204
05/17/2022 20:36:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.253130 on epoch=209
05/17/2022 20:36:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.181373 on epoch=214
05/17/2022 20:36:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.201669 on epoch=219
05/17/2022 20:36:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.171511 on epoch=224
05/17/2022 20:36:53 - INFO - __main__ - Global step 450 Train loss 0.210811 Classification-F1 0.906158357771261 on epoch=224
05/17/2022 20:36:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.269194 on epoch=229
05/17/2022 20:36:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.269131 on epoch=234
05/17/2022 20:37:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.188618 on epoch=239
05/17/2022 20:37:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.171482 on epoch=244
05/17/2022 20:37:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.196272 on epoch=249
05/17/2022 20:37:06 - INFO - __main__ - Global step 500 Train loss 0.218939 Classification-F1 0.9687194525904204 on epoch=249
05/17/2022 20:37:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.174838 on epoch=254
05/17/2022 20:37:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.181541 on epoch=259
05/17/2022 20:37:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.278797 on epoch=264
05/17/2022 20:37:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.151622 on epoch=269
05/17/2022 20:37:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.282589 on epoch=274
05/17/2022 20:37:19 - INFO - __main__ - Global step 550 Train loss 0.213878 Classification-F1 0.8398398398398398 on epoch=274
05/17/2022 20:37:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.298677 on epoch=279
05/17/2022 20:37:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.384931 on epoch=284
05/17/2022 20:37:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.326154 on epoch=289
05/17/2022 20:37:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.297380 on epoch=294
05/17/2022 20:37:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.232764 on epoch=299
05/17/2022 20:37:33 - INFO - __main__ - Global step 600 Train loss 0.307981 Classification-F1 0.8423645320197044 on epoch=299
05/17/2022 20:37:33 - INFO - __main__ - save last model!
05/17/2022 20:37:34 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:37:34 - INFO - __main__ - Printing 3 examples
05/17/2022 20:37:34 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/17/2022 20:37:34 - INFO - __main__ - ['negative']
05/17/2022 20:37:34 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/17/2022 20:37:34 - INFO - __main__ - ['negative']
05/17/2022 20:37:34 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/17/2022 20:37:34 - INFO - __main__ - ['negative']
05/17/2022 20:37:34 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:37:34 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:37:34 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:37:34 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:37:34 - INFO - __main__ - Printing 3 examples
05/17/2022 20:37:34 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
05/17/2022 20:37:34 - INFO - __main__ - ['negative']
05/17/2022 20:37:34 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
05/17/2022 20:37:34 - INFO - __main__ - ['negative']
05/17/2022 20:37:34 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
05/17/2022 20:37:34 - INFO - __main__ - ['negative']
05/17/2022 20:37:34 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:37:34 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:37:34 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:37:36 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 20:37:36 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 20:37:36 - INFO - __main__ - Printing 3 examples
05/17/2022 20:37:36 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 20:37:36 - INFO - __main__ - ['negative']
05/17/2022 20:37:36 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 20:37:36 - INFO - __main__ - ['negative']
05/17/2022 20:37:36 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 20:37:36 - INFO - __main__ - ['negative']
05/17/2022 20:37:36 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:37:36 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:37:37 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 20:37:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:37:38 - INFO - __main__ - Starting training!
05/17/2022 20:37:44 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_87_0.0003_8_predictions.txt
05/17/2022 20:37:44 - INFO - __main__ - Classification-F1 on test data: 0.9019
05/17/2022 20:37:44 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0003, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9019117205484937
05/17/2022 20:37:44 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0002, bsz=8 ...
05/17/2022 20:37:45 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:37:45 - INFO - __main__ - Printing 3 examples
05/17/2022 20:37:45 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/17/2022 20:37:45 - INFO - __main__ - ['negative']
05/17/2022 20:37:45 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/17/2022 20:37:45 - INFO - __main__ - ['negative']
05/17/2022 20:37:45 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/17/2022 20:37:45 - INFO - __main__ - ['negative']
05/17/2022 20:37:45 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:37:45 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:37:45 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:37:45 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:37:45 - INFO - __main__ - Printing 3 examples
05/17/2022 20:37:45 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
05/17/2022 20:37:45 - INFO - __main__ - ['negative']
05/17/2022 20:37:45 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
05/17/2022 20:37:45 - INFO - __main__ - ['negative']
05/17/2022 20:37:45 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
05/17/2022 20:37:45 - INFO - __main__ - ['negative']
05/17/2022 20:37:45 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:37:45 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:37:45 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:37:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:37:50 - INFO - __main__ - Starting training!
05/17/2022 20:37:52 - INFO - __main__ - Step 10 Global step 10 Train loss 23.493984 on epoch=4
05/17/2022 20:37:55 - INFO - __main__ - Step 20 Global step 20 Train loss 17.753107 on epoch=9
05/17/2022 20:37:57 - INFO - __main__ - Step 30 Global step 30 Train loss 13.691164 on epoch=14
05/17/2022 20:38:00 - INFO - __main__ - Step 40 Global step 40 Train loss 10.818569 on epoch=19
05/17/2022 20:38:02 - INFO - __main__ - Step 50 Global step 50 Train loss 10.817790 on epoch=24
05/17/2022 20:38:05 - INFO - __main__ - Global step 50 Train loss 15.314923 Classification-F1 0.0 on epoch=24
05/17/2022 20:38:08 - INFO - __main__ - Step 60 Global step 60 Train loss 7.768671 on epoch=29
05/17/2022 20:38:11 - INFO - __main__ - Step 70 Global step 70 Train loss 5.403620 on epoch=34
05/17/2022 20:38:13 - INFO - __main__ - Step 80 Global step 80 Train loss 4.133457 on epoch=39
05/17/2022 20:38:16 - INFO - __main__ - Step 90 Global step 90 Train loss 3.429869 on epoch=44
05/17/2022 20:38:18 - INFO - __main__ - Step 100 Global step 100 Train loss 2.106859 on epoch=49
05/17/2022 20:38:18 - INFO - __main__ - Global step 100 Train loss 4.568495 Classification-F1 0.3333333333333333 on epoch=49
05/17/2022 20:38:21 - INFO - __main__ - Step 110 Global step 110 Train loss 1.908298 on epoch=54
05/17/2022 20:38:24 - INFO - __main__ - Step 120 Global step 120 Train loss 1.319038 on epoch=59
05/17/2022 20:38:26 - INFO - __main__ - Step 130 Global step 130 Train loss 1.125858 on epoch=64
05/17/2022 20:38:29 - INFO - __main__ - Step 140 Global step 140 Train loss 0.587974 on epoch=69
05/17/2022 20:38:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.476374 on epoch=74
05/17/2022 20:38:32 - INFO - __main__ - Global step 150 Train loss 1.083508 Classification-F1 0.9375 on epoch=74
05/17/2022 20:38:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.638073 on epoch=79
05/17/2022 20:38:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.521095 on epoch=84
05/17/2022 20:38:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.649789 on epoch=89
05/17/2022 20:38:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.494604 on epoch=94
05/17/2022 20:38:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.320007 on epoch=99
05/17/2022 20:38:45 - INFO - __main__ - Global step 200 Train loss 0.524714 Classification-F1 0.9687194525904204 on epoch=99
05/17/2022 20:38:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.428039 on epoch=104
05/17/2022 20:38:50 - INFO - __main__ - Step 220 Global step 220 Train loss 0.505182 on epoch=109
05/17/2022 20:38:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.367237 on epoch=114
05/17/2022 20:38:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.422137 on epoch=119
05/17/2022 20:38:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.397062 on epoch=124
05/17/2022 20:38:58 - INFO - __main__ - Global step 250 Train loss 0.423931 Classification-F1 0.9687194525904204 on epoch=124
05/17/2022 20:39:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.401893 on epoch=129
05/17/2022 20:39:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.405687 on epoch=134
05/17/2022 20:39:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.407415 on epoch=139
05/17/2022 20:39:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.463265 on epoch=144
05/17/2022 20:39:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.401320 on epoch=149
05/17/2022 20:39:11 - INFO - __main__ - Global step 300 Train loss 0.415916 Classification-F1 0.9687194525904204 on epoch=149
05/17/2022 20:39:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.412036 on epoch=154
05/17/2022 20:39:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.350384 on epoch=159
05/17/2022 20:39:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.436394 on epoch=164
05/17/2022 20:39:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.369628 on epoch=169
05/17/2022 20:39:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.319311 on epoch=174
05/17/2022 20:39:24 - INFO - __main__ - Global step 350 Train loss 0.377551 Classification-F1 0.6945917285259808 on epoch=174
05/17/2022 20:39:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.378601 on epoch=179
05/17/2022 20:39:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.368813 on epoch=184
05/17/2022 20:39:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.366166 on epoch=189
05/17/2022 20:39:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.387193 on epoch=194
05/17/2022 20:39:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.385125 on epoch=199
05/17/2022 20:39:37 - INFO - __main__ - Global step 400 Train loss 0.377180 Classification-F1 0.6666666666666667 on epoch=199
05/17/2022 20:39:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.499458 on epoch=204
05/17/2022 20:39:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.332117 on epoch=209
05/17/2022 20:39:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.294222 on epoch=214
05/17/2022 20:39:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.361155 on epoch=219
05/17/2022 20:39:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.397884 on epoch=224
05/17/2022 20:39:50 - INFO - __main__ - Global step 450 Train loss 0.376967 Classification-F1 0.6101882613510521 on epoch=224
05/17/2022 20:39:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.410866 on epoch=229
05/17/2022 20:39:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.328186 on epoch=234
05/17/2022 20:39:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.315481 on epoch=239
05/17/2022 20:40:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.316134 on epoch=244
05/17/2022 20:40:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.291130 on epoch=249
05/17/2022 20:40:03 - INFO - __main__ - Global step 500 Train loss 0.332359 Classification-F1 0.49090909090909085 on epoch=249
05/17/2022 20:40:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.401702 on epoch=254
05/17/2022 20:40:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.345226 on epoch=259
05/17/2022 20:40:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.324107 on epoch=264
05/17/2022 20:40:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.390179 on epoch=269
05/17/2022 20:40:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.394669 on epoch=274
05/17/2022 20:40:16 - INFO - __main__ - Global step 550 Train loss 0.371177 Classification-F1 0.6945917285259808 on epoch=274
05/17/2022 20:40:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.386905 on epoch=279
05/17/2022 20:40:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.269168 on epoch=284
05/17/2022 20:40:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.388177 on epoch=289
05/17/2022 20:40:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.302225 on epoch=294
05/17/2022 20:40:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.332425 on epoch=299
05/17/2022 20:40:29 - INFO - __main__ - Global step 600 Train loss 0.335780 Classification-F1 0.6536796536796536 on epoch=299
05/17/2022 20:40:29 - INFO - __main__ - save last model!
05/17/2022 20:40:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:40:30 - INFO - __main__ - Printing 3 examples
05/17/2022 20:40:30 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/17/2022 20:40:30 - INFO - __main__ - ['negative']
05/17/2022 20:40:30 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/17/2022 20:40:30 - INFO - __main__ - ['negative']
05/17/2022 20:40:30 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/17/2022 20:40:30 - INFO - __main__ - ['negative']
05/17/2022 20:40:30 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:40:30 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:40:30 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:40:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:40:30 - INFO - __main__ - Printing 3 examples
05/17/2022 20:40:30 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
05/17/2022 20:40:30 - INFO - __main__ - ['negative']
05/17/2022 20:40:30 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
05/17/2022 20:40:30 - INFO - __main__ - ['negative']
05/17/2022 20:40:30 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
05/17/2022 20:40:30 - INFO - __main__ - ['negative']
05/17/2022 20:40:30 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:40:30 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:40:30 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:40:32 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 20:40:32 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 20:40:32 - INFO - __main__ - Printing 3 examples
05/17/2022 20:40:32 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 20:40:32 - INFO - __main__ - ['negative']
05/17/2022 20:40:32 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 20:40:32 - INFO - __main__ - ['negative']
05/17/2022 20:40:32 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 20:40:32 - INFO - __main__ - ['negative']
05/17/2022 20:40:32 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:40:33 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:40:34 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 20:40:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:40:34 - INFO - __main__ - Starting training!
05/17/2022 20:40:41 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_87_0.0002_8_predictions.txt
05/17/2022 20:40:41 - INFO - __main__ - Classification-F1 on test data: 0.9209
05/17/2022 20:40:41 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0002, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9208917007383108
05/17/2022 20:40:41 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0001, bsz=8 ...
05/17/2022 20:40:42 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:40:42 - INFO - __main__ - Printing 3 examples
05/17/2022 20:40:42 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/17/2022 20:40:42 - INFO - __main__ - ['negative']
05/17/2022 20:40:42 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/17/2022 20:40:42 - INFO - __main__ - ['negative']
05/17/2022 20:40:42 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/17/2022 20:40:42 - INFO - __main__ - ['negative']
05/17/2022 20:40:42 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:40:42 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:40:42 - INFO - __main__ - Loaded 32 examples from train data
05/17/2022 20:40:42 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 20:40:42 - INFO - __main__ - Printing 3 examples
05/17/2022 20:40:42 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
05/17/2022 20:40:42 - INFO - __main__ - ['negative']
05/17/2022 20:40:42 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
05/17/2022 20:40:42 - INFO - __main__ - ['negative']
05/17/2022 20:40:42 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
05/17/2022 20:40:42 - INFO - __main__ - ['negative']
05/17/2022 20:40:42 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:40:42 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:40:42 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 20:40:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 20:40:46 - INFO - __main__ - Starting training!
05/17/2022 20:40:49 - INFO - __main__ - Step 10 Global step 10 Train loss 23.667150 on epoch=4
05/17/2022 20:40:51 - INFO - __main__ - Step 20 Global step 20 Train loss 19.599203 on epoch=9
05/17/2022 20:40:54 - INFO - __main__ - Step 30 Global step 30 Train loss 16.860918 on epoch=14
05/17/2022 20:40:56 - INFO - __main__ - Step 40 Global step 40 Train loss 15.938431 on epoch=19
05/17/2022 20:40:59 - INFO - __main__ - Step 50 Global step 50 Train loss 13.883496 on epoch=24
05/17/2022 20:41:04 - INFO - __main__ - Global step 50 Train loss 17.989840 Classification-F1 0.0 on epoch=24
05/17/2022 20:41:06 - INFO - __main__ - Step 60 Global step 60 Train loss 12.425438 on epoch=29
05/17/2022 20:41:09 - INFO - __main__ - Step 70 Global step 70 Train loss 11.196416 on epoch=34
05/17/2022 20:41:12 - INFO - __main__ - Step 80 Global step 80 Train loss 10.035264 on epoch=39
05/17/2022 20:41:14 - INFO - __main__ - Step 90 Global step 90 Train loss 9.204966 on epoch=44
05/17/2022 20:41:17 - INFO - __main__ - Step 100 Global step 100 Train loss 8.181601 on epoch=49
05/17/2022 20:41:20 - INFO - __main__ - Global step 100 Train loss 10.208736 Classification-F1 0.0 on epoch=49
05/17/2022 20:41:22 - INFO - __main__ - Step 110 Global step 110 Train loss 7.029871 on epoch=54
05/17/2022 20:41:25 - INFO - __main__ - Step 120 Global step 120 Train loss 5.382645 on epoch=59
05/17/2022 20:41:28 - INFO - __main__ - Step 130 Global step 130 Train loss 5.524995 on epoch=64
05/17/2022 20:41:30 - INFO - __main__ - Step 140 Global step 140 Train loss 4.735152 on epoch=69
05/17/2022 20:41:33 - INFO - __main__ - Step 150 Global step 150 Train loss 4.210674 on epoch=74
05/17/2022 20:41:35 - INFO - __main__ - Global step 150 Train loss 5.376667 Classification-F1 0.3333333333333333 on epoch=74
05/17/2022 20:41:38 - INFO - __main__ - Step 160 Global step 160 Train loss 2.504831 on epoch=79
05/17/2022 20:41:40 - INFO - __main__ - Step 170 Global step 170 Train loss 2.895579 on epoch=84
05/17/2022 20:41:43 - INFO - __main__ - Step 180 Global step 180 Train loss 2.778458 on epoch=89
05/17/2022 20:41:46 - INFO - __main__ - Step 190 Global step 190 Train loss 2.861066 on epoch=94
05/17/2022 20:41:49 - INFO - __main__ - Step 200 Global step 200 Train loss 1.396843 on epoch=99
05/17/2022 20:41:49 - INFO - __main__ - Global step 200 Train loss 2.487355 Classification-F1 0.5134502923976608 on epoch=99
05/17/2022 20:41:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.947088 on epoch=104
05/17/2022 20:41:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.813535 on epoch=109
05/17/2022 20:41:57 - INFO - __main__ - Step 230 Global step 230 Train loss 1.051827 on epoch=114
05/17/2022 20:42:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.621806 on epoch=119
05/17/2022 20:42:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.648535 on epoch=124
05/17/2022 20:42:03 - INFO - __main__ - Global step 250 Train loss 0.816558 Classification-F1 0.9375 on epoch=124
05/17/2022 20:42:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.808774 on epoch=129
05/17/2022 20:42:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.336840 on epoch=134
05/17/2022 20:42:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.500938 on epoch=139
05/17/2022 20:42:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.394763 on epoch=144
05/17/2022 20:42:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.445213 on epoch=149
05/17/2022 20:42:17 - INFO - __main__ - Global step 300 Train loss 0.497305 Classification-F1 0.9375 on epoch=149
05/17/2022 20:42:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.424159 on epoch=154
05/17/2022 20:42:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.477079 on epoch=159
05/17/2022 20:42:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.406290 on epoch=164
05/17/2022 20:42:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.370924 on epoch=169
05/17/2022 20:42:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.399897 on epoch=174
05/17/2022 20:42:30 - INFO - __main__ - Global step 350 Train loss 0.415670 Classification-F1 0.9687194525904204 on epoch=174
05/17/2022 20:42:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.214115 on epoch=179
05/17/2022 20:42:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.239049 on epoch=184
05/17/2022 20:42:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.293843 on epoch=189
05/17/2022 20:42:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.376120 on epoch=194
05/17/2022 20:42:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.286601 on epoch=199
05/17/2022 20:42:43 - INFO - __main__ - Global step 400 Train loss 0.281946 Classification-F1 0.9687194525904204 on epoch=199
05/17/2022 20:42:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.457530 on epoch=204
05/17/2022 20:42:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.173709 on epoch=209
05/17/2022 20:42:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.321775 on epoch=214
05/17/2022 20:42:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.258768 on epoch=219
05/17/2022 20:42:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.166819 on epoch=224
05/17/2022 20:42:56 - INFO - __main__ - Global step 450 Train loss 0.275720 Classification-F1 0.9687194525904204 on epoch=224
05/17/2022 20:42:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.121410 on epoch=229
05/17/2022 20:43:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.136939 on epoch=234
05/17/2022 20:43:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.070564 on epoch=239
05/17/2022 20:43:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.095771 on epoch=244
05/17/2022 20:43:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.162396 on epoch=249
05/17/2022 20:43:09 - INFO - __main__ - Global step 500 Train loss 0.117416 Classification-F1 0.9687194525904204 on epoch=249
05/17/2022 20:43:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.133162 on epoch=254
05/17/2022 20:43:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.102892 on epoch=259
05/17/2022 20:43:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.068386 on epoch=264
05/17/2022 20:43:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.019918 on epoch=269
05/17/2022 20:43:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.065454 on epoch=274
05/17/2022 20:43:22 - INFO - __main__ - Global step 550 Train loss 0.077962 Classification-F1 0.9687194525904204 on epoch=274
05/17/2022 20:43:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.040474 on epoch=279
05/17/2022 20:43:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.017678 on epoch=284
05/17/2022 20:43:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.021308 on epoch=289
05/17/2022 20:43:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.016115 on epoch=294
05/17/2022 20:43:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.043654 on epoch=299
05/17/2022 20:43:34 - INFO - __main__ - Global step 600 Train loss 0.027846 Classification-F1 0.9687194525904204 on epoch=299
05/17/2022 20:43:34 - INFO - __main__ - save last model!
05/17/2022 20:43:37 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 20:43:38 - INFO - __main__ - Start tokenizing ... 1000 instances
05/17/2022 20:43:38 - INFO - __main__ - Printing 3 examples
05/17/2022 20:43:38 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/17/2022 20:43:38 - INFO - __main__ - ['negative']
05/17/2022 20:43:38 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/17/2022 20:43:38 - INFO - __main__ - ['negative']
05/17/2022 20:43:38 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/17/2022 20:43:38 - INFO - __main__ - ['negative']
05/17/2022 20:43:38 - INFO - __main__ - Tokenizing Input ...
05/17/2022 20:43:38 - INFO - __main__ - Tokenizing Output ...
05/17/2022 20:43:39 - INFO - __main__ - Loaded 1000 examples from test data
05/17/2022 20:43:46 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_87_0.0001_8_predictions.txt
05/17/2022 20:43:46 - INFO - __main__ - Classification-F1 on test data: 0.9299
05/17/2022 20:43:46 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0001, bsz=8, dev_performance=0.9687194525904204, test_performance=0.929936943248924
05/21/2022 17:03:13 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-base-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-cls2cls/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-base', cuda='2,3')
05/21/2022 17:03:13 - INFO - __main__ - models/T5-base-ft-cls2cls/singletask-amazon_polarity
05/21/2022 17:03:13 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-base-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-cls2cls/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-base', cuda='2,3')
05/21/2022 17:03:13 - INFO - __main__ - models/T5-base-ft-cls2cls/singletask-amazon_polarity
05/21/2022 17:03:15 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 17:03:15 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 17:03:15 - INFO - __main__ - args.device: cuda:0
05/21/2022 17:03:15 - INFO - __main__ - Using 2 gpus
05/21/2022 17:03:15 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
05/21/2022 17:03:15 - INFO - __main__ - args.device: cuda:1
05/21/2022 17:03:15 - INFO - __main__ - Using 2 gpus
05/21/2022 17:03:15 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
05/21/2022 17:03:19 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0005, bsz=8 ...
05/21/2022 17:03:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:03:20 - INFO - __main__ - Printing 3 examples
05/21/2022 17:03:20 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/21/2022 17:03:20 - INFO - __main__ - ['positive']
05/21/2022 17:03:20 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/21/2022 17:03:20 - INFO - __main__ - ['positive']
05/21/2022 17:03:20 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/21/2022 17:03:20 - INFO - __main__ - ['positive']
05/21/2022 17:03:20 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:03:20 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:03:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:03:20 - INFO - __main__ - Printing 3 examples
05/21/2022 17:03:20 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/21/2022 17:03:20 - INFO - __main__ - ['positive']
05/21/2022 17:03:20 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/21/2022 17:03:20 - INFO - __main__ - ['positive']
05/21/2022 17:03:20 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/21/2022 17:03:20 - INFO - __main__ - ['positive']
05/21/2022 17:03:20 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:03:20 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:03:20 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:03:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:03:20 - INFO - __main__ - Printing 3 examples
05/21/2022 17:03:20 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
05/21/2022 17:03:20 - INFO - __main__ - ['positive']
05/21/2022 17:03:20 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
05/21/2022 17:03:20 - INFO - __main__ - ['positive']
05/21/2022 17:03:20 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
05/21/2022 17:03:20 - INFO - __main__ - ['positive']
05/21/2022 17:03:20 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:03:20 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:03:20 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:03:20 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:03:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:03:20 - INFO - __main__ - Printing 3 examples
05/21/2022 17:03:20 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
05/21/2022 17:03:20 - INFO - __main__ - ['positive']
05/21/2022 17:03:20 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
05/21/2022 17:03:20 - INFO - __main__ - ['positive']
05/21/2022 17:03:20 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
05/21/2022 17:03:20 - INFO - __main__ - ['positive']
05/21/2022 17:03:20 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:03:20 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:03:20 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:03:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:03:25 - INFO - __main__ - Starting training!
05/21/2022 17:03:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:03:26 - INFO - __main__ - Starting training!
05/21/2022 17:03:29 - INFO - __main__ - Step 10 Global step 10 Train loss 23.661139 on epoch=4
05/21/2022 17:03:31 - INFO - __main__ - Step 20 Global step 20 Train loss 12.957950 on epoch=9
05/21/2022 17:03:33 - INFO - __main__ - Step 30 Global step 30 Train loss 6.977796 on epoch=14
05/21/2022 17:03:36 - INFO - __main__ - Step 40 Global step 40 Train loss 5.750542 on epoch=19
05/21/2022 17:03:38 - INFO - __main__ - Step 50 Global step 50 Train loss 2.891956 on epoch=24
05/21/2022 17:03:39 - INFO - __main__ - Global step 50 Train loss 10.447877 Classification-F1 0.3333333333333333 on epoch=24
05/21/2022 17:03:41 - INFO - __main__ - Step 60 Global step 60 Train loss 1.156420 on epoch=29
05/21/2022 17:03:44 - INFO - __main__ - Step 70 Global step 70 Train loss 0.757889 on epoch=34
05/21/2022 17:03:47 - INFO - __main__ - Step 80 Global step 80 Train loss 1.111740 on epoch=39
05/21/2022 17:03:49 - INFO - __main__ - Step 90 Global step 90 Train loss 1.792735 on epoch=44
05/21/2022 17:03:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.834326 on epoch=49
05/21/2022 17:03:52 - INFO - __main__ - Global step 100 Train loss 1.130622 Classification-F1 0.5835835835835835 on epoch=49
05/21/2022 17:03:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.669656 on epoch=54
05/21/2022 17:03:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.590878 on epoch=59
05/21/2022 17:04:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.419090 on epoch=64
05/21/2022 17:04:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.433872 on epoch=69
05/21/2022 17:04:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.563244 on epoch=74
05/21/2022 17:04:06 - INFO - __main__ - Global step 150 Train loss 0.535348 Classification-F1 0.3333333333333333 on epoch=74
05/21/2022 17:04:08 - INFO - __main__ - Step 160 Global step 160 Train loss 0.429037 on epoch=79
05/21/2022 17:04:11 - INFO - __main__ - Step 170 Global step 170 Train loss 0.549639 on epoch=84
05/21/2022 17:04:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.430576 on epoch=89
05/21/2022 17:04:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.449138 on epoch=94
05/21/2022 17:04:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.661719 on epoch=99
05/21/2022 17:04:18 - INFO - __main__ - Global step 200 Train loss 0.504022 Classification-F1 0.3333333333333333 on epoch=99
05/21/2022 17:04:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.517256 on epoch=104
05/21/2022 17:04:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.405508 on epoch=109
05/21/2022 17:04:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.450215 on epoch=114
05/21/2022 17:04:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.436179 on epoch=119
05/21/2022 17:04:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.407454 on epoch=124
05/21/2022 17:04:31 - INFO - __main__ - Global step 250 Train loss 0.443322 Classification-F1 0.3333333333333333 on epoch=124
05/21/2022 17:04:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.528839 on epoch=129
05/21/2022 17:04:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.461893 on epoch=134
05/21/2022 17:04:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.462175 on epoch=139
05/21/2022 17:04:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.438429 on epoch=144
05/21/2022 17:04:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.433231 on epoch=149
05/21/2022 17:04:44 - INFO - __main__ - Global step 300 Train loss 0.464913 Classification-F1 0.36374269005847953 on epoch=149
05/21/2022 17:04:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.370258 on epoch=154
05/21/2022 17:04:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.361674 on epoch=159
05/21/2022 17:04:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.369027 on epoch=164
05/21/2022 17:04:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.390920 on epoch=169
05/21/2022 17:04:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.376450 on epoch=174
05/21/2022 17:04:56 - INFO - __main__ - Global step 350 Train loss 0.373666 Classification-F1 0.41700404858299595 on epoch=174
05/21/2022 17:04:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.363572 on epoch=179
05/21/2022 17:05:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.384015 on epoch=184
05/21/2022 17:05:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.349541 on epoch=189
05/21/2022 17:05:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.375079 on epoch=194
05/21/2022 17:05:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.340723 on epoch=199
05/21/2022 17:05:09 - INFO - __main__ - Global step 400 Train loss 0.362586 Classification-F1 0.3333333333333333 on epoch=199
05/21/2022 17:05:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.355917 on epoch=204
05/21/2022 17:05:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.353619 on epoch=209
05/21/2022 17:05:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.363899 on epoch=214
05/21/2022 17:05:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.348104 on epoch=219
05/21/2022 17:05:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.374684 on epoch=224
05/21/2022 17:05:21 - INFO - __main__ - Global step 450 Train loss 0.359244 Classification-F1 0.3333333333333333 on epoch=224
05/21/2022 17:05:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.344305 on epoch=229
05/21/2022 17:05:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.350446 on epoch=234
05/21/2022 17:05:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.343447 on epoch=239
05/21/2022 17:05:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.346177 on epoch=244
05/21/2022 17:05:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.329744 on epoch=249
05/21/2022 17:05:34 - INFO - __main__ - Global step 500 Train loss 0.342824 Classification-F1 0.3191489361702127 on epoch=249
05/21/2022 17:05:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.351530 on epoch=254
05/21/2022 17:05:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.366867 on epoch=259
05/21/2022 17:05:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.344025 on epoch=264
05/21/2022 17:05:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.382259 on epoch=269
05/21/2022 17:05:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.346679 on epoch=274
05/21/2022 17:05:46 - INFO - __main__ - Global step 550 Train loss 0.358272 Classification-F1 0.3333333333333333 on epoch=274
05/21/2022 17:05:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.347637 on epoch=279
05/21/2022 17:05:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.348550 on epoch=284
05/21/2022 17:05:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.342637 on epoch=289
05/21/2022 17:05:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.357163 on epoch=294
05/21/2022 17:05:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.346524 on epoch=299
05/21/2022 17:05:59 - INFO - __main__ - Global step 600 Train loss 0.348502 Classification-F1 0.3454545454545454 on epoch=299
05/21/2022 17:05:59 - INFO - __main__ - save last model!
05/21/2022 17:06:00 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:06:00 - INFO - __main__ - Printing 3 examples
05/21/2022 17:06:00 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/21/2022 17:06:00 - INFO - __main__ - ['positive']
05/21/2022 17:06:00 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/21/2022 17:06:00 - INFO - __main__ - ['positive']
05/21/2022 17:06:00 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/21/2022 17:06:00 - INFO - __main__ - ['positive']
05/21/2022 17:06:00 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:06:00 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:06:00 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:06:00 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:06:00 - INFO - __main__ - Printing 3 examples
05/21/2022 17:06:00 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
05/21/2022 17:06:00 - INFO - __main__ - ['positive']
05/21/2022 17:06:00 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
05/21/2022 17:06:00 - INFO - __main__ - ['positive']
05/21/2022 17:06:00 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
05/21/2022 17:06:00 - INFO - __main__ - ['positive']
05/21/2022 17:06:00 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:06:00 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:06:00 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:06:02 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:06:02 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:06:02 - INFO - __main__ - Printing 3 examples
05/21/2022 17:06:02 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:06:02 - INFO - __main__ - ['negative']
05/21/2022 17:06:02 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:06:02 - INFO - __main__ - ['negative']
05/21/2022 17:06:02 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:06:02 - INFO - __main__ - ['negative']
05/21/2022 17:06:02 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:06:03 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:06:04 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:06:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:06:04 - INFO - __main__ - Starting training!
05/21/2022 17:06:10 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_100_0.0005_8_predictions.txt
05/21/2022 17:06:10 - INFO - __main__ - Classification-F1 on test data: 0.4792
05/21/2022 17:06:10 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0005, bsz=8, dev_performance=0.5835835835835835, test_performance=0.47919845387040994
05/21/2022 17:06:10 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0003, bsz=8 ...
05/21/2022 17:06:11 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:06:11 - INFO - __main__ - Printing 3 examples
05/21/2022 17:06:11 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/21/2022 17:06:11 - INFO - __main__ - ['positive']
05/21/2022 17:06:11 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/21/2022 17:06:11 - INFO - __main__ - ['positive']
05/21/2022 17:06:11 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/21/2022 17:06:11 - INFO - __main__ - ['positive']
05/21/2022 17:06:11 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:06:11 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:06:11 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:06:11 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:06:11 - INFO - __main__ - Printing 3 examples
05/21/2022 17:06:11 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
05/21/2022 17:06:11 - INFO - __main__ - ['positive']
05/21/2022 17:06:11 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
05/21/2022 17:06:11 - INFO - __main__ - ['positive']
05/21/2022 17:06:11 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
05/21/2022 17:06:11 - INFO - __main__ - ['positive']
05/21/2022 17:06:11 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:06:11 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:06:11 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:06:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:06:16 - INFO - __main__ - Starting training!
05/21/2022 17:06:18 - INFO - __main__ - Step 10 Global step 10 Train loss 23.343750 on epoch=4
05/21/2022 17:06:20 - INFO - __main__ - Step 20 Global step 20 Train loss 16.951300 on epoch=9
05/21/2022 17:06:23 - INFO - __main__ - Step 30 Global step 30 Train loss 13.714992 on epoch=14
05/21/2022 17:06:25 - INFO - __main__ - Step 40 Global step 40 Train loss 10.306339 on epoch=19
05/21/2022 17:06:28 - INFO - __main__ - Step 50 Global step 50 Train loss 7.203708 on epoch=24
05/21/2022 17:06:29 - INFO - __main__ - Global step 50 Train loss 14.304017 Classification-F1 0.10606060606060606 on epoch=24
05/21/2022 17:06:32 - INFO - __main__ - Step 60 Global step 60 Train loss 5.663486 on epoch=29
05/21/2022 17:06:35 - INFO - __main__ - Step 70 Global step 70 Train loss 3.164496 on epoch=34
05/21/2022 17:06:37 - INFO - __main__ - Step 80 Global step 80 Train loss 1.326816 on epoch=39
05/21/2022 17:06:40 - INFO - __main__ - Step 90 Global step 90 Train loss 1.947676 on epoch=44
05/21/2022 17:06:42 - INFO - __main__ - Step 100 Global step 100 Train loss 1.061263 on epoch=49
05/21/2022 17:06:42 - INFO - __main__ - Global step 100 Train loss 2.632747 Classification-F1 0.3333333333333333 on epoch=49
05/21/2022 17:06:45 - INFO - __main__ - Step 110 Global step 110 Train loss 0.561167 on epoch=54
05/21/2022 17:06:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.547699 on epoch=59
05/21/2022 17:06:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.547289 on epoch=64
05/21/2022 17:06:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.571439 on epoch=69
05/21/2022 17:06:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.586023 on epoch=74
05/21/2022 17:06:56 - INFO - __main__ - Global step 150 Train loss 0.562723 Classification-F1 0.4589371980676329 on epoch=74
05/21/2022 17:06:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.632440 on epoch=79
05/21/2022 17:07:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.499952 on epoch=84
05/21/2022 17:07:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.400522 on epoch=89
05/21/2022 17:07:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.520478 on epoch=94
05/21/2022 17:07:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.453114 on epoch=99
05/21/2022 17:07:09 - INFO - __main__ - Global step 200 Train loss 0.501301 Classification-F1 0.5333333333333333 on epoch=99
05/21/2022 17:07:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.490667 on epoch=104
05/21/2022 17:07:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.597295 on epoch=109
05/21/2022 17:07:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.550249 on epoch=114
05/21/2022 17:07:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.442752 on epoch=119
05/21/2022 17:07:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.459599 on epoch=124
05/21/2022 17:07:22 - INFO - __main__ - Global step 250 Train loss 0.508113 Classification-F1 0.5636363636363637 on epoch=124
05/21/2022 17:07:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.366902 on epoch=129
05/21/2022 17:07:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.462301 on epoch=134
05/21/2022 17:07:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.473623 on epoch=139
05/21/2022 17:07:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.375068 on epoch=144
05/21/2022 17:07:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.482325 on epoch=149
05/21/2022 17:07:36 - INFO - __main__ - Global step 300 Train loss 0.432044 Classification-F1 0.5636363636363637 on epoch=149
05/21/2022 17:07:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.425300 on epoch=154
05/21/2022 17:07:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.347335 on epoch=159
05/21/2022 17:07:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.403669 on epoch=164
05/21/2022 17:07:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.378408 on epoch=169
05/21/2022 17:07:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.421813 on epoch=174
05/21/2022 17:07:49 - INFO - __main__ - Global step 350 Train loss 0.395305 Classification-F1 0.805668016194332 on epoch=174
05/21/2022 17:07:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.351701 on epoch=179
05/21/2022 17:07:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.316351 on epoch=184
05/21/2022 17:07:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.418205 on epoch=189
05/21/2022 17:07:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.387913 on epoch=194
05/21/2022 17:08:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.392369 on epoch=199
05/21/2022 17:08:02 - INFO - __main__ - Global step 400 Train loss 0.373308 Classification-F1 0.7046153846153846 on epoch=199
05/21/2022 17:08:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.351762 on epoch=204
05/21/2022 17:08:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.327127 on epoch=209
05/21/2022 17:08:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.432403 on epoch=214
05/21/2022 17:08:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.337398 on epoch=219
05/21/2022 17:08:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.317020 on epoch=224
05/21/2022 17:08:15 - INFO - __main__ - Global step 450 Train loss 0.353142 Classification-F1 0.6862745098039216 on epoch=224
05/21/2022 17:08:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.350456 on epoch=229
05/21/2022 17:08:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.332441 on epoch=234
05/21/2022 17:08:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.297466 on epoch=239
05/21/2022 17:08:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.219415 on epoch=244
05/21/2022 17:08:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.449293 on epoch=249
05/21/2022 17:08:28 - INFO - __main__ - Global step 500 Train loss 0.329814 Classification-F1 0.8125 on epoch=249
05/21/2022 17:08:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.347977 on epoch=254
05/21/2022 17:08:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.306819 on epoch=259
05/21/2022 17:08:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.299589 on epoch=264
05/21/2022 17:08:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.337848 on epoch=269
05/21/2022 17:08:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.383960 on epoch=274
05/21/2022 17:08:41 - INFO - __main__ - Global step 550 Train loss 0.335239 Classification-F1 0.716256157635468 on epoch=274
05/21/2022 17:08:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.294643 on epoch=279
05/21/2022 17:08:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.297464 on epoch=284
05/21/2022 17:08:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.303745 on epoch=289
05/21/2022 17:08:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.337289 on epoch=294
05/21/2022 17:08:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.252847 on epoch=299
05/21/2022 17:08:54 - INFO - __main__ - Global step 600 Train loss 0.297198 Classification-F1 0.7757757757757757 on epoch=299
05/21/2022 17:08:54 - INFO - __main__ - save last model!
05/21/2022 17:08:55 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:08:55 - INFO - __main__ - Printing 3 examples
05/21/2022 17:08:55 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/21/2022 17:08:55 - INFO - __main__ - ['positive']
05/21/2022 17:08:55 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/21/2022 17:08:55 - INFO - __main__ - ['positive']
05/21/2022 17:08:55 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/21/2022 17:08:55 - INFO - __main__ - ['positive']
05/21/2022 17:08:55 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:08:55 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:08:55 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:08:55 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:08:55 - INFO - __main__ - Printing 3 examples
05/21/2022 17:08:55 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
05/21/2022 17:08:55 - INFO - __main__ - ['positive']
05/21/2022 17:08:55 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
05/21/2022 17:08:55 - INFO - __main__ - ['positive']
05/21/2022 17:08:55 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
05/21/2022 17:08:55 - INFO - __main__ - ['positive']
05/21/2022 17:08:55 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:08:55 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:08:55 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:08:57 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:08:57 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:08:57 - INFO - __main__ - Printing 3 examples
05/21/2022 17:08:57 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:08:57 - INFO - __main__ - ['negative']
05/21/2022 17:08:57 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:08:57 - INFO - __main__ - ['negative']
05/21/2022 17:08:57 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:08:57 - INFO - __main__ - ['negative']
05/21/2022 17:08:57 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:08:58 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:08:59 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:08:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:08:59 - INFO - __main__ - Starting training!
05/21/2022 17:09:05 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_100_0.0003_8_predictions.txt
05/21/2022 17:09:05 - INFO - __main__ - Classification-F1 on test data: 0.7046
05/21/2022 17:09:06 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0003, bsz=8, dev_performance=0.8125, test_performance=0.7045523063008743
05/21/2022 17:09:06 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0002, bsz=8 ...
05/21/2022 17:09:06 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:09:06 - INFO - __main__ - Printing 3 examples
05/21/2022 17:09:06 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/21/2022 17:09:06 - INFO - __main__ - ['positive']
05/21/2022 17:09:06 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/21/2022 17:09:06 - INFO - __main__ - ['positive']
05/21/2022 17:09:06 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/21/2022 17:09:06 - INFO - __main__ - ['positive']
05/21/2022 17:09:06 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:09:06 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:09:06 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:09:06 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:09:07 - INFO - __main__ - Printing 3 examples
05/21/2022 17:09:07 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
05/21/2022 17:09:07 - INFO - __main__ - ['positive']
05/21/2022 17:09:07 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
05/21/2022 17:09:07 - INFO - __main__ - ['positive']
05/21/2022 17:09:07 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
05/21/2022 17:09:07 - INFO - __main__ - ['positive']
05/21/2022 17:09:07 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:09:07 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:09:07 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:09:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:09:11 - INFO - __main__ - Starting training!
05/21/2022 17:09:13 - INFO - __main__ - Step 10 Global step 10 Train loss 23.025887 on epoch=4
05/21/2022 17:09:15 - INFO - __main__ - Step 20 Global step 20 Train loss 18.186964 on epoch=9
05/21/2022 17:09:17 - INFO - __main__ - Step 30 Global step 30 Train loss 14.659251 on epoch=14
05/21/2022 17:09:20 - INFO - __main__ - Step 40 Global step 40 Train loss 12.479498 on epoch=19
05/21/2022 17:09:22 - INFO - __main__ - Step 50 Global step 50 Train loss 9.964463 on epoch=24
05/21/2022 17:09:25 - INFO - __main__ - Global step 50 Train loss 15.663213 Classification-F1 0.0 on epoch=24
05/21/2022 17:09:28 - INFO - __main__ - Step 60 Global step 60 Train loss 6.776296 on epoch=29
05/21/2022 17:09:30 - INFO - __main__ - Step 70 Global step 70 Train loss 6.368814 on epoch=34
05/21/2022 17:09:32 - INFO - __main__ - Step 80 Global step 80 Train loss 4.949464 on epoch=39
05/21/2022 17:09:35 - INFO - __main__ - Step 90 Global step 90 Train loss 3.215489 on epoch=44
05/21/2022 17:09:37 - INFO - __main__ - Step 100 Global step 100 Train loss 2.306784 on epoch=49
05/21/2022 17:09:38 - INFO - __main__ - Global step 100 Train loss 4.723369 Classification-F1 0.3333333333333333 on epoch=49
05/21/2022 17:09:41 - INFO - __main__ - Step 110 Global step 110 Train loss 1.451317 on epoch=54
05/21/2022 17:09:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.981809 on epoch=59
05/21/2022 17:09:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.759899 on epoch=64
05/21/2022 17:09:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.663758 on epoch=69
05/21/2022 17:09:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.756769 on epoch=74
05/21/2022 17:09:51 - INFO - __main__ - Global step 150 Train loss 0.922710 Classification-F1 0.3333333333333333 on epoch=74
05/21/2022 17:09:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.745839 on epoch=79
05/21/2022 17:09:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.705602 on epoch=84
05/21/2022 17:09:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.622838 on epoch=89
05/21/2022 17:10:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.465309 on epoch=94
05/21/2022 17:10:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.533246 on epoch=99
05/21/2022 17:10:03 - INFO - __main__ - Global step 200 Train loss 0.614567 Classification-F1 0.4285714285714286 on epoch=99
05/21/2022 17:10:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.458323 on epoch=104
05/21/2022 17:10:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.583040 on epoch=109
05/21/2022 17:10:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.432174 on epoch=114
05/21/2022 17:10:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.707390 on epoch=119
05/21/2022 17:10:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.629417 on epoch=124
05/21/2022 17:10:17 - INFO - __main__ - Global step 250 Train loss 0.562069 Classification-F1 0.4980392156862745 on epoch=124
05/21/2022 17:10:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.518916 on epoch=129
05/21/2022 17:10:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.428617 on epoch=134
05/21/2022 17:10:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.532014 on epoch=139
05/21/2022 17:10:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.455975 on epoch=144
05/21/2022 17:10:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.597676 on epoch=149
05/21/2022 17:10:30 - INFO - __main__ - Global step 300 Train loss 0.506640 Classification-F1 0.3073593073593074 on epoch=149
05/21/2022 17:10:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.486172 on epoch=154
05/21/2022 17:10:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.484111 on epoch=159
05/21/2022 17:10:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.452814 on epoch=164
05/21/2022 17:10:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.570166 on epoch=169
05/21/2022 17:10:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.514866 on epoch=174
05/21/2022 17:10:42 - INFO - __main__ - Global step 350 Train loss 0.501626 Classification-F1 0.3333333333333333 on epoch=174
05/21/2022 17:10:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.489031 on epoch=179
05/21/2022 17:10:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.479337 on epoch=184
05/21/2022 17:10:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.472848 on epoch=189
05/21/2022 17:10:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.440612 on epoch=194
05/21/2022 17:10:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.429277 on epoch=199
05/21/2022 17:10:55 - INFO - __main__ - Global step 400 Train loss 0.462221 Classification-F1 0.3333333333333333 on epoch=199
05/21/2022 17:10:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.593375 on epoch=204
05/21/2022 17:11:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.457772 on epoch=209
05/21/2022 17:11:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.489369 on epoch=214
05/21/2022 17:11:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.378783 on epoch=219
05/21/2022 17:11:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.448385 on epoch=224
05/21/2022 17:11:07 - INFO - __main__ - Global step 450 Train loss 0.473537 Classification-F1 0.4920634920634921 on epoch=224
05/21/2022 17:11:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.472622 on epoch=229
05/21/2022 17:11:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.514958 on epoch=234
05/21/2022 17:11:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.462621 on epoch=239
05/21/2022 17:11:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.477887 on epoch=244
05/21/2022 17:11:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.389092 on epoch=249
05/21/2022 17:11:20 - INFO - __main__ - Global step 500 Train loss 0.463436 Classification-F1 0.3333333333333333 on epoch=249
05/21/2022 17:11:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.359210 on epoch=254
05/21/2022 17:11:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.433663 on epoch=259
05/21/2022 17:11:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.469312 on epoch=264
05/21/2022 17:11:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.438962 on epoch=269
05/21/2022 17:11:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.465420 on epoch=274
05/21/2022 17:11:33 - INFO - __main__ - Global step 550 Train loss 0.433313 Classification-F1 0.6476476476476476 on epoch=274
05/21/2022 17:11:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.370292 on epoch=279
05/21/2022 17:11:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.437483 on epoch=284
05/21/2022 17:11:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.408203 on epoch=289
05/21/2022 17:11:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.408618 on epoch=294
05/21/2022 17:11:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.446299 on epoch=299
05/21/2022 17:11:46 - INFO - __main__ - Global step 600 Train loss 0.414179 Classification-F1 0.5465587044534412 on epoch=299
05/21/2022 17:11:46 - INFO - __main__ - save last model!
05/21/2022 17:11:47 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:11:47 - INFO - __main__ - Printing 3 examples
05/21/2022 17:11:47 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/21/2022 17:11:47 - INFO - __main__ - ['positive']
05/21/2022 17:11:47 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/21/2022 17:11:47 - INFO - __main__ - ['positive']
05/21/2022 17:11:47 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/21/2022 17:11:47 - INFO - __main__ - ['positive']
05/21/2022 17:11:47 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:11:47 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:11:47 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:11:47 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:11:47 - INFO - __main__ - Printing 3 examples
05/21/2022 17:11:47 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
05/21/2022 17:11:47 - INFO - __main__ - ['positive']
05/21/2022 17:11:47 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
05/21/2022 17:11:47 - INFO - __main__ - ['positive']
05/21/2022 17:11:47 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
05/21/2022 17:11:47 - INFO - __main__ - ['positive']
05/21/2022 17:11:47 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:11:47 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:11:47 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:11:49 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:11:49 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:11:49 - INFO - __main__ - Printing 3 examples
05/21/2022 17:11:49 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:11:49 - INFO - __main__ - ['negative']
05/21/2022 17:11:49 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:11:49 - INFO - __main__ - ['negative']
05/21/2022 17:11:49 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:11:49 - INFO - __main__ - ['negative']
05/21/2022 17:11:49 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:11:50 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:11:51 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:11:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:11:51 - INFO - __main__ - Starting training!
05/21/2022 17:11:57 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_100_0.0002_8_predictions.txt
05/21/2022 17:11:57 - INFO - __main__ - Classification-F1 on test data: 0.6056
05/21/2022 17:11:57 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0002, bsz=8, dev_performance=0.6476476476476476, test_performance=0.6055974587023851
05/21/2022 17:11:57 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.0001, bsz=8 ...
05/21/2022 17:11:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:11:58 - INFO - __main__ - Printing 3 examples
05/21/2022 17:11:58 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
05/21/2022 17:11:58 - INFO - __main__ - ['positive']
05/21/2022 17:11:58 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
05/21/2022 17:11:58 - INFO - __main__ - ['positive']
05/21/2022 17:11:58 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
05/21/2022 17:11:58 - INFO - __main__ - ['positive']
05/21/2022 17:11:58 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:11:58 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:11:58 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:11:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:11:58 - INFO - __main__ - Printing 3 examples
05/21/2022 17:11:58 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
05/21/2022 17:11:58 - INFO - __main__ - ['positive']
05/21/2022 17:11:58 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
05/21/2022 17:11:58 - INFO - __main__ - ['positive']
05/21/2022 17:11:58 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
05/21/2022 17:11:58 - INFO - __main__ - ['positive']
05/21/2022 17:11:58 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:11:58 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:11:58 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:12:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:12:02 - INFO - __main__ - Starting training!
05/21/2022 17:12:04 - INFO - __main__ - Step 10 Global step 10 Train loss 23.066504 on epoch=4
05/21/2022 17:12:06 - INFO - __main__ - Step 20 Global step 20 Train loss 21.329798 on epoch=9
05/21/2022 17:12:09 - INFO - __main__ - Step 30 Global step 30 Train loss 16.673851 on epoch=14
05/21/2022 17:12:11 - INFO - __main__ - Step 40 Global step 40 Train loss 16.442656 on epoch=19
05/21/2022 17:12:14 - INFO - __main__ - Step 50 Global step 50 Train loss 16.270634 on epoch=24
05/21/2022 17:12:19 - INFO - __main__ - Global step 50 Train loss 18.756689 Classification-F1 0.0 on epoch=24
05/21/2022 17:12:21 - INFO - __main__ - Step 60 Global step 60 Train loss 12.869272 on epoch=29
05/21/2022 17:12:24 - INFO - __main__ - Step 70 Global step 70 Train loss 12.795426 on epoch=34
05/21/2022 17:12:26 - INFO - __main__ - Step 80 Global step 80 Train loss 12.587499 on epoch=39
05/21/2022 17:12:29 - INFO - __main__ - Step 90 Global step 90 Train loss 11.425797 on epoch=44
05/21/2022 17:12:31 - INFO - __main__ - Step 100 Global step 100 Train loss 10.580706 on epoch=49
05/21/2022 17:12:34 - INFO - __main__ - Global step 100 Train loss 12.051740 Classification-F1 0.0 on epoch=49
05/21/2022 17:12:37 - INFO - __main__ - Step 110 Global step 110 Train loss 9.018617 on epoch=54
05/21/2022 17:12:39 - INFO - __main__ - Step 120 Global step 120 Train loss 8.547529 on epoch=59
05/21/2022 17:12:41 - INFO - __main__ - Step 130 Global step 130 Train loss 6.698777 on epoch=64
05/21/2022 17:12:44 - INFO - __main__ - Step 140 Global step 140 Train loss 5.294441 on epoch=69
05/21/2022 17:12:46 - INFO - __main__ - Step 150 Global step 150 Train loss 5.898695 on epoch=74
05/21/2022 17:12:47 - INFO - __main__ - Global step 150 Train loss 7.091611 Classification-F1 0.3333333333333333 on epoch=74
05/21/2022 17:12:50 - INFO - __main__ - Step 160 Global step 160 Train loss 4.311020 on epoch=79
05/21/2022 17:12:52 - INFO - __main__ - Step 170 Global step 170 Train loss 4.009510 on epoch=84
05/21/2022 17:12:54 - INFO - __main__ - Step 180 Global step 180 Train loss 3.299757 on epoch=89
05/21/2022 17:12:57 - INFO - __main__ - Step 190 Global step 190 Train loss 3.511825 on epoch=94
05/21/2022 17:12:59 - INFO - __main__ - Step 200 Global step 200 Train loss 2.145335 on epoch=99
05/21/2022 17:13:00 - INFO - __main__ - Global step 200 Train loss 3.455489 Classification-F1 0.3333333333333333 on epoch=99
05/21/2022 17:13:02 - INFO - __main__ - Step 210 Global step 210 Train loss 2.240998 on epoch=104
05/21/2022 17:13:04 - INFO - __main__ - Step 220 Global step 220 Train loss 1.598090 on epoch=109
05/21/2022 17:13:07 - INFO - __main__ - Step 230 Global step 230 Train loss 1.305165 on epoch=114
05/21/2022 17:13:09 - INFO - __main__ - Step 240 Global step 240 Train loss 2.051868 on epoch=119
05/21/2022 17:13:12 - INFO - __main__ - Step 250 Global step 250 Train loss 1.187603 on epoch=124
05/21/2022 17:13:12 - INFO - __main__ - Global step 250 Train loss 1.676745 Classification-F1 0.873015873015873 on epoch=124
05/21/2022 17:13:15 - INFO - __main__ - Step 260 Global step 260 Train loss 1.028265 on epoch=129
05/21/2022 17:13:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.570232 on epoch=134
05/21/2022 17:13:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.847506 on epoch=139
05/21/2022 17:13:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.494514 on epoch=144
05/21/2022 17:13:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.428189 on epoch=149
05/21/2022 17:13:25 - INFO - __main__ - Global step 300 Train loss 0.673741 Classification-F1 0.9372549019607843 on epoch=149
05/21/2022 17:13:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.579125 on epoch=154
05/21/2022 17:13:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.400153 on epoch=159
05/21/2022 17:13:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.499650 on epoch=164
05/21/2022 17:13:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.390685 on epoch=169
05/21/2022 17:13:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.540405 on epoch=174
05/21/2022 17:13:38 - INFO - __main__ - Global step 350 Train loss 0.482004 Classification-F1 0.9687194525904204 on epoch=174
05/21/2022 17:13:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.392907 on epoch=179
05/21/2022 17:13:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.331169 on epoch=184
05/21/2022 17:13:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.343094 on epoch=189
05/21/2022 17:13:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.405425 on epoch=194
05/21/2022 17:13:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.333904 on epoch=199
05/21/2022 17:13:51 - INFO - __main__ - Global step 400 Train loss 0.361300 Classification-F1 0.9687194525904204 on epoch=199
05/21/2022 17:13:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.469549 on epoch=204
05/21/2022 17:13:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.280571 on epoch=209
05/21/2022 17:13:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.295417 on epoch=214
05/21/2022 17:14:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.312856 on epoch=219
05/21/2022 17:14:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.194278 on epoch=224
05/21/2022 17:14:03 - INFO - __main__ - Global step 450 Train loss 0.310534 Classification-F1 0.9687194525904204 on epoch=224
05/21/2022 17:14:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.200797 on epoch=229
05/21/2022 17:14:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.146843 on epoch=234
05/21/2022 17:14:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.148945 on epoch=239
05/21/2022 17:14:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.144146 on epoch=244
05/21/2022 17:14:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.065328 on epoch=249
05/21/2022 17:14:16 - INFO - __main__ - Global step 500 Train loss 0.141212 Classification-F1 1.0 on epoch=249
05/21/2022 17:14:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.138846 on epoch=254
05/21/2022 17:14:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.104573 on epoch=259
05/21/2022 17:14:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.149553 on epoch=264
05/21/2022 17:14:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.104114 on epoch=269
05/21/2022 17:14:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.032218 on epoch=274
05/21/2022 17:14:29 - INFO - __main__ - Global step 550 Train loss 0.105861 Classification-F1 0.9687194525904204 on epoch=274
05/21/2022 17:14:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.075268 on epoch=279
05/21/2022 17:14:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.102348 on epoch=284
05/21/2022 17:14:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.052427 on epoch=289
05/21/2022 17:14:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.031870 on epoch=294
05/21/2022 17:14:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.037894 on epoch=299
05/21/2022 17:14:41 - INFO - __main__ - Global step 600 Train loss 0.059961 Classification-F1 1.0 on epoch=299
05/21/2022 17:14:41 - INFO - __main__ - save last model!
05/21/2022 17:14:42 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:14:42 - INFO - __main__ - Printing 3 examples
05/21/2022 17:14:42 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/21/2022 17:14:42 - INFO - __main__ - ['negative']
05/21/2022 17:14:42 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/21/2022 17:14:42 - INFO - __main__ - ['negative']
05/21/2022 17:14:42 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/21/2022 17:14:42 - INFO - __main__ - ['negative']
05/21/2022 17:14:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:14:42 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:14:42 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:14:42 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:14:42 - INFO - __main__ - Printing 3 examples
05/21/2022 17:14:42 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
05/21/2022 17:14:42 - INFO - __main__ - ['negative']
05/21/2022 17:14:42 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
05/21/2022 17:14:42 - INFO - __main__ - ['negative']
05/21/2022 17:14:42 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
05/21/2022 17:14:42 - INFO - __main__ - ['negative']
05/21/2022 17:14:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:14:42 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:14:42 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:14:44 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:14:44 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:14:44 - INFO - __main__ - Printing 3 examples
05/21/2022 17:14:44 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:14:44 - INFO - __main__ - ['negative']
05/21/2022 17:14:44 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:14:44 - INFO - __main__ - ['negative']
05/21/2022 17:14:44 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:14:44 - INFO - __main__ - ['negative']
05/21/2022 17:14:44 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:14:45 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:14:46 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:14:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:14:46 - INFO - __main__ - Starting training!
05/21/2022 17:14:52 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_100_0.0001_8_predictions.txt
05/21/2022 17:14:52 - INFO - __main__ - Classification-F1 on test data: 0.9229
05/21/2022 17:14:52 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.0001, bsz=8, dev_performance=1.0, test_performance=0.922882704593687
05/21/2022 17:14:52 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0005, bsz=8 ...
05/21/2022 17:14:53 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:14:53 - INFO - __main__ - Printing 3 examples
05/21/2022 17:14:53 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/21/2022 17:14:53 - INFO - __main__ - ['negative']
05/21/2022 17:14:53 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/21/2022 17:14:53 - INFO - __main__ - ['negative']
05/21/2022 17:14:53 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/21/2022 17:14:53 - INFO - __main__ - ['negative']
05/21/2022 17:14:53 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:14:53 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:14:53 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:14:53 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:14:53 - INFO - __main__ - Printing 3 examples
05/21/2022 17:14:53 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
05/21/2022 17:14:53 - INFO - __main__ - ['negative']
05/21/2022 17:14:53 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
05/21/2022 17:14:53 - INFO - __main__ - ['negative']
05/21/2022 17:14:53 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
05/21/2022 17:14:53 - INFO - __main__ - ['negative']
05/21/2022 17:14:53 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:14:54 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:14:54 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:14:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:14:58 - INFO - __main__ - Starting training!
05/21/2022 17:15:00 - INFO - __main__ - Step 10 Global step 10 Train loss 20.977041 on epoch=4
05/21/2022 17:15:02 - INFO - __main__ - Step 20 Global step 20 Train loss 13.515381 on epoch=9
05/21/2022 17:15:05 - INFO - __main__ - Step 30 Global step 30 Train loss 6.956714 on epoch=14
05/21/2022 17:15:07 - INFO - __main__ - Step 40 Global step 40 Train loss 3.961160 on epoch=19
05/21/2022 17:15:10 - INFO - __main__ - Step 50 Global step 50 Train loss 1.341097 on epoch=24
05/21/2022 17:15:10 - INFO - __main__ - Global step 50 Train loss 9.350279 Classification-F1 0.3333333333333333 on epoch=24
05/21/2022 17:15:13 - INFO - __main__ - Step 60 Global step 60 Train loss 0.749617 on epoch=29
05/21/2022 17:15:15 - INFO - __main__ - Step 70 Global step 70 Train loss 0.619719 on epoch=34
05/21/2022 17:15:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.499908 on epoch=39
05/21/2022 17:15:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.390980 on epoch=44
05/21/2022 17:15:23 - INFO - __main__ - Step 100 Global step 100 Train loss 0.379992 on epoch=49
05/21/2022 17:15:23 - INFO - __main__ - Global step 100 Train loss 0.528043 Classification-F1 0.7810361681329424 on epoch=49
05/21/2022 17:15:26 - INFO - __main__ - Step 110 Global step 110 Train loss 0.484721 on epoch=54
05/21/2022 17:15:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.404455 on epoch=59
05/21/2022 17:15:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.375244 on epoch=64
05/21/2022 17:15:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.317886 on epoch=69
05/21/2022 17:15:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.342540 on epoch=74
05/21/2022 17:15:37 - INFO - __main__ - Global step 150 Train loss 0.384969 Classification-F1 0.7408906882591093 on epoch=74
05/21/2022 17:15:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.149154 on epoch=79
05/21/2022 17:15:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.067230 on epoch=84
05/21/2022 17:15:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.086326 on epoch=89
05/21/2022 17:15:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.104973 on epoch=94
05/21/2022 17:15:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.079982 on epoch=99
05/21/2022 17:15:50 - INFO - __main__ - Global step 200 Train loss 0.097533 Classification-F1 0.8125 on epoch=99
05/21/2022 17:15:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.050639 on epoch=104
05/21/2022 17:15:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.217225 on epoch=109
05/21/2022 17:15:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.059150 on epoch=114
05/21/2022 17:16:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.027804 on epoch=119
05/21/2022 17:16:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.026136 on epoch=124
05/21/2022 17:16:03 - INFO - __main__ - Global step 250 Train loss 0.076191 Classification-F1 0.8125 on epoch=124
05/21/2022 17:16:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.014625 on epoch=129
05/21/2022 17:16:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.017142 on epoch=134
05/21/2022 17:16:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.020084 on epoch=139
05/21/2022 17:16:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.003615 on epoch=144
05/21/2022 17:16:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.033052 on epoch=149
05/21/2022 17:16:16 - INFO - __main__ - Global step 300 Train loss 0.017704 Classification-F1 0.8125 on epoch=149
05/21/2022 17:16:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.035237 on epoch=154
05/21/2022 17:16:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.026470 on epoch=159
05/21/2022 17:16:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.014032 on epoch=164
05/21/2022 17:16:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.027457 on epoch=169
05/21/2022 17:16:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.053951 on epoch=174
05/21/2022 17:16:29 - INFO - __main__ - Global step 350 Train loss 0.031430 Classification-F1 0.7810361681329424 on epoch=174
05/21/2022 17:16:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.102712 on epoch=179
05/21/2022 17:16:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.060663 on epoch=184
05/21/2022 17:16:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.047482 on epoch=189
05/21/2022 17:16:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.263771 on epoch=194
05/21/2022 17:16:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.113289 on epoch=199
05/21/2022 17:16:41 - INFO - __main__ - Global step 400 Train loss 0.117583 Classification-F1 0.7117117117117117 on epoch=199
05/21/2022 17:16:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.039586 on epoch=204
05/21/2022 17:16:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.074541 on epoch=209
05/21/2022 17:16:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.026238 on epoch=214
05/21/2022 17:16:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.003384 on epoch=219
05/21/2022 17:16:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.007923 on epoch=224
05/21/2022 17:16:54 - INFO - __main__ - Global step 450 Train loss 0.030334 Classification-F1 0.8117647058823529 on epoch=224
05/21/2022 17:16:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.002415 on epoch=229
05/21/2022 17:16:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.009466 on epoch=234
05/21/2022 17:17:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.006555 on epoch=239
05/21/2022 17:17:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.004784 on epoch=244
05/21/2022 17:17:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.013992 on epoch=249
05/21/2022 17:17:07 - INFO - __main__ - Global step 500 Train loss 0.007442 Classification-F1 0.8125 on epoch=249
05/21/2022 17:17:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.120088 on epoch=254
05/21/2022 17:17:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.022304 on epoch=259
05/21/2022 17:17:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.060230 on epoch=264
05/21/2022 17:17:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.004352 on epoch=269
05/21/2022 17:17:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.014697 on epoch=274
05/21/2022 17:17:19 - INFO - __main__ - Global step 550 Train loss 0.044334 Classification-F1 0.716256157635468 on epoch=274
05/21/2022 17:17:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.000782 on epoch=279
05/21/2022 17:17:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.002574 on epoch=284
05/21/2022 17:17:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.112930 on epoch=289
05/21/2022 17:17:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.029203 on epoch=294
05/21/2022 17:17:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.022611 on epoch=299
05/21/2022 17:17:32 - INFO - __main__ - Global step 600 Train loss 0.033620 Classification-F1 0.7810361681329424 on epoch=299
05/21/2022 17:17:32 - INFO - __main__ - save last model!
05/21/2022 17:17:33 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:17:33 - INFO - __main__ - Printing 3 examples
05/21/2022 17:17:33 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/21/2022 17:17:33 - INFO - __main__ - ['negative']
05/21/2022 17:17:33 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/21/2022 17:17:33 - INFO - __main__ - ['negative']
05/21/2022 17:17:33 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/21/2022 17:17:33 - INFO - __main__ - ['negative']
05/21/2022 17:17:33 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:17:33 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:17:33 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:17:33 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:17:33 - INFO - __main__ - Printing 3 examples
05/21/2022 17:17:33 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
05/21/2022 17:17:33 - INFO - __main__ - ['negative']
05/21/2022 17:17:33 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
05/21/2022 17:17:33 - INFO - __main__ - ['negative']
05/21/2022 17:17:33 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
05/21/2022 17:17:33 - INFO - __main__ - ['negative']
05/21/2022 17:17:33 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:17:33 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:17:33 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:17:35 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:17:35 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:17:35 - INFO - __main__ - Printing 3 examples
05/21/2022 17:17:35 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:17:35 - INFO - __main__ - ['negative']
05/21/2022 17:17:35 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:17:35 - INFO - __main__ - ['negative']
05/21/2022 17:17:35 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:17:35 - INFO - __main__ - ['negative']
05/21/2022 17:17:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:17:36 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:17:37 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:17:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:17:37 - INFO - __main__ - Starting training!
05/21/2022 17:17:43 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_13_0.0005_8_predictions.txt
05/21/2022 17:17:43 - INFO - __main__ - Classification-F1 on test data: 0.8676
05/21/2022 17:17:43 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0005, bsz=8, dev_performance=0.8125, test_performance=0.8676498965951673
05/21/2022 17:17:43 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0003, bsz=8 ...
05/21/2022 17:17:44 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:17:44 - INFO - __main__ - Printing 3 examples
05/21/2022 17:17:44 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/21/2022 17:17:44 - INFO - __main__ - ['negative']
05/21/2022 17:17:44 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/21/2022 17:17:44 - INFO - __main__ - ['negative']
05/21/2022 17:17:44 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/21/2022 17:17:44 - INFO - __main__ - ['negative']
05/21/2022 17:17:44 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:17:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:17:44 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:17:44 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:17:44 - INFO - __main__ - Printing 3 examples
05/21/2022 17:17:44 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
05/21/2022 17:17:44 - INFO - __main__ - ['negative']
05/21/2022 17:17:44 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
05/21/2022 17:17:44 - INFO - __main__ - ['negative']
05/21/2022 17:17:44 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
05/21/2022 17:17:44 - INFO - __main__ - ['negative']
05/21/2022 17:17:44 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:17:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:17:44 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:17:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:17:48 - INFO - __main__ - Starting training!
05/21/2022 17:17:50 - INFO - __main__ - Step 10 Global step 10 Train loss 23.547005 on epoch=4
05/21/2022 17:17:53 - INFO - __main__ - Step 20 Global step 20 Train loss 18.799934 on epoch=9
05/21/2022 17:17:55 - INFO - __main__ - Step 30 Global step 30 Train loss 13.827263 on epoch=14
05/21/2022 17:17:58 - INFO - __main__ - Step 40 Global step 40 Train loss 9.364819 on epoch=19
05/21/2022 17:18:00 - INFO - __main__ - Step 50 Global step 50 Train loss 6.536245 on epoch=24
05/21/2022 17:18:00 - INFO - __main__ - Global step 50 Train loss 14.415053 Classification-F1 0.3333333333333333 on epoch=24
05/21/2022 17:18:03 - INFO - __main__ - Step 60 Global step 60 Train loss 3.630085 on epoch=29
05/21/2022 17:18:06 - INFO - __main__ - Step 70 Global step 70 Train loss 2.447478 on epoch=34
05/21/2022 17:18:08 - INFO - __main__ - Step 80 Global step 80 Train loss 0.911344 on epoch=39
05/21/2022 17:18:11 - INFO - __main__ - Step 90 Global step 90 Train loss 0.853284 on epoch=44
05/21/2022 17:18:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.561617 on epoch=49
05/21/2022 17:18:13 - INFO - __main__ - Global step 100 Train loss 1.680762 Classification-F1 0.7810361681329424 on epoch=49
05/21/2022 17:18:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.400667 on epoch=54
05/21/2022 17:18:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.592836 on epoch=59
05/21/2022 17:18:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.658156 on epoch=64
05/21/2022 17:18:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.349616 on epoch=69
05/21/2022 17:18:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.342492 on epoch=74
05/21/2022 17:18:26 - INFO - __main__ - Global step 150 Train loss 0.468753 Classification-F1 0.716256157635468 on epoch=74
05/21/2022 17:18:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.354464 on epoch=79
05/21/2022 17:18:31 - INFO - __main__ - Step 170 Global step 170 Train loss 0.334074 on epoch=84
05/21/2022 17:18:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.273495 on epoch=89
05/21/2022 17:18:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.192571 on epoch=94
05/21/2022 17:18:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.182291 on epoch=99
05/21/2022 17:18:39 - INFO - __main__ - Global step 200 Train loss 0.267379 Classification-F1 0.8435972629521017 on epoch=99
05/21/2022 17:18:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.199136 on epoch=104
05/21/2022 17:18:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.183730 on epoch=109
05/21/2022 17:18:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.143301 on epoch=114
05/21/2022 17:18:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.076849 on epoch=119
05/21/2022 17:18:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.185766 on epoch=124
05/21/2022 17:18:52 - INFO - __main__ - Global step 250 Train loss 0.157756 Classification-F1 0.7810361681329424 on epoch=124
05/21/2022 17:18:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.129487 on epoch=129
05/21/2022 17:18:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.127215 on epoch=134
05/21/2022 17:19:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.135828 on epoch=139
05/21/2022 17:19:02 - INFO - __main__ - Step 290 Global step 290 Train loss 0.125952 on epoch=144
05/21/2022 17:19:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.100352 on epoch=149
05/21/2022 17:19:05 - INFO - __main__ - Global step 300 Train loss 0.123767 Classification-F1 0.8125 on epoch=149
05/21/2022 17:19:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.107041 on epoch=154
05/21/2022 17:19:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.070897 on epoch=159
05/21/2022 17:19:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.028891 on epoch=164
05/21/2022 17:19:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.048351 on epoch=169
05/21/2022 17:19:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.043975 on epoch=174
05/21/2022 17:19:18 - INFO - __main__ - Global step 350 Train loss 0.059831 Classification-F1 0.8125 on epoch=174
05/21/2022 17:19:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.021868 on epoch=179
05/21/2022 17:19:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.095236 on epoch=184
05/21/2022 17:19:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.174868 on epoch=189
05/21/2022 17:19:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.193512 on epoch=194
05/21/2022 17:19:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.387844 on epoch=199
05/21/2022 17:19:30 - INFO - __main__ - Global step 400 Train loss 0.174666 Classification-F1 0.6862745098039216 on epoch=199
05/21/2022 17:19:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.343682 on epoch=204
05/21/2022 17:19:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.401983 on epoch=209
05/21/2022 17:19:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.552725 on epoch=214
05/21/2022 17:19:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.356257 on epoch=219
05/21/2022 17:19:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.432890 on epoch=224
05/21/2022 17:19:43 - INFO - __main__ - Global step 450 Train loss 0.417507 Classification-F1 0.6666666666666667 on epoch=224
05/21/2022 17:19:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.404804 on epoch=229
05/21/2022 17:19:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.320311 on epoch=234
05/21/2022 17:19:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.347179 on epoch=239
05/21/2022 17:19:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.393870 on epoch=244
05/21/2022 17:19:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.354403 on epoch=249
05/21/2022 17:19:55 - INFO - __main__ - Global step 500 Train loss 0.364114 Classification-F1 0.6532019704433498 on epoch=249
05/21/2022 17:19:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.319167 on epoch=254
05/21/2022 17:20:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.424678 on epoch=259
05/21/2022 17:20:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.269028 on epoch=264
05/21/2022 17:20:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.212524 on epoch=269
05/21/2022 17:20:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.238810 on epoch=274
05/21/2022 17:20:08 - INFO - __main__ - Global step 550 Train loss 0.292841 Classification-F1 0.746031746031746 on epoch=274
05/21/2022 17:20:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.279962 on epoch=279
05/21/2022 17:20:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.304568 on epoch=284
05/21/2022 17:20:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.265362 on epoch=289
05/21/2022 17:20:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.318004 on epoch=294
05/21/2022 17:20:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.527317 on epoch=299
05/21/2022 17:20:21 - INFO - __main__ - Global step 600 Train loss 0.339043 Classification-F1 0.7117117117117117 on epoch=299
05/21/2022 17:20:21 - INFO - __main__ - save last model!
05/21/2022 17:20:22 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:20:22 - INFO - __main__ - Printing 3 examples
05/21/2022 17:20:22 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/21/2022 17:20:22 - INFO - __main__ - ['negative']
05/21/2022 17:20:22 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/21/2022 17:20:22 - INFO - __main__ - ['negative']
05/21/2022 17:20:22 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/21/2022 17:20:22 - INFO - __main__ - ['negative']
05/21/2022 17:20:22 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:20:22 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:20:22 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:20:22 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:20:22 - INFO - __main__ - Printing 3 examples
05/21/2022 17:20:22 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
05/21/2022 17:20:22 - INFO - __main__ - ['negative']
05/21/2022 17:20:22 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
05/21/2022 17:20:22 - INFO - __main__ - ['negative']
05/21/2022 17:20:22 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
05/21/2022 17:20:22 - INFO - __main__ - ['negative']
05/21/2022 17:20:22 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:20:22 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:20:22 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:20:24 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:20:24 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:20:24 - INFO - __main__ - Printing 3 examples
05/21/2022 17:20:24 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:20:24 - INFO - __main__ - ['negative']
05/21/2022 17:20:24 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:20:24 - INFO - __main__ - ['negative']
05/21/2022 17:20:24 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:20:24 - INFO - __main__ - ['negative']
05/21/2022 17:20:24 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:20:24 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:20:25 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:20:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:20:26 - INFO - __main__ - Starting training!
05/21/2022 17:20:32 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_13_0.0003_8_predictions.txt
05/21/2022 17:20:32 - INFO - __main__ - Classification-F1 on test data: 0.8985
05/21/2022 17:20:32 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0003, bsz=8, dev_performance=0.8435972629521017, test_performance=0.8985445665592845
05/21/2022 17:20:32 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0002, bsz=8 ...
05/21/2022 17:20:33 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:20:33 - INFO - __main__ - Printing 3 examples
05/21/2022 17:20:33 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/21/2022 17:20:33 - INFO - __main__ - ['negative']
05/21/2022 17:20:33 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/21/2022 17:20:33 - INFO - __main__ - ['negative']
05/21/2022 17:20:33 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/21/2022 17:20:33 - INFO - __main__ - ['negative']
05/21/2022 17:20:33 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:20:33 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:20:33 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:20:33 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:20:33 - INFO - __main__ - Printing 3 examples
05/21/2022 17:20:33 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
05/21/2022 17:20:33 - INFO - __main__ - ['negative']
05/21/2022 17:20:33 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
05/21/2022 17:20:33 - INFO - __main__ - ['negative']
05/21/2022 17:20:33 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
05/21/2022 17:20:33 - INFO - __main__ - ['negative']
05/21/2022 17:20:33 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:20:33 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:20:33 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:20:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:20:37 - INFO - __main__ - Starting training!
05/21/2022 17:20:39 - INFO - __main__ - Step 10 Global step 10 Train loss 22.959497 on epoch=4
05/21/2022 17:20:42 - INFO - __main__ - Step 20 Global step 20 Train loss 17.088507 on epoch=9
05/21/2022 17:20:44 - INFO - __main__ - Step 30 Global step 30 Train loss 15.666501 on epoch=14
05/21/2022 17:20:47 - INFO - __main__ - Step 40 Global step 40 Train loss 11.695167 on epoch=19
05/21/2022 17:20:49 - INFO - __main__ - Step 50 Global step 50 Train loss 11.379983 on epoch=24
05/21/2022 17:20:52 - INFO - __main__ - Global step 50 Train loss 15.757931 Classification-F1 0.0 on epoch=24
05/21/2022 17:20:55 - INFO - __main__ - Step 60 Global step 60 Train loss 6.961893 on epoch=29
05/21/2022 17:20:57 - INFO - __main__ - Step 70 Global step 70 Train loss 4.865077 on epoch=34
05/21/2022 17:21:00 - INFO - __main__ - Step 80 Global step 80 Train loss 3.076273 on epoch=39
05/21/2022 17:21:02 - INFO - __main__ - Step 90 Global step 90 Train loss 1.816796 on epoch=44
05/21/2022 17:21:05 - INFO - __main__ - Step 100 Global step 100 Train loss 1.132157 on epoch=49
05/21/2022 17:21:05 - INFO - __main__ - Global step 100 Train loss 3.570439 Classification-F1 0.3333333333333333 on epoch=49
05/21/2022 17:21:08 - INFO - __main__ - Step 110 Global step 110 Train loss 1.204790 on epoch=54
05/21/2022 17:21:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.621285 on epoch=59
05/21/2022 17:21:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.653022 on epoch=64
05/21/2022 17:21:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.647672 on epoch=69
05/21/2022 17:21:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.415842 on epoch=74
05/21/2022 17:21:19 - INFO - __main__ - Global step 150 Train loss 0.708522 Classification-F1 0.6761133603238867 on epoch=74
05/21/2022 17:21:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.489275 on epoch=79
05/21/2022 17:21:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.517498 on epoch=84
05/21/2022 17:21:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.464962 on epoch=89
05/21/2022 17:21:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.466067 on epoch=94
05/21/2022 17:21:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.430436 on epoch=99
05/21/2022 17:21:32 - INFO - __main__ - Global step 200 Train loss 0.473648 Classification-F1 0.6875 on epoch=99
05/21/2022 17:21:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.492223 on epoch=104
05/21/2022 17:21:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.363682 on epoch=109
05/21/2022 17:21:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.431613 on epoch=114
05/21/2022 17:21:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.428044 on epoch=119
05/21/2022 17:21:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.412170 on epoch=124
05/21/2022 17:21:45 - INFO - __main__ - Global step 250 Train loss 0.425546 Classification-F1 0.6267232237539766 on epoch=124
05/21/2022 17:21:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.321962 on epoch=129
05/21/2022 17:21:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.384095 on epoch=134
05/21/2022 17:21:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.241743 on epoch=139
05/21/2022 17:21:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.280151 on epoch=144
05/21/2022 17:21:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.323077 on epoch=149
05/21/2022 17:21:58 - INFO - __main__ - Global step 300 Train loss 0.310206 Classification-F1 0.7490196078431373 on epoch=149
05/21/2022 17:22:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.328733 on epoch=154
05/21/2022 17:22:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.416599 on epoch=159
05/21/2022 17:22:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.435625 on epoch=164
05/21/2022 17:22:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.402681 on epoch=169
05/21/2022 17:22:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.352044 on epoch=174
05/21/2022 17:22:12 - INFO - __main__ - Global step 350 Train loss 0.387136 Classification-F1 0.7810361681329424 on epoch=174
05/21/2022 17:22:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.323725 on epoch=179
05/21/2022 17:22:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.355365 on epoch=184
05/21/2022 17:22:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.325961 on epoch=189
05/21/2022 17:22:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.354560 on epoch=194
05/21/2022 17:22:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.503923 on epoch=199
05/21/2022 17:22:25 - INFO - __main__ - Global step 400 Train loss 0.372707 Classification-F1 0.41700404858299595 on epoch=199
05/21/2022 17:22:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.518618 on epoch=204
05/21/2022 17:22:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.527155 on epoch=209
05/21/2022 17:22:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.367273 on epoch=214
05/21/2022 17:22:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.399783 on epoch=219
05/21/2022 17:22:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.465199 on epoch=224
05/21/2022 17:22:38 - INFO - __main__ - Global step 450 Train loss 0.455605 Classification-F1 0.6875 on epoch=224
05/21/2022 17:22:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.428586 on epoch=229
05/21/2022 17:22:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.469929 on epoch=234
05/21/2022 17:22:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.522920 on epoch=239
05/21/2022 17:22:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.373445 on epoch=244
05/21/2022 17:22:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.335567 on epoch=249
05/21/2022 17:22:51 - INFO - __main__ - Global step 500 Train loss 0.426089 Classification-F1 0.7046153846153846 on epoch=249
05/21/2022 17:22:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.419518 on epoch=254
05/21/2022 17:22:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.382998 on epoch=259
05/21/2022 17:22:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.530730 on epoch=264
05/21/2022 17:23:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.369233 on epoch=269
05/21/2022 17:23:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.389460 on epoch=274
05/21/2022 17:23:04 - INFO - __main__ - Global step 550 Train loss 0.418388 Classification-F1 0.49090909090909085 on epoch=274
05/21/2022 17:23:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.314953 on epoch=279
05/21/2022 17:23:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.352878 on epoch=284
05/21/2022 17:23:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.344506 on epoch=289
05/21/2022 17:23:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.361180 on epoch=294
05/21/2022 17:23:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.334150 on epoch=299
05/21/2022 17:23:17 - INFO - __main__ - Global step 600 Train loss 0.341533 Classification-F1 0.5733333333333335 on epoch=299
05/21/2022 17:23:17 - INFO - __main__ - save last model!
05/21/2022 17:23:18 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:23:18 - INFO - __main__ - Printing 3 examples
05/21/2022 17:23:18 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/21/2022 17:23:18 - INFO - __main__ - ['negative']
05/21/2022 17:23:18 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/21/2022 17:23:18 - INFO - __main__ - ['negative']
05/21/2022 17:23:18 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/21/2022 17:23:18 - INFO - __main__ - ['negative']
05/21/2022 17:23:18 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:23:18 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:23:18 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:23:18 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:23:18 - INFO - __main__ - Printing 3 examples
05/21/2022 17:23:18 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
05/21/2022 17:23:18 - INFO - __main__ - ['negative']
05/21/2022 17:23:18 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
05/21/2022 17:23:18 - INFO - __main__ - ['negative']
05/21/2022 17:23:18 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
05/21/2022 17:23:18 - INFO - __main__ - ['negative']
05/21/2022 17:23:18 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:23:18 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:23:18 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:23:20 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:23:20 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:23:20 - INFO - __main__ - Printing 3 examples
05/21/2022 17:23:20 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:23:20 - INFO - __main__ - ['negative']
05/21/2022 17:23:20 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:23:20 - INFO - __main__ - ['negative']
05/21/2022 17:23:20 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:23:20 - INFO - __main__ - ['negative']
05/21/2022 17:23:20 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:23:21 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:23:22 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:23:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:23:23 - INFO - __main__ - Starting training!
05/21/2022 17:23:28 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_13_0.0002_8_predictions.txt
05/21/2022 17:23:28 - INFO - __main__ - Classification-F1 on test data: 0.8266
05/21/2022 17:23:29 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0002, bsz=8, dev_performance=0.7810361681329424, test_performance=0.8265954229256982
05/21/2022 17:23:29 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.0001, bsz=8 ...
05/21/2022 17:23:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:23:29 - INFO - __main__ - Printing 3 examples
05/21/2022 17:23:29 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
05/21/2022 17:23:29 - INFO - __main__ - ['negative']
05/21/2022 17:23:29 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
05/21/2022 17:23:29 - INFO - __main__ - ['negative']
05/21/2022 17:23:29 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
05/21/2022 17:23:29 - INFO - __main__ - ['negative']
05/21/2022 17:23:29 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:23:30 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:23:30 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:23:30 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:23:30 - INFO - __main__ - Printing 3 examples
05/21/2022 17:23:30 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
05/21/2022 17:23:30 - INFO - __main__ - ['negative']
05/21/2022 17:23:30 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
05/21/2022 17:23:30 - INFO - __main__ - ['negative']
05/21/2022 17:23:30 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
05/21/2022 17:23:30 - INFO - __main__ - ['negative']
05/21/2022 17:23:30 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:23:30 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:23:30 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:23:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:23:34 - INFO - __main__ - Starting training!
05/21/2022 17:23:36 - INFO - __main__ - Step 10 Global step 10 Train loss 22.384211 on epoch=4
05/21/2022 17:23:38 - INFO - __main__ - Step 20 Global step 20 Train loss 19.902678 on epoch=9
05/21/2022 17:23:41 - INFO - __main__ - Step 30 Global step 30 Train loss 17.676655 on epoch=14
05/21/2022 17:23:43 - INFO - __main__ - Step 40 Global step 40 Train loss 15.222328 on epoch=19
05/21/2022 17:23:46 - INFO - __main__ - Step 50 Global step 50 Train loss 13.582125 on epoch=24
05/21/2022 17:23:48 - INFO - __main__ - Global step 50 Train loss 17.753599 Classification-F1 0.0 on epoch=24
05/21/2022 17:23:51 - INFO - __main__ - Step 60 Global step 60 Train loss 12.253461 on epoch=29
05/21/2022 17:23:54 - INFO - __main__ - Step 70 Global step 70 Train loss 11.388090 on epoch=34
05/21/2022 17:23:56 - INFO - __main__ - Step 80 Global step 80 Train loss 10.085814 on epoch=39
05/21/2022 17:23:59 - INFO - __main__ - Step 90 Global step 90 Train loss 8.321781 on epoch=44
05/21/2022 17:24:01 - INFO - __main__ - Step 100 Global step 100 Train loss 8.044699 on epoch=49
05/21/2022 17:24:03 - INFO - __main__ - Global step 100 Train loss 10.018769 Classification-F1 0.0 on epoch=49
05/21/2022 17:24:05 - INFO - __main__ - Step 110 Global step 110 Train loss 7.417597 on epoch=54
05/21/2022 17:24:08 - INFO - __main__ - Step 120 Global step 120 Train loss 5.060439 on epoch=59
05/21/2022 17:24:10 - INFO - __main__ - Step 130 Global step 130 Train loss 6.657136 on epoch=64
05/21/2022 17:24:13 - INFO - __main__ - Step 140 Global step 140 Train loss 5.098737 on epoch=69
05/21/2022 17:24:15 - INFO - __main__ - Step 150 Global step 150 Train loss 4.558659 on epoch=74
05/21/2022 17:24:15 - INFO - __main__ - Global step 150 Train loss 5.758514 Classification-F1 0.3333333333333333 on epoch=74
05/21/2022 17:24:18 - INFO - __main__ - Step 160 Global step 160 Train loss 3.122873 on epoch=79
05/21/2022 17:24:21 - INFO - __main__ - Step 170 Global step 170 Train loss 2.377743 on epoch=84
05/21/2022 17:24:23 - INFO - __main__ - Step 180 Global step 180 Train loss 2.613099 on epoch=89
05/21/2022 17:24:26 - INFO - __main__ - Step 190 Global step 190 Train loss 1.511306 on epoch=94
05/21/2022 17:24:28 - INFO - __main__ - Step 200 Global step 200 Train loss 1.647698 on epoch=99
05/21/2022 17:24:28 - INFO - __main__ - Global step 200 Train loss 2.254544 Classification-F1 0.3333333333333333 on epoch=99
05/21/2022 17:24:31 - INFO - __main__ - Step 210 Global step 210 Train loss 2.115567 on epoch=104
05/21/2022 17:24:33 - INFO - __main__ - Step 220 Global step 220 Train loss 1.557242 on epoch=109
05/21/2022 17:24:36 - INFO - __main__ - Step 230 Global step 230 Train loss 1.292981 on epoch=114
05/21/2022 17:24:38 - INFO - __main__ - Step 240 Global step 240 Train loss 1.175394 on epoch=119
05/21/2022 17:24:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.717857 on epoch=124
05/21/2022 17:24:41 - INFO - __main__ - Global step 250 Train loss 1.371808 Classification-F1 0.8435972629521017 on epoch=124
05/21/2022 17:24:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.861736 on epoch=129
05/21/2022 17:24:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.540782 on epoch=134
05/21/2022 17:24:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.467493 on epoch=139
05/21/2022 17:24:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.423702 on epoch=144
05/21/2022 17:24:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.397855 on epoch=149
05/21/2022 17:24:54 - INFO - __main__ - Global step 300 Train loss 0.538314 Classification-F1 0.9054187192118226 on epoch=149
05/21/2022 17:24:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.321003 on epoch=154
05/21/2022 17:24:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.236681 on epoch=159
05/21/2022 17:25:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.288165 on epoch=164
05/21/2022 17:25:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.237452 on epoch=169
05/21/2022 17:25:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.508934 on epoch=174
05/21/2022 17:25:07 - INFO - __main__ - Global step 350 Train loss 0.318447 Classification-F1 0.8745098039215686 on epoch=174
05/21/2022 17:25:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.249101 on epoch=179
05/21/2022 17:25:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.277386 on epoch=184
05/21/2022 17:25:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.197821 on epoch=189
05/21/2022 17:25:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.171572 on epoch=194
05/21/2022 17:25:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.104221 on epoch=199
05/21/2022 17:25:19 - INFO - __main__ - Global step 400 Train loss 0.200020 Classification-F1 0.8745098039215686 on epoch=199
05/21/2022 17:25:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.322926 on epoch=204
05/21/2022 17:25:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.198660 on epoch=209
05/21/2022 17:25:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.123564 on epoch=214
05/21/2022 17:25:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.115225 on epoch=219
05/21/2022 17:25:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.120990 on epoch=224
05/21/2022 17:25:32 - INFO - __main__ - Global step 450 Train loss 0.176273 Classification-F1 0.8745098039215686 on epoch=224
05/21/2022 17:25:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.085395 on epoch=229
05/21/2022 17:25:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.143340 on epoch=234
05/21/2022 17:25:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.221039 on epoch=239
05/21/2022 17:25:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.199914 on epoch=244
05/21/2022 17:25:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.199085 on epoch=249
05/21/2022 17:25:44 - INFO - __main__ - Global step 500 Train loss 0.169755 Classification-F1 0.8745098039215686 on epoch=249
05/21/2022 17:25:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.137350 on epoch=254
05/21/2022 17:25:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.121160 on epoch=259
05/21/2022 17:25:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.122803 on epoch=264
05/21/2022 17:25:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.106386 on epoch=269
05/21/2022 17:25:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.135321 on epoch=274
05/21/2022 17:25:57 - INFO - __main__ - Global step 550 Train loss 0.124604 Classification-F1 0.9375 on epoch=274
05/21/2022 17:25:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.116605 on epoch=279
05/21/2022 17:26:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.040996 on epoch=284
05/21/2022 17:26:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.103227 on epoch=289
05/21/2022 17:26:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.182302 on epoch=294
05/21/2022 17:26:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.243430 on epoch=299
05/21/2022 17:26:09 - INFO - __main__ - Global step 600 Train loss 0.137312 Classification-F1 0.875 on epoch=299
05/21/2022 17:26:09 - INFO - __main__ - save last model!
05/21/2022 17:26:10 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:26:10 - INFO - __main__ - Printing 3 examples
05/21/2022 17:26:10 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/21/2022 17:26:10 - INFO - __main__ - ['positive']
05/21/2022 17:26:10 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/21/2022 17:26:10 - INFO - __main__ - ['positive']
05/21/2022 17:26:10 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/21/2022 17:26:10 - INFO - __main__ - ['positive']
05/21/2022 17:26:10 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:26:10 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:26:10 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:26:10 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:26:10 - INFO - __main__ - Printing 3 examples
05/21/2022 17:26:10 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
05/21/2022 17:26:10 - INFO - __main__ - ['positive']
05/21/2022 17:26:10 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
05/21/2022 17:26:10 - INFO - __main__ - ['positive']
05/21/2022 17:26:10 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
05/21/2022 17:26:10 - INFO - __main__ - ['positive']
05/21/2022 17:26:10 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:26:10 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:26:10 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:26:12 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:26:12 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:26:12 - INFO - __main__ - Printing 3 examples
05/21/2022 17:26:12 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:26:12 - INFO - __main__ - ['negative']
05/21/2022 17:26:12 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:26:12 - INFO - __main__ - ['negative']
05/21/2022 17:26:12 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:26:12 - INFO - __main__ - ['negative']
05/21/2022 17:26:12 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:26:13 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:26:14 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:26:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:26:14 - INFO - __main__ - Starting training!
05/21/2022 17:26:21 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_13_0.0001_8_predictions.txt
05/21/2022 17:26:21 - INFO - __main__ - Classification-F1 on test data: 0.9055
05/21/2022 17:26:21 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.0001, bsz=8, dev_performance=0.9375, test_performance=0.9054539017364297
05/21/2022 17:26:21 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0005, bsz=8 ...
05/21/2022 17:26:22 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:26:22 - INFO - __main__ - Printing 3 examples
05/21/2022 17:26:22 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/21/2022 17:26:22 - INFO - __main__ - ['positive']
05/21/2022 17:26:22 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/21/2022 17:26:22 - INFO - __main__ - ['positive']
05/21/2022 17:26:22 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/21/2022 17:26:22 - INFO - __main__ - ['positive']
05/21/2022 17:26:22 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:26:22 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:26:22 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:26:22 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:26:22 - INFO - __main__ - Printing 3 examples
05/21/2022 17:26:22 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
05/21/2022 17:26:22 - INFO - __main__ - ['positive']
05/21/2022 17:26:22 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
05/21/2022 17:26:22 - INFO - __main__ - ['positive']
05/21/2022 17:26:22 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
05/21/2022 17:26:22 - INFO - __main__ - ['positive']
05/21/2022 17:26:22 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:26:22 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:26:22 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:26:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:26:26 - INFO - __main__ - Starting training!
05/21/2022 17:26:28 - INFO - __main__ - Step 10 Global step 10 Train loss 23.253136 on epoch=4
05/21/2022 17:26:31 - INFO - __main__ - Step 20 Global step 20 Train loss 17.285347 on epoch=9
05/21/2022 17:26:33 - INFO - __main__ - Step 30 Global step 30 Train loss 9.006243 on epoch=14
05/21/2022 17:26:36 - INFO - __main__ - Step 40 Global step 40 Train loss 6.233538 on epoch=19
05/21/2022 17:26:38 - INFO - __main__ - Step 50 Global step 50 Train loss 2.832517 on epoch=24
05/21/2022 17:26:39 - INFO - __main__ - Global step 50 Train loss 11.722157 Classification-F1 0.3333333333333333 on epoch=24
05/21/2022 17:26:41 - INFO - __main__ - Step 60 Global step 60 Train loss 2.119040 on epoch=29
05/21/2022 17:26:44 - INFO - __main__ - Step 70 Global step 70 Train loss 0.718546 on epoch=34
05/21/2022 17:26:46 - INFO - __main__ - Step 80 Global step 80 Train loss 0.624286 on epoch=39
05/21/2022 17:26:49 - INFO - __main__ - Step 90 Global step 90 Train loss 0.639297 on epoch=44
05/21/2022 17:26:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.724010 on epoch=49
05/21/2022 17:26:52 - INFO - __main__ - Global step 100 Train loss 0.965036 Classification-F1 0.7333333333333334 on epoch=49
05/21/2022 17:26:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.662028 on epoch=54
05/21/2022 17:26:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.536835 on epoch=59
05/21/2022 17:27:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.428294 on epoch=64
05/21/2022 17:27:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.491666 on epoch=69
05/21/2022 17:27:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.500777 on epoch=74
05/21/2022 17:27:05 - INFO - __main__ - Global step 150 Train loss 0.523920 Classification-F1 0.5835835835835835 on epoch=74
05/21/2022 17:27:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.412771 on epoch=79
05/21/2022 17:27:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.558114 on epoch=84
05/21/2022 17:27:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.443564 on epoch=89
05/21/2022 17:27:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.517498 on epoch=94
05/21/2022 17:27:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.553436 on epoch=99
05/21/2022 17:27:18 - INFO - __main__ - Global step 200 Train loss 0.497077 Classification-F1 0.3992490613266583 on epoch=99
05/21/2022 17:27:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.426690 on epoch=104
05/21/2022 17:27:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.450987 on epoch=109
05/21/2022 17:27:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.460141 on epoch=114
05/21/2022 17:27:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.465242 on epoch=119
05/21/2022 17:27:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.421311 on epoch=124
05/21/2022 17:27:31 - INFO - __main__ - Global step 250 Train loss 0.444874 Classification-F1 0.49090909090909085 on epoch=124
05/21/2022 17:27:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.360743 on epoch=129
05/21/2022 17:27:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.405495 on epoch=134
05/21/2022 17:27:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.345838 on epoch=139
05/21/2022 17:27:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.354858 on epoch=144
05/21/2022 17:27:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.513463 on epoch=149
05/21/2022 17:27:44 - INFO - __main__ - Global step 300 Train loss 0.396079 Classification-F1 0.4909862142099682 on epoch=149
05/21/2022 17:27:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.482873 on epoch=154
05/21/2022 17:27:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.434238 on epoch=159
05/21/2022 17:27:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.410876 on epoch=164
05/21/2022 17:27:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.451521 on epoch=169
05/21/2022 17:27:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.461304 on epoch=174
05/21/2022 17:27:57 - INFO - __main__ - Global step 350 Train loss 0.448163 Classification-F1 0.3333333333333333 on epoch=174
05/21/2022 17:27:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.415657 on epoch=179
05/21/2022 17:28:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.443715 on epoch=184
05/21/2022 17:28:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.391612 on epoch=189
05/21/2022 17:28:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.494674 on epoch=194
05/21/2022 17:28:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.391322 on epoch=199
05/21/2022 17:28:10 - INFO - __main__ - Global step 400 Train loss 0.427396 Classification-F1 0.3522267206477733 on epoch=199
05/21/2022 17:28:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.381091 on epoch=204
05/21/2022 17:28:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.441488 on epoch=209
05/21/2022 17:28:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.357401 on epoch=214
05/21/2022 17:28:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.357809 on epoch=219
05/21/2022 17:28:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.380382 on epoch=224
05/21/2022 17:28:23 - INFO - __main__ - Global step 450 Train loss 0.383634 Classification-F1 0.4666666666666667 on epoch=224
05/21/2022 17:28:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.428285 on epoch=229
05/21/2022 17:28:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.412903 on epoch=234
05/21/2022 17:28:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.412757 on epoch=239
05/21/2022 17:28:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.351999 on epoch=244
05/21/2022 17:28:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.473198 on epoch=249
05/21/2022 17:28:36 - INFO - __main__ - Global step 500 Train loss 0.415828 Classification-F1 0.49090909090909085 on epoch=249
05/21/2022 17:28:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.347546 on epoch=254
05/21/2022 17:28:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.377974 on epoch=259
05/21/2022 17:28:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.318049 on epoch=264
05/21/2022 17:28:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.389285 on epoch=269
05/21/2022 17:28:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.383316 on epoch=274
05/21/2022 17:28:49 - INFO - __main__ - Global step 550 Train loss 0.363234 Classification-F1 0.3333333333333333 on epoch=274
05/21/2022 17:28:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.309052 on epoch=279
05/21/2022 17:28:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.377269 on epoch=284
05/21/2022 17:28:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.324798 on epoch=289
05/21/2022 17:28:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.326514 on epoch=294
05/21/2022 17:29:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.325387 on epoch=299
05/21/2022 17:29:02 - INFO - __main__ - Global step 600 Train loss 0.332604 Classification-F1 0.6113360323886641 on epoch=299
05/21/2022 17:29:02 - INFO - __main__ - save last model!
05/21/2022 17:29:03 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:29:03 - INFO - __main__ - Printing 3 examples
05/21/2022 17:29:03 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/21/2022 17:29:03 - INFO - __main__ - ['positive']
05/21/2022 17:29:03 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/21/2022 17:29:03 - INFO - __main__ - ['positive']
05/21/2022 17:29:03 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/21/2022 17:29:03 - INFO - __main__ - ['positive']
05/21/2022 17:29:03 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:29:03 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:29:03 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:29:03 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:29:03 - INFO - __main__ - Printing 3 examples
05/21/2022 17:29:03 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
05/21/2022 17:29:03 - INFO - __main__ - ['positive']
05/21/2022 17:29:03 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
05/21/2022 17:29:03 - INFO - __main__ - ['positive']
05/21/2022 17:29:03 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
05/21/2022 17:29:03 - INFO - __main__ - ['positive']
05/21/2022 17:29:03 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:29:03 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:29:03 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:29:04 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:29:05 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:29:05 - INFO - __main__ - Printing 3 examples
05/21/2022 17:29:05 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:29:05 - INFO - __main__ - ['negative']
05/21/2022 17:29:05 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:29:05 - INFO - __main__ - ['negative']
05/21/2022 17:29:05 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:29:05 - INFO - __main__ - ['negative']
05/21/2022 17:29:05 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:29:05 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:29:06 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:29:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:29:08 - INFO - __main__ - Starting training!
05/21/2022 17:29:13 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_21_0.0005_8_predictions.txt
05/21/2022 17:29:13 - INFO - __main__ - Classification-F1 on test data: 0.6672
05/21/2022 17:29:13 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0005, bsz=8, dev_performance=0.7333333333333334, test_performance=0.667237325463788
05/21/2022 17:29:13 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0003, bsz=8 ...
05/21/2022 17:29:14 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:29:14 - INFO - __main__ - Printing 3 examples
05/21/2022 17:29:14 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/21/2022 17:29:14 - INFO - __main__ - ['positive']
05/21/2022 17:29:14 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/21/2022 17:29:14 - INFO - __main__ - ['positive']
05/21/2022 17:29:14 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/21/2022 17:29:14 - INFO - __main__ - ['positive']
05/21/2022 17:29:14 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:29:14 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:29:14 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:29:14 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:29:14 - INFO - __main__ - Printing 3 examples
05/21/2022 17:29:14 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
05/21/2022 17:29:14 - INFO - __main__ - ['positive']
05/21/2022 17:29:14 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
05/21/2022 17:29:14 - INFO - __main__ - ['positive']
05/21/2022 17:29:14 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
05/21/2022 17:29:14 - INFO - __main__ - ['positive']
05/21/2022 17:29:14 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:29:14 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:29:14 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:29:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:29:18 - INFO - __main__ - Starting training!
05/21/2022 17:29:20 - INFO - __main__ - Step 10 Global step 10 Train loss 23.075850 on epoch=4
05/21/2022 17:29:23 - INFO - __main__ - Step 20 Global step 20 Train loss 15.001192 on epoch=9
05/21/2022 17:29:25 - INFO - __main__ - Step 30 Global step 30 Train loss 11.753180 on epoch=14
05/21/2022 17:29:28 - INFO - __main__ - Step 40 Global step 40 Train loss 9.253186 on epoch=19
05/21/2022 17:29:30 - INFO - __main__ - Step 50 Global step 50 Train loss 5.902024 on epoch=24
05/21/2022 17:29:31 - INFO - __main__ - Global step 50 Train loss 12.997087 Classification-F1 0.3333333333333333 on epoch=24
05/21/2022 17:29:33 - INFO - __main__ - Step 60 Global step 60 Train loss 3.771093 on epoch=29
05/21/2022 17:29:36 - INFO - __main__ - Step 70 Global step 70 Train loss 2.323153 on epoch=34
05/21/2022 17:29:39 - INFO - __main__ - Step 80 Global step 80 Train loss 1.243892 on epoch=39
05/21/2022 17:29:41 - INFO - __main__ - Step 90 Global step 90 Train loss 0.622413 on epoch=44
05/21/2022 17:29:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.803816 on epoch=49
05/21/2022 17:29:44 - INFO - __main__ - Global step 100 Train loss 1.752873 Classification-F1 0.9687194525904204 on epoch=49
05/21/2022 17:29:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.509007 on epoch=54
05/21/2022 17:29:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.513864 on epoch=59
05/21/2022 17:29:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.489448 on epoch=64
05/21/2022 17:29:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.368396 on epoch=69
05/21/2022 17:29:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.335294 on epoch=74
05/21/2022 17:29:57 - INFO - __main__ - Global step 150 Train loss 0.443202 Classification-F1 0.9372549019607843 on epoch=74
05/21/2022 17:30:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.477575 on epoch=79
05/21/2022 17:30:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.328307 on epoch=84
05/21/2022 17:30:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.337549 on epoch=89
05/21/2022 17:30:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.377245 on epoch=94
05/21/2022 17:30:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.369957 on epoch=99
05/21/2022 17:30:10 - INFO - __main__ - Global step 200 Train loss 0.378127 Classification-F1 0.9054187192118226 on epoch=99
05/21/2022 17:30:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.386210 on epoch=104
05/21/2022 17:30:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.302200 on epoch=109
05/21/2022 17:30:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.300163 on epoch=114
05/21/2022 17:30:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.370618 on epoch=119
05/21/2022 17:30:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.471800 on epoch=124
05/21/2022 17:30:23 - INFO - __main__ - Global step 250 Train loss 0.366198 Classification-F1 0.8095238095238095 on epoch=124
05/21/2022 17:30:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.380883 on epoch=129
05/21/2022 17:30:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.451796 on epoch=134
05/21/2022 17:30:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.370143 on epoch=139
05/21/2022 17:30:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.240117 on epoch=144
05/21/2022 17:30:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.326774 on epoch=149
05/21/2022 17:30:36 - INFO - __main__ - Global step 300 Train loss 0.353943 Classification-F1 0.9054187192118226 on epoch=149
05/21/2022 17:30:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.344938 on epoch=154
05/21/2022 17:30:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.367750 on epoch=159
05/21/2022 17:30:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.318263 on epoch=164
05/21/2022 17:30:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.326285 on epoch=169
05/21/2022 17:30:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.287097 on epoch=174
05/21/2022 17:30:50 - INFO - __main__ - Global step 350 Train loss 0.328867 Classification-F1 0.7490196078431373 on epoch=174
05/21/2022 17:30:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.266275 on epoch=179
05/21/2022 17:30:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.288699 on epoch=184
05/21/2022 17:30:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.188107 on epoch=189
05/21/2022 17:31:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.144709 on epoch=194
05/21/2022 17:31:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.141774 on epoch=199
05/21/2022 17:31:02 - INFO - __main__ - Global step 400 Train loss 0.205913 Classification-F1 0.8117647058823529 on epoch=199
05/21/2022 17:31:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.110848 on epoch=204
05/21/2022 17:31:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.176652 on epoch=209
05/21/2022 17:31:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.147033 on epoch=214
05/21/2022 17:31:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.201129 on epoch=219
05/21/2022 17:31:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.151801 on epoch=224
05/21/2022 17:31:15 - INFO - __main__ - Global step 450 Train loss 0.157493 Classification-F1 0.875 on epoch=224
05/21/2022 17:31:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.172355 on epoch=229
05/21/2022 17:31:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.231924 on epoch=234
05/21/2022 17:31:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.126590 on epoch=239
05/21/2022 17:31:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.129253 on epoch=244
05/21/2022 17:31:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.056857 on epoch=249
05/21/2022 17:31:28 - INFO - __main__ - Global step 500 Train loss 0.143396 Classification-F1 0.9372549019607843 on epoch=249
05/21/2022 17:31:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.039393 on epoch=254
05/21/2022 17:31:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.041361 on epoch=259
05/21/2022 17:31:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.028053 on epoch=264
05/21/2022 17:31:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.046831 on epoch=269
05/21/2022 17:31:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.014734 on epoch=274
05/21/2022 17:31:41 - INFO - __main__ - Global step 550 Train loss 0.034075 Classification-F1 0.9054187192118226 on epoch=274
05/21/2022 17:31:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.007099 on epoch=279
05/21/2022 17:31:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.020892 on epoch=284
05/21/2022 17:31:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.016450 on epoch=289
05/21/2022 17:31:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.010869 on epoch=294
05/21/2022 17:31:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.002383 on epoch=299
05/21/2022 17:31:54 - INFO - __main__ - Global step 600 Train loss 0.011539 Classification-F1 0.9372549019607843 on epoch=299
05/21/2022 17:31:54 - INFO - __main__ - save last model!
05/21/2022 17:31:55 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:31:55 - INFO - __main__ - Printing 3 examples
05/21/2022 17:31:55 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/21/2022 17:31:55 - INFO - __main__ - ['positive']
05/21/2022 17:31:55 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/21/2022 17:31:55 - INFO - __main__ - ['positive']
05/21/2022 17:31:55 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/21/2022 17:31:55 - INFO - __main__ - ['positive']
05/21/2022 17:31:55 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:31:55 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:31:55 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:31:55 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:31:55 - INFO - __main__ - Printing 3 examples
05/21/2022 17:31:55 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
05/21/2022 17:31:55 - INFO - __main__ - ['positive']
05/21/2022 17:31:55 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
05/21/2022 17:31:55 - INFO - __main__ - ['positive']
05/21/2022 17:31:55 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
05/21/2022 17:31:55 - INFO - __main__ - ['positive']
05/21/2022 17:31:55 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:31:55 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:31:55 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:31:57 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:31:57 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:31:57 - INFO - __main__ - Printing 3 examples
05/21/2022 17:31:57 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:31:57 - INFO - __main__ - ['negative']
05/21/2022 17:31:57 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:31:57 - INFO - __main__ - ['negative']
05/21/2022 17:31:57 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:31:57 - INFO - __main__ - ['negative']
05/21/2022 17:31:57 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:31:58 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:31:59 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:32:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:32:00 - INFO - __main__ - Starting training!
05/21/2022 17:32:05 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_21_0.0003_8_predictions.txt
05/21/2022 17:32:05 - INFO - __main__ - Classification-F1 on test data: 0.9320
05/21/2022 17:32:05 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0003, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9319997279989121
05/21/2022 17:32:05 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0002, bsz=8 ...
05/21/2022 17:32:06 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:32:06 - INFO - __main__ - Printing 3 examples
05/21/2022 17:32:06 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/21/2022 17:32:06 - INFO - __main__ - ['positive']
05/21/2022 17:32:06 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/21/2022 17:32:06 - INFO - __main__ - ['positive']
05/21/2022 17:32:06 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/21/2022 17:32:06 - INFO - __main__ - ['positive']
05/21/2022 17:32:06 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:32:06 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:32:06 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:32:06 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:32:06 - INFO - __main__ - Printing 3 examples
05/21/2022 17:32:06 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
05/21/2022 17:32:06 - INFO - __main__ - ['positive']
05/21/2022 17:32:06 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
05/21/2022 17:32:06 - INFO - __main__ - ['positive']
05/21/2022 17:32:06 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
05/21/2022 17:32:06 - INFO - __main__ - ['positive']
05/21/2022 17:32:06 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:32:06 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:32:06 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:32:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:32:10 - INFO - __main__ - Starting training!
05/21/2022 17:32:12 - INFO - __main__ - Step 10 Global step 10 Train loss 24.045040 on epoch=4
05/21/2022 17:32:15 - INFO - __main__ - Step 20 Global step 20 Train loss 19.661203 on epoch=9
05/21/2022 17:32:17 - INFO - __main__ - Step 30 Global step 30 Train loss 14.514160 on epoch=14
05/21/2022 17:32:20 - INFO - __main__ - Step 40 Global step 40 Train loss 12.023011 on epoch=19
05/21/2022 17:32:22 - INFO - __main__ - Step 50 Global step 50 Train loss 8.357065 on epoch=24
05/21/2022 17:32:25 - INFO - __main__ - Global step 50 Train loss 15.720096 Classification-F1 0.0 on epoch=24
05/21/2022 17:32:28 - INFO - __main__ - Step 60 Global step 60 Train loss 7.699440 on epoch=29
05/21/2022 17:32:30 - INFO - __main__ - Step 70 Global step 70 Train loss 6.204771 on epoch=34
05/21/2022 17:32:33 - INFO - __main__ - Step 80 Global step 80 Train loss 4.678150 on epoch=39
05/21/2022 17:32:35 - INFO - __main__ - Step 90 Global step 90 Train loss 3.621038 on epoch=44
05/21/2022 17:32:38 - INFO - __main__ - Step 100 Global step 100 Train loss 2.611035 on epoch=49
05/21/2022 17:32:38 - INFO - __main__ - Global step 100 Train loss 4.962887 Classification-F1 0.3333333333333333 on epoch=49
05/21/2022 17:32:41 - INFO - __main__ - Step 110 Global step 110 Train loss 2.406062 on epoch=54
05/21/2022 17:32:43 - INFO - __main__ - Step 120 Global step 120 Train loss 1.364316 on epoch=59
05/21/2022 17:32:46 - INFO - __main__ - Step 130 Global step 130 Train loss 1.130327 on epoch=64
05/21/2022 17:32:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.879169 on epoch=69
05/21/2022 17:32:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.939974 on epoch=74
05/21/2022 17:32:51 - INFO - __main__ - Global step 150 Train loss 1.343970 Classification-F1 0.9054187192118226 on epoch=74
05/21/2022 17:32:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.682717 on epoch=79
05/21/2022 17:32:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.496115 on epoch=84
05/21/2022 17:32:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.616383 on epoch=89
05/21/2022 17:33:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.550101 on epoch=94
05/21/2022 17:33:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.648756 on epoch=99
05/21/2022 17:33:04 - INFO - __main__ - Global step 200 Train loss 0.598815 Classification-F1 0.8398398398398398 on epoch=99
05/21/2022 17:33:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.462276 on epoch=104
05/21/2022 17:33:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.408158 on epoch=109
05/21/2022 17:33:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.453879 on epoch=114
05/21/2022 17:33:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.444928 on epoch=119
05/21/2022 17:33:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.417622 on epoch=124
05/21/2022 17:33:17 - INFO - __main__ - Global step 250 Train loss 0.437373 Classification-F1 0.805668016194332 on epoch=124
05/21/2022 17:33:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.404397 on epoch=129
05/21/2022 17:33:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.398747 on epoch=134
05/21/2022 17:33:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.423018 on epoch=139
05/21/2022 17:33:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.359888 on epoch=144
05/21/2022 17:33:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.475722 on epoch=149
05/21/2022 17:33:30 - INFO - __main__ - Global step 300 Train loss 0.412354 Classification-F1 0.9372549019607843 on epoch=149
05/21/2022 17:33:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.202138 on epoch=154
05/21/2022 17:33:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.388624 on epoch=159
05/21/2022 17:33:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.258332 on epoch=164
05/21/2022 17:33:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.254199 on epoch=169
05/21/2022 17:33:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.287523 on epoch=174
05/21/2022 17:33:43 - INFO - __main__ - Global step 350 Train loss 0.278163 Classification-F1 0.9687194525904204 on epoch=174
05/21/2022 17:33:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.157123 on epoch=179
05/21/2022 17:33:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.242332 on epoch=184
05/21/2022 17:33:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.249132 on epoch=189
05/21/2022 17:33:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.218095 on epoch=194
05/21/2022 17:33:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.133686 on epoch=199
05/21/2022 17:33:56 - INFO - __main__ - Global step 400 Train loss 0.200073 Classification-F1 0.9687194525904204 on epoch=199
05/21/2022 17:33:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.217790 on epoch=204
05/21/2022 17:34:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.137326 on epoch=209
05/21/2022 17:34:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.214168 on epoch=214
05/21/2022 17:34:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.194538 on epoch=219
05/21/2022 17:34:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.147789 on epoch=224
05/21/2022 17:34:09 - INFO - __main__ - Global step 450 Train loss 0.182322 Classification-F1 0.9372549019607843 on epoch=224
05/21/2022 17:34:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.093911 on epoch=229
05/21/2022 17:34:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.144808 on epoch=234
05/21/2022 17:34:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.089197 on epoch=239
05/21/2022 17:34:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.189413 on epoch=244
05/21/2022 17:34:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.049132 on epoch=249
05/21/2022 17:34:22 - INFO - __main__ - Global step 500 Train loss 0.113292 Classification-F1 0.9687194525904204 on epoch=249
05/21/2022 17:34:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.092419 on epoch=254
05/21/2022 17:34:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.147853 on epoch=259
05/21/2022 17:34:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.107623 on epoch=264
05/21/2022 17:34:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.097265 on epoch=269
05/21/2022 17:34:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.055877 on epoch=274
05/21/2022 17:34:34 - INFO - __main__ - Global step 550 Train loss 0.100207 Classification-F1 0.9372549019607843 on epoch=274
05/21/2022 17:34:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.075820 on epoch=279
05/21/2022 17:34:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.045705 on epoch=284
05/21/2022 17:34:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.197463 on epoch=289
05/21/2022 17:34:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.038309 on epoch=294
05/21/2022 17:34:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.041223 on epoch=299
05/21/2022 17:34:47 - INFO - __main__ - Global step 600 Train loss 0.079704 Classification-F1 0.9372549019607843 on epoch=299
05/21/2022 17:34:47 - INFO - __main__ - save last model!
05/21/2022 17:34:48 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:34:48 - INFO - __main__ - Printing 3 examples
05/21/2022 17:34:48 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/21/2022 17:34:48 - INFO - __main__ - ['positive']
05/21/2022 17:34:48 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/21/2022 17:34:48 - INFO - __main__ - ['positive']
05/21/2022 17:34:48 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/21/2022 17:34:48 - INFO - __main__ - ['positive']
05/21/2022 17:34:48 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:34:48 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:34:48 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:34:48 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:34:48 - INFO - __main__ - Printing 3 examples
05/21/2022 17:34:48 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
05/21/2022 17:34:48 - INFO - __main__ - ['positive']
05/21/2022 17:34:48 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
05/21/2022 17:34:48 - INFO - __main__ - ['positive']
05/21/2022 17:34:48 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
05/21/2022 17:34:48 - INFO - __main__ - ['positive']
05/21/2022 17:34:48 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:34:48 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:34:48 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:34:50 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:34:50 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:34:50 - INFO - __main__ - Printing 3 examples
05/21/2022 17:34:50 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:34:50 - INFO - __main__ - ['negative']
05/21/2022 17:34:50 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:34:50 - INFO - __main__ - ['negative']
05/21/2022 17:34:50 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:34:50 - INFO - __main__ - ['negative']
05/21/2022 17:34:50 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:34:51 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:34:52 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:34:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:34:52 - INFO - __main__ - Starting training!
05/21/2022 17:34:58 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_21_0.0002_8_predictions.txt
05/21/2022 17:34:58 - INFO - __main__ - Classification-F1 on test data: 0.9137
05/21/2022 17:34:58 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0002, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9137294555726758
05/21/2022 17:34:58 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.0001, bsz=8 ...
05/21/2022 17:34:59 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:34:59 - INFO - __main__ - Printing 3 examples
05/21/2022 17:34:59 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
05/21/2022 17:34:59 - INFO - __main__ - ['positive']
05/21/2022 17:34:59 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
05/21/2022 17:34:59 - INFO - __main__ - ['positive']
05/21/2022 17:34:59 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
05/21/2022 17:34:59 - INFO - __main__ - ['positive']
05/21/2022 17:34:59 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:34:59 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:34:59 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:34:59 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:34:59 - INFO - __main__ - Printing 3 examples
05/21/2022 17:34:59 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
05/21/2022 17:34:59 - INFO - __main__ - ['positive']
05/21/2022 17:34:59 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
05/21/2022 17:34:59 - INFO - __main__ - ['positive']
05/21/2022 17:34:59 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
05/21/2022 17:34:59 - INFO - __main__ - ['positive']
05/21/2022 17:34:59 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:34:59 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:34:59 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:35:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:35:03 - INFO - __main__ - Starting training!
05/21/2022 17:35:05 - INFO - __main__ - Step 10 Global step 10 Train loss 23.104008 on epoch=4
05/21/2022 17:35:08 - INFO - __main__ - Step 20 Global step 20 Train loss 21.507864 on epoch=9
05/21/2022 17:35:10 - INFO - __main__ - Step 30 Global step 30 Train loss 17.235716 on epoch=14
05/21/2022 17:35:13 - INFO - __main__ - Step 40 Global step 40 Train loss 16.364555 on epoch=19
05/21/2022 17:35:15 - INFO - __main__ - Step 50 Global step 50 Train loss 14.133166 on epoch=24
05/21/2022 17:35:19 - INFO - __main__ - Global step 50 Train loss 18.469061 Classification-F1 0.0 on epoch=24
05/21/2022 17:35:22 - INFO - __main__ - Step 60 Global step 60 Train loss 11.753659 on epoch=29
05/21/2022 17:35:25 - INFO - __main__ - Step 70 Global step 70 Train loss 11.463918 on epoch=34
05/21/2022 17:35:27 - INFO - __main__ - Step 80 Global step 80 Train loss 11.613515 on epoch=39
05/21/2022 17:35:30 - INFO - __main__ - Step 90 Global step 90 Train loss 9.862890 on epoch=44
05/21/2022 17:35:33 - INFO - __main__ - Step 100 Global step 100 Train loss 8.322186 on epoch=49
05/21/2022 17:35:35 - INFO - __main__ - Global step 100 Train loss 10.603232 Classification-F1 0.0 on epoch=49
05/21/2022 17:35:38 - INFO - __main__ - Step 110 Global step 110 Train loss 8.450625 on epoch=54
05/21/2022 17:35:41 - INFO - __main__ - Step 120 Global step 120 Train loss 7.064596 on epoch=59
05/21/2022 17:35:44 - INFO - __main__ - Step 130 Global step 130 Train loss 6.820754 on epoch=64
05/21/2022 17:35:46 - INFO - __main__ - Step 140 Global step 140 Train loss 6.245649 on epoch=69
05/21/2022 17:35:49 - INFO - __main__ - Step 150 Global step 150 Train loss 4.841098 on epoch=74
05/21/2022 17:35:49 - INFO - __main__ - Global step 150 Train loss 6.684544 Classification-F1 0.3333333333333333 on epoch=74
05/21/2022 17:35:52 - INFO - __main__ - Step 160 Global step 160 Train loss 3.569440 on epoch=79
05/21/2022 17:35:54 - INFO - __main__ - Step 170 Global step 170 Train loss 3.195032 on epoch=84
05/21/2022 17:35:57 - INFO - __main__ - Step 180 Global step 180 Train loss 2.557412 on epoch=89
05/21/2022 17:36:00 - INFO - __main__ - Step 190 Global step 190 Train loss 3.054714 on epoch=94
05/21/2022 17:36:02 - INFO - __main__ - Step 200 Global step 200 Train loss 2.012224 on epoch=99
05/21/2022 17:36:02 - INFO - __main__ - Global step 200 Train loss 2.877764 Classification-F1 0.3333333333333333 on epoch=99
05/21/2022 17:36:05 - INFO - __main__ - Step 210 Global step 210 Train loss 2.177314 on epoch=104
05/21/2022 17:36:07 - INFO - __main__ - Step 220 Global step 220 Train loss 1.366000 on epoch=109
05/21/2022 17:36:10 - INFO - __main__ - Step 230 Global step 230 Train loss 1.097214 on epoch=114
05/21/2022 17:36:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.892595 on epoch=119
05/21/2022 17:36:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.808653 on epoch=124
05/21/2022 17:36:15 - INFO - __main__ - Global step 250 Train loss 1.268355 Classification-F1 0.9054187192118226 on epoch=124
05/21/2022 17:36:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.728233 on epoch=129
05/21/2022 17:36:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.661548 on epoch=134
05/21/2022 17:36:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.606353 on epoch=139
05/21/2022 17:36:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.671822 on epoch=144
05/21/2022 17:36:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.657460 on epoch=149
05/21/2022 17:36:29 - INFO - __main__ - Global step 300 Train loss 0.665083 Classification-F1 0.9372549019607843 on epoch=149
05/21/2022 17:36:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.510883 on epoch=154
05/21/2022 17:36:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.361170 on epoch=159
05/21/2022 17:36:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.773926 on epoch=164
05/21/2022 17:36:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.432750 on epoch=169
05/21/2022 17:36:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.372284 on epoch=174
05/21/2022 17:36:42 - INFO - __main__ - Global step 350 Train loss 0.490202 Classification-F1 0.873015873015873 on epoch=174
05/21/2022 17:36:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.463949 on epoch=179
05/21/2022 17:36:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.343288 on epoch=184
05/21/2022 17:36:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.346857 on epoch=189
05/21/2022 17:36:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.379962 on epoch=194
05/21/2022 17:36:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.528462 on epoch=199
05/21/2022 17:36:54 - INFO - __main__ - Global step 400 Train loss 0.412503 Classification-F1 0.9372549019607843 on epoch=199
05/21/2022 17:36:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.355990 on epoch=204
05/21/2022 17:36:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.447301 on epoch=209
05/21/2022 17:37:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.300874 on epoch=214
05/21/2022 17:37:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.316831 on epoch=219
05/21/2022 17:37:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.409797 on epoch=224
05/21/2022 17:37:07 - INFO - __main__ - Global step 450 Train loss 0.366159 Classification-F1 0.9372549019607843 on epoch=224
05/21/2022 17:37:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.319127 on epoch=229
05/21/2022 17:37:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.223502 on epoch=234
05/21/2022 17:37:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.297325 on epoch=239
05/21/2022 17:37:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.233650 on epoch=244
05/21/2022 17:37:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.222174 on epoch=249
05/21/2022 17:37:20 - INFO - __main__ - Global step 500 Train loss 0.259156 Classification-F1 0.9372549019607843 on epoch=249
05/21/2022 17:37:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.285676 on epoch=254
05/21/2022 17:37:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.181748 on epoch=259
05/21/2022 17:37:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.192369 on epoch=264
05/21/2022 17:37:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.210949 on epoch=269
05/21/2022 17:37:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.155525 on epoch=274
05/21/2022 17:37:33 - INFO - __main__ - Global step 550 Train loss 0.205253 Classification-F1 0.9372549019607843 on epoch=274
05/21/2022 17:37:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.213828 on epoch=279
05/21/2022 17:37:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.222191 on epoch=284
05/21/2022 17:37:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.218149 on epoch=289
05/21/2022 17:37:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.188681 on epoch=294
05/21/2022 17:37:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.259504 on epoch=299
05/21/2022 17:37:46 - INFO - __main__ - Global step 600 Train loss 0.220471 Classification-F1 0.9372549019607843 on epoch=299
05/21/2022 17:37:46 - INFO - __main__ - save last model!
05/21/2022 17:37:47 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:37:47 - INFO - __main__ - Printing 3 examples
05/21/2022 17:37:47 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/21/2022 17:37:47 - INFO - __main__ - ['negative']
05/21/2022 17:37:47 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/21/2022 17:37:47 - INFO - __main__ - ['negative']
05/21/2022 17:37:47 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/21/2022 17:37:47 - INFO - __main__ - ['negative']
05/21/2022 17:37:47 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:37:47 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:37:47 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:37:47 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:37:47 - INFO - __main__ - Printing 3 examples
05/21/2022 17:37:47 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
05/21/2022 17:37:47 - INFO - __main__ - ['negative']
05/21/2022 17:37:47 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
05/21/2022 17:37:47 - INFO - __main__ - ['negative']
05/21/2022 17:37:47 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
05/21/2022 17:37:47 - INFO - __main__ - ['negative']
05/21/2022 17:37:47 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:37:47 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:37:47 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:37:49 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:37:49 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:37:49 - INFO - __main__ - Printing 3 examples
05/21/2022 17:37:49 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:37:49 - INFO - __main__ - ['negative']
05/21/2022 17:37:49 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:37:49 - INFO - __main__ - ['negative']
05/21/2022 17:37:49 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:37:49 - INFO - __main__ - ['negative']
05/21/2022 17:37:49 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:37:50 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:37:51 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:37:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:37:51 - INFO - __main__ - Starting training!
05/21/2022 17:37:57 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_21_0.0001_8_predictions.txt
05/21/2022 17:37:57 - INFO - __main__ - Classification-F1 on test data: 0.8788
05/21/2022 17:37:57 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.0001, bsz=8, dev_performance=0.9372549019607843, test_performance=0.8788363442501786
05/21/2022 17:37:57 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0005, bsz=8 ...
05/21/2022 17:37:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:37:58 - INFO - __main__ - Printing 3 examples
05/21/2022 17:37:58 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/21/2022 17:37:58 - INFO - __main__ - ['negative']
05/21/2022 17:37:58 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/21/2022 17:37:58 - INFO - __main__ - ['negative']
05/21/2022 17:37:58 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/21/2022 17:37:58 - INFO - __main__ - ['negative']
05/21/2022 17:37:58 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:37:58 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:37:58 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:37:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:37:58 - INFO - __main__ - Printing 3 examples
05/21/2022 17:37:58 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
05/21/2022 17:37:58 - INFO - __main__ - ['negative']
05/21/2022 17:37:58 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
05/21/2022 17:37:58 - INFO - __main__ - ['negative']
05/21/2022 17:37:58 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
05/21/2022 17:37:58 - INFO - __main__ - ['negative']
05/21/2022 17:37:58 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:37:58 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:37:58 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:38:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:38:02 - INFO - __main__ - Starting training!
05/21/2022 17:38:04 - INFO - __main__ - Step 10 Global step 10 Train loss 22.337944 on epoch=4
05/21/2022 17:38:07 - INFO - __main__ - Step 20 Global step 20 Train loss 15.356854 on epoch=9
05/21/2022 17:38:09 - INFO - __main__ - Step 30 Global step 30 Train loss 10.307205 on epoch=14
05/21/2022 17:38:12 - INFO - __main__ - Step 40 Global step 40 Train loss 7.150568 on epoch=19
05/21/2022 17:38:14 - INFO - __main__ - Step 50 Global step 50 Train loss 2.875314 on epoch=24
05/21/2022 17:38:15 - INFO - __main__ - Global step 50 Train loss 11.605577 Classification-F1 0.3333333333333333 on epoch=24
05/21/2022 17:38:17 - INFO - __main__ - Step 60 Global step 60 Train loss 1.627203 on epoch=29
05/21/2022 17:38:20 - INFO - __main__ - Step 70 Global step 70 Train loss 0.864053 on epoch=34
05/21/2022 17:38:22 - INFO - __main__ - Step 80 Global step 80 Train loss 0.722600 on epoch=39
05/21/2022 17:38:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.515666 on epoch=44
05/21/2022 17:38:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.452131 on epoch=49
05/21/2022 17:38:28 - INFO - __main__ - Global step 100 Train loss 0.836330 Classification-F1 0.5844155844155844 on epoch=49
05/21/2022 17:38:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.525319 on epoch=54
05/21/2022 17:38:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.581154 on epoch=59
05/21/2022 17:38:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.522317 on epoch=64
05/21/2022 17:38:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.540084 on epoch=69
05/21/2022 17:38:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.476987 on epoch=74
05/21/2022 17:38:41 - INFO - __main__ - Global step 150 Train loss 0.529172 Classification-F1 0.3191489361702127 on epoch=74
05/21/2022 17:38:44 - INFO - __main__ - Step 160 Global step 160 Train loss 0.490449 on epoch=79
05/21/2022 17:38:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.563897 on epoch=84
05/21/2022 17:38:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.509253 on epoch=89
05/21/2022 17:38:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.446416 on epoch=94
05/21/2022 17:38:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.506881 on epoch=99
05/21/2022 17:38:54 - INFO - __main__ - Global step 200 Train loss 0.503379 Classification-F1 0.3333333333333333 on epoch=99
05/21/2022 17:38:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.339115 on epoch=104
05/21/2022 17:38:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.411783 on epoch=109
05/21/2022 17:39:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.452038 on epoch=114
05/21/2022 17:39:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.470480 on epoch=119
05/21/2022 17:39:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.434519 on epoch=124
05/21/2022 17:39:07 - INFO - __main__ - Global step 250 Train loss 0.421587 Classification-F1 0.5636363636363637 on epoch=124
05/21/2022 17:39:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.482151 on epoch=129
05/21/2022 17:39:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.494278 on epoch=134
05/21/2022 17:39:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.457343 on epoch=139
05/21/2022 17:39:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.425744 on epoch=144
05/21/2022 17:39:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.400701 on epoch=149
05/21/2022 17:39:20 - INFO - __main__ - Global step 300 Train loss 0.452044 Classification-F1 0.3816425120772947 on epoch=149
05/21/2022 17:39:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.421994 on epoch=154
05/21/2022 17:39:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.436395 on epoch=159
05/21/2022 17:39:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.433824 on epoch=164
05/21/2022 17:39:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.370004 on epoch=169
05/21/2022 17:39:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.466582 on epoch=174
05/21/2022 17:39:33 - INFO - __main__ - Global step 350 Train loss 0.425760 Classification-F1 0.6825396825396826 on epoch=174
05/21/2022 17:39:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.343293 on epoch=179
05/21/2022 17:39:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.396725 on epoch=184
05/21/2022 17:39:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.338535 on epoch=189
05/21/2022 17:39:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.372665 on epoch=194
05/21/2022 17:39:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.402821 on epoch=199
05/21/2022 17:39:47 - INFO - __main__ - Global step 400 Train loss 0.370808 Classification-F1 0.5636363636363637 on epoch=199
05/21/2022 17:39:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.387883 on epoch=204
05/21/2022 17:39:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.325749 on epoch=209
05/21/2022 17:39:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.360252 on epoch=214
05/21/2022 17:39:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.385518 on epoch=219
05/21/2022 17:39:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.373026 on epoch=224
05/21/2022 17:40:00 - INFO - __main__ - Global step 450 Train loss 0.366486 Classification-F1 0.716256157635468 on epoch=224
05/21/2022 17:40:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.403867 on epoch=229
05/21/2022 17:40:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.388849 on epoch=234
05/21/2022 17:40:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.360272 on epoch=239
05/21/2022 17:40:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.412811 on epoch=244
05/21/2022 17:40:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.389336 on epoch=249
05/21/2022 17:40:13 - INFO - __main__ - Global step 500 Train loss 0.391027 Classification-F1 0.5076923076923077 on epoch=249
05/21/2022 17:40:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.336195 on epoch=254
05/21/2022 17:40:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.387607 on epoch=259
05/21/2022 17:40:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.412364 on epoch=264
05/21/2022 17:40:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.352906 on epoch=269
05/21/2022 17:40:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.335379 on epoch=274
05/21/2022 17:40:26 - INFO - __main__ - Global step 550 Train loss 0.364890 Classification-F1 0.5844155844155844 on epoch=274
05/21/2022 17:40:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.336188 on epoch=279
05/21/2022 17:40:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.359402 on epoch=284
05/21/2022 17:40:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.343791 on epoch=289
05/21/2022 17:40:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.330145 on epoch=294
05/21/2022 17:40:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.363235 on epoch=299
05/21/2022 17:40:39 - INFO - __main__ - Global step 600 Train loss 0.346552 Classification-F1 0.4589371980676329 on epoch=299
05/21/2022 17:40:39 - INFO - __main__ - save last model!
05/21/2022 17:40:40 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:40:40 - INFO - __main__ - Printing 3 examples
05/21/2022 17:40:40 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/21/2022 17:40:40 - INFO - __main__ - ['negative']
05/21/2022 17:40:40 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/21/2022 17:40:40 - INFO - __main__ - ['negative']
05/21/2022 17:40:40 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/21/2022 17:40:40 - INFO - __main__ - ['negative']
05/21/2022 17:40:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:40:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:40:40 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:40:40 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:40:40 - INFO - __main__ - Printing 3 examples
05/21/2022 17:40:40 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
05/21/2022 17:40:40 - INFO - __main__ - ['negative']
05/21/2022 17:40:40 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
05/21/2022 17:40:40 - INFO - __main__ - ['negative']
05/21/2022 17:40:40 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
05/21/2022 17:40:40 - INFO - __main__ - ['negative']
05/21/2022 17:40:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:40:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:40:40 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:40:42 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:40:42 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:40:42 - INFO - __main__ - Printing 3 examples
05/21/2022 17:40:42 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:40:42 - INFO - __main__ - ['negative']
05/21/2022 17:40:42 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:40:42 - INFO - __main__ - ['negative']
05/21/2022 17:40:42 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:40:42 - INFO - __main__ - ['negative']
05/21/2022 17:40:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:40:43 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:40:44 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:40:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:40:44 - INFO - __main__ - Starting training!
05/21/2022 17:40:50 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_42_0.0005_8_predictions.txt
05/21/2022 17:40:50 - INFO - __main__ - Classification-F1 on test data: 0.5702
05/21/2022 17:40:50 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0005, bsz=8, dev_performance=0.716256157635468, test_performance=0.5702397018186491
05/21/2022 17:40:51 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0003, bsz=8 ...
05/21/2022 17:40:51 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:40:51 - INFO - __main__ - Printing 3 examples
05/21/2022 17:40:51 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/21/2022 17:40:51 - INFO - __main__ - ['negative']
05/21/2022 17:40:51 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/21/2022 17:40:51 - INFO - __main__ - ['negative']
05/21/2022 17:40:51 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/21/2022 17:40:51 - INFO - __main__ - ['negative']
05/21/2022 17:40:51 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:40:51 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:40:51 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:40:51 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:40:51 - INFO - __main__ - Printing 3 examples
05/21/2022 17:40:51 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
05/21/2022 17:40:51 - INFO - __main__ - ['negative']
05/21/2022 17:40:51 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
05/21/2022 17:40:51 - INFO - __main__ - ['negative']
05/21/2022 17:40:51 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
05/21/2022 17:40:51 - INFO - __main__ - ['negative']
05/21/2022 17:40:51 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:40:52 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:40:52 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:40:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:40:56 - INFO - __main__ - Starting training!
05/21/2022 17:40:58 - INFO - __main__ - Step 10 Global step 10 Train loss 22.502666 on epoch=4
05/21/2022 17:41:00 - INFO - __main__ - Step 20 Global step 20 Train loss 18.069433 on epoch=9
05/21/2022 17:41:03 - INFO - __main__ - Step 30 Global step 30 Train loss 12.965147 on epoch=14
05/21/2022 17:41:05 - INFO - __main__ - Step 40 Global step 40 Train loss 8.695524 on epoch=19
05/21/2022 17:41:08 - INFO - __main__ - Step 50 Global step 50 Train loss 5.376731 on epoch=24
05/21/2022 17:41:09 - INFO - __main__ - Global step 50 Train loss 13.521900 Classification-F1 0.21276595744680848 on epoch=24
05/21/2022 17:41:12 - INFO - __main__ - Step 60 Global step 60 Train loss 3.178871 on epoch=29
05/21/2022 17:41:15 - INFO - __main__ - Step 70 Global step 70 Train loss 2.360881 on epoch=34
05/21/2022 17:41:17 - INFO - __main__ - Step 80 Global step 80 Train loss 1.288979 on epoch=39
05/21/2022 17:41:20 - INFO - __main__ - Step 90 Global step 90 Train loss 1.061392 on epoch=44
05/21/2022 17:41:22 - INFO - __main__ - Step 100 Global step 100 Train loss 0.717994 on epoch=49
05/21/2022 17:41:22 - INFO - __main__ - Global step 100 Train loss 1.721623 Classification-F1 0.8398398398398398 on epoch=49
05/21/2022 17:41:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.688074 on epoch=54
05/21/2022 17:41:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.410030 on epoch=59
05/21/2022 17:41:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.348011 on epoch=64
05/21/2022 17:41:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.435327 on epoch=69
05/21/2022 17:41:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.292934 on epoch=74
05/21/2022 17:41:36 - INFO - __main__ - Global step 150 Train loss 0.434875 Classification-F1 0.8398398398398398 on epoch=74
05/21/2022 17:41:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.385264 on epoch=79
05/21/2022 17:41:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.323589 on epoch=84
05/21/2022 17:41:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.176888 on epoch=89
05/21/2022 17:41:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.132750 on epoch=94
05/21/2022 17:41:48 - INFO - __main__ - Step 200 Global step 200 Train loss 0.165264 on epoch=99
05/21/2022 17:41:48 - INFO - __main__ - Global step 200 Train loss 0.236751 Classification-F1 0.9054187192118226 on epoch=99
05/21/2022 17:41:51 - INFO - __main__ - Step 210 Global step 210 Train loss 0.223244 on epoch=104
05/21/2022 17:41:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.193932 on epoch=109
05/21/2022 17:41:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.082371 on epoch=114
05/21/2022 17:41:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.235333 on epoch=119
05/21/2022 17:42:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.113017 on epoch=124
05/21/2022 17:42:02 - INFO - __main__ - Global step 250 Train loss 0.169579 Classification-F1 0.9054187192118226 on epoch=124
05/21/2022 17:42:04 - INFO - __main__ - Step 260 Global step 260 Train loss 0.044656 on epoch=129
05/21/2022 17:42:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.030367 on epoch=134
05/21/2022 17:42:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.094354 on epoch=139
05/21/2022 17:42:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.052986 on epoch=144
05/21/2022 17:42:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.038969 on epoch=149
05/21/2022 17:42:14 - INFO - __main__ - Global step 300 Train loss 0.052267 Classification-F1 0.9054187192118226 on epoch=149
05/21/2022 17:42:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.075542 on epoch=154
05/21/2022 17:42:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.053617 on epoch=159
05/21/2022 17:42:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.023118 on epoch=164
05/21/2022 17:42:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.073829 on epoch=169
05/21/2022 17:42:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.020520 on epoch=174
05/21/2022 17:42:27 - INFO - __main__ - Global step 350 Train loss 0.049325 Classification-F1 0.9054187192118226 on epoch=174
05/21/2022 17:42:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.039540 on epoch=179
05/21/2022 17:42:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.045694 on epoch=184
05/21/2022 17:42:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.034075 on epoch=189
05/21/2022 17:42:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.038417 on epoch=194
05/21/2022 17:42:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.016087 on epoch=199
05/21/2022 17:42:40 - INFO - __main__ - Global step 400 Train loss 0.034762 Classification-F1 0.9687194525904204 on epoch=199
05/21/2022 17:42:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.069252 on epoch=204
05/21/2022 17:42:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.038882 on epoch=209
05/21/2022 17:42:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.028560 on epoch=214
05/21/2022 17:42:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.037676 on epoch=219
05/21/2022 17:42:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.006560 on epoch=224
05/21/2022 17:42:53 - INFO - __main__ - Global step 450 Train loss 0.036186 Classification-F1 0.9372549019607843 on epoch=224
05/21/2022 17:42:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.037645 on epoch=229
05/21/2022 17:42:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.037255 on epoch=234
05/21/2022 17:43:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.063125 on epoch=239
05/21/2022 17:43:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.015179 on epoch=244
05/21/2022 17:43:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.017556 on epoch=249
05/21/2022 17:43:06 - INFO - __main__ - Global step 500 Train loss 0.034152 Classification-F1 0.9054187192118226 on epoch=249
05/21/2022 17:43:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.032587 on epoch=254
05/21/2022 17:43:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.008831 on epoch=259
05/21/2022 17:43:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.034066 on epoch=264
05/21/2022 17:43:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.077398 on epoch=269
05/21/2022 17:43:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.036592 on epoch=274
05/21/2022 17:43:19 - INFO - __main__ - Global step 550 Train loss 0.037895 Classification-F1 0.9372549019607843 on epoch=274
05/21/2022 17:43:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.017081 on epoch=279
05/21/2022 17:43:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.036186 on epoch=284
05/21/2022 17:43:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.016026 on epoch=289
05/21/2022 17:43:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.046679 on epoch=294
05/21/2022 17:43:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.047649 on epoch=299
05/21/2022 17:43:32 - INFO - __main__ - Global step 600 Train loss 0.032724 Classification-F1 0.9372549019607843 on epoch=299
05/21/2022 17:43:32 - INFO - __main__ - save last model!
05/21/2022 17:43:33 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:43:33 - INFO - __main__ - Printing 3 examples
05/21/2022 17:43:33 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/21/2022 17:43:33 - INFO - __main__ - ['negative']
05/21/2022 17:43:33 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/21/2022 17:43:33 - INFO - __main__ - ['negative']
05/21/2022 17:43:33 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/21/2022 17:43:33 - INFO - __main__ - ['negative']
05/21/2022 17:43:33 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:43:33 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:43:33 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:43:33 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:43:33 - INFO - __main__ - Printing 3 examples
05/21/2022 17:43:33 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
05/21/2022 17:43:33 - INFO - __main__ - ['negative']
05/21/2022 17:43:33 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
05/21/2022 17:43:33 - INFO - __main__ - ['negative']
05/21/2022 17:43:33 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
05/21/2022 17:43:33 - INFO - __main__ - ['negative']
05/21/2022 17:43:33 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:43:33 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:43:33 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:43:35 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:43:35 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:43:35 - INFO - __main__ - Printing 3 examples
05/21/2022 17:43:35 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:43:35 - INFO - __main__ - ['negative']
05/21/2022 17:43:35 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:43:35 - INFO - __main__ - ['negative']
05/21/2022 17:43:35 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:43:35 - INFO - __main__ - ['negative']
05/21/2022 17:43:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:43:35 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:43:36 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:43:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:43:37 - INFO - __main__ - Starting training!
05/21/2022 17:43:43 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_42_0.0003_8_predictions.txt
05/21/2022 17:43:43 - INFO - __main__ - Classification-F1 on test data: 0.9219
05/21/2022 17:43:43 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0003, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9219472363317602
05/21/2022 17:43:43 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0002, bsz=8 ...
05/21/2022 17:43:44 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:43:44 - INFO - __main__ - Printing 3 examples
05/21/2022 17:43:44 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/21/2022 17:43:44 - INFO - __main__ - ['negative']
05/21/2022 17:43:44 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/21/2022 17:43:44 - INFO - __main__ - ['negative']
05/21/2022 17:43:44 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/21/2022 17:43:44 - INFO - __main__ - ['negative']
05/21/2022 17:43:44 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:43:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:43:44 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:43:44 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:43:44 - INFO - __main__ - Printing 3 examples
05/21/2022 17:43:44 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
05/21/2022 17:43:44 - INFO - __main__ - ['negative']
05/21/2022 17:43:44 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
05/21/2022 17:43:44 - INFO - __main__ - ['negative']
05/21/2022 17:43:44 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
05/21/2022 17:43:44 - INFO - __main__ - ['negative']
05/21/2022 17:43:44 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:43:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:43:44 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:43:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:43:48 - INFO - __main__ - Starting training!
05/21/2022 17:43:50 - INFO - __main__ - Step 10 Global step 10 Train loss 24.064505 on epoch=4
05/21/2022 17:43:53 - INFO - __main__ - Step 20 Global step 20 Train loss 18.720013 on epoch=9
05/21/2022 17:43:55 - INFO - __main__ - Step 30 Global step 30 Train loss 13.290155 on epoch=14
05/21/2022 17:43:58 - INFO - __main__ - Step 40 Global step 40 Train loss 10.528700 on epoch=19
05/21/2022 17:44:00 - INFO - __main__ - Step 50 Global step 50 Train loss 7.390696 on epoch=24
05/21/2022 17:44:03 - INFO - __main__ - Global step 50 Train loss 14.798812 Classification-F1 0.0 on epoch=24
05/21/2022 17:44:06 - INFO - __main__ - Step 60 Global step 60 Train loss 6.962415 on epoch=29
05/21/2022 17:44:08 - INFO - __main__ - Step 70 Global step 70 Train loss 5.632655 on epoch=34
05/21/2022 17:44:11 - INFO - __main__ - Step 80 Global step 80 Train loss 2.861608 on epoch=39
05/21/2022 17:44:13 - INFO - __main__ - Step 90 Global step 90 Train loss 1.639277 on epoch=44
05/21/2022 17:44:16 - INFO - __main__ - Step 100 Global step 100 Train loss 1.506745 on epoch=49
05/21/2022 17:44:16 - INFO - __main__ - Global step 100 Train loss 3.720540 Classification-F1 0.6101882613510521 on epoch=49
05/21/2022 17:44:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.702399 on epoch=54
05/21/2022 17:44:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.650741 on epoch=59
05/21/2022 17:44:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.479301 on epoch=64
05/21/2022 17:44:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.530710 on epoch=69
05/21/2022 17:44:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.387099 on epoch=74
05/21/2022 17:44:30 - INFO - __main__ - Global step 150 Train loss 0.550050 Classification-F1 0.8095238095238095 on epoch=74
05/21/2022 17:44:32 - INFO - __main__ - Step 160 Global step 160 Train loss 0.366801 on epoch=79
05/21/2022 17:44:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.470313 on epoch=84
05/21/2022 17:44:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.272424 on epoch=89
05/21/2022 17:44:40 - INFO - __main__ - Step 190 Global step 190 Train loss 0.360355 on epoch=94
05/21/2022 17:44:42 - INFO - __main__ - Step 200 Global step 200 Train loss 0.309588 on epoch=99
05/21/2022 17:44:43 - INFO - __main__ - Global step 200 Train loss 0.355896 Classification-F1 0.7702564102564102 on epoch=99
05/21/2022 17:44:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.281285 on epoch=104
05/21/2022 17:44:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.310519 on epoch=109
05/21/2022 17:44:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.202286 on epoch=114
05/21/2022 17:44:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.148536 on epoch=119
05/21/2022 17:44:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.216410 on epoch=124
05/21/2022 17:44:55 - INFO - __main__ - Global step 250 Train loss 0.231807 Classification-F1 0.805668016194332 on epoch=124
05/21/2022 17:44:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.157885 on epoch=129
05/21/2022 17:45:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.131938 on epoch=134
05/21/2022 17:45:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.202626 on epoch=139
05/21/2022 17:45:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.099636 on epoch=144
05/21/2022 17:45:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.121854 on epoch=149
05/21/2022 17:45:08 - INFO - __main__ - Global step 300 Train loss 0.142788 Classification-F1 0.873015873015873 on epoch=149
05/21/2022 17:45:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.095947 on epoch=154
05/21/2022 17:45:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.106509 on epoch=159
05/21/2022 17:45:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.104975 on epoch=164
05/21/2022 17:45:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.076311 on epoch=169
05/21/2022 17:45:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.141726 on epoch=174
05/21/2022 17:45:22 - INFO - __main__ - Global step 350 Train loss 0.105093 Classification-F1 0.873015873015873 on epoch=174
05/21/2022 17:45:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.100957 on epoch=179
05/21/2022 17:45:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.101457 on epoch=184
05/21/2022 17:45:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.183997 on epoch=189
05/21/2022 17:45:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.195775 on epoch=194
05/21/2022 17:45:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.128620 on epoch=199
05/21/2022 17:45:35 - INFO - __main__ - Global step 400 Train loss 0.142161 Classification-F1 0.9372549019607843 on epoch=199
05/21/2022 17:45:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.106055 on epoch=204
05/21/2022 17:45:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.269904 on epoch=209
05/21/2022 17:45:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.213726 on epoch=214
05/21/2022 17:45:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.274055 on epoch=219
05/21/2022 17:45:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.203523 on epoch=224
05/21/2022 17:45:48 - INFO - __main__ - Global step 450 Train loss 0.213453 Classification-F1 0.9054187192118226 on epoch=224
05/21/2022 17:45:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.148058 on epoch=229
05/21/2022 17:45:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.116368 on epoch=234
05/21/2022 17:45:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.125820 on epoch=239
05/21/2022 17:45:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.239583 on epoch=244
05/21/2022 17:46:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.244844 on epoch=249
05/21/2022 17:46:01 - INFO - __main__ - Global step 500 Train loss 0.174935 Classification-F1 0.9372549019607843 on epoch=249
05/21/2022 17:46:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.101245 on epoch=254
05/21/2022 17:46:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.139134 on epoch=259
05/21/2022 17:46:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.156940 on epoch=264
05/21/2022 17:46:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.342352 on epoch=269
05/21/2022 17:46:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.264275 on epoch=274
05/21/2022 17:46:13 - INFO - __main__ - Global step 550 Train loss 0.200789 Classification-F1 0.9372549019607843 on epoch=274
05/21/2022 17:46:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.306130 on epoch=279
05/21/2022 17:46:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.286041 on epoch=284
05/21/2022 17:46:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.299344 on epoch=289
05/21/2022 17:46:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.329087 on epoch=294
05/21/2022 17:46:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.384885 on epoch=299
05/21/2022 17:46:26 - INFO - __main__ - Global step 600 Train loss 0.321098 Classification-F1 0.9054187192118226 on epoch=299
05/21/2022 17:46:26 - INFO - __main__ - save last model!
05/21/2022 17:46:27 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:46:27 - INFO - __main__ - Printing 3 examples
05/21/2022 17:46:27 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/21/2022 17:46:27 - INFO - __main__ - ['negative']
05/21/2022 17:46:27 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/21/2022 17:46:27 - INFO - __main__ - ['negative']
05/21/2022 17:46:27 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/21/2022 17:46:27 - INFO - __main__ - ['negative']
05/21/2022 17:46:27 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:46:27 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:46:27 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:46:27 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:46:27 - INFO - __main__ - Printing 3 examples
05/21/2022 17:46:27 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
05/21/2022 17:46:27 - INFO - __main__ - ['negative']
05/21/2022 17:46:27 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
05/21/2022 17:46:27 - INFO - __main__ - ['negative']
05/21/2022 17:46:27 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
05/21/2022 17:46:27 - INFO - __main__ - ['negative']
05/21/2022 17:46:27 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:46:27 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:46:27 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:46:29 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:46:29 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:46:29 - INFO - __main__ - Printing 3 examples
05/21/2022 17:46:29 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:46:29 - INFO - __main__ - ['negative']
05/21/2022 17:46:29 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:46:29 - INFO - __main__ - ['negative']
05/21/2022 17:46:29 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:46:29 - INFO - __main__ - ['negative']
05/21/2022 17:46:29 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:46:30 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:46:31 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:46:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:46:31 - INFO - __main__ - Starting training!
05/21/2022 17:46:37 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_42_0.0002_8_predictions.txt
05/21/2022 17:46:37 - INFO - __main__ - Classification-F1 on test data: 0.9209
05/21/2022 17:46:37 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0002, bsz=8, dev_performance=0.9372549019607843, test_performance=0.920903106305224
05/21/2022 17:46:37 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.0001, bsz=8 ...
05/21/2022 17:46:38 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:46:38 - INFO - __main__ - Printing 3 examples
05/21/2022 17:46:38 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
05/21/2022 17:46:38 - INFO - __main__ - ['negative']
05/21/2022 17:46:38 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
05/21/2022 17:46:38 - INFO - __main__ - ['negative']
05/21/2022 17:46:38 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
05/21/2022 17:46:38 - INFO - __main__ - ['negative']
05/21/2022 17:46:38 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:46:38 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:46:38 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:46:38 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:46:38 - INFO - __main__ - Printing 3 examples
05/21/2022 17:46:38 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
05/21/2022 17:46:38 - INFO - __main__ - ['negative']
05/21/2022 17:46:38 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
05/21/2022 17:46:38 - INFO - __main__ - ['negative']
05/21/2022 17:46:38 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
05/21/2022 17:46:38 - INFO - __main__ - ['negative']
05/21/2022 17:46:38 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:46:38 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:46:38 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:46:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:46:42 - INFO - __main__ - Starting training!
05/21/2022 17:46:44 - INFO - __main__ - Step 10 Global step 10 Train loss 23.080467 on epoch=4
05/21/2022 17:46:47 - INFO - __main__ - Step 20 Global step 20 Train loss 20.416803 on epoch=9
05/21/2022 17:46:49 - INFO - __main__ - Step 30 Global step 30 Train loss 16.402710 on epoch=14
05/21/2022 17:46:52 - INFO - __main__ - Step 40 Global step 40 Train loss 14.290489 on epoch=19
05/21/2022 17:46:54 - INFO - __main__ - Step 50 Global step 50 Train loss 13.570227 on epoch=24
05/21/2022 17:46:59 - INFO - __main__ - Global step 50 Train loss 17.552139 Classification-F1 0.0 on epoch=24
05/21/2022 17:47:02 - INFO - __main__ - Step 60 Global step 60 Train loss 12.655910 on epoch=29
05/21/2022 17:47:04 - INFO - __main__ - Step 70 Global step 70 Train loss 9.575312 on epoch=34
05/21/2022 17:47:07 - INFO - __main__ - Step 80 Global step 80 Train loss 9.976664 on epoch=39
05/21/2022 17:47:09 - INFO - __main__ - Step 90 Global step 90 Train loss 7.592167 on epoch=44
05/21/2022 17:47:12 - INFO - __main__ - Step 100 Global step 100 Train loss 7.178473 on epoch=49
05/21/2022 17:47:14 - INFO - __main__ - Global step 100 Train loss 9.395705 Classification-F1 0.0 on epoch=49
05/21/2022 17:47:17 - INFO - __main__ - Step 110 Global step 110 Train loss 7.424104 on epoch=54
05/21/2022 17:47:19 - INFO - __main__ - Step 120 Global step 120 Train loss 5.164761 on epoch=59
05/21/2022 17:47:22 - INFO - __main__ - Step 130 Global step 130 Train loss 4.963780 on epoch=64
05/21/2022 17:47:25 - INFO - __main__ - Step 140 Global step 140 Train loss 3.609797 on epoch=69
05/21/2022 17:47:27 - INFO - __main__ - Step 150 Global step 150 Train loss 3.577845 on epoch=74
05/21/2022 17:47:27 - INFO - __main__ - Global step 150 Train loss 4.948057 Classification-F1 0.3333333333333333 on epoch=74
05/21/2022 17:47:30 - INFO - __main__ - Step 160 Global step 160 Train loss 2.537570 on epoch=79
05/21/2022 17:47:33 - INFO - __main__ - Step 170 Global step 170 Train loss 1.930163 on epoch=84
05/21/2022 17:47:35 - INFO - __main__ - Step 180 Global step 180 Train loss 2.298062 on epoch=89
05/21/2022 17:47:38 - INFO - __main__ - Step 190 Global step 190 Train loss 1.418861 on epoch=94
05/21/2022 17:47:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.837788 on epoch=99
05/21/2022 17:47:41 - INFO - __main__ - Global step 200 Train loss 1.804489 Classification-F1 0.7333333333333334 on epoch=99
05/21/2022 17:47:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.669911 on epoch=104
05/21/2022 17:47:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.905622 on epoch=109
05/21/2022 17:47:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.499725 on epoch=114
05/21/2022 17:47:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.461260 on epoch=119
05/21/2022 17:47:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.448899 on epoch=124
05/21/2022 17:47:54 - INFO - __main__ - Global step 250 Train loss 0.597083 Classification-F1 0.7333333333333334 on epoch=124
05/21/2022 17:47:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.612382 on epoch=129
05/21/2022 17:47:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.423664 on epoch=134
05/21/2022 17:48:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.383242 on epoch=139
05/21/2022 17:48:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.417458 on epoch=144
05/21/2022 17:48:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.281632 on epoch=149
05/21/2022 17:48:07 - INFO - __main__ - Global step 300 Train loss 0.423676 Classification-F1 0.805668016194332 on epoch=149
05/21/2022 17:48:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.452146 on epoch=154
05/21/2022 17:48:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.404379 on epoch=159
05/21/2022 17:48:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.226232 on epoch=164
05/21/2022 17:48:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.229576 on epoch=169
05/21/2022 17:48:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.405813 on epoch=174
05/21/2022 17:48:20 - INFO - __main__ - Global step 350 Train loss 0.343629 Classification-F1 0.873015873015873 on epoch=174
05/21/2022 17:48:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.280334 on epoch=179
05/21/2022 17:48:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.279462 on epoch=184
05/21/2022 17:48:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.187734 on epoch=189
05/21/2022 17:48:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.198403 on epoch=194
05/21/2022 17:48:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.353076 on epoch=199
05/21/2022 17:48:33 - INFO - __main__ - Global step 400 Train loss 0.259802 Classification-F1 0.873015873015873 on epoch=199
05/21/2022 17:48:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.149269 on epoch=204
05/21/2022 17:48:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.204869 on epoch=209
05/21/2022 17:48:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.110231 on epoch=214
05/21/2022 17:48:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.125313 on epoch=219
05/21/2022 17:48:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.109720 on epoch=224
05/21/2022 17:48:46 - INFO - __main__ - Global step 450 Train loss 0.139881 Classification-F1 0.9054187192118226 on epoch=224
05/21/2022 17:48:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.175363 on epoch=229
05/21/2022 17:48:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.205845 on epoch=234
05/21/2022 17:48:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.179358 on epoch=239
05/21/2022 17:48:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.135808 on epoch=244
05/21/2022 17:48:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.064266 on epoch=249
05/21/2022 17:49:00 - INFO - __main__ - Global step 500 Train loss 0.152128 Classification-F1 0.9054187192118226 on epoch=249
05/21/2022 17:49:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.076097 on epoch=254
05/21/2022 17:49:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.144290 on epoch=259
05/21/2022 17:49:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.086630 on epoch=264
05/21/2022 17:49:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.177044 on epoch=269
05/21/2022 17:49:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.092625 on epoch=274
05/21/2022 17:49:13 - INFO - __main__ - Global step 550 Train loss 0.115337 Classification-F1 0.9372549019607843 on epoch=274
05/21/2022 17:49:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.090488 on epoch=279
05/21/2022 17:49:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.050974 on epoch=284
05/21/2022 17:49:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.082687 on epoch=289
05/21/2022 17:49:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.072531 on epoch=294
05/21/2022 17:49:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.144913 on epoch=299
05/21/2022 17:49:26 - INFO - __main__ - Global step 600 Train loss 0.088319 Classification-F1 0.873015873015873 on epoch=299
05/21/2022 17:49:26 - INFO - __main__ - save last model!
05/21/2022 17:49:27 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:49:27 - INFO - __main__ - Printing 3 examples
05/21/2022 17:49:27 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/21/2022 17:49:27 - INFO - __main__ - ['negative']
05/21/2022 17:49:27 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/21/2022 17:49:27 - INFO - __main__ - ['negative']
05/21/2022 17:49:27 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/21/2022 17:49:27 - INFO - __main__ - ['negative']
05/21/2022 17:49:27 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:49:27 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:49:27 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:49:27 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:49:27 - INFO - __main__ - Printing 3 examples
05/21/2022 17:49:27 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
05/21/2022 17:49:27 - INFO - __main__ - ['negative']
05/21/2022 17:49:27 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
05/21/2022 17:49:27 - INFO - __main__ - ['negative']
05/21/2022 17:49:27 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
05/21/2022 17:49:27 - INFO - __main__ - ['negative']
05/21/2022 17:49:27 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:49:27 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:49:27 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:49:28 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:49:29 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:49:29 - INFO - __main__ - Printing 3 examples
05/21/2022 17:49:29 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:49:29 - INFO - __main__ - ['negative']
05/21/2022 17:49:29 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:49:29 - INFO - __main__ - ['negative']
05/21/2022 17:49:29 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:49:29 - INFO - __main__ - ['negative']
05/21/2022 17:49:29 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:49:29 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:49:30 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:49:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:49:30 - INFO - __main__ - Starting training!
05/21/2022 17:49:37 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_42_0.0001_8_predictions.txt
05/21/2022 17:49:37 - INFO - __main__ - Classification-F1 on test data: 0.9047
05/21/2022 17:49:37 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.0001, bsz=8, dev_performance=0.9372549019607843, test_performance=0.9046681498295568
05/21/2022 17:49:37 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0005, bsz=8 ...
05/21/2022 17:49:38 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:49:38 - INFO - __main__ - Printing 3 examples
05/21/2022 17:49:38 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/21/2022 17:49:38 - INFO - __main__ - ['negative']
05/21/2022 17:49:38 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/21/2022 17:49:38 - INFO - __main__ - ['negative']
05/21/2022 17:49:38 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/21/2022 17:49:38 - INFO - __main__ - ['negative']
05/21/2022 17:49:38 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:49:38 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:49:38 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:49:38 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:49:38 - INFO - __main__ - Printing 3 examples
05/21/2022 17:49:38 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
05/21/2022 17:49:38 - INFO - __main__ - ['negative']
05/21/2022 17:49:38 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
05/21/2022 17:49:38 - INFO - __main__ - ['negative']
05/21/2022 17:49:38 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
05/21/2022 17:49:38 - INFO - __main__ - ['negative']
05/21/2022 17:49:38 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:49:38 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:49:38 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:49:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:49:42 - INFO - __main__ - Starting training!
05/21/2022 17:49:44 - INFO - __main__ - Step 10 Global step 10 Train loss 23.061754 on epoch=4
05/21/2022 17:49:47 - INFO - __main__ - Step 20 Global step 20 Train loss 14.489832 on epoch=9
05/21/2022 17:49:49 - INFO - __main__ - Step 30 Global step 30 Train loss 7.925420 on epoch=14
05/21/2022 17:49:52 - INFO - __main__ - Step 40 Global step 40 Train loss 3.252630 on epoch=19
05/21/2022 17:49:54 - INFO - __main__ - Step 50 Global step 50 Train loss 3.232647 on epoch=24
05/21/2022 17:49:55 - INFO - __main__ - Global step 50 Train loss 10.392457 Classification-F1 0.6536796536796536 on epoch=24
05/21/2022 17:49:57 - INFO - __main__ - Step 60 Global step 60 Train loss 1.152914 on epoch=29
05/21/2022 17:50:00 - INFO - __main__ - Step 70 Global step 70 Train loss 0.579387 on epoch=34
05/21/2022 17:50:02 - INFO - __main__ - Step 80 Global step 80 Train loss 0.542897 on epoch=39
05/21/2022 17:50:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.443260 on epoch=44
05/21/2022 17:50:07 - INFO - __main__ - Step 100 Global step 100 Train loss 0.463908 on epoch=49
05/21/2022 17:50:08 - INFO - __main__ - Global step 100 Train loss 0.636473 Classification-F1 0.9687194525904204 on epoch=49
05/21/2022 17:50:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.368172 on epoch=54
05/21/2022 17:50:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.252658 on epoch=59
05/21/2022 17:50:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.228909 on epoch=64
05/21/2022 17:50:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.153645 on epoch=69
05/21/2022 17:50:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.074982 on epoch=74
05/21/2022 17:50:21 - INFO - __main__ - Global step 150 Train loss 0.215673 Classification-F1 0.9375 on epoch=74
05/21/2022 17:50:24 - INFO - __main__ - Step 160 Global step 160 Train loss 0.081963 on epoch=79
05/21/2022 17:50:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.036408 on epoch=84
05/21/2022 17:50:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.117529 on epoch=89
05/21/2022 17:50:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.022578 on epoch=94
05/21/2022 17:50:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.404472 on epoch=99
05/21/2022 17:50:34 - INFO - __main__ - Global step 200 Train loss 0.132590 Classification-F1 0.8745098039215686 on epoch=99
05/21/2022 17:50:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.058105 on epoch=104
05/21/2022 17:50:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.013744 on epoch=109
05/21/2022 17:50:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.019843 on epoch=114
05/21/2022 17:50:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.003990 on epoch=119
05/21/2022 17:50:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.056236 on epoch=124
05/21/2022 17:50:47 - INFO - __main__ - Global step 250 Train loss 0.030384 Classification-F1 0.873015873015873 on epoch=124
05/21/2022 17:50:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.063865 on epoch=129
05/21/2022 17:50:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.020403 on epoch=134
05/21/2022 17:50:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.001314 on epoch=139
05/21/2022 17:50:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.027714 on epoch=144
05/21/2022 17:51:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.178012 on epoch=149
05/21/2022 17:51:00 - INFO - __main__ - Global step 300 Train loss 0.058262 Classification-F1 0.875 on epoch=149
05/21/2022 17:51:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.015233 on epoch=154
05/21/2022 17:51:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.008720 on epoch=159
05/21/2022 17:51:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.022905 on epoch=164
05/21/2022 17:51:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.005772 on epoch=169
05/21/2022 17:51:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.029042 on epoch=174
05/21/2022 17:51:13 - INFO - __main__ - Global step 350 Train loss 0.016335 Classification-F1 0.873015873015873 on epoch=174
05/21/2022 17:51:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.027106 on epoch=179
05/21/2022 17:51:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.001169 on epoch=184
05/21/2022 17:51:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.052844 on epoch=189
05/21/2022 17:51:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.431270 on epoch=194
05/21/2022 17:51:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.218317 on epoch=199
05/21/2022 17:51:26 - INFO - __main__ - Global step 400 Train loss 0.146141 Classification-F1 0.873015873015873 on epoch=199
05/21/2022 17:51:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.227784 on epoch=204
05/21/2022 17:51:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.097021 on epoch=209
05/21/2022 17:51:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.348648 on epoch=214
05/21/2022 17:51:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.324151 on epoch=219
05/21/2022 17:51:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.395312 on epoch=224
05/21/2022 17:51:39 - INFO - __main__ - Global step 450 Train loss 0.278583 Classification-F1 0.746031746031746 on epoch=224
05/21/2022 17:51:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.448788 on epoch=229
05/21/2022 17:51:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.380909 on epoch=234
05/21/2022 17:51:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.399365 on epoch=239
05/21/2022 17:51:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.308831 on epoch=244
05/21/2022 17:51:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.244376 on epoch=249
05/21/2022 17:51:52 - INFO - __main__ - Global step 500 Train loss 0.356454 Classification-F1 0.7117117117117117 on epoch=249
05/21/2022 17:51:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.309813 on epoch=254
05/21/2022 17:51:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.228840 on epoch=259
05/21/2022 17:52:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.207017 on epoch=264
05/21/2022 17:52:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.229130 on epoch=269
05/21/2022 17:52:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.252780 on epoch=274
05/21/2022 17:52:05 - INFO - __main__ - Global step 550 Train loss 0.245516 Classification-F1 0.8745098039215686 on epoch=274
05/21/2022 17:52:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.348186 on epoch=279
05/21/2022 17:52:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.199025 on epoch=284
05/21/2022 17:52:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.324809 on epoch=289
05/21/2022 17:52:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.197676 on epoch=294
05/21/2022 17:52:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.215486 on epoch=299
05/21/2022 17:52:19 - INFO - __main__ - Global step 600 Train loss 0.257036 Classification-F1 0.7046153846153846 on epoch=299
05/21/2022 17:52:19 - INFO - __main__ - save last model!
05/21/2022 17:52:19 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:52:19 - INFO - __main__ - Printing 3 examples
05/21/2022 17:52:19 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/21/2022 17:52:19 - INFO - __main__ - ['negative']
05/21/2022 17:52:19 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/21/2022 17:52:19 - INFO - __main__ - ['negative']
05/21/2022 17:52:19 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/21/2022 17:52:19 - INFO - __main__ - ['negative']
05/21/2022 17:52:19 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:52:19 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:52:20 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:52:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:52:20 - INFO - __main__ - Printing 3 examples
05/21/2022 17:52:20 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
05/21/2022 17:52:20 - INFO - __main__ - ['negative']
05/21/2022 17:52:20 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
05/21/2022 17:52:20 - INFO - __main__ - ['negative']
05/21/2022 17:52:20 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
05/21/2022 17:52:20 - INFO - __main__ - ['negative']
05/21/2022 17:52:20 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:52:20 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:52:20 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:52:21 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:52:21 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:52:21 - INFO - __main__ - Printing 3 examples
05/21/2022 17:52:21 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:52:21 - INFO - __main__ - ['negative']
05/21/2022 17:52:21 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:52:21 - INFO - __main__ - ['negative']
05/21/2022 17:52:21 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:52:21 - INFO - __main__ - ['negative']
05/21/2022 17:52:21 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:52:22 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:52:23 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:52:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:52:23 - INFO - __main__ - Starting training!
05/21/2022 17:52:30 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_87_0.0005_8_predictions.txt
05/21/2022 17:52:30 - INFO - __main__ - Classification-F1 on test data: 0.8495
05/21/2022 17:52:30 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0005, bsz=8, dev_performance=0.9687194525904204, test_performance=0.8495347777994664
05/21/2022 17:52:30 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0003, bsz=8 ...
05/21/2022 17:52:31 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:52:31 - INFO - __main__ - Printing 3 examples
05/21/2022 17:52:31 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/21/2022 17:52:31 - INFO - __main__ - ['negative']
05/21/2022 17:52:31 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/21/2022 17:52:31 - INFO - __main__ - ['negative']
05/21/2022 17:52:31 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/21/2022 17:52:31 - INFO - __main__ - ['negative']
05/21/2022 17:52:31 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:52:31 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:52:31 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:52:31 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:52:31 - INFO - __main__ - Printing 3 examples
05/21/2022 17:52:31 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
05/21/2022 17:52:31 - INFO - __main__ - ['negative']
05/21/2022 17:52:31 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
05/21/2022 17:52:31 - INFO - __main__ - ['negative']
05/21/2022 17:52:31 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
05/21/2022 17:52:31 - INFO - __main__ - ['negative']
05/21/2022 17:52:31 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:52:31 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:52:31 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:52:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:52:35 - INFO - __main__ - Starting training!
05/21/2022 17:52:37 - INFO - __main__ - Step 10 Global step 10 Train loss 24.265236 on epoch=4
05/21/2022 17:52:39 - INFO - __main__ - Step 20 Global step 20 Train loss 16.066587 on epoch=9
05/21/2022 17:52:42 - INFO - __main__ - Step 30 Global step 30 Train loss 12.182312 on epoch=14
05/21/2022 17:52:44 - INFO - __main__ - Step 40 Global step 40 Train loss 6.715188 on epoch=19
05/21/2022 17:52:47 - INFO - __main__ - Step 50 Global step 50 Train loss 5.518987 on epoch=24
05/21/2022 17:52:47 - INFO - __main__ - Global step 50 Train loss 12.949662 Classification-F1 0.3333333333333333 on epoch=24
05/21/2022 17:52:50 - INFO - __main__ - Step 60 Global step 60 Train loss 4.087318 on epoch=29
05/21/2022 17:52:53 - INFO - __main__ - Step 70 Global step 70 Train loss 1.455246 on epoch=34
05/21/2022 17:52:55 - INFO - __main__ - Step 80 Global step 80 Train loss 0.915719 on epoch=39
05/21/2022 17:52:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.790845 on epoch=44
05/21/2022 17:53:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.680356 on epoch=49
05/21/2022 17:53:00 - INFO - __main__ - Global step 100 Train loss 1.585897 Classification-F1 0.3333333333333333 on epoch=49
05/21/2022 17:53:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.489878 on epoch=54
05/21/2022 17:53:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.721565 on epoch=59
05/21/2022 17:53:08 - INFO - __main__ - Step 130 Global step 130 Train loss 0.582968 on epoch=64
05/21/2022 17:53:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.557389 on epoch=69
05/21/2022 17:53:13 - INFO - __main__ - Step 150 Global step 150 Train loss 0.402826 on epoch=74
05/21/2022 17:53:13 - INFO - __main__ - Global step 150 Train loss 0.550925 Classification-F1 0.7757757757757757 on epoch=74
05/21/2022 17:53:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.423569 on epoch=79
05/21/2022 17:53:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.503054 on epoch=84
05/21/2022 17:53:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.377649 on epoch=89
05/21/2022 17:53:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.465672 on epoch=94
05/21/2022 17:53:27 - INFO - __main__ - Step 200 Global step 200 Train loss 0.625819 on epoch=99
05/21/2022 17:53:27 - INFO - __main__ - Global step 200 Train loss 0.479153 Classification-F1 0.9372549019607843 on epoch=99
05/21/2022 17:53:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.541317 on epoch=104
05/21/2022 17:53:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.357812 on epoch=109
05/21/2022 17:53:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.357524 on epoch=114
05/21/2022 17:53:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.342800 on epoch=119
05/21/2022 17:53:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.328275 on epoch=124
05/21/2022 17:53:40 - INFO - __main__ - Global step 250 Train loss 0.385546 Classification-F1 0.906158357771261 on epoch=124
05/21/2022 17:53:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.317272 on epoch=129
05/21/2022 17:53:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.247968 on epoch=134
05/21/2022 17:53:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.228915 on epoch=139
05/21/2022 17:53:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.296713 on epoch=144
05/21/2022 17:53:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.452892 on epoch=149
05/21/2022 17:53:53 - INFO - __main__ - Global step 300 Train loss 0.308752 Classification-F1 0.7702564102564102 on epoch=149
05/21/2022 17:53:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.323392 on epoch=154
05/21/2022 17:53:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.276614 on epoch=159
05/21/2022 17:54:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.279414 on epoch=164
05/21/2022 17:54:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.355095 on epoch=169
05/21/2022 17:54:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.252283 on epoch=174
05/21/2022 17:54:06 - INFO - __main__ - Global step 350 Train loss 0.297359 Classification-F1 0.873015873015873 on epoch=174
05/21/2022 17:54:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.324579 on epoch=179
05/21/2022 17:54:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.202201 on epoch=184
05/21/2022 17:54:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.210660 on epoch=189
05/21/2022 17:54:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.215593 on epoch=194
05/21/2022 17:54:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.198482 on epoch=199
05/21/2022 17:54:19 - INFO - __main__ - Global step 400 Train loss 0.230303 Classification-F1 0.9372549019607843 on epoch=199
05/21/2022 17:54:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.246374 on epoch=204
05/21/2022 17:54:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.253130 on epoch=209
05/21/2022 17:54:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.181373 on epoch=214
05/21/2022 17:54:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.201669 on epoch=219
05/21/2022 17:54:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.171511 on epoch=224
05/21/2022 17:54:32 - INFO - __main__ - Global step 450 Train loss 0.210811 Classification-F1 0.906158357771261 on epoch=224
05/21/2022 17:54:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.269194 on epoch=229
05/21/2022 17:54:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.269131 on epoch=234
05/21/2022 17:54:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.188618 on epoch=239
05/21/2022 17:54:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.171482 on epoch=244
05/21/2022 17:54:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.196272 on epoch=249
05/21/2022 17:54:45 - INFO - __main__ - Global step 500 Train loss 0.218939 Classification-F1 0.9687194525904204 on epoch=249
05/21/2022 17:54:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.174838 on epoch=254
05/21/2022 17:54:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.181541 on epoch=259
05/21/2022 17:54:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.278797 on epoch=264
05/21/2022 17:54:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.151622 on epoch=269
05/21/2022 17:54:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.282589 on epoch=274
05/21/2022 17:54:58 - INFO - __main__ - Global step 550 Train loss 0.213878 Classification-F1 0.8398398398398398 on epoch=274
05/21/2022 17:55:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.298677 on epoch=279
05/21/2022 17:55:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.384931 on epoch=284
05/21/2022 17:55:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.326154 on epoch=289
05/21/2022 17:55:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.297380 on epoch=294
05/21/2022 17:55:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.232764 on epoch=299
05/21/2022 17:55:11 - INFO - __main__ - Global step 600 Train loss 0.307981 Classification-F1 0.8423645320197044 on epoch=299
05/21/2022 17:55:11 - INFO - __main__ - save last model!
05/21/2022 17:55:12 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:55:12 - INFO - __main__ - Printing 3 examples
05/21/2022 17:55:12 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/21/2022 17:55:12 - INFO - __main__ - ['negative']
05/21/2022 17:55:12 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/21/2022 17:55:12 - INFO - __main__ - ['negative']
05/21/2022 17:55:12 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/21/2022 17:55:12 - INFO - __main__ - ['negative']
05/21/2022 17:55:12 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:55:12 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:55:12 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:55:12 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:55:12 - INFO - __main__ - Printing 3 examples
05/21/2022 17:55:12 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
05/21/2022 17:55:12 - INFO - __main__ - ['negative']
05/21/2022 17:55:12 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
05/21/2022 17:55:12 - INFO - __main__ - ['negative']
05/21/2022 17:55:12 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
05/21/2022 17:55:12 - INFO - __main__ - ['negative']
05/21/2022 17:55:12 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:55:12 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:55:12 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:55:14 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:55:14 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:55:14 - INFO - __main__ - Printing 3 examples
05/21/2022 17:55:14 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:55:14 - INFO - __main__ - ['negative']
05/21/2022 17:55:14 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:55:14 - INFO - __main__ - ['negative']
05/21/2022 17:55:14 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:55:14 - INFO - __main__ - ['negative']
05/21/2022 17:55:14 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:55:15 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:55:16 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:55:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:55:17 - INFO - __main__ - Starting training!
05/21/2022 17:55:23 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_87_0.0003_8_predictions.txt
05/21/2022 17:55:23 - INFO - __main__ - Classification-F1 on test data: 0.9019
05/21/2022 17:55:23 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0003, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9019117205484937
05/21/2022 17:55:23 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0002, bsz=8 ...
05/21/2022 17:55:24 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:55:24 - INFO - __main__ - Printing 3 examples
05/21/2022 17:55:24 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/21/2022 17:55:24 - INFO - __main__ - ['negative']
05/21/2022 17:55:24 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/21/2022 17:55:24 - INFO - __main__ - ['negative']
05/21/2022 17:55:24 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/21/2022 17:55:24 - INFO - __main__ - ['negative']
05/21/2022 17:55:24 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:55:24 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:55:24 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:55:24 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:55:24 - INFO - __main__ - Printing 3 examples
05/21/2022 17:55:24 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
05/21/2022 17:55:24 - INFO - __main__ - ['negative']
05/21/2022 17:55:24 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
05/21/2022 17:55:24 - INFO - __main__ - ['negative']
05/21/2022 17:55:24 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
05/21/2022 17:55:24 - INFO - __main__ - ['negative']
05/21/2022 17:55:24 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:55:24 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:55:24 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:55:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:55:28 - INFO - __main__ - Starting training!
05/21/2022 17:55:30 - INFO - __main__ - Step 10 Global step 10 Train loss 23.493984 on epoch=4
05/21/2022 17:55:33 - INFO - __main__ - Step 20 Global step 20 Train loss 17.753107 on epoch=9
05/21/2022 17:55:36 - INFO - __main__ - Step 30 Global step 30 Train loss 13.691164 on epoch=14
05/21/2022 17:55:39 - INFO - __main__ - Step 40 Global step 40 Train loss 10.818569 on epoch=19
05/21/2022 17:55:42 - INFO - __main__ - Step 50 Global step 50 Train loss 10.817790 on epoch=24
05/21/2022 17:55:45 - INFO - __main__ - Global step 50 Train loss 15.314923 Classification-F1 0.0 on epoch=24
05/21/2022 17:55:47 - INFO - __main__ - Step 60 Global step 60 Train loss 7.768671 on epoch=29
05/21/2022 17:55:50 - INFO - __main__ - Step 70 Global step 70 Train loss 5.403620 on epoch=34
05/21/2022 17:55:52 - INFO - __main__ - Step 80 Global step 80 Train loss 4.133457 on epoch=39
05/21/2022 17:55:55 - INFO - __main__ - Step 90 Global step 90 Train loss 3.429869 on epoch=44
05/21/2022 17:55:58 - INFO - __main__ - Step 100 Global step 100 Train loss 2.106859 on epoch=49
05/21/2022 17:55:58 - INFO - __main__ - Global step 100 Train loss 4.568495 Classification-F1 0.3333333333333333 on epoch=49
05/21/2022 17:56:01 - INFO - __main__ - Step 110 Global step 110 Train loss 1.908298 on epoch=54
05/21/2022 17:56:04 - INFO - __main__ - Step 120 Global step 120 Train loss 1.319038 on epoch=59
05/21/2022 17:56:06 - INFO - __main__ - Step 130 Global step 130 Train loss 1.125858 on epoch=64
05/21/2022 17:56:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.587974 on epoch=69
05/21/2022 17:56:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.476374 on epoch=74
05/21/2022 17:56:12 - INFO - __main__ - Global step 150 Train loss 1.083508 Classification-F1 0.9375 on epoch=74
05/21/2022 17:56:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.638073 on epoch=79
05/21/2022 17:56:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.521095 on epoch=84
05/21/2022 17:56:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.649789 on epoch=89
05/21/2022 17:56:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.494604 on epoch=94
05/21/2022 17:56:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.320007 on epoch=99
05/21/2022 17:56:25 - INFO - __main__ - Global step 200 Train loss 0.524714 Classification-F1 0.9687194525904204 on epoch=99
05/21/2022 17:56:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.428039 on epoch=104
05/21/2022 17:56:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.505182 on epoch=109
05/21/2022 17:56:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.367237 on epoch=114
05/21/2022 17:56:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.422137 on epoch=119
05/21/2022 17:56:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.397062 on epoch=124
05/21/2022 17:56:38 - INFO - __main__ - Global step 250 Train loss 0.423931 Classification-F1 0.9687194525904204 on epoch=124
05/21/2022 17:56:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.401893 on epoch=129
05/21/2022 17:56:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.405687 on epoch=134
05/21/2022 17:56:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.407415 on epoch=139
05/21/2022 17:56:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.463265 on epoch=144
05/21/2022 17:56:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.401320 on epoch=149
05/21/2022 17:56:51 - INFO - __main__ - Global step 300 Train loss 0.415916 Classification-F1 0.9687194525904204 on epoch=149
05/21/2022 17:56:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.412036 on epoch=154
05/21/2022 17:56:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.350384 on epoch=159
05/21/2022 17:56:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.436394 on epoch=164
05/21/2022 17:57:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.369628 on epoch=169
05/21/2022 17:57:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.319311 on epoch=174
05/21/2022 17:57:04 - INFO - __main__ - Global step 350 Train loss 0.377551 Classification-F1 0.6945917285259808 on epoch=174
05/21/2022 17:57:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.378601 on epoch=179
05/21/2022 17:57:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.368813 on epoch=184
05/21/2022 17:57:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.366166 on epoch=189
05/21/2022 17:57:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.387193 on epoch=194
05/21/2022 17:57:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.385125 on epoch=199
05/21/2022 17:57:17 - INFO - __main__ - Global step 400 Train loss 0.377180 Classification-F1 0.6666666666666667 on epoch=199
05/21/2022 17:57:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.499458 on epoch=204
05/21/2022 17:57:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.332117 on epoch=209
05/21/2022 17:57:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.294222 on epoch=214
05/21/2022 17:57:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.361155 on epoch=219
05/21/2022 17:57:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.397884 on epoch=224
05/21/2022 17:57:30 - INFO - __main__ - Global step 450 Train loss 0.376967 Classification-F1 0.6101882613510521 on epoch=224
05/21/2022 17:57:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.410866 on epoch=229
05/21/2022 17:57:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.328186 on epoch=234
05/21/2022 17:57:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.315481 on epoch=239
05/21/2022 17:57:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.316134 on epoch=244
05/21/2022 17:57:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.291130 on epoch=249
05/21/2022 17:57:43 - INFO - __main__ - Global step 500 Train loss 0.332359 Classification-F1 0.49090909090909085 on epoch=249
05/21/2022 17:57:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.401702 on epoch=254
05/21/2022 17:57:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.345226 on epoch=259
05/21/2022 17:57:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.324107 on epoch=264
05/21/2022 17:57:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.390179 on epoch=269
05/21/2022 17:57:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.394669 on epoch=274
05/21/2022 17:57:56 - INFO - __main__ - Global step 550 Train loss 0.371177 Classification-F1 0.6945917285259808 on epoch=274
05/21/2022 17:57:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.386905 on epoch=279
05/21/2022 17:58:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.269168 on epoch=284
05/21/2022 17:58:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.388177 on epoch=289
05/21/2022 17:58:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.302225 on epoch=294
05/21/2022 17:58:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.332425 on epoch=299
05/21/2022 17:58:09 - INFO - __main__ - Global step 600 Train loss 0.335780 Classification-F1 0.6536796536796536 on epoch=299
05/21/2022 17:58:09 - INFO - __main__ - save last model!
05/21/2022 17:58:10 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:58:10 - INFO - __main__ - Printing 3 examples
05/21/2022 17:58:10 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/21/2022 17:58:10 - INFO - __main__ - ['negative']
05/21/2022 17:58:10 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/21/2022 17:58:10 - INFO - __main__ - ['negative']
05/21/2022 17:58:10 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/21/2022 17:58:10 - INFO - __main__ - ['negative']
05/21/2022 17:58:10 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:58:10 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:58:10 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:58:10 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:58:10 - INFO - __main__ - Printing 3 examples
05/21/2022 17:58:10 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
05/21/2022 17:58:10 - INFO - __main__ - ['negative']
05/21/2022 17:58:10 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
05/21/2022 17:58:10 - INFO - __main__ - ['negative']
05/21/2022 17:58:10 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
05/21/2022 17:58:10 - INFO - __main__ - ['negative']
05/21/2022 17:58:10 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:58:10 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:58:10 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:58:11 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 17:58:12 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 17:58:12 - INFO - __main__ - Printing 3 examples
05/21/2022 17:58:12 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 17:58:12 - INFO - __main__ - ['negative']
05/21/2022 17:58:12 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 17:58:12 - INFO - __main__ - ['negative']
05/21/2022 17:58:12 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 17:58:12 - INFO - __main__ - ['negative']
05/21/2022 17:58:12 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:58:12 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:58:13 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 17:58:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:58:13 - INFO - __main__ - Starting training!
05/21/2022 17:58:20 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_87_0.0002_8_predictions.txt
05/21/2022 17:58:20 - INFO - __main__ - Classification-F1 on test data: 0.9209
05/21/2022 17:58:20 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0002, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9208917007383108
05/21/2022 17:58:20 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.0001, bsz=8 ...
05/21/2022 17:58:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:58:21 - INFO - __main__ - Printing 3 examples
05/21/2022 17:58:21 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
05/21/2022 17:58:21 - INFO - __main__ - ['negative']
05/21/2022 17:58:21 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
05/21/2022 17:58:21 - INFO - __main__ - ['negative']
05/21/2022 17:58:21 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
05/21/2022 17:58:21 - INFO - __main__ - ['negative']
05/21/2022 17:58:21 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:58:21 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:58:21 - INFO - __main__ - Loaded 32 examples from train data
05/21/2022 17:58:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 17:58:21 - INFO - __main__ - Printing 3 examples
05/21/2022 17:58:21 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
05/21/2022 17:58:21 - INFO - __main__ - ['negative']
05/21/2022 17:58:21 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
05/21/2022 17:58:21 - INFO - __main__ - ['negative']
05/21/2022 17:58:21 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
05/21/2022 17:58:21 - INFO - __main__ - ['negative']
05/21/2022 17:58:21 - INFO - __main__ - Tokenizing Input ...
05/21/2022 17:58:21 - INFO - __main__ - Tokenizing Output ...
05/21/2022 17:58:21 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 17:58:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 17:58:25 - INFO - __main__ - Starting training!
05/21/2022 17:58:27 - INFO - __main__ - Step 10 Global step 10 Train loss 23.667150 on epoch=4
05/21/2022 17:58:29 - INFO - __main__ - Step 20 Global step 20 Train loss 19.599203 on epoch=9
05/21/2022 17:58:32 - INFO - __main__ - Step 30 Global step 30 Train loss 16.860918 on epoch=14
05/21/2022 17:58:34 - INFO - __main__ - Step 40 Global step 40 Train loss 15.938431 on epoch=19
05/21/2022 17:58:37 - INFO - __main__ - Step 50 Global step 50 Train loss 13.883496 on epoch=24
05/21/2022 17:58:42 - INFO - __main__ - Global step 50 Train loss 17.989840 Classification-F1 0.0 on epoch=24
05/21/2022 17:58:44 - INFO - __main__ - Step 60 Global step 60 Train loss 12.425438 on epoch=29
05/21/2022 17:58:47 - INFO - __main__ - Step 70 Global step 70 Train loss 11.196416 on epoch=34
05/21/2022 17:58:50 - INFO - __main__ - Step 80 Global step 80 Train loss 10.035264 on epoch=39
05/21/2022 17:58:52 - INFO - __main__ - Step 90 Global step 90 Train loss 9.204966 on epoch=44
05/21/2022 17:58:55 - INFO - __main__ - Step 100 Global step 100 Train loss 8.181601 on epoch=49
05/21/2022 17:58:57 - INFO - __main__ - Global step 100 Train loss 10.208736 Classification-F1 0.0 on epoch=49
05/21/2022 17:59:00 - INFO - __main__ - Step 110 Global step 110 Train loss 7.029871 on epoch=54
05/21/2022 17:59:02 - INFO - __main__ - Step 120 Global step 120 Train loss 5.382645 on epoch=59
05/21/2022 17:59:05 - INFO - __main__ - Step 130 Global step 130 Train loss 5.524995 on epoch=64
05/21/2022 17:59:07 - INFO - __main__ - Step 140 Global step 140 Train loss 4.735152 on epoch=69
05/21/2022 17:59:10 - INFO - __main__ - Step 150 Global step 150 Train loss 4.210674 on epoch=74
05/21/2022 17:59:12 - INFO - __main__ - Global step 150 Train loss 5.376667 Classification-F1 0.3333333333333333 on epoch=74
05/21/2022 17:59:15 - INFO - __main__ - Step 160 Global step 160 Train loss 2.504831 on epoch=79
05/21/2022 17:59:17 - INFO - __main__ - Step 170 Global step 170 Train loss 2.895579 on epoch=84
05/21/2022 17:59:20 - INFO - __main__ - Step 180 Global step 180 Train loss 2.778458 on epoch=89
05/21/2022 17:59:22 - INFO - __main__ - Step 190 Global step 190 Train loss 2.861066 on epoch=94
05/21/2022 17:59:25 - INFO - __main__ - Step 200 Global step 200 Train loss 1.396843 on epoch=99
05/21/2022 17:59:25 - INFO - __main__ - Global step 200 Train loss 2.487355 Classification-F1 0.5134502923976608 on epoch=99
05/21/2022 17:59:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.947088 on epoch=104
05/21/2022 17:59:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.813535 on epoch=109
05/21/2022 17:59:33 - INFO - __main__ - Step 230 Global step 230 Train loss 1.051827 on epoch=114
05/21/2022 17:59:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.621806 on epoch=119
05/21/2022 17:59:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.648535 on epoch=124
05/21/2022 17:59:38 - INFO - __main__ - Global step 250 Train loss 0.816558 Classification-F1 0.9375 on epoch=124
05/21/2022 17:59:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.808774 on epoch=129
05/21/2022 17:59:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.336840 on epoch=134
05/21/2022 17:59:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.500938 on epoch=139
05/21/2022 17:59:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.394763 on epoch=144
05/21/2022 17:59:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.445213 on epoch=149
05/21/2022 17:59:52 - INFO - __main__ - Global step 300 Train loss 0.497305 Classification-F1 0.9375 on epoch=149
05/21/2022 17:59:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.424159 on epoch=154
05/21/2022 17:59:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.477079 on epoch=159
05/21/2022 17:59:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.406290 on epoch=164
05/21/2022 18:00:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.370924 on epoch=169
05/21/2022 18:00:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.399897 on epoch=174
05/21/2022 18:00:04 - INFO - __main__ - Global step 350 Train loss 0.415670 Classification-F1 0.9687194525904204 on epoch=174
05/21/2022 18:00:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.214115 on epoch=179
05/21/2022 18:00:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.239049 on epoch=184
05/21/2022 18:00:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.293843 on epoch=189
05/21/2022 18:00:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.376120 on epoch=194
05/21/2022 18:00:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.286601 on epoch=199
05/21/2022 18:00:18 - INFO - __main__ - Global step 400 Train loss 0.281946 Classification-F1 0.9687194525904204 on epoch=199
05/21/2022 18:00:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.457530 on epoch=204
05/21/2022 18:00:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.173709 on epoch=209
05/21/2022 18:00:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.321775 on epoch=214
05/21/2022 18:00:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.258768 on epoch=219
05/21/2022 18:00:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.166819 on epoch=224
05/21/2022 18:00:30 - INFO - __main__ - Global step 450 Train loss 0.275720 Classification-F1 0.9687194525904204 on epoch=224
05/21/2022 18:00:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.121410 on epoch=229
05/21/2022 18:00:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.136939 on epoch=234
05/21/2022 18:00:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.070564 on epoch=239
05/21/2022 18:00:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.095771 on epoch=244
05/21/2022 18:00:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.162396 on epoch=249
05/21/2022 18:00:43 - INFO - __main__ - Global step 500 Train loss 0.117416 Classification-F1 0.9687194525904204 on epoch=249
05/21/2022 18:00:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.133162 on epoch=254
05/21/2022 18:00:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.102892 on epoch=259
05/21/2022 18:00:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.068386 on epoch=264
05/21/2022 18:00:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.019918 on epoch=269
05/21/2022 18:00:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.065454 on epoch=274
05/21/2022 18:00:56 - INFO - __main__ - Global step 550 Train loss 0.077962 Classification-F1 0.9687194525904204 on epoch=274
05/21/2022 18:00:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.040474 on epoch=279
05/21/2022 18:01:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.017678 on epoch=284
05/21/2022 18:01:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.021308 on epoch=289
05/21/2022 18:01:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.016115 on epoch=294
05/21/2022 18:01:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.043654 on epoch=299
05/21/2022 18:01:09 - INFO - __main__ - Global step 600 Train loss 0.027846 Classification-F1 0.9687194525904204 on epoch=299
05/21/2022 18:01:09 - INFO - __main__ - save last model!
05/21/2022 18:01:12 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 18:01:12 - INFO - __main__ - Start tokenizing ... 1000 instances
05/21/2022 18:01:12 - INFO - __main__ - Printing 3 examples
05/21/2022 18:01:12 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
05/21/2022 18:01:12 - INFO - __main__ - ['negative']
05/21/2022 18:01:12 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
05/21/2022 18:01:12 - INFO - __main__ - ['negative']
05/21/2022 18:01:12 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
05/21/2022 18:01:12 - INFO - __main__ - ['negative']
05/21/2022 18:01:12 - INFO - __main__ - Tokenizing Input ...
05/21/2022 18:01:13 - INFO - __main__ - Tokenizing Output ...
05/21/2022 18:01:13 - INFO - __main__ - Loaded 1000 examples from test data
05/21/2022 18:01:20 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-amazon_polarity/amazon_polarity_16_87_0.0001_8_predictions.txt
05/21/2022 18:01:20 - INFO - __main__ - Classification-F1 on test data: 0.9299
05/21/2022 18:01:20 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.0001, bsz=8, dev_performance=0.9687194525904204, test_performance=0.929936943248924
