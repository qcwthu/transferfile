05/17/2022 10:43:03 - INFO - __main__ - Namespace(task_dir='data/superglue-cb/', task_name='superglue-cb', identifier='T5-base-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-cls2cls/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-base', cuda='2,3')
05/17/2022 10:43:03 - INFO - __main__ - models/T5-base-ft-cls2cls/singletask-superglue-cb
05/17/2022 10:43:03 - INFO - __main__ - Namespace(task_dir='data/superglue-cb/', task_name='superglue-cb', identifier='T5-base-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-cls2cls/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-base', cuda='2,3')
05/17/2022 10:43:03 - INFO - __main__ - models/T5-base-ft-cls2cls/singletask-superglue-cb
05/17/2022 10:43:05 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/17/2022 10:43:05 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/17/2022 10:43:05 - INFO - __main__ - args.device: cuda:0
05/17/2022 10:43:05 - INFO - __main__ - Using 2 gpus
05/17/2022 10:43:05 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_16_100', 'superglue-cb_16_13', 'superglue-cb_16_21', 'superglue-cb_16_42', 'superglue-cb_16_87']
05/17/2022 10:43:05 - INFO - __main__ - args.device: cuda:1
05/17/2022 10:43:05 - INFO - __main__ - Using 2 gpus
05/17/2022 10:43:05 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_16_100', 'superglue-cb_16_13', 'superglue-cb_16_21', 'superglue-cb_16_42', 'superglue-cb_16_87']
05/17/2022 10:43:10 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.0005, bsz=8 ...
05/17/2022 10:43:11 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 10:43:11 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 10:43:11 - INFO - __main__ - Printing 3 examples
05/17/2022 10:43:11 - INFO - __main__ - Printing 3 examples
05/17/2022 10:43:11 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/17/2022 10:43:11 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/17/2022 10:43:11 - INFO - __main__ - ['contradiction']
05/17/2022 10:43:11 - INFO - __main__ - ['contradiction']
05/17/2022 10:43:11 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/17/2022 10:43:11 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/17/2022 10:43:11 - INFO - __main__ - ['contradiction']
05/17/2022 10:43:11 - INFO - __main__ - ['contradiction']
05/17/2022 10:43:11 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/17/2022 10:43:11 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/17/2022 10:43:11 - INFO - __main__ - ['contradiction']
05/17/2022 10:43:11 - INFO - __main__ - ['contradiction']
05/17/2022 10:43:11 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:43:11 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:43:11 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:43:11 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:43:11 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 10:43:11 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 10:43:11 - INFO - __main__ - Printing 3 examples
05/17/2022 10:43:11 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
05/17/2022 10:43:11 - INFO - __main__ - ['contradiction']
05/17/2022 10:43:11 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/17/2022 10:43:11 - INFO - __main__ - ['contradiction']
05/17/2022 10:43:11 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
05/17/2022 10:43:11 - INFO - __main__ - ['contradiction']
05/17/2022 10:43:11 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:43:11 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 10:43:11 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 10:43:11 - INFO - __main__ - Printing 3 examples
05/17/2022 10:43:11 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
05/17/2022 10:43:11 - INFO - __main__ - ['contradiction']
05/17/2022 10:43:11 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/17/2022 10:43:11 - INFO - __main__ - ['contradiction']
05/17/2022 10:43:11 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
05/17/2022 10:43:11 - INFO - __main__ - ['contradiction']
05/17/2022 10:43:11 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:43:11 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:43:11 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:43:11 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 10:43:11 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 10:43:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 10:43:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 10:43:17 - INFO - __main__ - Starting training!
05/17/2022 10:43:17 - INFO - __main__ - Starting training!
05/17/2022 10:43:23 - INFO - __main__ - Step 10 Global step 10 Train loss 18.069210 on epoch=3
05/17/2022 10:43:26 - INFO - __main__ - Step 20 Global step 20 Train loss 13.451063 on epoch=6
05/17/2022 10:43:29 - INFO - __main__ - Step 30 Global step 30 Train loss 10.135462 on epoch=9
05/17/2022 10:43:32 - INFO - __main__ - Step 40 Global step 40 Train loss 7.505745 on epoch=13
05/17/2022 10:43:34 - INFO - __main__ - Step 50 Global step 50 Train loss 5.102290 on epoch=16
05/17/2022 10:43:35 - INFO - __main__ - Global step 50 Train loss 10.852754 ACC 0.0 on epoch=16
05/17/2022 10:43:39 - INFO - __main__ - Step 60 Global step 60 Train loss 4.711202 on epoch=19
05/17/2022 10:43:42 - INFO - __main__ - Step 70 Global step 70 Train loss 3.482101 on epoch=23
05/17/2022 10:43:44 - INFO - __main__ - Step 80 Global step 80 Train loss 3.781743 on epoch=26
05/17/2022 10:43:47 - INFO - __main__ - Step 90 Global step 90 Train loss 3.654217 on epoch=29
05/17/2022 10:43:50 - INFO - __main__ - Step 100 Global step 100 Train loss 2.859874 on epoch=33
05/17/2022 10:43:51 - INFO - __main__ - Global step 100 Train loss 3.697828 ACC 0.46875 on epoch=33
05/17/2022 10:43:54 - INFO - __main__ - Step 110 Global step 110 Train loss 2.163067 on epoch=36
05/17/2022 10:43:57 - INFO - __main__ - Step 120 Global step 120 Train loss 1.793645 on epoch=39
05/17/2022 10:44:00 - INFO - __main__ - Step 130 Global step 130 Train loss 1.813309 on epoch=43
05/17/2022 10:44:03 - INFO - __main__ - Step 140 Global step 140 Train loss 2.177996 on epoch=46
05/17/2022 10:44:06 - INFO - __main__ - Step 150 Global step 150 Train loss 1.755212 on epoch=49
05/17/2022 10:44:07 - INFO - __main__ - Global step 150 Train loss 1.940646 ACC 0.375 on epoch=49
05/17/2022 10:44:10 - INFO - __main__ - Step 160 Global step 160 Train loss 1.840376 on epoch=53
05/17/2022 10:44:13 - INFO - __main__ - Step 170 Global step 170 Train loss 1.932276 on epoch=56
05/17/2022 10:44:16 - INFO - __main__ - Step 180 Global step 180 Train loss 1.738884 on epoch=59
05/17/2022 10:44:19 - INFO - __main__ - Step 190 Global step 190 Train loss 1.549128 on epoch=63
05/17/2022 10:44:22 - INFO - __main__ - Step 200 Global step 200 Train loss 1.148577 on epoch=66
05/17/2022 10:44:22 - INFO - __main__ - Global step 200 Train loss 1.641848 ACC 0.5 on epoch=66
05/17/2022 10:44:25 - INFO - __main__ - Step 210 Global step 210 Train loss 1.263128 on epoch=69
05/17/2022 10:44:28 - INFO - __main__ - Step 220 Global step 220 Train loss 1.102771 on epoch=73
05/17/2022 10:44:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.767447 on epoch=76
05/17/2022 10:44:34 - INFO - __main__ - Step 240 Global step 240 Train loss 1.027618 on epoch=79
05/17/2022 10:44:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.911441 on epoch=83
05/17/2022 10:44:38 - INFO - __main__ - Global step 250 Train loss 1.014481 ACC 0.25 on epoch=83
05/17/2022 10:44:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.977483 on epoch=86
05/17/2022 10:44:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.805465 on epoch=89
05/17/2022 10:44:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.772846 on epoch=93
05/17/2022 10:44:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.733679 on epoch=96
05/17/2022 10:44:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.813928 on epoch=99
05/17/2022 10:44:52 - INFO - __main__ - Global step 300 Train loss 0.820680 ACC 0.0 on epoch=99
05/17/2022 10:44:55 - INFO - __main__ - Step 310 Global step 310 Train loss 1.069412 on epoch=103
05/17/2022 10:44:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.996318 on epoch=106
05/17/2022 10:45:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.748070 on epoch=109
05/17/2022 10:45:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.688340 on epoch=113
05/17/2022 10:45:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.688671 on epoch=116
05/17/2022 10:45:07 - INFO - __main__ - Global step 350 Train loss 0.838162 ACC 0.28125 on epoch=116
05/17/2022 10:45:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.676827 on epoch=119
05/17/2022 10:45:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.689766 on epoch=123
05/17/2022 10:45:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.604313 on epoch=126
05/17/2022 10:45:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.558935 on epoch=129
05/17/2022 10:45:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.513421 on epoch=133
05/17/2022 10:45:21 - INFO - __main__ - Global step 400 Train loss 0.608652 ACC 0.28125 on epoch=133
05/17/2022 10:45:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.574906 on epoch=136
05/17/2022 10:45:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.528374 on epoch=139
05/17/2022 10:45:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.586345 on epoch=143
05/17/2022 10:45:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.516958 on epoch=146
05/17/2022 10:45:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.591386 on epoch=149
05/17/2022 10:45:34 - INFO - __main__ - Global step 450 Train loss 0.559594 ACC 0.0 on epoch=149
05/17/2022 10:45:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.505911 on epoch=153
05/17/2022 10:45:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.551863 on epoch=156
05/17/2022 10:45:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.452422 on epoch=159
05/17/2022 10:45:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.526092 on epoch=163
05/17/2022 10:45:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.475104 on epoch=166
05/17/2022 10:45:47 - INFO - __main__ - Global step 500 Train loss 0.502278 ACC 0.21875 on epoch=166
05/17/2022 10:45:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.506913 on epoch=169
05/17/2022 10:45:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.430662 on epoch=173
05/17/2022 10:45:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.500739 on epoch=176
05/17/2022 10:45:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.526276 on epoch=179
05/17/2022 10:46:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.681980 on epoch=183
05/17/2022 10:46:01 - INFO - __main__ - Global step 550 Train loss 0.529314 ACC 0.3125 on epoch=183
05/17/2022 10:46:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.490018 on epoch=186
05/17/2022 10:46:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.458874 on epoch=189
05/17/2022 10:46:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.453374 on epoch=193
05/17/2022 10:46:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.491853 on epoch=196
05/17/2022 10:46:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.487386 on epoch=199
05/17/2022 10:46:16 - INFO - __main__ - Global step 600 Train loss 0.476301 ACC 0.0 on epoch=199
05/17/2022 10:46:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.410894 on epoch=203
05/17/2022 10:46:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.446064 on epoch=206
05/17/2022 10:46:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.468123 on epoch=209
05/17/2022 10:46:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.541337 on epoch=213
05/17/2022 10:46:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.438976 on epoch=216
05/17/2022 10:46:30 - INFO - __main__ - Global step 650 Train loss 0.461079 ACC 0.0 on epoch=216
05/17/2022 10:46:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.445668 on epoch=219
05/17/2022 10:46:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.411300 on epoch=223
05/17/2022 10:46:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.409043 on epoch=226
05/17/2022 10:46:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.436991 on epoch=229
05/17/2022 10:46:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.426985 on epoch=233
05/17/2022 10:46:44 - INFO - __main__ - Global step 700 Train loss 0.425997 ACC 0.0 on epoch=233
05/17/2022 10:46:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.412814 on epoch=236
05/17/2022 10:46:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.442172 on epoch=239
05/17/2022 10:46:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.398071 on epoch=243
05/17/2022 10:46:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.441720 on epoch=246
05/17/2022 10:46:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.426879 on epoch=249
05/17/2022 10:46:58 - INFO - __main__ - Global step 750 Train loss 0.424331 ACC 0.1875 on epoch=249
05/17/2022 10:47:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.451233 on epoch=253
05/17/2022 10:47:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.442171 on epoch=256
05/17/2022 10:47:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.440526 on epoch=259
05/17/2022 10:47:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.408843 on epoch=263
05/17/2022 10:47:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.474116 on epoch=266
05/17/2022 10:47:13 - INFO - __main__ - Global step 800 Train loss 0.443378 ACC 0.40625 on epoch=266
05/17/2022 10:47:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.462799 on epoch=269
05/17/2022 10:47:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.455379 on epoch=273
05/17/2022 10:47:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.405695 on epoch=276
05/17/2022 10:47:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.409609 on epoch=279
05/17/2022 10:47:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.378478 on epoch=283
05/17/2022 10:47:28 - INFO - __main__ - Global step 850 Train loss 0.422392 ACC 0.34375 on epoch=283
05/17/2022 10:47:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.425585 on epoch=286
05/17/2022 10:47:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.440362 on epoch=289
05/17/2022 10:47:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.425262 on epoch=293
05/17/2022 10:47:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.427677 on epoch=296
05/17/2022 10:47:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.432462 on epoch=299
05/17/2022 10:47:42 - INFO - __main__ - Global step 900 Train loss 0.430270 ACC 0.0 on epoch=299
05/17/2022 10:47:42 - INFO - __main__ - save last model!
05/17/2022 10:47:42 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 10:47:42 - INFO - __main__ - Printing 3 examples
05/17/2022 10:47:42 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/17/2022 10:47:42 - INFO - __main__ - ['contradiction']
05/17/2022 10:47:42 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/17/2022 10:47:42 - INFO - __main__ - ['contradiction']
05/17/2022 10:47:42 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/17/2022 10:47:42 - INFO - __main__ - ['contradiction']
05/17/2022 10:47:42 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:47:43 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:47:43 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 10:47:43 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 10:47:43 - INFO - __main__ - Printing 3 examples
05/17/2022 10:47:43 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
05/17/2022 10:47:43 - INFO - __main__ - ['contradiction']
05/17/2022 10:47:43 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/17/2022 10:47:43 - INFO - __main__ - ['contradiction']
05/17/2022 10:47:43 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
05/17/2022 10:47:43 - INFO - __main__ - ['contradiction']
05/17/2022 10:47:43 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:47:43 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:47:43 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 10:47:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 10:47:48 - INFO - __main__ - Starting training!
05/17/2022 10:47:58 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 10:47:59 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 10:47:59 - INFO - __main__ - Printing 3 examples
05/17/2022 10:47:59 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 10:47:59 - INFO - __main__ - ['contradiction']
05/17/2022 10:47:59 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 10:47:59 - INFO - __main__ - ['neutral']
05/17/2022 10:47:59 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 10:47:59 - INFO - __main__ - ['entailment']
05/17/2022 10:47:59 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:47:59 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:47:59 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 10:48:00 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_100_0.0005_8_predictions.txt
05/17/2022 10:48:00 - INFO - __main__ - ACC on test data: 0.5000
05/17/2022 10:48:00 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.0005, bsz=8, dev_performance=0.5, test_performance=0.5
05/17/2022 10:48:00 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.0003, bsz=8 ...
05/17/2022 10:48:01 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 10:48:01 - INFO - __main__ - Printing 3 examples
05/17/2022 10:48:01 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/17/2022 10:48:01 - INFO - __main__ - ['contradiction']
05/17/2022 10:48:01 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/17/2022 10:48:01 - INFO - __main__ - ['contradiction']
05/17/2022 10:48:01 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/17/2022 10:48:01 - INFO - __main__ - ['contradiction']
05/17/2022 10:48:01 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:48:01 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:48:01 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 10:48:01 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 10:48:01 - INFO - __main__ - Printing 3 examples
05/17/2022 10:48:01 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
05/17/2022 10:48:01 - INFO - __main__ - ['contradiction']
05/17/2022 10:48:01 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/17/2022 10:48:01 - INFO - __main__ - ['contradiction']
05/17/2022 10:48:01 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
05/17/2022 10:48:01 - INFO - __main__ - ['contradiction']
05/17/2022 10:48:01 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:48:01 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:48:01 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 10:48:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 10:48:05 - INFO - __main__ - Starting training!
05/17/2022 10:48:07 - INFO - __main__ - Step 10 Global step 10 Train loss 18.208471 on epoch=3
05/17/2022 10:48:10 - INFO - __main__ - Step 20 Global step 20 Train loss 13.665120 on epoch=6
05/17/2022 10:48:12 - INFO - __main__ - Step 30 Global step 30 Train loss 9.351595 on epoch=9
05/17/2022 10:48:15 - INFO - __main__ - Step 40 Global step 40 Train loss 6.736904 on epoch=13
05/17/2022 10:48:18 - INFO - __main__ - Step 50 Global step 50 Train loss 5.818716 on epoch=16
05/17/2022 10:48:18 - INFO - __main__ - Global step 50 Train loss 10.756161 ACC 0.03125 on epoch=16
05/17/2022 10:48:22 - INFO - __main__ - Step 60 Global step 60 Train loss 5.226154 on epoch=19
05/17/2022 10:48:25 - INFO - __main__ - Step 70 Global step 70 Train loss 2.961105 on epoch=23
05/17/2022 10:48:27 - INFO - __main__ - Step 80 Global step 80 Train loss 2.913536 on epoch=26
05/17/2022 10:48:30 - INFO - __main__ - Step 90 Global step 90 Train loss 2.068753 on epoch=29
05/17/2022 10:48:33 - INFO - __main__ - Step 100 Global step 100 Train loss 1.890414 on epoch=33
05/17/2022 10:48:33 - INFO - __main__ - Global step 100 Train loss 3.011992 ACC 0.5 on epoch=33
05/17/2022 10:48:37 - INFO - __main__ - Step 110 Global step 110 Train loss 1.782371 on epoch=36
05/17/2022 10:48:39 - INFO - __main__ - Step 120 Global step 120 Train loss 2.355693 on epoch=39
05/17/2022 10:48:42 - INFO - __main__ - Step 130 Global step 130 Train loss 2.002289 on epoch=43
05/17/2022 10:48:45 - INFO - __main__ - Step 140 Global step 140 Train loss 1.490078 on epoch=46
05/17/2022 10:48:48 - INFO - __main__ - Step 150 Global step 150 Train loss 1.517266 on epoch=49
05/17/2022 10:48:48 - INFO - __main__ - Global step 150 Train loss 1.829539 ACC 0.34375 on epoch=49
05/17/2022 10:48:51 - INFO - __main__ - Step 160 Global step 160 Train loss 1.107567 on epoch=53
05/17/2022 10:48:53 - INFO - __main__ - Step 170 Global step 170 Train loss 1.231424 on epoch=56
05/17/2022 10:48:56 - INFO - __main__ - Step 180 Global step 180 Train loss 1.117061 on epoch=59
05/17/2022 10:48:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.997883 on epoch=63
05/17/2022 10:49:02 - INFO - __main__ - Step 200 Global step 200 Train loss 1.232193 on epoch=66
05/17/2022 10:49:02 - INFO - __main__ - Global step 200 Train loss 1.137226 ACC 0.71875 on epoch=66
05/17/2022 10:49:05 - INFO - __main__ - Step 210 Global step 210 Train loss 1.171496 on epoch=69
05/17/2022 10:49:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.921945 on epoch=73
05/17/2022 10:49:10 - INFO - __main__ - Step 230 Global step 230 Train loss 1.154161 on epoch=76
05/17/2022 10:49:13 - INFO - __main__ - Step 240 Global step 240 Train loss 1.002902 on epoch=79
05/17/2022 10:49:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.908175 on epoch=83
05/17/2022 10:49:16 - INFO - __main__ - Global step 250 Train loss 1.031736 ACC 0.46875 on epoch=83
05/17/2022 10:49:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.959818 on epoch=86
05/17/2022 10:49:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.915545 on epoch=89
05/17/2022 10:49:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.825126 on epoch=93
05/17/2022 10:49:27 - INFO - __main__ - Step 290 Global step 290 Train loss 1.397204 on epoch=96
05/17/2022 10:49:30 - INFO - __main__ - Step 300 Global step 300 Train loss 2.373159 on epoch=99
05/17/2022 10:49:30 - INFO - __main__ - Global step 300 Train loss 1.294170 ACC 0.0 on epoch=99
05/17/2022 10:49:33 - INFO - __main__ - Step 310 Global step 310 Train loss 1.005992 on epoch=103
05/17/2022 10:49:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.800220 on epoch=106
05/17/2022 10:49:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.680669 on epoch=109
05/17/2022 10:49:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.549873 on epoch=113
05/17/2022 10:49:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.715907 on epoch=116
05/17/2022 10:49:44 - INFO - __main__ - Global step 350 Train loss 0.750532 ACC 0.21875 on epoch=116
05/17/2022 10:49:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.817438 on epoch=119
05/17/2022 10:49:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.526189 on epoch=123
05/17/2022 10:49:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.815544 on epoch=126
05/17/2022 10:49:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.689956 on epoch=129
05/17/2022 10:49:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.631201 on epoch=133
05/17/2022 10:49:58 - INFO - __main__ - Global step 400 Train loss 0.696066 ACC 0.4375 on epoch=133
05/17/2022 10:50:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.528320 on epoch=136
05/17/2022 10:50:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.598322 on epoch=139
05/17/2022 10:50:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.541725 on epoch=143
05/17/2022 10:50:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.621492 on epoch=146
05/17/2022 10:50:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.557846 on epoch=149
05/17/2022 10:50:12 - INFO - __main__ - Global step 450 Train loss 0.569541 ACC 0.40625 on epoch=149
05/17/2022 10:50:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.569996 on epoch=153
05/17/2022 10:50:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.582605 on epoch=156
05/17/2022 10:50:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.499054 on epoch=159
05/17/2022 10:50:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.559842 on epoch=163
05/17/2022 10:50:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.776790 on epoch=166
05/17/2022 10:50:27 - INFO - __main__ - Global step 500 Train loss 0.597657 ACC 0.375 on epoch=166
05/17/2022 10:50:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.439129 on epoch=169
05/17/2022 10:50:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.487116 on epoch=173
05/17/2022 10:50:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.402033 on epoch=176
05/17/2022 10:50:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.455119 on epoch=179
05/17/2022 10:50:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.501026 on epoch=183
05/17/2022 10:50:41 - INFO - __main__ - Global step 550 Train loss 0.456885 ACC 0.0625 on epoch=183
05/17/2022 10:50:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.471582 on epoch=186
05/17/2022 10:50:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.452876 on epoch=189
05/17/2022 10:50:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.462356 on epoch=193
05/17/2022 10:50:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.421879 on epoch=196
05/17/2022 10:50:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.445089 on epoch=199
05/17/2022 10:50:55 - INFO - __main__ - Global step 600 Train loss 0.450756 ACC 0.3125 on epoch=199
05/17/2022 10:50:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.439509 on epoch=203
05/17/2022 10:51:01 - INFO - __main__ - Step 620 Global step 620 Train loss 0.495654 on epoch=206
05/17/2022 10:51:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.438061 on epoch=209
05/17/2022 10:51:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.507439 on epoch=213
05/17/2022 10:51:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.420150 on epoch=216
05/17/2022 10:51:10 - INFO - __main__ - Global step 650 Train loss 0.460162 ACC 0.4375 on epoch=216
05/17/2022 10:51:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.384472 on epoch=219
05/17/2022 10:51:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.460968 on epoch=223
05/17/2022 10:51:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.411414 on epoch=226
05/17/2022 10:51:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.500018 on epoch=229
05/17/2022 10:51:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.379526 on epoch=233
05/17/2022 10:51:24 - INFO - __main__ - Global step 700 Train loss 0.427280 ACC 0.3125 on epoch=233
05/17/2022 10:51:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.348955 on epoch=236
05/17/2022 10:51:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.427538 on epoch=239
05/17/2022 10:51:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.477085 on epoch=243
05/17/2022 10:51:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.401981 on epoch=246
05/17/2022 10:51:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.402123 on epoch=249
05/17/2022 10:51:39 - INFO - __main__ - Global step 750 Train loss 0.411536 ACC 0.375 on epoch=249
05/17/2022 10:51:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.407357 on epoch=253
05/17/2022 10:51:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.419714 on epoch=256
05/17/2022 10:51:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.412050 on epoch=259
05/17/2022 10:51:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.410273 on epoch=263
05/17/2022 10:51:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.405645 on epoch=266
05/17/2022 10:51:53 - INFO - __main__ - Global step 800 Train loss 0.411008 ACC 0.46875 on epoch=266
05/17/2022 10:51:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.387894 on epoch=269
05/17/2022 10:51:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.415206 on epoch=273
05/17/2022 10:52:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.472360 on epoch=276
05/17/2022 10:52:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.371256 on epoch=279
05/17/2022 10:52:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.412997 on epoch=283
05/17/2022 10:52:07 - INFO - __main__ - Global step 850 Train loss 0.411943 ACC 0.0 on epoch=283
05/17/2022 10:52:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.420679 on epoch=286
05/17/2022 10:52:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.413472 on epoch=289
05/17/2022 10:52:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.340723 on epoch=293
05/17/2022 10:52:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.414929 on epoch=296
05/17/2022 10:52:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.425667 on epoch=299
05/17/2022 10:52:22 - INFO - __main__ - Global step 900 Train loss 0.403094 ACC 0.34375 on epoch=299
05/17/2022 10:52:22 - INFO - __main__ - save last model!
05/17/2022 10:52:22 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 10:52:22 - INFO - __main__ - Printing 3 examples
05/17/2022 10:52:22 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/17/2022 10:52:22 - INFO - __main__ - ['contradiction']
05/17/2022 10:52:22 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/17/2022 10:52:22 - INFO - __main__ - ['contradiction']
05/17/2022 10:52:22 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/17/2022 10:52:22 - INFO - __main__ - ['contradiction']
05/17/2022 10:52:22 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:52:23 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:52:23 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 10:52:23 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 10:52:23 - INFO - __main__ - Printing 3 examples
05/17/2022 10:52:23 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
05/17/2022 10:52:23 - INFO - __main__ - ['contradiction']
05/17/2022 10:52:23 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/17/2022 10:52:23 - INFO - __main__ - ['contradiction']
05/17/2022 10:52:23 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
05/17/2022 10:52:23 - INFO - __main__ - ['contradiction']
05/17/2022 10:52:23 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:52:23 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:52:23 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 10:52:25 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 10:52:25 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 10:52:25 - INFO - __main__ - Printing 3 examples
05/17/2022 10:52:25 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 10:52:25 - INFO - __main__ - ['contradiction']
05/17/2022 10:52:25 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 10:52:25 - INFO - __main__ - ['neutral']
05/17/2022 10:52:25 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 10:52:25 - INFO - __main__ - ['entailment']
05/17/2022 10:52:25 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:52:25 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:52:25 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 10:52:26 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_100_0.0003_8_predictions.txt
05/17/2022 10:52:26 - INFO - __main__ - ACC on test data: 0.6429
05/17/2022 10:52:26 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.0003, bsz=8, dev_performance=0.71875, test_performance=0.6428571428571429
05/17/2022 10:52:26 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.0002, bsz=8 ...
05/17/2022 10:52:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 10:52:27 - INFO - __main__ - Starting training!
05/17/2022 10:52:27 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 10:52:27 - INFO - __main__ - Printing 3 examples
05/17/2022 10:52:27 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/17/2022 10:52:27 - INFO - __main__ - ['contradiction']
05/17/2022 10:52:27 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/17/2022 10:52:27 - INFO - __main__ - ['contradiction']
05/17/2022 10:52:27 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/17/2022 10:52:27 - INFO - __main__ - ['contradiction']
05/17/2022 10:52:27 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:52:27 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:52:27 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 10:52:27 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 10:52:27 - INFO - __main__ - Printing 3 examples
05/17/2022 10:52:27 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
05/17/2022 10:52:27 - INFO - __main__ - ['contradiction']
05/17/2022 10:52:27 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/17/2022 10:52:27 - INFO - __main__ - ['contradiction']
05/17/2022 10:52:27 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
05/17/2022 10:52:27 - INFO - __main__ - ['contradiction']
05/17/2022 10:52:27 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:52:27 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:52:27 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 10:52:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 10:52:32 - INFO - __main__ - Starting training!
05/17/2022 10:52:34 - INFO - __main__ - Step 10 Global step 10 Train loss 17.816353 on epoch=3
05/17/2022 10:52:37 - INFO - __main__ - Step 20 Global step 20 Train loss 16.527905 on epoch=6
05/17/2022 10:52:39 - INFO - __main__ - Step 30 Global step 30 Train loss 11.381261 on epoch=9
05/17/2022 10:52:42 - INFO - __main__ - Step 40 Global step 40 Train loss 8.486368 on epoch=13
05/17/2022 10:52:45 - INFO - __main__ - Step 50 Global step 50 Train loss 8.205546 on epoch=16
05/17/2022 10:52:47 - INFO - __main__ - Global step 50 Train loss 12.483486 ACC 0.0 on epoch=16
05/17/2022 10:52:50 - INFO - __main__ - Step 60 Global step 60 Train loss 6.074880 on epoch=19
05/17/2022 10:52:53 - INFO - __main__ - Step 70 Global step 70 Train loss 5.492188 on epoch=23
05/17/2022 10:52:56 - INFO - __main__ - Step 80 Global step 80 Train loss 6.005363 on epoch=26
05/17/2022 10:52:59 - INFO - __main__ - Step 90 Global step 90 Train loss 5.648243 on epoch=29
05/17/2022 10:53:02 - INFO - __main__ - Step 100 Global step 100 Train loss 4.105854 on epoch=33
05/17/2022 10:53:02 - INFO - __main__ - Global step 100 Train loss 5.465305 ACC 0.375 on epoch=33
05/17/2022 10:53:05 - INFO - __main__ - Step 110 Global step 110 Train loss 3.792688 on epoch=36
05/17/2022 10:53:08 - INFO - __main__ - Step 120 Global step 120 Train loss 3.959990 on epoch=39
05/17/2022 10:53:11 - INFO - __main__ - Step 130 Global step 130 Train loss 2.982111 on epoch=43
05/17/2022 10:53:14 - INFO - __main__ - Step 140 Global step 140 Train loss 3.235652 on epoch=46
05/17/2022 10:53:16 - INFO - __main__ - Step 150 Global step 150 Train loss 2.626867 on epoch=49
05/17/2022 10:53:17 - INFO - __main__ - Global step 150 Train loss 3.319462 ACC 0.46875 on epoch=49
05/17/2022 10:53:20 - INFO - __main__ - Step 160 Global step 160 Train loss 2.045851 on epoch=53
05/17/2022 10:53:22 - INFO - __main__ - Step 170 Global step 170 Train loss 2.526281 on epoch=56
05/17/2022 10:53:25 - INFO - __main__ - Step 180 Global step 180 Train loss 2.534952 on epoch=59
05/17/2022 10:53:28 - INFO - __main__ - Step 190 Global step 190 Train loss 2.323677 on epoch=63
05/17/2022 10:53:30 - INFO - __main__ - Step 200 Global step 200 Train loss 1.845547 on epoch=66
05/17/2022 10:53:31 - INFO - __main__ - Global step 200 Train loss 2.255261 ACC 0.5 on epoch=66
05/17/2022 10:53:34 - INFO - __main__ - Step 210 Global step 210 Train loss 2.543047 on epoch=69
05/17/2022 10:53:36 - INFO - __main__ - Step 220 Global step 220 Train loss 2.087103 on epoch=73
05/17/2022 10:53:39 - INFO - __main__ - Step 230 Global step 230 Train loss 1.651908 on epoch=76
05/17/2022 10:53:42 - INFO - __main__ - Step 240 Global step 240 Train loss 2.185256 on epoch=79
05/17/2022 10:53:45 - INFO - __main__ - Step 250 Global step 250 Train loss 1.981224 on epoch=83
05/17/2022 10:53:45 - INFO - __main__ - Global step 250 Train loss 2.089708 ACC 0.46875 on epoch=83
05/17/2022 10:53:48 - INFO - __main__ - Step 260 Global step 260 Train loss 1.136929 on epoch=86
05/17/2022 10:53:51 - INFO - __main__ - Step 270 Global step 270 Train loss 1.796272 on epoch=89
05/17/2022 10:53:54 - INFO - __main__ - Step 280 Global step 280 Train loss 1.440464 on epoch=93
05/17/2022 10:53:57 - INFO - __main__ - Step 290 Global step 290 Train loss 1.730653 on epoch=96
05/17/2022 10:54:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.996431 on epoch=99
05/17/2022 10:54:00 - INFO - __main__ - Global step 300 Train loss 1.420150 ACC 0.0625 on epoch=99
05/17/2022 10:54:03 - INFO - __main__ - Step 310 Global step 310 Train loss 1.085885 on epoch=103
05/17/2022 10:54:06 - INFO - __main__ - Step 320 Global step 320 Train loss 1.268331 on epoch=106
05/17/2022 10:54:09 - INFO - __main__ - Step 330 Global step 330 Train loss 1.418426 on epoch=109
05/17/2022 10:54:12 - INFO - __main__ - Step 340 Global step 340 Train loss 1.181787 on epoch=113
05/17/2022 10:54:15 - INFO - __main__ - Step 350 Global step 350 Train loss 1.226343 on epoch=116
05/17/2022 10:54:15 - INFO - __main__ - Global step 350 Train loss 1.236154 ACC 0.46875 on epoch=116
05/17/2022 10:54:18 - INFO - __main__ - Step 360 Global step 360 Train loss 1.413497 on epoch=119
05/17/2022 10:54:21 - INFO - __main__ - Step 370 Global step 370 Train loss 1.294210 on epoch=123
05/17/2022 10:54:24 - INFO - __main__ - Step 380 Global step 380 Train loss 1.047632 on epoch=126
05/17/2022 10:54:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.730776 on epoch=129
05/17/2022 10:54:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.854072 on epoch=133
05/17/2022 10:54:31 - INFO - __main__ - Global step 400 Train loss 1.068038 ACC 0.71875 on epoch=133
05/17/2022 10:54:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.981441 on epoch=136
05/17/2022 10:54:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.776958 on epoch=139
05/17/2022 10:54:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.864409 on epoch=143
05/17/2022 10:54:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.947666 on epoch=146
05/17/2022 10:54:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.769707 on epoch=149
05/17/2022 10:54:45 - INFO - __main__ - Global step 450 Train loss 0.868036 ACC 0.84375 on epoch=149
05/17/2022 10:54:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.543331 on epoch=153
05/17/2022 10:54:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.696148 on epoch=156
05/17/2022 10:54:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.762256 on epoch=159
05/17/2022 10:54:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.745769 on epoch=163
05/17/2022 10:54:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.872445 on epoch=166
05/17/2022 10:54:59 - INFO - __main__ - Global step 500 Train loss 0.723990 ACC 0.8125 on epoch=166
05/17/2022 10:55:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.437544 on epoch=169
05/17/2022 10:55:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.506833 on epoch=173
05/17/2022 10:55:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.581323 on epoch=176
05/17/2022 10:55:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.671546 on epoch=179
05/17/2022 10:55:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.388590 on epoch=183
05/17/2022 10:55:13 - INFO - __main__ - Global step 550 Train loss 0.517167 ACC 0.8125 on epoch=183
05/17/2022 10:55:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.379787 on epoch=186
05/17/2022 10:55:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.403136 on epoch=189
05/17/2022 10:55:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.314300 on epoch=193
05/17/2022 10:55:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.494393 on epoch=196
05/17/2022 10:55:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.535627 on epoch=199
05/17/2022 10:55:28 - INFO - __main__ - Global step 600 Train loss 0.425449 ACC 0.84375 on epoch=199
05/17/2022 10:55:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.456056 on epoch=203
05/17/2022 10:55:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.407678 on epoch=206
05/17/2022 10:55:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.465351 on epoch=209
05/17/2022 10:55:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.332267 on epoch=213
05/17/2022 10:55:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.216313 on epoch=216
05/17/2022 10:55:42 - INFO - __main__ - Global step 650 Train loss 0.375533 ACC 0.8125 on epoch=216
05/17/2022 10:55:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.276408 on epoch=219
05/17/2022 10:55:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.232203 on epoch=223
05/17/2022 10:55:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.317603 on epoch=226
05/17/2022 10:55:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.222574 on epoch=229
05/17/2022 10:55:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.186442 on epoch=233
05/17/2022 10:55:58 - INFO - __main__ - Global step 700 Train loss 0.247046 ACC 0.84375 on epoch=233
05/17/2022 10:56:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.189474 on epoch=236
05/17/2022 10:56:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.248456 on epoch=239
05/17/2022 10:56:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.171288 on epoch=243
05/17/2022 10:56:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.324983 on epoch=246
05/17/2022 10:56:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.369148 on epoch=249
05/17/2022 10:56:12 - INFO - __main__ - Global step 750 Train loss 0.260670 ACC 0.8125 on epoch=249
05/17/2022 10:56:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.262367 on epoch=253
05/17/2022 10:56:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.467077 on epoch=256
05/17/2022 10:56:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.577964 on epoch=259
05/17/2022 10:56:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.664171 on epoch=263
05/17/2022 10:56:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.607810 on epoch=266
05/17/2022 10:56:26 - INFO - __main__ - Global step 800 Train loss 0.515878 ACC 0.71875 on epoch=266
05/17/2022 10:56:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.388941 on epoch=269
05/17/2022 10:56:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.398409 on epoch=273
05/17/2022 10:56:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.510049 on epoch=276
05/17/2022 10:56:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.447020 on epoch=279
05/17/2022 10:56:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.543881 on epoch=283
05/17/2022 10:56:41 - INFO - __main__ - Global step 850 Train loss 0.457660 ACC 0.6875 on epoch=283
05/17/2022 10:56:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.481822 on epoch=286
05/17/2022 10:56:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.547985 on epoch=289
05/17/2022 10:56:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.591086 on epoch=293
05/17/2022 10:56:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.560733 on epoch=296
05/17/2022 10:56:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.387409 on epoch=299
05/17/2022 10:56:55 - INFO - __main__ - Global step 900 Train loss 0.513807 ACC 0.71875 on epoch=299
05/17/2022 10:56:55 - INFO - __main__ - save last model!
05/17/2022 10:56:56 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 10:56:56 - INFO - __main__ - Printing 3 examples
05/17/2022 10:56:56 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/17/2022 10:56:56 - INFO - __main__ - ['contradiction']
05/17/2022 10:56:56 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/17/2022 10:56:56 - INFO - __main__ - ['contradiction']
05/17/2022 10:56:56 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/17/2022 10:56:56 - INFO - __main__ - ['contradiction']
05/17/2022 10:56:56 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:56:56 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:56:56 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 10:56:56 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 10:56:56 - INFO - __main__ - Printing 3 examples
05/17/2022 10:56:56 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
05/17/2022 10:56:56 - INFO - __main__ - ['contradiction']
05/17/2022 10:56:56 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/17/2022 10:56:56 - INFO - __main__ - ['contradiction']
05/17/2022 10:56:56 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
05/17/2022 10:56:56 - INFO - __main__ - ['contradiction']
05/17/2022 10:56:56 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:56:56 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:56:56 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 10:56:58 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 10:56:58 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 10:56:58 - INFO - __main__ - Printing 3 examples
05/17/2022 10:56:58 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 10:56:58 - INFO - __main__ - ['contradiction']
05/17/2022 10:56:58 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 10:56:58 - INFO - __main__ - ['neutral']
05/17/2022 10:56:58 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 10:56:58 - INFO - __main__ - ['entailment']
05/17/2022 10:56:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:56:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:56:58 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 10:56:59 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_100_0.0002_8_predictions.txt
05/17/2022 10:56:59 - INFO - __main__ - ACC on test data: 0.7321
05/17/2022 10:56:59 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.0002, bsz=8, dev_performance=0.84375, test_performance=0.7321428571428571
05/17/2022 10:56:59 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.0001, bsz=8 ...
05/17/2022 10:57:00 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 10:57:00 - INFO - __main__ - Printing 3 examples
05/17/2022 10:57:00 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/17/2022 10:57:00 - INFO - __main__ - ['contradiction']
05/17/2022 10:57:00 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/17/2022 10:57:00 - INFO - __main__ - ['contradiction']
05/17/2022 10:57:00 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/17/2022 10:57:00 - INFO - __main__ - ['contradiction']
05/17/2022 10:57:00 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:57:00 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:57:00 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 10:57:00 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 10:57:00 - INFO - __main__ - Printing 3 examples
05/17/2022 10:57:00 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
05/17/2022 10:57:00 - INFO - __main__ - ['contradiction']
05/17/2022 10:57:00 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/17/2022 10:57:00 - INFO - __main__ - ['contradiction']
05/17/2022 10:57:00 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
05/17/2022 10:57:00 - INFO - __main__ - ['contradiction']
05/17/2022 10:57:00 - INFO - __main__ - Tokenizing Input ...
05/17/2022 10:57:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 10:57:00 - INFO - __main__ - Starting training!
05/17/2022 10:57:00 - INFO - __main__ - Tokenizing Output ...
05/17/2022 10:57:00 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 10:57:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 10:57:05 - INFO - __main__ - Starting training!
05/17/2022 10:57:08 - INFO - __main__ - Step 10 Global step 10 Train loss 18.054287 on epoch=3
05/17/2022 10:57:11 - INFO - __main__ - Step 20 Global step 20 Train loss 16.446690 on epoch=6
05/17/2022 10:57:14 - INFO - __main__ - Step 30 Global step 30 Train loss 15.013252 on epoch=9
05/17/2022 10:57:17 - INFO - __main__ - Step 40 Global step 40 Train loss 12.680166 on epoch=13
05/17/2022 10:57:20 - INFO - __main__ - Step 50 Global step 50 Train loss 11.581114 on epoch=16
05/17/2022 10:57:25 - INFO - __main__ - Global step 50 Train loss 14.755101 ACC 0.0 on epoch=16
05/17/2022 10:57:28 - INFO - __main__ - Step 60 Global step 60 Train loss 9.765803 on epoch=19
05/17/2022 10:57:31 - INFO - __main__ - Step 70 Global step 70 Train loss 10.139643 on epoch=23
05/17/2022 10:57:33 - INFO - __main__ - Step 80 Global step 80 Train loss 8.021867 on epoch=26
05/17/2022 10:57:36 - INFO - __main__ - Step 90 Global step 90 Train loss 8.190500 on epoch=29
05/17/2022 10:57:39 - INFO - __main__ - Step 100 Global step 100 Train loss 7.388425 on epoch=33
05/17/2022 10:57:42 - INFO - __main__ - Global step 100 Train loss 8.701247 ACC 0.0 on epoch=33
05/17/2022 10:57:44 - INFO - __main__ - Step 110 Global step 110 Train loss 6.839972 on epoch=36
05/17/2022 10:57:47 - INFO - __main__ - Step 120 Global step 120 Train loss 6.982501 on epoch=39
05/17/2022 10:57:50 - INFO - __main__ - Step 130 Global step 130 Train loss 6.637609 on epoch=43
05/17/2022 10:57:52 - INFO - __main__ - Step 140 Global step 140 Train loss 5.451562 on epoch=46
05/17/2022 10:57:56 - INFO - __main__ - Step 150 Global step 150 Train loss 5.388244 on epoch=49
05/17/2022 10:57:57 - INFO - __main__ - Global step 150 Train loss 6.259977 ACC 0.0 on epoch=49
05/17/2022 10:58:00 - INFO - __main__ - Step 160 Global step 160 Train loss 5.812444 on epoch=53
05/17/2022 10:58:02 - INFO - __main__ - Step 170 Global step 170 Train loss 5.167206 on epoch=56
05/17/2022 10:58:05 - INFO - __main__ - Step 180 Global step 180 Train loss 4.528750 on epoch=59
05/17/2022 10:58:08 - INFO - __main__ - Step 190 Global step 190 Train loss 4.402617 on epoch=63
05/17/2022 10:58:11 - INFO - __main__ - Step 200 Global step 200 Train loss 4.185829 on epoch=66
05/17/2022 10:58:12 - INFO - __main__ - Global step 200 Train loss 4.819369 ACC 0.0625 on epoch=66
05/17/2022 10:58:15 - INFO - __main__ - Step 210 Global step 210 Train loss 3.376174 on epoch=69
05/17/2022 10:58:18 - INFO - __main__ - Step 220 Global step 220 Train loss 3.308680 on epoch=73
05/17/2022 10:58:21 - INFO - __main__ - Step 230 Global step 230 Train loss 4.393818 on epoch=76
05/17/2022 10:58:24 - INFO - __main__ - Step 240 Global step 240 Train loss 3.173639 on epoch=79
05/17/2022 10:58:26 - INFO - __main__ - Step 250 Global step 250 Train loss 3.273522 on epoch=83
05/17/2022 10:58:27 - INFO - __main__ - Global step 250 Train loss 3.505167 ACC 0.0 on epoch=83
05/17/2022 10:58:30 - INFO - __main__ - Step 260 Global step 260 Train loss 2.966399 on epoch=86
05/17/2022 10:58:32 - INFO - __main__ - Step 270 Global step 270 Train loss 2.487091 on epoch=89
05/17/2022 10:58:35 - INFO - __main__ - Step 280 Global step 280 Train loss 2.652775 on epoch=93
05/17/2022 10:58:38 - INFO - __main__ - Step 290 Global step 290 Train loss 2.927990 on epoch=96
05/17/2022 10:58:41 - INFO - __main__ - Step 300 Global step 300 Train loss 2.788317 on epoch=99
05/17/2022 10:58:41 - INFO - __main__ - Global step 300 Train loss 2.764514 ACC 0.5 on epoch=99
05/17/2022 10:58:44 - INFO - __main__ - Step 310 Global step 310 Train loss 3.056637 on epoch=103
05/17/2022 10:58:47 - INFO - __main__ - Step 320 Global step 320 Train loss 3.132118 on epoch=106
05/17/2022 10:58:50 - INFO - __main__ - Step 330 Global step 330 Train loss 2.313915 on epoch=109
05/17/2022 10:58:54 - INFO - __main__ - Step 340 Global step 340 Train loss 2.268750 on epoch=113
05/17/2022 10:58:56 - INFO - __main__ - Step 350 Global step 350 Train loss 2.521248 on epoch=116
05/17/2022 10:58:57 - INFO - __main__ - Global step 350 Train loss 2.658534 ACC 0.5 on epoch=116
05/17/2022 10:58:59 - INFO - __main__ - Step 360 Global step 360 Train loss 2.793455 on epoch=119
05/17/2022 10:59:03 - INFO - __main__ - Step 370 Global step 370 Train loss 2.391754 on epoch=123
05/17/2022 10:59:06 - INFO - __main__ - Step 380 Global step 380 Train loss 2.944279 on epoch=126
05/17/2022 10:59:08 - INFO - __main__ - Step 390 Global step 390 Train loss 2.050341 on epoch=129
05/17/2022 10:59:12 - INFO - __main__ - Step 400 Global step 400 Train loss 2.500139 on epoch=133
05/17/2022 10:59:12 - INFO - __main__ - Global step 400 Train loss 2.535994 ACC 0.5 on epoch=133
05/17/2022 10:59:15 - INFO - __main__ - Step 410 Global step 410 Train loss 1.814083 on epoch=136
05/17/2022 10:59:18 - INFO - __main__ - Step 420 Global step 420 Train loss 1.884964 on epoch=139
05/17/2022 10:59:21 - INFO - __main__ - Step 430 Global step 430 Train loss 1.847994 on epoch=143
05/17/2022 10:59:24 - INFO - __main__ - Step 440 Global step 440 Train loss 2.098856 on epoch=146
05/17/2022 10:59:27 - INFO - __main__ - Step 450 Global step 450 Train loss 2.014855 on epoch=149
05/17/2022 10:59:28 - INFO - __main__ - Global step 450 Train loss 1.932151 ACC 0.5 on epoch=149
05/17/2022 10:59:30 - INFO - __main__ - Step 460 Global step 460 Train loss 1.336164 on epoch=153
05/17/2022 10:59:33 - INFO - __main__ - Step 470 Global step 470 Train loss 2.067892 on epoch=156
05/17/2022 10:59:36 - INFO - __main__ - Step 480 Global step 480 Train loss 1.630446 on epoch=159
05/17/2022 10:59:39 - INFO - __main__ - Step 490 Global step 490 Train loss 2.370286 on epoch=163
05/17/2022 10:59:42 - INFO - __main__ - Step 500 Global step 500 Train loss 1.703479 on epoch=166
05/17/2022 10:59:43 - INFO - __main__ - Global step 500 Train loss 1.821653 ACC 0.4375 on epoch=166
05/17/2022 10:59:46 - INFO - __main__ - Step 510 Global step 510 Train loss 1.787974 on epoch=169
05/17/2022 10:59:48 - INFO - __main__ - Step 520 Global step 520 Train loss 1.677778 on epoch=173
05/17/2022 10:59:51 - INFO - __main__ - Step 530 Global step 530 Train loss 1.427010 on epoch=176
05/17/2022 10:59:54 - INFO - __main__ - Step 540 Global step 540 Train loss 1.739063 on epoch=179
05/17/2022 10:59:57 - INFO - __main__ - Step 550 Global step 550 Train loss 1.877576 on epoch=183
05/17/2022 10:59:57 - INFO - __main__ - Global step 550 Train loss 1.701880 ACC 0.21875 on epoch=183
05/17/2022 11:00:00 - INFO - __main__ - Step 560 Global step 560 Train loss 1.597274 on epoch=186
05/17/2022 11:00:03 - INFO - __main__ - Step 570 Global step 570 Train loss 1.841639 on epoch=189
05/17/2022 11:00:06 - INFO - __main__ - Step 580 Global step 580 Train loss 1.456765 on epoch=193
05/17/2022 11:00:08 - INFO - __main__ - Step 590 Global step 590 Train loss 1.075347 on epoch=196
05/17/2022 11:00:11 - INFO - __main__ - Step 600 Global step 600 Train loss 1.354891 on epoch=199
05/17/2022 11:00:12 - INFO - __main__ - Global step 600 Train loss 1.465183 ACC 0.46875 on epoch=199
05/17/2022 11:00:14 - INFO - __main__ - Step 610 Global step 610 Train loss 1.078667 on epoch=203
05/17/2022 11:00:17 - INFO - __main__ - Step 620 Global step 620 Train loss 1.444024 on epoch=206
05/17/2022 11:00:20 - INFO - __main__ - Step 630 Global step 630 Train loss 1.270421 on epoch=209
05/17/2022 11:00:23 - INFO - __main__ - Step 640 Global step 640 Train loss 1.136547 on epoch=213
05/17/2022 11:00:26 - INFO - __main__ - Step 650 Global step 650 Train loss 1.041497 on epoch=216
05/17/2022 11:00:26 - INFO - __main__ - Global step 650 Train loss 1.194231 ACC 0.5625 on epoch=216
05/17/2022 11:00:29 - INFO - __main__ - Step 660 Global step 660 Train loss 1.219794 on epoch=219
05/17/2022 11:00:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.961019 on epoch=223
05/17/2022 11:00:35 - INFO - __main__ - Step 680 Global step 680 Train loss 1.124922 on epoch=226
05/17/2022 11:00:38 - INFO - __main__ - Step 690 Global step 690 Train loss 1.117065 on epoch=229
05/17/2022 11:00:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.962196 on epoch=233
05/17/2022 11:00:41 - INFO - __main__ - Global step 700 Train loss 1.076999 ACC 0.40625 on epoch=233
05/17/2022 11:00:44 - INFO - __main__ - Step 710 Global step 710 Train loss 1.157532 on epoch=236
05/17/2022 11:00:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.931941 on epoch=239
05/17/2022 11:00:49 - INFO - __main__ - Step 730 Global step 730 Train loss 1.032238 on epoch=243
05/17/2022 11:00:52 - INFO - __main__ - Step 740 Global step 740 Train loss 1.236868 on epoch=246
05/17/2022 11:00:55 - INFO - __main__ - Step 750 Global step 750 Train loss 0.653850 on epoch=249
05/17/2022 11:00:55 - INFO - __main__ - Global step 750 Train loss 1.002486 ACC 0.5625 on epoch=249
05/17/2022 11:00:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.912768 on epoch=253
05/17/2022 11:01:01 - INFO - __main__ - Step 770 Global step 770 Train loss 1.055803 on epoch=256
05/17/2022 11:01:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.897315 on epoch=259
05/17/2022 11:01:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.677077 on epoch=263
05/17/2022 11:01:10 - INFO - __main__ - Step 800 Global step 800 Train loss 0.882948 on epoch=266
05/17/2022 11:01:10 - INFO - __main__ - Global step 800 Train loss 0.885182 ACC 0.65625 on epoch=266
05/17/2022 11:01:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.984336 on epoch=269
05/17/2022 11:01:16 - INFO - __main__ - Step 820 Global step 820 Train loss 1.033654 on epoch=273
05/17/2022 11:01:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.587646 on epoch=276
05/17/2022 11:01:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.679590 on epoch=279
05/17/2022 11:01:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.783383 on epoch=283
05/17/2022 11:01:25 - INFO - __main__ - Global step 850 Train loss 0.813722 ACC 0.78125 on epoch=283
05/17/2022 11:01:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.621058 on epoch=286
05/17/2022 11:01:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.827568 on epoch=289
05/17/2022 11:01:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.584950 on epoch=293
05/17/2022 11:01:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.553511 on epoch=296
05/17/2022 11:01:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.711247 on epoch=299
05/17/2022 11:01:39 - INFO - __main__ - Global step 900 Train loss 0.659667 ACC 0.78125 on epoch=299
05/17/2022 11:01:39 - INFO - __main__ - save last model!
05/17/2022 11:01:40 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:01:40 - INFO - __main__ - Printing 3 examples
05/17/2022 11:01:40 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/17/2022 11:01:40 - INFO - __main__ - ['contradiction']
05/17/2022 11:01:40 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/17/2022 11:01:40 - INFO - __main__ - ['contradiction']
05/17/2022 11:01:40 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/17/2022 11:01:40 - INFO - __main__ - ['contradiction']
05/17/2022 11:01:40 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:01:40 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:01:40 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:01:40 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:01:40 - INFO - __main__ - Printing 3 examples
05/17/2022 11:01:40 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
05/17/2022 11:01:40 - INFO - __main__ - ['contradiction']
05/17/2022 11:01:40 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/17/2022 11:01:40 - INFO - __main__ - ['contradiction']
05/17/2022 11:01:40 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/17/2022 11:01:40 - INFO - __main__ - ['contradiction']
05/17/2022 11:01:40 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:01:40 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:01:40 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:01:42 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 11:01:43 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 11:01:43 - INFO - __main__ - Printing 3 examples
05/17/2022 11:01:43 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 11:01:43 - INFO - __main__ - ['contradiction']
05/17/2022 11:01:43 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 11:01:43 - INFO - __main__ - ['neutral']
05/17/2022 11:01:43 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 11:01:43 - INFO - __main__ - ['entailment']
05/17/2022 11:01:43 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:01:43 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:01:43 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 11:01:44 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_100_0.0001_8_predictions.txt
05/17/2022 11:01:44 - INFO - __main__ - ACC on test data: 0.7500
05/17/2022 11:01:44 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.0001, bsz=8, dev_performance=0.78125, test_performance=0.75
05/17/2022 11:01:44 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.0005, bsz=8 ...
05/17/2022 11:01:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:01:44 - INFO - __main__ - Starting training!
05/17/2022 11:01:45 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:01:45 - INFO - __main__ - Printing 3 examples
05/17/2022 11:01:45 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/17/2022 11:01:45 - INFO - __main__ - ['contradiction']
05/17/2022 11:01:45 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/17/2022 11:01:45 - INFO - __main__ - ['contradiction']
05/17/2022 11:01:45 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/17/2022 11:01:45 - INFO - __main__ - ['contradiction']
05/17/2022 11:01:45 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:01:45 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:01:45 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:01:45 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:01:45 - INFO - __main__ - Printing 3 examples
05/17/2022 11:01:45 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
05/17/2022 11:01:45 - INFO - __main__ - ['contradiction']
05/17/2022 11:01:45 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/17/2022 11:01:45 - INFO - __main__ - ['contradiction']
05/17/2022 11:01:45 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/17/2022 11:01:45 - INFO - __main__ - ['contradiction']
05/17/2022 11:01:45 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:01:45 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:01:45 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:01:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:01:49 - INFO - __main__ - Starting training!
05/17/2022 11:01:52 - INFO - __main__ - Step 10 Global step 10 Train loss 18.985415 on epoch=3
05/17/2022 11:01:55 - INFO - __main__ - Step 20 Global step 20 Train loss 14.888077 on epoch=6
05/17/2022 11:01:57 - INFO - __main__ - Step 30 Global step 30 Train loss 10.078260 on epoch=9
05/17/2022 11:02:00 - INFO - __main__ - Step 40 Global step 40 Train loss 8.068144 on epoch=13
05/17/2022 11:02:03 - INFO - __main__ - Step 50 Global step 50 Train loss 5.601339 on epoch=16
05/17/2022 11:02:03 - INFO - __main__ - Global step 50 Train loss 11.524248 ACC 0.03125 on epoch=16
05/17/2022 11:02:06 - INFO - __main__ - Step 60 Global step 60 Train loss 4.496961 on epoch=19
05/17/2022 11:02:09 - INFO - __main__ - Step 70 Global step 70 Train loss 3.227134 on epoch=23
05/17/2022 11:02:12 - INFO - __main__ - Step 80 Global step 80 Train loss 2.906109 on epoch=26
05/17/2022 11:02:14 - INFO - __main__ - Step 90 Global step 90 Train loss 2.684344 on epoch=29
05/17/2022 11:02:17 - INFO - __main__ - Step 100 Global step 100 Train loss 2.571135 on epoch=33
05/17/2022 11:02:17 - INFO - __main__ - Global step 100 Train loss 3.177137 ACC 0.09375 on epoch=33
05/17/2022 11:02:21 - INFO - __main__ - Step 110 Global step 110 Train loss 2.711050 on epoch=36
05/17/2022 11:02:23 - INFO - __main__ - Step 120 Global step 120 Train loss 2.245724 on epoch=39
05/17/2022 11:02:26 - INFO - __main__ - Step 130 Global step 130 Train loss 2.418781 on epoch=43
05/17/2022 11:02:29 - INFO - __main__ - Step 140 Global step 140 Train loss 1.578905 on epoch=46
05/17/2022 11:02:31 - INFO - __main__ - Step 150 Global step 150 Train loss 1.676890 on epoch=49
05/17/2022 11:02:32 - INFO - __main__ - Global step 150 Train loss 2.126270 ACC 0.3125 on epoch=49
05/17/2022 11:02:35 - INFO - __main__ - Step 160 Global step 160 Train loss 1.718027 on epoch=53
05/17/2022 11:02:37 - INFO - __main__ - Step 170 Global step 170 Train loss 2.042394 on epoch=56
05/17/2022 11:02:40 - INFO - __main__ - Step 180 Global step 180 Train loss 1.927824 on epoch=59
05/17/2022 11:02:43 - INFO - __main__ - Step 190 Global step 190 Train loss 1.313477 on epoch=63
05/17/2022 11:02:45 - INFO - __main__ - Step 200 Global step 200 Train loss 1.136946 on epoch=66
05/17/2022 11:02:46 - INFO - __main__ - Global step 200 Train loss 1.627734 ACC 0.40625 on epoch=66
05/17/2022 11:02:49 - INFO - __main__ - Step 210 Global step 210 Train loss 1.221123 on epoch=69
05/17/2022 11:02:52 - INFO - __main__ - Step 220 Global step 220 Train loss 1.125398 on epoch=73
05/17/2022 11:02:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.833160 on epoch=76
05/17/2022 11:02:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.964339 on epoch=79
05/17/2022 11:03:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.980548 on epoch=83
05/17/2022 11:03:00 - INFO - __main__ - Global step 250 Train loss 1.024914 ACC 0.21875 on epoch=83
05/17/2022 11:03:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.986872 on epoch=86
05/17/2022 11:03:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.677894 on epoch=89
05/17/2022 11:03:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.885783 on epoch=93
05/17/2022 11:03:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.822485 on epoch=96
05/17/2022 11:03:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.923253 on epoch=99
05/17/2022 11:03:15 - INFO - __main__ - Global step 300 Train loss 0.859257 ACC 0.625 on epoch=99
05/17/2022 11:03:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.781494 on epoch=103
05/17/2022 11:03:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.726464 on epoch=106
05/17/2022 11:03:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.754084 on epoch=109
05/17/2022 11:03:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.859487 on epoch=113
05/17/2022 11:03:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.676508 on epoch=116
05/17/2022 11:03:29 - INFO - __main__ - Global step 350 Train loss 0.759607 ACC 0.375 on epoch=116
05/17/2022 11:03:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.696022 on epoch=119
05/17/2022 11:03:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.797419 on epoch=123
05/17/2022 11:03:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.670214 on epoch=126
05/17/2022 11:03:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.626481 on epoch=129
05/17/2022 11:03:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.683771 on epoch=133
05/17/2022 11:03:43 - INFO - __main__ - Global step 400 Train loss 0.694781 ACC 0.09375 on epoch=133
05/17/2022 11:03:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.522682 on epoch=136
05/17/2022 11:03:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.609015 on epoch=139
05/17/2022 11:03:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.461989 on epoch=143
05/17/2022 11:03:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.632190 on epoch=146
05/17/2022 11:03:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.596123 on epoch=149
05/17/2022 11:03:57 - INFO - __main__ - Global step 450 Train loss 0.564400 ACC 0.5 on epoch=149
05/17/2022 11:04:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.571454 on epoch=153
05/17/2022 11:04:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.482098 on epoch=156
05/17/2022 11:04:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.536659 on epoch=159
05/17/2022 11:04:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.587210 on epoch=163
05/17/2022 11:04:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.445608 on epoch=166
05/17/2022 11:04:12 - INFO - __main__ - Global step 500 Train loss 0.524606 ACC 0.65625 on epoch=166
05/17/2022 11:04:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.518754 on epoch=169
05/17/2022 11:04:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.477250 on epoch=173
05/17/2022 11:04:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.445129 on epoch=176
05/17/2022 11:04:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.433993 on epoch=179
05/17/2022 11:04:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.442273 on epoch=183
05/17/2022 11:04:26 - INFO - __main__ - Global step 550 Train loss 0.463480 ACC 0.28125 on epoch=183
05/17/2022 11:04:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.390440 on epoch=186
05/17/2022 11:04:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.366602 on epoch=189
05/17/2022 11:04:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.479831 on epoch=193
05/17/2022 11:04:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.519144 on epoch=196
05/17/2022 11:04:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.465466 on epoch=199
05/17/2022 11:04:40 - INFO - __main__ - Global step 600 Train loss 0.444297 ACC 0.59375 on epoch=199
05/17/2022 11:04:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.362469 on epoch=203
05/17/2022 11:04:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.430625 on epoch=206
05/17/2022 11:04:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.414787 on epoch=209
05/17/2022 11:04:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.329069 on epoch=213
05/17/2022 11:04:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.373812 on epoch=216
05/17/2022 11:04:54 - INFO - __main__ - Global step 650 Train loss 0.382152 ACC 0.4375 on epoch=216
05/17/2022 11:04:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.421138 on epoch=219
05/17/2022 11:05:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.379307 on epoch=223
05/17/2022 11:05:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.420022 on epoch=226
05/17/2022 11:05:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.355593 on epoch=229
05/17/2022 11:05:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.408647 on epoch=233
05/17/2022 11:05:10 - INFO - __main__ - Global step 700 Train loss 0.396941 ACC 0.3125 on epoch=233
05/17/2022 11:05:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.364875 on epoch=236
05/17/2022 11:05:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.413566 on epoch=239
05/17/2022 11:05:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.357795 on epoch=243
05/17/2022 11:05:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.406995 on epoch=246
05/17/2022 11:05:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.411584 on epoch=249
05/17/2022 11:05:26 - INFO - __main__ - Global step 750 Train loss 0.390963 ACC 0.5 on epoch=249
05/17/2022 11:05:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.337275 on epoch=253
05/17/2022 11:05:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.344681 on epoch=256
05/17/2022 11:05:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.363345 on epoch=259
05/17/2022 11:05:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.375351 on epoch=263
05/17/2022 11:05:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.314940 on epoch=266
05/17/2022 11:05:39 - INFO - __main__ - Global step 800 Train loss 0.347118 ACC 0.65625 on epoch=266
05/17/2022 11:05:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.304875 on epoch=269
05/17/2022 11:05:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.326304 on epoch=273
05/17/2022 11:05:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.310078 on epoch=276
05/17/2022 11:05:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.348819 on epoch=279
05/17/2022 11:05:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.397507 on epoch=283
05/17/2022 11:05:53 - INFO - __main__ - Global step 850 Train loss 0.337516 ACC 0.28125 on epoch=283
05/17/2022 11:05:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.327003 on epoch=286
05/17/2022 11:05:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.361494 on epoch=289
05/17/2022 11:06:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.280911 on epoch=293
05/17/2022 11:06:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.336863 on epoch=296
05/17/2022 11:06:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.360697 on epoch=299
05/17/2022 11:06:07 - INFO - __main__ - Global step 900 Train loss 0.333394 ACC 0.28125 on epoch=299
05/17/2022 11:06:07 - INFO - __main__ - save last model!
05/17/2022 11:06:08 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:06:08 - INFO - __main__ - Printing 3 examples
05/17/2022 11:06:08 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/17/2022 11:06:08 - INFO - __main__ - ['contradiction']
05/17/2022 11:06:08 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/17/2022 11:06:08 - INFO - __main__ - ['contradiction']
05/17/2022 11:06:08 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/17/2022 11:06:08 - INFO - __main__ - ['contradiction']
05/17/2022 11:06:08 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:06:08 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:06:08 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:06:08 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:06:08 - INFO - __main__ - Printing 3 examples
05/17/2022 11:06:08 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
05/17/2022 11:06:08 - INFO - __main__ - ['contradiction']
05/17/2022 11:06:08 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/17/2022 11:06:08 - INFO - __main__ - ['contradiction']
05/17/2022 11:06:08 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/17/2022 11:06:08 - INFO - __main__ - ['contradiction']
05/17/2022 11:06:08 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:06:08 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:06:08 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:06:10 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 11:06:11 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 11:06:11 - INFO - __main__ - Printing 3 examples
05/17/2022 11:06:11 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 11:06:11 - INFO - __main__ - ['contradiction']
05/17/2022 11:06:11 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 11:06:11 - INFO - __main__ - ['neutral']
05/17/2022 11:06:11 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 11:06:11 - INFO - __main__ - ['entailment']
05/17/2022 11:06:11 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:06:11 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:06:11 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 11:06:12 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_13_0.0005_8_predictions.txt
05/17/2022 11:06:12 - INFO - __main__ - ACC on test data: 0.6429
05/17/2022 11:06:12 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.0005, bsz=8, dev_performance=0.65625, test_performance=0.6428571428571429
05/17/2022 11:06:12 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.0003, bsz=8 ...
05/17/2022 11:06:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:06:13 - INFO - __main__ - Starting training!
05/17/2022 11:06:13 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:06:13 - INFO - __main__ - Printing 3 examples
05/17/2022 11:06:13 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/17/2022 11:06:13 - INFO - __main__ - ['contradiction']
05/17/2022 11:06:13 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/17/2022 11:06:13 - INFO - __main__ - ['contradiction']
05/17/2022 11:06:13 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/17/2022 11:06:13 - INFO - __main__ - ['contradiction']
05/17/2022 11:06:13 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:06:13 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:06:13 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:06:13 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:06:13 - INFO - __main__ - Printing 3 examples
05/17/2022 11:06:13 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
05/17/2022 11:06:13 - INFO - __main__ - ['contradiction']
05/17/2022 11:06:13 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/17/2022 11:06:13 - INFO - __main__ - ['contradiction']
05/17/2022 11:06:13 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/17/2022 11:06:13 - INFO - __main__ - ['contradiction']
05/17/2022 11:06:13 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:06:13 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:06:13 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:06:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:06:17 - INFO - __main__ - Starting training!
05/17/2022 11:06:20 - INFO - __main__ - Step 10 Global step 10 Train loss 17.982336 on epoch=3
05/17/2022 11:06:23 - INFO - __main__ - Step 20 Global step 20 Train loss 14.772840 on epoch=6
05/17/2022 11:06:25 - INFO - __main__ - Step 30 Global step 30 Train loss 10.283721 on epoch=9
05/17/2022 11:06:28 - INFO - __main__ - Step 40 Global step 40 Train loss 7.922715 on epoch=13
05/17/2022 11:06:31 - INFO - __main__ - Step 50 Global step 50 Train loss 6.272850 on epoch=16
05/17/2022 11:06:32 - INFO - __main__ - Global step 50 Train loss 11.446894 ACC 0.125 on epoch=16
05/17/2022 11:06:35 - INFO - __main__ - Step 60 Global step 60 Train loss 5.354599 on epoch=19
05/17/2022 11:06:38 - INFO - __main__ - Step 70 Global step 70 Train loss 4.576821 on epoch=23
05/17/2022 11:06:41 - INFO - __main__ - Step 80 Global step 80 Train loss 2.515085 on epoch=26
05/17/2022 11:06:44 - INFO - __main__ - Step 90 Global step 90 Train loss 2.806145 on epoch=29
05/17/2022 11:06:46 - INFO - __main__ - Step 100 Global step 100 Train loss 1.720531 on epoch=33
05/17/2022 11:06:47 - INFO - __main__ - Global step 100 Train loss 3.394636 ACC 0.09375 on epoch=33
05/17/2022 11:06:50 - INFO - __main__ - Step 110 Global step 110 Train loss 2.239373 on epoch=36
05/17/2022 11:06:52 - INFO - __main__ - Step 120 Global step 120 Train loss 1.780025 on epoch=39
05/17/2022 11:06:56 - INFO - __main__ - Step 130 Global step 130 Train loss 2.068594 on epoch=43
05/17/2022 11:06:59 - INFO - __main__ - Step 140 Global step 140 Train loss 1.798621 on epoch=46
05/17/2022 11:07:02 - INFO - __main__ - Step 150 Global step 150 Train loss 1.356867 on epoch=49
05/17/2022 11:07:02 - INFO - __main__ - Global step 150 Train loss 1.848696 ACC 0.0625 on epoch=49
05/17/2022 11:07:05 - INFO - __main__ - Step 160 Global step 160 Train loss 1.418966 on epoch=53
05/17/2022 11:07:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.937758 on epoch=56
05/17/2022 11:07:11 - INFO - __main__ - Step 180 Global step 180 Train loss 1.013927 on epoch=59
05/17/2022 11:07:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.963942 on epoch=63
05/17/2022 11:07:17 - INFO - __main__ - Step 200 Global step 200 Train loss 1.138825 on epoch=66
05/17/2022 11:07:17 - INFO - __main__ - Global step 200 Train loss 1.094684 ACC 0.625 on epoch=66
05/17/2022 11:07:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.778960 on epoch=69
05/17/2022 11:07:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.619746 on epoch=73
05/17/2022 11:07:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.603446 on epoch=76
05/17/2022 11:07:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.735264 on epoch=79
05/17/2022 11:07:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.576024 on epoch=83
05/17/2022 11:07:33 - INFO - __main__ - Global step 250 Train loss 0.662688 ACC 0.65625 on epoch=83
05/17/2022 11:07:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.336480 on epoch=86
05/17/2022 11:07:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.334881 on epoch=89
05/17/2022 11:07:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.243921 on epoch=93
05/17/2022 11:07:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.204133 on epoch=96
05/17/2022 11:07:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.281131 on epoch=99
05/17/2022 11:07:48 - INFO - __main__ - Global step 300 Train loss 0.280109 ACC 0.5625 on epoch=99
05/17/2022 11:07:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.234232 on epoch=103
05/17/2022 11:07:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.040176 on epoch=106
05/17/2022 11:07:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.518749 on epoch=109
05/17/2022 11:07:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.407773 on epoch=113
05/17/2022 11:08:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.156925 on epoch=116
05/17/2022 11:08:02 - INFO - __main__ - Global step 350 Train loss 0.271571 ACC 0.65625 on epoch=116
05/17/2022 11:08:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.110151 on epoch=119
05/17/2022 11:08:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.085543 on epoch=123
05/17/2022 11:08:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.047599 on epoch=126
05/17/2022 11:08:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.029542 on epoch=129
05/17/2022 11:08:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.106630 on epoch=133
05/17/2022 11:08:16 - INFO - __main__ - Global step 400 Train loss 0.075893 ACC 0.59375 on epoch=133
05/17/2022 11:08:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.049316 on epoch=136
05/17/2022 11:08:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.066948 on epoch=139
05/17/2022 11:08:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.052194 on epoch=143
05/17/2022 11:08:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.050937 on epoch=146
05/17/2022 11:08:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.063822 on epoch=149
05/17/2022 11:08:30 - INFO - __main__ - Global step 450 Train loss 0.056643 ACC 0.5625 on epoch=149
05/17/2022 11:08:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.024020 on epoch=153
05/17/2022 11:08:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.090316 on epoch=156
05/17/2022 11:08:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.041919 on epoch=159
05/17/2022 11:08:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.097440 on epoch=163
05/17/2022 11:08:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.034982 on epoch=166
05/17/2022 11:08:46 - INFO - __main__ - Global step 500 Train loss 0.057735 ACC 0.6875 on epoch=166
05/17/2022 11:08:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.014864 on epoch=169
05/17/2022 11:08:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.019457 on epoch=173
05/17/2022 11:08:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.007509 on epoch=176
05/17/2022 11:08:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.033947 on epoch=179
05/17/2022 11:09:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.015471 on epoch=183
05/17/2022 11:09:01 - INFO - __main__ - Global step 550 Train loss 0.018249 ACC 0.65625 on epoch=183
05/17/2022 11:09:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.003438 on epoch=186
05/17/2022 11:09:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.026920 on epoch=189
05/17/2022 11:09:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.004749 on epoch=193
05/17/2022 11:09:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.005597 on epoch=196
05/17/2022 11:09:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.028682 on epoch=199
05/17/2022 11:09:16 - INFO - __main__ - Global step 600 Train loss 0.013877 ACC 0.6875 on epoch=199
05/17/2022 11:09:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.019628 on epoch=203
05/17/2022 11:09:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.001451 on epoch=206
05/17/2022 11:09:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.002081 on epoch=209
05/17/2022 11:09:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000917 on epoch=213
05/17/2022 11:09:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000892 on epoch=216
05/17/2022 11:09:30 - INFO - __main__ - Global step 650 Train loss 0.004994 ACC 0.71875 on epoch=216
05/17/2022 11:09:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.040122 on epoch=219
05/17/2022 11:09:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.014872 on epoch=223
05/17/2022 11:09:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.017442 on epoch=226
05/17/2022 11:09:42 - INFO - __main__ - Step 690 Global step 690 Train loss 0.014063 on epoch=229
05/17/2022 11:09:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.029500 on epoch=233
05/17/2022 11:09:45 - INFO - __main__ - Global step 700 Train loss 0.023200 ACC 0.59375 on epoch=233
05/17/2022 11:09:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.032091 on epoch=236
05/17/2022 11:09:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000608 on epoch=239
05/17/2022 11:09:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.016195 on epoch=243
05/17/2022 11:09:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.025488 on epoch=246
05/17/2022 11:09:59 - INFO - __main__ - Step 750 Global step 750 Train loss 0.002533 on epoch=249
05/17/2022 11:10:00 - INFO - __main__ - Global step 750 Train loss 0.015383 ACC 0.625 on epoch=249
05/17/2022 11:10:03 - INFO - __main__ - Step 760 Global step 760 Train loss 0.039215 on epoch=253
05/17/2022 11:10:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.032086 on epoch=256
05/17/2022 11:10:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.003539 on epoch=259
05/17/2022 11:10:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.002245 on epoch=263
05/17/2022 11:10:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.037889 on epoch=266
05/17/2022 11:10:15 - INFO - __main__ - Global step 800 Train loss 0.022995 ACC 0.75 on epoch=266
05/17/2022 11:10:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.004452 on epoch=269
05/17/2022 11:10:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.020670 on epoch=273
05/17/2022 11:10:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.055602 on epoch=276
05/17/2022 11:10:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.040749 on epoch=279
05/17/2022 11:10:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.010688 on epoch=283
05/17/2022 11:10:30 - INFO - __main__ - Global step 850 Train loss 0.026432 ACC 0.71875 on epoch=283
05/17/2022 11:10:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.002438 on epoch=286
05/17/2022 11:10:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.021758 on epoch=289
05/17/2022 11:10:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.006480 on epoch=293
05/17/2022 11:10:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.003854 on epoch=296
05/17/2022 11:10:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.002628 on epoch=299
05/17/2022 11:10:44 - INFO - __main__ - Global step 900 Train loss 0.007432 ACC 0.65625 on epoch=299
05/17/2022 11:10:44 - INFO - __main__ - save last model!
05/17/2022 11:10:45 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:10:45 - INFO - __main__ - Printing 3 examples
05/17/2022 11:10:45 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/17/2022 11:10:45 - INFO - __main__ - ['contradiction']
05/17/2022 11:10:45 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/17/2022 11:10:45 - INFO - __main__ - ['contradiction']
05/17/2022 11:10:45 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/17/2022 11:10:45 - INFO - __main__ - ['contradiction']
05/17/2022 11:10:45 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:10:45 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:10:45 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:10:45 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:10:45 - INFO - __main__ - Printing 3 examples
05/17/2022 11:10:45 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
05/17/2022 11:10:45 - INFO - __main__ - ['contradiction']
05/17/2022 11:10:45 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/17/2022 11:10:45 - INFO - __main__ - ['contradiction']
05/17/2022 11:10:45 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/17/2022 11:10:45 - INFO - __main__ - ['contradiction']
05/17/2022 11:10:45 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:10:46 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:10:46 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:10:47 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 11:10:47 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 11:10:47 - INFO - __main__ - Printing 3 examples
05/17/2022 11:10:47 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 11:10:47 - INFO - __main__ - ['contradiction']
05/17/2022 11:10:47 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 11:10:47 - INFO - __main__ - ['neutral']
05/17/2022 11:10:47 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 11:10:47 - INFO - __main__ - ['entailment']
05/17/2022 11:10:47 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:10:47 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:10:48 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 11:10:48 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_13_0.0003_8_predictions.txt
05/17/2022 11:10:48 - INFO - __main__ - ACC on test data: 0.7679
05/17/2022 11:10:49 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.0003, bsz=8, dev_performance=0.75, test_performance=0.7678571428571429
05/17/2022 11:10:49 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.0002, bsz=8 ...
05/17/2022 11:10:50 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:10:50 - INFO - __main__ - Printing 3 examples
05/17/2022 11:10:50 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/17/2022 11:10:50 - INFO - __main__ - ['contradiction']
05/17/2022 11:10:50 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/17/2022 11:10:50 - INFO - __main__ - ['contradiction']
05/17/2022 11:10:50 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/17/2022 11:10:50 - INFO - __main__ - ['contradiction']
05/17/2022 11:10:50 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:10:50 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:10:50 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:10:50 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:10:50 - INFO - __main__ - Printing 3 examples
05/17/2022 11:10:50 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
05/17/2022 11:10:50 - INFO - __main__ - ['contradiction']
05/17/2022 11:10:50 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/17/2022 11:10:50 - INFO - __main__ - ['contradiction']
05/17/2022 11:10:50 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/17/2022 11:10:50 - INFO - __main__ - ['contradiction']
05/17/2022 11:10:50 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:10:50 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:10:50 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:10:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:10:50 - INFO - __main__ - Starting training!
05/17/2022 11:10:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:10:54 - INFO - __main__ - Starting training!
05/17/2022 11:10:56 - INFO - __main__ - Step 10 Global step 10 Train loss 18.022726 on epoch=3
05/17/2022 11:10:59 - INFO - __main__ - Step 20 Global step 20 Train loss 15.733739 on epoch=6
05/17/2022 11:11:01 - INFO - __main__ - Step 30 Global step 30 Train loss 10.752584 on epoch=9
05/17/2022 11:11:04 - INFO - __main__ - Step 40 Global step 40 Train loss 8.542622 on epoch=13
05/17/2022 11:11:07 - INFO - __main__ - Step 50 Global step 50 Train loss 7.591319 on epoch=16
05/17/2022 11:11:10 - INFO - __main__ - Global step 50 Train loss 12.128597 ACC 0.0 on epoch=16
05/17/2022 11:11:13 - INFO - __main__ - Step 60 Global step 60 Train loss 6.553370 on epoch=19
05/17/2022 11:11:16 - INFO - __main__ - Step 70 Global step 70 Train loss 5.609338 on epoch=23
05/17/2022 11:11:19 - INFO - __main__ - Step 80 Global step 80 Train loss 5.037160 on epoch=26
05/17/2022 11:11:22 - INFO - __main__ - Step 90 Global step 90 Train loss 4.223451 on epoch=29
05/17/2022 11:11:25 - INFO - __main__ - Step 100 Global step 100 Train loss 3.142295 on epoch=33
05/17/2022 11:11:26 - INFO - __main__ - Global step 100 Train loss 4.913123 ACC 0.0 on epoch=33
05/17/2022 11:11:29 - INFO - __main__ - Step 110 Global step 110 Train loss 2.317284 on epoch=36
05/17/2022 11:11:32 - INFO - __main__ - Step 120 Global step 120 Train loss 2.488838 on epoch=39
05/17/2022 11:11:35 - INFO - __main__ - Step 130 Global step 130 Train loss 2.414434 on epoch=43
05/17/2022 11:11:38 - INFO - __main__ - Step 140 Global step 140 Train loss 2.133783 on epoch=46
05/17/2022 11:11:41 - INFO - __main__ - Step 150 Global step 150 Train loss 2.080021 on epoch=49
05/17/2022 11:11:42 - INFO - __main__ - Global step 150 Train loss 2.286872 ACC 0.25 on epoch=49
05/17/2022 11:11:45 - INFO - __main__ - Step 160 Global step 160 Train loss 1.864599 on epoch=53
05/17/2022 11:11:47 - INFO - __main__ - Step 170 Global step 170 Train loss 2.042387 on epoch=56
05/17/2022 11:11:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.916211 on epoch=59
05/17/2022 11:11:53 - INFO - __main__ - Step 190 Global step 190 Train loss 1.277058 on epoch=63
05/17/2022 11:11:56 - INFO - __main__ - Step 200 Global step 200 Train loss 1.362839 on epoch=66
05/17/2022 11:11:56 - INFO - __main__ - Global step 200 Train loss 1.492619 ACC 0.40625 on epoch=66
05/17/2022 11:12:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.944841 on epoch=69
05/17/2022 11:12:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.993560 on epoch=73
05/17/2022 11:12:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.838973 on epoch=76
05/17/2022 11:12:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.842348 on epoch=79
05/17/2022 11:12:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.877341 on epoch=83
05/17/2022 11:12:12 - INFO - __main__ - Global step 250 Train loss 0.899413 ACC 0.625 on epoch=83
05/17/2022 11:12:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.560606 on epoch=86
05/17/2022 11:12:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.997836 on epoch=89
05/17/2022 11:12:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.734304 on epoch=93
05/17/2022 11:12:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.618435 on epoch=96
05/17/2022 11:12:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.462280 on epoch=99
05/17/2022 11:12:27 - INFO - __main__ - Global step 300 Train loss 0.674692 ACC 0.6875 on epoch=99
05/17/2022 11:12:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.626981 on epoch=103
05/17/2022 11:12:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.385350 on epoch=106
05/17/2022 11:12:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.508251 on epoch=109
05/17/2022 11:12:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.247091 on epoch=113
05/17/2022 11:12:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.275906 on epoch=116
05/17/2022 11:12:42 - INFO - __main__ - Global step 350 Train loss 0.408716 ACC 0.6875 on epoch=116
05/17/2022 11:12:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.252346 on epoch=119
05/17/2022 11:12:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.267279 on epoch=123
05/17/2022 11:12:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.214876 on epoch=126
05/17/2022 11:12:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.305273 on epoch=129
05/17/2022 11:12:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.477442 on epoch=133
05/17/2022 11:12:56 - INFO - __main__ - Global step 400 Train loss 0.303443 ACC 0.625 on epoch=133
05/17/2022 11:12:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.046023 on epoch=136
05/17/2022 11:13:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.297775 on epoch=139
05/17/2022 11:13:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.115982 on epoch=143
05/17/2022 11:13:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.142328 on epoch=146
05/17/2022 11:13:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.072989 on epoch=149
05/17/2022 11:13:10 - INFO - __main__ - Global step 450 Train loss 0.135019 ACC 0.6875 on epoch=149
05/17/2022 11:13:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.022350 on epoch=153
05/17/2022 11:13:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.125533 on epoch=156
05/17/2022 11:13:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.011483 on epoch=159
05/17/2022 11:13:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.054817 on epoch=163
05/17/2022 11:13:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.051219 on epoch=166
05/17/2022 11:13:25 - INFO - __main__ - Global step 500 Train loss 0.053081 ACC 0.6875 on epoch=166
05/17/2022 11:13:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.041472 on epoch=169
05/17/2022 11:13:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.089820 on epoch=173
05/17/2022 11:13:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.050060 on epoch=176
05/17/2022 11:13:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.103739 on epoch=179
05/17/2022 11:13:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.163870 on epoch=183
05/17/2022 11:13:41 - INFO - __main__ - Global step 550 Train loss 0.089792 ACC 0.625 on epoch=183
05/17/2022 11:13:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.103862 on epoch=186
05/17/2022 11:13:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.205353 on epoch=189
05/17/2022 11:13:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.026886 on epoch=193
05/17/2022 11:13:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.036653 on epoch=196
05/17/2022 11:13:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.005987 on epoch=199
05/17/2022 11:13:57 - INFO - __main__ - Global step 600 Train loss 0.075748 ACC 0.53125 on epoch=199
05/17/2022 11:14:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.017676 on epoch=203
05/17/2022 11:14:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.129592 on epoch=206
05/17/2022 11:14:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.062979 on epoch=209
05/17/2022 11:14:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.013043 on epoch=213
05/17/2022 11:14:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.039034 on epoch=216
05/17/2022 11:14:12 - INFO - __main__ - Global step 650 Train loss 0.052465 ACC 0.625 on epoch=216
05/17/2022 11:14:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.026951 on epoch=219
05/17/2022 11:14:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.040519 on epoch=223
05/17/2022 11:14:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.011795 on epoch=226
05/17/2022 11:14:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.022316 on epoch=229
05/17/2022 11:14:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.013802 on epoch=233
05/17/2022 11:14:27 - INFO - __main__ - Global step 700 Train loss 0.023077 ACC 0.625 on epoch=233
05/17/2022 11:14:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.006999 on epoch=236
05/17/2022 11:14:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.044467 on epoch=239
05/17/2022 11:14:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.007829 on epoch=243
05/17/2022 11:14:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.009154 on epoch=246
05/17/2022 11:14:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.004160 on epoch=249
05/17/2022 11:14:41 - INFO - __main__ - Global step 750 Train loss 0.014522 ACC 0.65625 on epoch=249
05/17/2022 11:14:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.004495 on epoch=253
05/17/2022 11:14:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.044807 on epoch=256
05/17/2022 11:14:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.001271 on epoch=259
05/17/2022 11:14:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.005837 on epoch=263
05/17/2022 11:14:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.009207 on epoch=266
05/17/2022 11:14:58 - INFO - __main__ - Global step 800 Train loss 0.013124 ACC 0.5625 on epoch=266
05/17/2022 11:15:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.005294 on epoch=269
05/17/2022 11:15:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.037847 on epoch=273
05/17/2022 11:15:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.003194 on epoch=276
05/17/2022 11:15:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.002814 on epoch=279
05/17/2022 11:15:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000978 on epoch=283
05/17/2022 11:15:12 - INFO - __main__ - Global step 850 Train loss 0.010025 ACC 0.5625 on epoch=283
05/17/2022 11:15:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.002215 on epoch=286
05/17/2022 11:15:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.006065 on epoch=289
05/17/2022 11:15:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.071149 on epoch=293
05/17/2022 11:15:24 - INFO - __main__ - Step 890 Global step 890 Train loss 0.004057 on epoch=296
05/17/2022 11:15:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.005325 on epoch=299
05/17/2022 11:15:28 - INFO - __main__ - Global step 900 Train loss 0.017762 ACC 0.5625 on epoch=299
05/17/2022 11:15:28 - INFO - __main__ - save last model!
05/17/2022 11:15:28 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:15:28 - INFO - __main__ - Printing 3 examples
05/17/2022 11:15:28 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/17/2022 11:15:28 - INFO - __main__ - ['contradiction']
05/17/2022 11:15:28 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/17/2022 11:15:28 - INFO - __main__ - ['contradiction']
05/17/2022 11:15:28 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/17/2022 11:15:28 - INFO - __main__ - ['contradiction']
05/17/2022 11:15:28 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:15:28 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:15:29 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:15:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:15:29 - INFO - __main__ - Printing 3 examples
05/17/2022 11:15:29 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
05/17/2022 11:15:29 - INFO - __main__ - ['contradiction']
05/17/2022 11:15:29 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/17/2022 11:15:29 - INFO - __main__ - ['contradiction']
05/17/2022 11:15:29 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/17/2022 11:15:29 - INFO - __main__ - ['contradiction']
05/17/2022 11:15:29 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:15:29 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:15:29 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:15:31 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 11:15:31 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 11:15:31 - INFO - __main__ - Printing 3 examples
05/17/2022 11:15:31 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 11:15:31 - INFO - __main__ - ['contradiction']
05/17/2022 11:15:31 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 11:15:31 - INFO - __main__ - ['neutral']
05/17/2022 11:15:31 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 11:15:31 - INFO - __main__ - ['entailment']
05/17/2022 11:15:31 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:15:31 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:15:31 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 11:15:32 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_13_0.0002_8_predictions.txt
05/17/2022 11:15:32 - INFO - __main__ - ACC on test data: 0.8036
05/17/2022 11:15:32 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.0002, bsz=8, dev_performance=0.6875, test_performance=0.8035714285714286
05/17/2022 11:15:32 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.0001, bsz=8 ...
05/17/2022 11:15:33 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:15:33 - INFO - __main__ - Printing 3 examples
05/17/2022 11:15:33 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/17/2022 11:15:33 - INFO - __main__ - ['contradiction']
05/17/2022 11:15:33 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/17/2022 11:15:33 - INFO - __main__ - ['contradiction']
05/17/2022 11:15:33 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/17/2022 11:15:33 - INFO - __main__ - ['contradiction']
05/17/2022 11:15:33 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:15:33 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:15:33 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:15:33 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:15:33 - INFO - __main__ - Printing 3 examples
05/17/2022 11:15:33 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
05/17/2022 11:15:33 - INFO - __main__ - ['contradiction']
05/17/2022 11:15:33 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/17/2022 11:15:33 - INFO - __main__ - ['contradiction']
05/17/2022 11:15:33 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/17/2022 11:15:33 - INFO - __main__ - ['contradiction']
05/17/2022 11:15:33 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:15:33 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:15:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:15:33 - INFO - __main__ - Starting training!
05/17/2022 11:15:33 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:15:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:15:38 - INFO - __main__ - Starting training!
05/17/2022 11:15:40 - INFO - __main__ - Step 10 Global step 10 Train loss 18.403929 on epoch=3
05/17/2022 11:15:43 - INFO - __main__ - Step 20 Global step 20 Train loss 17.294094 on epoch=6
05/17/2022 11:15:45 - INFO - __main__ - Step 30 Global step 30 Train loss 16.307276 on epoch=9
05/17/2022 11:15:48 - INFO - __main__ - Step 40 Global step 40 Train loss 13.816793 on epoch=13
05/17/2022 11:15:51 - INFO - __main__ - Step 50 Global step 50 Train loss 11.070391 on epoch=16
05/17/2022 11:15:55 - INFO - __main__ - Global step 50 Train loss 15.378496 ACC 0.0 on epoch=16
05/17/2022 11:15:58 - INFO - __main__ - Step 60 Global step 60 Train loss 9.793314 on epoch=19
05/17/2022 11:16:00 - INFO - __main__ - Step 70 Global step 70 Train loss 9.715104 on epoch=23
05/17/2022 11:16:03 - INFO - __main__ - Step 80 Global step 80 Train loss 8.610879 on epoch=26
05/17/2022 11:16:06 - INFO - __main__ - Step 90 Global step 90 Train loss 8.353514 on epoch=29
05/17/2022 11:16:10 - INFO - __main__ - Step 100 Global step 100 Train loss 8.243409 on epoch=33
05/17/2022 11:16:11 - INFO - __main__ - Global step 100 Train loss 8.943244 ACC 0.0 on epoch=33
05/17/2022 11:16:14 - INFO - __main__ - Step 110 Global step 110 Train loss 7.207053 on epoch=36
05/17/2022 11:16:17 - INFO - __main__ - Step 120 Global step 120 Train loss 6.117818 on epoch=39
05/17/2022 11:16:20 - INFO - __main__ - Step 130 Global step 130 Train loss 6.819085 on epoch=43
05/17/2022 11:16:23 - INFO - __main__ - Step 140 Global step 140 Train loss 5.668572 on epoch=46
05/17/2022 11:16:26 - INFO - __main__ - Step 150 Global step 150 Train loss 5.306703 on epoch=49
05/17/2022 11:16:27 - INFO - __main__ - Global step 150 Train loss 6.223846 ACC 0.09375 on epoch=49
05/17/2022 11:16:31 - INFO - __main__ - Step 160 Global step 160 Train loss 5.331809 on epoch=53
05/17/2022 11:16:34 - INFO - __main__ - Step 170 Global step 170 Train loss 4.621961 on epoch=56
05/17/2022 11:16:37 - INFO - __main__ - Step 180 Global step 180 Train loss 3.592441 on epoch=59
05/17/2022 11:16:39 - INFO - __main__ - Step 190 Global step 190 Train loss 3.372695 on epoch=63
05/17/2022 11:16:42 - INFO - __main__ - Step 200 Global step 200 Train loss 3.394685 on epoch=66
05/17/2022 11:16:43 - INFO - __main__ - Global step 200 Train loss 4.062718 ACC 0.0 on epoch=66
05/17/2022 11:16:45 - INFO - __main__ - Step 210 Global step 210 Train loss 3.242065 on epoch=69
05/17/2022 11:16:48 - INFO - __main__ - Step 220 Global step 220 Train loss 3.478693 on epoch=73
05/17/2022 11:16:51 - INFO - __main__ - Step 230 Global step 230 Train loss 2.399326 on epoch=76
05/17/2022 11:16:54 - INFO - __main__ - Step 240 Global step 240 Train loss 2.408394 on epoch=79
05/17/2022 11:16:57 - INFO - __main__ - Step 250 Global step 250 Train loss 2.842597 on epoch=83
05/17/2022 11:16:57 - INFO - __main__ - Global step 250 Train loss 2.874214 ACC 0.0 on epoch=83
05/17/2022 11:17:00 - INFO - __main__ - Step 260 Global step 260 Train loss 2.915946 on epoch=86
05/17/2022 11:17:02 - INFO - __main__ - Step 270 Global step 270 Train loss 2.046961 on epoch=89
05/17/2022 11:17:05 - INFO - __main__ - Step 280 Global step 280 Train loss 2.937120 on epoch=93
05/17/2022 11:17:08 - INFO - __main__ - Step 290 Global step 290 Train loss 2.241133 on epoch=96
05/17/2022 11:17:11 - INFO - __main__ - Step 300 Global step 300 Train loss 2.067172 on epoch=99
05/17/2022 11:17:11 - INFO - __main__ - Global step 300 Train loss 2.441667 ACC 0.40625 on epoch=99
05/17/2022 11:17:14 - INFO - __main__ - Step 310 Global step 310 Train loss 2.128892 on epoch=103
05/17/2022 11:17:17 - INFO - __main__ - Step 320 Global step 320 Train loss 1.456323 on epoch=106
05/17/2022 11:17:20 - INFO - __main__ - Step 330 Global step 330 Train loss 2.120302 on epoch=109
05/17/2022 11:17:23 - INFO - __main__ - Step 340 Global step 340 Train loss 2.304533 on epoch=113
05/17/2022 11:17:26 - INFO - __main__ - Step 350 Global step 350 Train loss 2.114899 on epoch=116
05/17/2022 11:17:26 - INFO - __main__ - Global step 350 Train loss 2.024990 ACC 0.4375 on epoch=116
05/17/2022 11:17:30 - INFO - __main__ - Step 360 Global step 360 Train loss 1.970169 on epoch=119
05/17/2022 11:17:32 - INFO - __main__ - Step 370 Global step 370 Train loss 1.960430 on epoch=123
05/17/2022 11:17:35 - INFO - __main__ - Step 380 Global step 380 Train loss 1.332984 on epoch=126
05/17/2022 11:17:38 - INFO - __main__ - Step 390 Global step 390 Train loss 1.847233 on epoch=129
05/17/2022 11:17:41 - INFO - __main__ - Step 400 Global step 400 Train loss 1.747937 on epoch=133
05/17/2022 11:17:42 - INFO - __main__ - Global step 400 Train loss 1.771751 ACC 0.34375 on epoch=133
05/17/2022 11:17:44 - INFO - __main__ - Step 410 Global step 410 Train loss 2.051048 on epoch=136
05/17/2022 11:17:47 - INFO - __main__ - Step 420 Global step 420 Train loss 1.275383 on epoch=139
05/17/2022 11:17:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.950006 on epoch=143
05/17/2022 11:17:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.967355 on epoch=146
05/17/2022 11:17:56 - INFO - __main__ - Step 450 Global step 450 Train loss 1.228619 on epoch=149
05/17/2022 11:17:57 - INFO - __main__ - Global step 450 Train loss 1.294482 ACC 0.46875 on epoch=149
05/17/2022 11:18:00 - INFO - __main__ - Step 460 Global step 460 Train loss 1.092095 on epoch=153
05/17/2022 11:18:03 - INFO - __main__ - Step 470 Global step 470 Train loss 1.381583 on epoch=156
05/17/2022 11:18:05 - INFO - __main__ - Step 480 Global step 480 Train loss 1.004831 on epoch=159
05/17/2022 11:18:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.956967 on epoch=163
05/17/2022 11:18:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.893028 on epoch=166
05/17/2022 11:18:12 - INFO - __main__ - Global step 500 Train loss 1.065701 ACC 0.625 on epoch=166
05/17/2022 11:18:15 - INFO - __main__ - Step 510 Global step 510 Train loss 1.117901 on epoch=169
05/17/2022 11:18:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.965035 on epoch=173
05/17/2022 11:18:21 - INFO - __main__ - Step 530 Global step 530 Train loss 1.060766 on epoch=176
05/17/2022 11:18:24 - INFO - __main__ - Step 540 Global step 540 Train loss 1.042120 on epoch=179
05/17/2022 11:18:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.962077 on epoch=183
05/17/2022 11:18:27 - INFO - __main__ - Global step 550 Train loss 1.029580 ACC 0.65625 on epoch=183
05/17/2022 11:18:30 - INFO - __main__ - Step 560 Global step 560 Train loss 1.046471 on epoch=186
05/17/2022 11:18:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.978902 on epoch=189
05/17/2022 11:18:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.983001 on epoch=193
05/17/2022 11:18:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.637020 on epoch=196
05/17/2022 11:18:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.772341 on epoch=199
05/17/2022 11:18:42 - INFO - __main__ - Global step 600 Train loss 0.883547 ACC 0.65625 on epoch=199
05/17/2022 11:18:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.426331 on epoch=203
05/17/2022 11:18:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.601781 on epoch=206
05/17/2022 11:18:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.677150 on epoch=209
05/17/2022 11:18:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.675850 on epoch=213
05/17/2022 11:18:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.494980 on epoch=216
05/17/2022 11:18:58 - INFO - __main__ - Global step 650 Train loss 0.575218 ACC 0.65625 on epoch=216
05/17/2022 11:19:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.508444 on epoch=219
05/17/2022 11:19:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.636170 on epoch=223
05/17/2022 11:19:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.538133 on epoch=226
05/17/2022 11:19:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.603028 on epoch=229
05/17/2022 11:19:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.480888 on epoch=233
05/17/2022 11:19:13 - INFO - __main__ - Global step 700 Train loss 0.553333 ACC 0.6875 on epoch=233
05/17/2022 11:19:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.577784 on epoch=236
05/17/2022 11:19:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.706783 on epoch=239
05/17/2022 11:19:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.524061 on epoch=243
05/17/2022 11:19:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.424533 on epoch=246
05/17/2022 11:19:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.283675 on epoch=249
05/17/2022 11:19:28 - INFO - __main__ - Global step 750 Train loss 0.503367 ACC 0.71875 on epoch=249
05/17/2022 11:19:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.508076 on epoch=253
05/17/2022 11:19:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.352733 on epoch=256
05/17/2022 11:19:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.178169 on epoch=259
05/17/2022 11:19:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.382152 on epoch=263
05/17/2022 11:19:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.432247 on epoch=266
05/17/2022 11:19:42 - INFO - __main__ - Global step 800 Train loss 0.370676 ACC 0.65625 on epoch=266
05/17/2022 11:19:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.261287 on epoch=269
05/17/2022 11:19:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.242806 on epoch=273
05/17/2022 11:19:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.297440 on epoch=276
05/17/2022 11:19:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.290513 on epoch=279
05/17/2022 11:19:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.295589 on epoch=283
05/17/2022 11:19:57 - INFO - __main__ - Global step 850 Train loss 0.277527 ACC 0.625 on epoch=283
05/17/2022 11:19:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.211522 on epoch=286
05/17/2022 11:20:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.228721 on epoch=289
05/17/2022 11:20:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.190569 on epoch=293
05/17/2022 11:20:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.214229 on epoch=296
05/17/2022 11:20:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.207964 on epoch=299
05/17/2022 11:20:12 - INFO - __main__ - Global step 900 Train loss 0.210601 ACC 0.65625 on epoch=299
05/17/2022 11:20:12 - INFO - __main__ - save last model!
05/17/2022 11:20:13 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:20:13 - INFO - __main__ - Printing 3 examples
05/17/2022 11:20:13 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/17/2022 11:20:13 - INFO - __main__ - ['contradiction']
05/17/2022 11:20:13 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/17/2022 11:20:13 - INFO - __main__ - ['contradiction']
05/17/2022 11:20:13 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/17/2022 11:20:13 - INFO - __main__ - ['contradiction']
05/17/2022 11:20:13 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:20:13 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:20:13 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:20:13 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:20:13 - INFO - __main__ - Printing 3 examples
05/17/2022 11:20:13 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
05/17/2022 11:20:13 - INFO - __main__ - ['contradiction']
05/17/2022 11:20:13 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
05/17/2022 11:20:13 - INFO - __main__ - ['contradiction']
05/17/2022 11:20:13 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
05/17/2022 11:20:13 - INFO - __main__ - ['contradiction']
05/17/2022 11:20:13 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:20:13 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:20:13 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:20:15 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 11:20:15 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 11:20:15 - INFO - __main__ - Printing 3 examples
05/17/2022 11:20:15 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 11:20:15 - INFO - __main__ - ['contradiction']
05/17/2022 11:20:15 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 11:20:15 - INFO - __main__ - ['neutral']
05/17/2022 11:20:15 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 11:20:15 - INFO - __main__ - ['entailment']
05/17/2022 11:20:15 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:20:15 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:20:15 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 11:20:16 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_13_0.0001_8_predictions.txt
05/17/2022 11:20:16 - INFO - __main__ - ACC on test data: 0.8214
05/17/2022 11:20:16 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.0001, bsz=8, dev_performance=0.71875, test_performance=0.8214285714285714
05/17/2022 11:20:16 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.0005, bsz=8 ...
05/17/2022 11:20:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:20:17 - INFO - __main__ - Starting training!
05/17/2022 11:20:17 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:20:17 - INFO - __main__ - Printing 3 examples
05/17/2022 11:20:17 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/17/2022 11:20:17 - INFO - __main__ - ['contradiction']
05/17/2022 11:20:17 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/17/2022 11:20:17 - INFO - __main__ - ['contradiction']
05/17/2022 11:20:17 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/17/2022 11:20:17 - INFO - __main__ - ['contradiction']
05/17/2022 11:20:17 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:20:17 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:20:17 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:20:17 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:20:17 - INFO - __main__ - Printing 3 examples
05/17/2022 11:20:17 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
05/17/2022 11:20:17 - INFO - __main__ - ['contradiction']
05/17/2022 11:20:17 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
05/17/2022 11:20:17 - INFO - __main__ - ['contradiction']
05/17/2022 11:20:17 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
05/17/2022 11:20:17 - INFO - __main__ - ['contradiction']
05/17/2022 11:20:17 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:20:17 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:20:17 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:20:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:20:22 - INFO - __main__ - Starting training!
05/17/2022 11:20:24 - INFO - __main__ - Step 10 Global step 10 Train loss 17.712831 on epoch=3
05/17/2022 11:20:27 - INFO - __main__ - Step 20 Global step 20 Train loss 14.376268 on epoch=6
05/17/2022 11:20:30 - INFO - __main__ - Step 30 Global step 30 Train loss 8.281266 on epoch=9
05/17/2022 11:20:33 - INFO - __main__ - Step 40 Global step 40 Train loss 4.556888 on epoch=13
05/17/2022 11:20:36 - INFO - __main__ - Step 50 Global step 50 Train loss 3.476071 on epoch=16
05/17/2022 11:20:36 - INFO - __main__ - Global step 50 Train loss 9.680665 ACC 0.0 on epoch=16
05/17/2022 11:20:39 - INFO - __main__ - Step 60 Global step 60 Train loss 3.397503 on epoch=19
05/17/2022 11:20:42 - INFO - __main__ - Step 70 Global step 70 Train loss 3.094753 on epoch=23
05/17/2022 11:20:45 - INFO - __main__ - Step 80 Global step 80 Train loss 2.318676 on epoch=26
05/17/2022 11:20:48 - INFO - __main__ - Step 90 Global step 90 Train loss 1.890079 on epoch=29
05/17/2022 11:20:51 - INFO - __main__ - Step 100 Global step 100 Train loss 1.295075 on epoch=33
05/17/2022 11:20:51 - INFO - __main__ - Global step 100 Train loss 2.399217 ACC 0.125 on epoch=33
05/17/2022 11:20:54 - INFO - __main__ - Step 110 Global step 110 Train loss 1.411049 on epoch=36
05/17/2022 11:20:57 - INFO - __main__ - Step 120 Global step 120 Train loss 1.178170 on epoch=39
05/17/2022 11:21:00 - INFO - __main__ - Step 130 Global step 130 Train loss 1.095411 on epoch=43
05/17/2022 11:21:03 - INFO - __main__ - Step 140 Global step 140 Train loss 1.355322 on epoch=46
05/17/2022 11:21:06 - INFO - __main__ - Step 150 Global step 150 Train loss 1.031906 on epoch=49
05/17/2022 11:21:06 - INFO - __main__ - Global step 150 Train loss 1.214372 ACC 0.46875 on epoch=49
05/17/2022 11:21:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.911217 on epoch=53
05/17/2022 11:21:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.855067 on epoch=56
05/17/2022 11:21:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.757374 on epoch=59
05/17/2022 11:21:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.876001 on epoch=63
05/17/2022 11:21:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.593543 on epoch=66
05/17/2022 11:21:23 - INFO - __main__ - Global step 200 Train loss 0.798641 ACC 0.59375 on epoch=66
05/17/2022 11:21:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.949577 on epoch=69
05/17/2022 11:21:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.752145 on epoch=73
05/17/2022 11:21:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.480879 on epoch=76
05/17/2022 11:21:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.570174 on epoch=79
05/17/2022 11:21:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.500104 on epoch=83
05/17/2022 11:21:38 - INFO - __main__ - Global step 250 Train loss 0.650576 ACC 0.46875 on epoch=83
05/17/2022 11:21:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.505364 on epoch=86
05/17/2022 11:21:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.579965 on epoch=89
05/17/2022 11:21:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.636020 on epoch=93
05/17/2022 11:21:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.715814 on epoch=96
05/17/2022 11:21:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.523941 on epoch=99
05/17/2022 11:21:53 - INFO - __main__ - Global step 300 Train loss 0.592221 ACC 0.21875 on epoch=99
05/17/2022 11:21:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.505227 on epoch=103
05/17/2022 11:21:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.432413 on epoch=106
05/17/2022 11:22:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.474335 on epoch=109
05/17/2022 11:22:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.466914 on epoch=113
05/17/2022 11:22:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.520658 on epoch=116
05/17/2022 11:22:08 - INFO - __main__ - Global step 350 Train loss 0.479909 ACC 0.65625 on epoch=116
05/17/2022 11:22:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.543060 on epoch=119
05/17/2022 11:22:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.461226 on epoch=123
05/17/2022 11:22:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.485107 on epoch=126
05/17/2022 11:22:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.500806 on epoch=129
05/17/2022 11:22:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.453319 on epoch=133
05/17/2022 11:22:23 - INFO - __main__ - Global step 400 Train loss 0.488704 ACC 0.46875 on epoch=133
05/17/2022 11:22:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.397222 on epoch=136
05/17/2022 11:22:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.470117 on epoch=139
05/17/2022 11:22:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.429561 on epoch=143
05/17/2022 11:22:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.341865 on epoch=146
05/17/2022 11:22:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.411917 on epoch=149
05/17/2022 11:22:37 - INFO - __main__ - Global step 450 Train loss 0.410137 ACC 0.25 on epoch=149
05/17/2022 11:22:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.365542 on epoch=153
05/17/2022 11:22:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.443966 on epoch=156
05/17/2022 11:22:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.418035 on epoch=159
05/17/2022 11:22:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.417869 on epoch=163
05/17/2022 11:22:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.384302 on epoch=166
05/17/2022 11:22:52 - INFO - __main__ - Global step 500 Train loss 0.405943 ACC 0.46875 on epoch=166
05/17/2022 11:22:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.387217 on epoch=169
05/17/2022 11:22:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.349521 on epoch=173
05/17/2022 11:23:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.432532 on epoch=176
05/17/2022 11:23:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.301624 on epoch=179
05/17/2022 11:23:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.364800 on epoch=183
05/17/2022 11:23:06 - INFO - __main__ - Global step 550 Train loss 0.367139 ACC 0.25 on epoch=183
05/17/2022 11:23:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.352330 on epoch=186
05/17/2022 11:23:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.407534 on epoch=189
05/17/2022 11:23:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.306663 on epoch=193
05/17/2022 11:23:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.368837 on epoch=196
05/17/2022 11:23:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.338985 on epoch=199
05/17/2022 11:23:20 - INFO - __main__ - Global step 600 Train loss 0.354870 ACC 0.59375 on epoch=199
05/17/2022 11:23:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.283265 on epoch=203
05/17/2022 11:23:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.250705 on epoch=206
05/17/2022 11:23:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.268461 on epoch=209
05/17/2022 11:23:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.220920 on epoch=213
05/17/2022 11:23:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.221938 on epoch=216
05/17/2022 11:23:34 - INFO - __main__ - Global step 650 Train loss 0.249058 ACC 0.5 on epoch=216
05/17/2022 11:23:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.268890 on epoch=219
05/17/2022 11:23:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.193863 on epoch=223
05/17/2022 11:23:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.252389 on epoch=226
05/17/2022 11:23:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.200545 on epoch=229
05/17/2022 11:23:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.113340 on epoch=233
05/17/2022 11:23:49 - INFO - __main__ - Global step 700 Train loss 0.205806 ACC 0.46875 on epoch=233
05/17/2022 11:23:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.152846 on epoch=236
05/17/2022 11:23:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.146821 on epoch=239
05/17/2022 11:23:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.090555 on epoch=243
05/17/2022 11:24:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.108170 on epoch=246
05/17/2022 11:24:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.179793 on epoch=249
05/17/2022 11:24:03 - INFO - __main__ - Global step 750 Train loss 0.135637 ACC 0.5 on epoch=249
05/17/2022 11:24:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.091941 on epoch=253
05/17/2022 11:24:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.105256 on epoch=256
05/17/2022 11:24:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.090607 on epoch=259
05/17/2022 11:24:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.118449 on epoch=263
05/17/2022 11:24:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.124702 on epoch=266
05/17/2022 11:24:18 - INFO - __main__ - Global step 800 Train loss 0.106191 ACC 0.625 on epoch=266
05/17/2022 11:24:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.088791 on epoch=269
05/17/2022 11:24:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.145325 on epoch=273
05/17/2022 11:24:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.073517 on epoch=276
05/17/2022 11:24:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.096670 on epoch=279
05/17/2022 11:24:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.062630 on epoch=283
05/17/2022 11:24:32 - INFO - __main__ - Global step 850 Train loss 0.093387 ACC 0.71875 on epoch=283
05/17/2022 11:24:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.089179 on epoch=286
05/17/2022 11:24:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.067247 on epoch=289
05/17/2022 11:24:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.076821 on epoch=293
05/17/2022 11:24:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.065586 on epoch=296
05/17/2022 11:24:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.048502 on epoch=299
05/17/2022 11:24:47 - INFO - __main__ - Global step 900 Train loss 0.069467 ACC 0.5625 on epoch=299
05/17/2022 11:24:47 - INFO - __main__ - save last model!
05/17/2022 11:24:48 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:24:48 - INFO - __main__ - Printing 3 examples
05/17/2022 11:24:48 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/17/2022 11:24:48 - INFO - __main__ - ['contradiction']
05/17/2022 11:24:48 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/17/2022 11:24:48 - INFO - __main__ - ['contradiction']
05/17/2022 11:24:48 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/17/2022 11:24:48 - INFO - __main__ - ['contradiction']
05/17/2022 11:24:48 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:24:48 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:24:48 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:24:48 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:24:48 - INFO - __main__ - Printing 3 examples
05/17/2022 11:24:48 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
05/17/2022 11:24:48 - INFO - __main__ - ['contradiction']
05/17/2022 11:24:48 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
05/17/2022 11:24:48 - INFO - __main__ - ['contradiction']
05/17/2022 11:24:48 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
05/17/2022 11:24:48 - INFO - __main__ - ['contradiction']
05/17/2022 11:24:48 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:24:48 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:24:48 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:24:50 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 11:24:50 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 11:24:50 - INFO - __main__ - Printing 3 examples
05/17/2022 11:24:50 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 11:24:50 - INFO - __main__ - ['contradiction']
05/17/2022 11:24:50 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 11:24:50 - INFO - __main__ - ['neutral']
05/17/2022 11:24:50 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 11:24:50 - INFO - __main__ - ['entailment']
05/17/2022 11:24:50 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:24:50 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:24:50 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 11:24:51 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_21_0.0005_8_predictions.txt
05/17/2022 11:24:51 - INFO - __main__ - ACC on test data: 0.6071
05/17/2022 11:24:51 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.0005, bsz=8, dev_performance=0.71875, test_performance=0.6071428571428571
05/17/2022 11:24:51 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.0003, bsz=8 ...
05/17/2022 11:24:52 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:24:52 - INFO - __main__ - Printing 3 examples
05/17/2022 11:24:52 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/17/2022 11:24:52 - INFO - __main__ - ['contradiction']
05/17/2022 11:24:52 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/17/2022 11:24:52 - INFO - __main__ - ['contradiction']
05/17/2022 11:24:52 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/17/2022 11:24:52 - INFO - __main__ - ['contradiction']
05/17/2022 11:24:52 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:24:52 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:24:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:24:52 - INFO - __main__ - Starting training!
05/17/2022 11:24:52 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:24:52 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:24:52 - INFO - __main__ - Printing 3 examples
05/17/2022 11:24:52 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
05/17/2022 11:24:52 - INFO - __main__ - ['contradiction']
05/17/2022 11:24:52 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
05/17/2022 11:24:52 - INFO - __main__ - ['contradiction']
05/17/2022 11:24:52 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
05/17/2022 11:24:52 - INFO - __main__ - ['contradiction']
05/17/2022 11:24:52 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:24:52 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:24:52 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:24:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:24:56 - INFO - __main__ - Starting training!
05/17/2022 11:24:59 - INFO - __main__ - Step 10 Global step 10 Train loss 17.883980 on epoch=3
05/17/2022 11:25:01 - INFO - __main__ - Step 20 Global step 20 Train loss 14.770889 on epoch=6
05/17/2022 11:25:04 - INFO - __main__ - Step 30 Global step 30 Train loss 10.933594 on epoch=9
05/17/2022 11:25:06 - INFO - __main__ - Step 40 Global step 40 Train loss 8.009176 on epoch=13
05/17/2022 11:25:09 - INFO - __main__ - Step 50 Global step 50 Train loss 7.704830 on epoch=16
05/17/2022 11:25:10 - INFO - __main__ - Global step 50 Train loss 11.860495 ACC 0.03125 on epoch=16
05/17/2022 11:25:13 - INFO - __main__ - Step 60 Global step 60 Train loss 5.658946 on epoch=19
05/17/2022 11:25:16 - INFO - __main__ - Step 70 Global step 70 Train loss 4.613204 on epoch=23
05/17/2022 11:25:18 - INFO - __main__ - Step 80 Global step 80 Train loss 3.662251 on epoch=26
05/17/2022 11:25:21 - INFO - __main__ - Step 90 Global step 90 Train loss 3.891008 on epoch=29
05/17/2022 11:25:24 - INFO - __main__ - Step 100 Global step 100 Train loss 2.649142 on epoch=33
05/17/2022 11:25:24 - INFO - __main__ - Global step 100 Train loss 4.094910 ACC 0.15625 on epoch=33
05/17/2022 11:25:27 - INFO - __main__ - Step 110 Global step 110 Train loss 2.359614 on epoch=36
05/17/2022 11:25:30 - INFO - __main__ - Step 120 Global step 120 Train loss 2.777361 on epoch=39
05/17/2022 11:25:33 - INFO - __main__ - Step 130 Global step 130 Train loss 2.316587 on epoch=43
05/17/2022 11:25:36 - INFO - __main__ - Step 140 Global step 140 Train loss 1.722988 on epoch=46
05/17/2022 11:25:39 - INFO - __main__ - Step 150 Global step 150 Train loss 2.601162 on epoch=49
05/17/2022 11:25:39 - INFO - __main__ - Global step 150 Train loss 2.355542 ACC 0.5 on epoch=49
05/17/2022 11:25:42 - INFO - __main__ - Step 160 Global step 160 Train loss 2.716719 on epoch=53
05/17/2022 11:25:45 - INFO - __main__ - Step 170 Global step 170 Train loss 2.475309 on epoch=56
05/17/2022 11:25:48 - INFO - __main__ - Step 180 Global step 180 Train loss 2.054110 on epoch=59
05/17/2022 11:25:51 - INFO - __main__ - Step 190 Global step 190 Train loss 2.194606 on epoch=63
05/17/2022 11:25:53 - INFO - __main__ - Step 200 Global step 200 Train loss 1.634800 on epoch=66
05/17/2022 11:25:54 - INFO - __main__ - Global step 200 Train loss 2.215109 ACC 0.5 on epoch=66
05/17/2022 11:25:56 - INFO - __main__ - Step 210 Global step 210 Train loss 1.730690 on epoch=69
05/17/2022 11:25:59 - INFO - __main__ - Step 220 Global step 220 Train loss 1.655513 on epoch=73
05/17/2022 11:26:02 - INFO - __main__ - Step 230 Global step 230 Train loss 1.164503 on epoch=76
05/17/2022 11:26:05 - INFO - __main__ - Step 240 Global step 240 Train loss 1.192025 on epoch=79
05/17/2022 11:26:08 - INFO - __main__ - Step 250 Global step 250 Train loss 1.580746 on epoch=83
05/17/2022 11:26:08 - INFO - __main__ - Global step 250 Train loss 1.464695 ACC 0.03125 on epoch=83
05/17/2022 11:26:11 - INFO - __main__ - Step 260 Global step 260 Train loss 1.375498 on epoch=86
05/17/2022 11:26:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.955589 on epoch=89
05/17/2022 11:26:16 - INFO - __main__ - Step 280 Global step 280 Train loss 1.326697 on epoch=93
05/17/2022 11:26:19 - INFO - __main__ - Step 290 Global step 290 Train loss 1.120721 on epoch=96
05/17/2022 11:26:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.907115 on epoch=99
05/17/2022 11:26:22 - INFO - __main__ - Global step 300 Train loss 1.137124 ACC 0.71875 on epoch=99
05/17/2022 11:26:26 - INFO - __main__ - Step 310 Global step 310 Train loss 1.101985 on epoch=103
05/17/2022 11:26:28 - INFO - __main__ - Step 320 Global step 320 Train loss 1.128769 on epoch=106
05/17/2022 11:26:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.798851 on epoch=109
05/17/2022 11:26:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.964400 on epoch=113
05/17/2022 11:26:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.979712 on epoch=116
05/17/2022 11:26:37 - INFO - __main__ - Global step 350 Train loss 0.994743 ACC 0.46875 on epoch=116
05/17/2022 11:26:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.781710 on epoch=119
05/17/2022 11:26:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.952246 on epoch=123
05/17/2022 11:26:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.929535 on epoch=126
05/17/2022 11:26:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.827576 on epoch=129
05/17/2022 11:26:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.762293 on epoch=133
05/17/2022 11:26:53 - INFO - __main__ - Global step 400 Train loss 0.850672 ACC 0.46875 on epoch=133
05/17/2022 11:26:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.840737 on epoch=136
05/17/2022 11:26:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.750946 on epoch=139
05/17/2022 11:27:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.750880 on epoch=143
05/17/2022 11:27:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.698794 on epoch=146
05/17/2022 11:27:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.769756 on epoch=149
05/17/2022 11:27:08 - INFO - __main__ - Global step 450 Train loss 0.762223 ACC 0.5 on epoch=149
05/17/2022 11:27:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.667942 on epoch=153
05/17/2022 11:27:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.763717 on epoch=156
05/17/2022 11:27:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.710205 on epoch=159
05/17/2022 11:27:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.560238 on epoch=163
05/17/2022 11:27:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.602427 on epoch=166
05/17/2022 11:27:22 - INFO - __main__ - Global step 500 Train loss 0.660906 ACC 0.46875 on epoch=166
05/17/2022 11:27:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.660039 on epoch=169
05/17/2022 11:27:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.572742 on epoch=173
05/17/2022 11:27:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.624090 on epoch=176
05/17/2022 11:27:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.564782 on epoch=179
05/17/2022 11:27:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.546362 on epoch=183
05/17/2022 11:27:36 - INFO - __main__ - Global step 550 Train loss 0.593603 ACC 0.4375 on epoch=183
05/17/2022 11:27:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.524658 on epoch=186
05/17/2022 11:27:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.494267 on epoch=189
05/17/2022 11:27:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.572787 on epoch=193
05/17/2022 11:27:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.463228 on epoch=196
05/17/2022 11:27:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.531457 on epoch=199
05/17/2022 11:27:50 - INFO - __main__ - Global step 600 Train loss 0.517279 ACC 0.46875 on epoch=199
05/17/2022 11:27:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.498446 on epoch=203
05/17/2022 11:27:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.466893 on epoch=206
05/17/2022 11:27:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.545255 on epoch=209
05/17/2022 11:28:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.431586 on epoch=213
05/17/2022 11:28:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.476364 on epoch=216
05/17/2022 11:28:05 - INFO - __main__ - Global step 650 Train loss 0.483709 ACC 0.5 on epoch=216
05/17/2022 11:28:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.444959 on epoch=219
05/17/2022 11:28:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.401046 on epoch=223
05/17/2022 11:28:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.418204 on epoch=226
05/17/2022 11:28:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.425628 on epoch=229
05/17/2022 11:28:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.496670 on epoch=233
05/17/2022 11:28:20 - INFO - __main__ - Global step 700 Train loss 0.437301 ACC 0.09375 on epoch=233
05/17/2022 11:28:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.464906 on epoch=236
05/17/2022 11:28:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.477384 on epoch=239
05/17/2022 11:28:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.393671 on epoch=243
05/17/2022 11:28:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.395660 on epoch=246
05/17/2022 11:28:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.396940 on epoch=249
05/17/2022 11:28:35 - INFO - __main__ - Global step 750 Train loss 0.425712 ACC 0.34375 on epoch=249
05/17/2022 11:28:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.473413 on epoch=253
05/17/2022 11:28:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.426653 on epoch=256
05/17/2022 11:28:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.436967 on epoch=259
05/17/2022 11:28:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.410321 on epoch=263
05/17/2022 11:28:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.389657 on epoch=266
05/17/2022 11:28:49 - INFO - __main__ - Global step 800 Train loss 0.427402 ACC 0.5 on epoch=266
05/17/2022 11:28:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.347043 on epoch=269
05/17/2022 11:28:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.367236 on epoch=273
05/17/2022 11:28:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.363969 on epoch=276
05/17/2022 11:29:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.394333 on epoch=279
05/17/2022 11:29:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.403736 on epoch=283
05/17/2022 11:29:04 - INFO - __main__ - Global step 850 Train loss 0.375263 ACC 0.46875 on epoch=283
05/17/2022 11:29:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.391917 on epoch=286
05/17/2022 11:29:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.375228 on epoch=289
05/17/2022 11:29:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.345440 on epoch=293
05/17/2022 11:29:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.356964 on epoch=296
05/17/2022 11:29:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.344525 on epoch=299
05/17/2022 11:29:18 - INFO - __main__ - Global step 900 Train loss 0.362815 ACC 0.40625 on epoch=299
05/17/2022 11:29:18 - INFO - __main__ - save last model!
05/17/2022 11:29:20 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:29:20 - INFO - __main__ - Printing 3 examples
05/17/2022 11:29:20 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/17/2022 11:29:20 - INFO - __main__ - ['contradiction']
05/17/2022 11:29:20 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/17/2022 11:29:20 - INFO - __main__ - ['contradiction']
05/17/2022 11:29:20 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/17/2022 11:29:20 - INFO - __main__ - ['contradiction']
05/17/2022 11:29:20 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:29:20 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:29:20 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:29:20 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:29:20 - INFO - __main__ - Printing 3 examples
05/17/2022 11:29:20 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
05/17/2022 11:29:20 - INFO - __main__ - ['contradiction']
05/17/2022 11:29:20 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
05/17/2022 11:29:20 - INFO - __main__ - ['contradiction']
05/17/2022 11:29:20 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
05/17/2022 11:29:20 - INFO - __main__ - ['contradiction']
05/17/2022 11:29:20 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:29:20 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:29:20 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:29:21 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 11:29:21 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 11:29:21 - INFO - __main__ - Printing 3 examples
05/17/2022 11:29:21 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 11:29:21 - INFO - __main__ - ['contradiction']
05/17/2022 11:29:21 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 11:29:21 - INFO - __main__ - ['neutral']
05/17/2022 11:29:21 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 11:29:21 - INFO - __main__ - ['entailment']
05/17/2022 11:29:21 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:29:21 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:29:21 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 11:29:22 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_21_0.0003_8_predictions.txt
05/17/2022 11:29:22 - INFO - __main__ - ACC on test data: 0.3750
05/17/2022 11:29:22 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.0003, bsz=8, dev_performance=0.71875, test_performance=0.375
05/17/2022 11:29:22 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.0002, bsz=8 ...
05/17/2022 11:29:23 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:29:23 - INFO - __main__ - Printing 3 examples
05/17/2022 11:29:23 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/17/2022 11:29:23 - INFO - __main__ - ['contradiction']
05/17/2022 11:29:23 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/17/2022 11:29:23 - INFO - __main__ - ['contradiction']
05/17/2022 11:29:23 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/17/2022 11:29:23 - INFO - __main__ - ['contradiction']
05/17/2022 11:29:23 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:29:23 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:29:23 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:29:23 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:29:23 - INFO - __main__ - Printing 3 examples
05/17/2022 11:29:23 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
05/17/2022 11:29:23 - INFO - __main__ - ['contradiction']
05/17/2022 11:29:23 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
05/17/2022 11:29:23 - INFO - __main__ - ['contradiction']
05/17/2022 11:29:23 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
05/17/2022 11:29:23 - INFO - __main__ - ['contradiction']
05/17/2022 11:29:23 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:29:23 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:29:23 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:29:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:29:24 - INFO - __main__ - Starting training!
05/17/2022 11:29:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:29:26 - INFO - __main__ - Starting training!
05/17/2022 11:29:29 - INFO - __main__ - Step 10 Global step 10 Train loss 17.865986 on epoch=3
05/17/2022 11:29:31 - INFO - __main__ - Step 20 Global step 20 Train loss 15.517568 on epoch=6
05/17/2022 11:29:34 - INFO - __main__ - Step 30 Global step 30 Train loss 11.925739 on epoch=9
05/17/2022 11:29:37 - INFO - __main__ - Step 40 Global step 40 Train loss 10.517013 on epoch=13
05/17/2022 11:29:39 - INFO - __main__ - Step 50 Global step 50 Train loss 8.266142 on epoch=16
05/17/2022 11:29:42 - INFO - __main__ - Global step 50 Train loss 12.818489 ACC 0.03125 on epoch=16
05/17/2022 11:29:45 - INFO - __main__ - Step 60 Global step 60 Train loss 6.969693 on epoch=19
05/17/2022 11:29:48 - INFO - __main__ - Step 70 Global step 70 Train loss 6.521017 on epoch=23
05/17/2022 11:29:51 - INFO - __main__ - Step 80 Global step 80 Train loss 5.964061 on epoch=26
05/17/2022 11:29:53 - INFO - __main__ - Step 90 Global step 90 Train loss 5.196848 on epoch=29
05/17/2022 11:29:57 - INFO - __main__ - Step 100 Global step 100 Train loss 4.175569 on epoch=33
05/17/2022 11:29:57 - INFO - __main__ - Global step 100 Train loss 5.765437 ACC 0.25 on epoch=33
05/17/2022 11:30:00 - INFO - __main__ - Step 110 Global step 110 Train loss 4.432315 on epoch=36
05/17/2022 11:30:03 - INFO - __main__ - Step 120 Global step 120 Train loss 3.463728 on epoch=39
05/17/2022 11:30:06 - INFO - __main__ - Step 130 Global step 130 Train loss 2.778622 on epoch=43
05/17/2022 11:30:09 - INFO - __main__ - Step 140 Global step 140 Train loss 2.642925 on epoch=46
05/17/2022 11:30:12 - INFO - __main__ - Step 150 Global step 150 Train loss 2.358929 on epoch=49
05/17/2022 11:30:12 - INFO - __main__ - Global step 150 Train loss 3.135304 ACC 0.5 on epoch=49
05/17/2022 11:30:16 - INFO - __main__ - Step 160 Global step 160 Train loss 2.949240 on epoch=53
05/17/2022 11:30:18 - INFO - __main__ - Step 170 Global step 170 Train loss 2.351861 on epoch=56
05/17/2022 11:30:21 - INFO - __main__ - Step 180 Global step 180 Train loss 2.365159 on epoch=59
05/17/2022 11:30:24 - INFO - __main__ - Step 190 Global step 190 Train loss 2.144425 on epoch=63
05/17/2022 11:30:27 - INFO - __main__ - Step 200 Global step 200 Train loss 1.849633 on epoch=66
05/17/2022 11:30:27 - INFO - __main__ - Global step 200 Train loss 2.332064 ACC 0.40625 on epoch=66
05/17/2022 11:30:30 - INFO - __main__ - Step 210 Global step 210 Train loss 1.880533 on epoch=69
05/17/2022 11:30:33 - INFO - __main__ - Step 220 Global step 220 Train loss 2.403905 on epoch=73
05/17/2022 11:30:35 - INFO - __main__ - Step 230 Global step 230 Train loss 2.144798 on epoch=76
05/17/2022 11:30:38 - INFO - __main__ - Step 240 Global step 240 Train loss 1.447082 on epoch=79
05/17/2022 11:30:41 - INFO - __main__ - Step 250 Global step 250 Train loss 1.980792 on epoch=83
05/17/2022 11:30:41 - INFO - __main__ - Global step 250 Train loss 1.971422 ACC 0.5 on epoch=83
05/17/2022 11:30:44 - INFO - __main__ - Step 260 Global step 260 Train loss 2.041331 on epoch=86
05/17/2022 11:30:47 - INFO - __main__ - Step 270 Global step 270 Train loss 1.470147 on epoch=89
05/17/2022 11:30:50 - INFO - __main__ - Step 280 Global step 280 Train loss 1.351584 on epoch=93
05/17/2022 11:30:52 - INFO - __main__ - Step 290 Global step 290 Train loss 1.300498 on epoch=96
05/17/2022 11:30:55 - INFO - __main__ - Step 300 Global step 300 Train loss 1.626443 on epoch=99
05/17/2022 11:30:56 - INFO - __main__ - Global step 300 Train loss 1.558000 ACC 0.53125 on epoch=99
05/17/2022 11:30:59 - INFO - __main__ - Step 310 Global step 310 Train loss 1.345333 on epoch=103
05/17/2022 11:31:02 - INFO - __main__ - Step 320 Global step 320 Train loss 1.191231 on epoch=106
05/17/2022 11:31:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.768781 on epoch=109
05/17/2022 11:31:07 - INFO - __main__ - Step 340 Global step 340 Train loss 1.178682 on epoch=113
05/17/2022 11:31:10 - INFO - __main__ - Step 350 Global step 350 Train loss 1.195448 on epoch=116
05/17/2022 11:31:11 - INFO - __main__ - Global step 350 Train loss 1.135895 ACC 0.71875 on epoch=116
05/17/2022 11:31:14 - INFO - __main__ - Step 360 Global step 360 Train loss 1.369120 on epoch=119
05/17/2022 11:31:16 - INFO - __main__ - Step 370 Global step 370 Train loss 1.127189 on epoch=123
05/17/2022 11:31:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.971240 on epoch=126
05/17/2022 11:31:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.982180 on epoch=129
05/17/2022 11:31:26 - INFO - __main__ - Step 400 Global step 400 Train loss 1.070975 on epoch=133
05/17/2022 11:31:26 - INFO - __main__ - Global step 400 Train loss 1.104141 ACC 0.21875 on epoch=133
05/17/2022 11:31:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.940539 on epoch=136
05/17/2022 11:31:33 - INFO - __main__ - Step 420 Global step 420 Train loss 1.016370 on epoch=139
05/17/2022 11:31:36 - INFO - __main__ - Step 430 Global step 430 Train loss 1.069418 on epoch=143
05/17/2022 11:31:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.787344 on epoch=146
05/17/2022 11:31:43 - INFO - __main__ - Step 450 Global step 450 Train loss 1.268100 on epoch=149
05/17/2022 11:31:43 - INFO - __main__ - Global step 450 Train loss 1.016354 ACC 0.5 on epoch=149
05/17/2022 11:31:47 - INFO - __main__ - Step 460 Global step 460 Train loss 1.059968 on epoch=153
05/17/2022 11:31:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.797994 on epoch=156
05/17/2022 11:31:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.942415 on epoch=159
05/17/2022 11:31:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.907881 on epoch=163
05/17/2022 11:31:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.856765 on epoch=166
05/17/2022 11:31:58 - INFO - __main__ - Global step 500 Train loss 0.913005 ACC 0.4375 on epoch=166
05/17/2022 11:32:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.807405 on epoch=169
05/17/2022 11:32:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.843632 on epoch=173
05/17/2022 11:32:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.870442 on epoch=176
05/17/2022 11:32:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.776868 on epoch=179
05/17/2022 11:32:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.627420 on epoch=183
05/17/2022 11:32:12 - INFO - __main__ - Global step 550 Train loss 0.785154 ACC 0.4375 on epoch=183
05/17/2022 11:32:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.654170 on epoch=186
05/17/2022 11:32:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.798782 on epoch=189
05/17/2022 11:32:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.854055 on epoch=193
05/17/2022 11:32:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.557921 on epoch=196
05/17/2022 11:32:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.631424 on epoch=199
05/17/2022 11:32:26 - INFO - __main__ - Global step 600 Train loss 0.699270 ACC 0.34375 on epoch=199
05/17/2022 11:32:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.654795 on epoch=203
05/17/2022 11:32:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.700053 on epoch=206
05/17/2022 11:32:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.720155 on epoch=209
05/17/2022 11:32:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.623562 on epoch=213
05/17/2022 11:32:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.711708 on epoch=216
05/17/2022 11:32:41 - INFO - __main__ - Global step 650 Train loss 0.682055 ACC 0.46875 on epoch=216
05/17/2022 11:32:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.697959 on epoch=219
05/17/2022 11:32:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.560682 on epoch=223
05/17/2022 11:32:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.664511 on epoch=226
05/17/2022 11:32:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.574091 on epoch=229
05/17/2022 11:32:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.611555 on epoch=233
05/17/2022 11:32:55 - INFO - __main__ - Global step 700 Train loss 0.621760 ACC 0.3125 on epoch=233
05/17/2022 11:32:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.641196 on epoch=236
05/17/2022 11:33:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.565806 on epoch=239
05/17/2022 11:33:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.479642 on epoch=243
05/17/2022 11:33:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.604394 on epoch=246
05/17/2022 11:33:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.497316 on epoch=249
05/17/2022 11:33:10 - INFO - __main__ - Global step 750 Train loss 0.557671 ACC 0.5 on epoch=249
05/17/2022 11:33:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.556118 on epoch=253
05/17/2022 11:33:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.516831 on epoch=256
05/17/2022 11:33:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.480224 on epoch=259
05/17/2022 11:33:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.600461 on epoch=263
05/17/2022 11:33:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.452652 on epoch=266
05/17/2022 11:33:24 - INFO - __main__ - Global step 800 Train loss 0.521257 ACC 0.53125 on epoch=266
05/17/2022 11:33:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.471872 on epoch=269
05/17/2022 11:33:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.487065 on epoch=273
05/17/2022 11:33:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.418615 on epoch=276
05/17/2022 11:33:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.517954 on epoch=279
05/17/2022 11:33:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.445350 on epoch=283
05/17/2022 11:33:40 - INFO - __main__ - Global step 850 Train loss 0.468171 ACC 0.84375 on epoch=283
05/17/2022 11:33:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.353569 on epoch=286
05/17/2022 11:33:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.379984 on epoch=289
05/17/2022 11:33:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.291657 on epoch=293
05/17/2022 11:33:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.364500 on epoch=296
05/17/2022 11:33:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.415328 on epoch=299
05/17/2022 11:33:55 - INFO - __main__ - Global step 900 Train loss 0.361008 ACC 0.5 on epoch=299
05/17/2022 11:33:55 - INFO - __main__ - save last model!
05/17/2022 11:33:56 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:33:56 - INFO - __main__ - Printing 3 examples
05/17/2022 11:33:56 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/17/2022 11:33:56 - INFO - __main__ - ['contradiction']
05/17/2022 11:33:56 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/17/2022 11:33:56 - INFO - __main__ - ['contradiction']
05/17/2022 11:33:56 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/17/2022 11:33:56 - INFO - __main__ - ['contradiction']
05/17/2022 11:33:56 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:33:56 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:33:56 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:33:56 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:33:56 - INFO - __main__ - Printing 3 examples
05/17/2022 11:33:56 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
05/17/2022 11:33:56 - INFO - __main__ - ['contradiction']
05/17/2022 11:33:56 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
05/17/2022 11:33:56 - INFO - __main__ - ['contradiction']
05/17/2022 11:33:56 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
05/17/2022 11:33:56 - INFO - __main__ - ['contradiction']
05/17/2022 11:33:56 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:33:56 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:33:56 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:33:58 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 11:33:58 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 11:33:58 - INFO - __main__ - Printing 3 examples
05/17/2022 11:33:58 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 11:33:58 - INFO - __main__ - ['contradiction']
05/17/2022 11:33:58 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 11:33:58 - INFO - __main__ - ['neutral']
05/17/2022 11:33:58 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 11:33:58 - INFO - __main__ - ['entailment']
05/17/2022 11:33:58 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:33:58 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:33:58 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 11:33:59 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_21_0.0002_8_predictions.txt
05/17/2022 11:33:59 - INFO - __main__ - ACC on test data: 0.6250
05/17/2022 11:33:59 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.0002, bsz=8, dev_performance=0.84375, test_performance=0.625
05/17/2022 11:33:59 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.0001, bsz=8 ...
05/17/2022 11:34:00 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:34:00 - INFO - __main__ - Printing 3 examples
05/17/2022 11:34:00 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/17/2022 11:34:00 - INFO - __main__ - ['contradiction']
05/17/2022 11:34:00 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/17/2022 11:34:00 - INFO - __main__ - ['contradiction']
05/17/2022 11:34:00 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/17/2022 11:34:00 - INFO - __main__ - ['contradiction']
05/17/2022 11:34:00 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:34:00 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:34:00 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:34:00 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:34:00 - INFO - __main__ - Printing 3 examples
05/17/2022 11:34:00 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
05/17/2022 11:34:00 - INFO - __main__ - ['contradiction']
05/17/2022 11:34:00 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
05/17/2022 11:34:00 - INFO - __main__ - ['contradiction']
05/17/2022 11:34:00 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
05/17/2022 11:34:00 - INFO - __main__ - ['contradiction']
05/17/2022 11:34:00 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:34:00 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:34:00 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:34:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:34:00 - INFO - __main__ - Starting training!
05/17/2022 11:34:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:34:05 - INFO - __main__ - Starting training!
05/17/2022 11:34:07 - INFO - __main__ - Step 10 Global step 10 Train loss 17.458256 on epoch=3
05/17/2022 11:34:09 - INFO - __main__ - Step 20 Global step 20 Train loss 17.512932 on epoch=6
05/17/2022 11:34:12 - INFO - __main__ - Step 30 Global step 30 Train loss 15.179060 on epoch=9
05/17/2022 11:34:15 - INFO - __main__ - Step 40 Global step 40 Train loss 12.524983 on epoch=13
05/17/2022 11:34:18 - INFO - __main__ - Step 50 Global step 50 Train loss 11.024161 on epoch=16
05/17/2022 11:34:22 - INFO - __main__ - Global step 50 Train loss 14.739878 ACC 0.0 on epoch=16
05/17/2022 11:34:25 - INFO - __main__ - Step 60 Global step 60 Train loss 11.140974 on epoch=19
05/17/2022 11:34:28 - INFO - __main__ - Step 70 Global step 70 Train loss 9.914094 on epoch=23
05/17/2022 11:34:30 - INFO - __main__ - Step 80 Global step 80 Train loss 9.861036 on epoch=26
05/17/2022 11:34:33 - INFO - __main__ - Step 90 Global step 90 Train loss 8.691222 on epoch=29
05/17/2022 11:34:36 - INFO - __main__ - Step 100 Global step 100 Train loss 8.749198 on epoch=33
05/17/2022 11:34:39 - INFO - __main__ - Global step 100 Train loss 9.671305 ACC 0.0 on epoch=33
05/17/2022 11:34:42 - INFO - __main__ - Step 110 Global step 110 Train loss 8.178869 on epoch=36
05/17/2022 11:34:44 - INFO - __main__ - Step 120 Global step 120 Train loss 7.470286 on epoch=39
05/17/2022 11:34:47 - INFO - __main__ - Step 130 Global step 130 Train loss 6.749068 on epoch=43
05/17/2022 11:34:50 - INFO - __main__ - Step 140 Global step 140 Train loss 6.444604 on epoch=46
05/17/2022 11:34:53 - INFO - __main__ - Step 150 Global step 150 Train loss 6.357968 on epoch=49
05/17/2022 11:34:54 - INFO - __main__ - Global step 150 Train loss 7.040160 ACC 0.0 on epoch=49
05/17/2022 11:34:57 - INFO - __main__ - Step 160 Global step 160 Train loss 5.994840 on epoch=53
05/17/2022 11:35:00 - INFO - __main__ - Step 170 Global step 170 Train loss 5.438458 on epoch=56
05/17/2022 11:35:03 - INFO - __main__ - Step 180 Global step 180 Train loss 5.990800 on epoch=59
05/17/2022 11:35:05 - INFO - __main__ - Step 190 Global step 190 Train loss 5.516978 on epoch=63
05/17/2022 11:35:08 - INFO - __main__ - Step 200 Global step 200 Train loss 4.971193 on epoch=66
05/17/2022 11:35:10 - INFO - __main__ - Global step 200 Train loss 5.582454 ACC 0.0 on epoch=66
05/17/2022 11:35:12 - INFO - __main__ - Step 210 Global step 210 Train loss 5.089768 on epoch=69
05/17/2022 11:35:15 - INFO - __main__ - Step 220 Global step 220 Train loss 4.064639 on epoch=73
05/17/2022 11:35:18 - INFO - __main__ - Step 230 Global step 230 Train loss 4.712951 on epoch=76
05/17/2022 11:35:20 - INFO - __main__ - Step 240 Global step 240 Train loss 4.496560 on epoch=79
05/17/2022 11:35:23 - INFO - __main__ - Step 250 Global step 250 Train loss 3.712530 on epoch=83
05/17/2022 11:35:24 - INFO - __main__ - Global step 250 Train loss 4.415289 ACC 0.03125 on epoch=83
05/17/2022 11:35:27 - INFO - __main__ - Step 260 Global step 260 Train loss 4.343583 on epoch=86
05/17/2022 11:35:30 - INFO - __main__ - Step 270 Global step 270 Train loss 3.668590 on epoch=89
05/17/2022 11:35:33 - INFO - __main__ - Step 280 Global step 280 Train loss 2.617352 on epoch=93
05/17/2022 11:35:35 - INFO - __main__ - Step 290 Global step 290 Train loss 3.572001 on epoch=96
05/17/2022 11:35:38 - INFO - __main__ - Step 300 Global step 300 Train loss 3.370040 on epoch=99
05/17/2022 11:35:38 - INFO - __main__ - Global step 300 Train loss 3.514313 ACC 0.0 on epoch=99
05/17/2022 11:35:41 - INFO - __main__ - Step 310 Global step 310 Train loss 2.544584 on epoch=103
05/17/2022 11:35:44 - INFO - __main__ - Step 320 Global step 320 Train loss 2.425373 on epoch=106
05/17/2022 11:35:47 - INFO - __main__ - Step 330 Global step 330 Train loss 2.788986 on epoch=109
05/17/2022 11:35:49 - INFO - __main__ - Step 340 Global step 340 Train loss 3.374707 on epoch=113
05/17/2022 11:35:52 - INFO - __main__ - Step 350 Global step 350 Train loss 2.775785 on epoch=116
05/17/2022 11:35:53 - INFO - __main__ - Global step 350 Train loss 2.781887 ACC 0.34375 on epoch=116
05/17/2022 11:35:56 - INFO - __main__ - Step 360 Global step 360 Train loss 2.559336 on epoch=119
05/17/2022 11:35:58 - INFO - __main__ - Step 370 Global step 370 Train loss 2.685481 on epoch=123
05/17/2022 11:36:01 - INFO - __main__ - Step 380 Global step 380 Train loss 2.454350 on epoch=126
05/17/2022 11:36:04 - INFO - __main__ - Step 390 Global step 390 Train loss 2.273454 on epoch=129
05/17/2022 11:36:07 - INFO - __main__ - Step 400 Global step 400 Train loss 2.788641 on epoch=133
05/17/2022 11:36:07 - INFO - __main__ - Global step 400 Train loss 2.552253 ACC 0.5 on epoch=133
05/17/2022 11:36:10 - INFO - __main__ - Step 410 Global step 410 Train loss 2.483339 on epoch=136
05/17/2022 11:36:13 - INFO - __main__ - Step 420 Global step 420 Train loss 2.045261 on epoch=139
05/17/2022 11:36:16 - INFO - __main__ - Step 430 Global step 430 Train loss 2.632843 on epoch=143
05/17/2022 11:36:19 - INFO - __main__ - Step 440 Global step 440 Train loss 1.981108 on epoch=146
05/17/2022 11:36:21 - INFO - __main__ - Step 450 Global step 450 Train loss 2.430002 on epoch=149
05/17/2022 11:36:21 - INFO - __main__ - Global step 450 Train loss 2.314510 ACC 0.34375 on epoch=149
05/17/2022 11:36:24 - INFO - __main__ - Step 460 Global step 460 Train loss 1.846711 on epoch=153
05/17/2022 11:36:27 - INFO - __main__ - Step 470 Global step 470 Train loss 1.673982 on epoch=156
05/17/2022 11:36:30 - INFO - __main__ - Step 480 Global step 480 Train loss 2.233728 on epoch=159
05/17/2022 11:36:33 - INFO - __main__ - Step 490 Global step 490 Train loss 2.204515 on epoch=163
05/17/2022 11:36:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.974440 on epoch=166
05/17/2022 11:36:36 - INFO - __main__ - Global step 500 Train loss 1.786675 ACC 0.0 on epoch=166
05/17/2022 11:36:38 - INFO - __main__ - Step 510 Global step 510 Train loss 1.898750 on epoch=169
05/17/2022 11:36:41 - INFO - __main__ - Step 520 Global step 520 Train loss 1.345731 on epoch=173
05/17/2022 11:36:44 - INFO - __main__ - Step 530 Global step 530 Train loss 2.231788 on epoch=176
05/17/2022 11:36:47 - INFO - __main__ - Step 540 Global step 540 Train loss 1.519132 on epoch=179
05/17/2022 11:36:50 - INFO - __main__ - Step 550 Global step 550 Train loss 1.669683 on epoch=183
05/17/2022 11:36:50 - INFO - __main__ - Global step 550 Train loss 1.733017 ACC 0.28125 on epoch=183
05/17/2022 11:36:53 - INFO - __main__ - Step 560 Global step 560 Train loss 1.788174 on epoch=186
05/17/2022 11:36:57 - INFO - __main__ - Step 570 Global step 570 Train loss 1.912606 on epoch=189
05/17/2022 11:37:00 - INFO - __main__ - Step 580 Global step 580 Train loss 1.950065 on epoch=193
05/17/2022 11:37:03 - INFO - __main__ - Step 590 Global step 590 Train loss 1.449809 on epoch=196
05/17/2022 11:37:06 - INFO - __main__ - Step 600 Global step 600 Train loss 2.040528 on epoch=199
05/17/2022 11:37:06 - INFO - __main__ - Global step 600 Train loss 1.828236 ACC 0.4375 on epoch=199
05/17/2022 11:37:09 - INFO - __main__ - Step 610 Global step 610 Train loss 1.704407 on epoch=203
05/17/2022 11:37:12 - INFO - __main__ - Step 620 Global step 620 Train loss 1.317989 on epoch=206
05/17/2022 11:37:15 - INFO - __main__ - Step 630 Global step 630 Train loss 1.831717 on epoch=209
05/17/2022 11:37:18 - INFO - __main__ - Step 640 Global step 640 Train loss 1.578259 on epoch=213
05/17/2022 11:37:21 - INFO - __main__ - Step 650 Global step 650 Train loss 1.604377 on epoch=216
05/17/2022 11:37:22 - INFO - __main__ - Global step 650 Train loss 1.607350 ACC 0.4375 on epoch=216
05/17/2022 11:37:25 - INFO - __main__ - Step 660 Global step 660 Train loss 1.596393 on epoch=219
05/17/2022 11:37:27 - INFO - __main__ - Step 670 Global step 670 Train loss 1.335881 on epoch=223
05/17/2022 11:37:30 - INFO - __main__ - Step 680 Global step 680 Train loss 1.830206 on epoch=226
05/17/2022 11:37:33 - INFO - __main__ - Step 690 Global step 690 Train loss 1.528528 on epoch=229
05/17/2022 11:37:36 - INFO - __main__ - Step 700 Global step 700 Train loss 1.219349 on epoch=233
05/17/2022 11:37:36 - INFO - __main__ - Global step 700 Train loss 1.502071 ACC 0.40625 on epoch=233
05/17/2022 11:37:39 - INFO - __main__ - Step 710 Global step 710 Train loss 1.036768 on epoch=236
05/17/2022 11:37:42 - INFO - __main__ - Step 720 Global step 720 Train loss 1.112151 on epoch=239
05/17/2022 11:37:45 - INFO - __main__ - Step 730 Global step 730 Train loss 1.138405 on epoch=243
05/17/2022 11:37:47 - INFO - __main__ - Step 740 Global step 740 Train loss 1.116561 on epoch=246
05/17/2022 11:37:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.942389 on epoch=249
05/17/2022 11:37:50 - INFO - __main__ - Global step 750 Train loss 1.069255 ACC 0.71875 on epoch=249
05/17/2022 11:37:53 - INFO - __main__ - Step 760 Global step 760 Train loss 1.052311 on epoch=253
05/17/2022 11:37:56 - INFO - __main__ - Step 770 Global step 770 Train loss 1.372425 on epoch=256
05/17/2022 11:37:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.900417 on epoch=259
05/17/2022 11:38:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.998779 on epoch=263
05/17/2022 11:38:05 - INFO - __main__ - Step 800 Global step 800 Train loss 1.017505 on epoch=266
05/17/2022 11:38:05 - INFO - __main__ - Global step 800 Train loss 1.068287 ACC 0.875 on epoch=266
05/17/2022 11:38:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.847196 on epoch=269
05/17/2022 11:38:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.700025 on epoch=273
05/17/2022 11:38:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.904795 on epoch=276
05/17/2022 11:38:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.692834 on epoch=279
05/17/2022 11:38:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.725342 on epoch=283
05/17/2022 11:38:20 - INFO - __main__ - Global step 850 Train loss 0.774039 ACC 0.875 on epoch=283
05/17/2022 11:38:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.949916 on epoch=286
05/17/2022 11:38:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.765543 on epoch=289
05/17/2022 11:38:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.675498 on epoch=293
05/17/2022 11:38:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.524438 on epoch=296
05/17/2022 11:38:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.656385 on epoch=299
05/17/2022 11:38:35 - INFO - __main__ - Global step 900 Train loss 0.714356 ACC 0.875 on epoch=299
05/17/2022 11:38:35 - INFO - __main__ - save last model!
05/17/2022 11:38:36 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:38:36 - INFO - __main__ - Printing 3 examples
05/17/2022 11:38:36 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/17/2022 11:38:36 - INFO - __main__ - ['contradiction']
05/17/2022 11:38:36 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/17/2022 11:38:36 - INFO - __main__ - ['contradiction']
05/17/2022 11:38:36 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/17/2022 11:38:36 - INFO - __main__ - ['contradiction']
05/17/2022 11:38:36 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:38:36 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:38:36 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:38:36 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:38:36 - INFO - __main__ - Printing 3 examples
05/17/2022 11:38:36 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
05/17/2022 11:38:36 - INFO - __main__ - ['contradiction']
05/17/2022 11:38:36 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
05/17/2022 11:38:36 - INFO - __main__ - ['contradiction']
05/17/2022 11:38:36 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
05/17/2022 11:38:36 - INFO - __main__ - ['contradiction']
05/17/2022 11:38:36 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:38:36 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:38:36 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:38:38 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 11:38:38 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 11:38:38 - INFO - __main__ - Printing 3 examples
05/17/2022 11:38:38 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 11:38:38 - INFO - __main__ - ['contradiction']
05/17/2022 11:38:38 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 11:38:38 - INFO - __main__ - ['neutral']
05/17/2022 11:38:38 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 11:38:38 - INFO - __main__ - ['entailment']
05/17/2022 11:38:38 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:38:38 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:38:38 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 11:38:39 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_21_0.0001_8_predictions.txt
05/17/2022 11:38:39 - INFO - __main__ - ACC on test data: 0.7321
05/17/2022 11:38:39 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.0001, bsz=8, dev_performance=0.875, test_performance=0.7321428571428571
05/17/2022 11:38:39 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.0005, bsz=8 ...
05/17/2022 11:38:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:38:40 - INFO - __main__ - Starting training!
05/17/2022 11:38:40 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:38:40 - INFO - __main__ - Printing 3 examples
05/17/2022 11:38:40 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/17/2022 11:38:40 - INFO - __main__ - ['contradiction']
05/17/2022 11:38:40 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/17/2022 11:38:40 - INFO - __main__ - ['contradiction']
05/17/2022 11:38:40 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/17/2022 11:38:40 - INFO - __main__ - ['contradiction']
05/17/2022 11:38:40 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:38:40 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:38:40 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:38:40 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:38:40 - INFO - __main__ - Printing 3 examples
05/17/2022 11:38:40 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
05/17/2022 11:38:40 - INFO - __main__ - ['contradiction']
05/17/2022 11:38:40 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
05/17/2022 11:38:40 - INFO - __main__ - ['contradiction']
05/17/2022 11:38:40 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
05/17/2022 11:38:40 - INFO - __main__ - ['contradiction']
05/17/2022 11:38:40 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:38:40 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:38:40 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:38:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:38:44 - INFO - __main__ - Starting training!
05/17/2022 11:38:46 - INFO - __main__ - Step 10 Global step 10 Train loss 17.411467 on epoch=3
05/17/2022 11:38:49 - INFO - __main__ - Step 20 Global step 20 Train loss 14.794703 on epoch=6
05/17/2022 11:38:52 - INFO - __main__ - Step 30 Global step 30 Train loss 10.820940 on epoch=9
05/17/2022 11:38:55 - INFO - __main__ - Step 40 Global step 40 Train loss 6.750342 on epoch=13
05/17/2022 11:38:58 - INFO - __main__ - Step 50 Global step 50 Train loss 4.236140 on epoch=16
05/17/2022 11:38:58 - INFO - __main__ - Global step 50 Train loss 10.802718 ACC 0.3125 on epoch=16
05/17/2022 11:39:01 - INFO - __main__ - Step 60 Global step 60 Train loss 2.857960 on epoch=19
05/17/2022 11:39:04 - INFO - __main__ - Step 70 Global step 70 Train loss 3.412297 on epoch=23
05/17/2022 11:39:07 - INFO - __main__ - Step 80 Global step 80 Train loss 2.111652 on epoch=26
05/17/2022 11:39:09 - INFO - __main__ - Step 90 Global step 90 Train loss 2.066828 on epoch=29
05/17/2022 11:39:12 - INFO - __main__ - Step 100 Global step 100 Train loss 2.104773 on epoch=33
05/17/2022 11:39:12 - INFO - __main__ - Global step 100 Train loss 2.510702 ACC 0.4375 on epoch=33
05/17/2022 11:39:16 - INFO - __main__ - Step 110 Global step 110 Train loss 1.124453 on epoch=36
05/17/2022 11:39:18 - INFO - __main__ - Step 120 Global step 120 Train loss 1.204793 on epoch=39
05/17/2022 11:39:21 - INFO - __main__ - Step 130 Global step 130 Train loss 1.430546 on epoch=43
05/17/2022 11:39:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.905472 on epoch=46
05/17/2022 11:39:27 - INFO - __main__ - Step 150 Global step 150 Train loss 0.910571 on epoch=49
05/17/2022 11:39:27 - INFO - __main__ - Global step 150 Train loss 1.115167 ACC 0.4375 on epoch=49
05/17/2022 11:39:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.949755 on epoch=53
05/17/2022 11:39:32 - INFO - __main__ - Step 170 Global step 170 Train loss 1.066290 on epoch=56
05/17/2022 11:39:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.585870 on epoch=59
05/17/2022 11:39:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.685811 on epoch=63
05/17/2022 11:39:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.580661 on epoch=66
05/17/2022 11:39:41 - INFO - __main__ - Global step 200 Train loss 0.773677 ACC 0.53125 on epoch=66
05/17/2022 11:39:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.576330 on epoch=69
05/17/2022 11:39:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.415899 on epoch=73
05/17/2022 11:39:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.218163 on epoch=76
05/17/2022 11:39:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.397329 on epoch=79
05/17/2022 11:39:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.264411 on epoch=83
05/17/2022 11:39:55 - INFO - __main__ - Global step 250 Train loss 0.374427 ACC 0.78125 on epoch=83
05/17/2022 11:39:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.236235 on epoch=86
05/17/2022 11:40:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.100452 on epoch=89
05/17/2022 11:40:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.100271 on epoch=93
05/17/2022 11:40:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.234537 on epoch=96
05/17/2022 11:40:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.160515 on epoch=99
05/17/2022 11:40:09 - INFO - __main__ - Global step 300 Train loss 0.166402 ACC 0.78125 on epoch=99
05/17/2022 11:40:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.102299 on epoch=103
05/17/2022 11:40:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.040321 on epoch=106
05/17/2022 11:40:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.098825 on epoch=109
05/17/2022 11:40:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.071321 on epoch=113
05/17/2022 11:40:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.020709 on epoch=116
05/17/2022 11:40:24 - INFO - __main__ - Global step 350 Train loss 0.066695 ACC 0.75 on epoch=116
05/17/2022 11:40:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.341232 on epoch=119
05/17/2022 11:40:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.046580 on epoch=123
05/17/2022 11:40:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.054012 on epoch=126
05/17/2022 11:40:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.008384 on epoch=129
05/17/2022 11:40:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.029630 on epoch=133
05/17/2022 11:40:38 - INFO - __main__ - Global step 400 Train loss 0.095967 ACC 0.84375 on epoch=133
05/17/2022 11:40:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.007680 on epoch=136
05/17/2022 11:40:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.016163 on epoch=139
05/17/2022 11:40:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.003941 on epoch=143
05/17/2022 11:40:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.008709 on epoch=146
05/17/2022 11:40:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.066614 on epoch=149
05/17/2022 11:40:53 - INFO - __main__ - Global step 450 Train loss 0.020621 ACC 0.78125 on epoch=149
05/17/2022 11:40:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.020597 on epoch=153
05/17/2022 11:40:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.017862 on epoch=156
05/17/2022 11:41:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.109160 on epoch=159
05/17/2022 11:41:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.023842 on epoch=163
05/17/2022 11:41:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.013471 on epoch=166
05/17/2022 11:41:08 - INFO - __main__ - Global step 500 Train loss 0.036986 ACC 0.78125 on epoch=166
05/17/2022 11:41:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.021919 on epoch=169
05/17/2022 11:41:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.012377 on epoch=173
05/17/2022 11:41:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.005999 on epoch=176
05/17/2022 11:41:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.024776 on epoch=179
05/17/2022 11:41:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000826 on epoch=183
05/17/2022 11:41:21 - INFO - __main__ - Global step 550 Train loss 0.013180 ACC 0.6875 on epoch=183
05/17/2022 11:41:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.006375 on epoch=186
05/17/2022 11:41:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.051830 on epoch=189
05/17/2022 11:41:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000104 on epoch=193
05/17/2022 11:41:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.123402 on epoch=196
05/17/2022 11:41:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.011040 on epoch=199
05/17/2022 11:41:37 - INFO - __main__ - Global step 600 Train loss 0.038550 ACC 0.78125 on epoch=199
05/17/2022 11:41:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.106015 on epoch=203
05/17/2022 11:41:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.058220 on epoch=206
05/17/2022 11:41:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.002688 on epoch=209
05/17/2022 11:41:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.015343 on epoch=213
05/17/2022 11:41:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.005950 on epoch=216
05/17/2022 11:41:52 - INFO - __main__ - Global step 650 Train loss 0.037643 ACC 0.84375 on epoch=216
05/17/2022 11:41:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.020432 on epoch=219
05/17/2022 11:41:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.237836 on epoch=223
05/17/2022 11:42:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.736000 on epoch=226
05/17/2022 11:42:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.505764 on epoch=229
05/17/2022 11:42:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.426469 on epoch=233
05/17/2022 11:42:06 - INFO - __main__ - Global step 700 Train loss 0.385300 ACC 0.75 on epoch=233
05/17/2022 11:42:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.456677 on epoch=236
05/17/2022 11:42:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.410115 on epoch=239
05/17/2022 11:42:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.489161 on epoch=243
05/17/2022 11:42:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.414659 on epoch=246
05/17/2022 11:42:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.147196 on epoch=249
05/17/2022 11:42:21 - INFO - __main__ - Global step 750 Train loss 0.383561 ACC 0.6875 on epoch=249
05/17/2022 11:42:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.233244 on epoch=253
05/17/2022 11:42:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.125958 on epoch=256
05/17/2022 11:42:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.142290 on epoch=259
05/17/2022 11:42:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.157553 on epoch=263
05/17/2022 11:42:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.220861 on epoch=266
05/17/2022 11:42:35 - INFO - __main__ - Global step 800 Train loss 0.175981 ACC 0.625 on epoch=266
05/17/2022 11:42:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.329255 on epoch=269
05/17/2022 11:42:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.313462 on epoch=273
05/17/2022 11:42:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.186821 on epoch=276
05/17/2022 11:42:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.194221 on epoch=279
05/17/2022 11:42:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.163464 on epoch=283
05/17/2022 11:42:49 - INFO - __main__ - Global step 850 Train loss 0.237445 ACC 0.78125 on epoch=283
05/17/2022 11:42:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.134229 on epoch=286
05/17/2022 11:42:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.225519 on epoch=289
05/17/2022 11:42:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.166804 on epoch=293
05/17/2022 11:43:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.162989 on epoch=296
05/17/2022 11:43:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.146883 on epoch=299
05/17/2022 11:43:03 - INFO - __main__ - Global step 900 Train loss 0.167285 ACC 0.6875 on epoch=299
05/17/2022 11:43:03 - INFO - __main__ - save last model!
05/17/2022 11:43:04 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:43:04 - INFO - __main__ - Printing 3 examples
05/17/2022 11:43:04 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/17/2022 11:43:04 - INFO - __main__ - ['contradiction']
05/17/2022 11:43:04 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/17/2022 11:43:04 - INFO - __main__ - ['contradiction']
05/17/2022 11:43:04 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/17/2022 11:43:04 - INFO - __main__ - ['contradiction']
05/17/2022 11:43:04 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:43:04 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:43:04 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:43:04 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:43:04 - INFO - __main__ - Printing 3 examples
05/17/2022 11:43:04 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
05/17/2022 11:43:04 - INFO - __main__ - ['contradiction']
05/17/2022 11:43:04 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
05/17/2022 11:43:04 - INFO - __main__ - ['contradiction']
05/17/2022 11:43:04 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
05/17/2022 11:43:04 - INFO - __main__ - ['contradiction']
05/17/2022 11:43:04 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:43:04 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:43:04 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:43:07 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 11:43:07 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 11:43:07 - INFO - __main__ - Printing 3 examples
05/17/2022 11:43:07 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 11:43:07 - INFO - __main__ - ['contradiction']
05/17/2022 11:43:07 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 11:43:07 - INFO - __main__ - ['neutral']
05/17/2022 11:43:07 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 11:43:07 - INFO - __main__ - ['entailment']
05/17/2022 11:43:07 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:43:07 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:43:07 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 11:43:08 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_42_0.0005_8_predictions.txt
05/17/2022 11:43:08 - INFO - __main__ - ACC on test data: 0.7679
05/17/2022 11:43:08 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.0005, bsz=8, dev_performance=0.84375, test_performance=0.7678571428571429
05/17/2022 11:43:08 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.0003, bsz=8 ...
05/17/2022 11:43:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:43:09 - INFO - __main__ - Starting training!
05/17/2022 11:43:09 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:43:09 - INFO - __main__ - Printing 3 examples
05/17/2022 11:43:09 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/17/2022 11:43:09 - INFO - __main__ - ['contradiction']
05/17/2022 11:43:09 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/17/2022 11:43:09 - INFO - __main__ - ['contradiction']
05/17/2022 11:43:09 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/17/2022 11:43:09 - INFO - __main__ - ['contradiction']
05/17/2022 11:43:09 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:43:09 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:43:09 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:43:09 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:43:09 - INFO - __main__ - Printing 3 examples
05/17/2022 11:43:09 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
05/17/2022 11:43:09 - INFO - __main__ - ['contradiction']
05/17/2022 11:43:09 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
05/17/2022 11:43:09 - INFO - __main__ - ['contradiction']
05/17/2022 11:43:09 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
05/17/2022 11:43:09 - INFO - __main__ - ['contradiction']
05/17/2022 11:43:09 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:43:09 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:43:09 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:43:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:43:14 - INFO - __main__ - Starting training!
05/17/2022 11:43:16 - INFO - __main__ - Step 10 Global step 10 Train loss 17.773434 on epoch=3
05/17/2022 11:43:19 - INFO - __main__ - Step 20 Global step 20 Train loss 15.125708 on epoch=6
05/17/2022 11:43:22 - INFO - __main__ - Step 30 Global step 30 Train loss 11.356887 on epoch=9
05/17/2022 11:43:25 - INFO - __main__ - Step 40 Global step 40 Train loss 10.837031 on epoch=13
05/17/2022 11:43:27 - INFO - __main__ - Step 50 Global step 50 Train loss 9.178959 on epoch=16
05/17/2022 11:43:30 - INFO - __main__ - Global step 50 Train loss 12.854404 ACC 0.0 on epoch=16
05/17/2022 11:43:33 - INFO - __main__ - Step 60 Global step 60 Train loss 7.702201 on epoch=19
05/17/2022 11:43:36 - INFO - __main__ - Step 70 Global step 70 Train loss 6.758745 on epoch=23
05/17/2022 11:43:39 - INFO - __main__ - Step 80 Global step 80 Train loss 6.598228 on epoch=26
05/17/2022 11:43:41 - INFO - __main__ - Step 90 Global step 90 Train loss 4.956061 on epoch=29
05/17/2022 11:43:44 - INFO - __main__ - Step 100 Global step 100 Train loss 5.351860 on epoch=33
05/17/2022 11:43:45 - INFO - __main__ - Global step 100 Train loss 6.273419 ACC 0.0 on epoch=33
05/17/2022 11:43:48 - INFO - __main__ - Step 110 Global step 110 Train loss 5.025592 on epoch=36
05/17/2022 11:43:51 - INFO - __main__ - Step 120 Global step 120 Train loss 3.349128 on epoch=39
05/17/2022 11:43:53 - INFO - __main__ - Step 130 Global step 130 Train loss 2.372939 on epoch=43
05/17/2022 11:43:56 - INFO - __main__ - Step 140 Global step 140 Train loss 2.445758 on epoch=46
05/17/2022 11:43:59 - INFO - __main__ - Step 150 Global step 150 Train loss 1.941038 on epoch=49
05/17/2022 11:43:59 - INFO - __main__ - Global step 150 Train loss 3.026891 ACC 0.5 on epoch=49
05/17/2022 11:44:02 - INFO - __main__ - Step 160 Global step 160 Train loss 2.716059 on epoch=53
05/17/2022 11:44:05 - INFO - __main__ - Step 170 Global step 170 Train loss 2.508104 on epoch=56
05/17/2022 11:44:08 - INFO - __main__ - Step 180 Global step 180 Train loss 2.546188 on epoch=59
05/17/2022 11:44:11 - INFO - __main__ - Step 190 Global step 190 Train loss 2.191183 on epoch=63
05/17/2022 11:44:14 - INFO - __main__ - Step 200 Global step 200 Train loss 2.268154 on epoch=66
05/17/2022 11:44:14 - INFO - __main__ - Global step 200 Train loss 2.445938 ACC 0.5 on epoch=66
05/17/2022 11:44:17 - INFO - __main__ - Step 210 Global step 210 Train loss 2.370132 on epoch=69
05/17/2022 11:44:20 - INFO - __main__ - Step 220 Global step 220 Train loss 2.010246 on epoch=73
05/17/2022 11:44:23 - INFO - __main__ - Step 230 Global step 230 Train loss 1.980780 on epoch=76
05/17/2022 11:44:25 - INFO - __main__ - Step 240 Global step 240 Train loss 2.017209 on epoch=79
05/17/2022 11:44:28 - INFO - __main__ - Step 250 Global step 250 Train loss 1.690322 on epoch=83
05/17/2022 11:44:28 - INFO - __main__ - Global step 250 Train loss 2.013738 ACC 0.0 on epoch=83
05/17/2022 11:44:31 - INFO - __main__ - Step 260 Global step 260 Train loss 1.971666 on epoch=86
05/17/2022 11:44:34 - INFO - __main__ - Step 270 Global step 270 Train loss 1.355290 on epoch=89
05/17/2022 11:44:37 - INFO - __main__ - Step 280 Global step 280 Train loss 1.410526 on epoch=93
05/17/2022 11:44:39 - INFO - __main__ - Step 290 Global step 290 Train loss 1.106569 on epoch=96
05/17/2022 11:44:42 - INFO - __main__ - Step 300 Global step 300 Train loss 1.080023 on epoch=99
05/17/2022 11:44:43 - INFO - __main__ - Global step 300 Train loss 1.384815 ACC 0.5 on epoch=99
05/17/2022 11:44:45 - INFO - __main__ - Step 310 Global step 310 Train loss 1.278691 on epoch=103
05/17/2022 11:44:48 - INFO - __main__ - Step 320 Global step 320 Train loss 1.143389 on epoch=106
05/17/2022 11:44:51 - INFO - __main__ - Step 330 Global step 330 Train loss 1.146327 on epoch=109
05/17/2022 11:44:54 - INFO - __main__ - Step 340 Global step 340 Train loss 1.186579 on epoch=113
05/17/2022 11:44:57 - INFO - __main__ - Step 350 Global step 350 Train loss 1.110185 on epoch=116
05/17/2022 11:44:57 - INFO - __main__ - Global step 350 Train loss 1.173034 ACC 0.46875 on epoch=116
05/17/2022 11:45:00 - INFO - __main__ - Step 360 Global step 360 Train loss 1.101812 on epoch=119
05/17/2022 11:45:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.975618 on epoch=123
05/17/2022 11:45:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.875269 on epoch=126
05/17/2022 11:45:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.728876 on epoch=129
05/17/2022 11:45:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.959961 on epoch=133
05/17/2022 11:45:12 - INFO - __main__ - Global step 400 Train loss 0.928307 ACC 0.28125 on epoch=133
05/17/2022 11:45:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.788723 on epoch=136
05/17/2022 11:45:17 - INFO - __main__ - Step 420 Global step 420 Train loss 1.080578 on epoch=139
05/17/2022 11:45:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.837776 on epoch=143
05/17/2022 11:45:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.874462 on epoch=146
05/17/2022 11:45:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.902393 on epoch=149
05/17/2022 11:45:26 - INFO - __main__ - Global step 450 Train loss 0.896787 ACC 0.3125 on epoch=149
05/17/2022 11:45:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.900317 on epoch=153
05/17/2022 11:45:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.810983 on epoch=156
05/17/2022 11:45:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.839131 on epoch=159
05/17/2022 11:45:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.924511 on epoch=163
05/17/2022 11:45:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.834821 on epoch=166
05/17/2022 11:45:41 - INFO - __main__ - Global step 500 Train loss 0.861953 ACC 0.40625 on epoch=166
05/17/2022 11:45:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.823371 on epoch=169
05/17/2022 11:45:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.725379 on epoch=173
05/17/2022 11:45:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.857483 on epoch=176
05/17/2022 11:45:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.726878 on epoch=179
05/17/2022 11:45:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.785744 on epoch=183
05/17/2022 11:45:56 - INFO - __main__ - Global step 550 Train loss 0.783771 ACC 0.4375 on epoch=183
05/17/2022 11:45:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.800441 on epoch=186
05/17/2022 11:46:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.819250 on epoch=189
05/17/2022 11:46:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.527374 on epoch=193
05/17/2022 11:46:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.758991 on epoch=196
05/17/2022 11:46:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.584957 on epoch=199
05/17/2022 11:46:10 - INFO - __main__ - Global step 600 Train loss 0.698202 ACC 0.5 on epoch=199
05/17/2022 11:46:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.714062 on epoch=203
05/17/2022 11:46:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.631793 on epoch=206
05/17/2022 11:46:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.817930 on epoch=209
05/17/2022 11:46:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.549007 on epoch=213
05/17/2022 11:46:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.573437 on epoch=216
05/17/2022 11:46:24 - INFO - __main__ - Global step 650 Train loss 0.657246 ACC 0.125 on epoch=216
05/17/2022 11:46:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.586949 on epoch=219
05/17/2022 11:46:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.663421 on epoch=223
05/17/2022 11:46:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.587173 on epoch=226
05/17/2022 11:46:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.538288 on epoch=229
05/17/2022 11:46:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.522650 on epoch=233
05/17/2022 11:46:38 - INFO - __main__ - Global step 700 Train loss 0.579696 ACC 0.1875 on epoch=233
05/17/2022 11:46:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.611272 on epoch=236
05/17/2022 11:46:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.532603 on epoch=239
05/17/2022 11:46:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.491656 on epoch=243
05/17/2022 11:46:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.487894 on epoch=246
05/17/2022 11:46:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.523151 on epoch=249
05/17/2022 11:46:52 - INFO - __main__ - Global step 750 Train loss 0.529315 ACC 0.53125 on epoch=249
05/17/2022 11:46:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.559882 on epoch=253
05/17/2022 11:46:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.550031 on epoch=256
05/17/2022 11:47:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.517553 on epoch=259
05/17/2022 11:47:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.613236 on epoch=263
05/17/2022 11:47:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.464225 on epoch=266
05/17/2022 11:47:06 - INFO - __main__ - Global step 800 Train loss 0.540985 ACC 0.46875 on epoch=266
05/17/2022 11:47:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.437743 on epoch=269
05/17/2022 11:47:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.448608 on epoch=273
05/17/2022 11:47:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.438828 on epoch=276
05/17/2022 11:47:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.434482 on epoch=279
05/17/2022 11:47:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.475346 on epoch=283
05/17/2022 11:47:21 - INFO - __main__ - Global step 850 Train loss 0.447001 ACC 0.40625 on epoch=283
05/17/2022 11:47:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.433249 on epoch=286
05/17/2022 11:47:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.460739 on epoch=289
05/17/2022 11:47:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.472686 on epoch=293
05/17/2022 11:47:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.397585 on epoch=296
05/17/2022 11:47:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.389357 on epoch=299
05/17/2022 11:47:35 - INFO - __main__ - Global step 900 Train loss 0.430723 ACC 0.375 on epoch=299
05/17/2022 11:47:35 - INFO - __main__ - save last model!
05/17/2022 11:47:36 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:47:36 - INFO - __main__ - Printing 3 examples
05/17/2022 11:47:36 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/17/2022 11:47:36 - INFO - __main__ - ['contradiction']
05/17/2022 11:47:36 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/17/2022 11:47:36 - INFO - __main__ - ['contradiction']
05/17/2022 11:47:36 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/17/2022 11:47:36 - INFO - __main__ - ['contradiction']
05/17/2022 11:47:36 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:47:36 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:47:36 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:47:36 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:47:36 - INFO - __main__ - Printing 3 examples
05/17/2022 11:47:36 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
05/17/2022 11:47:36 - INFO - __main__ - ['contradiction']
05/17/2022 11:47:36 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
05/17/2022 11:47:36 - INFO - __main__ - ['contradiction']
05/17/2022 11:47:36 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
05/17/2022 11:47:36 - INFO - __main__ - ['contradiction']
05/17/2022 11:47:36 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:47:36 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:47:36 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:47:38 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 11:47:39 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 11:47:39 - INFO - __main__ - Printing 3 examples
05/17/2022 11:47:39 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 11:47:39 - INFO - __main__ - ['contradiction']
05/17/2022 11:47:39 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 11:47:39 - INFO - __main__ - ['neutral']
05/17/2022 11:47:39 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 11:47:39 - INFO - __main__ - ['entailment']
05/17/2022 11:47:39 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:47:39 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:47:39 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 11:47:40 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_42_0.0003_8_predictions.txt
05/17/2022 11:47:40 - INFO - __main__ - ACC on test data: 0.3750
05/17/2022 11:47:40 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.0003, bsz=8, dev_performance=0.53125, test_performance=0.375
05/17/2022 11:47:40 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.0002, bsz=8 ...
05/17/2022 11:47:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:47:41 - INFO - __main__ - Starting training!
05/17/2022 11:47:41 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:47:41 - INFO - __main__ - Printing 3 examples
05/17/2022 11:47:41 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/17/2022 11:47:41 - INFO - __main__ - ['contradiction']
05/17/2022 11:47:41 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/17/2022 11:47:41 - INFO - __main__ - ['contradiction']
05/17/2022 11:47:41 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/17/2022 11:47:41 - INFO - __main__ - ['contradiction']
05/17/2022 11:47:41 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:47:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:47:41 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:47:41 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:47:41 - INFO - __main__ - Printing 3 examples
05/17/2022 11:47:41 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
05/17/2022 11:47:41 - INFO - __main__ - ['contradiction']
05/17/2022 11:47:41 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
05/17/2022 11:47:41 - INFO - __main__ - ['contradiction']
05/17/2022 11:47:41 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
05/17/2022 11:47:41 - INFO - __main__ - ['contradiction']
05/17/2022 11:47:41 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:47:41 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:47:41 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:47:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:47:45 - INFO - __main__ - Starting training!
05/17/2022 11:47:48 - INFO - __main__ - Step 10 Global step 10 Train loss 17.822971 on epoch=3
05/17/2022 11:47:51 - INFO - __main__ - Step 20 Global step 20 Train loss 15.827909 on epoch=6
05/17/2022 11:47:54 - INFO - __main__ - Step 30 Global step 30 Train loss 12.698389 on epoch=9
05/17/2022 11:47:57 - INFO - __main__ - Step 40 Global step 40 Train loss 9.340483 on epoch=13
05/17/2022 11:48:00 - INFO - __main__ - Step 50 Global step 50 Train loss 7.629359 on epoch=16
05/17/2022 11:48:01 - INFO - __main__ - Global step 50 Train loss 12.663822 ACC 0.0 on epoch=16
05/17/2022 11:48:04 - INFO - __main__ - Step 60 Global step 60 Train loss 7.961833 on epoch=19
05/17/2022 11:48:07 - INFO - __main__ - Step 70 Global step 70 Train loss 6.015177 on epoch=23
05/17/2022 11:48:10 - INFO - __main__ - Step 80 Global step 80 Train loss 5.149039 on epoch=26
05/17/2022 11:48:13 - INFO - __main__ - Step 90 Global step 90 Train loss 4.820226 on epoch=29
05/17/2022 11:48:15 - INFO - __main__ - Step 100 Global step 100 Train loss 3.933856 on epoch=33
05/17/2022 11:48:16 - INFO - __main__ - Global step 100 Train loss 5.576026 ACC 0.0 on epoch=33
05/17/2022 11:48:19 - INFO - __main__ - Step 110 Global step 110 Train loss 4.079686 on epoch=36
05/17/2022 11:48:21 - INFO - __main__ - Step 120 Global step 120 Train loss 3.873788 on epoch=39
05/17/2022 11:48:24 - INFO - __main__ - Step 130 Global step 130 Train loss 2.670695 on epoch=43
05/17/2022 11:48:27 - INFO - __main__ - Step 140 Global step 140 Train loss 2.192530 on epoch=46
05/17/2022 11:48:30 - INFO - __main__ - Step 150 Global step 150 Train loss 2.839796 on epoch=49
05/17/2022 11:48:30 - INFO - __main__ - Global step 150 Train loss 3.131299 ACC 0.375 on epoch=49
05/17/2022 11:48:33 - INFO - __main__ - Step 160 Global step 160 Train loss 2.189899 on epoch=53
05/17/2022 11:48:36 - INFO - __main__ - Step 170 Global step 170 Train loss 2.159996 on epoch=56
05/17/2022 11:48:39 - INFO - __main__ - Step 180 Global step 180 Train loss 2.041858 on epoch=59
05/17/2022 11:48:42 - INFO - __main__ - Step 190 Global step 190 Train loss 2.098958 on epoch=63
05/17/2022 11:48:45 - INFO - __main__ - Step 200 Global step 200 Train loss 1.698630 on epoch=66
05/17/2022 11:48:45 - INFO - __main__ - Global step 200 Train loss 2.037868 ACC 0.34375 on epoch=66
05/17/2022 11:48:48 - INFO - __main__ - Step 210 Global step 210 Train loss 1.567174 on epoch=69
05/17/2022 11:48:51 - INFO - __main__ - Step 220 Global step 220 Train loss 1.410494 on epoch=73
05/17/2022 11:48:53 - INFO - __main__ - Step 230 Global step 230 Train loss 1.672249 on epoch=76
05/17/2022 11:48:56 - INFO - __main__ - Step 240 Global step 240 Train loss 1.994810 on epoch=79
05/17/2022 11:48:59 - INFO - __main__ - Step 250 Global step 250 Train loss 1.065915 on epoch=83
05/17/2022 11:48:59 - INFO - __main__ - Global step 250 Train loss 1.542128 ACC 0.5 on epoch=83
05/17/2022 11:49:03 - INFO - __main__ - Step 260 Global step 260 Train loss 1.007325 on epoch=86
05/17/2022 11:49:05 - INFO - __main__ - Step 270 Global step 270 Train loss 1.063102 on epoch=89
05/17/2022 11:49:08 - INFO - __main__ - Step 280 Global step 280 Train loss 1.121963 on epoch=93
05/17/2022 11:49:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.831799 on epoch=96
05/17/2022 11:49:14 - INFO - __main__ - Step 300 Global step 300 Train loss 1.190945 on epoch=99
05/17/2022 11:49:14 - INFO - __main__ - Global step 300 Train loss 1.043027 ACC 0.6875 on epoch=99
05/17/2022 11:49:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.920523 on epoch=103
05/17/2022 11:49:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.869141 on epoch=106
05/17/2022 11:49:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.783171 on epoch=109
05/17/2022 11:49:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.941026 on epoch=113
05/17/2022 11:49:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.702215 on epoch=116
05/17/2022 11:49:29 - INFO - __main__ - Global step 350 Train loss 0.843215 ACC 0.6875 on epoch=116
05/17/2022 11:49:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.746539 on epoch=119
05/17/2022 11:49:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.591950 on epoch=123
05/17/2022 11:49:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.715868 on epoch=126
05/17/2022 11:49:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.531398 on epoch=129
05/17/2022 11:49:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.606562 on epoch=133
05/17/2022 11:49:45 - INFO - __main__ - Global step 400 Train loss 0.638463 ACC 0.65625 on epoch=133
05/17/2022 11:49:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.337546 on epoch=136
05/17/2022 11:49:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.428367 on epoch=139
05/17/2022 11:49:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.337461 on epoch=143
05/17/2022 11:49:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.509133 on epoch=146
05/17/2022 11:50:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.411782 on epoch=149
05/17/2022 11:50:00 - INFO - __main__ - Global step 450 Train loss 0.404858 ACC 0.6875 on epoch=149
05/17/2022 11:50:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.287572 on epoch=153
05/17/2022 11:50:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.422125 on epoch=156
05/17/2022 11:50:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.348877 on epoch=159
05/17/2022 11:50:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.335240 on epoch=163
05/17/2022 11:50:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.276451 on epoch=166
05/17/2022 11:50:15 - INFO - __main__ - Global step 500 Train loss 0.334053 ACC 0.6875 on epoch=166
05/17/2022 11:50:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.376734 on epoch=169
05/17/2022 11:50:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.258781 on epoch=173
05/17/2022 11:50:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.230278 on epoch=176
05/17/2022 11:50:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.202366 on epoch=179
05/17/2022 11:50:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.439448 on epoch=183
05/17/2022 11:50:30 - INFO - __main__ - Global step 550 Train loss 0.301521 ACC 0.6875 on epoch=183
05/17/2022 11:50:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.266432 on epoch=186
05/17/2022 11:50:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.271576 on epoch=189
05/17/2022 11:50:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.128100 on epoch=193
05/17/2022 11:50:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.184028 on epoch=196
05/17/2022 11:50:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.096998 on epoch=199
05/17/2022 11:50:44 - INFO - __main__ - Global step 600 Train loss 0.189427 ACC 0.71875 on epoch=199
05/17/2022 11:50:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.375620 on epoch=203
05/17/2022 11:50:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.191040 on epoch=206
05/17/2022 11:50:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.316554 on epoch=209
05/17/2022 11:50:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.289593 on epoch=213
05/17/2022 11:50:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.341142 on epoch=216
05/17/2022 11:50:59 - INFO - __main__ - Global step 650 Train loss 0.302790 ACC 0.625 on epoch=216
05/17/2022 11:51:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.279704 on epoch=219
05/17/2022 11:51:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.265901 on epoch=223
05/17/2022 11:51:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.484018 on epoch=226
05/17/2022 11:51:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.418516 on epoch=229
05/17/2022 11:51:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.439068 on epoch=233
05/17/2022 11:51:13 - INFO - __main__ - Global step 700 Train loss 0.377441 ACC 0.71875 on epoch=233
05/17/2022 11:51:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.375345 on epoch=236
05/17/2022 11:51:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.525486 on epoch=239
05/17/2022 11:51:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.466167 on epoch=243
05/17/2022 11:51:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.455201 on epoch=246
05/17/2022 11:51:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.407239 on epoch=249
05/17/2022 11:51:28 - INFO - __main__ - Global step 750 Train loss 0.445888 ACC 0.5 on epoch=249
05/17/2022 11:51:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.414408 on epoch=253
05/17/2022 11:51:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.348847 on epoch=256
05/17/2022 11:51:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.367788 on epoch=259
05/17/2022 11:51:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.610768 on epoch=263
05/17/2022 11:51:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.635928 on epoch=266
05/17/2022 11:51:42 - INFO - __main__ - Global step 800 Train loss 0.475548 ACC 0.40625 on epoch=266
05/17/2022 11:51:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.563589 on epoch=269
05/17/2022 11:51:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.487217 on epoch=273
05/17/2022 11:51:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.442549 on epoch=276
05/17/2022 11:51:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.573641 on epoch=279
05/17/2022 11:51:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.365316 on epoch=283
05/17/2022 11:51:57 - INFO - __main__ - Global step 850 Train loss 0.486462 ACC 0.34375 on epoch=283
05/17/2022 11:51:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.474732 on epoch=286
05/17/2022 11:52:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.649796 on epoch=289
05/17/2022 11:52:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.365476 on epoch=293
05/17/2022 11:52:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.386713 on epoch=296
05/17/2022 11:52:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.365559 on epoch=299
05/17/2022 11:52:11 - INFO - __main__ - Global step 900 Train loss 0.448455 ACC 0.5625 on epoch=299
05/17/2022 11:52:11 - INFO - __main__ - save last model!
05/17/2022 11:52:12 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:52:12 - INFO - __main__ - Printing 3 examples
05/17/2022 11:52:12 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/17/2022 11:52:12 - INFO - __main__ - ['contradiction']
05/17/2022 11:52:12 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/17/2022 11:52:12 - INFO - __main__ - ['contradiction']
05/17/2022 11:52:12 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/17/2022 11:52:12 - INFO - __main__ - ['contradiction']
05/17/2022 11:52:12 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:52:12 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:52:12 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:52:12 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:52:12 - INFO - __main__ - Printing 3 examples
05/17/2022 11:52:12 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
05/17/2022 11:52:12 - INFO - __main__ - ['contradiction']
05/17/2022 11:52:12 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
05/17/2022 11:52:12 - INFO - __main__ - ['contradiction']
05/17/2022 11:52:12 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
05/17/2022 11:52:12 - INFO - __main__ - ['contradiction']
05/17/2022 11:52:12 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:52:12 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:52:12 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:52:14 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 11:52:14 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 11:52:14 - INFO - __main__ - Printing 3 examples
05/17/2022 11:52:14 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 11:52:14 - INFO - __main__ - ['contradiction']
05/17/2022 11:52:14 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 11:52:14 - INFO - __main__ - ['neutral']
05/17/2022 11:52:14 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 11:52:14 - INFO - __main__ - ['entailment']
05/17/2022 11:52:14 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:52:14 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:52:14 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 11:52:15 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_42_0.0002_8_predictions.txt
05/17/2022 11:52:15 - INFO - __main__ - ACC on test data: 0.6964
05/17/2022 11:52:15 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.0002, bsz=8, dev_performance=0.71875, test_performance=0.6964285714285714
05/17/2022 11:52:15 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.0001, bsz=8 ...
05/17/2022 11:52:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:52:16 - INFO - __main__ - Starting training!
05/17/2022 11:52:16 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:52:16 - INFO - __main__ - Printing 3 examples
05/17/2022 11:52:16 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/17/2022 11:52:16 - INFO - __main__ - ['contradiction']
05/17/2022 11:52:16 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/17/2022 11:52:16 - INFO - __main__ - ['contradiction']
05/17/2022 11:52:16 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/17/2022 11:52:16 - INFO - __main__ - ['contradiction']
05/17/2022 11:52:16 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:52:16 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:52:16 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:52:16 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:52:16 - INFO - __main__ - Printing 3 examples
05/17/2022 11:52:16 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
05/17/2022 11:52:16 - INFO - __main__ - ['contradiction']
05/17/2022 11:52:16 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
05/17/2022 11:52:16 - INFO - __main__ - ['contradiction']
05/17/2022 11:52:16 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
05/17/2022 11:52:16 - INFO - __main__ - ['contradiction']
05/17/2022 11:52:16 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:52:16 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:52:16 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:52:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:52:20 - INFO - __main__ - Starting training!
05/17/2022 11:52:23 - INFO - __main__ - Step 10 Global step 10 Train loss 18.828011 on epoch=3
05/17/2022 11:52:25 - INFO - __main__ - Step 20 Global step 20 Train loss 17.718145 on epoch=6
05/17/2022 11:52:28 - INFO - __main__ - Step 30 Global step 30 Train loss 16.765324 on epoch=9
05/17/2022 11:52:31 - INFO - __main__ - Step 40 Global step 40 Train loss 12.981749 on epoch=13
05/17/2022 11:52:33 - INFO - __main__ - Step 50 Global step 50 Train loss 12.152166 on epoch=16
05/17/2022 11:52:39 - INFO - __main__ - Global step 50 Train loss 15.689079 ACC 0.0 on epoch=16
05/17/2022 11:52:42 - INFO - __main__ - Step 60 Global step 60 Train loss 10.552298 on epoch=19
05/17/2022 11:52:45 - INFO - __main__ - Step 70 Global step 70 Train loss 9.748098 on epoch=23
05/17/2022 11:52:48 - INFO - __main__ - Step 80 Global step 80 Train loss 8.772783 on epoch=26
05/17/2022 11:52:51 - INFO - __main__ - Step 90 Global step 90 Train loss 9.432001 on epoch=29
05/17/2022 11:52:53 - INFO - __main__ - Step 100 Global step 100 Train loss 7.822571 on epoch=33
05/17/2022 11:52:56 - INFO - __main__ - Global step 100 Train loss 9.265551 ACC 0.0 on epoch=33
05/17/2022 11:52:59 - INFO - __main__ - Step 110 Global step 110 Train loss 7.350550 on epoch=36
05/17/2022 11:53:02 - INFO - __main__ - Step 120 Global step 120 Train loss 7.013356 on epoch=39
05/17/2022 11:53:05 - INFO - __main__ - Step 130 Global step 130 Train loss 6.616500 on epoch=43
05/17/2022 11:53:07 - INFO - __main__ - Step 140 Global step 140 Train loss 7.375540 on epoch=46
05/17/2022 11:53:10 - INFO - __main__ - Step 150 Global step 150 Train loss 5.711111 on epoch=49
05/17/2022 11:53:12 - INFO - __main__ - Global step 150 Train loss 6.813411 ACC 0.03125 on epoch=49
05/17/2022 11:53:15 - INFO - __main__ - Step 160 Global step 160 Train loss 6.706889 on epoch=53
05/17/2022 11:53:18 - INFO - __main__ - Step 170 Global step 170 Train loss 6.253606 on epoch=56
05/17/2022 11:53:21 - INFO - __main__ - Step 180 Global step 180 Train loss 5.644779 on epoch=59
05/17/2022 11:53:23 - INFO - __main__ - Step 190 Global step 190 Train loss 6.101637 on epoch=63
05/17/2022 11:53:26 - INFO - __main__ - Step 200 Global step 200 Train loss 4.670512 on epoch=66
05/17/2022 11:53:27 - INFO - __main__ - Global step 200 Train loss 5.875484 ACC 0.0 on epoch=66
05/17/2022 11:53:30 - INFO - __main__ - Step 210 Global step 210 Train loss 4.500087 on epoch=69
05/17/2022 11:53:33 - INFO - __main__ - Step 220 Global step 220 Train loss 4.471770 on epoch=73
05/17/2022 11:53:35 - INFO - __main__ - Step 230 Global step 230 Train loss 3.598917 on epoch=76
05/17/2022 11:53:38 - INFO - __main__ - Step 240 Global step 240 Train loss 5.126571 on epoch=79
05/17/2022 11:53:41 - INFO - __main__ - Step 250 Global step 250 Train loss 4.364676 on epoch=83
05/17/2022 11:53:42 - INFO - __main__ - Global step 250 Train loss 4.412404 ACC 0.0 on epoch=83
05/17/2022 11:53:45 - INFO - __main__ - Step 260 Global step 260 Train loss 4.027638 on epoch=86
05/17/2022 11:53:47 - INFO - __main__ - Step 270 Global step 270 Train loss 4.025723 on epoch=89
05/17/2022 11:53:50 - INFO - __main__ - Step 280 Global step 280 Train loss 3.370914 on epoch=93
05/17/2022 11:53:53 - INFO - __main__ - Step 290 Global step 290 Train loss 3.602575 on epoch=96
05/17/2022 11:53:55 - INFO - __main__ - Step 300 Global step 300 Train loss 2.482873 on epoch=99
05/17/2022 11:53:56 - INFO - __main__ - Global step 300 Train loss 3.501945 ACC 0.03125 on epoch=99
05/17/2022 11:53:58 - INFO - __main__ - Step 310 Global step 310 Train loss 2.895839 on epoch=103
05/17/2022 11:54:01 - INFO - __main__ - Step 320 Global step 320 Train loss 1.986842 on epoch=106
05/17/2022 11:54:04 - INFO - __main__ - Step 330 Global step 330 Train loss 2.783939 on epoch=109
05/17/2022 11:54:07 - INFO - __main__ - Step 340 Global step 340 Train loss 2.750453 on epoch=113
05/17/2022 11:54:10 - INFO - __main__ - Step 350 Global step 350 Train loss 3.073442 on epoch=116
05/17/2022 11:54:10 - INFO - __main__ - Global step 350 Train loss 2.698103 ACC 0.4375 on epoch=116
05/17/2022 11:54:13 - INFO - __main__ - Step 360 Global step 360 Train loss 2.796826 on epoch=119
05/17/2022 11:54:17 - INFO - __main__ - Step 370 Global step 370 Train loss 2.332779 on epoch=123
05/17/2022 11:54:20 - INFO - __main__ - Step 380 Global step 380 Train loss 2.789720 on epoch=126
05/17/2022 11:54:22 - INFO - __main__ - Step 390 Global step 390 Train loss 2.118664 on epoch=129
05/17/2022 11:54:25 - INFO - __main__ - Step 400 Global step 400 Train loss 2.175478 on epoch=133
05/17/2022 11:54:25 - INFO - __main__ - Global step 400 Train loss 2.442693 ACC 0.46875 on epoch=133
05/17/2022 11:54:28 - INFO - __main__ - Step 410 Global step 410 Train loss 2.106241 on epoch=136
05/17/2022 11:54:31 - INFO - __main__ - Step 420 Global step 420 Train loss 2.285949 on epoch=139
05/17/2022 11:54:34 - INFO - __main__ - Step 430 Global step 430 Train loss 2.682654 on epoch=143
05/17/2022 11:54:37 - INFO - __main__ - Step 440 Global step 440 Train loss 2.454930 on epoch=146
05/17/2022 11:54:39 - INFO - __main__ - Step 450 Global step 450 Train loss 2.665346 on epoch=149
05/17/2022 11:54:40 - INFO - __main__ - Global step 450 Train loss 2.439024 ACC 0.46875 on epoch=149
05/17/2022 11:54:42 - INFO - __main__ - Step 460 Global step 460 Train loss 2.433584 on epoch=153
05/17/2022 11:54:45 - INFO - __main__ - Step 470 Global step 470 Train loss 1.946449 on epoch=156
05/17/2022 11:54:48 - INFO - __main__ - Step 480 Global step 480 Train loss 1.407893 on epoch=159
05/17/2022 11:54:51 - INFO - __main__ - Step 490 Global step 490 Train loss 2.280381 on epoch=163
05/17/2022 11:54:53 - INFO - __main__ - Step 500 Global step 500 Train loss 1.683374 on epoch=166
05/17/2022 11:54:54 - INFO - __main__ - Global step 500 Train loss 1.950336 ACC 0.46875 on epoch=166
05/17/2022 11:54:56 - INFO - __main__ - Step 510 Global step 510 Train loss 1.378986 on epoch=169
05/17/2022 11:54:59 - INFO - __main__ - Step 520 Global step 520 Train loss 1.963835 on epoch=173
05/17/2022 11:55:02 - INFO - __main__ - Step 530 Global step 530 Train loss 2.119918 on epoch=176
05/17/2022 11:55:05 - INFO - __main__ - Step 540 Global step 540 Train loss 1.876404 on epoch=179
05/17/2022 11:55:08 - INFO - __main__ - Step 550 Global step 550 Train loss 1.687752 on epoch=183
05/17/2022 11:55:08 - INFO - __main__ - Global step 550 Train loss 1.805379 ACC 0.40625 on epoch=183
05/17/2022 11:55:11 - INFO - __main__ - Step 560 Global step 560 Train loss 2.180130 on epoch=186
05/17/2022 11:55:14 - INFO - __main__ - Step 570 Global step 570 Train loss 2.224423 on epoch=189
05/17/2022 11:55:17 - INFO - __main__ - Step 580 Global step 580 Train loss 1.410357 on epoch=193
05/17/2022 11:55:20 - INFO - __main__ - Step 590 Global step 590 Train loss 1.609993 on epoch=196
05/17/2022 11:55:23 - INFO - __main__ - Step 600 Global step 600 Train loss 1.960648 on epoch=199
05/17/2022 11:55:23 - INFO - __main__ - Global step 600 Train loss 1.877110 ACC 0.4375 on epoch=199
05/17/2022 11:55:26 - INFO - __main__ - Step 610 Global step 610 Train loss 1.468540 on epoch=203
05/17/2022 11:55:29 - INFO - __main__ - Step 620 Global step 620 Train loss 1.821492 on epoch=206
05/17/2022 11:55:32 - INFO - __main__ - Step 630 Global step 630 Train loss 1.564231 on epoch=209
05/17/2022 11:55:35 - INFO - __main__ - Step 640 Global step 640 Train loss 1.884744 on epoch=213
05/17/2022 11:55:38 - INFO - __main__ - Step 650 Global step 650 Train loss 1.558496 on epoch=216
05/17/2022 11:55:38 - INFO - __main__ - Global step 650 Train loss 1.659501 ACC 0.4375 on epoch=216
05/17/2022 11:55:41 - INFO - __main__ - Step 660 Global step 660 Train loss 1.614911 on epoch=219
05/17/2022 11:55:44 - INFO - __main__ - Step 670 Global step 670 Train loss 1.558990 on epoch=223
05/17/2022 11:55:47 - INFO - __main__ - Step 680 Global step 680 Train loss 1.602239 on epoch=226
05/17/2022 11:55:50 - INFO - __main__ - Step 690 Global step 690 Train loss 1.496449 on epoch=229
05/17/2022 11:55:53 - INFO - __main__ - Step 700 Global step 700 Train loss 1.271463 on epoch=233
05/17/2022 11:55:53 - INFO - __main__ - Global step 700 Train loss 1.508810 ACC 0.4375 on epoch=233
05/17/2022 11:55:56 - INFO - __main__ - Step 710 Global step 710 Train loss 1.388196 on epoch=236
05/17/2022 11:55:59 - INFO - __main__ - Step 720 Global step 720 Train loss 1.493272 on epoch=239
05/17/2022 11:56:02 - INFO - __main__ - Step 730 Global step 730 Train loss 1.101797 on epoch=243
05/17/2022 11:56:05 - INFO - __main__ - Step 740 Global step 740 Train loss 1.504632 on epoch=246
05/17/2022 11:56:08 - INFO - __main__ - Step 750 Global step 750 Train loss 1.450984 on epoch=249
05/17/2022 11:56:08 - INFO - __main__ - Global step 750 Train loss 1.387776 ACC 0.46875 on epoch=249
05/17/2022 11:56:11 - INFO - __main__ - Step 760 Global step 760 Train loss 1.195171 on epoch=253
05/17/2022 11:56:14 - INFO - __main__ - Step 770 Global step 770 Train loss 1.106019 on epoch=256
05/17/2022 11:56:17 - INFO - __main__ - Step 780 Global step 780 Train loss 1.137439 on epoch=259
05/17/2022 11:56:19 - INFO - __main__ - Step 790 Global step 790 Train loss 1.414202 on epoch=263
05/17/2022 11:56:22 - INFO - __main__ - Step 800 Global step 800 Train loss 1.316236 on epoch=266
05/17/2022 11:56:22 - INFO - __main__ - Global step 800 Train loss 1.233813 ACC 0.46875 on epoch=266
05/17/2022 11:56:25 - INFO - __main__ - Step 810 Global step 810 Train loss 1.167492 on epoch=269
05/17/2022 11:56:28 - INFO - __main__ - Step 820 Global step 820 Train loss 1.195509 on epoch=273
05/17/2022 11:56:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.831792 on epoch=276
05/17/2022 11:56:33 - INFO - __main__ - Step 840 Global step 840 Train loss 1.042898 on epoch=279
05/17/2022 11:56:36 - INFO - __main__ - Step 850 Global step 850 Train loss 1.224836 on epoch=283
05/17/2022 11:56:37 - INFO - __main__ - Global step 850 Train loss 1.092505 ACC 0.5 on epoch=283
05/17/2022 11:56:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.970930 on epoch=286
05/17/2022 11:56:42 - INFO - __main__ - Step 870 Global step 870 Train loss 1.040211 on epoch=289
05/17/2022 11:56:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.745046 on epoch=293
05/17/2022 11:56:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.751484 on epoch=296
05/17/2022 11:56:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.956074 on epoch=299
05/17/2022 11:56:51 - INFO - __main__ - Global step 900 Train loss 0.892749 ACC 0.6875 on epoch=299
05/17/2022 11:56:51 - INFO - __main__ - save last model!
05/17/2022 11:56:52 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:56:52 - INFO - __main__ - Printing 3 examples
05/17/2022 11:56:52 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/17/2022 11:56:52 - INFO - __main__ - ['contradiction']
05/17/2022 11:56:52 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/17/2022 11:56:52 - INFO - __main__ - ['contradiction']
05/17/2022 11:56:52 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/17/2022 11:56:52 - INFO - __main__ - ['contradiction']
05/17/2022 11:56:52 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:56:52 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:56:52 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:56:52 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:56:52 - INFO - __main__ - Printing 3 examples
05/17/2022 11:56:52 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
05/17/2022 11:56:52 - INFO - __main__ - ['contradiction']
05/17/2022 11:56:52 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
05/17/2022 11:56:52 - INFO - __main__ - ['contradiction']
05/17/2022 11:56:52 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
05/17/2022 11:56:52 - INFO - __main__ - ['contradiction']
05/17/2022 11:56:52 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:56:52 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:56:52 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:56:54 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 11:56:55 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 11:56:55 - INFO - __main__ - Printing 3 examples
05/17/2022 11:56:55 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 11:56:55 - INFO - __main__ - ['contradiction']
05/17/2022 11:56:55 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 11:56:55 - INFO - __main__ - ['neutral']
05/17/2022 11:56:55 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 11:56:55 - INFO - __main__ - ['entailment']
05/17/2022 11:56:55 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:56:55 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:56:55 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 11:56:56 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_42_0.0001_8_predictions.txt
05/17/2022 11:56:56 - INFO - __main__ - ACC on test data: 0.6964
05/17/2022 11:56:56 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.0001, bsz=8, dev_performance=0.6875, test_performance=0.6964285714285714
05/17/2022 11:56:56 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.0005, bsz=8 ...
05/17/2022 11:56:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:56:56 - INFO - __main__ - Starting training!
05/17/2022 11:56:57 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 11:56:57 - INFO - __main__ - Printing 3 examples
05/17/2022 11:56:57 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/17/2022 11:56:57 - INFO - __main__ - ['contradiction']
05/17/2022 11:56:57 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/17/2022 11:56:57 - INFO - __main__ - ['contradiction']
05/17/2022 11:56:57 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/17/2022 11:56:57 - INFO - __main__ - ['contradiction']
05/17/2022 11:56:57 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:56:57 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:56:57 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 11:56:57 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 11:56:57 - INFO - __main__ - Printing 3 examples
05/17/2022 11:56:57 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
05/17/2022 11:56:57 - INFO - __main__ - ['contradiction']
05/17/2022 11:56:57 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
05/17/2022 11:56:57 - INFO - __main__ - ['contradiction']
05/17/2022 11:56:57 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
05/17/2022 11:56:57 - INFO - __main__ - ['contradiction']
05/17/2022 11:56:57 - INFO - __main__ - Tokenizing Input ...
05/17/2022 11:56:57 - INFO - __main__ - Tokenizing Output ...
05/17/2022 11:56:57 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 11:57:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 11:57:01 - INFO - __main__ - Starting training!
05/17/2022 11:57:03 - INFO - __main__ - Step 10 Global step 10 Train loss 17.483997 on epoch=3
05/17/2022 11:57:06 - INFO - __main__ - Step 20 Global step 20 Train loss 15.317737 on epoch=6
05/17/2022 11:57:09 - INFO - __main__ - Step 30 Global step 30 Train loss 10.703677 on epoch=9
05/17/2022 11:57:11 - INFO - __main__ - Step 40 Global step 40 Train loss 10.171167 on epoch=13
05/17/2022 11:57:14 - INFO - __main__ - Step 50 Global step 50 Train loss 7.369840 on epoch=16
05/17/2022 11:57:15 - INFO - __main__ - Global step 50 Train loss 12.209284 ACC 0.03125 on epoch=16
05/17/2022 11:57:18 - INFO - __main__ - Step 60 Global step 60 Train loss 4.918841 on epoch=19
05/17/2022 11:57:20 - INFO - __main__ - Step 70 Global step 70 Train loss 3.667533 on epoch=23
05/17/2022 11:57:23 - INFO - __main__ - Step 80 Global step 80 Train loss 3.481613 on epoch=26
05/17/2022 11:57:26 - INFO - __main__ - Step 90 Global step 90 Train loss 2.546658 on epoch=29
05/17/2022 11:57:28 - INFO - __main__ - Step 100 Global step 100 Train loss 2.410139 on epoch=33
05/17/2022 11:57:29 - INFO - __main__ - Global step 100 Train loss 3.404957 ACC 0.5 on epoch=33
05/17/2022 11:57:32 - INFO - __main__ - Step 110 Global step 110 Train loss 1.735660 on epoch=36
05/17/2022 11:57:35 - INFO - __main__ - Step 120 Global step 120 Train loss 2.389328 on epoch=39
05/17/2022 11:57:38 - INFO - __main__ - Step 130 Global step 130 Train loss 2.273929 on epoch=43
05/17/2022 11:57:40 - INFO - __main__ - Step 140 Global step 140 Train loss 1.469707 on epoch=46
05/17/2022 11:57:43 - INFO - __main__ - Step 150 Global step 150 Train loss 2.232392 on epoch=49
05/17/2022 11:57:43 - INFO - __main__ - Global step 150 Train loss 2.020203 ACC 0.46875 on epoch=49
05/17/2022 11:57:46 - INFO - __main__ - Step 160 Global step 160 Train loss 2.696791 on epoch=53
05/17/2022 11:57:49 - INFO - __main__ - Step 170 Global step 170 Train loss 1.687750 on epoch=56
05/17/2022 11:57:52 - INFO - __main__ - Step 180 Global step 180 Train loss 2.001987 on epoch=59
05/17/2022 11:57:54 - INFO - __main__ - Step 190 Global step 190 Train loss 1.366884 on epoch=63
05/17/2022 11:57:57 - INFO - __main__ - Step 200 Global step 200 Train loss 1.481909 on epoch=66
05/17/2022 11:57:58 - INFO - __main__ - Global step 200 Train loss 1.847064 ACC 0.0 on epoch=66
05/17/2022 11:58:00 - INFO - __main__ - Step 210 Global step 210 Train loss 1.595973 on epoch=69
05/17/2022 11:58:03 - INFO - __main__ - Step 220 Global step 220 Train loss 1.437305 on epoch=73
05/17/2022 11:58:06 - INFO - __main__ - Step 230 Global step 230 Train loss 1.076501 on epoch=76
05/17/2022 11:58:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.900849 on epoch=79
05/17/2022 11:58:11 - INFO - __main__ - Step 250 Global step 250 Train loss 1.042809 on epoch=83
05/17/2022 11:58:12 - INFO - __main__ - Global step 250 Train loss 1.210687 ACC 0.375 on epoch=83
05/17/2022 11:58:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.990670 on epoch=86
05/17/2022 11:58:17 - INFO - __main__ - Step 270 Global step 270 Train loss 1.234722 on epoch=89
05/17/2022 11:58:20 - INFO - __main__ - Step 280 Global step 280 Train loss 1.005261 on epoch=93
05/17/2022 11:58:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.960984 on epoch=96
05/17/2022 11:58:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.919641 on epoch=99
05/17/2022 11:58:26 - INFO - __main__ - Global step 300 Train loss 1.022256 ACC 0.03125 on epoch=99
05/17/2022 11:58:29 - INFO - __main__ - Step 310 Global step 310 Train loss 1.011630 on epoch=103
05/17/2022 11:58:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.791506 on epoch=106
05/17/2022 11:58:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.960349 on epoch=109
05/17/2022 11:58:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.873569 on epoch=113
05/17/2022 11:58:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.833031 on epoch=116
05/17/2022 11:58:41 - INFO - __main__ - Global step 350 Train loss 0.894017 ACC 0.4375 on epoch=116
05/17/2022 11:58:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.984563 on epoch=119
05/17/2022 11:58:46 - INFO - __main__ - Step 370 Global step 370 Train loss 1.080997 on epoch=123
05/17/2022 11:58:49 - INFO - __main__ - Step 380 Global step 380 Train loss 1.165389 on epoch=126
05/17/2022 11:58:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.799815 on epoch=129
05/17/2022 11:58:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.773706 on epoch=133
05/17/2022 11:58:55 - INFO - __main__ - Global step 400 Train loss 0.960894 ACC 0.28125 on epoch=133
05/17/2022 11:58:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.825979 on epoch=136
05/17/2022 11:59:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.797166 on epoch=139
05/17/2022 11:59:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.925201 on epoch=143
05/17/2022 11:59:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.784326 on epoch=146
05/17/2022 11:59:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.612841 on epoch=149
05/17/2022 11:59:08 - INFO - __main__ - Global step 450 Train loss 0.789103 ACC 0.34375 on epoch=149
05/17/2022 11:59:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.734985 on epoch=153
05/17/2022 11:59:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.564739 on epoch=156
05/17/2022 11:59:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.691833 on epoch=159
05/17/2022 11:59:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.651560 on epoch=163
05/17/2022 11:59:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.680957 on epoch=166
05/17/2022 11:59:22 - INFO - __main__ - Global step 500 Train loss 0.664815 ACC 0.15625 on epoch=166
05/17/2022 11:59:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.824051 on epoch=169
05/17/2022 11:59:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.621668 on epoch=173
05/17/2022 11:59:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.587621 on epoch=176
05/17/2022 11:59:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.678205 on epoch=179
05/17/2022 11:59:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.648067 on epoch=183
05/17/2022 11:59:36 - INFO - __main__ - Global step 550 Train loss 0.671922 ACC 0.59375 on epoch=183
05/17/2022 11:59:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.616368 on epoch=186
05/17/2022 11:59:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.696452 on epoch=189
05/17/2022 11:59:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.579695 on epoch=193
05/17/2022 11:59:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.554407 on epoch=196
05/17/2022 11:59:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.612765 on epoch=199
05/17/2022 11:59:52 - INFO - __main__ - Global step 600 Train loss 0.611937 ACC 0.1875 on epoch=199
05/17/2022 11:59:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.606433 on epoch=203
05/17/2022 11:59:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.508192 on epoch=206
05/17/2022 12:00:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.605494 on epoch=209
05/17/2022 12:00:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.559151 on epoch=213
05/17/2022 12:00:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.702910 on epoch=216
05/17/2022 12:00:07 - INFO - __main__ - Global step 650 Train loss 0.596436 ACC 0.4375 on epoch=216
05/17/2022 12:00:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.619745 on epoch=219
05/17/2022 12:00:12 - INFO - __main__ - Step 670 Global step 670 Train loss 0.488442 on epoch=223
05/17/2022 12:00:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.595245 on epoch=226
05/17/2022 12:00:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.527666 on epoch=229
05/17/2022 12:00:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.502083 on epoch=233
05/17/2022 12:00:21 - INFO - __main__ - Global step 700 Train loss 0.546636 ACC 0.5 on epoch=233
05/17/2022 12:00:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.523483 on epoch=236
05/17/2022 12:00:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.499697 on epoch=239
05/17/2022 12:00:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.574520 on epoch=243
05/17/2022 12:00:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.572960 on epoch=246
05/17/2022 12:00:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.447701 on epoch=249
05/17/2022 12:00:34 - INFO - __main__ - Global step 750 Train loss 0.523672 ACC 0.375 on epoch=249
05/17/2022 12:00:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.441938 on epoch=253
05/17/2022 12:00:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.460398 on epoch=256
05/17/2022 12:00:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.422448 on epoch=259
05/17/2022 12:00:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.425766 on epoch=263
05/17/2022 12:00:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.432223 on epoch=266
05/17/2022 12:00:48 - INFO - __main__ - Global step 800 Train loss 0.436555 ACC 0.375 on epoch=266
05/17/2022 12:00:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.470739 on epoch=269
05/17/2022 12:00:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.404585 on epoch=273
05/17/2022 12:00:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.381578 on epoch=276
05/17/2022 12:00:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.416059 on epoch=279
05/17/2022 12:01:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.387946 on epoch=283
05/17/2022 12:01:02 - INFO - __main__ - Global step 850 Train loss 0.412181 ACC 0.46875 on epoch=283
05/17/2022 12:01:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.424610 on epoch=286
05/17/2022 12:01:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.415812 on epoch=289
05/17/2022 12:01:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.430358 on epoch=293
05/17/2022 12:01:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.431428 on epoch=296
05/17/2022 12:01:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.427368 on epoch=299
05/17/2022 12:01:16 - INFO - __main__ - Global step 900 Train loss 0.425915 ACC 0.25 on epoch=299
05/17/2022 12:01:16 - INFO - __main__ - save last model!
05/17/2022 12:01:17 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 12:01:17 - INFO - __main__ - Printing 3 examples
05/17/2022 12:01:17 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/17/2022 12:01:17 - INFO - __main__ - ['contradiction']
05/17/2022 12:01:17 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/17/2022 12:01:17 - INFO - __main__ - ['contradiction']
05/17/2022 12:01:17 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/17/2022 12:01:17 - INFO - __main__ - ['contradiction']
05/17/2022 12:01:17 - INFO - __main__ - Tokenizing Input ...
05/17/2022 12:01:17 - INFO - __main__ - Tokenizing Output ...
05/17/2022 12:01:17 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 12:01:17 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 12:01:17 - INFO - __main__ - Printing 3 examples
05/17/2022 12:01:17 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
05/17/2022 12:01:17 - INFO - __main__ - ['contradiction']
05/17/2022 12:01:17 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
05/17/2022 12:01:17 - INFO - __main__ - ['contradiction']
05/17/2022 12:01:17 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
05/17/2022 12:01:17 - INFO - __main__ - ['contradiction']
05/17/2022 12:01:17 - INFO - __main__ - Tokenizing Input ...
05/17/2022 12:01:17 - INFO - __main__ - Tokenizing Output ...
05/17/2022 12:01:17 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 12:01:20 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 12:01:20 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 12:01:20 - INFO - __main__ - Printing 3 examples
05/17/2022 12:01:20 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 12:01:20 - INFO - __main__ - ['contradiction']
05/17/2022 12:01:20 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 12:01:20 - INFO - __main__ - ['neutral']
05/17/2022 12:01:20 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 12:01:20 - INFO - __main__ - ['entailment']
05/17/2022 12:01:20 - INFO - __main__ - Tokenizing Input ...
05/17/2022 12:01:20 - INFO - __main__ - Tokenizing Output ...
05/17/2022 12:01:20 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 12:01:22 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_87_0.0005_8_predictions.txt
05/17/2022 12:01:22 - INFO - __main__ - ACC on test data: 0.6250
05/17/2022 12:01:22 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.0005, bsz=8, dev_performance=0.59375, test_performance=0.625
05/17/2022 12:01:22 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.0003, bsz=8 ...
05/17/2022 12:01:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 12:01:22 - INFO - __main__ - Starting training!
05/17/2022 12:01:23 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 12:01:23 - INFO - __main__ - Printing 3 examples
05/17/2022 12:01:23 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/17/2022 12:01:23 - INFO - __main__ - ['contradiction']
05/17/2022 12:01:23 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/17/2022 12:01:23 - INFO - __main__ - ['contradiction']
05/17/2022 12:01:23 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/17/2022 12:01:23 - INFO - __main__ - ['contradiction']
05/17/2022 12:01:23 - INFO - __main__ - Tokenizing Input ...
05/17/2022 12:01:23 - INFO - __main__ - Tokenizing Output ...
05/17/2022 12:01:23 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 12:01:23 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 12:01:23 - INFO - __main__ - Printing 3 examples
05/17/2022 12:01:23 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
05/17/2022 12:01:23 - INFO - __main__ - ['contradiction']
05/17/2022 12:01:23 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
05/17/2022 12:01:23 - INFO - __main__ - ['contradiction']
05/17/2022 12:01:23 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
05/17/2022 12:01:23 - INFO - __main__ - ['contradiction']
05/17/2022 12:01:23 - INFO - __main__ - Tokenizing Input ...
05/17/2022 12:01:23 - INFO - __main__ - Tokenizing Output ...
05/17/2022 12:01:23 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 12:01:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 12:01:27 - INFO - __main__ - Starting training!
05/17/2022 12:01:29 - INFO - __main__ - Step 10 Global step 10 Train loss 18.338139 on epoch=3
05/17/2022 12:01:32 - INFO - __main__ - Step 20 Global step 20 Train loss 14.279963 on epoch=6
05/17/2022 12:01:35 - INFO - __main__ - Step 30 Global step 30 Train loss 9.660048 on epoch=9
05/17/2022 12:01:38 - INFO - __main__ - Step 40 Global step 40 Train loss 7.981909 on epoch=13
05/17/2022 12:01:41 - INFO - __main__ - Step 50 Global step 50 Train loss 6.759780 on epoch=16
05/17/2022 12:01:43 - INFO - __main__ - Global step 50 Train loss 11.403967 ACC 0.0 on epoch=16
05/17/2022 12:01:46 - INFO - __main__ - Step 60 Global step 60 Train loss 4.992792 on epoch=19
05/17/2022 12:01:48 - INFO - __main__ - Step 70 Global step 70 Train loss 5.373463 on epoch=23
05/17/2022 12:01:51 - INFO - __main__ - Step 80 Global step 80 Train loss 3.528971 on epoch=26
05/17/2022 12:01:54 - INFO - __main__ - Step 90 Global step 90 Train loss 3.193351 on epoch=29
05/17/2022 12:01:57 - INFO - __main__ - Step 100 Global step 100 Train loss 2.344924 on epoch=33
05/17/2022 12:01:58 - INFO - __main__ - Global step 100 Train loss 3.886700 ACC 0.0 on epoch=33
05/17/2022 12:02:00 - INFO - __main__ - Step 110 Global step 110 Train loss 3.238068 on epoch=36
05/17/2022 12:02:03 - INFO - __main__ - Step 120 Global step 120 Train loss 2.479419 on epoch=39
05/17/2022 12:02:06 - INFO - __main__ - Step 130 Global step 130 Train loss 2.120551 on epoch=43
05/17/2022 12:02:09 - INFO - __main__ - Step 140 Global step 140 Train loss 2.446624 on epoch=46
05/17/2022 12:02:11 - INFO - __main__ - Step 150 Global step 150 Train loss 2.354990 on epoch=49
05/17/2022 12:02:12 - INFO - __main__ - Global step 150 Train loss 2.527930 ACC 0.0 on epoch=49
05/17/2022 12:02:15 - INFO - __main__ - Step 160 Global step 160 Train loss 2.042262 on epoch=53
05/17/2022 12:02:17 - INFO - __main__ - Step 170 Global step 170 Train loss 1.611120 on epoch=56
05/17/2022 12:02:20 - INFO - __main__ - Step 180 Global step 180 Train loss 2.334049 on epoch=59
05/17/2022 12:02:23 - INFO - __main__ - Step 190 Global step 190 Train loss 1.506557 on epoch=63
05/17/2022 12:02:25 - INFO - __main__ - Step 200 Global step 200 Train loss 1.590665 on epoch=66
05/17/2022 12:02:26 - INFO - __main__ - Global step 200 Train loss 1.816931 ACC 0.03125 on epoch=66
05/17/2022 12:02:29 - INFO - __main__ - Step 210 Global step 210 Train loss 1.316586 on epoch=69
05/17/2022 12:02:32 - INFO - __main__ - Step 220 Global step 220 Train loss 1.462216 on epoch=73
05/17/2022 12:02:35 - INFO - __main__ - Step 230 Global step 230 Train loss 1.255347 on epoch=76
05/17/2022 12:02:37 - INFO - __main__ - Step 240 Global step 240 Train loss 1.297764 on epoch=79
05/17/2022 12:02:40 - INFO - __main__ - Step 250 Global step 250 Train loss 1.020163 on epoch=83
05/17/2022 12:02:41 - INFO - __main__ - Global step 250 Train loss 1.270415 ACC 0.0 on epoch=83
05/17/2022 12:02:44 - INFO - __main__ - Step 260 Global step 260 Train loss 1.098928 on epoch=86
05/17/2022 12:02:46 - INFO - __main__ - Step 270 Global step 270 Train loss 1.197032 on epoch=89
05/17/2022 12:02:49 - INFO - __main__ - Step 280 Global step 280 Train loss 1.131472 on epoch=93
05/17/2022 12:02:52 - INFO - __main__ - Step 290 Global step 290 Train loss 1.083851 on epoch=96
05/17/2022 12:02:55 - INFO - __main__ - Step 300 Global step 300 Train loss 1.174153 on epoch=99
05/17/2022 12:02:55 - INFO - __main__ - Global step 300 Train loss 1.137087 ACC 0.15625 on epoch=99
05/17/2022 12:02:59 - INFO - __main__ - Step 310 Global step 310 Train loss 1.031613 on epoch=103
05/17/2022 12:03:01 - INFO - __main__ - Step 320 Global step 320 Train loss 1.002920 on epoch=106
05/17/2022 12:03:04 - INFO - __main__ - Step 330 Global step 330 Train loss 1.076766 on epoch=109
05/17/2022 12:03:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.979966 on epoch=113
05/17/2022 12:03:09 - INFO - __main__ - Step 350 Global step 350 Train loss 1.228605 on epoch=116
05/17/2022 12:03:10 - INFO - __main__ - Global step 350 Train loss 1.063974 ACC 0.375 on epoch=116
05/17/2022 12:03:13 - INFO - __main__ - Step 360 Global step 360 Train loss 1.100095 on epoch=119
05/17/2022 12:03:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.957588 on epoch=123
05/17/2022 12:03:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.829314 on epoch=126
05/17/2022 12:03:21 - INFO - __main__ - Step 390 Global step 390 Train loss 1.053593 on epoch=129
05/17/2022 12:03:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.693104 on epoch=133
05/17/2022 12:03:25 - INFO - __main__ - Global step 400 Train loss 0.926739 ACC 0.4375 on epoch=133
05/17/2022 12:03:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.707089 on epoch=136
05/17/2022 12:03:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.959442 on epoch=139
05/17/2022 12:03:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.816541 on epoch=143
05/17/2022 12:03:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.840944 on epoch=146
05/17/2022 12:03:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.771823 on epoch=149
05/17/2022 12:03:39 - INFO - __main__ - Global step 450 Train loss 0.819168 ACC 0.34375 on epoch=149
05/17/2022 12:03:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.668201 on epoch=153
05/17/2022 12:03:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.819280 on epoch=156
05/17/2022 12:03:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.643028 on epoch=159
05/17/2022 12:03:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.748256 on epoch=163
05/17/2022 12:03:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.602467 on epoch=166
05/17/2022 12:03:54 - INFO - __main__ - Global step 500 Train loss 0.696246 ACC 0.46875 on epoch=166
05/17/2022 12:03:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.690800 on epoch=169
05/17/2022 12:04:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.600926 on epoch=173
05/17/2022 12:04:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.548086 on epoch=176
05/17/2022 12:04:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.634865 on epoch=179
05/17/2022 12:04:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.553564 on epoch=183
05/17/2022 12:04:08 - INFO - __main__ - Global step 550 Train loss 0.605648 ACC 0.59375 on epoch=183
05/17/2022 12:04:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.691825 on epoch=186
05/17/2022 12:04:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.680828 on epoch=189
05/17/2022 12:04:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.611212 on epoch=193
05/17/2022 12:04:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.417491 on epoch=196
05/17/2022 12:04:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.498852 on epoch=199
05/17/2022 12:04:23 - INFO - __main__ - Global step 600 Train loss 0.580042 ACC 0.5 on epoch=199
05/17/2022 12:04:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.506808 on epoch=203
05/17/2022 12:04:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.372429 on epoch=206
05/17/2022 12:04:31 - INFO - __main__ - Step 630 Global step 630 Train loss 0.496811 on epoch=209
05/17/2022 12:04:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.564928 on epoch=213
05/17/2022 12:04:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.488939 on epoch=216
05/17/2022 12:04:37 - INFO - __main__ - Global step 650 Train loss 0.485983 ACC 0.46875 on epoch=216
05/17/2022 12:04:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.623998 on epoch=219
05/17/2022 12:04:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.396626 on epoch=223
05/17/2022 12:04:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.499085 on epoch=226
05/17/2022 12:04:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.486632 on epoch=229
05/17/2022 12:04:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.471640 on epoch=233
05/17/2022 12:04:51 - INFO - __main__ - Global step 700 Train loss 0.495596 ACC 0.40625 on epoch=233
05/17/2022 12:04:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.479411 on epoch=236
05/17/2022 12:04:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.489885 on epoch=239
05/17/2022 12:04:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.536805 on epoch=243
05/17/2022 12:05:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.426967 on epoch=246
05/17/2022 12:05:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.471719 on epoch=249
05/17/2022 12:05:05 - INFO - __main__ - Global step 750 Train loss 0.480958 ACC 0.46875 on epoch=249
05/17/2022 12:05:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.436159 on epoch=253
05/17/2022 12:05:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.371762 on epoch=256
05/17/2022 12:05:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.460271 on epoch=259
05/17/2022 12:05:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.424470 on epoch=263
05/17/2022 12:05:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.422490 on epoch=266
05/17/2022 12:05:19 - INFO - __main__ - Global step 800 Train loss 0.423031 ACC 0.4375 on epoch=266
05/17/2022 12:05:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.456854 on epoch=269
05/17/2022 12:05:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.417933 on epoch=273
05/17/2022 12:05:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.430471 on epoch=276
05/17/2022 12:05:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.386358 on epoch=279
05/17/2022 12:05:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.416419 on epoch=283
05/17/2022 12:05:34 - INFO - __main__ - Global step 850 Train loss 0.421607 ACC 0.4375 on epoch=283
05/17/2022 12:05:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.374191 on epoch=286
05/17/2022 12:05:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.438348 on epoch=289
05/17/2022 12:05:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.381213 on epoch=293
05/17/2022 12:05:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.359521 on epoch=296
05/17/2022 12:05:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.422219 on epoch=299
05/17/2022 12:05:48 - INFO - __main__ - Global step 900 Train loss 0.395098 ACC 0.46875 on epoch=299
05/17/2022 12:05:48 - INFO - __main__ - save last model!
05/17/2022 12:05:49 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 12:05:49 - INFO - __main__ - Printing 3 examples
05/17/2022 12:05:49 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/17/2022 12:05:49 - INFO - __main__ - ['contradiction']
05/17/2022 12:05:49 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/17/2022 12:05:49 - INFO - __main__ - ['contradiction']
05/17/2022 12:05:49 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/17/2022 12:05:49 - INFO - __main__ - ['contradiction']
05/17/2022 12:05:49 - INFO - __main__ - Tokenizing Input ...
05/17/2022 12:05:49 - INFO - __main__ - Tokenizing Output ...
05/17/2022 12:05:49 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 12:05:49 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 12:05:49 - INFO - __main__ - Printing 3 examples
05/17/2022 12:05:49 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
05/17/2022 12:05:49 - INFO - __main__ - ['contradiction']
05/17/2022 12:05:49 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
05/17/2022 12:05:49 - INFO - __main__ - ['contradiction']
05/17/2022 12:05:49 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
05/17/2022 12:05:49 - INFO - __main__ - ['contradiction']
05/17/2022 12:05:49 - INFO - __main__ - Tokenizing Input ...
05/17/2022 12:05:49 - INFO - __main__ - Tokenizing Output ...
05/17/2022 12:05:49 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 12:05:51 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 12:05:51 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 12:05:51 - INFO - __main__ - Printing 3 examples
05/17/2022 12:05:51 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 12:05:51 - INFO - __main__ - ['contradiction']
05/17/2022 12:05:51 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 12:05:51 - INFO - __main__ - ['neutral']
05/17/2022 12:05:51 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 12:05:51 - INFO - __main__ - ['entailment']
05/17/2022 12:05:51 - INFO - __main__ - Tokenizing Input ...
05/17/2022 12:05:51 - INFO - __main__ - Tokenizing Output ...
05/17/2022 12:05:51 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 12:05:52 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_87_0.0003_8_predictions.txt
05/17/2022 12:05:52 - INFO - __main__ - ACC on test data: 0.4643
05/17/2022 12:05:52 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.0003, bsz=8, dev_performance=0.59375, test_performance=0.4642857142857143
05/17/2022 12:05:52 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.0002, bsz=8 ...
05/17/2022 12:05:53 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 12:05:53 - INFO - __main__ - Printing 3 examples
05/17/2022 12:05:53 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/17/2022 12:05:53 - INFO - __main__ - ['contradiction']
05/17/2022 12:05:53 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/17/2022 12:05:53 - INFO - __main__ - ['contradiction']
05/17/2022 12:05:53 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/17/2022 12:05:53 - INFO - __main__ - ['contradiction']
05/17/2022 12:05:53 - INFO - __main__ - Tokenizing Input ...
05/17/2022 12:05:53 - INFO - __main__ - Tokenizing Output ...
05/17/2022 12:05:53 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 12:05:53 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 12:05:53 - INFO - __main__ - Printing 3 examples
05/17/2022 12:05:53 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
05/17/2022 12:05:53 - INFO - __main__ - ['contradiction']
05/17/2022 12:05:53 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
05/17/2022 12:05:53 - INFO - __main__ - ['contradiction']
05/17/2022 12:05:53 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
05/17/2022 12:05:53 - INFO - __main__ - ['contradiction']
05/17/2022 12:05:53 - INFO - __main__ - Tokenizing Input ...
05/17/2022 12:05:53 - INFO - __main__ - Tokenizing Output ...
05/17/2022 12:05:53 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 12:05:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 12:05:54 - INFO - __main__ - Starting training!
05/17/2022 12:05:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 12:05:58 - INFO - __main__ - Starting training!
05/17/2022 12:06:00 - INFO - __main__ - Step 10 Global step 10 Train loss 17.293161 on epoch=3
05/17/2022 12:06:02 - INFO - __main__ - Step 20 Global step 20 Train loss 16.232920 on epoch=6
05/17/2022 12:06:05 - INFO - __main__ - Step 30 Global step 30 Train loss 12.428438 on epoch=9
05/17/2022 12:06:08 - INFO - __main__ - Step 40 Global step 40 Train loss 9.761236 on epoch=13
05/17/2022 12:06:11 - INFO - __main__ - Step 50 Global step 50 Train loss 8.536390 on epoch=16
05/17/2022 12:06:13 - INFO - __main__ - Global step 50 Train loss 12.850430 ACC 0.0 on epoch=16
05/17/2022 12:06:16 - INFO - __main__ - Step 60 Global step 60 Train loss 7.011492 on epoch=19
05/17/2022 12:06:19 - INFO - __main__ - Step 70 Global step 70 Train loss 6.901046 on epoch=23
05/17/2022 12:06:22 - INFO - __main__ - Step 80 Global step 80 Train loss 6.768648 on epoch=26
05/17/2022 12:06:24 - INFO - __main__ - Step 90 Global step 90 Train loss 5.051389 on epoch=29
05/17/2022 12:06:27 - INFO - __main__ - Step 100 Global step 100 Train loss 4.760290 on epoch=33
05/17/2022 12:06:28 - INFO - __main__ - Global step 100 Train loss 6.098572 ACC 0.0 on epoch=33
05/17/2022 12:06:31 - INFO - __main__ - Step 110 Global step 110 Train loss 4.147763 on epoch=36
05/17/2022 12:06:34 - INFO - __main__ - Step 120 Global step 120 Train loss 3.136886 on epoch=39
05/17/2022 12:06:36 - INFO - __main__ - Step 130 Global step 130 Train loss 3.094576 on epoch=43
05/17/2022 12:06:39 - INFO - __main__ - Step 140 Global step 140 Train loss 2.548669 on epoch=46
05/17/2022 12:06:42 - INFO - __main__ - Step 150 Global step 150 Train loss 3.477159 on epoch=49
05/17/2022 12:06:42 - INFO - __main__ - Global step 150 Train loss 3.281010 ACC 0.28125 on epoch=49
05/17/2022 12:06:46 - INFO - __main__ - Step 160 Global step 160 Train loss 2.502173 on epoch=53
05/17/2022 12:06:49 - INFO - __main__ - Step 170 Global step 170 Train loss 2.630351 on epoch=56
05/17/2022 12:06:51 - INFO - __main__ - Step 180 Global step 180 Train loss 2.236140 on epoch=59
05/17/2022 12:06:54 - INFO - __main__ - Step 190 Global step 190 Train loss 2.312088 on epoch=63
05/17/2022 12:06:57 - INFO - __main__ - Step 200 Global step 200 Train loss 2.747312 on epoch=66
05/17/2022 12:06:57 - INFO - __main__ - Global step 200 Train loss 2.485612 ACC 0.5 on epoch=66
05/17/2022 12:07:00 - INFO - __main__ - Step 210 Global step 210 Train loss 1.975900 on epoch=69
05/17/2022 12:07:03 - INFO - __main__ - Step 220 Global step 220 Train loss 1.620886 on epoch=73
05/17/2022 12:07:05 - INFO - __main__ - Step 230 Global step 230 Train loss 1.968355 on epoch=76
05/17/2022 12:07:08 - INFO - __main__ - Step 240 Global step 240 Train loss 1.769626 on epoch=79
05/17/2022 12:07:11 - INFO - __main__ - Step 250 Global step 250 Train loss 1.555990 on epoch=83
05/17/2022 12:07:11 - INFO - __main__ - Global step 250 Train loss 1.778152 ACC 0.1875 on epoch=83
05/17/2022 12:07:14 - INFO - __main__ - Step 260 Global step 260 Train loss 1.683258 on epoch=86
05/17/2022 12:07:17 - INFO - __main__ - Step 270 Global step 270 Train loss 2.029918 on epoch=89
05/17/2022 12:07:20 - INFO - __main__ - Step 280 Global step 280 Train loss 2.001233 on epoch=93
05/17/2022 12:07:22 - INFO - __main__ - Step 290 Global step 290 Train loss 1.678200 on epoch=96
05/17/2022 12:07:25 - INFO - __main__ - Step 300 Global step 300 Train loss 1.297187 on epoch=99
05/17/2022 12:07:26 - INFO - __main__ - Global step 300 Train loss 1.737959 ACC 0.0 on epoch=99
05/17/2022 12:07:28 - INFO - __main__ - Step 310 Global step 310 Train loss 1.226691 on epoch=103
05/17/2022 12:07:31 - INFO - __main__ - Step 320 Global step 320 Train loss 1.117116 on epoch=106
05/17/2022 12:07:34 - INFO - __main__ - Step 330 Global step 330 Train loss 1.194091 on epoch=109
05/17/2022 12:07:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.906742 on epoch=113
05/17/2022 12:07:39 - INFO - __main__ - Step 350 Global step 350 Train loss 1.390575 on epoch=116
05/17/2022 12:07:40 - INFO - __main__ - Global step 350 Train loss 1.167043 ACC 0.375 on epoch=116
05/17/2022 12:07:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.969878 on epoch=119
05/17/2022 12:07:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.935022 on epoch=123
05/17/2022 12:07:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.560544 on epoch=126
05/17/2022 12:07:51 - INFO - __main__ - Step 390 Global step 390 Train loss 1.033641 on epoch=129
05/17/2022 12:07:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.745192 on epoch=133
05/17/2022 12:07:54 - INFO - __main__ - Global step 400 Train loss 0.848855 ACC 0.71875 on epoch=133
05/17/2022 12:07:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.952705 on epoch=136
05/17/2022 12:08:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.724884 on epoch=139
05/17/2022 12:08:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.680236 on epoch=143
05/17/2022 12:08:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.634075 on epoch=146
05/17/2022 12:08:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.739162 on epoch=149
05/17/2022 12:08:09 - INFO - __main__ - Global step 450 Train loss 0.746212 ACC 0.8125 on epoch=149
05/17/2022 12:08:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.635855 on epoch=153
05/17/2022 12:08:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.753116 on epoch=156
05/17/2022 12:08:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.730201 on epoch=159
05/17/2022 12:08:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.516827 on epoch=163
05/17/2022 12:08:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.599391 on epoch=166
05/17/2022 12:08:24 - INFO - __main__ - Global step 500 Train loss 0.647078 ACC 0.78125 on epoch=166
05/17/2022 12:08:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.392033 on epoch=169
05/17/2022 12:08:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.378275 on epoch=173
05/17/2022 12:08:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.431715 on epoch=176
05/17/2022 12:08:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.346536 on epoch=179
05/17/2022 12:08:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.443266 on epoch=183
05/17/2022 12:08:38 - INFO - __main__ - Global step 550 Train loss 0.398365 ACC 0.65625 on epoch=183
05/17/2022 12:08:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.288820 on epoch=186
05/17/2022 12:08:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.566536 on epoch=189
05/17/2022 12:08:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.818989 on epoch=193
05/17/2022 12:08:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.327184 on epoch=196
05/17/2022 12:08:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.304603 on epoch=199
05/17/2022 12:08:52 - INFO - __main__ - Global step 600 Train loss 0.461226 ACC 0.8125 on epoch=199
05/17/2022 12:08:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.319427 on epoch=203
05/17/2022 12:08:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.455166 on epoch=206
05/17/2022 12:09:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.471343 on epoch=209
05/17/2022 12:09:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.330414 on epoch=213
05/17/2022 12:09:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.223056 on epoch=216
05/17/2022 12:09:06 - INFO - __main__ - Global step 650 Train loss 0.359881 ACC 0.8125 on epoch=216
05/17/2022 12:09:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.199015 on epoch=219
05/17/2022 12:09:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.230084 on epoch=223
05/17/2022 12:09:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.163632 on epoch=226
05/17/2022 12:09:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.284145 on epoch=229
05/17/2022 12:09:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.156448 on epoch=233
05/17/2022 12:09:20 - INFO - __main__ - Global step 700 Train loss 0.206665 ACC 0.84375 on epoch=233
05/17/2022 12:09:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.212174 on epoch=236
05/17/2022 12:09:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.272835 on epoch=239
05/17/2022 12:09:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.099930 on epoch=243
05/17/2022 12:09:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.285001 on epoch=246
05/17/2022 12:09:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.197564 on epoch=249
05/17/2022 12:09:34 - INFO - __main__ - Global step 750 Train loss 0.213501 ACC 0.78125 on epoch=249
05/17/2022 12:09:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.262325 on epoch=253
05/17/2022 12:09:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.177812 on epoch=256
05/17/2022 12:09:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.131320 on epoch=259
05/17/2022 12:09:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.095691 on epoch=263
05/17/2022 12:09:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.139471 on epoch=266
05/17/2022 12:09:49 - INFO - __main__ - Global step 800 Train loss 0.161324 ACC 0.84375 on epoch=266
05/17/2022 12:09:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.143633 on epoch=269
05/17/2022 12:09:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.158792 on epoch=273
05/17/2022 12:09:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.292052 on epoch=276
05/17/2022 12:10:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.256271 on epoch=279
05/17/2022 12:10:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.173457 on epoch=283
05/17/2022 12:10:03 - INFO - __main__ - Global step 850 Train loss 0.204841 ACC 0.75 on epoch=283
05/17/2022 12:10:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.148799 on epoch=286
05/17/2022 12:10:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.225328 on epoch=289
05/17/2022 12:10:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.412482 on epoch=293
05/17/2022 12:10:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.450596 on epoch=296
05/17/2022 12:10:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.727307 on epoch=299
05/17/2022 12:10:17 - INFO - __main__ - Global step 900 Train loss 0.392902 ACC 0.46875 on epoch=299
05/17/2022 12:10:17 - INFO - __main__ - save last model!
05/17/2022 12:10:18 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 12:10:18 - INFO - __main__ - Printing 3 examples
05/17/2022 12:10:18 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/17/2022 12:10:18 - INFO - __main__ - ['contradiction']
05/17/2022 12:10:18 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/17/2022 12:10:18 - INFO - __main__ - ['contradiction']
05/17/2022 12:10:18 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/17/2022 12:10:18 - INFO - __main__ - ['contradiction']
05/17/2022 12:10:18 - INFO - __main__ - Tokenizing Input ...
05/17/2022 12:10:18 - INFO - __main__ - Tokenizing Output ...
05/17/2022 12:10:18 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 12:10:18 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 12:10:18 - INFO - __main__ - Printing 3 examples
05/17/2022 12:10:18 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
05/17/2022 12:10:18 - INFO - __main__ - ['contradiction']
05/17/2022 12:10:18 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
05/17/2022 12:10:18 - INFO - __main__ - ['contradiction']
05/17/2022 12:10:18 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
05/17/2022 12:10:18 - INFO - __main__ - ['contradiction']
05/17/2022 12:10:18 - INFO - __main__ - Tokenizing Input ...
05/17/2022 12:10:18 - INFO - __main__ - Tokenizing Output ...
05/17/2022 12:10:18 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 12:10:20 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 12:10:20 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 12:10:20 - INFO - __main__ - Printing 3 examples
05/17/2022 12:10:20 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 12:10:20 - INFO - __main__ - ['contradiction']
05/17/2022 12:10:20 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 12:10:20 - INFO - __main__ - ['neutral']
05/17/2022 12:10:20 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 12:10:20 - INFO - __main__ - ['entailment']
05/17/2022 12:10:20 - INFO - __main__ - Tokenizing Input ...
05/17/2022 12:10:20 - INFO - __main__ - Tokenizing Output ...
05/17/2022 12:10:20 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 12:10:21 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_87_0.0002_8_predictions.txt
05/17/2022 12:10:21 - INFO - __main__ - ACC on test data: 0.7321
05/17/2022 12:10:22 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.0002, bsz=8, dev_performance=0.84375, test_performance=0.7321428571428571
05/17/2022 12:10:22 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.0001, bsz=8 ...
05/17/2022 12:10:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 12:10:22 - INFO - __main__ - Starting training!
05/17/2022 12:10:23 - INFO - __main__ - Start tokenizing ... 48 instances
05/17/2022 12:10:23 - INFO - __main__ - Printing 3 examples
05/17/2022 12:10:23 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/17/2022 12:10:23 - INFO - __main__ - ['contradiction']
05/17/2022 12:10:23 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/17/2022 12:10:23 - INFO - __main__ - ['contradiction']
05/17/2022 12:10:23 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/17/2022 12:10:23 - INFO - __main__ - ['contradiction']
05/17/2022 12:10:23 - INFO - __main__ - Tokenizing Input ...
05/17/2022 12:10:23 - INFO - __main__ - Tokenizing Output ...
05/17/2022 12:10:23 - INFO - __main__ - Loaded 48 examples from train data
05/17/2022 12:10:23 - INFO - __main__ - Start tokenizing ... 32 instances
05/17/2022 12:10:23 - INFO - __main__ - Printing 3 examples
05/17/2022 12:10:23 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
05/17/2022 12:10:23 - INFO - __main__ - ['contradiction']
05/17/2022 12:10:23 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
05/17/2022 12:10:23 - INFO - __main__ - ['contradiction']
05/17/2022 12:10:23 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
05/17/2022 12:10:23 - INFO - __main__ - ['contradiction']
05/17/2022 12:10:23 - INFO - __main__ - Tokenizing Input ...
05/17/2022 12:10:23 - INFO - __main__ - Tokenizing Output ...
05/17/2022 12:10:23 - INFO - __main__ - Loaded 32 examples from dev data
05/17/2022 12:10:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/17/2022 12:10:28 - INFO - __main__ - Starting training!
05/17/2022 12:10:30 - INFO - __main__ - Step 10 Global step 10 Train loss 18.390162 on epoch=3
05/17/2022 12:10:33 - INFO - __main__ - Step 20 Global step 20 Train loss 16.249537 on epoch=6
05/17/2022 12:10:36 - INFO - __main__ - Step 30 Global step 30 Train loss 14.358335 on epoch=9
05/17/2022 12:10:39 - INFO - __main__ - Step 40 Global step 40 Train loss 12.300209 on epoch=13
05/17/2022 12:10:41 - INFO - __main__ - Step 50 Global step 50 Train loss 10.788472 on epoch=16
05/17/2022 12:10:45 - INFO - __main__ - Global step 50 Train loss 14.417342 ACC 0.0 on epoch=16
05/17/2022 12:10:48 - INFO - __main__ - Step 60 Global step 60 Train loss 9.787730 on epoch=19
05/17/2022 12:10:51 - INFO - __main__ - Step 70 Global step 70 Train loss 9.735407 on epoch=23
05/17/2022 12:10:54 - INFO - __main__ - Step 80 Global step 80 Train loss 7.992747 on epoch=26
05/17/2022 12:10:57 - INFO - __main__ - Step 90 Global step 90 Train loss 8.195810 on epoch=29
05/17/2022 12:10:59 - INFO - __main__ - Step 100 Global step 100 Train loss 7.437413 on epoch=33
05/17/2022 12:11:01 - INFO - __main__ - Global step 100 Train loss 8.629822 ACC 0.0 on epoch=33
05/17/2022 12:11:04 - INFO - __main__ - Step 110 Global step 110 Train loss 7.578204 on epoch=36
05/17/2022 12:11:06 - INFO - __main__ - Step 120 Global step 120 Train loss 6.762218 on epoch=39
05/17/2022 12:11:09 - INFO - __main__ - Step 130 Global step 130 Train loss 6.736846 on epoch=43
05/17/2022 12:11:12 - INFO - __main__ - Step 140 Global step 140 Train loss 5.773082 on epoch=46
05/17/2022 12:11:15 - INFO - __main__ - Step 150 Global step 150 Train loss 6.195779 on epoch=49
05/17/2022 12:11:16 - INFO - __main__ - Global step 150 Train loss 6.609226 ACC 0.0625 on epoch=49
05/17/2022 12:11:19 - INFO - __main__ - Step 160 Global step 160 Train loss 4.514611 on epoch=53
05/17/2022 12:11:22 - INFO - __main__ - Step 170 Global step 170 Train loss 3.994349 on epoch=56
05/17/2022 12:11:25 - INFO - __main__ - Step 180 Global step 180 Train loss 4.679502 on epoch=59
05/17/2022 12:11:28 - INFO - __main__ - Step 190 Global step 190 Train loss 4.132262 on epoch=63
05/17/2022 12:11:31 - INFO - __main__ - Step 200 Global step 200 Train loss 3.910724 on epoch=66
05/17/2022 12:11:31 - INFO - __main__ - Global step 200 Train loss 4.246289 ACC 0.09375 on epoch=66
05/17/2022 12:11:34 - INFO - __main__ - Step 210 Global step 210 Train loss 4.303822 on epoch=69
05/17/2022 12:11:37 - INFO - __main__ - Step 220 Global step 220 Train loss 4.080168 on epoch=73
05/17/2022 12:11:40 - INFO - __main__ - Step 230 Global step 230 Train loss 2.432859 on epoch=76
05/17/2022 12:11:42 - INFO - __main__ - Step 240 Global step 240 Train loss 3.435077 on epoch=79
05/17/2022 12:11:45 - INFO - __main__ - Step 250 Global step 250 Train loss 2.799812 on epoch=83
05/17/2022 12:11:45 - INFO - __main__ - Global step 250 Train loss 3.410348 ACC 0.0 on epoch=83
05/17/2022 12:11:48 - INFO - __main__ - Step 260 Global step 260 Train loss 1.867717 on epoch=86
05/17/2022 12:11:51 - INFO - __main__ - Step 270 Global step 270 Train loss 3.170618 on epoch=89
05/17/2022 12:11:53 - INFO - __main__ - Step 280 Global step 280 Train loss 2.582996 on epoch=93
05/17/2022 12:11:56 - INFO - __main__ - Step 290 Global step 290 Train loss 2.244475 on epoch=96
05/17/2022 12:11:59 - INFO - __main__ - Step 300 Global step 300 Train loss 2.049377 on epoch=99
05/17/2022 12:11:59 - INFO - __main__ - Global step 300 Train loss 2.383037 ACC 0.28125 on epoch=99
05/17/2022 12:12:02 - INFO - __main__ - Step 310 Global step 310 Train loss 2.376173 on epoch=103
05/17/2022 12:12:05 - INFO - __main__ - Step 320 Global step 320 Train loss 2.953645 on epoch=106
05/17/2022 12:12:08 - INFO - __main__ - Step 330 Global step 330 Train loss 1.726395 on epoch=109
05/17/2022 12:12:10 - INFO - __main__ - Step 340 Global step 340 Train loss 1.798719 on epoch=113
05/17/2022 12:12:13 - INFO - __main__ - Step 350 Global step 350 Train loss 1.669724 on epoch=116
05/17/2022 12:12:13 - INFO - __main__ - Global step 350 Train loss 2.104931 ACC 0.28125 on epoch=116
05/17/2022 12:12:16 - INFO - __main__ - Step 360 Global step 360 Train loss 1.927041 on epoch=119
05/17/2022 12:12:19 - INFO - __main__ - Step 370 Global step 370 Train loss 1.530251 on epoch=123
05/17/2022 12:12:22 - INFO - __main__ - Step 380 Global step 380 Train loss 1.741296 on epoch=126
05/17/2022 12:12:24 - INFO - __main__ - Step 390 Global step 390 Train loss 1.871913 on epoch=129
05/17/2022 12:12:27 - INFO - __main__ - Step 400 Global step 400 Train loss 2.047292 on epoch=133
05/17/2022 12:12:28 - INFO - __main__ - Global step 400 Train loss 1.823558 ACC 0.3125 on epoch=133
05/17/2022 12:12:31 - INFO - __main__ - Step 410 Global step 410 Train loss 1.345265 on epoch=136
05/17/2022 12:12:33 - INFO - __main__ - Step 420 Global step 420 Train loss 1.956664 on epoch=139
05/17/2022 12:12:36 - INFO - __main__ - Step 430 Global step 430 Train loss 1.578287 on epoch=143
05/17/2022 12:12:39 - INFO - __main__ - Step 440 Global step 440 Train loss 1.892831 on epoch=146
05/17/2022 12:12:42 - INFO - __main__ - Step 450 Global step 450 Train loss 1.508194 on epoch=149
05/17/2022 12:12:42 - INFO - __main__ - Global step 450 Train loss 1.656248 ACC 0.4375 on epoch=149
05/17/2022 12:12:45 - INFO - __main__ - Step 460 Global step 460 Train loss 2.233759 on epoch=153
05/17/2022 12:12:48 - INFO - __main__ - Step 470 Global step 470 Train loss 1.309374 on epoch=156
05/17/2022 12:12:51 - INFO - __main__ - Step 480 Global step 480 Train loss 1.029503 on epoch=159
05/17/2022 12:12:53 - INFO - __main__ - Step 490 Global step 490 Train loss 1.623459 on epoch=163
05/17/2022 12:12:56 - INFO - __main__ - Step 500 Global step 500 Train loss 1.502686 on epoch=166
05/17/2022 12:12:57 - INFO - __main__ - Global step 500 Train loss 1.539756 ACC 0.375 on epoch=166
05/17/2022 12:12:59 - INFO - __main__ - Step 510 Global step 510 Train loss 1.508913 on epoch=169
05/17/2022 12:13:02 - INFO - __main__ - Step 520 Global step 520 Train loss 1.645747 on epoch=173
05/17/2022 12:13:05 - INFO - __main__ - Step 530 Global step 530 Train loss 1.718818 on epoch=176
05/17/2022 12:13:07 - INFO - __main__ - Step 540 Global step 540 Train loss 1.277923 on epoch=179
05/17/2022 12:13:10 - INFO - __main__ - Step 550 Global step 550 Train loss 1.350639 on epoch=183
05/17/2022 12:13:11 - INFO - __main__ - Global step 550 Train loss 1.500408 ACC 0.1875 on epoch=183
05/17/2022 12:13:13 - INFO - __main__ - Step 560 Global step 560 Train loss 1.241127 on epoch=186
05/17/2022 12:13:16 - INFO - __main__ - Step 570 Global step 570 Train loss 1.478887 on epoch=189
05/17/2022 12:13:19 - INFO - __main__ - Step 580 Global step 580 Train loss 1.220935 on epoch=193
05/17/2022 12:13:21 - INFO - __main__ - Step 590 Global step 590 Train loss 1.307218 on epoch=196
05/17/2022 12:13:24 - INFO - __main__ - Step 600 Global step 600 Train loss 1.239865 on epoch=199
05/17/2022 12:13:25 - INFO - __main__ - Global step 600 Train loss 1.297607 ACC 0.21875 on epoch=199
05/17/2022 12:13:27 - INFO - __main__ - Step 610 Global step 610 Train loss 1.073960 on epoch=203
05/17/2022 12:13:30 - INFO - __main__ - Step 620 Global step 620 Train loss 1.028601 on epoch=206
05/17/2022 12:13:33 - INFO - __main__ - Step 630 Global step 630 Train loss 1.188648 on epoch=209
05/17/2022 12:13:36 - INFO - __main__ - Step 640 Global step 640 Train loss 1.062482 on epoch=213
05/17/2022 12:13:38 - INFO - __main__ - Step 650 Global step 650 Train loss 1.053721 on epoch=216
05/17/2022 12:13:39 - INFO - __main__ - Global step 650 Train loss 1.081482 ACC 0.40625 on epoch=216
05/17/2022 12:13:41 - INFO - __main__ - Step 660 Global step 660 Train loss 1.197414 on epoch=219
05/17/2022 12:13:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.993385 on epoch=223
05/17/2022 12:13:47 - INFO - __main__ - Step 680 Global step 680 Train loss 1.037402 on epoch=226
05/17/2022 12:13:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.938668 on epoch=229
05/17/2022 12:13:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.999651 on epoch=233
05/17/2022 12:13:53 - INFO - __main__ - Global step 700 Train loss 1.033304 ACC 0.59375 on epoch=233
05/17/2022 12:13:56 - INFO - __main__ - Step 710 Global step 710 Train loss 1.232694 on epoch=236
05/17/2022 12:13:59 - INFO - __main__ - Step 720 Global step 720 Train loss 1.183468 on epoch=239
05/17/2022 12:14:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.890550 on epoch=243
05/17/2022 12:14:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.793534 on epoch=246
05/17/2022 12:14:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.631054 on epoch=249
05/17/2022 12:14:07 - INFO - __main__ - Global step 750 Train loss 0.946260 ACC 0.5625 on epoch=249
05/17/2022 12:14:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.717319 on epoch=253
05/17/2022 12:14:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.811038 on epoch=256
05/17/2022 12:14:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.738758 on epoch=259
05/17/2022 12:14:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.764606 on epoch=263
05/17/2022 12:14:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.681079 on epoch=266
05/17/2022 12:14:21 - INFO - __main__ - Global step 800 Train loss 0.742560 ACC 0.78125 on epoch=266
05/17/2022 12:14:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.690146 on epoch=269
05/17/2022 12:14:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.827090 on epoch=273
05/17/2022 12:14:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.802338 on epoch=276
05/17/2022 12:14:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.625082 on epoch=279
05/17/2022 12:14:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.796899 on epoch=283
05/17/2022 12:14:36 - INFO - __main__ - Global step 850 Train loss 0.748311 ACC 0.6875 on epoch=283
05/17/2022 12:14:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.667351 on epoch=286
05/17/2022 12:14:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.575173 on epoch=289
05/17/2022 12:14:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.721611 on epoch=293
05/17/2022 12:14:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.889104 on epoch=296
05/17/2022 12:14:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.688528 on epoch=299
05/17/2022 12:14:50 - INFO - __main__ - Global step 900 Train loss 0.708354 ACC 0.78125 on epoch=299
05/17/2022 12:14:50 - INFO - __main__ - save last model!
05/17/2022 12:14:52 - INFO - __main__ - Loading checkpoint on the fly
05/17/2022 12:14:53 - INFO - __main__ - Start tokenizing ... 56 instances
05/17/2022 12:14:53 - INFO - __main__ - Printing 3 examples
05/17/2022 12:14:53 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/17/2022 12:14:53 - INFO - __main__ - ['contradiction']
05/17/2022 12:14:53 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/17/2022 12:14:53 - INFO - __main__ - ['neutral']
05/17/2022 12:14:53 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/17/2022 12:14:53 - INFO - __main__ - ['entailment']
05/17/2022 12:14:53 - INFO - __main__ - Tokenizing Input ...
05/17/2022 12:14:53 - INFO - __main__ - Tokenizing Output ...
05/17/2022 12:14:53 - INFO - __main__ - Loaded 56 examples from test data
05/17/2022 12:14:54 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_87_0.0001_8_predictions.txt
05/17/2022 12:14:54 - INFO - __main__ - ACC on test data: 0.6429
05/17/2022 12:14:54 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.0001, bsz=8, dev_performance=0.78125, test_performance=0.6428571428571429
05/21/2022 08:00:27 - INFO - __main__ - Namespace(task_dir='data/superglue-cb/', task_name='superglue-cb', identifier='T5-base-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-cls2cls/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-base', cuda='2,3')
05/21/2022 08:00:27 - INFO - __main__ - models/T5-base-ft-cls2cls/singletask-superglue-cb
05/21/2022 08:00:27 - INFO - __main__ - Namespace(task_dir='data/superglue-cb/', task_name='superglue-cb', identifier='T5-base-ft-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-ft-cls2cls/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-base', cuda='2,3')
05/21/2022 08:00:27 - INFO - __main__ - models/T5-base-ft-cls2cls/singletask-superglue-cb
05/21/2022 08:00:29 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 08:00:29 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 08:00:29 - INFO - __main__ - args.device: cuda:0
05/21/2022 08:00:29 - INFO - __main__ - Using 2 gpus
05/21/2022 08:00:29 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_16_100', 'superglue-cb_16_13', 'superglue-cb_16_21', 'superglue-cb_16_42', 'superglue-cb_16_87']
05/21/2022 08:00:29 - INFO - __main__ - args.device: cuda:1
05/21/2022 08:00:29 - INFO - __main__ - Using 2 gpus
05/21/2022 08:00:29 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_16_100', 'superglue-cb_16_13', 'superglue-cb_16_21', 'superglue-cb_16_42', 'superglue-cb_16_87']
05/21/2022 08:00:34 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.0005, bsz=8 ...
05/21/2022 08:00:35 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:00:35 - INFO - __main__ - Printing 3 examples
05/21/2022 08:00:35 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/21/2022 08:00:35 - INFO - __main__ - ['contradiction']
05/21/2022 08:00:35 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/21/2022 08:00:35 - INFO - __main__ - ['contradiction']
05/21/2022 08:00:35 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/21/2022 08:00:35 - INFO - __main__ - ['contradiction']
05/21/2022 08:00:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:00:35 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:00:35 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:00:35 - INFO - __main__ - Printing 3 examples
05/21/2022 08:00:35 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/21/2022 08:00:35 - INFO - __main__ - ['contradiction']
05/21/2022 08:00:35 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/21/2022 08:00:35 - INFO - __main__ - ['contradiction']
05/21/2022 08:00:35 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/21/2022 08:00:35 - INFO - __main__ - ['contradiction']
05/21/2022 08:00:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:00:35 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:00:35 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:00:35 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:00:35 - INFO - __main__ - Printing 3 examples
05/21/2022 08:00:35 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
05/21/2022 08:00:35 - INFO - __main__ - ['contradiction']
05/21/2022 08:00:35 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/21/2022 08:00:35 - INFO - __main__ - ['contradiction']
05/21/2022 08:00:35 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
05/21/2022 08:00:35 - INFO - __main__ - ['contradiction']
05/21/2022 08:00:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:00:35 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:00:35 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:00:35 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:00:35 - INFO - __main__ - Printing 3 examples
05/21/2022 08:00:35 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
05/21/2022 08:00:35 - INFO - __main__ - ['contradiction']
05/21/2022 08:00:35 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/21/2022 08:00:35 - INFO - __main__ - ['contradiction']
05/21/2022 08:00:35 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
05/21/2022 08:00:35 - INFO - __main__ - ['contradiction']
05/21/2022 08:00:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:00:35 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:00:35 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:00:35 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:00:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:00:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:00:39 - INFO - __main__ - Starting training!
05/21/2022 08:00:39 - INFO - __main__ - Starting training!
05/21/2022 08:00:42 - INFO - __main__ - Step 10 Global step 10 Train loss 18.069210 on epoch=3
05/21/2022 08:00:44 - INFO - __main__ - Step 20 Global step 20 Train loss 13.451063 on epoch=6
05/21/2022 08:00:47 - INFO - __main__ - Step 30 Global step 30 Train loss 10.135462 on epoch=9
05/21/2022 08:00:49 - INFO - __main__ - Step 40 Global step 40 Train loss 7.505745 on epoch=13
05/21/2022 08:00:52 - INFO - __main__ - Step 50 Global step 50 Train loss 5.102290 on epoch=16
05/21/2022 08:00:52 - INFO - __main__ - Global step 50 Train loss 10.852754 ACC 0.0 on epoch=16
05/21/2022 08:00:55 - INFO - __main__ - Step 60 Global step 60 Train loss 4.711202 on epoch=19
05/21/2022 08:00:58 - INFO - __main__ - Step 70 Global step 70 Train loss 3.482101 on epoch=23
05/21/2022 08:01:00 - INFO - __main__ - Step 80 Global step 80 Train loss 3.781743 on epoch=26
05/21/2022 08:01:03 - INFO - __main__ - Step 90 Global step 90 Train loss 3.654217 on epoch=29
05/21/2022 08:01:05 - INFO - __main__ - Step 100 Global step 100 Train loss 2.859874 on epoch=33
05/21/2022 08:01:05 - INFO - __main__ - Global step 100 Train loss 3.697828 ACC 0.46875 on epoch=33
05/21/2022 08:01:09 - INFO - __main__ - Step 110 Global step 110 Train loss 2.163067 on epoch=36
05/21/2022 08:01:11 - INFO - __main__ - Step 120 Global step 120 Train loss 1.793645 on epoch=39
05/21/2022 08:01:14 - INFO - __main__ - Step 130 Global step 130 Train loss 1.813309 on epoch=43
05/21/2022 08:01:16 - INFO - __main__ - Step 140 Global step 140 Train loss 2.177996 on epoch=46
05/21/2022 08:01:19 - INFO - __main__ - Step 150 Global step 150 Train loss 1.755212 on epoch=49
05/21/2022 08:01:19 - INFO - __main__ - Global step 150 Train loss 1.940646 ACC 0.375 on epoch=49
05/21/2022 08:01:21 - INFO - __main__ - Step 160 Global step 160 Train loss 1.840376 on epoch=53
05/21/2022 08:01:24 - INFO - __main__ - Step 170 Global step 170 Train loss 1.932276 on epoch=56
05/21/2022 08:01:26 - INFO - __main__ - Step 180 Global step 180 Train loss 1.738884 on epoch=59
05/21/2022 08:01:29 - INFO - __main__ - Step 190 Global step 190 Train loss 1.549128 on epoch=63
05/21/2022 08:01:31 - INFO - __main__ - Step 200 Global step 200 Train loss 1.148577 on epoch=66
05/21/2022 08:01:32 - INFO - __main__ - Global step 200 Train loss 1.641848 ACC 0.5 on epoch=66
05/21/2022 08:01:34 - INFO - __main__ - Step 210 Global step 210 Train loss 1.263128 on epoch=69
05/21/2022 08:01:37 - INFO - __main__ - Step 220 Global step 220 Train loss 1.102771 on epoch=73
05/21/2022 08:01:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.767447 on epoch=76
05/21/2022 08:01:42 - INFO - __main__ - Step 240 Global step 240 Train loss 1.027618 on epoch=79
05/21/2022 08:01:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.911441 on epoch=83
05/21/2022 08:01:45 - INFO - __main__ - Global step 250 Train loss 1.014481 ACC 0.25 on epoch=83
05/21/2022 08:01:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.977483 on epoch=86
05/21/2022 08:01:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.805465 on epoch=89
05/21/2022 08:01:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.772846 on epoch=93
05/21/2022 08:01:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.733679 on epoch=96
05/21/2022 08:01:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.813928 on epoch=99
05/21/2022 08:01:57 - INFO - __main__ - Global step 300 Train loss 0.820680 ACC 0.0 on epoch=99
05/21/2022 08:02:00 - INFO - __main__ - Step 310 Global step 310 Train loss 1.069412 on epoch=103
05/21/2022 08:02:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.996318 on epoch=106
05/21/2022 08:02:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.748070 on epoch=109
05/21/2022 08:02:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.688340 on epoch=113
05/21/2022 08:02:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.688671 on epoch=116
05/21/2022 08:02:10 - INFO - __main__ - Global step 350 Train loss 0.838162 ACC 0.28125 on epoch=116
05/21/2022 08:02:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.676827 on epoch=119
05/21/2022 08:02:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.689766 on epoch=123
05/21/2022 08:02:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.604313 on epoch=126
05/21/2022 08:02:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.558935 on epoch=129
05/21/2022 08:02:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.513421 on epoch=133
05/21/2022 08:02:23 - INFO - __main__ - Global step 400 Train loss 0.608652 ACC 0.28125 on epoch=133
05/21/2022 08:02:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.574906 on epoch=136
05/21/2022 08:02:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.528374 on epoch=139
05/21/2022 08:02:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.586345 on epoch=143
05/21/2022 08:02:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.516958 on epoch=146
05/21/2022 08:02:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.591386 on epoch=149
05/21/2022 08:02:36 - INFO - __main__ - Global step 450 Train loss 0.559594 ACC 0.0 on epoch=149
05/21/2022 08:02:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.505911 on epoch=153
05/21/2022 08:02:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.551863 on epoch=156
05/21/2022 08:02:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.452422 on epoch=159
05/21/2022 08:02:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.526092 on epoch=163
05/21/2022 08:02:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.475104 on epoch=166
05/21/2022 08:02:48 - INFO - __main__ - Global step 500 Train loss 0.502278 ACC 0.21875 on epoch=166
05/21/2022 08:02:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.506913 on epoch=169
05/21/2022 08:02:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.430662 on epoch=173
05/21/2022 08:02:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.500739 on epoch=176
05/21/2022 08:02:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.526276 on epoch=179
05/21/2022 08:03:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.681980 on epoch=183
05/21/2022 08:03:01 - INFO - __main__ - Global step 550 Train loss 0.529314 ACC 0.3125 on epoch=183
05/21/2022 08:03:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.490018 on epoch=186
05/21/2022 08:03:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.458874 on epoch=189
05/21/2022 08:03:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.453374 on epoch=193
05/21/2022 08:03:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.491853 on epoch=196
05/21/2022 08:03:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.487386 on epoch=199
05/21/2022 08:03:14 - INFO - __main__ - Global step 600 Train loss 0.476301 ACC 0.0 on epoch=199
05/21/2022 08:03:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.410894 on epoch=203
05/21/2022 08:03:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.446064 on epoch=206
05/21/2022 08:03:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.468123 on epoch=209
05/21/2022 08:03:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.541337 on epoch=213
05/21/2022 08:03:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.438976 on epoch=216
05/21/2022 08:03:27 - INFO - __main__ - Global step 650 Train loss 0.461079 ACC 0.0 on epoch=216
05/21/2022 08:03:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.445668 on epoch=219
05/21/2022 08:03:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.411300 on epoch=223
05/21/2022 08:03:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.409043 on epoch=226
05/21/2022 08:03:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.436991 on epoch=229
05/21/2022 08:03:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.426985 on epoch=233
05/21/2022 08:03:39 - INFO - __main__ - Global step 700 Train loss 0.425997 ACC 0.0 on epoch=233
05/21/2022 08:03:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.412814 on epoch=236
05/21/2022 08:03:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.442172 on epoch=239
05/21/2022 08:03:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.398071 on epoch=243
05/21/2022 08:03:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.441720 on epoch=246
05/21/2022 08:03:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.426879 on epoch=249
05/21/2022 08:03:52 - INFO - __main__ - Global step 750 Train loss 0.424331 ACC 0.1875 on epoch=249
05/21/2022 08:03:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.451233 on epoch=253
05/21/2022 08:03:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.442171 on epoch=256
05/21/2022 08:04:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.440526 on epoch=259
05/21/2022 08:04:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.408843 on epoch=263
05/21/2022 08:04:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.474116 on epoch=266
05/21/2022 08:04:05 - INFO - __main__ - Global step 800 Train loss 0.443378 ACC 0.40625 on epoch=266
05/21/2022 08:04:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.462799 on epoch=269
05/21/2022 08:04:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.455379 on epoch=273
05/21/2022 08:04:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.405695 on epoch=276
05/21/2022 08:04:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.409609 on epoch=279
05/21/2022 08:04:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.378478 on epoch=283
05/21/2022 08:04:18 - INFO - __main__ - Global step 850 Train loss 0.422392 ACC 0.34375 on epoch=283
05/21/2022 08:04:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.425585 on epoch=286
05/21/2022 08:04:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.440362 on epoch=289
05/21/2022 08:04:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.425262 on epoch=293
05/21/2022 08:04:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.427677 on epoch=296
05/21/2022 08:04:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.432462 on epoch=299
05/21/2022 08:04:30 - INFO - __main__ - Global step 900 Train loss 0.430270 ACC 0.0 on epoch=299
05/21/2022 08:04:30 - INFO - __main__ - save last model!
05/21/2022 08:04:31 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:04:31 - INFO - __main__ - Printing 3 examples
05/21/2022 08:04:31 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/21/2022 08:04:31 - INFO - __main__ - ['contradiction']
05/21/2022 08:04:31 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/21/2022 08:04:31 - INFO - __main__ - ['contradiction']
05/21/2022 08:04:31 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/21/2022 08:04:31 - INFO - __main__ - ['contradiction']
05/21/2022 08:04:31 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:04:31 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:04:31 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:04:31 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:04:31 - INFO - __main__ - Printing 3 examples
05/21/2022 08:04:31 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
05/21/2022 08:04:31 - INFO - __main__ - ['contradiction']
05/21/2022 08:04:31 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/21/2022 08:04:31 - INFO - __main__ - ['contradiction']
05/21/2022 08:04:31 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
05/21/2022 08:04:31 - INFO - __main__ - ['contradiction']
05/21/2022 08:04:31 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:04:31 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:04:31 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:04:34 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 08:04:34 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 08:04:34 - INFO - __main__ - Printing 3 examples
05/21/2022 08:04:34 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 08:04:34 - INFO - __main__ - ['contradiction']
05/21/2022 08:04:34 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 08:04:34 - INFO - __main__ - ['neutral']
05/21/2022 08:04:34 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 08:04:34 - INFO - __main__ - ['entailment']
05/21/2022 08:04:34 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:04:34 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:04:34 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 08:04:35 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_100_0.0005_8_predictions.txt
05/21/2022 08:04:35 - INFO - __main__ - ACC on test data: 0.5000
05/21/2022 08:04:35 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.0005, bsz=8, dev_performance=0.5, test_performance=0.5
05/21/2022 08:04:35 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.0003, bsz=8 ...
05/21/2022 08:04:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:04:35 - INFO - __main__ - Starting training!
05/21/2022 08:04:36 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:04:36 - INFO - __main__ - Printing 3 examples
05/21/2022 08:04:36 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/21/2022 08:04:36 - INFO - __main__ - ['contradiction']
05/21/2022 08:04:36 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/21/2022 08:04:36 - INFO - __main__ - ['contradiction']
05/21/2022 08:04:36 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/21/2022 08:04:36 - INFO - __main__ - ['contradiction']
05/21/2022 08:04:36 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:04:36 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:04:36 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:04:36 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:04:36 - INFO - __main__ - Printing 3 examples
05/21/2022 08:04:36 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
05/21/2022 08:04:36 - INFO - __main__ - ['contradiction']
05/21/2022 08:04:36 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/21/2022 08:04:36 - INFO - __main__ - ['contradiction']
05/21/2022 08:04:36 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
05/21/2022 08:04:36 - INFO - __main__ - ['contradiction']
05/21/2022 08:04:36 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:04:36 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:04:36 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:04:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:04:40 - INFO - __main__ - Starting training!
05/21/2022 08:04:42 - INFO - __main__ - Step 10 Global step 10 Train loss 18.208471 on epoch=3
05/21/2022 08:04:45 - INFO - __main__ - Step 20 Global step 20 Train loss 13.665120 on epoch=6
05/21/2022 08:04:47 - INFO - __main__ - Step 30 Global step 30 Train loss 9.351595 on epoch=9
05/21/2022 08:04:49 - INFO - __main__ - Step 40 Global step 40 Train loss 6.736904 on epoch=13
05/21/2022 08:04:52 - INFO - __main__ - Step 50 Global step 50 Train loss 5.818716 on epoch=16
05/21/2022 08:04:52 - INFO - __main__ - Global step 50 Train loss 10.756161 ACC 0.03125 on epoch=16
05/21/2022 08:04:55 - INFO - __main__ - Step 60 Global step 60 Train loss 5.226154 on epoch=19
05/21/2022 08:04:58 - INFO - __main__ - Step 70 Global step 70 Train loss 2.961105 on epoch=23
05/21/2022 08:05:00 - INFO - __main__ - Step 80 Global step 80 Train loss 2.913536 on epoch=26
05/21/2022 08:05:03 - INFO - __main__ - Step 90 Global step 90 Train loss 2.068753 on epoch=29
05/21/2022 08:05:05 - INFO - __main__ - Step 100 Global step 100 Train loss 1.890414 on epoch=33
05/21/2022 08:05:05 - INFO - __main__ - Global step 100 Train loss 3.011992 ACC 0.5 on epoch=33
05/21/2022 08:05:09 - INFO - __main__ - Step 110 Global step 110 Train loss 1.782371 on epoch=36
05/21/2022 08:05:11 - INFO - __main__ - Step 120 Global step 120 Train loss 2.355693 on epoch=39
05/21/2022 08:05:14 - INFO - __main__ - Step 130 Global step 130 Train loss 2.002289 on epoch=43
05/21/2022 08:05:16 - INFO - __main__ - Step 140 Global step 140 Train loss 1.490078 on epoch=46
05/21/2022 08:05:19 - INFO - __main__ - Step 150 Global step 150 Train loss 1.517266 on epoch=49
05/21/2022 08:05:19 - INFO - __main__ - Global step 150 Train loss 1.829539 ACC 0.34375 on epoch=49
05/21/2022 08:05:21 - INFO - __main__ - Step 160 Global step 160 Train loss 1.107567 on epoch=53
05/21/2022 08:05:24 - INFO - __main__ - Step 170 Global step 170 Train loss 1.231424 on epoch=56
05/21/2022 08:05:27 - INFO - __main__ - Step 180 Global step 180 Train loss 1.117061 on epoch=59
05/21/2022 08:05:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.997883 on epoch=63
05/21/2022 08:05:32 - INFO - __main__ - Step 200 Global step 200 Train loss 1.232193 on epoch=66
05/21/2022 08:05:32 - INFO - __main__ - Global step 200 Train loss 1.137226 ACC 0.71875 on epoch=66
05/21/2022 08:05:35 - INFO - __main__ - Step 210 Global step 210 Train loss 1.171496 on epoch=69
05/21/2022 08:05:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.921945 on epoch=73
05/21/2022 08:05:40 - INFO - __main__ - Step 230 Global step 230 Train loss 1.154161 on epoch=76
05/21/2022 08:05:43 - INFO - __main__ - Step 240 Global step 240 Train loss 1.002902 on epoch=79
05/21/2022 08:05:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.908175 on epoch=83
05/21/2022 08:05:46 - INFO - __main__ - Global step 250 Train loss 1.031736 ACC 0.46875 on epoch=83
05/21/2022 08:05:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.959818 on epoch=86
05/21/2022 08:05:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.915545 on epoch=89
05/21/2022 08:05:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.825126 on epoch=93
05/21/2022 08:05:56 - INFO - __main__ - Step 290 Global step 290 Train loss 1.397204 on epoch=96
05/21/2022 08:05:58 - INFO - __main__ - Step 300 Global step 300 Train loss 2.373159 on epoch=99
05/21/2022 08:05:59 - INFO - __main__ - Global step 300 Train loss 1.294170 ACC 0.0 on epoch=99
05/21/2022 08:06:01 - INFO - __main__ - Step 310 Global step 310 Train loss 1.005992 on epoch=103
05/21/2022 08:06:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.800220 on epoch=106
05/21/2022 08:06:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.680669 on epoch=109
05/21/2022 08:06:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.549873 on epoch=113
05/21/2022 08:06:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.715907 on epoch=116
05/21/2022 08:06:12 - INFO - __main__ - Global step 350 Train loss 0.750532 ACC 0.21875 on epoch=116
05/21/2022 08:06:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.817438 on epoch=119
05/21/2022 08:06:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.526189 on epoch=123
05/21/2022 08:06:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.815544 on epoch=126
05/21/2022 08:06:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.689956 on epoch=129
05/21/2022 08:06:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.631201 on epoch=133
05/21/2022 08:06:25 - INFO - __main__ - Global step 400 Train loss 0.696066 ACC 0.4375 on epoch=133
05/21/2022 08:06:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.528320 on epoch=136
05/21/2022 08:06:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.598322 on epoch=139
05/21/2022 08:06:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.541725 on epoch=143
05/21/2022 08:06:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.621492 on epoch=146
05/21/2022 08:06:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.557846 on epoch=149
05/21/2022 08:06:38 - INFO - __main__ - Global step 450 Train loss 0.569541 ACC 0.40625 on epoch=149
05/21/2022 08:06:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.569996 on epoch=153
05/21/2022 08:06:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.582605 on epoch=156
05/21/2022 08:06:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.499054 on epoch=159
05/21/2022 08:06:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.559842 on epoch=163
05/21/2022 08:06:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.776790 on epoch=166
05/21/2022 08:06:51 - INFO - __main__ - Global step 500 Train loss 0.597657 ACC 0.375 on epoch=166
05/21/2022 08:06:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.439129 on epoch=169
05/21/2022 08:06:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.487116 on epoch=173
05/21/2022 08:06:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.402033 on epoch=176
05/21/2022 08:07:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.455119 on epoch=179
05/21/2022 08:07:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.501026 on epoch=183
05/21/2022 08:07:04 - INFO - __main__ - Global step 550 Train loss 0.456885 ACC 0.0625 on epoch=183
05/21/2022 08:07:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.471582 on epoch=186
05/21/2022 08:07:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.452876 on epoch=189
05/21/2022 08:07:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.462356 on epoch=193
05/21/2022 08:07:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.421879 on epoch=196
05/21/2022 08:07:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.445089 on epoch=199
05/21/2022 08:07:17 - INFO - __main__ - Global step 600 Train loss 0.450756 ACC 0.3125 on epoch=199
05/21/2022 08:07:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.439509 on epoch=203
05/21/2022 08:07:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.495654 on epoch=206
05/21/2022 08:07:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.438061 on epoch=209
05/21/2022 08:07:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.507439 on epoch=213
05/21/2022 08:07:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.420150 on epoch=216
05/21/2022 08:07:30 - INFO - __main__ - Global step 650 Train loss 0.460162 ACC 0.4375 on epoch=216
05/21/2022 08:07:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.384472 on epoch=219
05/21/2022 08:07:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.460968 on epoch=223
05/21/2022 08:07:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.411414 on epoch=226
05/21/2022 08:07:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.500018 on epoch=229
05/21/2022 08:07:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.379526 on epoch=233
05/21/2022 08:07:43 - INFO - __main__ - Global step 700 Train loss 0.427280 ACC 0.3125 on epoch=233
05/21/2022 08:07:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.348955 on epoch=236
05/21/2022 08:07:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.427538 on epoch=239
05/21/2022 08:07:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.477085 on epoch=243
05/21/2022 08:07:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.401981 on epoch=246
05/21/2022 08:07:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.402123 on epoch=249
05/21/2022 08:07:56 - INFO - __main__ - Global step 750 Train loss 0.411536 ACC 0.375 on epoch=249
05/21/2022 08:07:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.407357 on epoch=253
05/21/2022 08:08:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.419714 on epoch=256
05/21/2022 08:08:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.412050 on epoch=259
05/21/2022 08:08:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.410273 on epoch=263
05/21/2022 08:08:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.405645 on epoch=266
05/21/2022 08:08:09 - INFO - __main__ - Global step 800 Train loss 0.411008 ACC 0.46875 on epoch=266
05/21/2022 08:08:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.387894 on epoch=269
05/21/2022 08:08:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.415206 on epoch=273
05/21/2022 08:08:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.472360 on epoch=276
05/21/2022 08:08:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.371256 on epoch=279
05/21/2022 08:08:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.412997 on epoch=283
05/21/2022 08:08:22 - INFO - __main__ - Global step 850 Train loss 0.411943 ACC 0.0 on epoch=283
05/21/2022 08:08:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.420679 on epoch=286
05/21/2022 08:08:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.413472 on epoch=289
05/21/2022 08:08:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.340723 on epoch=293
05/21/2022 08:08:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.414929 on epoch=296
05/21/2022 08:08:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.425667 on epoch=299
05/21/2022 08:08:35 - INFO - __main__ - Global step 900 Train loss 0.403094 ACC 0.34375 on epoch=299
05/21/2022 08:08:35 - INFO - __main__ - save last model!
05/21/2022 08:08:36 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:08:36 - INFO - __main__ - Printing 3 examples
05/21/2022 08:08:36 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/21/2022 08:08:36 - INFO - __main__ - ['contradiction']
05/21/2022 08:08:36 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/21/2022 08:08:36 - INFO - __main__ - ['contradiction']
05/21/2022 08:08:36 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/21/2022 08:08:36 - INFO - __main__ - ['contradiction']
05/21/2022 08:08:36 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:08:36 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:08:36 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:08:36 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:08:36 - INFO - __main__ - Printing 3 examples
05/21/2022 08:08:36 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
05/21/2022 08:08:36 - INFO - __main__ - ['contradiction']
05/21/2022 08:08:36 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/21/2022 08:08:36 - INFO - __main__ - ['contradiction']
05/21/2022 08:08:36 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
05/21/2022 08:08:36 - INFO - __main__ - ['contradiction']
05/21/2022 08:08:36 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:08:36 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:08:36 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:08:38 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 08:08:38 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 08:08:38 - INFO - __main__ - Printing 3 examples
05/21/2022 08:08:38 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 08:08:38 - INFO - __main__ - ['contradiction']
05/21/2022 08:08:38 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 08:08:38 - INFO - __main__ - ['neutral']
05/21/2022 08:08:38 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 08:08:38 - INFO - __main__ - ['entailment']
05/21/2022 08:08:38 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:08:38 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:08:39 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 08:08:39 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_100_0.0003_8_predictions.txt
05/21/2022 08:08:39 - INFO - __main__ - ACC on test data: 0.6429
05/21/2022 08:08:39 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.0003, bsz=8, dev_performance=0.71875, test_performance=0.6428571428571429
05/21/2022 08:08:40 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.0002, bsz=8 ...
05/21/2022 08:08:40 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:08:40 - INFO - __main__ - Printing 3 examples
05/21/2022 08:08:40 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/21/2022 08:08:40 - INFO - __main__ - ['contradiction']
05/21/2022 08:08:40 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/21/2022 08:08:40 - INFO - __main__ - ['contradiction']
05/21/2022 08:08:40 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/21/2022 08:08:40 - INFO - __main__ - ['contradiction']
05/21/2022 08:08:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:08:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:08:40 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:08:40 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:08:40 - INFO - __main__ - Printing 3 examples
05/21/2022 08:08:40 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
05/21/2022 08:08:40 - INFO - __main__ - ['contradiction']
05/21/2022 08:08:40 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/21/2022 08:08:40 - INFO - __main__ - ['contradiction']
05/21/2022 08:08:40 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
05/21/2022 08:08:40 - INFO - __main__ - ['contradiction']
05/21/2022 08:08:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:08:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:08:41 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:08:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:08:41 - INFO - __main__ - Starting training!
05/21/2022 08:08:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:08:44 - INFO - __main__ - Starting training!
05/21/2022 08:08:46 - INFO - __main__ - Step 10 Global step 10 Train loss 17.816353 on epoch=3
05/21/2022 08:08:49 - INFO - __main__ - Step 20 Global step 20 Train loss 16.527905 on epoch=6
05/21/2022 08:08:51 - INFO - __main__ - Step 30 Global step 30 Train loss 11.381261 on epoch=9
05/21/2022 08:08:54 - INFO - __main__ - Step 40 Global step 40 Train loss 8.486368 on epoch=13
05/21/2022 08:08:56 - INFO - __main__ - Step 50 Global step 50 Train loss 8.205546 on epoch=16
05/21/2022 08:08:58 - INFO - __main__ - Global step 50 Train loss 12.483486 ACC 0.0 on epoch=16
05/21/2022 08:09:01 - INFO - __main__ - Step 60 Global step 60 Train loss 6.074880 on epoch=19
05/21/2022 08:09:04 - INFO - __main__ - Step 70 Global step 70 Train loss 5.492188 on epoch=23
05/21/2022 08:09:06 - INFO - __main__ - Step 80 Global step 80 Train loss 6.005363 on epoch=26
05/21/2022 08:09:09 - INFO - __main__ - Step 90 Global step 90 Train loss 5.648243 on epoch=29
05/21/2022 08:09:11 - INFO - __main__ - Step 100 Global step 100 Train loss 4.105854 on epoch=33
05/21/2022 08:09:12 - INFO - __main__ - Global step 100 Train loss 5.465305 ACC 0.375 on epoch=33
05/21/2022 08:09:15 - INFO - __main__ - Step 110 Global step 110 Train loss 3.792688 on epoch=36
05/21/2022 08:09:17 - INFO - __main__ - Step 120 Global step 120 Train loss 3.959990 on epoch=39
05/21/2022 08:09:20 - INFO - __main__ - Step 130 Global step 130 Train loss 2.982111 on epoch=43
05/21/2022 08:09:22 - INFO - __main__ - Step 140 Global step 140 Train loss 3.235652 on epoch=46
05/21/2022 08:09:25 - INFO - __main__ - Step 150 Global step 150 Train loss 2.626867 on epoch=49
05/21/2022 08:09:25 - INFO - __main__ - Global step 150 Train loss 3.319462 ACC 0.46875 on epoch=49
05/21/2022 08:09:28 - INFO - __main__ - Step 160 Global step 160 Train loss 2.045851 on epoch=53
05/21/2022 08:09:31 - INFO - __main__ - Step 170 Global step 170 Train loss 2.526281 on epoch=56
05/21/2022 08:09:33 - INFO - __main__ - Step 180 Global step 180 Train loss 2.534952 on epoch=59
05/21/2022 08:09:36 - INFO - __main__ - Step 190 Global step 190 Train loss 2.323677 on epoch=63
05/21/2022 08:09:38 - INFO - __main__ - Step 200 Global step 200 Train loss 1.845547 on epoch=66
05/21/2022 08:09:38 - INFO - __main__ - Global step 200 Train loss 2.255261 ACC 0.5 on epoch=66
05/21/2022 08:09:41 - INFO - __main__ - Step 210 Global step 210 Train loss 2.543047 on epoch=69
05/21/2022 08:09:44 - INFO - __main__ - Step 220 Global step 220 Train loss 2.087103 on epoch=73
05/21/2022 08:09:46 - INFO - __main__ - Step 230 Global step 230 Train loss 1.651908 on epoch=76
05/21/2022 08:09:49 - INFO - __main__ - Step 240 Global step 240 Train loss 2.185256 on epoch=79
05/21/2022 08:09:51 - INFO - __main__ - Step 250 Global step 250 Train loss 1.981224 on epoch=83
05/21/2022 08:09:52 - INFO - __main__ - Global step 250 Train loss 2.089708 ACC 0.46875 on epoch=83
05/21/2022 08:09:54 - INFO - __main__ - Step 260 Global step 260 Train loss 1.136929 on epoch=86
05/21/2022 08:09:57 - INFO - __main__ - Step 270 Global step 270 Train loss 1.796272 on epoch=89
05/21/2022 08:09:59 - INFO - __main__ - Step 280 Global step 280 Train loss 1.440464 on epoch=93
05/21/2022 08:10:02 - INFO - __main__ - Step 290 Global step 290 Train loss 1.730653 on epoch=96
05/21/2022 08:10:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.996431 on epoch=99
05/21/2022 08:10:05 - INFO - __main__ - Global step 300 Train loss 1.420150 ACC 0.0625 on epoch=99
05/21/2022 08:10:07 - INFO - __main__ - Step 310 Global step 310 Train loss 1.085885 on epoch=103
05/21/2022 08:10:10 - INFO - __main__ - Step 320 Global step 320 Train loss 1.268331 on epoch=106
05/21/2022 08:10:12 - INFO - __main__ - Step 330 Global step 330 Train loss 1.418426 on epoch=109
05/21/2022 08:10:15 - INFO - __main__ - Step 340 Global step 340 Train loss 1.181787 on epoch=113
05/21/2022 08:10:17 - INFO - __main__ - Step 350 Global step 350 Train loss 1.226343 on epoch=116
05/21/2022 08:10:18 - INFO - __main__ - Global step 350 Train loss 1.236154 ACC 0.46875 on epoch=116
05/21/2022 08:10:20 - INFO - __main__ - Step 360 Global step 360 Train loss 1.413497 on epoch=119
05/21/2022 08:10:23 - INFO - __main__ - Step 370 Global step 370 Train loss 1.294210 on epoch=123
05/21/2022 08:10:25 - INFO - __main__ - Step 380 Global step 380 Train loss 1.047632 on epoch=126
05/21/2022 08:10:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.730776 on epoch=129
05/21/2022 08:10:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.854072 on epoch=133
05/21/2022 08:10:31 - INFO - __main__ - Global step 400 Train loss 1.068038 ACC 0.71875 on epoch=133
05/21/2022 08:10:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.981441 on epoch=136
05/21/2022 08:10:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.776958 on epoch=139
05/21/2022 08:10:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.864409 on epoch=143
05/21/2022 08:10:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.947666 on epoch=146
05/21/2022 08:10:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.769707 on epoch=149
05/21/2022 08:10:44 - INFO - __main__ - Global step 450 Train loss 0.868036 ACC 0.84375 on epoch=149
05/21/2022 08:10:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.543331 on epoch=153
05/21/2022 08:10:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.696148 on epoch=156
05/21/2022 08:10:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.762256 on epoch=159
05/21/2022 08:10:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.745769 on epoch=163
05/21/2022 08:10:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.872445 on epoch=166
05/21/2022 08:10:58 - INFO - __main__ - Global step 500 Train loss 0.723990 ACC 0.8125 on epoch=166
05/21/2022 08:11:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.437544 on epoch=169
05/21/2022 08:11:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.506833 on epoch=173
05/21/2022 08:11:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.581323 on epoch=176
05/21/2022 08:11:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.671546 on epoch=179
05/21/2022 08:11:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.388590 on epoch=183
05/21/2022 08:11:11 - INFO - __main__ - Global step 550 Train loss 0.517167 ACC 0.8125 on epoch=183
05/21/2022 08:11:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.379787 on epoch=186
05/21/2022 08:11:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.403136 on epoch=189
05/21/2022 08:11:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.314300 on epoch=193
05/21/2022 08:11:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.494393 on epoch=196
05/21/2022 08:11:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.535627 on epoch=199
05/21/2022 08:11:24 - INFO - __main__ - Global step 600 Train loss 0.425449 ACC 0.84375 on epoch=199
05/21/2022 08:11:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.456056 on epoch=203
05/21/2022 08:11:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.407678 on epoch=206
05/21/2022 08:11:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.465351 on epoch=209
05/21/2022 08:11:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.332267 on epoch=213
05/21/2022 08:11:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.216313 on epoch=216
05/21/2022 08:11:37 - INFO - __main__ - Global step 650 Train loss 0.375533 ACC 0.8125 on epoch=216
05/21/2022 08:11:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.276408 on epoch=219
05/21/2022 08:11:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.232203 on epoch=223
05/21/2022 08:11:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.317603 on epoch=226
05/21/2022 08:11:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.222574 on epoch=229
05/21/2022 08:11:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.186442 on epoch=233
05/21/2022 08:11:51 - INFO - __main__ - Global step 700 Train loss 0.247046 ACC 0.84375 on epoch=233
05/21/2022 08:11:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.189474 on epoch=236
05/21/2022 08:11:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.248456 on epoch=239
05/21/2022 08:11:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.171288 on epoch=243
05/21/2022 08:12:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.324983 on epoch=246
05/21/2022 08:12:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.369148 on epoch=249
05/21/2022 08:12:04 - INFO - __main__ - Global step 750 Train loss 0.260670 ACC 0.8125 on epoch=249
05/21/2022 08:12:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.262367 on epoch=253
05/21/2022 08:12:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.467077 on epoch=256
05/21/2022 08:12:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.577964 on epoch=259
05/21/2022 08:12:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.664171 on epoch=263
05/21/2022 08:12:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.607810 on epoch=266
05/21/2022 08:12:17 - INFO - __main__ - Global step 800 Train loss 0.515878 ACC 0.71875 on epoch=266
05/21/2022 08:12:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.388941 on epoch=269
05/21/2022 08:12:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.398409 on epoch=273
05/21/2022 08:12:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.510049 on epoch=276
05/21/2022 08:12:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.447020 on epoch=279
05/21/2022 08:12:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.543881 on epoch=283
05/21/2022 08:12:30 - INFO - __main__ - Global step 850 Train loss 0.457660 ACC 0.6875 on epoch=283
05/21/2022 08:12:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.481822 on epoch=286
05/21/2022 08:12:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.547985 on epoch=289
05/21/2022 08:12:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.591086 on epoch=293
05/21/2022 08:12:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.560733 on epoch=296
05/21/2022 08:12:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.387409 on epoch=299
05/21/2022 08:12:43 - INFO - __main__ - Global step 900 Train loss 0.513807 ACC 0.71875 on epoch=299
05/21/2022 08:12:43 - INFO - __main__ - save last model!
05/21/2022 08:12:44 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:12:44 - INFO - __main__ - Printing 3 examples
05/21/2022 08:12:44 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/21/2022 08:12:44 - INFO - __main__ - ['contradiction']
05/21/2022 08:12:44 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/21/2022 08:12:44 - INFO - __main__ - ['contradiction']
05/21/2022 08:12:44 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/21/2022 08:12:44 - INFO - __main__ - ['contradiction']
05/21/2022 08:12:44 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:12:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:12:44 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:12:44 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:12:44 - INFO - __main__ - Printing 3 examples
05/21/2022 08:12:44 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
05/21/2022 08:12:44 - INFO - __main__ - ['contradiction']
05/21/2022 08:12:44 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/21/2022 08:12:44 - INFO - __main__ - ['contradiction']
05/21/2022 08:12:44 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
05/21/2022 08:12:44 - INFO - __main__ - ['contradiction']
05/21/2022 08:12:44 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:12:44 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:12:44 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:12:46 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 08:12:46 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 08:12:46 - INFO - __main__ - Printing 3 examples
05/21/2022 08:12:46 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 08:12:46 - INFO - __main__ - ['contradiction']
05/21/2022 08:12:46 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 08:12:46 - INFO - __main__ - ['neutral']
05/21/2022 08:12:46 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 08:12:46 - INFO - __main__ - ['entailment']
05/21/2022 08:12:46 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:12:46 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:12:46 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 08:12:47 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_100_0.0002_8_predictions.txt
05/21/2022 08:12:47 - INFO - __main__ - ACC on test data: 0.7321
05/21/2022 08:12:47 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.0002, bsz=8, dev_performance=0.84375, test_performance=0.7321428571428571
05/21/2022 08:12:47 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.0001, bsz=8 ...
05/21/2022 08:12:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:12:48 - INFO - __main__ - Starting training!
05/21/2022 08:12:48 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:12:48 - INFO - __main__ - Printing 3 examples
05/21/2022 08:12:48 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/21/2022 08:12:48 - INFO - __main__ - ['contradiction']
05/21/2022 08:12:48 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
05/21/2022 08:12:48 - INFO - __main__ - ['contradiction']
05/21/2022 08:12:48 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
05/21/2022 08:12:48 - INFO - __main__ - ['contradiction']
05/21/2022 08:12:48 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:12:48 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:12:48 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:12:48 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:12:48 - INFO - __main__ - Printing 3 examples
05/21/2022 08:12:48 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
05/21/2022 08:12:48 - INFO - __main__ - ['contradiction']
05/21/2022 08:12:48 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/21/2022 08:12:48 - INFO - __main__ - ['contradiction']
05/21/2022 08:12:48 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
05/21/2022 08:12:48 - INFO - __main__ - ['contradiction']
05/21/2022 08:12:48 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:12:48 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:12:48 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:12:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:12:53 - INFO - __main__ - Starting training!
05/21/2022 08:12:55 - INFO - __main__ - Step 10 Global step 10 Train loss 18.054287 on epoch=3
05/21/2022 08:12:57 - INFO - __main__ - Step 20 Global step 20 Train loss 16.446690 on epoch=6
05/21/2022 08:13:00 - INFO - __main__ - Step 30 Global step 30 Train loss 15.013252 on epoch=9
05/21/2022 08:13:02 - INFO - __main__ - Step 40 Global step 40 Train loss 12.680166 on epoch=13
05/21/2022 08:13:05 - INFO - __main__ - Step 50 Global step 50 Train loss 11.581114 on epoch=16
05/21/2022 08:13:08 - INFO - __main__ - Global step 50 Train loss 14.755101 ACC 0.0 on epoch=16
05/21/2022 08:13:11 - INFO - __main__ - Step 60 Global step 60 Train loss 9.765803 on epoch=19
05/21/2022 08:13:14 - INFO - __main__ - Step 70 Global step 70 Train loss 10.139643 on epoch=23
05/21/2022 08:13:16 - INFO - __main__ - Step 80 Global step 80 Train loss 8.021867 on epoch=26
05/21/2022 08:13:19 - INFO - __main__ - Step 90 Global step 90 Train loss 8.190500 on epoch=29
05/21/2022 08:13:21 - INFO - __main__ - Step 100 Global step 100 Train loss 7.388425 on epoch=33
05/21/2022 08:13:24 - INFO - __main__ - Global step 100 Train loss 8.701247 ACC 0.0 on epoch=33
05/21/2022 08:13:26 - INFO - __main__ - Step 110 Global step 110 Train loss 6.839972 on epoch=36
05/21/2022 08:13:29 - INFO - __main__ - Step 120 Global step 120 Train loss 6.982501 on epoch=39
05/21/2022 08:13:31 - INFO - __main__ - Step 130 Global step 130 Train loss 6.637609 on epoch=43
05/21/2022 08:13:34 - INFO - __main__ - Step 140 Global step 140 Train loss 5.451562 on epoch=46
05/21/2022 08:13:36 - INFO - __main__ - Step 150 Global step 150 Train loss 5.388244 on epoch=49
05/21/2022 08:13:37 - INFO - __main__ - Global step 150 Train loss 6.259977 ACC 0.0 on epoch=49
05/21/2022 08:13:40 - INFO - __main__ - Step 160 Global step 160 Train loss 5.812444 on epoch=53
05/21/2022 08:13:43 - INFO - __main__ - Step 170 Global step 170 Train loss 5.167206 on epoch=56
05/21/2022 08:13:45 - INFO - __main__ - Step 180 Global step 180 Train loss 4.528750 on epoch=59
05/21/2022 08:13:48 - INFO - __main__ - Step 190 Global step 190 Train loss 4.402617 on epoch=63
05/21/2022 08:13:50 - INFO - __main__ - Step 200 Global step 200 Train loss 4.185829 on epoch=66
05/21/2022 08:13:51 - INFO - __main__ - Global step 200 Train loss 4.819369 ACC 0.0625 on epoch=66
05/21/2022 08:13:54 - INFO - __main__ - Step 210 Global step 210 Train loss 3.376174 on epoch=69
05/21/2022 08:13:57 - INFO - __main__ - Step 220 Global step 220 Train loss 3.308680 on epoch=73
05/21/2022 08:13:59 - INFO - __main__ - Step 230 Global step 230 Train loss 4.393818 on epoch=76
05/21/2022 08:14:02 - INFO - __main__ - Step 240 Global step 240 Train loss 3.173639 on epoch=79
05/21/2022 08:14:04 - INFO - __main__ - Step 250 Global step 250 Train loss 3.273522 on epoch=83
05/21/2022 08:14:05 - INFO - __main__ - Global step 250 Train loss 3.505167 ACC 0.0 on epoch=83
05/21/2022 08:14:07 - INFO - __main__ - Step 260 Global step 260 Train loss 2.966399 on epoch=86
05/21/2022 08:14:10 - INFO - __main__ - Step 270 Global step 270 Train loss 2.487091 on epoch=89
05/21/2022 08:14:12 - INFO - __main__ - Step 280 Global step 280 Train loss 2.652775 on epoch=93
05/21/2022 08:14:15 - INFO - __main__ - Step 290 Global step 290 Train loss 2.927990 on epoch=96
05/21/2022 08:14:18 - INFO - __main__ - Step 300 Global step 300 Train loss 2.788317 on epoch=99
05/21/2022 08:14:18 - INFO - __main__ - Global step 300 Train loss 2.764514 ACC 0.5 on epoch=99
05/21/2022 08:14:21 - INFO - __main__ - Step 310 Global step 310 Train loss 3.056637 on epoch=103
05/21/2022 08:14:23 - INFO - __main__ - Step 320 Global step 320 Train loss 3.132118 on epoch=106
05/21/2022 08:14:26 - INFO - __main__ - Step 330 Global step 330 Train loss 2.313915 on epoch=109
05/21/2022 08:14:28 - INFO - __main__ - Step 340 Global step 340 Train loss 2.268750 on epoch=113
05/21/2022 08:14:31 - INFO - __main__ - Step 350 Global step 350 Train loss 2.521248 on epoch=116
05/21/2022 08:14:31 - INFO - __main__ - Global step 350 Train loss 2.658534 ACC 0.5 on epoch=116
05/21/2022 08:14:34 - INFO - __main__ - Step 360 Global step 360 Train loss 2.793455 on epoch=119
05/21/2022 08:14:36 - INFO - __main__ - Step 370 Global step 370 Train loss 2.391754 on epoch=123
05/21/2022 08:14:39 - INFO - __main__ - Step 380 Global step 380 Train loss 2.944279 on epoch=126
05/21/2022 08:14:41 - INFO - __main__ - Step 390 Global step 390 Train loss 2.050341 on epoch=129
05/21/2022 08:14:44 - INFO - __main__ - Step 400 Global step 400 Train loss 2.500139 on epoch=133
05/21/2022 08:14:44 - INFO - __main__ - Global step 400 Train loss 2.535994 ACC 0.5 on epoch=133
05/21/2022 08:14:46 - INFO - __main__ - Step 410 Global step 410 Train loss 1.814083 on epoch=136
05/21/2022 08:14:49 - INFO - __main__ - Step 420 Global step 420 Train loss 1.884964 on epoch=139
05/21/2022 08:14:52 - INFO - __main__ - Step 430 Global step 430 Train loss 1.847994 on epoch=143
05/21/2022 08:14:54 - INFO - __main__ - Step 440 Global step 440 Train loss 2.098856 on epoch=146
05/21/2022 08:14:57 - INFO - __main__ - Step 450 Global step 450 Train loss 2.014855 on epoch=149
05/21/2022 08:14:57 - INFO - __main__ - Global step 450 Train loss 1.932151 ACC 0.5 on epoch=149
05/21/2022 08:14:59 - INFO - __main__ - Step 460 Global step 460 Train loss 1.336164 on epoch=153
05/21/2022 08:15:02 - INFO - __main__ - Step 470 Global step 470 Train loss 2.067892 on epoch=156
05/21/2022 08:15:04 - INFO - __main__ - Step 480 Global step 480 Train loss 1.630446 on epoch=159
05/21/2022 08:15:07 - INFO - __main__ - Step 490 Global step 490 Train loss 2.370286 on epoch=163
05/21/2022 08:15:10 - INFO - __main__ - Step 500 Global step 500 Train loss 1.703479 on epoch=166
05/21/2022 08:15:10 - INFO - __main__ - Global step 500 Train loss 1.821653 ACC 0.4375 on epoch=166
05/21/2022 08:15:12 - INFO - __main__ - Step 510 Global step 510 Train loss 1.787974 on epoch=169
05/21/2022 08:15:15 - INFO - __main__ - Step 520 Global step 520 Train loss 1.677778 on epoch=173
05/21/2022 08:15:17 - INFO - __main__ - Step 530 Global step 530 Train loss 1.427010 on epoch=176
05/21/2022 08:15:20 - INFO - __main__ - Step 540 Global step 540 Train loss 1.739063 on epoch=179
05/21/2022 08:15:22 - INFO - __main__ - Step 550 Global step 550 Train loss 1.877576 on epoch=183
05/21/2022 08:15:23 - INFO - __main__ - Global step 550 Train loss 1.701880 ACC 0.21875 on epoch=183
05/21/2022 08:15:25 - INFO - __main__ - Step 560 Global step 560 Train loss 1.597274 on epoch=186
05/21/2022 08:15:28 - INFO - __main__ - Step 570 Global step 570 Train loss 1.841639 on epoch=189
05/21/2022 08:15:30 - INFO - __main__ - Step 580 Global step 580 Train loss 1.456765 on epoch=193
05/21/2022 08:15:33 - INFO - __main__ - Step 590 Global step 590 Train loss 1.075347 on epoch=196
05/21/2022 08:15:35 - INFO - __main__ - Step 600 Global step 600 Train loss 1.354891 on epoch=199
05/21/2022 08:15:36 - INFO - __main__ - Global step 600 Train loss 1.465183 ACC 0.46875 on epoch=199
05/21/2022 08:15:38 - INFO - __main__ - Step 610 Global step 610 Train loss 1.078667 on epoch=203
05/21/2022 08:15:41 - INFO - __main__ - Step 620 Global step 620 Train loss 1.444024 on epoch=206
05/21/2022 08:15:43 - INFO - __main__ - Step 630 Global step 630 Train loss 1.270421 on epoch=209
05/21/2022 08:15:46 - INFO - __main__ - Step 640 Global step 640 Train loss 1.136547 on epoch=213
05/21/2022 08:15:48 - INFO - __main__ - Step 650 Global step 650 Train loss 1.041497 on epoch=216
05/21/2022 08:15:49 - INFO - __main__ - Global step 650 Train loss 1.194231 ACC 0.5625 on epoch=216
05/21/2022 08:15:52 - INFO - __main__ - Step 660 Global step 660 Train loss 1.219794 on epoch=219
05/21/2022 08:15:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.961019 on epoch=223
05/21/2022 08:15:57 - INFO - __main__ - Step 680 Global step 680 Train loss 1.124922 on epoch=226
05/21/2022 08:15:59 - INFO - __main__ - Step 690 Global step 690 Train loss 1.117065 on epoch=229
05/21/2022 08:16:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.962196 on epoch=233
05/21/2022 08:16:02 - INFO - __main__ - Global step 700 Train loss 1.076999 ACC 0.40625 on epoch=233
05/21/2022 08:16:04 - INFO - __main__ - Step 710 Global step 710 Train loss 1.157532 on epoch=236
05/21/2022 08:16:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.931941 on epoch=239
05/21/2022 08:16:10 - INFO - __main__ - Step 730 Global step 730 Train loss 1.032238 on epoch=243
05/21/2022 08:16:12 - INFO - __main__ - Step 740 Global step 740 Train loss 1.236868 on epoch=246
05/21/2022 08:16:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.653850 on epoch=249
05/21/2022 08:16:15 - INFO - __main__ - Global step 750 Train loss 1.002486 ACC 0.5625 on epoch=249
05/21/2022 08:16:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.912768 on epoch=253
05/21/2022 08:16:20 - INFO - __main__ - Step 770 Global step 770 Train loss 1.055803 on epoch=256
05/21/2022 08:16:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.897315 on epoch=259
05/21/2022 08:16:25 - INFO - __main__ - Step 790 Global step 790 Train loss 0.677077 on epoch=263
05/21/2022 08:16:28 - INFO - __main__ - Step 800 Global step 800 Train loss 0.882948 on epoch=266
05/21/2022 08:16:28 - INFO - __main__ - Global step 800 Train loss 0.885182 ACC 0.65625 on epoch=266
05/21/2022 08:16:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.984336 on epoch=269
05/21/2022 08:16:33 - INFO - __main__ - Step 820 Global step 820 Train loss 1.033654 on epoch=273
05/21/2022 08:16:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.587646 on epoch=276
05/21/2022 08:16:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.679590 on epoch=279
05/21/2022 08:16:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.783383 on epoch=283
05/21/2022 08:16:41 - INFO - __main__ - Global step 850 Train loss 0.813722 ACC 0.78125 on epoch=283
05/21/2022 08:16:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.621058 on epoch=286
05/21/2022 08:16:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.827568 on epoch=289
05/21/2022 08:16:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.584950 on epoch=293
05/21/2022 08:16:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.553511 on epoch=296
05/21/2022 08:16:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.711247 on epoch=299
05/21/2022 08:16:55 - INFO - __main__ - Global step 900 Train loss 0.659667 ACC 0.78125 on epoch=299
05/21/2022 08:16:55 - INFO - __main__ - save last model!
05/21/2022 08:16:55 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:16:55 - INFO - __main__ - Printing 3 examples
05/21/2022 08:16:55 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/21/2022 08:16:55 - INFO - __main__ - ['contradiction']
05/21/2022 08:16:55 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/21/2022 08:16:55 - INFO - __main__ - ['contradiction']
05/21/2022 08:16:55 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/21/2022 08:16:55 - INFO - __main__ - ['contradiction']
05/21/2022 08:16:55 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:16:55 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:16:55 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:16:55 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:16:55 - INFO - __main__ - Printing 3 examples
05/21/2022 08:16:55 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
05/21/2022 08:16:55 - INFO - __main__ - ['contradiction']
05/21/2022 08:16:55 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/21/2022 08:16:55 - INFO - __main__ - ['contradiction']
05/21/2022 08:16:55 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/21/2022 08:16:55 - INFO - __main__ - ['contradiction']
05/21/2022 08:16:55 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:16:55 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:16:55 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:16:57 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 08:16:57 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 08:16:57 - INFO - __main__ - Printing 3 examples
05/21/2022 08:16:57 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 08:16:57 - INFO - __main__ - ['contradiction']
05/21/2022 08:16:57 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 08:16:57 - INFO - __main__ - ['neutral']
05/21/2022 08:16:57 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 08:16:57 - INFO - __main__ - ['entailment']
05/21/2022 08:16:57 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:16:57 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:16:57 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 08:16:58 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_100_0.0001_8_predictions.txt
05/21/2022 08:16:58 - INFO - __main__ - ACC on test data: 0.7500
05/21/2022 08:16:58 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.0001, bsz=8, dev_performance=0.78125, test_performance=0.75
05/21/2022 08:16:58 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.0005, bsz=8 ...
05/21/2022 08:16:59 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:16:59 - INFO - __main__ - Printing 3 examples
05/21/2022 08:16:59 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/21/2022 08:16:59 - INFO - __main__ - ['contradiction']
05/21/2022 08:16:59 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/21/2022 08:16:59 - INFO - __main__ - ['contradiction']
05/21/2022 08:16:59 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/21/2022 08:16:59 - INFO - __main__ - ['contradiction']
05/21/2022 08:16:59 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:16:59 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:16:59 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:16:59 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:16:59 - INFO - __main__ - Printing 3 examples
05/21/2022 08:16:59 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
05/21/2022 08:16:59 - INFO - __main__ - ['contradiction']
05/21/2022 08:16:59 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/21/2022 08:16:59 - INFO - __main__ - ['contradiction']
05/21/2022 08:16:59 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/21/2022 08:16:59 - INFO - __main__ - ['contradiction']
05/21/2022 08:16:59 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:16:59 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:16:59 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:17:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:17:00 - INFO - __main__ - Starting training!
05/21/2022 08:17:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:17:04 - INFO - __main__ - Starting training!
05/21/2022 08:17:06 - INFO - __main__ - Step 10 Global step 10 Train loss 18.985415 on epoch=3
05/21/2022 08:17:08 - INFO - __main__ - Step 20 Global step 20 Train loss 14.888077 on epoch=6
05/21/2022 08:17:11 - INFO - __main__ - Step 30 Global step 30 Train loss 10.078260 on epoch=9
05/21/2022 08:17:13 - INFO - __main__ - Step 40 Global step 40 Train loss 8.068144 on epoch=13
05/21/2022 08:17:16 - INFO - __main__ - Step 50 Global step 50 Train loss 5.601339 on epoch=16
05/21/2022 08:17:16 - INFO - __main__ - Global step 50 Train loss 11.524248 ACC 0.03125 on epoch=16
05/21/2022 08:17:19 - INFO - __main__ - Step 60 Global step 60 Train loss 4.496961 on epoch=19
05/21/2022 08:17:21 - INFO - __main__ - Step 70 Global step 70 Train loss 3.227134 on epoch=23
05/21/2022 08:17:24 - INFO - __main__ - Step 80 Global step 80 Train loss 2.906109 on epoch=26
05/21/2022 08:17:27 - INFO - __main__ - Step 90 Global step 90 Train loss 2.684344 on epoch=29
05/21/2022 08:17:29 - INFO - __main__ - Step 100 Global step 100 Train loss 2.571135 on epoch=33
05/21/2022 08:17:29 - INFO - __main__ - Global step 100 Train loss 3.177137 ACC 0.09375 on epoch=33
05/21/2022 08:17:32 - INFO - __main__ - Step 110 Global step 110 Train loss 2.711050 on epoch=36
05/21/2022 08:17:35 - INFO - __main__ - Step 120 Global step 120 Train loss 2.245724 on epoch=39
05/21/2022 08:17:37 - INFO - __main__ - Step 130 Global step 130 Train loss 2.418781 on epoch=43
05/21/2022 08:17:40 - INFO - __main__ - Step 140 Global step 140 Train loss 1.578905 on epoch=46
05/21/2022 08:17:43 - INFO - __main__ - Step 150 Global step 150 Train loss 1.676890 on epoch=49
05/21/2022 08:17:43 - INFO - __main__ - Global step 150 Train loss 2.126270 ACC 0.3125 on epoch=49
05/21/2022 08:17:46 - INFO - __main__ - Step 160 Global step 160 Train loss 1.718027 on epoch=53
05/21/2022 08:17:48 - INFO - __main__ - Step 170 Global step 170 Train loss 2.042394 on epoch=56
05/21/2022 08:17:51 - INFO - __main__ - Step 180 Global step 180 Train loss 1.927824 on epoch=59
05/21/2022 08:17:53 - INFO - __main__ - Step 190 Global step 190 Train loss 1.313477 on epoch=63
05/21/2022 08:17:56 - INFO - __main__ - Step 200 Global step 200 Train loss 1.136946 on epoch=66
05/21/2022 08:17:56 - INFO - __main__ - Global step 200 Train loss 1.627734 ACC 0.40625 on epoch=66
05/21/2022 08:17:59 - INFO - __main__ - Step 210 Global step 210 Train loss 1.221123 on epoch=69
05/21/2022 08:18:02 - INFO - __main__ - Step 220 Global step 220 Train loss 1.125398 on epoch=73
05/21/2022 08:18:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.833160 on epoch=76
05/21/2022 08:18:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.964339 on epoch=79
05/21/2022 08:18:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.980548 on epoch=83
05/21/2022 08:18:10 - INFO - __main__ - Global step 250 Train loss 1.024914 ACC 0.21875 on epoch=83
05/21/2022 08:18:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.986872 on epoch=86
05/21/2022 08:18:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.677894 on epoch=89
05/21/2022 08:18:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.885783 on epoch=93
05/21/2022 08:18:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.822485 on epoch=96
05/21/2022 08:18:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.923253 on epoch=99
05/21/2022 08:18:23 - INFO - __main__ - Global step 300 Train loss 0.859257 ACC 0.625 on epoch=99
05/21/2022 08:18:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.781494 on epoch=103
05/21/2022 08:18:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.726464 on epoch=106
05/21/2022 08:18:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.754084 on epoch=109
05/21/2022 08:18:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.859487 on epoch=113
05/21/2022 08:18:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.676508 on epoch=116
05/21/2022 08:18:36 - INFO - __main__ - Global step 350 Train loss 0.759607 ACC 0.375 on epoch=116
05/21/2022 08:18:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.696022 on epoch=119
05/21/2022 08:18:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.797419 on epoch=123
05/21/2022 08:18:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.670214 on epoch=126
05/21/2022 08:18:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.626481 on epoch=129
05/21/2022 08:18:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.683771 on epoch=133
05/21/2022 08:18:50 - INFO - __main__ - Global step 400 Train loss 0.694781 ACC 0.09375 on epoch=133
05/21/2022 08:18:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.522682 on epoch=136
05/21/2022 08:18:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.609015 on epoch=139
05/21/2022 08:18:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.461989 on epoch=143
05/21/2022 08:19:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.632190 on epoch=146
05/21/2022 08:19:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.596123 on epoch=149
05/21/2022 08:19:03 - INFO - __main__ - Global step 450 Train loss 0.564400 ACC 0.5 on epoch=149
05/21/2022 08:19:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.571454 on epoch=153
05/21/2022 08:19:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.482098 on epoch=156
05/21/2022 08:19:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.536659 on epoch=159
05/21/2022 08:19:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.587210 on epoch=163
05/21/2022 08:19:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.445608 on epoch=166
05/21/2022 08:19:16 - INFO - __main__ - Global step 500 Train loss 0.524606 ACC 0.65625 on epoch=166
05/21/2022 08:19:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.518754 on epoch=169
05/21/2022 08:19:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.477250 on epoch=173
05/21/2022 08:19:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.445129 on epoch=176
05/21/2022 08:19:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.433993 on epoch=179
05/21/2022 08:19:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.442273 on epoch=183
05/21/2022 08:19:29 - INFO - __main__ - Global step 550 Train loss 0.463480 ACC 0.28125 on epoch=183
05/21/2022 08:19:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.390440 on epoch=186
05/21/2022 08:19:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.366602 on epoch=189
05/21/2022 08:19:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.479831 on epoch=193
05/21/2022 08:19:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.519144 on epoch=196
05/21/2022 08:19:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.465466 on epoch=199
05/21/2022 08:19:42 - INFO - __main__ - Global step 600 Train loss 0.444297 ACC 0.59375 on epoch=199
05/21/2022 08:19:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.362469 on epoch=203
05/21/2022 08:19:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.430625 on epoch=206
05/21/2022 08:19:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.414787 on epoch=209
05/21/2022 08:19:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.329069 on epoch=213
05/21/2022 08:19:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.373812 on epoch=216
05/21/2022 08:19:55 - INFO - __main__ - Global step 650 Train loss 0.382152 ACC 0.4375 on epoch=216
05/21/2022 08:19:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.421138 on epoch=219
05/21/2022 08:20:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.379307 on epoch=223
05/21/2022 08:20:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.420022 on epoch=226
05/21/2022 08:20:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.355593 on epoch=229
05/21/2022 08:20:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.408647 on epoch=233
05/21/2022 08:20:09 - INFO - __main__ - Global step 700 Train loss 0.396941 ACC 0.3125 on epoch=233
05/21/2022 08:20:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.364875 on epoch=236
05/21/2022 08:20:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.413566 on epoch=239
05/21/2022 08:20:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.357795 on epoch=243
05/21/2022 08:20:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.406995 on epoch=246
05/21/2022 08:20:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.411584 on epoch=249
05/21/2022 08:20:22 - INFO - __main__ - Global step 750 Train loss 0.390963 ACC 0.5 on epoch=249
05/21/2022 08:20:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.337275 on epoch=253
05/21/2022 08:20:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.344681 on epoch=256
05/21/2022 08:20:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.363345 on epoch=259
05/21/2022 08:20:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.375351 on epoch=263
05/21/2022 08:20:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.314940 on epoch=266
05/21/2022 08:20:35 - INFO - __main__ - Global step 800 Train loss 0.347118 ACC 0.65625 on epoch=266
05/21/2022 08:20:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.304875 on epoch=269
05/21/2022 08:20:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.326304 on epoch=273
05/21/2022 08:20:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.310078 on epoch=276
05/21/2022 08:20:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.348819 on epoch=279
05/21/2022 08:20:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.397507 on epoch=283
05/21/2022 08:20:48 - INFO - __main__ - Global step 850 Train loss 0.337516 ACC 0.28125 on epoch=283
05/21/2022 08:20:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.327003 on epoch=286
05/21/2022 08:20:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.361494 on epoch=289
05/21/2022 08:20:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.280911 on epoch=293
05/21/2022 08:20:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.336863 on epoch=296
05/21/2022 08:21:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.360697 on epoch=299
05/21/2022 08:21:02 - INFO - __main__ - Global step 900 Train loss 0.333394 ACC 0.28125 on epoch=299
05/21/2022 08:21:02 - INFO - __main__ - save last model!
05/21/2022 08:21:02 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:21:02 - INFO - __main__ - Printing 3 examples
05/21/2022 08:21:02 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/21/2022 08:21:02 - INFO - __main__ - ['contradiction']
05/21/2022 08:21:02 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/21/2022 08:21:02 - INFO - __main__ - ['contradiction']
05/21/2022 08:21:02 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/21/2022 08:21:02 - INFO - __main__ - ['contradiction']
05/21/2022 08:21:02 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:21:02 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:21:02 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:21:02 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:21:02 - INFO - __main__ - Printing 3 examples
05/21/2022 08:21:02 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
05/21/2022 08:21:02 - INFO - __main__ - ['contradiction']
05/21/2022 08:21:02 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/21/2022 08:21:02 - INFO - __main__ - ['contradiction']
05/21/2022 08:21:02 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/21/2022 08:21:02 - INFO - __main__ - ['contradiction']
05/21/2022 08:21:02 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:21:02 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:21:03 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:21:04 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 08:21:05 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 08:21:05 - INFO - __main__ - Printing 3 examples
05/21/2022 08:21:05 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 08:21:05 - INFO - __main__ - ['contradiction']
05/21/2022 08:21:05 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 08:21:05 - INFO - __main__ - ['neutral']
05/21/2022 08:21:05 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 08:21:05 - INFO - __main__ - ['entailment']
05/21/2022 08:21:05 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:21:05 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:21:05 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 08:21:05 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_13_0.0005_8_predictions.txt
05/21/2022 08:21:05 - INFO - __main__ - ACC on test data: 0.6429
05/21/2022 08:21:06 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.0005, bsz=8, dev_performance=0.65625, test_performance=0.6428571428571429
05/21/2022 08:21:06 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.0003, bsz=8 ...
05/21/2022 08:21:07 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:21:07 - INFO - __main__ - Printing 3 examples
05/21/2022 08:21:07 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/21/2022 08:21:07 - INFO - __main__ - ['contradiction']
05/21/2022 08:21:07 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/21/2022 08:21:07 - INFO - __main__ - ['contradiction']
05/21/2022 08:21:07 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/21/2022 08:21:07 - INFO - __main__ - ['contradiction']
05/21/2022 08:21:07 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:21:07 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:21:07 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:21:07 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:21:07 - INFO - __main__ - Printing 3 examples
05/21/2022 08:21:07 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
05/21/2022 08:21:07 - INFO - __main__ - ['contradiction']
05/21/2022 08:21:07 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/21/2022 08:21:07 - INFO - __main__ - ['contradiction']
05/21/2022 08:21:07 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/21/2022 08:21:07 - INFO - __main__ - ['contradiction']
05/21/2022 08:21:07 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:21:07 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:21:07 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:21:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:21:07 - INFO - __main__ - Starting training!
05/21/2022 08:21:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:21:11 - INFO - __main__ - Starting training!
05/21/2022 08:21:13 - INFO - __main__ - Step 10 Global step 10 Train loss 17.982336 on epoch=3
05/21/2022 08:21:15 - INFO - __main__ - Step 20 Global step 20 Train loss 14.772840 on epoch=6
05/21/2022 08:21:18 - INFO - __main__ - Step 30 Global step 30 Train loss 10.283721 on epoch=9
05/21/2022 08:21:21 - INFO - __main__ - Step 40 Global step 40 Train loss 7.922715 on epoch=13
05/21/2022 08:21:23 - INFO - __main__ - Step 50 Global step 50 Train loss 6.272850 on epoch=16
05/21/2022 08:21:25 - INFO - __main__ - Global step 50 Train loss 11.446894 ACC 0.125 on epoch=16
05/21/2022 08:21:27 - INFO - __main__ - Step 60 Global step 60 Train loss 5.354599 on epoch=19
05/21/2022 08:21:30 - INFO - __main__ - Step 70 Global step 70 Train loss 4.576821 on epoch=23
05/21/2022 08:21:32 - INFO - __main__ - Step 80 Global step 80 Train loss 2.515085 on epoch=26
05/21/2022 08:21:35 - INFO - __main__ - Step 90 Global step 90 Train loss 2.806145 on epoch=29
05/21/2022 08:21:38 - INFO - __main__ - Step 100 Global step 100 Train loss 1.720531 on epoch=33
05/21/2022 08:21:38 - INFO - __main__ - Global step 100 Train loss 3.394636 ACC 0.09375 on epoch=33
05/21/2022 08:21:41 - INFO - __main__ - Step 110 Global step 110 Train loss 2.239373 on epoch=36
05/21/2022 08:21:43 - INFO - __main__ - Step 120 Global step 120 Train loss 1.780025 on epoch=39
05/21/2022 08:21:46 - INFO - __main__ - Step 130 Global step 130 Train loss 2.068594 on epoch=43
05/21/2022 08:21:48 - INFO - __main__ - Step 140 Global step 140 Train loss 1.798621 on epoch=46
05/21/2022 08:21:51 - INFO - __main__ - Step 150 Global step 150 Train loss 1.356867 on epoch=49
05/21/2022 08:21:51 - INFO - __main__ - Global step 150 Train loss 1.848696 ACC 0.0625 on epoch=49
05/21/2022 08:21:54 - INFO - __main__ - Step 160 Global step 160 Train loss 1.418966 on epoch=53
05/21/2022 08:21:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.937758 on epoch=56
05/21/2022 08:21:59 - INFO - __main__ - Step 180 Global step 180 Train loss 1.013927 on epoch=59
05/21/2022 08:22:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.963942 on epoch=63
05/21/2022 08:22:04 - INFO - __main__ - Step 200 Global step 200 Train loss 1.138825 on epoch=66
05/21/2022 08:22:04 - INFO - __main__ - Global step 200 Train loss 1.094684 ACC 0.625 on epoch=66
05/21/2022 08:22:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.778960 on epoch=69
05/21/2022 08:22:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.619746 on epoch=73
05/21/2022 08:22:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.603446 on epoch=76
05/21/2022 08:22:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.735264 on epoch=79
05/21/2022 08:22:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.576024 on epoch=83
05/21/2022 08:22:18 - INFO - __main__ - Global step 250 Train loss 0.662688 ACC 0.65625 on epoch=83
05/21/2022 08:22:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.336480 on epoch=86
05/21/2022 08:22:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.334881 on epoch=89
05/21/2022 08:22:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.243921 on epoch=93
05/21/2022 08:22:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.204133 on epoch=96
05/21/2022 08:22:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.281131 on epoch=99
05/21/2022 08:22:31 - INFO - __main__ - Global step 300 Train loss 0.280109 ACC 0.5625 on epoch=99
05/21/2022 08:22:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.234232 on epoch=103
05/21/2022 08:22:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.040176 on epoch=106
05/21/2022 08:22:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.518749 on epoch=109
05/21/2022 08:22:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.407773 on epoch=113
05/21/2022 08:22:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.156925 on epoch=116
05/21/2022 08:22:44 - INFO - __main__ - Global step 350 Train loss 0.271571 ACC 0.65625 on epoch=116
05/21/2022 08:22:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.110151 on epoch=119
05/21/2022 08:22:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.085543 on epoch=123
05/21/2022 08:22:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.047599 on epoch=126
05/21/2022 08:22:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.029542 on epoch=129
05/21/2022 08:22:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.106630 on epoch=133
05/21/2022 08:22:58 - INFO - __main__ - Global step 400 Train loss 0.075893 ACC 0.59375 on epoch=133
05/21/2022 08:23:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.049316 on epoch=136
05/21/2022 08:23:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.066948 on epoch=139
05/21/2022 08:23:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.052194 on epoch=143
05/21/2022 08:23:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.050937 on epoch=146
05/21/2022 08:23:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.063822 on epoch=149
05/21/2022 08:23:11 - INFO - __main__ - Global step 450 Train loss 0.056643 ACC 0.5625 on epoch=149
05/21/2022 08:23:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.024020 on epoch=153
05/21/2022 08:23:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.090316 on epoch=156
05/21/2022 08:23:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.041919 on epoch=159
05/21/2022 08:23:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.097440 on epoch=163
05/21/2022 08:23:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.034982 on epoch=166
05/21/2022 08:23:24 - INFO - __main__ - Global step 500 Train loss 0.057735 ACC 0.6875 on epoch=166
05/21/2022 08:23:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.014864 on epoch=169
05/21/2022 08:23:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.019457 on epoch=173
05/21/2022 08:23:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.007509 on epoch=176
05/21/2022 08:23:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.033947 on epoch=179
05/21/2022 08:23:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.015471 on epoch=183
05/21/2022 08:23:37 - INFO - __main__ - Global step 550 Train loss 0.018249 ACC 0.65625 on epoch=183
05/21/2022 08:23:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.003438 on epoch=186
05/21/2022 08:23:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.026920 on epoch=189
05/21/2022 08:23:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.004749 on epoch=193
05/21/2022 08:23:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.005597 on epoch=196
05/21/2022 08:23:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.028682 on epoch=199
05/21/2022 08:23:51 - INFO - __main__ - Global step 600 Train loss 0.013877 ACC 0.6875 on epoch=199
05/21/2022 08:23:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.019628 on epoch=203
05/21/2022 08:23:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.001451 on epoch=206
05/21/2022 08:23:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.002081 on epoch=209
05/21/2022 08:24:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.000917 on epoch=213
05/21/2022 08:24:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000892 on epoch=216
05/21/2022 08:24:04 - INFO - __main__ - Global step 650 Train loss 0.004994 ACC 0.71875 on epoch=216
05/21/2022 08:24:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.040122 on epoch=219
05/21/2022 08:24:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.014872 on epoch=223
05/21/2022 08:24:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.017442 on epoch=226
05/21/2022 08:24:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.014063 on epoch=229
05/21/2022 08:24:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.029500 on epoch=233
05/21/2022 08:24:17 - INFO - __main__ - Global step 700 Train loss 0.023200 ACC 0.59375 on epoch=233
05/21/2022 08:24:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.032091 on epoch=236
05/21/2022 08:24:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000608 on epoch=239
05/21/2022 08:24:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.016195 on epoch=243
05/21/2022 08:24:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.025488 on epoch=246
05/21/2022 08:24:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.002533 on epoch=249
05/21/2022 08:24:30 - INFO - __main__ - Global step 750 Train loss 0.015383 ACC 0.625 on epoch=249
05/21/2022 08:24:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.039215 on epoch=253
05/21/2022 08:24:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.032086 on epoch=256
05/21/2022 08:24:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.003539 on epoch=259
05/21/2022 08:24:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.002245 on epoch=263
05/21/2022 08:24:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.037889 on epoch=266
05/21/2022 08:24:44 - INFO - __main__ - Global step 800 Train loss 0.022995 ACC 0.75 on epoch=266
05/21/2022 08:24:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.004452 on epoch=269
05/21/2022 08:24:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.020670 on epoch=273
05/21/2022 08:24:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.055602 on epoch=276
05/21/2022 08:24:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.040749 on epoch=279
05/21/2022 08:24:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.010688 on epoch=283
05/21/2022 08:24:57 - INFO - __main__ - Global step 850 Train loss 0.026432 ACC 0.71875 on epoch=283
05/21/2022 08:25:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.002438 on epoch=286
05/21/2022 08:25:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.021758 on epoch=289
05/21/2022 08:25:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.006480 on epoch=293
05/21/2022 08:25:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.003854 on epoch=296
05/21/2022 08:25:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.002628 on epoch=299
05/21/2022 08:25:10 - INFO - __main__ - Global step 900 Train loss 0.007432 ACC 0.65625 on epoch=299
05/21/2022 08:25:10 - INFO - __main__ - save last model!
05/21/2022 08:25:11 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:25:11 - INFO - __main__ - Printing 3 examples
05/21/2022 08:25:11 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/21/2022 08:25:11 - INFO - __main__ - ['contradiction']
05/21/2022 08:25:11 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/21/2022 08:25:11 - INFO - __main__ - ['contradiction']
05/21/2022 08:25:11 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/21/2022 08:25:11 - INFO - __main__ - ['contradiction']
05/21/2022 08:25:11 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:25:11 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:25:11 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:25:11 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:25:11 - INFO - __main__ - Printing 3 examples
05/21/2022 08:25:11 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
05/21/2022 08:25:11 - INFO - __main__ - ['contradiction']
05/21/2022 08:25:11 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/21/2022 08:25:11 - INFO - __main__ - ['contradiction']
05/21/2022 08:25:11 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/21/2022 08:25:11 - INFO - __main__ - ['contradiction']
05/21/2022 08:25:11 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:25:11 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:25:11 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:25:13 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 08:25:13 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 08:25:13 - INFO - __main__ - Printing 3 examples
05/21/2022 08:25:13 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 08:25:13 - INFO - __main__ - ['contradiction']
05/21/2022 08:25:13 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 08:25:13 - INFO - __main__ - ['neutral']
05/21/2022 08:25:13 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 08:25:13 - INFO - __main__ - ['entailment']
05/21/2022 08:25:13 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:25:13 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:25:13 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 08:25:14 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_13_0.0003_8_predictions.txt
05/21/2022 08:25:14 - INFO - __main__ - ACC on test data: 0.7679
05/21/2022 08:25:14 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.0003, bsz=8, dev_performance=0.75, test_performance=0.7678571428571429
05/21/2022 08:25:14 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.0002, bsz=8 ...
05/21/2022 08:25:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:25:15 - INFO - __main__ - Starting training!
05/21/2022 08:25:15 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:25:15 - INFO - __main__ - Printing 3 examples
05/21/2022 08:25:15 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/21/2022 08:25:15 - INFO - __main__ - ['contradiction']
05/21/2022 08:25:15 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/21/2022 08:25:15 - INFO - __main__ - ['contradiction']
05/21/2022 08:25:15 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/21/2022 08:25:15 - INFO - __main__ - ['contradiction']
05/21/2022 08:25:15 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:25:15 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:25:15 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:25:15 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:25:15 - INFO - __main__ - Printing 3 examples
05/21/2022 08:25:15 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
05/21/2022 08:25:15 - INFO - __main__ - ['contradiction']
05/21/2022 08:25:15 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/21/2022 08:25:15 - INFO - __main__ - ['contradiction']
05/21/2022 08:25:15 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/21/2022 08:25:15 - INFO - __main__ - ['contradiction']
05/21/2022 08:25:15 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:25:15 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:25:15 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:25:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:25:19 - INFO - __main__ - Starting training!
05/21/2022 08:25:22 - INFO - __main__ - Step 10 Global step 10 Train loss 18.022726 on epoch=3
05/21/2022 08:25:25 - INFO - __main__ - Step 20 Global step 20 Train loss 15.733739 on epoch=6
05/21/2022 08:25:27 - INFO - __main__ - Step 30 Global step 30 Train loss 10.752584 on epoch=9
05/21/2022 08:25:30 - INFO - __main__ - Step 40 Global step 40 Train loss 8.542622 on epoch=13
05/21/2022 08:25:32 - INFO - __main__ - Step 50 Global step 50 Train loss 7.591319 on epoch=16
05/21/2022 08:25:35 - INFO - __main__ - Global step 50 Train loss 12.128597 ACC 0.0 on epoch=16
05/21/2022 08:25:38 - INFO - __main__ - Step 60 Global step 60 Train loss 6.553370 on epoch=19
05/21/2022 08:25:40 - INFO - __main__ - Step 70 Global step 70 Train loss 5.609338 on epoch=23
05/21/2022 08:25:43 - INFO - __main__ - Step 80 Global step 80 Train loss 5.037160 on epoch=26
05/21/2022 08:25:45 - INFO - __main__ - Step 90 Global step 90 Train loss 4.223451 on epoch=29
05/21/2022 08:25:48 - INFO - __main__ - Step 100 Global step 100 Train loss 3.142295 on epoch=33
05/21/2022 08:25:48 - INFO - __main__ - Global step 100 Train loss 4.913123 ACC 0.0 on epoch=33
05/21/2022 08:25:51 - INFO - __main__ - Step 110 Global step 110 Train loss 2.317284 on epoch=36
05/21/2022 08:25:53 - INFO - __main__ - Step 120 Global step 120 Train loss 2.488838 on epoch=39
05/21/2022 08:25:56 - INFO - __main__ - Step 130 Global step 130 Train loss 2.414434 on epoch=43
05/21/2022 08:25:58 - INFO - __main__ - Step 140 Global step 140 Train loss 2.133783 on epoch=46
05/21/2022 08:26:01 - INFO - __main__ - Step 150 Global step 150 Train loss 2.080021 on epoch=49
05/21/2022 08:26:01 - INFO - __main__ - Global step 150 Train loss 2.286872 ACC 0.25 on epoch=49
05/21/2022 08:26:04 - INFO - __main__ - Step 160 Global step 160 Train loss 1.864599 on epoch=53
05/21/2022 08:26:07 - INFO - __main__ - Step 170 Global step 170 Train loss 2.042387 on epoch=56
05/21/2022 08:26:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.916211 on epoch=59
05/21/2022 08:26:12 - INFO - __main__ - Step 190 Global step 190 Train loss 1.277058 on epoch=63
05/21/2022 08:26:14 - INFO - __main__ - Step 200 Global step 200 Train loss 1.362839 on epoch=66
05/21/2022 08:26:15 - INFO - __main__ - Global step 200 Train loss 1.492619 ACC 0.40625 on epoch=66
05/21/2022 08:26:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.944841 on epoch=69
05/21/2022 08:26:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.993560 on epoch=73
05/21/2022 08:26:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.838973 on epoch=76
05/21/2022 08:26:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.842348 on epoch=79
05/21/2022 08:26:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.877341 on epoch=83
05/21/2022 08:26:28 - INFO - __main__ - Global step 250 Train loss 0.899413 ACC 0.625 on epoch=83
05/21/2022 08:26:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.560606 on epoch=86
05/21/2022 08:26:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.997836 on epoch=89
05/21/2022 08:26:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.734304 on epoch=93
05/21/2022 08:26:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.618435 on epoch=96
05/21/2022 08:26:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.462280 on epoch=99
05/21/2022 08:26:42 - INFO - __main__ - Global step 300 Train loss 0.674692 ACC 0.6875 on epoch=99
05/21/2022 08:26:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.626981 on epoch=103
05/21/2022 08:26:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.385350 on epoch=106
05/21/2022 08:26:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.508251 on epoch=109
05/21/2022 08:26:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.247091 on epoch=113
05/21/2022 08:26:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.275906 on epoch=116
05/21/2022 08:26:56 - INFO - __main__ - Global step 350 Train loss 0.408716 ACC 0.6875 on epoch=116
05/21/2022 08:26:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.252346 on epoch=119
05/21/2022 08:27:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.267279 on epoch=123
05/21/2022 08:27:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.214876 on epoch=126
05/21/2022 08:27:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.305273 on epoch=129
05/21/2022 08:27:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.477442 on epoch=133
05/21/2022 08:27:12 - INFO - __main__ - Global step 400 Train loss 0.303443 ACC 0.625 on epoch=133
05/21/2022 08:27:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.046023 on epoch=136
05/21/2022 08:27:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.297775 on epoch=139
05/21/2022 08:27:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.115982 on epoch=143
05/21/2022 08:27:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.142328 on epoch=146
05/21/2022 08:27:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.072989 on epoch=149
05/21/2022 08:27:27 - INFO - __main__ - Global step 450 Train loss 0.135019 ACC 0.6875 on epoch=149
05/21/2022 08:27:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.022350 on epoch=153
05/21/2022 08:27:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.125533 on epoch=156
05/21/2022 08:27:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.011483 on epoch=159
05/21/2022 08:27:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.054817 on epoch=163
05/21/2022 08:27:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.051219 on epoch=166
05/21/2022 08:27:41 - INFO - __main__ - Global step 500 Train loss 0.053081 ACC 0.6875 on epoch=166
05/21/2022 08:27:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.041472 on epoch=169
05/21/2022 08:27:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.089820 on epoch=173
05/21/2022 08:27:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.050060 on epoch=176
05/21/2022 08:27:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.103739 on epoch=179
05/21/2022 08:27:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.163870 on epoch=183
05/21/2022 08:27:56 - INFO - __main__ - Global step 550 Train loss 0.089792 ACC 0.625 on epoch=183
05/21/2022 08:27:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.103862 on epoch=186
05/21/2022 08:28:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.205353 on epoch=189
05/21/2022 08:28:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.026886 on epoch=193
05/21/2022 08:28:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.036653 on epoch=196
05/21/2022 08:28:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.005987 on epoch=199
05/21/2022 08:28:10 - INFO - __main__ - Global step 600 Train loss 0.075748 ACC 0.53125 on epoch=199
05/21/2022 08:28:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.017676 on epoch=203
05/21/2022 08:28:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.129592 on epoch=206
05/21/2022 08:28:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.062979 on epoch=209
05/21/2022 08:28:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.013043 on epoch=213
05/21/2022 08:28:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.039034 on epoch=216
05/21/2022 08:28:26 - INFO - __main__ - Global step 650 Train loss 0.052465 ACC 0.625 on epoch=216
05/21/2022 08:28:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.026951 on epoch=219
05/21/2022 08:28:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.040519 on epoch=223
05/21/2022 08:28:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.011795 on epoch=226
05/21/2022 08:28:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.022316 on epoch=229
05/21/2022 08:28:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.013802 on epoch=233
05/21/2022 08:28:41 - INFO - __main__ - Global step 700 Train loss 0.023077 ACC 0.625 on epoch=233
05/21/2022 08:28:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.006999 on epoch=236
05/21/2022 08:28:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.044467 on epoch=239
05/21/2022 08:28:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.007829 on epoch=243
05/21/2022 08:28:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.009154 on epoch=246
05/21/2022 08:28:55 - INFO - __main__ - Step 750 Global step 750 Train loss 0.004160 on epoch=249
05/21/2022 08:28:55 - INFO - __main__ - Global step 750 Train loss 0.014522 ACC 0.65625 on epoch=249
05/21/2022 08:28:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.004495 on epoch=253
05/21/2022 08:29:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.044807 on epoch=256
05/21/2022 08:29:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.001271 on epoch=259
05/21/2022 08:29:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.005837 on epoch=263
05/21/2022 08:29:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.009207 on epoch=266
05/21/2022 08:29:09 - INFO - __main__ - Global step 800 Train loss 0.013124 ACC 0.5625 on epoch=266
05/21/2022 08:29:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.005294 on epoch=269
05/21/2022 08:29:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.037847 on epoch=273
05/21/2022 08:29:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.003194 on epoch=276
05/21/2022 08:29:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.002814 on epoch=279
05/21/2022 08:29:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000978 on epoch=283
05/21/2022 08:29:24 - INFO - __main__ - Global step 850 Train loss 0.010025 ACC 0.5625 on epoch=283
05/21/2022 08:29:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.002215 on epoch=286
05/21/2022 08:29:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.006065 on epoch=289
05/21/2022 08:29:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.071149 on epoch=293
05/21/2022 08:29:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.004057 on epoch=296
05/21/2022 08:29:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.005325 on epoch=299
05/21/2022 08:29:37 - INFO - __main__ - Global step 900 Train loss 0.017762 ACC 0.5625 on epoch=299
05/21/2022 08:29:37 - INFO - __main__ - save last model!
05/21/2022 08:29:38 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:29:38 - INFO - __main__ - Printing 3 examples
05/21/2022 08:29:38 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/21/2022 08:29:38 - INFO - __main__ - ['contradiction']
05/21/2022 08:29:38 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/21/2022 08:29:38 - INFO - __main__ - ['contradiction']
05/21/2022 08:29:38 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/21/2022 08:29:38 - INFO - __main__ - ['contradiction']
05/21/2022 08:29:38 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:29:38 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:29:38 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:29:38 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:29:38 - INFO - __main__ - Printing 3 examples
05/21/2022 08:29:38 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
05/21/2022 08:29:38 - INFO - __main__ - ['contradiction']
05/21/2022 08:29:38 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/21/2022 08:29:38 - INFO - __main__ - ['contradiction']
05/21/2022 08:29:38 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/21/2022 08:29:38 - INFO - __main__ - ['contradiction']
05/21/2022 08:29:38 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:29:38 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:29:38 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:29:40 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 08:29:40 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 08:29:40 - INFO - __main__ - Printing 3 examples
05/21/2022 08:29:40 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 08:29:40 - INFO - __main__ - ['contradiction']
05/21/2022 08:29:40 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 08:29:40 - INFO - __main__ - ['neutral']
05/21/2022 08:29:40 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 08:29:40 - INFO - __main__ - ['entailment']
05/21/2022 08:29:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:29:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:29:40 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 08:29:41 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_13_0.0002_8_predictions.txt
05/21/2022 08:29:41 - INFO - __main__ - ACC on test data: 0.8036
05/21/2022 08:29:41 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.0002, bsz=8, dev_performance=0.6875, test_performance=0.8035714285714286
05/21/2022 08:29:41 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.0001, bsz=8 ...
05/21/2022 08:29:42 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:29:42 - INFO - __main__ - Printing 3 examples
05/21/2022 08:29:42 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
05/21/2022 08:29:42 - INFO - __main__ - ['contradiction']
05/21/2022 08:29:42 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
05/21/2022 08:29:42 - INFO - __main__ - ['contradiction']
05/21/2022 08:29:42 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
05/21/2022 08:29:42 - INFO - __main__ - ['contradiction']
05/21/2022 08:29:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:29:42 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:29:42 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:29:42 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:29:42 - INFO - __main__ - Printing 3 examples
05/21/2022 08:29:42 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
05/21/2022 08:29:42 - INFO - __main__ - ['contradiction']
05/21/2022 08:29:42 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
05/21/2022 08:29:42 - INFO - __main__ - ['contradiction']
05/21/2022 08:29:42 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
05/21/2022 08:29:42 - INFO - __main__ - ['contradiction']
05/21/2022 08:29:42 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:29:43 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:29:43 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:29:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:29:43 - INFO - __main__ - Starting training!
05/21/2022 08:29:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:29:47 - INFO - __main__ - Starting training!
05/21/2022 08:29:50 - INFO - __main__ - Step 10 Global step 10 Train loss 18.403929 on epoch=3
05/21/2022 08:29:53 - INFO - __main__ - Step 20 Global step 20 Train loss 17.294094 on epoch=6
05/21/2022 08:29:56 - INFO - __main__ - Step 30 Global step 30 Train loss 16.307276 on epoch=9
05/21/2022 08:29:59 - INFO - __main__ - Step 40 Global step 40 Train loss 13.816793 on epoch=13
05/21/2022 08:30:01 - INFO - __main__ - Step 50 Global step 50 Train loss 11.070391 on epoch=16
05/21/2022 08:30:05 - INFO - __main__ - Global step 50 Train loss 15.378496 ACC 0.0 on epoch=16
05/21/2022 08:30:08 - INFO - __main__ - Step 60 Global step 60 Train loss 9.793314 on epoch=19
05/21/2022 08:30:10 - INFO - __main__ - Step 70 Global step 70 Train loss 9.715104 on epoch=23
05/21/2022 08:30:13 - INFO - __main__ - Step 80 Global step 80 Train loss 8.610879 on epoch=26
05/21/2022 08:30:16 - INFO - __main__ - Step 90 Global step 90 Train loss 8.353514 on epoch=29
05/21/2022 08:30:19 - INFO - __main__ - Step 100 Global step 100 Train loss 8.243409 on epoch=33
05/21/2022 08:30:20 - INFO - __main__ - Global step 100 Train loss 8.943244 ACC 0.0 on epoch=33
05/21/2022 08:30:23 - INFO - __main__ - Step 110 Global step 110 Train loss 7.207053 on epoch=36
05/21/2022 08:30:26 - INFO - __main__ - Step 120 Global step 120 Train loss 6.117818 on epoch=39
05/21/2022 08:30:29 - INFO - __main__ - Step 130 Global step 130 Train loss 6.819085 on epoch=43
05/21/2022 08:30:32 - INFO - __main__ - Step 140 Global step 140 Train loss 5.668572 on epoch=46
05/21/2022 08:30:35 - INFO - __main__ - Step 150 Global step 150 Train loss 5.306703 on epoch=49
05/21/2022 08:30:36 - INFO - __main__ - Global step 150 Train loss 6.223846 ACC 0.09375 on epoch=49
05/21/2022 08:30:39 - INFO - __main__ - Step 160 Global step 160 Train loss 5.331809 on epoch=53
05/21/2022 08:30:42 - INFO - __main__ - Step 170 Global step 170 Train loss 4.621961 on epoch=56
05/21/2022 08:30:45 - INFO - __main__ - Step 180 Global step 180 Train loss 3.592441 on epoch=59
05/21/2022 08:30:47 - INFO - __main__ - Step 190 Global step 190 Train loss 3.372695 on epoch=63
05/21/2022 08:30:50 - INFO - __main__ - Step 200 Global step 200 Train loss 3.394685 on epoch=66
05/21/2022 08:30:50 - INFO - __main__ - Global step 200 Train loss 4.062718 ACC 0.0 on epoch=66
05/21/2022 08:30:53 - INFO - __main__ - Step 210 Global step 210 Train loss 3.242065 on epoch=69
05/21/2022 08:30:56 - INFO - __main__ - Step 220 Global step 220 Train loss 3.478693 on epoch=73
05/21/2022 08:30:59 - INFO - __main__ - Step 230 Global step 230 Train loss 2.399326 on epoch=76
05/21/2022 08:31:02 - INFO - __main__ - Step 240 Global step 240 Train loss 2.408394 on epoch=79
05/21/2022 08:31:05 - INFO - __main__ - Step 250 Global step 250 Train loss 2.842597 on epoch=83
05/21/2022 08:31:05 - INFO - __main__ - Global step 250 Train loss 2.874214 ACC 0.0 on epoch=83
05/21/2022 08:31:09 - INFO - __main__ - Step 260 Global step 260 Train loss 2.915946 on epoch=86
05/21/2022 08:31:11 - INFO - __main__ - Step 270 Global step 270 Train loss 2.046961 on epoch=89
05/21/2022 08:31:15 - INFO - __main__ - Step 280 Global step 280 Train loss 2.937120 on epoch=93
05/21/2022 08:31:17 - INFO - __main__ - Step 290 Global step 290 Train loss 2.241133 on epoch=96
05/21/2022 08:31:20 - INFO - __main__ - Step 300 Global step 300 Train loss 2.067172 on epoch=99
05/21/2022 08:31:21 - INFO - __main__ - Global step 300 Train loss 2.441667 ACC 0.40625 on epoch=99
05/21/2022 08:31:24 - INFO - __main__ - Step 310 Global step 310 Train loss 2.128892 on epoch=103
05/21/2022 08:31:27 - INFO - __main__ - Step 320 Global step 320 Train loss 1.456323 on epoch=106
05/21/2022 08:31:30 - INFO - __main__ - Step 330 Global step 330 Train loss 2.120302 on epoch=109
05/21/2022 08:31:33 - INFO - __main__ - Step 340 Global step 340 Train loss 2.304533 on epoch=113
05/21/2022 08:31:35 - INFO - __main__ - Step 350 Global step 350 Train loss 2.114899 on epoch=116
05/21/2022 08:31:36 - INFO - __main__ - Global step 350 Train loss 2.024990 ACC 0.4375 on epoch=116
05/21/2022 08:31:39 - INFO - __main__ - Step 360 Global step 360 Train loss 1.970169 on epoch=119
05/21/2022 08:31:42 - INFO - __main__ - Step 370 Global step 370 Train loss 1.960430 on epoch=123
05/21/2022 08:31:44 - INFO - __main__ - Step 380 Global step 380 Train loss 1.332984 on epoch=126
05/21/2022 08:31:47 - INFO - __main__ - Step 390 Global step 390 Train loss 1.847233 on epoch=129
05/21/2022 08:31:49 - INFO - __main__ - Step 400 Global step 400 Train loss 1.747937 on epoch=133
05/21/2022 08:31:50 - INFO - __main__ - Global step 400 Train loss 1.771751 ACC 0.34375 on epoch=133
05/21/2022 08:31:52 - INFO - __main__ - Step 410 Global step 410 Train loss 2.051048 on epoch=136
05/21/2022 08:31:55 - INFO - __main__ - Step 420 Global step 420 Train loss 1.275383 on epoch=139
05/21/2022 08:31:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.950006 on epoch=143
05/21/2022 08:32:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.967355 on epoch=146
05/21/2022 08:32:03 - INFO - __main__ - Step 450 Global step 450 Train loss 1.228619 on epoch=149
05/21/2022 08:32:04 - INFO - __main__ - Global step 450 Train loss 1.294482 ACC 0.46875 on epoch=149
05/21/2022 08:32:07 - INFO - __main__ - Step 460 Global step 460 Train loss 1.092095 on epoch=153
05/21/2022 08:32:10 - INFO - __main__ - Step 470 Global step 470 Train loss 1.381583 on epoch=156
05/21/2022 08:32:13 - INFO - __main__ - Step 480 Global step 480 Train loss 1.004831 on epoch=159
05/21/2022 08:32:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.956967 on epoch=163
05/21/2022 08:32:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.893028 on epoch=166
05/21/2022 08:32:19 - INFO - __main__ - Global step 500 Train loss 1.065701 ACC 0.625 on epoch=166
05/21/2022 08:32:23 - INFO - __main__ - Step 510 Global step 510 Train loss 1.117901 on epoch=169
05/21/2022 08:32:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.965035 on epoch=173
05/21/2022 08:32:28 - INFO - __main__ - Step 530 Global step 530 Train loss 1.060766 on epoch=176
05/21/2022 08:32:31 - INFO - __main__ - Step 540 Global step 540 Train loss 1.042120 on epoch=179
05/21/2022 08:32:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.962077 on epoch=183
05/21/2022 08:32:35 - INFO - __main__ - Global step 550 Train loss 1.029580 ACC 0.65625 on epoch=183
05/21/2022 08:32:38 - INFO - __main__ - Step 560 Global step 560 Train loss 1.046471 on epoch=186
05/21/2022 08:32:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.978902 on epoch=189
05/21/2022 08:32:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.983001 on epoch=193
05/21/2022 08:32:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.637020 on epoch=196
05/21/2022 08:32:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.772341 on epoch=199
05/21/2022 08:32:50 - INFO - __main__ - Global step 600 Train loss 0.883547 ACC 0.65625 on epoch=199
05/21/2022 08:32:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.426331 on epoch=203
05/21/2022 08:32:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.601781 on epoch=206
05/21/2022 08:32:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.677150 on epoch=209
05/21/2022 08:33:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.675850 on epoch=213
05/21/2022 08:33:02 - INFO - __main__ - Step 650 Global step 650 Train loss 0.494980 on epoch=216
05/21/2022 08:33:03 - INFO - __main__ - Global step 650 Train loss 0.575218 ACC 0.65625 on epoch=216
05/21/2022 08:33:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.508444 on epoch=219
05/21/2022 08:33:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.636170 on epoch=223
05/21/2022 08:33:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.538133 on epoch=226
05/21/2022 08:33:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.603028 on epoch=229
05/21/2022 08:33:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.480888 on epoch=233
05/21/2022 08:33:18 - INFO - __main__ - Global step 700 Train loss 0.553333 ACC 0.6875 on epoch=233
05/21/2022 08:33:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.577784 on epoch=236
05/21/2022 08:33:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.706783 on epoch=239
05/21/2022 08:33:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.524061 on epoch=243
05/21/2022 08:33:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.424533 on epoch=246
05/21/2022 08:33:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.283675 on epoch=249
05/21/2022 08:33:33 - INFO - __main__ - Global step 750 Train loss 0.503367 ACC 0.71875 on epoch=249
05/21/2022 08:33:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.508076 on epoch=253
05/21/2022 08:33:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.352733 on epoch=256
05/21/2022 08:33:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.178169 on epoch=259
05/21/2022 08:33:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.382152 on epoch=263
05/21/2022 08:33:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.432247 on epoch=266
05/21/2022 08:33:48 - INFO - __main__ - Global step 800 Train loss 0.370676 ACC 0.65625 on epoch=266
05/21/2022 08:33:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.261287 on epoch=269
05/21/2022 08:33:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.242806 on epoch=273
05/21/2022 08:33:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.297440 on epoch=276
05/21/2022 08:34:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.290513 on epoch=279
05/21/2022 08:34:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.295589 on epoch=283
05/21/2022 08:34:04 - INFO - __main__ - Global step 850 Train loss 0.277527 ACC 0.625 on epoch=283
05/21/2022 08:34:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.211522 on epoch=286
05/21/2022 08:34:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.228721 on epoch=289
05/21/2022 08:34:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.190569 on epoch=293
05/21/2022 08:34:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.214229 on epoch=296
05/21/2022 08:34:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.207964 on epoch=299
05/21/2022 08:34:19 - INFO - __main__ - Global step 900 Train loss 0.210601 ACC 0.65625 on epoch=299
05/21/2022 08:34:19 - INFO - __main__ - save last model!
05/21/2022 08:34:21 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:34:21 - INFO - __main__ - Printing 3 examples
05/21/2022 08:34:21 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/21/2022 08:34:21 - INFO - __main__ - ['contradiction']
05/21/2022 08:34:21 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/21/2022 08:34:21 - INFO - __main__ - ['contradiction']
05/21/2022 08:34:21 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/21/2022 08:34:21 - INFO - __main__ - ['contradiction']
05/21/2022 08:34:21 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:34:21 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:34:21 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:34:21 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:34:21 - INFO - __main__ - Printing 3 examples
05/21/2022 08:34:21 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
05/21/2022 08:34:21 - INFO - __main__ - ['contradiction']
05/21/2022 08:34:21 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
05/21/2022 08:34:21 - INFO - __main__ - ['contradiction']
05/21/2022 08:34:21 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
05/21/2022 08:34:21 - INFO - __main__ - ['contradiction']
05/21/2022 08:34:21 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:34:21 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:34:21 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:34:22 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 08:34:22 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 08:34:22 - INFO - __main__ - Printing 3 examples
05/21/2022 08:34:22 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 08:34:22 - INFO - __main__ - ['contradiction']
05/21/2022 08:34:22 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 08:34:22 - INFO - __main__ - ['neutral']
05/21/2022 08:34:22 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 08:34:22 - INFO - __main__ - ['entailment']
05/21/2022 08:34:22 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:34:22 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:34:22 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 08:34:23 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_13_0.0001_8_predictions.txt
05/21/2022 08:34:23 - INFO - __main__ - ACC on test data: 0.8214
05/21/2022 08:34:23 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.0001, bsz=8, dev_performance=0.71875, test_performance=0.8214285714285714
05/21/2022 08:34:23 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.0005, bsz=8 ...
05/21/2022 08:34:24 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:34:24 - INFO - __main__ - Printing 3 examples
05/21/2022 08:34:24 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/21/2022 08:34:24 - INFO - __main__ - ['contradiction']
05/21/2022 08:34:24 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/21/2022 08:34:24 - INFO - __main__ - ['contradiction']
05/21/2022 08:34:24 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/21/2022 08:34:24 - INFO - __main__ - ['contradiction']
05/21/2022 08:34:24 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:34:24 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:34:24 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:34:24 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:34:24 - INFO - __main__ - Printing 3 examples
05/21/2022 08:34:24 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
05/21/2022 08:34:24 - INFO - __main__ - ['contradiction']
05/21/2022 08:34:24 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
05/21/2022 08:34:24 - INFO - __main__ - ['contradiction']
05/21/2022 08:34:24 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
05/21/2022 08:34:24 - INFO - __main__ - ['contradiction']
05/21/2022 08:34:24 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:34:24 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:34:24 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:34:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:34:25 - INFO - __main__ - Starting training!
05/21/2022 08:34:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:34:30 - INFO - __main__ - Starting training!
05/21/2022 08:34:33 - INFO - __main__ - Step 10 Global step 10 Train loss 17.712831 on epoch=3
05/21/2022 08:34:35 - INFO - __main__ - Step 20 Global step 20 Train loss 14.376268 on epoch=6
05/21/2022 08:34:38 - INFO - __main__ - Step 30 Global step 30 Train loss 8.281266 on epoch=9
05/21/2022 08:34:41 - INFO - __main__ - Step 40 Global step 40 Train loss 4.556888 on epoch=13
05/21/2022 08:34:44 - INFO - __main__ - Step 50 Global step 50 Train loss 3.476071 on epoch=16
05/21/2022 08:34:44 - INFO - __main__ - Global step 50 Train loss 9.680665 ACC 0.0 on epoch=16
05/21/2022 08:34:47 - INFO - __main__ - Step 60 Global step 60 Train loss 3.397503 on epoch=19
05/21/2022 08:34:50 - INFO - __main__ - Step 70 Global step 70 Train loss 3.094753 on epoch=23
05/21/2022 08:34:52 - INFO - __main__ - Step 80 Global step 80 Train loss 2.318676 on epoch=26
05/21/2022 08:34:55 - INFO - __main__ - Step 90 Global step 90 Train loss 1.890079 on epoch=29
05/21/2022 08:34:57 - INFO - __main__ - Step 100 Global step 100 Train loss 1.295075 on epoch=33
05/21/2022 08:34:58 - INFO - __main__ - Global step 100 Train loss 2.399217 ACC 0.125 on epoch=33
05/21/2022 08:35:01 - INFO - __main__ - Step 110 Global step 110 Train loss 1.411049 on epoch=36
05/21/2022 08:35:03 - INFO - __main__ - Step 120 Global step 120 Train loss 1.178170 on epoch=39
05/21/2022 08:35:06 - INFO - __main__ - Step 130 Global step 130 Train loss 1.095411 on epoch=43
05/21/2022 08:35:09 - INFO - __main__ - Step 140 Global step 140 Train loss 1.355322 on epoch=46
05/21/2022 08:35:11 - INFO - __main__ - Step 150 Global step 150 Train loss 1.031906 on epoch=49
05/21/2022 08:35:12 - INFO - __main__ - Global step 150 Train loss 1.214372 ACC 0.46875 on epoch=49
05/21/2022 08:35:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.911217 on epoch=53
05/21/2022 08:35:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.855067 on epoch=56
05/21/2022 08:35:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.757374 on epoch=59
05/21/2022 08:35:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.876001 on epoch=63
05/21/2022 08:35:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.593543 on epoch=66
05/21/2022 08:35:26 - INFO - __main__ - Global step 200 Train loss 0.798641 ACC 0.59375 on epoch=66
05/21/2022 08:35:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.949577 on epoch=69
05/21/2022 08:35:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.752145 on epoch=73
05/21/2022 08:35:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.480879 on epoch=76
05/21/2022 08:35:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.570174 on epoch=79
05/21/2022 08:35:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.500104 on epoch=83
05/21/2022 08:35:40 - INFO - __main__ - Global step 250 Train loss 0.650576 ACC 0.46875 on epoch=83
05/21/2022 08:35:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.505364 on epoch=86
05/21/2022 08:35:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.579965 on epoch=89
05/21/2022 08:35:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.636020 on epoch=93
05/21/2022 08:35:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.715814 on epoch=96
05/21/2022 08:35:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.523941 on epoch=99
05/21/2022 08:35:54 - INFO - __main__ - Global step 300 Train loss 0.592221 ACC 0.21875 on epoch=99
05/21/2022 08:35:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.505227 on epoch=103
05/21/2022 08:35:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.432413 on epoch=106
05/21/2022 08:36:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.474335 on epoch=109
05/21/2022 08:36:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.466914 on epoch=113
05/21/2022 08:36:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.520658 on epoch=116
05/21/2022 08:36:08 - INFO - __main__ - Global step 350 Train loss 0.479909 ACC 0.65625 on epoch=116
05/21/2022 08:36:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.543060 on epoch=119
05/21/2022 08:36:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.461226 on epoch=123
05/21/2022 08:36:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.485107 on epoch=126
05/21/2022 08:36:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.500806 on epoch=129
05/21/2022 08:36:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.453319 on epoch=133
05/21/2022 08:36:22 - INFO - __main__ - Global step 400 Train loss 0.488704 ACC 0.46875 on epoch=133
05/21/2022 08:36:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.397222 on epoch=136
05/21/2022 08:36:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.470117 on epoch=139
05/21/2022 08:36:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.429561 on epoch=143
05/21/2022 08:36:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.341865 on epoch=146
05/21/2022 08:36:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.411917 on epoch=149
05/21/2022 08:36:37 - INFO - __main__ - Global step 450 Train loss 0.410137 ACC 0.25 on epoch=149
05/21/2022 08:36:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.365542 on epoch=153
05/21/2022 08:36:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.443966 on epoch=156
05/21/2022 08:36:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.418035 on epoch=159
05/21/2022 08:36:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.417869 on epoch=163
05/21/2022 08:36:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.384302 on epoch=166
05/21/2022 08:36:50 - INFO - __main__ - Global step 500 Train loss 0.405943 ACC 0.46875 on epoch=166
05/21/2022 08:36:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.387217 on epoch=169
05/21/2022 08:36:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.349521 on epoch=173
05/21/2022 08:36:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.432532 on epoch=176
05/21/2022 08:37:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.301624 on epoch=179
05/21/2022 08:37:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.364800 on epoch=183
05/21/2022 08:37:05 - INFO - __main__ - Global step 550 Train loss 0.367139 ACC 0.25 on epoch=183
05/21/2022 08:37:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.352330 on epoch=186
05/21/2022 08:37:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.407534 on epoch=189
05/21/2022 08:37:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.306663 on epoch=193
05/21/2022 08:37:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.368837 on epoch=196
05/21/2022 08:37:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.338985 on epoch=199
05/21/2022 08:37:20 - INFO - __main__ - Global step 600 Train loss 0.354870 ACC 0.59375 on epoch=199
05/21/2022 08:37:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.283265 on epoch=203
05/21/2022 08:37:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.250705 on epoch=206
05/21/2022 08:37:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.268461 on epoch=209
05/21/2022 08:37:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.220920 on epoch=213
05/21/2022 08:37:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.221938 on epoch=216
05/21/2022 08:37:34 - INFO - __main__ - Global step 650 Train loss 0.249058 ACC 0.5 on epoch=216
05/21/2022 08:37:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.268890 on epoch=219
05/21/2022 08:37:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.193863 on epoch=223
05/21/2022 08:37:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.252389 on epoch=226
05/21/2022 08:37:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.200545 on epoch=229
05/21/2022 08:37:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.113340 on epoch=233
05/21/2022 08:37:48 - INFO - __main__ - Global step 700 Train loss 0.205806 ACC 0.46875 on epoch=233
05/21/2022 08:37:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.152846 on epoch=236
05/21/2022 08:37:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.146821 on epoch=239
05/21/2022 08:37:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.090555 on epoch=243
05/21/2022 08:38:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.108170 on epoch=246
05/21/2022 08:38:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.179793 on epoch=249
05/21/2022 08:38:03 - INFO - __main__ - Global step 750 Train loss 0.135637 ACC 0.5 on epoch=249
05/21/2022 08:38:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.091941 on epoch=253
05/21/2022 08:38:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.105256 on epoch=256
05/21/2022 08:38:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.090607 on epoch=259
05/21/2022 08:38:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.118449 on epoch=263
05/21/2022 08:38:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.124702 on epoch=266
05/21/2022 08:38:17 - INFO - __main__ - Global step 800 Train loss 0.106191 ACC 0.625 on epoch=266
05/21/2022 08:38:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.088791 on epoch=269
05/21/2022 08:38:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.145325 on epoch=273
05/21/2022 08:38:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.073517 on epoch=276
05/21/2022 08:38:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.096670 on epoch=279
05/21/2022 08:38:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.062630 on epoch=283
05/21/2022 08:38:31 - INFO - __main__ - Global step 850 Train loss 0.093387 ACC 0.71875 on epoch=283
05/21/2022 08:38:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.089179 on epoch=286
05/21/2022 08:38:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.067247 on epoch=289
05/21/2022 08:38:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.076821 on epoch=293
05/21/2022 08:38:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.065586 on epoch=296
05/21/2022 08:38:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.048502 on epoch=299
05/21/2022 08:38:45 - INFO - __main__ - Global step 900 Train loss 0.069467 ACC 0.5625 on epoch=299
05/21/2022 08:38:45 - INFO - __main__ - save last model!
05/21/2022 08:38:46 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:38:46 - INFO - __main__ - Printing 3 examples
05/21/2022 08:38:46 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/21/2022 08:38:46 - INFO - __main__ - ['contradiction']
05/21/2022 08:38:46 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/21/2022 08:38:46 - INFO - __main__ - ['contradiction']
05/21/2022 08:38:46 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/21/2022 08:38:46 - INFO - __main__ - ['contradiction']
05/21/2022 08:38:46 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:38:46 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:38:46 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:38:46 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:38:46 - INFO - __main__ - Printing 3 examples
05/21/2022 08:38:46 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
05/21/2022 08:38:46 - INFO - __main__ - ['contradiction']
05/21/2022 08:38:46 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
05/21/2022 08:38:46 - INFO - __main__ - ['contradiction']
05/21/2022 08:38:46 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
05/21/2022 08:38:46 - INFO - __main__ - ['contradiction']
05/21/2022 08:38:46 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:38:46 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:38:46 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:38:48 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 08:38:48 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 08:38:48 - INFO - __main__ - Printing 3 examples
05/21/2022 08:38:48 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 08:38:48 - INFO - __main__ - ['contradiction']
05/21/2022 08:38:48 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 08:38:48 - INFO - __main__ - ['neutral']
05/21/2022 08:38:48 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 08:38:48 - INFO - __main__ - ['entailment']
05/21/2022 08:38:48 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:38:48 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:38:48 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 08:38:49 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_21_0.0005_8_predictions.txt
05/21/2022 08:38:49 - INFO - __main__ - ACC on test data: 0.6071
05/21/2022 08:38:50 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.0005, bsz=8, dev_performance=0.71875, test_performance=0.6071428571428571
05/21/2022 08:38:50 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.0003, bsz=8 ...
05/21/2022 08:38:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:38:50 - INFO - __main__ - Starting training!
05/21/2022 08:38:51 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:38:51 - INFO - __main__ - Printing 3 examples
05/21/2022 08:38:51 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/21/2022 08:38:51 - INFO - __main__ - ['contradiction']
05/21/2022 08:38:51 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/21/2022 08:38:51 - INFO - __main__ - ['contradiction']
05/21/2022 08:38:51 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/21/2022 08:38:51 - INFO - __main__ - ['contradiction']
05/21/2022 08:38:51 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:38:51 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:38:51 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:38:51 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:38:51 - INFO - __main__ - Printing 3 examples
05/21/2022 08:38:51 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
05/21/2022 08:38:51 - INFO - __main__ - ['contradiction']
05/21/2022 08:38:51 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
05/21/2022 08:38:51 - INFO - __main__ - ['contradiction']
05/21/2022 08:38:51 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
05/21/2022 08:38:51 - INFO - __main__ - ['contradiction']
05/21/2022 08:38:51 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:38:51 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:38:51 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:38:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:38:55 - INFO - __main__ - Starting training!
05/21/2022 08:38:57 - INFO - __main__ - Step 10 Global step 10 Train loss 17.883980 on epoch=3
05/21/2022 08:39:00 - INFO - __main__ - Step 20 Global step 20 Train loss 14.770889 on epoch=6
05/21/2022 08:39:03 - INFO - __main__ - Step 30 Global step 30 Train loss 10.933594 on epoch=9
05/21/2022 08:39:06 - INFO - __main__ - Step 40 Global step 40 Train loss 8.009176 on epoch=13
05/21/2022 08:39:09 - INFO - __main__ - Step 50 Global step 50 Train loss 7.704830 on epoch=16
05/21/2022 08:39:10 - INFO - __main__ - Global step 50 Train loss 11.860495 ACC 0.03125 on epoch=16
05/21/2022 08:39:13 - INFO - __main__ - Step 60 Global step 60 Train loss 5.658946 on epoch=19
05/21/2022 08:39:16 - INFO - __main__ - Step 70 Global step 70 Train loss 4.613204 on epoch=23
05/21/2022 08:39:19 - INFO - __main__ - Step 80 Global step 80 Train loss 3.662251 on epoch=26
05/21/2022 08:39:21 - INFO - __main__ - Step 90 Global step 90 Train loss 3.891008 on epoch=29
05/21/2022 08:39:24 - INFO - __main__ - Step 100 Global step 100 Train loss 2.649142 on epoch=33
05/21/2022 08:39:25 - INFO - __main__ - Global step 100 Train loss 4.094910 ACC 0.15625 on epoch=33
05/21/2022 08:39:28 - INFO - __main__ - Step 110 Global step 110 Train loss 2.359614 on epoch=36
05/21/2022 08:39:31 - INFO - __main__ - Step 120 Global step 120 Train loss 2.777361 on epoch=39
05/21/2022 08:39:33 - INFO - __main__ - Step 130 Global step 130 Train loss 2.316587 on epoch=43
05/21/2022 08:39:36 - INFO - __main__ - Step 140 Global step 140 Train loss 1.722988 on epoch=46
05/21/2022 08:39:39 - INFO - __main__ - Step 150 Global step 150 Train loss 2.601162 on epoch=49
05/21/2022 08:39:39 - INFO - __main__ - Global step 150 Train loss 2.355542 ACC 0.5 on epoch=49
05/21/2022 08:39:42 - INFO - __main__ - Step 160 Global step 160 Train loss 2.716719 on epoch=53
05/21/2022 08:39:45 - INFO - __main__ - Step 170 Global step 170 Train loss 2.475309 on epoch=56
05/21/2022 08:39:48 - INFO - __main__ - Step 180 Global step 180 Train loss 2.054110 on epoch=59
05/21/2022 08:39:50 - INFO - __main__ - Step 190 Global step 190 Train loss 2.194606 on epoch=63
05/21/2022 08:39:53 - INFO - __main__ - Step 200 Global step 200 Train loss 1.634800 on epoch=66
05/21/2022 08:39:54 - INFO - __main__ - Global step 200 Train loss 2.215109 ACC 0.5 on epoch=66
05/21/2022 08:39:56 - INFO - __main__ - Step 210 Global step 210 Train loss 1.730690 on epoch=69
05/21/2022 08:39:59 - INFO - __main__ - Step 220 Global step 220 Train loss 1.655513 on epoch=73
05/21/2022 08:40:02 - INFO - __main__ - Step 230 Global step 230 Train loss 1.164503 on epoch=76
05/21/2022 08:40:04 - INFO - __main__ - Step 240 Global step 240 Train loss 1.192025 on epoch=79
05/21/2022 08:40:07 - INFO - __main__ - Step 250 Global step 250 Train loss 1.580746 on epoch=83
05/21/2022 08:40:07 - INFO - __main__ - Global step 250 Train loss 1.464695 ACC 0.03125 on epoch=83
05/21/2022 08:40:10 - INFO - __main__ - Step 260 Global step 260 Train loss 1.375498 on epoch=86
05/21/2022 08:40:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.955589 on epoch=89
05/21/2022 08:40:15 - INFO - __main__ - Step 280 Global step 280 Train loss 1.326697 on epoch=93
05/21/2022 08:40:18 - INFO - __main__ - Step 290 Global step 290 Train loss 1.120721 on epoch=96
05/21/2022 08:40:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.907115 on epoch=99
05/21/2022 08:40:22 - INFO - __main__ - Global step 300 Train loss 1.137124 ACC 0.71875 on epoch=99
05/21/2022 08:40:25 - INFO - __main__ - Step 310 Global step 310 Train loss 1.101985 on epoch=103
05/21/2022 08:40:28 - INFO - __main__ - Step 320 Global step 320 Train loss 1.128769 on epoch=106
05/21/2022 08:40:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.798851 on epoch=109
05/21/2022 08:40:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.964400 on epoch=113
05/21/2022 08:40:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.979712 on epoch=116
05/21/2022 08:40:37 - INFO - __main__ - Global step 350 Train loss 0.994743 ACC 0.46875 on epoch=116
05/21/2022 08:40:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.781710 on epoch=119
05/21/2022 08:40:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.952246 on epoch=123
05/21/2022 08:40:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.929535 on epoch=126
05/21/2022 08:40:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.827576 on epoch=129
05/21/2022 08:40:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.762293 on epoch=133
05/21/2022 08:40:51 - INFO - __main__ - Global step 400 Train loss 0.850672 ACC 0.46875 on epoch=133
05/21/2022 08:40:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.840737 on epoch=136
05/21/2022 08:40:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.750946 on epoch=139
05/21/2022 08:40:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.750880 on epoch=143
05/21/2022 08:41:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.698794 on epoch=146
05/21/2022 08:41:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.769756 on epoch=149
05/21/2022 08:41:05 - INFO - __main__ - Global step 450 Train loss 0.762223 ACC 0.5 on epoch=149
05/21/2022 08:41:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.667942 on epoch=153
05/21/2022 08:41:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.763717 on epoch=156
05/21/2022 08:41:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.710205 on epoch=159
05/21/2022 08:41:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.560238 on epoch=163
05/21/2022 08:41:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.602427 on epoch=166
05/21/2022 08:41:19 - INFO - __main__ - Global step 500 Train loss 0.660906 ACC 0.46875 on epoch=166
05/21/2022 08:41:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.660039 on epoch=169
05/21/2022 08:41:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.572742 on epoch=173
05/21/2022 08:41:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.624090 on epoch=176
05/21/2022 08:41:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.564782 on epoch=179
05/21/2022 08:41:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.546362 on epoch=183
05/21/2022 08:41:34 - INFO - __main__ - Global step 550 Train loss 0.593603 ACC 0.4375 on epoch=183
05/21/2022 08:41:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.524658 on epoch=186
05/21/2022 08:41:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.494267 on epoch=189
05/21/2022 08:41:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.572787 on epoch=193
05/21/2022 08:41:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.463228 on epoch=196
05/21/2022 08:41:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.531457 on epoch=199
05/21/2022 08:41:48 - INFO - __main__ - Global step 600 Train loss 0.517279 ACC 0.46875 on epoch=199
05/21/2022 08:41:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.498446 on epoch=203
05/21/2022 08:41:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.466893 on epoch=206
05/21/2022 08:41:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.545255 on epoch=209
05/21/2022 08:42:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.431586 on epoch=213
05/21/2022 08:42:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.476364 on epoch=216
05/21/2022 08:42:03 - INFO - __main__ - Global step 650 Train loss 0.483709 ACC 0.5 on epoch=216
05/21/2022 08:42:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.444959 on epoch=219
05/21/2022 08:42:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.401046 on epoch=223
05/21/2022 08:42:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.418204 on epoch=226
05/21/2022 08:42:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.425628 on epoch=229
05/21/2022 08:42:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.496670 on epoch=233
05/21/2022 08:42:18 - INFO - __main__ - Global step 700 Train loss 0.437301 ACC 0.09375 on epoch=233
05/21/2022 08:42:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.464906 on epoch=236
05/21/2022 08:42:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.477384 on epoch=239
05/21/2022 08:42:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.393671 on epoch=243
05/21/2022 08:42:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.395660 on epoch=246
05/21/2022 08:42:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.396940 on epoch=249
05/21/2022 08:42:32 - INFO - __main__ - Global step 750 Train loss 0.425712 ACC 0.34375 on epoch=249
05/21/2022 08:42:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.473413 on epoch=253
05/21/2022 08:42:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.426653 on epoch=256
05/21/2022 08:42:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.436967 on epoch=259
05/21/2022 08:42:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.410321 on epoch=263
05/21/2022 08:42:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.389657 on epoch=266
05/21/2022 08:42:47 - INFO - __main__ - Global step 800 Train loss 0.427402 ACC 0.5 on epoch=266
05/21/2022 08:42:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.347043 on epoch=269
05/21/2022 08:42:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.367236 on epoch=273
05/21/2022 08:42:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.363969 on epoch=276
05/21/2022 08:42:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.394333 on epoch=279
05/21/2022 08:43:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.403736 on epoch=283
05/21/2022 08:43:02 - INFO - __main__ - Global step 850 Train loss 0.375263 ACC 0.46875 on epoch=283
05/21/2022 08:43:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.391917 on epoch=286
05/21/2022 08:43:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.375228 on epoch=289
05/21/2022 08:43:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.345440 on epoch=293
05/21/2022 08:43:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.356964 on epoch=296
05/21/2022 08:43:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.344525 on epoch=299
05/21/2022 08:43:16 - INFO - __main__ - Global step 900 Train loss 0.362815 ACC 0.40625 on epoch=299
05/21/2022 08:43:16 - INFO - __main__ - save last model!
05/21/2022 08:43:17 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:43:17 - INFO - __main__ - Printing 3 examples
05/21/2022 08:43:17 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/21/2022 08:43:17 - INFO - __main__ - ['contradiction']
05/21/2022 08:43:17 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/21/2022 08:43:17 - INFO - __main__ - ['contradiction']
05/21/2022 08:43:17 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/21/2022 08:43:17 - INFO - __main__ - ['contradiction']
05/21/2022 08:43:17 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:43:17 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:43:17 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:43:17 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:43:17 - INFO - __main__ - Printing 3 examples
05/21/2022 08:43:17 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
05/21/2022 08:43:17 - INFO - __main__ - ['contradiction']
05/21/2022 08:43:17 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
05/21/2022 08:43:17 - INFO - __main__ - ['contradiction']
05/21/2022 08:43:17 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
05/21/2022 08:43:17 - INFO - __main__ - ['contradiction']
05/21/2022 08:43:17 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:43:17 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:43:17 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:43:19 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 08:43:19 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 08:43:19 - INFO - __main__ - Printing 3 examples
05/21/2022 08:43:19 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 08:43:19 - INFO - __main__ - ['contradiction']
05/21/2022 08:43:19 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 08:43:19 - INFO - __main__ - ['neutral']
05/21/2022 08:43:19 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 08:43:19 - INFO - __main__ - ['entailment']
05/21/2022 08:43:19 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:43:19 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:43:19 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 08:43:21 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_21_0.0003_8_predictions.txt
05/21/2022 08:43:21 - INFO - __main__ - ACC on test data: 0.3750
05/21/2022 08:43:21 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.0003, bsz=8, dev_performance=0.71875, test_performance=0.375
05/21/2022 08:43:21 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.0002, bsz=8 ...
05/21/2022 08:43:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:43:22 - INFO - __main__ - Starting training!
05/21/2022 08:43:22 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:43:22 - INFO - __main__ - Printing 3 examples
05/21/2022 08:43:22 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/21/2022 08:43:22 - INFO - __main__ - ['contradiction']
05/21/2022 08:43:22 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/21/2022 08:43:22 - INFO - __main__ - ['contradiction']
05/21/2022 08:43:22 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/21/2022 08:43:22 - INFO - __main__ - ['contradiction']
05/21/2022 08:43:22 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:43:22 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:43:23 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:43:23 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:43:23 - INFO - __main__ - Printing 3 examples
05/21/2022 08:43:23 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
05/21/2022 08:43:23 - INFO - __main__ - ['contradiction']
05/21/2022 08:43:23 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
05/21/2022 08:43:23 - INFO - __main__ - ['contradiction']
05/21/2022 08:43:23 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
05/21/2022 08:43:23 - INFO - __main__ - ['contradiction']
05/21/2022 08:43:23 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:43:23 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:43:23 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:43:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:43:27 - INFO - __main__ - Starting training!
05/21/2022 08:43:29 - INFO - __main__ - Step 10 Global step 10 Train loss 17.865986 on epoch=3
05/21/2022 08:43:32 - INFO - __main__ - Step 20 Global step 20 Train loss 15.517568 on epoch=6
05/21/2022 08:43:35 - INFO - __main__ - Step 30 Global step 30 Train loss 11.925739 on epoch=9
05/21/2022 08:43:37 - INFO - __main__ - Step 40 Global step 40 Train loss 10.517013 on epoch=13
05/21/2022 08:43:40 - INFO - __main__ - Step 50 Global step 50 Train loss 8.266142 on epoch=16
05/21/2022 08:43:43 - INFO - __main__ - Global step 50 Train loss 12.818489 ACC 0.03125 on epoch=16
05/21/2022 08:43:47 - INFO - __main__ - Step 60 Global step 60 Train loss 6.969693 on epoch=19
05/21/2022 08:43:50 - INFO - __main__ - Step 70 Global step 70 Train loss 6.521017 on epoch=23
05/21/2022 08:43:53 - INFO - __main__ - Step 80 Global step 80 Train loss 5.964061 on epoch=26
05/21/2022 08:43:56 - INFO - __main__ - Step 90 Global step 90 Train loss 5.196848 on epoch=29
05/21/2022 08:43:58 - INFO - __main__ - Step 100 Global step 100 Train loss 4.175569 on epoch=33
05/21/2022 08:43:59 - INFO - __main__ - Global step 100 Train loss 5.765437 ACC 0.25 on epoch=33
05/21/2022 08:44:02 - INFO - __main__ - Step 110 Global step 110 Train loss 4.432315 on epoch=36
05/21/2022 08:44:05 - INFO - __main__ - Step 120 Global step 120 Train loss 3.463728 on epoch=39
05/21/2022 08:44:07 - INFO - __main__ - Step 130 Global step 130 Train loss 2.778622 on epoch=43
05/21/2022 08:44:10 - INFO - __main__ - Step 140 Global step 140 Train loss 2.642925 on epoch=46
05/21/2022 08:44:13 - INFO - __main__ - Step 150 Global step 150 Train loss 2.358929 on epoch=49
05/21/2022 08:44:13 - INFO - __main__ - Global step 150 Train loss 3.135304 ACC 0.5 on epoch=49
05/21/2022 08:44:17 - INFO - __main__ - Step 160 Global step 160 Train loss 2.949240 on epoch=53
05/21/2022 08:44:19 - INFO - __main__ - Step 170 Global step 170 Train loss 2.351861 on epoch=56
05/21/2022 08:44:22 - INFO - __main__ - Step 180 Global step 180 Train loss 2.365159 on epoch=59
05/21/2022 08:44:24 - INFO - __main__ - Step 190 Global step 190 Train loss 2.144425 on epoch=63
05/21/2022 08:44:27 - INFO - __main__ - Step 200 Global step 200 Train loss 1.849633 on epoch=66
05/21/2022 08:44:28 - INFO - __main__ - Global step 200 Train loss 2.332064 ACC 0.40625 on epoch=66
05/21/2022 08:44:30 - INFO - __main__ - Step 210 Global step 210 Train loss 1.880533 on epoch=69
05/21/2022 08:44:33 - INFO - __main__ - Step 220 Global step 220 Train loss 2.403905 on epoch=73
05/21/2022 08:44:36 - INFO - __main__ - Step 230 Global step 230 Train loss 2.144798 on epoch=76
05/21/2022 08:44:39 - INFO - __main__ - Step 240 Global step 240 Train loss 1.447082 on epoch=79
05/21/2022 08:44:41 - INFO - __main__ - Step 250 Global step 250 Train loss 1.980792 on epoch=83
05/21/2022 08:44:42 - INFO - __main__ - Global step 250 Train loss 1.971422 ACC 0.5 on epoch=83
05/21/2022 08:44:45 - INFO - __main__ - Step 260 Global step 260 Train loss 2.041331 on epoch=86
05/21/2022 08:44:48 - INFO - __main__ - Step 270 Global step 270 Train loss 1.470147 on epoch=89
05/21/2022 08:44:51 - INFO - __main__ - Step 280 Global step 280 Train loss 1.351584 on epoch=93
05/21/2022 08:44:54 - INFO - __main__ - Step 290 Global step 290 Train loss 1.300498 on epoch=96
05/21/2022 08:44:57 - INFO - __main__ - Step 300 Global step 300 Train loss 1.626443 on epoch=99
05/21/2022 08:44:57 - INFO - __main__ - Global step 300 Train loss 1.558000 ACC 0.53125 on epoch=99
05/21/2022 08:45:00 - INFO - __main__ - Step 310 Global step 310 Train loss 1.345333 on epoch=103
05/21/2022 08:45:03 - INFO - __main__ - Step 320 Global step 320 Train loss 1.191231 on epoch=106
05/21/2022 08:45:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.768781 on epoch=109
05/21/2022 08:45:08 - INFO - __main__ - Step 340 Global step 340 Train loss 1.178682 on epoch=113
05/21/2022 08:45:11 - INFO - __main__ - Step 350 Global step 350 Train loss 1.195448 on epoch=116
05/21/2022 08:45:12 - INFO - __main__ - Global step 350 Train loss 1.135895 ACC 0.71875 on epoch=116
05/21/2022 08:45:15 - INFO - __main__ - Step 360 Global step 360 Train loss 1.369120 on epoch=119
05/21/2022 08:45:18 - INFO - __main__ - Step 370 Global step 370 Train loss 1.127189 on epoch=123
05/21/2022 08:45:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.971240 on epoch=126
05/21/2022 08:45:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.982180 on epoch=129
05/21/2022 08:45:27 - INFO - __main__ - Step 400 Global step 400 Train loss 1.070975 on epoch=133
05/21/2022 08:45:27 - INFO - __main__ - Global step 400 Train loss 1.104141 ACC 0.21875 on epoch=133
05/21/2022 08:45:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.940539 on epoch=136
05/21/2022 08:45:33 - INFO - __main__ - Step 420 Global step 420 Train loss 1.016370 on epoch=139
05/21/2022 08:45:36 - INFO - __main__ - Step 430 Global step 430 Train loss 1.069418 on epoch=143
05/21/2022 08:45:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.787344 on epoch=146
05/21/2022 08:45:42 - INFO - __main__ - Step 450 Global step 450 Train loss 1.268100 on epoch=149
05/21/2022 08:45:42 - INFO - __main__ - Global step 450 Train loss 1.016354 ACC 0.5 on epoch=149
05/21/2022 08:45:45 - INFO - __main__ - Step 460 Global step 460 Train loss 1.059968 on epoch=153
05/21/2022 08:45:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.797994 on epoch=156
05/21/2022 08:45:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.942415 on epoch=159
05/21/2022 08:45:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.907881 on epoch=163
05/21/2022 08:45:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.856765 on epoch=166
05/21/2022 08:45:56 - INFO - __main__ - Global step 500 Train loss 0.913005 ACC 0.4375 on epoch=166
05/21/2022 08:45:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.807405 on epoch=169
05/21/2022 08:46:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.843632 on epoch=173
05/21/2022 08:46:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.870442 on epoch=176
05/21/2022 08:46:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.776868 on epoch=179
05/21/2022 08:46:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.627420 on epoch=183
05/21/2022 08:46:11 - INFO - __main__ - Global step 550 Train loss 0.785154 ACC 0.4375 on epoch=183
05/21/2022 08:46:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.654170 on epoch=186
05/21/2022 08:46:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.798782 on epoch=189
05/21/2022 08:46:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.854055 on epoch=193
05/21/2022 08:46:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.557921 on epoch=196
05/21/2022 08:46:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.631424 on epoch=199
05/21/2022 08:46:25 - INFO - __main__ - Global step 600 Train loss 0.699270 ACC 0.34375 on epoch=199
05/21/2022 08:46:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.654795 on epoch=203
05/21/2022 08:46:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.700053 on epoch=206
05/21/2022 08:46:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.720155 on epoch=209
05/21/2022 08:46:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.623562 on epoch=213
05/21/2022 08:46:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.711708 on epoch=216
05/21/2022 08:46:40 - INFO - __main__ - Global step 650 Train loss 0.682055 ACC 0.46875 on epoch=216
05/21/2022 08:46:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.697959 on epoch=219
05/21/2022 08:46:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.560682 on epoch=223
05/21/2022 08:46:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.664511 on epoch=226
05/21/2022 08:46:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.574091 on epoch=229
05/21/2022 08:46:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.611555 on epoch=233
05/21/2022 08:46:54 - INFO - __main__ - Global step 700 Train loss 0.621760 ACC 0.3125 on epoch=233
05/21/2022 08:46:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.641196 on epoch=236
05/21/2022 08:46:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.565806 on epoch=239
05/21/2022 08:47:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.479642 on epoch=243
05/21/2022 08:47:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.604394 on epoch=246
05/21/2022 08:47:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.497316 on epoch=249
05/21/2022 08:47:08 - INFO - __main__ - Global step 750 Train loss 0.557671 ACC 0.5 on epoch=249
05/21/2022 08:47:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.556118 on epoch=253
05/21/2022 08:47:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.516831 on epoch=256
05/21/2022 08:47:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.480224 on epoch=259
05/21/2022 08:47:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.600461 on epoch=263
05/21/2022 08:47:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.452652 on epoch=266
05/21/2022 08:47:23 - INFO - __main__ - Global step 800 Train loss 0.521257 ACC 0.53125 on epoch=266
05/21/2022 08:47:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.471872 on epoch=269
05/21/2022 08:47:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.487065 on epoch=273
05/21/2022 08:47:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.418615 on epoch=276
05/21/2022 08:47:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.517954 on epoch=279
05/21/2022 08:47:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.445350 on epoch=283
05/21/2022 08:47:37 - INFO - __main__ - Global step 850 Train loss 0.468171 ACC 0.84375 on epoch=283
05/21/2022 08:47:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.353569 on epoch=286
05/21/2022 08:47:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.379984 on epoch=289
05/21/2022 08:47:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.291657 on epoch=293
05/21/2022 08:47:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.364500 on epoch=296
05/21/2022 08:47:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.415328 on epoch=299
05/21/2022 08:47:52 - INFO - __main__ - Global step 900 Train loss 0.361008 ACC 0.5 on epoch=299
05/21/2022 08:47:52 - INFO - __main__ - save last model!
05/21/2022 08:47:53 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:47:53 - INFO - __main__ - Printing 3 examples
05/21/2022 08:47:53 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/21/2022 08:47:53 - INFO - __main__ - ['contradiction']
05/21/2022 08:47:53 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/21/2022 08:47:53 - INFO - __main__ - ['contradiction']
05/21/2022 08:47:53 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/21/2022 08:47:53 - INFO - __main__ - ['contradiction']
05/21/2022 08:47:53 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:47:53 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:47:53 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:47:53 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:47:53 - INFO - __main__ - Printing 3 examples
05/21/2022 08:47:53 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
05/21/2022 08:47:53 - INFO - __main__ - ['contradiction']
05/21/2022 08:47:53 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
05/21/2022 08:47:53 - INFO - __main__ - ['contradiction']
05/21/2022 08:47:53 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
05/21/2022 08:47:53 - INFO - __main__ - ['contradiction']
05/21/2022 08:47:53 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:47:53 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:47:53 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:47:55 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 08:47:55 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 08:47:55 - INFO - __main__ - Printing 3 examples
05/21/2022 08:47:55 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 08:47:55 - INFO - __main__ - ['contradiction']
05/21/2022 08:47:55 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 08:47:55 - INFO - __main__ - ['neutral']
05/21/2022 08:47:55 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 08:47:55 - INFO - __main__ - ['entailment']
05/21/2022 08:47:55 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:47:55 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:47:55 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 08:47:56 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_21_0.0002_8_predictions.txt
05/21/2022 08:47:56 - INFO - __main__ - ACC on test data: 0.6250
05/21/2022 08:47:56 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.0002, bsz=8, dev_performance=0.84375, test_performance=0.625
05/21/2022 08:47:56 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.0001, bsz=8 ...
05/21/2022 08:47:57 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:47:57 - INFO - __main__ - Printing 3 examples
05/21/2022 08:47:57 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
05/21/2022 08:47:57 - INFO - __main__ - ['contradiction']
05/21/2022 08:47:57 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
05/21/2022 08:47:57 - INFO - __main__ - ['contradiction']
05/21/2022 08:47:57 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
05/21/2022 08:47:57 - INFO - __main__ - ['contradiction']
05/21/2022 08:47:57 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:47:57 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:47:57 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:47:57 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:47:57 - INFO - __main__ - Printing 3 examples
05/21/2022 08:47:57 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
05/21/2022 08:47:57 - INFO - __main__ - ['contradiction']
05/21/2022 08:47:57 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
05/21/2022 08:47:57 - INFO - __main__ - ['contradiction']
05/21/2022 08:47:57 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
05/21/2022 08:47:57 - INFO - __main__ - ['contradiction']
05/21/2022 08:47:57 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:47:57 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:47:57 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:47:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:47:58 - INFO - __main__ - Starting training!
05/21/2022 08:48:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:48:02 - INFO - __main__ - Starting training!
05/21/2022 08:48:04 - INFO - __main__ - Step 10 Global step 10 Train loss 17.458256 on epoch=3
05/21/2022 08:48:07 - INFO - __main__ - Step 20 Global step 20 Train loss 17.512932 on epoch=6
05/21/2022 08:48:10 - INFO - __main__ - Step 30 Global step 30 Train loss 15.179060 on epoch=9
05/21/2022 08:48:13 - INFO - __main__ - Step 40 Global step 40 Train loss 12.524983 on epoch=13
05/21/2022 08:48:15 - INFO - __main__ - Step 50 Global step 50 Train loss 11.024161 on epoch=16
05/21/2022 08:48:21 - INFO - __main__ - Global step 50 Train loss 14.739878 ACC 0.0 on epoch=16
05/21/2022 08:48:24 - INFO - __main__ - Step 60 Global step 60 Train loss 11.140974 on epoch=19
05/21/2022 08:48:27 - INFO - __main__ - Step 70 Global step 70 Train loss 9.914094 on epoch=23
05/21/2022 08:48:29 - INFO - __main__ - Step 80 Global step 80 Train loss 9.861036 on epoch=26
05/21/2022 08:48:32 - INFO - __main__ - Step 90 Global step 90 Train loss 8.691222 on epoch=29
05/21/2022 08:48:35 - INFO - __main__ - Step 100 Global step 100 Train loss 8.749198 on epoch=33
05/21/2022 08:48:38 - INFO - __main__ - Global step 100 Train loss 9.671305 ACC 0.0 on epoch=33
05/21/2022 08:48:41 - INFO - __main__ - Step 110 Global step 110 Train loss 8.178869 on epoch=36
05/21/2022 08:48:44 - INFO - __main__ - Step 120 Global step 120 Train loss 7.470286 on epoch=39
05/21/2022 08:48:47 - INFO - __main__ - Step 130 Global step 130 Train loss 6.749068 on epoch=43
05/21/2022 08:48:50 - INFO - __main__ - Step 140 Global step 140 Train loss 6.444604 on epoch=46
05/21/2022 08:48:52 - INFO - __main__ - Step 150 Global step 150 Train loss 6.357968 on epoch=49
05/21/2022 08:48:54 - INFO - __main__ - Global step 150 Train loss 7.040160 ACC 0.0 on epoch=49
05/21/2022 08:48:56 - INFO - __main__ - Step 160 Global step 160 Train loss 5.994840 on epoch=53
05/21/2022 08:48:59 - INFO - __main__ - Step 170 Global step 170 Train loss 5.438458 on epoch=56
05/21/2022 08:49:02 - INFO - __main__ - Step 180 Global step 180 Train loss 5.990800 on epoch=59
05/21/2022 08:49:05 - INFO - __main__ - Step 190 Global step 190 Train loss 5.516978 on epoch=63
05/21/2022 08:49:07 - INFO - __main__ - Step 200 Global step 200 Train loss 4.971193 on epoch=66
05/21/2022 08:49:09 - INFO - __main__ - Global step 200 Train loss 5.582454 ACC 0.0 on epoch=66
05/21/2022 08:49:11 - INFO - __main__ - Step 210 Global step 210 Train loss 5.089768 on epoch=69
05/21/2022 08:49:14 - INFO - __main__ - Step 220 Global step 220 Train loss 4.064639 on epoch=73
05/21/2022 08:49:18 - INFO - __main__ - Step 230 Global step 230 Train loss 4.712951 on epoch=76
05/21/2022 08:49:21 - INFO - __main__ - Step 240 Global step 240 Train loss 4.496560 on epoch=79
05/21/2022 08:49:24 - INFO - __main__ - Step 250 Global step 250 Train loss 3.712530 on epoch=83
05/21/2022 08:49:24 - INFO - __main__ - Global step 250 Train loss 4.415289 ACC 0.03125 on epoch=83
05/21/2022 08:49:27 - INFO - __main__ - Step 260 Global step 260 Train loss 4.343583 on epoch=86
05/21/2022 08:49:30 - INFO - __main__ - Step 270 Global step 270 Train loss 3.668590 on epoch=89
05/21/2022 08:49:33 - INFO - __main__ - Step 280 Global step 280 Train loss 2.617352 on epoch=93
05/21/2022 08:49:36 - INFO - __main__ - Step 290 Global step 290 Train loss 3.572001 on epoch=96
05/21/2022 08:49:38 - INFO - __main__ - Step 300 Global step 300 Train loss 3.370040 on epoch=99
05/21/2022 08:49:39 - INFO - __main__ - Global step 300 Train loss 3.514313 ACC 0.0 on epoch=99
05/21/2022 08:49:41 - INFO - __main__ - Step 310 Global step 310 Train loss 2.544584 on epoch=103
05/21/2022 08:49:44 - INFO - __main__ - Step 320 Global step 320 Train loss 2.425373 on epoch=106
05/21/2022 08:49:46 - INFO - __main__ - Step 330 Global step 330 Train loss 2.788986 on epoch=109
05/21/2022 08:49:49 - INFO - __main__ - Step 340 Global step 340 Train loss 3.374707 on epoch=113
05/21/2022 08:49:52 - INFO - __main__ - Step 350 Global step 350 Train loss 2.775785 on epoch=116
05/21/2022 08:49:52 - INFO - __main__ - Global step 350 Train loss 2.781887 ACC 0.34375 on epoch=116
05/21/2022 08:49:55 - INFO - __main__ - Step 360 Global step 360 Train loss 2.559336 on epoch=119
05/21/2022 08:49:58 - INFO - __main__ - Step 370 Global step 370 Train loss 2.685481 on epoch=123
05/21/2022 08:50:01 - INFO - __main__ - Step 380 Global step 380 Train loss 2.454350 on epoch=126
05/21/2022 08:50:04 - INFO - __main__ - Step 390 Global step 390 Train loss 2.273454 on epoch=129
05/21/2022 08:50:07 - INFO - __main__ - Step 400 Global step 400 Train loss 2.788641 on epoch=133
05/21/2022 08:50:07 - INFO - __main__ - Global step 400 Train loss 2.552253 ACC 0.5 on epoch=133
05/21/2022 08:50:10 - INFO - __main__ - Step 410 Global step 410 Train loss 2.483339 on epoch=136
05/21/2022 08:50:13 - INFO - __main__ - Step 420 Global step 420 Train loss 2.045261 on epoch=139
05/21/2022 08:50:16 - INFO - __main__ - Step 430 Global step 430 Train loss 2.632843 on epoch=143
05/21/2022 08:50:19 - INFO - __main__ - Step 440 Global step 440 Train loss 1.981108 on epoch=146
05/21/2022 08:50:22 - INFO - __main__ - Step 450 Global step 450 Train loss 2.430002 on epoch=149
05/21/2022 08:50:22 - INFO - __main__ - Global step 450 Train loss 2.314510 ACC 0.34375 on epoch=149
05/21/2022 08:50:25 - INFO - __main__ - Step 460 Global step 460 Train loss 1.846711 on epoch=153
05/21/2022 08:50:27 - INFO - __main__ - Step 470 Global step 470 Train loss 1.673982 on epoch=156
05/21/2022 08:50:30 - INFO - __main__ - Step 480 Global step 480 Train loss 2.233728 on epoch=159
05/21/2022 08:50:33 - INFO - __main__ - Step 490 Global step 490 Train loss 2.204515 on epoch=163
05/21/2022 08:50:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.974440 on epoch=166
05/21/2022 08:50:36 - INFO - __main__ - Global step 500 Train loss 1.786675 ACC 0.0 on epoch=166
05/21/2022 08:50:39 - INFO - __main__ - Step 510 Global step 510 Train loss 1.898750 on epoch=169
05/21/2022 08:50:41 - INFO - __main__ - Step 520 Global step 520 Train loss 1.345731 on epoch=173
05/21/2022 08:50:44 - INFO - __main__ - Step 530 Global step 530 Train loss 2.231788 on epoch=176
05/21/2022 08:50:47 - INFO - __main__ - Step 540 Global step 540 Train loss 1.519132 on epoch=179
05/21/2022 08:50:50 - INFO - __main__ - Step 550 Global step 550 Train loss 1.669683 on epoch=183
05/21/2022 08:50:50 - INFO - __main__ - Global step 550 Train loss 1.733017 ACC 0.28125 on epoch=183
05/21/2022 08:50:53 - INFO - __main__ - Step 560 Global step 560 Train loss 1.788174 on epoch=186
05/21/2022 08:50:56 - INFO - __main__ - Step 570 Global step 570 Train loss 1.912606 on epoch=189
05/21/2022 08:50:59 - INFO - __main__ - Step 580 Global step 580 Train loss 1.950065 on epoch=193
05/21/2022 08:51:02 - INFO - __main__ - Step 590 Global step 590 Train loss 1.449809 on epoch=196
05/21/2022 08:51:05 - INFO - __main__ - Step 600 Global step 600 Train loss 2.040528 on epoch=199
05/21/2022 08:51:05 - INFO - __main__ - Global step 600 Train loss 1.828236 ACC 0.4375 on epoch=199
05/21/2022 08:51:08 - INFO - __main__ - Step 610 Global step 610 Train loss 1.704407 on epoch=203
05/21/2022 08:51:11 - INFO - __main__ - Step 620 Global step 620 Train loss 1.317989 on epoch=206
05/21/2022 08:51:14 - INFO - __main__ - Step 630 Global step 630 Train loss 1.831717 on epoch=209
05/21/2022 08:51:17 - INFO - __main__ - Step 640 Global step 640 Train loss 1.578259 on epoch=213
05/21/2022 08:51:20 - INFO - __main__ - Step 650 Global step 650 Train loss 1.604377 on epoch=216
05/21/2022 08:51:20 - INFO - __main__ - Global step 650 Train loss 1.607350 ACC 0.4375 on epoch=216
05/21/2022 08:51:23 - INFO - __main__ - Step 660 Global step 660 Train loss 1.596393 on epoch=219
05/21/2022 08:51:26 - INFO - __main__ - Step 670 Global step 670 Train loss 1.335881 on epoch=223
05/21/2022 08:51:29 - INFO - __main__ - Step 680 Global step 680 Train loss 1.830206 on epoch=226
05/21/2022 08:51:32 - INFO - __main__ - Step 690 Global step 690 Train loss 1.528528 on epoch=229
05/21/2022 08:51:35 - INFO - __main__ - Step 700 Global step 700 Train loss 1.219349 on epoch=233
05/21/2022 08:51:35 - INFO - __main__ - Global step 700 Train loss 1.502071 ACC 0.40625 on epoch=233
05/21/2022 08:51:38 - INFO - __main__ - Step 710 Global step 710 Train loss 1.036768 on epoch=236
05/21/2022 08:51:40 - INFO - __main__ - Step 720 Global step 720 Train loss 1.112151 on epoch=239
05/21/2022 08:51:43 - INFO - __main__ - Step 730 Global step 730 Train loss 1.138405 on epoch=243
05/21/2022 08:51:46 - INFO - __main__ - Step 740 Global step 740 Train loss 1.116561 on epoch=246
05/21/2022 08:51:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.942389 on epoch=249
05/21/2022 08:51:49 - INFO - __main__ - Global step 750 Train loss 1.069255 ACC 0.71875 on epoch=249
05/21/2022 08:51:52 - INFO - __main__ - Step 760 Global step 760 Train loss 1.052311 on epoch=253
05/21/2022 08:51:55 - INFO - __main__ - Step 770 Global step 770 Train loss 1.372425 on epoch=256
05/21/2022 08:51:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.900417 on epoch=259
05/21/2022 08:52:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.998779 on epoch=263
05/21/2022 08:52:03 - INFO - __main__ - Step 800 Global step 800 Train loss 1.017505 on epoch=266
05/21/2022 08:52:03 - INFO - __main__ - Global step 800 Train loss 1.068287 ACC 0.875 on epoch=266
05/21/2022 08:52:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.847196 on epoch=269
05/21/2022 08:52:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.700025 on epoch=273
05/21/2022 08:52:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.904795 on epoch=276
05/21/2022 08:52:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.692834 on epoch=279
05/21/2022 08:52:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.725342 on epoch=283
05/21/2022 08:52:17 - INFO - __main__ - Global step 850 Train loss 0.774039 ACC 0.875 on epoch=283
05/21/2022 08:52:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.949916 on epoch=286
05/21/2022 08:52:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.765543 on epoch=289
05/21/2022 08:52:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.675498 on epoch=293
05/21/2022 08:52:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.524438 on epoch=296
05/21/2022 08:52:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.656385 on epoch=299
05/21/2022 08:52:32 - INFO - __main__ - Global step 900 Train loss 0.714356 ACC 0.875 on epoch=299
05/21/2022 08:52:32 - INFO - __main__ - save last model!
05/21/2022 08:52:33 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:52:33 - INFO - __main__ - Printing 3 examples
05/21/2022 08:52:33 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/21/2022 08:52:33 - INFO - __main__ - ['contradiction']
05/21/2022 08:52:33 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/21/2022 08:52:33 - INFO - __main__ - ['contradiction']
05/21/2022 08:52:33 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/21/2022 08:52:33 - INFO - __main__ - ['contradiction']
05/21/2022 08:52:33 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:52:33 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:52:33 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:52:33 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:52:33 - INFO - __main__ - Printing 3 examples
05/21/2022 08:52:33 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
05/21/2022 08:52:33 - INFO - __main__ - ['contradiction']
05/21/2022 08:52:33 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
05/21/2022 08:52:33 - INFO - __main__ - ['contradiction']
05/21/2022 08:52:33 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
05/21/2022 08:52:33 - INFO - __main__ - ['contradiction']
05/21/2022 08:52:33 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:52:33 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:52:33 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:52:35 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 08:52:35 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 08:52:35 - INFO - __main__ - Printing 3 examples
05/21/2022 08:52:35 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 08:52:35 - INFO - __main__ - ['contradiction']
05/21/2022 08:52:35 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 08:52:35 - INFO - __main__ - ['neutral']
05/21/2022 08:52:35 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 08:52:35 - INFO - __main__ - ['entailment']
05/21/2022 08:52:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:52:35 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:52:35 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 08:52:36 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_21_0.0001_8_predictions.txt
05/21/2022 08:52:36 - INFO - __main__ - ACC on test data: 0.7321
05/21/2022 08:52:36 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.0001, bsz=8, dev_performance=0.875, test_performance=0.7321428571428571
05/21/2022 08:52:36 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.0005, bsz=8 ...
05/21/2022 08:52:37 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:52:37 - INFO - __main__ - Printing 3 examples
05/21/2022 08:52:37 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/21/2022 08:52:37 - INFO - __main__ - ['contradiction']
05/21/2022 08:52:37 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/21/2022 08:52:37 - INFO - __main__ - ['contradiction']
05/21/2022 08:52:37 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/21/2022 08:52:37 - INFO - __main__ - ['contradiction']
05/21/2022 08:52:37 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:52:37 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:52:37 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:52:37 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:52:37 - INFO - __main__ - Printing 3 examples
05/21/2022 08:52:37 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
05/21/2022 08:52:37 - INFO - __main__ - ['contradiction']
05/21/2022 08:52:37 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
05/21/2022 08:52:37 - INFO - __main__ - ['contradiction']
05/21/2022 08:52:37 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
05/21/2022 08:52:37 - INFO - __main__ - ['contradiction']
05/21/2022 08:52:37 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:52:37 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:52:37 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:52:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:52:38 - INFO - __main__ - Starting training!
05/21/2022 08:52:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:52:41 - INFO - __main__ - Starting training!
05/21/2022 08:52:44 - INFO - __main__ - Step 10 Global step 10 Train loss 17.411467 on epoch=3
05/21/2022 08:52:46 - INFO - __main__ - Step 20 Global step 20 Train loss 14.794703 on epoch=6
05/21/2022 08:52:49 - INFO - __main__ - Step 30 Global step 30 Train loss 10.820940 on epoch=9
05/21/2022 08:52:52 - INFO - __main__ - Step 40 Global step 40 Train loss 6.750342 on epoch=13
05/21/2022 08:52:55 - INFO - __main__ - Step 50 Global step 50 Train loss 4.236140 on epoch=16
05/21/2022 08:52:56 - INFO - __main__ - Global step 50 Train loss 10.802718 ACC 0.3125 on epoch=16
05/21/2022 08:52:59 - INFO - __main__ - Step 60 Global step 60 Train loss 2.857960 on epoch=19
05/21/2022 08:53:02 - INFO - __main__ - Step 70 Global step 70 Train loss 3.412297 on epoch=23
05/21/2022 08:53:05 - INFO - __main__ - Step 80 Global step 80 Train loss 2.111652 on epoch=26
05/21/2022 08:53:08 - INFO - __main__ - Step 90 Global step 90 Train loss 2.066828 on epoch=29
05/21/2022 08:53:10 - INFO - __main__ - Step 100 Global step 100 Train loss 2.104773 on epoch=33
05/21/2022 08:53:10 - INFO - __main__ - Global step 100 Train loss 2.510702 ACC 0.4375 on epoch=33
05/21/2022 08:53:14 - INFO - __main__ - Step 110 Global step 110 Train loss 1.124453 on epoch=36
05/21/2022 08:53:16 - INFO - __main__ - Step 120 Global step 120 Train loss 1.204793 on epoch=39
05/21/2022 08:53:19 - INFO - __main__ - Step 130 Global step 130 Train loss 1.430546 on epoch=43
05/21/2022 08:53:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.905472 on epoch=46
05/21/2022 08:53:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.910571 on epoch=49
05/21/2022 08:53:25 - INFO - __main__ - Global step 150 Train loss 1.115167 ACC 0.4375 on epoch=49
05/21/2022 08:53:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.949755 on epoch=53
05/21/2022 08:53:31 - INFO - __main__ - Step 170 Global step 170 Train loss 1.066290 on epoch=56
05/21/2022 08:53:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.585870 on epoch=59
05/21/2022 08:53:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.685811 on epoch=63
05/21/2022 08:53:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.580661 on epoch=66
05/21/2022 08:53:40 - INFO - __main__ - Global step 200 Train loss 0.773677 ACC 0.53125 on epoch=66
05/21/2022 08:53:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.576330 on epoch=69
05/21/2022 08:53:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.415899 on epoch=73
05/21/2022 08:53:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.218163 on epoch=76
05/21/2022 08:53:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.397329 on epoch=79
05/21/2022 08:53:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.264411 on epoch=83
05/21/2022 08:53:54 - INFO - __main__ - Global step 250 Train loss 0.374427 ACC 0.78125 on epoch=83
05/21/2022 08:53:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.236235 on epoch=86
05/21/2022 08:54:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.100452 on epoch=89
05/21/2022 08:54:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.100271 on epoch=93
05/21/2022 08:54:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.234537 on epoch=96
05/21/2022 08:54:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.160515 on epoch=99
05/21/2022 08:54:10 - INFO - __main__ - Global step 300 Train loss 0.166402 ACC 0.78125 on epoch=99
05/21/2022 08:54:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.102299 on epoch=103
05/21/2022 08:54:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.040321 on epoch=106
05/21/2022 08:54:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.098825 on epoch=109
05/21/2022 08:54:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.071321 on epoch=113
05/21/2022 08:54:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.020709 on epoch=116
05/21/2022 08:54:24 - INFO - __main__ - Global step 350 Train loss 0.066695 ACC 0.75 on epoch=116
05/21/2022 08:54:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.341232 on epoch=119
05/21/2022 08:54:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.046580 on epoch=123
05/21/2022 08:54:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.054012 on epoch=126
05/21/2022 08:54:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.008384 on epoch=129
05/21/2022 08:54:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.029630 on epoch=133
05/21/2022 08:54:38 - INFO - __main__ - Global step 400 Train loss 0.095967 ACC 0.84375 on epoch=133
05/21/2022 08:54:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.007680 on epoch=136
05/21/2022 08:54:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.016163 on epoch=139
05/21/2022 08:54:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.003941 on epoch=143
05/21/2022 08:54:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.008709 on epoch=146
05/21/2022 08:54:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.066614 on epoch=149
05/21/2022 08:54:53 - INFO - __main__ - Global step 450 Train loss 0.020621 ACC 0.78125 on epoch=149
05/21/2022 08:54:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.020597 on epoch=153
05/21/2022 08:54:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.017862 on epoch=156
05/21/2022 08:55:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.109160 on epoch=159
05/21/2022 08:55:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.023842 on epoch=163
05/21/2022 08:55:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.013471 on epoch=166
05/21/2022 08:55:08 - INFO - __main__ - Global step 500 Train loss 0.036986 ACC 0.78125 on epoch=166
05/21/2022 08:55:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.021919 on epoch=169
05/21/2022 08:55:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.012377 on epoch=173
05/21/2022 08:55:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.005999 on epoch=176
05/21/2022 08:55:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.024776 on epoch=179
05/21/2022 08:55:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.000826 on epoch=183
05/21/2022 08:55:23 - INFO - __main__ - Global step 550 Train loss 0.013180 ACC 0.6875 on epoch=183
05/21/2022 08:55:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.006375 on epoch=186
05/21/2022 08:55:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.051830 on epoch=189
05/21/2022 08:55:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.000104 on epoch=193
05/21/2022 08:55:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.123402 on epoch=196
05/21/2022 08:55:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.011040 on epoch=199
05/21/2022 08:55:37 - INFO - __main__ - Global step 600 Train loss 0.038550 ACC 0.78125 on epoch=199
05/21/2022 08:55:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.106015 on epoch=203
05/21/2022 08:55:42 - INFO - __main__ - Step 620 Global step 620 Train loss 0.058220 on epoch=206
05/21/2022 08:55:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.002688 on epoch=209
05/21/2022 08:55:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.015343 on epoch=213
05/21/2022 08:55:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.005950 on epoch=216
05/21/2022 08:55:51 - INFO - __main__ - Global step 650 Train loss 0.037643 ACC 0.84375 on epoch=216
05/21/2022 08:55:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.020432 on epoch=219
05/21/2022 08:55:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.237836 on epoch=223
05/21/2022 08:55:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.736000 on epoch=226
05/21/2022 08:56:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.505764 on epoch=229
05/21/2022 08:56:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.426469 on epoch=233
05/21/2022 08:56:06 - INFO - __main__ - Global step 700 Train loss 0.385300 ACC 0.75 on epoch=233
05/21/2022 08:56:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.456677 on epoch=236
05/21/2022 08:56:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.410115 on epoch=239
05/21/2022 08:56:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.489161 on epoch=243
05/21/2022 08:56:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.414659 on epoch=246
05/21/2022 08:56:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.147196 on epoch=249
05/21/2022 08:56:21 - INFO - __main__ - Global step 750 Train loss 0.383561 ACC 0.6875 on epoch=249
05/21/2022 08:56:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.233244 on epoch=253
05/21/2022 08:56:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.125958 on epoch=256
05/21/2022 08:56:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.142290 on epoch=259
05/21/2022 08:56:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.157553 on epoch=263
05/21/2022 08:56:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.220861 on epoch=266
05/21/2022 08:56:36 - INFO - __main__ - Global step 800 Train loss 0.175981 ACC 0.625 on epoch=266
05/21/2022 08:56:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.329255 on epoch=269
05/21/2022 08:56:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.313462 on epoch=273
05/21/2022 08:56:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.186821 on epoch=276
05/21/2022 08:56:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.194221 on epoch=279
05/21/2022 08:56:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.163464 on epoch=283
05/21/2022 08:56:49 - INFO - __main__ - Global step 850 Train loss 0.237445 ACC 0.78125 on epoch=283
05/21/2022 08:56:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.134229 on epoch=286
05/21/2022 08:56:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.225519 on epoch=289
05/21/2022 08:56:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.166804 on epoch=293
05/21/2022 08:57:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.162989 on epoch=296
05/21/2022 08:57:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.146883 on epoch=299
05/21/2022 08:57:03 - INFO - __main__ - Global step 900 Train loss 0.167285 ACC 0.6875 on epoch=299
05/21/2022 08:57:03 - INFO - __main__ - save last model!
05/21/2022 08:57:04 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:57:04 - INFO - __main__ - Printing 3 examples
05/21/2022 08:57:04 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/21/2022 08:57:04 - INFO - __main__ - ['contradiction']
05/21/2022 08:57:04 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/21/2022 08:57:04 - INFO - __main__ - ['contradiction']
05/21/2022 08:57:04 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/21/2022 08:57:04 - INFO - __main__ - ['contradiction']
05/21/2022 08:57:04 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:57:04 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:57:04 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:57:04 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:57:04 - INFO - __main__ - Printing 3 examples
05/21/2022 08:57:04 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
05/21/2022 08:57:04 - INFO - __main__ - ['contradiction']
05/21/2022 08:57:04 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
05/21/2022 08:57:04 - INFO - __main__ - ['contradiction']
05/21/2022 08:57:04 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
05/21/2022 08:57:04 - INFO - __main__ - ['contradiction']
05/21/2022 08:57:04 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:57:04 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:57:04 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:57:06 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 08:57:06 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 08:57:07 - INFO - __main__ - Printing 3 examples
05/21/2022 08:57:07 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 08:57:07 - INFO - __main__ - ['contradiction']
05/21/2022 08:57:07 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 08:57:07 - INFO - __main__ - ['neutral']
05/21/2022 08:57:07 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 08:57:07 - INFO - __main__ - ['entailment']
05/21/2022 08:57:07 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:57:07 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:57:07 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 08:57:08 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_42_0.0005_8_predictions.txt
05/21/2022 08:57:08 - INFO - __main__ - ACC on test data: 0.7679
05/21/2022 08:57:08 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.0005, bsz=8, dev_performance=0.84375, test_performance=0.7678571428571429
05/21/2022 08:57:08 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.0003, bsz=8 ...
05/21/2022 08:57:09 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 08:57:09 - INFO - __main__ - Printing 3 examples
05/21/2022 08:57:09 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/21/2022 08:57:09 - INFO - __main__ - ['contradiction']
05/21/2022 08:57:09 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/21/2022 08:57:09 - INFO - __main__ - ['contradiction']
05/21/2022 08:57:09 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/21/2022 08:57:09 - INFO - __main__ - ['contradiction']
05/21/2022 08:57:09 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:57:09 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:57:09 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 08:57:09 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 08:57:09 - INFO - __main__ - Printing 3 examples
05/21/2022 08:57:09 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
05/21/2022 08:57:09 - INFO - __main__ - ['contradiction']
05/21/2022 08:57:09 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
05/21/2022 08:57:09 - INFO - __main__ - ['contradiction']
05/21/2022 08:57:09 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
05/21/2022 08:57:09 - INFO - __main__ - ['contradiction']
05/21/2022 08:57:09 - INFO - __main__ - Tokenizing Input ...
05/21/2022 08:57:09 - INFO - __main__ - Tokenizing Output ...
05/21/2022 08:57:09 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 08:57:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:57:09 - INFO - __main__ - Starting training!
05/21/2022 08:57:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 08:57:13 - INFO - __main__ - Starting training!
05/21/2022 08:57:15 - INFO - __main__ - Step 10 Global step 10 Train loss 17.773434 on epoch=3
05/21/2022 08:57:17 - INFO - __main__ - Step 20 Global step 20 Train loss 15.125708 on epoch=6
05/21/2022 08:57:20 - INFO - __main__ - Step 30 Global step 30 Train loss 11.356887 on epoch=9
05/21/2022 08:57:23 - INFO - __main__ - Step 40 Global step 40 Train loss 10.837031 on epoch=13
05/21/2022 08:57:26 - INFO - __main__ - Step 50 Global step 50 Train loss 9.178959 on epoch=16
05/21/2022 08:57:28 - INFO - __main__ - Global step 50 Train loss 12.854404 ACC 0.0 on epoch=16
05/21/2022 08:57:32 - INFO - __main__ - Step 60 Global step 60 Train loss 7.702201 on epoch=19
05/21/2022 08:57:35 - INFO - __main__ - Step 70 Global step 70 Train loss 6.758745 on epoch=23
05/21/2022 08:57:38 - INFO - __main__ - Step 80 Global step 80 Train loss 6.598228 on epoch=26
05/21/2022 08:57:41 - INFO - __main__ - Step 90 Global step 90 Train loss 4.956061 on epoch=29
05/21/2022 08:57:44 - INFO - __main__ - Step 100 Global step 100 Train loss 5.351860 on epoch=33
05/21/2022 08:57:45 - INFO - __main__ - Global step 100 Train loss 6.273419 ACC 0.0 on epoch=33
05/21/2022 08:57:47 - INFO - __main__ - Step 110 Global step 110 Train loss 5.025592 on epoch=36
05/21/2022 08:57:50 - INFO - __main__ - Step 120 Global step 120 Train loss 3.349128 on epoch=39
05/21/2022 08:57:53 - INFO - __main__ - Step 130 Global step 130 Train loss 2.372939 on epoch=43
05/21/2022 08:57:56 - INFO - __main__ - Step 140 Global step 140 Train loss 2.445758 on epoch=46
05/21/2022 08:57:58 - INFO - __main__ - Step 150 Global step 150 Train loss 1.941038 on epoch=49
05/21/2022 08:57:59 - INFO - __main__ - Global step 150 Train loss 3.026891 ACC 0.5 on epoch=49
05/21/2022 08:58:02 - INFO - __main__ - Step 160 Global step 160 Train loss 2.716059 on epoch=53
05/21/2022 08:58:04 - INFO - __main__ - Step 170 Global step 170 Train loss 2.508104 on epoch=56
05/21/2022 08:58:07 - INFO - __main__ - Step 180 Global step 180 Train loss 2.546188 on epoch=59
05/21/2022 08:58:11 - INFO - __main__ - Step 190 Global step 190 Train loss 2.191183 on epoch=63
05/21/2022 08:58:14 - INFO - __main__ - Step 200 Global step 200 Train loss 2.268154 on epoch=66
05/21/2022 08:58:14 - INFO - __main__ - Global step 200 Train loss 2.445938 ACC 0.5 on epoch=66
05/21/2022 08:58:17 - INFO - __main__ - Step 210 Global step 210 Train loss 2.370132 on epoch=69
05/21/2022 08:58:19 - INFO - __main__ - Step 220 Global step 220 Train loss 2.010246 on epoch=73
05/21/2022 08:58:22 - INFO - __main__ - Step 230 Global step 230 Train loss 1.980780 on epoch=76
05/21/2022 08:58:25 - INFO - __main__ - Step 240 Global step 240 Train loss 2.017209 on epoch=79
05/21/2022 08:58:28 - INFO - __main__ - Step 250 Global step 250 Train loss 1.690322 on epoch=83
05/21/2022 08:58:28 - INFO - __main__ - Global step 250 Train loss 2.013738 ACC 0.0 on epoch=83
05/21/2022 08:58:30 - INFO - __main__ - Step 260 Global step 260 Train loss 1.971666 on epoch=86
05/21/2022 08:58:34 - INFO - __main__ - Step 270 Global step 270 Train loss 1.355290 on epoch=89
05/21/2022 08:58:36 - INFO - __main__ - Step 280 Global step 280 Train loss 1.410526 on epoch=93
05/21/2022 08:58:39 - INFO - __main__ - Step 290 Global step 290 Train loss 1.106569 on epoch=96
05/21/2022 08:58:42 - INFO - __main__ - Step 300 Global step 300 Train loss 1.080023 on epoch=99
05/21/2022 08:58:42 - INFO - __main__ - Global step 300 Train loss 1.384815 ACC 0.5 on epoch=99
05/21/2022 08:58:45 - INFO - __main__ - Step 310 Global step 310 Train loss 1.278691 on epoch=103
05/21/2022 08:58:48 - INFO - __main__ - Step 320 Global step 320 Train loss 1.143389 on epoch=106
05/21/2022 08:58:51 - INFO - __main__ - Step 330 Global step 330 Train loss 1.146327 on epoch=109
05/21/2022 08:58:53 - INFO - __main__ - Step 340 Global step 340 Train loss 1.186579 on epoch=113
05/21/2022 08:58:56 - INFO - __main__ - Step 350 Global step 350 Train loss 1.110185 on epoch=116
05/21/2022 08:58:56 - INFO - __main__ - Global step 350 Train loss 1.173034 ACC 0.46875 on epoch=116
05/21/2022 08:58:59 - INFO - __main__ - Step 360 Global step 360 Train loss 1.101812 on epoch=119
05/21/2022 08:59:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.975618 on epoch=123
05/21/2022 08:59:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.875269 on epoch=126
05/21/2022 08:59:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.728876 on epoch=129
05/21/2022 08:59:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.959961 on epoch=133
05/21/2022 08:59:12 - INFO - __main__ - Global step 400 Train loss 0.928307 ACC 0.28125 on epoch=133
05/21/2022 08:59:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.788723 on epoch=136
05/21/2022 08:59:17 - INFO - __main__ - Step 420 Global step 420 Train loss 1.080578 on epoch=139
05/21/2022 08:59:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.837776 on epoch=143
05/21/2022 08:59:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.874462 on epoch=146
05/21/2022 08:59:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.902393 on epoch=149
05/21/2022 08:59:26 - INFO - __main__ - Global step 450 Train loss 0.896787 ACC 0.3125 on epoch=149
05/21/2022 08:59:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.900317 on epoch=153
05/21/2022 08:59:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.810983 on epoch=156
05/21/2022 08:59:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.839131 on epoch=159
05/21/2022 08:59:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.924511 on epoch=163
05/21/2022 08:59:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.834821 on epoch=166
05/21/2022 08:59:41 - INFO - __main__ - Global step 500 Train loss 0.861953 ACC 0.40625 on epoch=166
05/21/2022 08:59:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.823371 on epoch=169
05/21/2022 08:59:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.725379 on epoch=173
05/21/2022 08:59:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.857483 on epoch=176
05/21/2022 08:59:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.726878 on epoch=179
05/21/2022 08:59:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.785744 on epoch=183
05/21/2022 08:59:55 - INFO - __main__ - Global step 550 Train loss 0.783771 ACC 0.4375 on epoch=183
05/21/2022 08:59:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.800441 on epoch=186
05/21/2022 09:00:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.819250 on epoch=189
05/21/2022 09:00:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.527374 on epoch=193
05/21/2022 09:00:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.758991 on epoch=196
05/21/2022 09:00:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.584957 on epoch=199
05/21/2022 09:00:09 - INFO - __main__ - Global step 600 Train loss 0.698202 ACC 0.5 on epoch=199
05/21/2022 09:00:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.714062 on epoch=203
05/21/2022 09:00:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.631793 on epoch=206
05/21/2022 09:00:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.817930 on epoch=209
05/21/2022 09:00:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.549007 on epoch=213
05/21/2022 09:00:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.573437 on epoch=216
05/21/2022 09:00:23 - INFO - __main__ - Global step 650 Train loss 0.657246 ACC 0.125 on epoch=216
05/21/2022 09:00:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.586949 on epoch=219
05/21/2022 09:00:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.663421 on epoch=223
05/21/2022 09:00:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.587173 on epoch=226
05/21/2022 09:00:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.538288 on epoch=229
05/21/2022 09:00:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.522650 on epoch=233
05/21/2022 09:00:38 - INFO - __main__ - Global step 700 Train loss 0.579696 ACC 0.1875 on epoch=233
05/21/2022 09:00:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.611272 on epoch=236
05/21/2022 09:00:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.532603 on epoch=239
05/21/2022 09:00:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.491656 on epoch=243
05/21/2022 09:00:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.487894 on epoch=246
05/21/2022 09:00:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.523151 on epoch=249
05/21/2022 09:00:53 - INFO - __main__ - Global step 750 Train loss 0.529315 ACC 0.53125 on epoch=249
05/21/2022 09:00:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.559882 on epoch=253
05/21/2022 09:00:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.550031 on epoch=256
05/21/2022 09:01:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.517553 on epoch=259
05/21/2022 09:01:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.613236 on epoch=263
05/21/2022 09:01:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.464225 on epoch=266
05/21/2022 09:01:08 - INFO - __main__ - Global step 800 Train loss 0.540985 ACC 0.46875 on epoch=266
05/21/2022 09:01:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.437743 on epoch=269
05/21/2022 09:01:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.448608 on epoch=273
05/21/2022 09:01:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.438828 on epoch=276
05/21/2022 09:01:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.434482 on epoch=279
05/21/2022 09:01:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.475346 on epoch=283
05/21/2022 09:01:22 - INFO - __main__ - Global step 850 Train loss 0.447001 ACC 0.40625 on epoch=283
05/21/2022 09:01:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.433249 on epoch=286
05/21/2022 09:01:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.460739 on epoch=289
05/21/2022 09:01:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.472686 on epoch=293
05/21/2022 09:01:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.397585 on epoch=296
05/21/2022 09:01:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.389357 on epoch=299
05/21/2022 09:01:36 - INFO - __main__ - Global step 900 Train loss 0.430723 ACC 0.375 on epoch=299
05/21/2022 09:01:36 - INFO - __main__ - save last model!
05/21/2022 09:01:37 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 09:01:37 - INFO - __main__ - Printing 3 examples
05/21/2022 09:01:37 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/21/2022 09:01:37 - INFO - __main__ - ['contradiction']
05/21/2022 09:01:37 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/21/2022 09:01:37 - INFO - __main__ - ['contradiction']
05/21/2022 09:01:37 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/21/2022 09:01:37 - INFO - __main__ - ['contradiction']
05/21/2022 09:01:37 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:01:37 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:01:37 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 09:01:37 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 09:01:37 - INFO - __main__ - Printing 3 examples
05/21/2022 09:01:37 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
05/21/2022 09:01:37 - INFO - __main__ - ['contradiction']
05/21/2022 09:01:37 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
05/21/2022 09:01:37 - INFO - __main__ - ['contradiction']
05/21/2022 09:01:37 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
05/21/2022 09:01:37 - INFO - __main__ - ['contradiction']
05/21/2022 09:01:37 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:01:37 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:01:37 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 09:01:39 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 09:01:39 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 09:01:39 - INFO - __main__ - Printing 3 examples
05/21/2022 09:01:39 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 09:01:39 - INFO - __main__ - ['contradiction']
05/21/2022 09:01:39 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 09:01:39 - INFO - __main__ - ['neutral']
05/21/2022 09:01:39 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 09:01:39 - INFO - __main__ - ['entailment']
05/21/2022 09:01:39 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:01:39 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:01:39 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 09:01:40 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_42_0.0003_8_predictions.txt
05/21/2022 09:01:40 - INFO - __main__ - ACC on test data: 0.3750
05/21/2022 09:01:40 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.0003, bsz=8, dev_performance=0.53125, test_performance=0.375
05/21/2022 09:01:40 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.0002, bsz=8 ...
05/21/2022 09:01:41 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 09:01:41 - INFO - __main__ - Printing 3 examples
05/21/2022 09:01:41 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/21/2022 09:01:41 - INFO - __main__ - ['contradiction']
05/21/2022 09:01:41 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/21/2022 09:01:41 - INFO - __main__ - ['contradiction']
05/21/2022 09:01:41 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/21/2022 09:01:41 - INFO - __main__ - ['contradiction']
05/21/2022 09:01:41 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:01:41 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:01:41 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 09:01:41 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 09:01:41 - INFO - __main__ - Printing 3 examples
05/21/2022 09:01:41 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
05/21/2022 09:01:41 - INFO - __main__ - ['contradiction']
05/21/2022 09:01:41 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
05/21/2022 09:01:41 - INFO - __main__ - ['contradiction']
05/21/2022 09:01:41 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
05/21/2022 09:01:41 - INFO - __main__ - ['contradiction']
05/21/2022 09:01:41 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:01:41 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:01:41 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 09:01:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 09:01:43 - INFO - __main__ - Starting training!
05/21/2022 09:01:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 09:01:46 - INFO - __main__ - Starting training!
05/21/2022 09:01:50 - INFO - __main__ - Step 10 Global step 10 Train loss 17.822971 on epoch=3
05/21/2022 09:01:52 - INFO - __main__ - Step 20 Global step 20 Train loss 15.827909 on epoch=6
05/21/2022 09:01:55 - INFO - __main__ - Step 30 Global step 30 Train loss 12.698389 on epoch=9
05/21/2022 09:01:58 - INFO - __main__ - Step 40 Global step 40 Train loss 9.340483 on epoch=13
05/21/2022 09:02:00 - INFO - __main__ - Step 50 Global step 50 Train loss 7.629359 on epoch=16
05/21/2022 09:02:02 - INFO - __main__ - Global step 50 Train loss 12.663822 ACC 0.0 on epoch=16
05/21/2022 09:02:04 - INFO - __main__ - Step 60 Global step 60 Train loss 7.961833 on epoch=19
05/21/2022 09:02:07 - INFO - __main__ - Step 70 Global step 70 Train loss 6.015177 on epoch=23
05/21/2022 09:02:10 - INFO - __main__ - Step 80 Global step 80 Train loss 5.149039 on epoch=26
05/21/2022 09:02:12 - INFO - __main__ - Step 90 Global step 90 Train loss 4.820226 on epoch=29
05/21/2022 09:02:15 - INFO - __main__ - Step 100 Global step 100 Train loss 3.933856 on epoch=33
05/21/2022 09:02:16 - INFO - __main__ - Global step 100 Train loss 5.576026 ACC 0.0 on epoch=33
05/21/2022 09:02:18 - INFO - __main__ - Step 110 Global step 110 Train loss 4.079686 on epoch=36
05/21/2022 09:02:21 - INFO - __main__ - Step 120 Global step 120 Train loss 3.873788 on epoch=39
05/21/2022 09:02:24 - INFO - __main__ - Step 130 Global step 130 Train loss 2.670695 on epoch=43
05/21/2022 09:02:26 - INFO - __main__ - Step 140 Global step 140 Train loss 2.192530 on epoch=46
05/21/2022 09:02:29 - INFO - __main__ - Step 150 Global step 150 Train loss 2.839796 on epoch=49
05/21/2022 09:02:29 - INFO - __main__ - Global step 150 Train loss 3.131299 ACC 0.375 on epoch=49
05/21/2022 09:02:32 - INFO - __main__ - Step 160 Global step 160 Train loss 2.189899 on epoch=53
05/21/2022 09:02:35 - INFO - __main__ - Step 170 Global step 170 Train loss 2.159996 on epoch=56
05/21/2022 09:02:38 - INFO - __main__ - Step 180 Global step 180 Train loss 2.041858 on epoch=59
05/21/2022 09:02:41 - INFO - __main__ - Step 190 Global step 190 Train loss 2.098958 on epoch=63
05/21/2022 09:02:44 - INFO - __main__ - Step 200 Global step 200 Train loss 1.698630 on epoch=66
05/21/2022 09:02:45 - INFO - __main__ - Global step 200 Train loss 2.037868 ACC 0.34375 on epoch=66
05/21/2022 09:02:47 - INFO - __main__ - Step 210 Global step 210 Train loss 1.567174 on epoch=69
05/21/2022 09:02:50 - INFO - __main__ - Step 220 Global step 220 Train loss 1.410494 on epoch=73
05/21/2022 09:02:53 - INFO - __main__ - Step 230 Global step 230 Train loss 1.672249 on epoch=76
05/21/2022 09:02:56 - INFO - __main__ - Step 240 Global step 240 Train loss 1.994810 on epoch=79
05/21/2022 09:02:59 - INFO - __main__ - Step 250 Global step 250 Train loss 1.065915 on epoch=83
05/21/2022 09:02:59 - INFO - __main__ - Global step 250 Train loss 1.542128 ACC 0.5 on epoch=83
05/21/2022 09:03:02 - INFO - __main__ - Step 260 Global step 260 Train loss 1.007325 on epoch=86
05/21/2022 09:03:04 - INFO - __main__ - Step 270 Global step 270 Train loss 1.063102 on epoch=89
05/21/2022 09:03:07 - INFO - __main__ - Step 280 Global step 280 Train loss 1.121963 on epoch=93
05/21/2022 09:03:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.831799 on epoch=96
05/21/2022 09:03:12 - INFO - __main__ - Step 300 Global step 300 Train loss 1.190945 on epoch=99
05/21/2022 09:03:13 - INFO - __main__ - Global step 300 Train loss 1.043027 ACC 0.6875 on epoch=99
05/21/2022 09:03:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.920523 on epoch=103
05/21/2022 09:03:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.869141 on epoch=106
05/21/2022 09:03:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.783171 on epoch=109
05/21/2022 09:03:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.941026 on epoch=113
05/21/2022 09:03:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.702215 on epoch=116
05/21/2022 09:03:28 - INFO - __main__ - Global step 350 Train loss 0.843215 ACC 0.6875 on epoch=116
05/21/2022 09:03:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.746539 on epoch=119
05/21/2022 09:03:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.591950 on epoch=123
05/21/2022 09:03:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.715868 on epoch=126
05/21/2022 09:03:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.531398 on epoch=129
05/21/2022 09:03:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.606562 on epoch=133
05/21/2022 09:03:43 - INFO - __main__ - Global step 400 Train loss 0.638463 ACC 0.65625 on epoch=133
05/21/2022 09:03:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.337546 on epoch=136
05/21/2022 09:03:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.428367 on epoch=139
05/21/2022 09:03:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.337461 on epoch=143
05/21/2022 09:03:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.509133 on epoch=146
05/21/2022 09:03:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.411782 on epoch=149
05/21/2022 09:03:58 - INFO - __main__ - Global step 450 Train loss 0.404858 ACC 0.6875 on epoch=149
05/21/2022 09:04:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.287572 on epoch=153
05/21/2022 09:04:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.422125 on epoch=156
05/21/2022 09:04:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.348877 on epoch=159
05/21/2022 09:04:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.335240 on epoch=163
05/21/2022 09:04:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.276451 on epoch=166
05/21/2022 09:04:12 - INFO - __main__ - Global step 500 Train loss 0.334053 ACC 0.6875 on epoch=166
05/21/2022 09:04:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.376734 on epoch=169
05/21/2022 09:04:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.258781 on epoch=173
05/21/2022 09:04:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.230278 on epoch=176
05/21/2022 09:04:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.202366 on epoch=179
05/21/2022 09:04:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.439448 on epoch=183
05/21/2022 09:04:27 - INFO - __main__ - Global step 550 Train loss 0.301521 ACC 0.6875 on epoch=183
05/21/2022 09:04:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.266432 on epoch=186
05/21/2022 09:04:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.271576 on epoch=189
05/21/2022 09:04:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.128100 on epoch=193
05/21/2022 09:04:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.184028 on epoch=196
05/21/2022 09:04:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.096998 on epoch=199
05/21/2022 09:04:41 - INFO - __main__ - Global step 600 Train loss 0.189427 ACC 0.71875 on epoch=199
05/21/2022 09:04:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.375620 on epoch=203
05/21/2022 09:04:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.191040 on epoch=206
05/21/2022 09:04:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.316554 on epoch=209
05/21/2022 09:04:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.289593 on epoch=213
05/21/2022 09:04:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.341142 on epoch=216
05/21/2022 09:04:56 - INFO - __main__ - Global step 650 Train loss 0.302790 ACC 0.625 on epoch=216
05/21/2022 09:04:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.279704 on epoch=219
05/21/2022 09:05:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.265901 on epoch=223
05/21/2022 09:05:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.484018 on epoch=226
05/21/2022 09:05:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.418516 on epoch=229
05/21/2022 09:05:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.439068 on epoch=233
05/21/2022 09:05:11 - INFO - __main__ - Global step 700 Train loss 0.377441 ACC 0.71875 on epoch=233
05/21/2022 09:05:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.375345 on epoch=236
05/21/2022 09:05:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.525486 on epoch=239
05/21/2022 09:05:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.466167 on epoch=243
05/21/2022 09:05:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.455201 on epoch=246
05/21/2022 09:05:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.407239 on epoch=249
05/21/2022 09:05:26 - INFO - __main__ - Global step 750 Train loss 0.445888 ACC 0.5 on epoch=249
05/21/2022 09:05:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.414408 on epoch=253
05/21/2022 09:05:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.348847 on epoch=256
05/21/2022 09:05:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.367788 on epoch=259
05/21/2022 09:05:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.610768 on epoch=263
05/21/2022 09:05:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.635928 on epoch=266
05/21/2022 09:05:40 - INFO - __main__ - Global step 800 Train loss 0.475548 ACC 0.40625 on epoch=266
05/21/2022 09:05:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.563589 on epoch=269
05/21/2022 09:05:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.487217 on epoch=273
05/21/2022 09:05:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.442549 on epoch=276
05/21/2022 09:05:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.573641 on epoch=279
05/21/2022 09:05:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.365316 on epoch=283
05/21/2022 09:05:54 - INFO - __main__ - Global step 850 Train loss 0.486462 ACC 0.34375 on epoch=283
05/21/2022 09:05:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.474732 on epoch=286
05/21/2022 09:06:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.649796 on epoch=289
05/21/2022 09:06:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.365476 on epoch=293
05/21/2022 09:06:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.386713 on epoch=296
05/21/2022 09:06:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.365559 on epoch=299
05/21/2022 09:06:08 - INFO - __main__ - Global step 900 Train loss 0.448455 ACC 0.5625 on epoch=299
05/21/2022 09:06:08 - INFO - __main__ - save last model!
05/21/2022 09:06:09 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 09:06:09 - INFO - __main__ - Printing 3 examples
05/21/2022 09:06:09 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/21/2022 09:06:09 - INFO - __main__ - ['contradiction']
05/21/2022 09:06:09 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/21/2022 09:06:09 - INFO - __main__ - ['contradiction']
05/21/2022 09:06:09 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/21/2022 09:06:09 - INFO - __main__ - ['contradiction']
05/21/2022 09:06:09 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:06:09 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:06:09 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 09:06:09 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 09:06:09 - INFO - __main__ - Printing 3 examples
05/21/2022 09:06:09 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
05/21/2022 09:06:09 - INFO - __main__ - ['contradiction']
05/21/2022 09:06:09 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
05/21/2022 09:06:09 - INFO - __main__ - ['contradiction']
05/21/2022 09:06:09 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
05/21/2022 09:06:09 - INFO - __main__ - ['contradiction']
05/21/2022 09:06:09 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:06:09 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:06:09 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 09:06:11 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 09:06:11 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 09:06:11 - INFO - __main__ - Printing 3 examples
05/21/2022 09:06:11 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 09:06:11 - INFO - __main__ - ['contradiction']
05/21/2022 09:06:11 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 09:06:11 - INFO - __main__ - ['neutral']
05/21/2022 09:06:11 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 09:06:11 - INFO - __main__ - ['entailment']
05/21/2022 09:06:11 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:06:11 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:06:11 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 09:06:12 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_42_0.0002_8_predictions.txt
05/21/2022 09:06:12 - INFO - __main__ - ACC on test data: 0.6964
05/21/2022 09:06:12 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.0002, bsz=8, dev_performance=0.71875, test_performance=0.6964285714285714
05/21/2022 09:06:12 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.0001, bsz=8 ...
05/21/2022 09:06:13 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 09:06:13 - INFO - __main__ - Printing 3 examples
05/21/2022 09:06:13 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
05/21/2022 09:06:13 - INFO - __main__ - ['contradiction']
05/21/2022 09:06:13 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
05/21/2022 09:06:13 - INFO - __main__ - ['contradiction']
05/21/2022 09:06:13 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
05/21/2022 09:06:13 - INFO - __main__ - ['contradiction']
05/21/2022 09:06:13 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:06:13 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:06:13 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 09:06:13 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 09:06:13 - INFO - __main__ - Printing 3 examples
05/21/2022 09:06:13 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
05/21/2022 09:06:13 - INFO - __main__ - ['contradiction']
05/21/2022 09:06:13 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
05/21/2022 09:06:13 - INFO - __main__ - ['contradiction']
05/21/2022 09:06:13 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
05/21/2022 09:06:13 - INFO - __main__ - ['contradiction']
05/21/2022 09:06:13 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:06:13 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:06:13 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 09:06:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 09:06:14 - INFO - __main__ - Starting training!
05/21/2022 09:06:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 09:06:18 - INFO - __main__ - Starting training!
05/21/2022 09:06:21 - INFO - __main__ - Step 10 Global step 10 Train loss 18.828011 on epoch=3
05/21/2022 09:06:23 - INFO - __main__ - Step 20 Global step 20 Train loss 17.718145 on epoch=6
05/21/2022 09:06:26 - INFO - __main__ - Step 30 Global step 30 Train loss 16.765324 on epoch=9
05/21/2022 09:06:29 - INFO - __main__ - Step 40 Global step 40 Train loss 12.981749 on epoch=13
05/21/2022 09:06:32 - INFO - __main__ - Step 50 Global step 50 Train loss 12.152166 on epoch=16
05/21/2022 09:06:39 - INFO - __main__ - Global step 50 Train loss 15.689079 ACC 0.0 on epoch=16
05/21/2022 09:06:42 - INFO - __main__ - Step 60 Global step 60 Train loss 10.552298 on epoch=19
05/21/2022 09:06:44 - INFO - __main__ - Step 70 Global step 70 Train loss 9.748098 on epoch=23
05/21/2022 09:06:47 - INFO - __main__ - Step 80 Global step 80 Train loss 8.772783 on epoch=26
05/21/2022 09:06:50 - INFO - __main__ - Step 90 Global step 90 Train loss 9.432001 on epoch=29
05/21/2022 09:06:52 - INFO - __main__ - Step 100 Global step 100 Train loss 7.822571 on epoch=33
05/21/2022 09:06:55 - INFO - __main__ - Global step 100 Train loss 9.265551 ACC 0.0 on epoch=33
05/21/2022 09:06:58 - INFO - __main__ - Step 110 Global step 110 Train loss 7.350550 on epoch=36
05/21/2022 09:07:01 - INFO - __main__ - Step 120 Global step 120 Train loss 7.013356 on epoch=39
05/21/2022 09:07:04 - INFO - __main__ - Step 130 Global step 130 Train loss 6.616500 on epoch=43
05/21/2022 09:07:07 - INFO - __main__ - Step 140 Global step 140 Train loss 7.375540 on epoch=46
05/21/2022 09:07:10 - INFO - __main__ - Step 150 Global step 150 Train loss 5.711111 on epoch=49
05/21/2022 09:07:11 - INFO - __main__ - Global step 150 Train loss 6.813411 ACC 0.03125 on epoch=49
05/21/2022 09:07:15 - INFO - __main__ - Step 160 Global step 160 Train loss 6.706889 on epoch=53
05/21/2022 09:07:17 - INFO - __main__ - Step 170 Global step 170 Train loss 6.253606 on epoch=56
05/21/2022 09:07:20 - INFO - __main__ - Step 180 Global step 180 Train loss 5.644779 on epoch=59
05/21/2022 09:07:23 - INFO - __main__ - Step 190 Global step 190 Train loss 6.101637 on epoch=63
05/21/2022 09:07:26 - INFO - __main__ - Step 200 Global step 200 Train loss 4.670512 on epoch=66
05/21/2022 09:07:27 - INFO - __main__ - Global step 200 Train loss 5.875484 ACC 0.0 on epoch=66
05/21/2022 09:07:29 - INFO - __main__ - Step 210 Global step 210 Train loss 4.500087 on epoch=69
05/21/2022 09:07:32 - INFO - __main__ - Step 220 Global step 220 Train loss 4.471770 on epoch=73
05/21/2022 09:07:35 - INFO - __main__ - Step 230 Global step 230 Train loss 3.598917 on epoch=76
05/21/2022 09:07:38 - INFO - __main__ - Step 240 Global step 240 Train loss 5.126571 on epoch=79
05/21/2022 09:07:41 - INFO - __main__ - Step 250 Global step 250 Train loss 4.364676 on epoch=83
05/21/2022 09:07:41 - INFO - __main__ - Global step 250 Train loss 4.412404 ACC 0.0 on epoch=83
05/21/2022 09:07:44 - INFO - __main__ - Step 260 Global step 260 Train loss 4.027638 on epoch=86
05/21/2022 09:07:47 - INFO - __main__ - Step 270 Global step 270 Train loss 4.025723 on epoch=89
05/21/2022 09:07:49 - INFO - __main__ - Step 280 Global step 280 Train loss 3.370914 on epoch=93
05/21/2022 09:07:52 - INFO - __main__ - Step 290 Global step 290 Train loss 3.602575 on epoch=96
05/21/2022 09:07:55 - INFO - __main__ - Step 300 Global step 300 Train loss 2.482873 on epoch=99
05/21/2022 09:07:55 - INFO - __main__ - Global step 300 Train loss 3.501945 ACC 0.03125 on epoch=99
05/21/2022 09:07:58 - INFO - __main__ - Step 310 Global step 310 Train loss 2.895839 on epoch=103
05/21/2022 09:08:01 - INFO - __main__ - Step 320 Global step 320 Train loss 1.986842 on epoch=106
05/21/2022 09:08:03 - INFO - __main__ - Step 330 Global step 330 Train loss 2.783939 on epoch=109
05/21/2022 09:08:06 - INFO - __main__ - Step 340 Global step 340 Train loss 2.750453 on epoch=113
05/21/2022 09:08:09 - INFO - __main__ - Step 350 Global step 350 Train loss 3.073442 on epoch=116
05/21/2022 09:08:09 - INFO - __main__ - Global step 350 Train loss 2.698103 ACC 0.4375 on epoch=116
05/21/2022 09:08:12 - INFO - __main__ - Step 360 Global step 360 Train loss 2.796826 on epoch=119
05/21/2022 09:08:15 - INFO - __main__ - Step 370 Global step 370 Train loss 2.332779 on epoch=123
05/21/2022 09:08:18 - INFO - __main__ - Step 380 Global step 380 Train loss 2.789720 on epoch=126
05/21/2022 09:08:21 - INFO - __main__ - Step 390 Global step 390 Train loss 2.118664 on epoch=129
05/21/2022 09:08:24 - INFO - __main__ - Step 400 Global step 400 Train loss 2.175478 on epoch=133
05/21/2022 09:08:24 - INFO - __main__ - Global step 400 Train loss 2.442693 ACC 0.46875 on epoch=133
05/21/2022 09:08:27 - INFO - __main__ - Step 410 Global step 410 Train loss 2.106241 on epoch=136
05/21/2022 09:08:30 - INFO - __main__ - Step 420 Global step 420 Train loss 2.285949 on epoch=139
05/21/2022 09:08:33 - INFO - __main__ - Step 430 Global step 430 Train loss 2.682654 on epoch=143
05/21/2022 09:08:35 - INFO - __main__ - Step 440 Global step 440 Train loss 2.454930 on epoch=146
05/21/2022 09:08:38 - INFO - __main__ - Step 450 Global step 450 Train loss 2.665346 on epoch=149
05/21/2022 09:08:39 - INFO - __main__ - Global step 450 Train loss 2.439024 ACC 0.46875 on epoch=149
05/21/2022 09:08:42 - INFO - __main__ - Step 460 Global step 460 Train loss 2.433584 on epoch=153
05/21/2022 09:08:44 - INFO - __main__ - Step 470 Global step 470 Train loss 1.946449 on epoch=156
05/21/2022 09:08:47 - INFO - __main__ - Step 480 Global step 480 Train loss 1.407893 on epoch=159
05/21/2022 09:08:50 - INFO - __main__ - Step 490 Global step 490 Train loss 2.280381 on epoch=163
05/21/2022 09:08:52 - INFO - __main__ - Step 500 Global step 500 Train loss 1.683374 on epoch=166
05/21/2022 09:08:52 - INFO - __main__ - Global step 500 Train loss 1.950336 ACC 0.46875 on epoch=166
05/21/2022 09:08:55 - INFO - __main__ - Step 510 Global step 510 Train loss 1.378986 on epoch=169
05/21/2022 09:08:58 - INFO - __main__ - Step 520 Global step 520 Train loss 1.963835 on epoch=173
05/21/2022 09:09:01 - INFO - __main__ - Step 530 Global step 530 Train loss 2.119918 on epoch=176
05/21/2022 09:09:03 - INFO - __main__ - Step 540 Global step 540 Train loss 1.876404 on epoch=179
05/21/2022 09:09:06 - INFO - __main__ - Step 550 Global step 550 Train loss 1.687752 on epoch=183
05/21/2022 09:09:06 - INFO - __main__ - Global step 550 Train loss 1.805379 ACC 0.40625 on epoch=183
05/21/2022 09:09:09 - INFO - __main__ - Step 560 Global step 560 Train loss 2.180130 on epoch=186
05/21/2022 09:09:12 - INFO - __main__ - Step 570 Global step 570 Train loss 2.224423 on epoch=189
05/21/2022 09:09:15 - INFO - __main__ - Step 580 Global step 580 Train loss 1.410357 on epoch=193
05/21/2022 09:09:18 - INFO - __main__ - Step 590 Global step 590 Train loss 1.609993 on epoch=196
05/21/2022 09:09:20 - INFO - __main__ - Step 600 Global step 600 Train loss 1.960648 on epoch=199
05/21/2022 09:09:20 - INFO - __main__ - Global step 600 Train loss 1.877110 ACC 0.4375 on epoch=199
05/21/2022 09:09:23 - INFO - __main__ - Step 610 Global step 610 Train loss 1.468540 on epoch=203
05/21/2022 09:09:26 - INFO - __main__ - Step 620 Global step 620 Train loss 1.821492 on epoch=206
05/21/2022 09:09:28 - INFO - __main__ - Step 630 Global step 630 Train loss 1.564231 on epoch=209
05/21/2022 09:09:31 - INFO - __main__ - Step 640 Global step 640 Train loss 1.884744 on epoch=213
05/21/2022 09:09:34 - INFO - __main__ - Step 650 Global step 650 Train loss 1.558496 on epoch=216
05/21/2022 09:09:34 - INFO - __main__ - Global step 650 Train loss 1.659501 ACC 0.4375 on epoch=216
05/21/2022 09:09:37 - INFO - __main__ - Step 660 Global step 660 Train loss 1.614911 on epoch=219
05/21/2022 09:09:40 - INFO - __main__ - Step 670 Global step 670 Train loss 1.558990 on epoch=223
05/21/2022 09:09:43 - INFO - __main__ - Step 680 Global step 680 Train loss 1.602239 on epoch=226
05/21/2022 09:09:46 - INFO - __main__ - Step 690 Global step 690 Train loss 1.496449 on epoch=229
05/21/2022 09:09:49 - INFO - __main__ - Step 700 Global step 700 Train loss 1.271463 on epoch=233
05/21/2022 09:09:50 - INFO - __main__ - Global step 700 Train loss 1.508810 ACC 0.4375 on epoch=233
05/21/2022 09:09:53 - INFO - __main__ - Step 710 Global step 710 Train loss 1.388196 on epoch=236
05/21/2022 09:09:55 - INFO - __main__ - Step 720 Global step 720 Train loss 1.493272 on epoch=239
05/21/2022 09:09:58 - INFO - __main__ - Step 730 Global step 730 Train loss 1.101797 on epoch=243
05/21/2022 09:10:00 - INFO - __main__ - Step 740 Global step 740 Train loss 1.504632 on epoch=246
05/21/2022 09:10:03 - INFO - __main__ - Step 750 Global step 750 Train loss 1.450984 on epoch=249
05/21/2022 09:10:03 - INFO - __main__ - Global step 750 Train loss 1.387776 ACC 0.46875 on epoch=249
05/21/2022 09:10:06 - INFO - __main__ - Step 760 Global step 760 Train loss 1.195171 on epoch=253
05/21/2022 09:10:09 - INFO - __main__ - Step 770 Global step 770 Train loss 1.106019 on epoch=256
05/21/2022 09:10:12 - INFO - __main__ - Step 780 Global step 780 Train loss 1.137439 on epoch=259
05/21/2022 09:10:15 - INFO - __main__ - Step 790 Global step 790 Train loss 1.414202 on epoch=263
05/21/2022 09:10:18 - INFO - __main__ - Step 800 Global step 800 Train loss 1.316236 on epoch=266
05/21/2022 09:10:18 - INFO - __main__ - Global step 800 Train loss 1.233813 ACC 0.46875 on epoch=266
05/21/2022 09:10:21 - INFO - __main__ - Step 810 Global step 810 Train loss 1.167492 on epoch=269
05/21/2022 09:10:24 - INFO - __main__ - Step 820 Global step 820 Train loss 1.195509 on epoch=273
05/21/2022 09:10:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.831792 on epoch=276
05/21/2022 09:10:30 - INFO - __main__ - Step 840 Global step 840 Train loss 1.042898 on epoch=279
05/21/2022 09:10:32 - INFO - __main__ - Step 850 Global step 850 Train loss 1.224836 on epoch=283
05/21/2022 09:10:33 - INFO - __main__ - Global step 850 Train loss 1.092505 ACC 0.5 on epoch=283
05/21/2022 09:10:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.970930 on epoch=286
05/21/2022 09:10:39 - INFO - __main__ - Step 870 Global step 870 Train loss 1.040211 on epoch=289
05/21/2022 09:10:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.745046 on epoch=293
05/21/2022 09:10:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.751484 on epoch=296
05/21/2022 09:10:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.956074 on epoch=299
05/21/2022 09:10:47 - INFO - __main__ - Global step 900 Train loss 0.892749 ACC 0.6875 on epoch=299
05/21/2022 09:10:48 - INFO - __main__ - save last model!
05/21/2022 09:10:49 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 09:10:49 - INFO - __main__ - Printing 3 examples
05/21/2022 09:10:49 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/21/2022 09:10:49 - INFO - __main__ - ['contradiction']
05/21/2022 09:10:49 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/21/2022 09:10:49 - INFO - __main__ - ['contradiction']
05/21/2022 09:10:49 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/21/2022 09:10:49 - INFO - __main__ - ['contradiction']
05/21/2022 09:10:49 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:10:49 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:10:49 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 09:10:49 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 09:10:49 - INFO - __main__ - Printing 3 examples
05/21/2022 09:10:49 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
05/21/2022 09:10:49 - INFO - __main__ - ['contradiction']
05/21/2022 09:10:49 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
05/21/2022 09:10:49 - INFO - __main__ - ['contradiction']
05/21/2022 09:10:49 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
05/21/2022 09:10:49 - INFO - __main__ - ['contradiction']
05/21/2022 09:10:49 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:10:49 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:10:49 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 09:10:51 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 09:10:52 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 09:10:52 - INFO - __main__ - Printing 3 examples
05/21/2022 09:10:52 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 09:10:52 - INFO - __main__ - ['contradiction']
05/21/2022 09:10:52 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 09:10:52 - INFO - __main__ - ['neutral']
05/21/2022 09:10:52 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 09:10:52 - INFO - __main__ - ['entailment']
05/21/2022 09:10:52 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:10:52 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:10:52 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 09:10:53 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_42_0.0001_8_predictions.txt
05/21/2022 09:10:53 - INFO - __main__ - ACC on test data: 0.6964
05/21/2022 09:10:53 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.0001, bsz=8, dev_performance=0.6875, test_performance=0.6964285714285714
05/21/2022 09:10:53 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.0005, bsz=8 ...
05/21/2022 09:10:54 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 09:10:54 - INFO - __main__ - Printing 3 examples
05/21/2022 09:10:54 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/21/2022 09:10:54 - INFO - __main__ - ['contradiction']
05/21/2022 09:10:54 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/21/2022 09:10:54 - INFO - __main__ - ['contradiction']
05/21/2022 09:10:54 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/21/2022 09:10:54 - INFO - __main__ - ['contradiction']
05/21/2022 09:10:54 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:10:54 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:10:54 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 09:10:54 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 09:10:54 - INFO - __main__ - Printing 3 examples
05/21/2022 09:10:54 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
05/21/2022 09:10:54 - INFO - __main__ - ['contradiction']
05/21/2022 09:10:54 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
05/21/2022 09:10:54 - INFO - __main__ - ['contradiction']
05/21/2022 09:10:54 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
05/21/2022 09:10:54 - INFO - __main__ - ['contradiction']
05/21/2022 09:10:54 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:10:54 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:10:54 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 09:10:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 09:10:54 - INFO - __main__ - Starting training!
05/21/2022 09:10:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 09:10:58 - INFO - __main__ - Starting training!
05/21/2022 09:11:01 - INFO - __main__ - Step 10 Global step 10 Train loss 17.483997 on epoch=3
05/21/2022 09:11:03 - INFO - __main__ - Step 20 Global step 20 Train loss 15.317737 on epoch=6
05/21/2022 09:11:06 - INFO - __main__ - Step 30 Global step 30 Train loss 10.703677 on epoch=9
05/21/2022 09:11:09 - INFO - __main__ - Step 40 Global step 40 Train loss 10.171167 on epoch=13
05/21/2022 09:11:12 - INFO - __main__ - Step 50 Global step 50 Train loss 7.369840 on epoch=16
05/21/2022 09:11:12 - INFO - __main__ - Global step 50 Train loss 12.209284 ACC 0.03125 on epoch=16
05/21/2022 09:11:15 - INFO - __main__ - Step 60 Global step 60 Train loss 4.918841 on epoch=19
05/21/2022 09:11:18 - INFO - __main__ - Step 70 Global step 70 Train loss 3.667533 on epoch=23
05/21/2022 09:11:21 - INFO - __main__ - Step 80 Global step 80 Train loss 3.481613 on epoch=26
05/21/2022 09:11:24 - INFO - __main__ - Step 90 Global step 90 Train loss 2.546658 on epoch=29
05/21/2022 09:11:27 - INFO - __main__ - Step 100 Global step 100 Train loss 2.410139 on epoch=33
05/21/2022 09:11:27 - INFO - __main__ - Global step 100 Train loss 3.404957 ACC 0.5 on epoch=33
05/21/2022 09:11:30 - INFO - __main__ - Step 110 Global step 110 Train loss 1.735660 on epoch=36
05/21/2022 09:11:33 - INFO - __main__ - Step 120 Global step 120 Train loss 2.389328 on epoch=39
05/21/2022 09:11:37 - INFO - __main__ - Step 130 Global step 130 Train loss 2.273929 on epoch=43
05/21/2022 09:11:40 - INFO - __main__ - Step 140 Global step 140 Train loss 1.469707 on epoch=46
05/21/2022 09:11:42 - INFO - __main__ - Step 150 Global step 150 Train loss 2.232392 on epoch=49
05/21/2022 09:11:43 - INFO - __main__ - Global step 150 Train loss 2.020203 ACC 0.46875 on epoch=49
05/21/2022 09:11:45 - INFO - __main__ - Step 160 Global step 160 Train loss 2.696791 on epoch=53
05/21/2022 09:11:48 - INFO - __main__ - Step 170 Global step 170 Train loss 1.687750 on epoch=56
05/21/2022 09:11:51 - INFO - __main__ - Step 180 Global step 180 Train loss 2.001987 on epoch=59
05/21/2022 09:11:54 - INFO - __main__ - Step 190 Global step 190 Train loss 1.366884 on epoch=63
05/21/2022 09:11:57 - INFO - __main__ - Step 200 Global step 200 Train loss 1.481909 on epoch=66
05/21/2022 09:11:57 - INFO - __main__ - Global step 200 Train loss 1.847064 ACC 0.0 on epoch=66
05/21/2022 09:12:00 - INFO - __main__ - Step 210 Global step 210 Train loss 1.595973 on epoch=69
05/21/2022 09:12:02 - INFO - __main__ - Step 220 Global step 220 Train loss 1.437305 on epoch=73
05/21/2022 09:12:05 - INFO - __main__ - Step 230 Global step 230 Train loss 1.076501 on epoch=76
05/21/2022 09:12:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.900849 on epoch=79
05/21/2022 09:12:11 - INFO - __main__ - Step 250 Global step 250 Train loss 1.042809 on epoch=83
05/21/2022 09:12:11 - INFO - __main__ - Global step 250 Train loss 1.210687 ACC 0.375 on epoch=83
05/21/2022 09:12:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.990670 on epoch=86
05/21/2022 09:12:17 - INFO - __main__ - Step 270 Global step 270 Train loss 1.234722 on epoch=89
05/21/2022 09:12:20 - INFO - __main__ - Step 280 Global step 280 Train loss 1.005261 on epoch=93
05/21/2022 09:12:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.960984 on epoch=96
05/21/2022 09:12:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.919641 on epoch=99
05/21/2022 09:12:26 - INFO - __main__ - Global step 300 Train loss 1.022256 ACC 0.03125 on epoch=99
05/21/2022 09:12:29 - INFO - __main__ - Step 310 Global step 310 Train loss 1.011630 on epoch=103
05/21/2022 09:12:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.791506 on epoch=106
05/21/2022 09:12:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.960349 on epoch=109
05/21/2022 09:12:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.873569 on epoch=113
05/21/2022 09:12:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.833031 on epoch=116
05/21/2022 09:12:42 - INFO - __main__ - Global step 350 Train loss 0.894017 ACC 0.4375 on epoch=116
05/21/2022 09:12:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.984563 on epoch=119
05/21/2022 09:12:48 - INFO - __main__ - Step 370 Global step 370 Train loss 1.080997 on epoch=123
05/21/2022 09:12:51 - INFO - __main__ - Step 380 Global step 380 Train loss 1.165389 on epoch=126
05/21/2022 09:12:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.799815 on epoch=129
05/21/2022 09:12:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.773706 on epoch=133
05/21/2022 09:12:57 - INFO - __main__ - Global step 400 Train loss 0.960894 ACC 0.28125 on epoch=133
05/21/2022 09:12:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.825979 on epoch=136
05/21/2022 09:13:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.797166 on epoch=139
05/21/2022 09:13:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.925201 on epoch=143
05/21/2022 09:13:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.784326 on epoch=146
05/21/2022 09:13:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.612841 on epoch=149
05/21/2022 09:13:12 - INFO - __main__ - Global step 450 Train loss 0.789103 ACC 0.34375 on epoch=149
05/21/2022 09:13:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.734985 on epoch=153
05/21/2022 09:13:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.564739 on epoch=156
05/21/2022 09:13:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.691833 on epoch=159
05/21/2022 09:13:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.651560 on epoch=163
05/21/2022 09:13:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.680957 on epoch=166
05/21/2022 09:13:26 - INFO - __main__ - Global step 500 Train loss 0.664815 ACC 0.15625 on epoch=166
05/21/2022 09:13:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.824051 on epoch=169
05/21/2022 09:13:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.621668 on epoch=173
05/21/2022 09:13:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.587621 on epoch=176
05/21/2022 09:13:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.678205 on epoch=179
05/21/2022 09:13:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.648067 on epoch=183
05/21/2022 09:13:41 - INFO - __main__ - Global step 550 Train loss 0.671922 ACC 0.59375 on epoch=183
05/21/2022 09:13:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.616368 on epoch=186
05/21/2022 09:13:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.696452 on epoch=189
05/21/2022 09:13:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.579695 on epoch=193
05/21/2022 09:13:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.554407 on epoch=196
05/21/2022 09:13:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.612765 on epoch=199
05/21/2022 09:13:55 - INFO - __main__ - Global step 600 Train loss 0.611937 ACC 0.1875 on epoch=199
05/21/2022 09:13:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.606433 on epoch=203
05/21/2022 09:14:01 - INFO - __main__ - Step 620 Global step 620 Train loss 0.508192 on epoch=206
05/21/2022 09:14:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.605494 on epoch=209
05/21/2022 09:14:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.559151 on epoch=213
05/21/2022 09:14:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.702910 on epoch=216
05/21/2022 09:14:09 - INFO - __main__ - Global step 650 Train loss 0.596436 ACC 0.4375 on epoch=216
05/21/2022 09:14:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.619745 on epoch=219
05/21/2022 09:14:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.488442 on epoch=223
05/21/2022 09:14:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.595245 on epoch=226
05/21/2022 09:14:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.527666 on epoch=229
05/21/2022 09:14:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.502083 on epoch=233
05/21/2022 09:14:24 - INFO - __main__ - Global step 700 Train loss 0.546636 ACC 0.5 on epoch=233
05/21/2022 09:14:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.523483 on epoch=236
05/21/2022 09:14:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.499697 on epoch=239
05/21/2022 09:14:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.574520 on epoch=243
05/21/2022 09:14:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.572960 on epoch=246
05/21/2022 09:14:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.447701 on epoch=249
05/21/2022 09:14:39 - INFO - __main__ - Global step 750 Train loss 0.523672 ACC 0.375 on epoch=249
05/21/2022 09:14:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.441938 on epoch=253
05/21/2022 09:14:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.460398 on epoch=256
05/21/2022 09:14:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.422448 on epoch=259
05/21/2022 09:14:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.425766 on epoch=263
05/21/2022 09:14:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.432223 on epoch=266
05/21/2022 09:14:53 - INFO - __main__ - Global step 800 Train loss 0.436555 ACC 0.375 on epoch=266
05/21/2022 09:14:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.470739 on epoch=269
05/21/2022 09:14:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.404585 on epoch=273
05/21/2022 09:15:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.381578 on epoch=276
05/21/2022 09:15:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.416059 on epoch=279
05/21/2022 09:15:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.387946 on epoch=283
05/21/2022 09:15:08 - INFO - __main__ - Global step 850 Train loss 0.412181 ACC 0.46875 on epoch=283
05/21/2022 09:15:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.424610 on epoch=286
05/21/2022 09:15:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.415812 on epoch=289
05/21/2022 09:15:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.430358 on epoch=293
05/21/2022 09:15:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.431428 on epoch=296
05/21/2022 09:15:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.427368 on epoch=299
05/21/2022 09:15:23 - INFO - __main__ - Global step 900 Train loss 0.425915 ACC 0.25 on epoch=299
05/21/2022 09:15:23 - INFO - __main__ - save last model!
05/21/2022 09:15:23 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 09:15:23 - INFO - __main__ - Printing 3 examples
05/21/2022 09:15:23 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/21/2022 09:15:23 - INFO - __main__ - ['contradiction']
05/21/2022 09:15:23 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/21/2022 09:15:23 - INFO - __main__ - ['contradiction']
05/21/2022 09:15:23 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/21/2022 09:15:23 - INFO - __main__ - ['contradiction']
05/21/2022 09:15:23 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:15:23 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:15:24 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 09:15:24 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 09:15:24 - INFO - __main__ - Printing 3 examples
05/21/2022 09:15:24 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
05/21/2022 09:15:24 - INFO - __main__ - ['contradiction']
05/21/2022 09:15:24 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
05/21/2022 09:15:24 - INFO - __main__ - ['contradiction']
05/21/2022 09:15:24 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
05/21/2022 09:15:24 - INFO - __main__ - ['contradiction']
05/21/2022 09:15:24 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:15:24 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:15:24 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 09:15:27 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 09:15:27 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 09:15:27 - INFO - __main__ - Printing 3 examples
05/21/2022 09:15:27 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 09:15:27 - INFO - __main__ - ['contradiction']
05/21/2022 09:15:27 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 09:15:27 - INFO - __main__ - ['neutral']
05/21/2022 09:15:27 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 09:15:27 - INFO - __main__ - ['entailment']
05/21/2022 09:15:27 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:15:27 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:15:27 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 09:15:28 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_87_0.0005_8_predictions.txt
05/21/2022 09:15:28 - INFO - __main__ - ACC on test data: 0.6250
05/21/2022 09:15:28 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.0005, bsz=8, dev_performance=0.59375, test_performance=0.625
05/21/2022 09:15:28 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.0003, bsz=8 ...
05/21/2022 09:15:29 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 09:15:29 - INFO - __main__ - Printing 3 examples
05/21/2022 09:15:29 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/21/2022 09:15:29 - INFO - __main__ - ['contradiction']
05/21/2022 09:15:29 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/21/2022 09:15:29 - INFO - __main__ - ['contradiction']
05/21/2022 09:15:29 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/21/2022 09:15:29 - INFO - __main__ - ['contradiction']
05/21/2022 09:15:29 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:15:29 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:15:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 09:15:29 - INFO - __main__ - Starting training!
05/21/2022 09:15:29 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 09:15:29 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 09:15:29 - INFO - __main__ - Printing 3 examples
05/21/2022 09:15:29 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
05/21/2022 09:15:29 - INFO - __main__ - ['contradiction']
05/21/2022 09:15:29 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
05/21/2022 09:15:29 - INFO - __main__ - ['contradiction']
05/21/2022 09:15:29 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
05/21/2022 09:15:29 - INFO - __main__ - ['contradiction']
05/21/2022 09:15:29 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:15:29 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:15:29 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 09:15:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 09:15:34 - INFO - __main__ - Starting training!
05/21/2022 09:15:36 - INFO - __main__ - Step 10 Global step 10 Train loss 18.338139 on epoch=3
05/21/2022 09:15:39 - INFO - __main__ - Step 20 Global step 20 Train loss 14.279963 on epoch=6
05/21/2022 09:15:42 - INFO - __main__ - Step 30 Global step 30 Train loss 9.660048 on epoch=9
05/21/2022 09:15:45 - INFO - __main__ - Step 40 Global step 40 Train loss 7.981909 on epoch=13
05/21/2022 09:15:48 - INFO - __main__ - Step 50 Global step 50 Train loss 6.759780 on epoch=16
05/21/2022 09:15:50 - INFO - __main__ - Global step 50 Train loss 11.403967 ACC 0.0 on epoch=16
05/21/2022 09:15:53 - INFO - __main__ - Step 60 Global step 60 Train loss 4.992792 on epoch=19
05/21/2022 09:15:56 - INFO - __main__ - Step 70 Global step 70 Train loss 5.373463 on epoch=23
05/21/2022 09:15:59 - INFO - __main__ - Step 80 Global step 80 Train loss 3.528971 on epoch=26
05/21/2022 09:16:02 - INFO - __main__ - Step 90 Global step 90 Train loss 3.193351 on epoch=29
05/21/2022 09:16:05 - INFO - __main__ - Step 100 Global step 100 Train loss 2.344924 on epoch=33
05/21/2022 09:16:05 - INFO - __main__ - Global step 100 Train loss 3.886700 ACC 0.0 on epoch=33
05/21/2022 09:16:08 - INFO - __main__ - Step 110 Global step 110 Train loss 3.238068 on epoch=36
05/21/2022 09:16:10 - INFO - __main__ - Step 120 Global step 120 Train loss 2.479419 on epoch=39
05/21/2022 09:16:13 - INFO - __main__ - Step 130 Global step 130 Train loss 2.120551 on epoch=43
05/21/2022 09:16:16 - INFO - __main__ - Step 140 Global step 140 Train loss 2.446624 on epoch=46
05/21/2022 09:16:18 - INFO - __main__ - Step 150 Global step 150 Train loss 2.354990 on epoch=49
05/21/2022 09:16:19 - INFO - __main__ - Global step 150 Train loss 2.527930 ACC 0.0 on epoch=49
05/21/2022 09:16:21 - INFO - __main__ - Step 160 Global step 160 Train loss 2.042262 on epoch=53
05/21/2022 09:16:24 - INFO - __main__ - Step 170 Global step 170 Train loss 1.611120 on epoch=56
05/21/2022 09:16:26 - INFO - __main__ - Step 180 Global step 180 Train loss 2.334049 on epoch=59
05/21/2022 09:16:29 - INFO - __main__ - Step 190 Global step 190 Train loss 1.506557 on epoch=63
05/21/2022 09:16:32 - INFO - __main__ - Step 200 Global step 200 Train loss 1.590665 on epoch=66
05/21/2022 09:16:32 - INFO - __main__ - Global step 200 Train loss 1.816931 ACC 0.03125 on epoch=66
05/21/2022 09:16:36 - INFO - __main__ - Step 210 Global step 210 Train loss 1.316586 on epoch=69
05/21/2022 09:16:38 - INFO - __main__ - Step 220 Global step 220 Train loss 1.462216 on epoch=73
05/21/2022 09:16:41 - INFO - __main__ - Step 230 Global step 230 Train loss 1.255347 on epoch=76
05/21/2022 09:16:44 - INFO - __main__ - Step 240 Global step 240 Train loss 1.297764 on epoch=79
05/21/2022 09:16:46 - INFO - __main__ - Step 250 Global step 250 Train loss 1.020163 on epoch=83
05/21/2022 09:16:47 - INFO - __main__ - Global step 250 Train loss 1.270415 ACC 0.0 on epoch=83
05/21/2022 09:16:49 - INFO - __main__ - Step 260 Global step 260 Train loss 1.098928 on epoch=86
05/21/2022 09:16:52 - INFO - __main__ - Step 270 Global step 270 Train loss 1.197032 on epoch=89
05/21/2022 09:16:55 - INFO - __main__ - Step 280 Global step 280 Train loss 1.131472 on epoch=93
05/21/2022 09:16:58 - INFO - __main__ - Step 290 Global step 290 Train loss 1.083851 on epoch=96
05/21/2022 09:17:00 - INFO - __main__ - Step 300 Global step 300 Train loss 1.174153 on epoch=99
05/21/2022 09:17:01 - INFO - __main__ - Global step 300 Train loss 1.137087 ACC 0.15625 on epoch=99
05/21/2022 09:17:04 - INFO - __main__ - Step 310 Global step 310 Train loss 1.031613 on epoch=103
05/21/2022 09:17:07 - INFO - __main__ - Step 320 Global step 320 Train loss 1.002920 on epoch=106
05/21/2022 09:17:10 - INFO - __main__ - Step 330 Global step 330 Train loss 1.076766 on epoch=109
05/21/2022 09:17:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.979966 on epoch=113
05/21/2022 09:17:15 - INFO - __main__ - Step 350 Global step 350 Train loss 1.228605 on epoch=116
05/21/2022 09:17:16 - INFO - __main__ - Global step 350 Train loss 1.063974 ACC 0.375 on epoch=116
05/21/2022 09:17:19 - INFO - __main__ - Step 360 Global step 360 Train loss 1.100095 on epoch=119
05/21/2022 09:17:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.957588 on epoch=123
05/21/2022 09:17:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.829314 on epoch=126
05/21/2022 09:17:28 - INFO - __main__ - Step 390 Global step 390 Train loss 1.053593 on epoch=129
05/21/2022 09:17:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.693104 on epoch=133
05/21/2022 09:17:31 - INFO - __main__ - Global step 400 Train loss 0.926739 ACC 0.4375 on epoch=133
05/21/2022 09:17:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.707089 on epoch=136
05/21/2022 09:17:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.959442 on epoch=139
05/21/2022 09:17:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.816541 on epoch=143
05/21/2022 09:17:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.840944 on epoch=146
05/21/2022 09:17:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.771823 on epoch=149
05/21/2022 09:17:46 - INFO - __main__ - Global step 450 Train loss 0.819168 ACC 0.34375 on epoch=149
05/21/2022 09:17:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.668201 on epoch=153
05/21/2022 09:17:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.819280 on epoch=156
05/21/2022 09:17:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.643028 on epoch=159
05/21/2022 09:17:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.748256 on epoch=163
05/21/2022 09:18:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.602467 on epoch=166
05/21/2022 09:18:01 - INFO - __main__ - Global step 500 Train loss 0.696246 ACC 0.46875 on epoch=166
05/21/2022 09:18:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.690800 on epoch=169
05/21/2022 09:18:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.600926 on epoch=173
05/21/2022 09:18:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.548086 on epoch=176
05/21/2022 09:18:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.634865 on epoch=179
05/21/2022 09:18:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.553564 on epoch=183
05/21/2022 09:18:14 - INFO - __main__ - Global step 550 Train loss 0.605648 ACC 0.59375 on epoch=183
05/21/2022 09:18:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.691825 on epoch=186
05/21/2022 09:18:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.680828 on epoch=189
05/21/2022 09:18:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.611212 on epoch=193
05/21/2022 09:18:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.417491 on epoch=196
05/21/2022 09:18:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.498852 on epoch=199
05/21/2022 09:18:30 - INFO - __main__ - Global step 600 Train loss 0.580042 ACC 0.5 on epoch=199
05/21/2022 09:18:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.506808 on epoch=203
05/21/2022 09:18:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.372429 on epoch=206
05/21/2022 09:18:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.496811 on epoch=209
05/21/2022 09:18:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.564928 on epoch=213
05/21/2022 09:18:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.488939 on epoch=216
05/21/2022 09:18:45 - INFO - __main__ - Global step 650 Train loss 0.485983 ACC 0.46875 on epoch=216
05/21/2022 09:18:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.623998 on epoch=219
05/21/2022 09:18:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.396626 on epoch=223
05/21/2022 09:18:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.499085 on epoch=226
05/21/2022 09:18:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.486632 on epoch=229
05/21/2022 09:18:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.471640 on epoch=233
05/21/2022 09:18:59 - INFO - __main__ - Global step 700 Train loss 0.495596 ACC 0.40625 on epoch=233
05/21/2022 09:19:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.479411 on epoch=236
05/21/2022 09:19:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.489885 on epoch=239
05/21/2022 09:19:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.536805 on epoch=243
05/21/2022 09:19:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.426967 on epoch=246
05/21/2022 09:19:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.471719 on epoch=249
05/21/2022 09:19:14 - INFO - __main__ - Global step 750 Train loss 0.480958 ACC 0.46875 on epoch=249
05/21/2022 09:19:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.436159 on epoch=253
05/21/2022 09:19:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.371762 on epoch=256
05/21/2022 09:19:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.460271 on epoch=259
05/21/2022 09:19:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.424470 on epoch=263
05/21/2022 09:19:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.422490 on epoch=266
05/21/2022 09:19:29 - INFO - __main__ - Global step 800 Train loss 0.423031 ACC 0.4375 on epoch=266
05/21/2022 09:19:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.456854 on epoch=269
05/21/2022 09:19:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.417933 on epoch=273
05/21/2022 09:19:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.430471 on epoch=276
05/21/2022 09:19:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.386358 on epoch=279
05/21/2022 09:19:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.416419 on epoch=283
05/21/2022 09:19:43 - INFO - __main__ - Global step 850 Train loss 0.421607 ACC 0.4375 on epoch=283
05/21/2022 09:19:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.374191 on epoch=286
05/21/2022 09:19:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.438348 on epoch=289
05/21/2022 09:19:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.381213 on epoch=293
05/21/2022 09:19:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.359521 on epoch=296
05/21/2022 09:19:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.422219 on epoch=299
05/21/2022 09:19:57 - INFO - __main__ - Global step 900 Train loss 0.395098 ACC 0.46875 on epoch=299
05/21/2022 09:19:57 - INFO - __main__ - save last model!
05/21/2022 09:19:58 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 09:19:58 - INFO - __main__ - Printing 3 examples
05/21/2022 09:19:58 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/21/2022 09:19:58 - INFO - __main__ - ['contradiction']
05/21/2022 09:19:58 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/21/2022 09:19:58 - INFO - __main__ - ['contradiction']
05/21/2022 09:19:58 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/21/2022 09:19:58 - INFO - __main__ - ['contradiction']
05/21/2022 09:19:58 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:19:58 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:19:58 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 09:19:58 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 09:19:58 - INFO - __main__ - Printing 3 examples
05/21/2022 09:19:58 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
05/21/2022 09:19:58 - INFO - __main__ - ['contradiction']
05/21/2022 09:19:58 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
05/21/2022 09:19:58 - INFO - __main__ - ['contradiction']
05/21/2022 09:19:58 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
05/21/2022 09:19:58 - INFO - __main__ - ['contradiction']
05/21/2022 09:19:58 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:19:58 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:19:58 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 09:20:01 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 09:20:01 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 09:20:01 - INFO - __main__ - Printing 3 examples
05/21/2022 09:20:01 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 09:20:01 - INFO - __main__ - ['contradiction']
05/21/2022 09:20:01 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 09:20:01 - INFO - __main__ - ['neutral']
05/21/2022 09:20:01 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 09:20:01 - INFO - __main__ - ['entailment']
05/21/2022 09:20:01 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:20:01 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:20:01 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 09:20:02 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_87_0.0003_8_predictions.txt
05/21/2022 09:20:02 - INFO - __main__ - ACC on test data: 0.4643
05/21/2022 09:20:02 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.0003, bsz=8, dev_performance=0.59375, test_performance=0.4642857142857143
05/21/2022 09:20:02 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.0002, bsz=8 ...
05/21/2022 09:20:03 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 09:20:03 - INFO - __main__ - Printing 3 examples
05/21/2022 09:20:03 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/21/2022 09:20:03 - INFO - __main__ - ['contradiction']
05/21/2022 09:20:03 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/21/2022 09:20:03 - INFO - __main__ - ['contradiction']
05/21/2022 09:20:03 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/21/2022 09:20:03 - INFO - __main__ - ['contradiction']
05/21/2022 09:20:03 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:20:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 09:20:03 - INFO - __main__ - Starting training!
05/21/2022 09:20:03 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:20:03 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 09:20:03 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 09:20:03 - INFO - __main__ - Printing 3 examples
05/21/2022 09:20:03 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
05/21/2022 09:20:03 - INFO - __main__ - ['contradiction']
05/21/2022 09:20:03 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
05/21/2022 09:20:03 - INFO - __main__ - ['contradiction']
05/21/2022 09:20:03 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
05/21/2022 09:20:03 - INFO - __main__ - ['contradiction']
05/21/2022 09:20:03 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:20:03 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:20:03 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 09:20:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 09:20:09 - INFO - __main__ - Starting training!
05/21/2022 09:20:11 - INFO - __main__ - Step 10 Global step 10 Train loss 17.293161 on epoch=3
05/21/2022 09:20:13 - INFO - __main__ - Step 20 Global step 20 Train loss 16.232920 on epoch=6
05/21/2022 09:20:17 - INFO - __main__ - Step 30 Global step 30 Train loss 12.428438 on epoch=9
05/21/2022 09:20:20 - INFO - __main__ - Step 40 Global step 40 Train loss 9.761236 on epoch=13
05/21/2022 09:20:22 - INFO - __main__ - Step 50 Global step 50 Train loss 8.536390 on epoch=16
05/21/2022 09:20:25 - INFO - __main__ - Global step 50 Train loss 12.850430 ACC 0.0 on epoch=16
05/21/2022 09:20:28 - INFO - __main__ - Step 60 Global step 60 Train loss 7.011492 on epoch=19
05/21/2022 09:20:31 - INFO - __main__ - Step 70 Global step 70 Train loss 6.901046 on epoch=23
05/21/2022 09:20:34 - INFO - __main__ - Step 80 Global step 80 Train loss 6.768648 on epoch=26
05/21/2022 09:20:37 - INFO - __main__ - Step 90 Global step 90 Train loss 5.051389 on epoch=29
05/21/2022 09:20:40 - INFO - __main__ - Step 100 Global step 100 Train loss 4.760290 on epoch=33
05/21/2022 09:20:41 - INFO - __main__ - Global step 100 Train loss 6.098572 ACC 0.0 on epoch=33
05/21/2022 09:20:43 - INFO - __main__ - Step 110 Global step 110 Train loss 4.147763 on epoch=36
05/21/2022 09:20:46 - INFO - __main__ - Step 120 Global step 120 Train loss 3.136886 on epoch=39
05/21/2022 09:20:49 - INFO - __main__ - Step 130 Global step 130 Train loss 3.094576 on epoch=43
05/21/2022 09:20:51 - INFO - __main__ - Step 140 Global step 140 Train loss 2.548669 on epoch=46
05/21/2022 09:20:54 - INFO - __main__ - Step 150 Global step 150 Train loss 3.477159 on epoch=49
05/21/2022 09:20:54 - INFO - __main__ - Global step 150 Train loss 3.281010 ACC 0.28125 on epoch=49
05/21/2022 09:20:57 - INFO - __main__ - Step 160 Global step 160 Train loss 2.502173 on epoch=53
05/21/2022 09:21:00 - INFO - __main__ - Step 170 Global step 170 Train loss 2.630351 on epoch=56
05/21/2022 09:21:03 - INFO - __main__ - Step 180 Global step 180 Train loss 2.236140 on epoch=59
05/21/2022 09:21:06 - INFO - __main__ - Step 190 Global step 190 Train loss 2.312088 on epoch=63
05/21/2022 09:21:09 - INFO - __main__ - Step 200 Global step 200 Train loss 2.747312 on epoch=66
05/21/2022 09:21:09 - INFO - __main__ - Global step 200 Train loss 2.485612 ACC 0.5 on epoch=66
05/21/2022 09:21:12 - INFO - __main__ - Step 210 Global step 210 Train loss 1.975900 on epoch=69
05/21/2022 09:21:15 - INFO - __main__ - Step 220 Global step 220 Train loss 1.620886 on epoch=73
05/21/2022 09:21:18 - INFO - __main__ - Step 230 Global step 230 Train loss 1.968355 on epoch=76
05/21/2022 09:21:21 - INFO - __main__ - Step 240 Global step 240 Train loss 1.769626 on epoch=79
05/21/2022 09:21:24 - INFO - __main__ - Step 250 Global step 250 Train loss 1.555990 on epoch=83
05/21/2022 09:21:24 - INFO - __main__ - Global step 250 Train loss 1.778152 ACC 0.1875 on epoch=83
05/21/2022 09:21:27 - INFO - __main__ - Step 260 Global step 260 Train loss 1.683258 on epoch=86
05/21/2022 09:21:30 - INFO - __main__ - Step 270 Global step 270 Train loss 2.029918 on epoch=89
05/21/2022 09:21:33 - INFO - __main__ - Step 280 Global step 280 Train loss 2.001233 on epoch=93
05/21/2022 09:21:35 - INFO - __main__ - Step 290 Global step 290 Train loss 1.678200 on epoch=96
05/21/2022 09:21:38 - INFO - __main__ - Step 300 Global step 300 Train loss 1.297187 on epoch=99
05/21/2022 09:21:39 - INFO - __main__ - Global step 300 Train loss 1.737959 ACC 0.0 on epoch=99
05/21/2022 09:21:41 - INFO - __main__ - Step 310 Global step 310 Train loss 1.226691 on epoch=103
05/21/2022 09:21:45 - INFO - __main__ - Step 320 Global step 320 Train loss 1.117116 on epoch=106
05/21/2022 09:21:48 - INFO - __main__ - Step 330 Global step 330 Train loss 1.194091 on epoch=109
05/21/2022 09:21:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.906742 on epoch=113
05/21/2022 09:21:53 - INFO - __main__ - Step 350 Global step 350 Train loss 1.390575 on epoch=116
05/21/2022 09:21:54 - INFO - __main__ - Global step 350 Train loss 1.167043 ACC 0.375 on epoch=116
05/21/2022 09:21:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.969878 on epoch=119
05/21/2022 09:21:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.935022 on epoch=123
05/21/2022 09:22:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.560544 on epoch=126
05/21/2022 09:22:04 - INFO - __main__ - Step 390 Global step 390 Train loss 1.033641 on epoch=129
05/21/2022 09:22:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.745192 on epoch=133
05/21/2022 09:22:08 - INFO - __main__ - Global step 400 Train loss 0.848855 ACC 0.71875 on epoch=133
05/21/2022 09:22:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.952705 on epoch=136
05/21/2022 09:22:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.724884 on epoch=139
05/21/2022 09:22:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.680236 on epoch=143
05/21/2022 09:22:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.634075 on epoch=146
05/21/2022 09:22:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.739162 on epoch=149
05/21/2022 09:22:22 - INFO - __main__ - Global step 450 Train loss 0.746212 ACC 0.8125 on epoch=149
05/21/2022 09:22:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.635855 on epoch=153
05/21/2022 09:22:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.753116 on epoch=156
05/21/2022 09:22:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.730201 on epoch=159
05/21/2022 09:22:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.516827 on epoch=163
05/21/2022 09:22:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.599391 on epoch=166
05/21/2022 09:22:37 - INFO - __main__ - Global step 500 Train loss 0.647078 ACC 0.78125 on epoch=166
05/21/2022 09:22:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.392033 on epoch=169
05/21/2022 09:22:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.378275 on epoch=173
05/21/2022 09:22:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.431715 on epoch=176
05/21/2022 09:22:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.346536 on epoch=179
05/21/2022 09:22:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.443266 on epoch=183
05/21/2022 09:22:52 - INFO - __main__ - Global step 550 Train loss 0.398365 ACC 0.65625 on epoch=183
05/21/2022 09:22:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.288820 on epoch=186
05/21/2022 09:22:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.566536 on epoch=189
05/21/2022 09:23:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.818989 on epoch=193
05/21/2022 09:23:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.327184 on epoch=196
05/21/2022 09:23:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.304603 on epoch=199
05/21/2022 09:23:06 - INFO - __main__ - Global step 600 Train loss 0.461226 ACC 0.8125 on epoch=199
05/21/2022 09:23:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.319427 on epoch=203
05/21/2022 09:23:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.455166 on epoch=206
05/21/2022 09:23:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.471343 on epoch=209
05/21/2022 09:23:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.330414 on epoch=213
05/21/2022 09:23:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.223056 on epoch=216
05/21/2022 09:23:21 - INFO - __main__ - Global step 650 Train loss 0.359881 ACC 0.8125 on epoch=216
05/21/2022 09:23:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.199015 on epoch=219
05/21/2022 09:23:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.230084 on epoch=223
05/21/2022 09:23:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.163632 on epoch=226
05/21/2022 09:23:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.284145 on epoch=229
05/21/2022 09:23:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.156448 on epoch=233
05/21/2022 09:23:36 - INFO - __main__ - Global step 700 Train loss 0.206665 ACC 0.84375 on epoch=233
05/21/2022 09:23:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.212174 on epoch=236
05/21/2022 09:23:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.272835 on epoch=239
05/21/2022 09:23:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.099930 on epoch=243
05/21/2022 09:23:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.285001 on epoch=246
05/21/2022 09:23:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.197564 on epoch=249
05/21/2022 09:23:50 - INFO - __main__ - Global step 750 Train loss 0.213501 ACC 0.78125 on epoch=249
05/21/2022 09:23:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.262325 on epoch=253
05/21/2022 09:23:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.177812 on epoch=256
05/21/2022 09:23:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.131320 on epoch=259
05/21/2022 09:24:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.095691 on epoch=263
05/21/2022 09:24:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.139471 on epoch=266
05/21/2022 09:24:06 - INFO - __main__ - Global step 800 Train loss 0.161324 ACC 0.84375 on epoch=266
05/21/2022 09:24:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.143633 on epoch=269
05/21/2022 09:24:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.158792 on epoch=273
05/21/2022 09:24:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.292052 on epoch=276
05/21/2022 09:24:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.256271 on epoch=279
05/21/2022 09:24:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.173457 on epoch=283
05/21/2022 09:24:20 - INFO - __main__ - Global step 850 Train loss 0.204841 ACC 0.75 on epoch=283
05/21/2022 09:24:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.148799 on epoch=286
05/21/2022 09:24:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.225328 on epoch=289
05/21/2022 09:24:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.412482 on epoch=293
05/21/2022 09:24:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.450596 on epoch=296
05/21/2022 09:24:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.727307 on epoch=299
05/21/2022 09:24:34 - INFO - __main__ - Global step 900 Train loss 0.392902 ACC 0.46875 on epoch=299
05/21/2022 09:24:34 - INFO - __main__ - save last model!
05/21/2022 09:24:35 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 09:24:35 - INFO - __main__ - Printing 3 examples
05/21/2022 09:24:35 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/21/2022 09:24:35 - INFO - __main__ - ['contradiction']
05/21/2022 09:24:35 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/21/2022 09:24:35 - INFO - __main__ - ['contradiction']
05/21/2022 09:24:35 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/21/2022 09:24:35 - INFO - __main__ - ['contradiction']
05/21/2022 09:24:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:24:35 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:24:35 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 09:24:35 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 09:24:35 - INFO - __main__ - Printing 3 examples
05/21/2022 09:24:35 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
05/21/2022 09:24:35 - INFO - __main__ - ['contradiction']
05/21/2022 09:24:35 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
05/21/2022 09:24:35 - INFO - __main__ - ['contradiction']
05/21/2022 09:24:35 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
05/21/2022 09:24:35 - INFO - __main__ - ['contradiction']
05/21/2022 09:24:35 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:24:35 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:24:35 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 09:24:38 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 09:24:38 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 09:24:38 - INFO - __main__ - Printing 3 examples
05/21/2022 09:24:38 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 09:24:38 - INFO - __main__ - ['contradiction']
05/21/2022 09:24:38 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 09:24:38 - INFO - __main__ - ['neutral']
05/21/2022 09:24:38 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 09:24:38 - INFO - __main__ - ['entailment']
05/21/2022 09:24:38 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:24:38 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:24:38 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 09:24:39 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_87_0.0002_8_predictions.txt
05/21/2022 09:24:39 - INFO - __main__ - ACC on test data: 0.7321
05/21/2022 09:24:39 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.0002, bsz=8, dev_performance=0.84375, test_performance=0.7321428571428571
05/21/2022 09:24:39 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.0001, bsz=8 ...
05/21/2022 09:24:40 - INFO - __main__ - Start tokenizing ... 48 instances
05/21/2022 09:24:40 - INFO - __main__ - Printing 3 examples
05/21/2022 09:24:40 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
05/21/2022 09:24:40 - INFO - __main__ - ['contradiction']
05/21/2022 09:24:40 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
05/21/2022 09:24:40 - INFO - __main__ - ['contradiction']
05/21/2022 09:24:40 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
05/21/2022 09:24:40 - INFO - __main__ - ['contradiction']
05/21/2022 09:24:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:24:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:24:40 - INFO - __main__ - Loaded 48 examples from train data
05/21/2022 09:24:40 - INFO - __main__ - Start tokenizing ... 32 instances
05/21/2022 09:24:40 - INFO - __main__ - Printing 3 examples
05/21/2022 09:24:40 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
05/21/2022 09:24:40 - INFO - __main__ - ['contradiction']
05/21/2022 09:24:40 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
05/21/2022 09:24:40 - INFO - __main__ - ['contradiction']
05/21/2022 09:24:40 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
05/21/2022 09:24:40 - INFO - __main__ - ['contradiction']
05/21/2022 09:24:40 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:24:40 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:24:40 - INFO - __main__ - Loaded 32 examples from dev data
05/21/2022 09:24:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 09:24:40 - INFO - __main__ - Starting training!
05/21/2022 09:24:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.11M parameters
05/21/2022 09:24:44 - INFO - __main__ - Starting training!
05/21/2022 09:24:46 - INFO - __main__ - Step 10 Global step 10 Train loss 18.390162 on epoch=3
05/21/2022 09:24:49 - INFO - __main__ - Step 20 Global step 20 Train loss 16.249537 on epoch=6
05/21/2022 09:24:52 - INFO - __main__ - Step 30 Global step 30 Train loss 14.358335 on epoch=9
05/21/2022 09:24:55 - INFO - __main__ - Step 40 Global step 40 Train loss 12.300209 on epoch=13
05/21/2022 09:24:58 - INFO - __main__ - Step 50 Global step 50 Train loss 10.788472 on epoch=16
05/21/2022 09:25:03 - INFO - __main__ - Global step 50 Train loss 14.417342 ACC 0.0 on epoch=16
05/21/2022 09:25:06 - INFO - __main__ - Step 60 Global step 60 Train loss 9.787730 on epoch=19
05/21/2022 09:25:09 - INFO - __main__ - Step 70 Global step 70 Train loss 9.735407 on epoch=23
05/21/2022 09:25:11 - INFO - __main__ - Step 80 Global step 80 Train loss 7.992747 on epoch=26
05/21/2022 09:25:15 - INFO - __main__ - Step 90 Global step 90 Train loss 8.195810 on epoch=29
05/21/2022 09:25:17 - INFO - __main__ - Step 100 Global step 100 Train loss 7.437413 on epoch=33
05/21/2022 09:25:19 - INFO - __main__ - Global step 100 Train loss 8.629822 ACC 0.0 on epoch=33
05/21/2022 09:25:21 - INFO - __main__ - Step 110 Global step 110 Train loss 7.578204 on epoch=36
05/21/2022 09:25:24 - INFO - __main__ - Step 120 Global step 120 Train loss 6.762218 on epoch=39
05/21/2022 09:25:27 - INFO - __main__ - Step 130 Global step 130 Train loss 6.736846 on epoch=43
05/21/2022 09:25:30 - INFO - __main__ - Step 140 Global step 140 Train loss 5.773082 on epoch=46
05/21/2022 09:25:32 - INFO - __main__ - Step 150 Global step 150 Train loss 6.195779 on epoch=49
05/21/2022 09:25:33 - INFO - __main__ - Global step 150 Train loss 6.609226 ACC 0.0625 on epoch=49
05/21/2022 09:25:36 - INFO - __main__ - Step 160 Global step 160 Train loss 4.514611 on epoch=53
05/21/2022 09:25:39 - INFO - __main__ - Step 170 Global step 170 Train loss 3.994349 on epoch=56
05/21/2022 09:25:42 - INFO - __main__ - Step 180 Global step 180 Train loss 4.679502 on epoch=59
05/21/2022 09:25:45 - INFO - __main__ - Step 190 Global step 190 Train loss 4.132262 on epoch=63
05/21/2022 09:25:48 - INFO - __main__ - Step 200 Global step 200 Train loss 3.910724 on epoch=66
05/21/2022 09:25:48 - INFO - __main__ - Global step 200 Train loss 4.246289 ACC 0.09375 on epoch=66
05/21/2022 09:25:51 - INFO - __main__ - Step 210 Global step 210 Train loss 4.303822 on epoch=69
05/21/2022 09:25:54 - INFO - __main__ - Step 220 Global step 220 Train loss 4.080168 on epoch=73
05/21/2022 09:25:57 - INFO - __main__ - Step 230 Global step 230 Train loss 2.432859 on epoch=76
05/21/2022 09:26:00 - INFO - __main__ - Step 240 Global step 240 Train loss 3.435077 on epoch=79
05/21/2022 09:26:03 - INFO - __main__ - Step 250 Global step 250 Train loss 2.799812 on epoch=83
05/21/2022 09:26:03 - INFO - __main__ - Global step 250 Train loss 3.410348 ACC 0.0 on epoch=83
05/21/2022 09:26:06 - INFO - __main__ - Step 260 Global step 260 Train loss 1.867717 on epoch=86
05/21/2022 09:26:08 - INFO - __main__ - Step 270 Global step 270 Train loss 3.170618 on epoch=89
05/21/2022 09:26:11 - INFO - __main__ - Step 280 Global step 280 Train loss 2.582996 on epoch=93
05/21/2022 09:26:14 - INFO - __main__ - Step 290 Global step 290 Train loss 2.244475 on epoch=96
05/21/2022 09:26:16 - INFO - __main__ - Step 300 Global step 300 Train loss 2.049377 on epoch=99
05/21/2022 09:26:17 - INFO - __main__ - Global step 300 Train loss 2.383037 ACC 0.28125 on epoch=99
05/21/2022 09:26:20 - INFO - __main__ - Step 310 Global step 310 Train loss 2.376173 on epoch=103
05/21/2022 09:26:23 - INFO - __main__ - Step 320 Global step 320 Train loss 2.953645 on epoch=106
05/21/2022 09:26:26 - INFO - __main__ - Step 330 Global step 330 Train loss 1.726395 on epoch=109
05/21/2022 09:26:29 - INFO - __main__ - Step 340 Global step 340 Train loss 1.798719 on epoch=113
05/21/2022 09:26:32 - INFO - __main__ - Step 350 Global step 350 Train loss 1.669724 on epoch=116
05/21/2022 09:26:32 - INFO - __main__ - Global step 350 Train loss 2.104931 ACC 0.28125 on epoch=116
05/21/2022 09:26:35 - INFO - __main__ - Step 360 Global step 360 Train loss 1.927041 on epoch=119
05/21/2022 09:26:38 - INFO - __main__ - Step 370 Global step 370 Train loss 1.530251 on epoch=123
05/21/2022 09:26:42 - INFO - __main__ - Step 380 Global step 380 Train loss 1.741296 on epoch=126
05/21/2022 09:26:44 - INFO - __main__ - Step 390 Global step 390 Train loss 1.871913 on epoch=129
05/21/2022 09:26:47 - INFO - __main__ - Step 400 Global step 400 Train loss 2.047292 on epoch=133
05/21/2022 09:26:47 - INFO - __main__ - Global step 400 Train loss 1.823558 ACC 0.3125 on epoch=133
05/21/2022 09:26:51 - INFO - __main__ - Step 410 Global step 410 Train loss 1.345265 on epoch=136
05/21/2022 09:26:54 - INFO - __main__ - Step 420 Global step 420 Train loss 1.956664 on epoch=139
05/21/2022 09:26:57 - INFO - __main__ - Step 430 Global step 430 Train loss 1.578287 on epoch=143
05/21/2022 09:27:00 - INFO - __main__ - Step 440 Global step 440 Train loss 1.892831 on epoch=146
05/21/2022 09:27:03 - INFO - __main__ - Step 450 Global step 450 Train loss 1.508194 on epoch=149
05/21/2022 09:27:04 - INFO - __main__ - Global step 450 Train loss 1.656248 ACC 0.4375 on epoch=149
05/21/2022 09:27:07 - INFO - __main__ - Step 460 Global step 460 Train loss 2.233759 on epoch=153
05/21/2022 09:27:09 - INFO - __main__ - Step 470 Global step 470 Train loss 1.309374 on epoch=156
05/21/2022 09:27:12 - INFO - __main__ - Step 480 Global step 480 Train loss 1.029503 on epoch=159
05/21/2022 09:27:15 - INFO - __main__ - Step 490 Global step 490 Train loss 1.623459 on epoch=163
05/21/2022 09:27:18 - INFO - __main__ - Step 500 Global step 500 Train loss 1.502686 on epoch=166
05/21/2022 09:27:18 - INFO - __main__ - Global step 500 Train loss 1.539756 ACC 0.375 on epoch=166
05/21/2022 09:27:21 - INFO - __main__ - Step 510 Global step 510 Train loss 1.508913 on epoch=169
05/21/2022 09:27:24 - INFO - __main__ - Step 520 Global step 520 Train loss 1.645747 on epoch=173
05/21/2022 09:27:27 - INFO - __main__ - Step 530 Global step 530 Train loss 1.718818 on epoch=176
05/21/2022 09:27:30 - INFO - __main__ - Step 540 Global step 540 Train loss 1.277923 on epoch=179
05/21/2022 09:27:32 - INFO - __main__ - Step 550 Global step 550 Train loss 1.350639 on epoch=183
05/21/2022 09:27:33 - INFO - __main__ - Global step 550 Train loss 1.500408 ACC 0.1875 on epoch=183
05/21/2022 09:27:35 - INFO - __main__ - Step 560 Global step 560 Train loss 1.241127 on epoch=186
05/21/2022 09:27:38 - INFO - __main__ - Step 570 Global step 570 Train loss 1.478887 on epoch=189
05/21/2022 09:27:41 - INFO - __main__ - Step 580 Global step 580 Train loss 1.220935 on epoch=193
05/21/2022 09:27:44 - INFO - __main__ - Step 590 Global step 590 Train loss 1.307218 on epoch=196
05/21/2022 09:27:46 - INFO - __main__ - Step 600 Global step 600 Train loss 1.239865 on epoch=199
05/21/2022 09:27:47 - INFO - __main__ - Global step 600 Train loss 1.297607 ACC 0.21875 on epoch=199
05/21/2022 09:27:50 - INFO - __main__ - Step 610 Global step 610 Train loss 1.073960 on epoch=203
05/21/2022 09:27:53 - INFO - __main__ - Step 620 Global step 620 Train loss 1.028601 on epoch=206
05/21/2022 09:27:56 - INFO - __main__ - Step 630 Global step 630 Train loss 1.188648 on epoch=209
05/21/2022 09:27:59 - INFO - __main__ - Step 640 Global step 640 Train loss 1.062482 on epoch=213
05/21/2022 09:28:02 - INFO - __main__ - Step 650 Global step 650 Train loss 1.053721 on epoch=216
05/21/2022 09:28:02 - INFO - __main__ - Global step 650 Train loss 1.081482 ACC 0.40625 on epoch=216
05/21/2022 09:28:05 - INFO - __main__ - Step 660 Global step 660 Train loss 1.197414 on epoch=219
05/21/2022 09:28:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.993385 on epoch=223
05/21/2022 09:28:10 - INFO - __main__ - Step 680 Global step 680 Train loss 1.037402 on epoch=226
05/21/2022 09:28:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.938668 on epoch=229
05/21/2022 09:28:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.999651 on epoch=233
05/21/2022 09:28:17 - INFO - __main__ - Global step 700 Train loss 1.033304 ACC 0.59375 on epoch=233
05/21/2022 09:28:19 - INFO - __main__ - Step 710 Global step 710 Train loss 1.232694 on epoch=236
05/21/2022 09:28:22 - INFO - __main__ - Step 720 Global step 720 Train loss 1.183468 on epoch=239
05/21/2022 09:28:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.890550 on epoch=243
05/21/2022 09:28:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.793534 on epoch=246
05/21/2022 09:28:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.631054 on epoch=249
05/21/2022 09:28:31 - INFO - __main__ - Global step 750 Train loss 0.946260 ACC 0.5625 on epoch=249
05/21/2022 09:28:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.717319 on epoch=253
05/21/2022 09:28:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.811038 on epoch=256
05/21/2022 09:28:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.738758 on epoch=259
05/21/2022 09:28:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.764606 on epoch=263
05/21/2022 09:28:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.681079 on epoch=266
05/21/2022 09:28:46 - INFO - __main__ - Global step 800 Train loss 0.742560 ACC 0.78125 on epoch=266
05/21/2022 09:28:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.690146 on epoch=269
05/21/2022 09:28:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.827090 on epoch=273
05/21/2022 09:28:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.802338 on epoch=276
05/21/2022 09:28:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.625082 on epoch=279
05/21/2022 09:29:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.796899 on epoch=283
05/21/2022 09:29:02 - INFO - __main__ - Global step 850 Train loss 0.748311 ACC 0.6875 on epoch=283
05/21/2022 09:29:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.667351 on epoch=286
05/21/2022 09:29:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.575173 on epoch=289
05/21/2022 09:29:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.721611 on epoch=293
05/21/2022 09:29:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.889104 on epoch=296
05/21/2022 09:29:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.688528 on epoch=299
05/21/2022 09:29:16 - INFO - __main__ - Global step 900 Train loss 0.708354 ACC 0.78125 on epoch=299
05/21/2022 09:29:16 - INFO - __main__ - save last model!
05/21/2022 09:29:20 - INFO - __main__ - Loading checkpoint on the fly
05/21/2022 09:29:20 - INFO - __main__ - Start tokenizing ... 56 instances
05/21/2022 09:29:20 - INFO - __main__ - Printing 3 examples
05/21/2022 09:29:20 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
05/21/2022 09:29:20 - INFO - __main__ - ['contradiction']
05/21/2022 09:29:20 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
05/21/2022 09:29:20 - INFO - __main__ - ['neutral']
05/21/2022 09:29:20 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
05/21/2022 09:29:20 - INFO - __main__ - ['entailment']
05/21/2022 09:29:20 - INFO - __main__ - Tokenizing Input ...
05/21/2022 09:29:20 - INFO - __main__ - Tokenizing Output ...
05/21/2022 09:29:20 - INFO - __main__ - Loaded 56 examples from test data
05/21/2022 09:29:21 - INFO - __main__ - Saved prediction in models/T5-base-ft-cls2cls/singletask-superglue-cb/superglue-cb_16_87_0.0001_8_predictions.txt
05/21/2022 09:29:21 - INFO - __main__ - ACC on test data: 0.6429
05/21/2022 09:29:21 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.0001, bsz=8, dev_performance=0.78125, test_performance=0.6428571428571429
