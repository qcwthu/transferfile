05/27/2022 04:58:31 - INFO - __main__ - Namespace(task_dir='data_128/emo/', task_name='emo', identifier='T5-large-cls2cls-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls-down128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
05/27/2022 04:58:31 - INFO - __main__ - models/T5-large-cls2cls-down128shot/singletask-emo
05/27/2022 04:58:31 - INFO - __main__ - Namespace(task_dir='data_128/emo/', task_name='emo', identifier='T5-large-cls2cls-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls-down128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
05/27/2022 04:58:31 - INFO - __main__ - models/T5-large-cls2cls-down128shot/singletask-emo
05/27/2022 04:58:33 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/27/2022 04:58:33 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/27/2022 04:58:33 - INFO - __main__ - args.device: cuda:0
05/27/2022 04:58:33 - INFO - __main__ - Using 2 gpus
05/27/2022 04:58:33 - INFO - __main__ - args.device: cuda:1
05/27/2022 04:58:33 - INFO - __main__ - Using 2 gpus
05/27/2022 04:58:33 - INFO - __main__ - Fine-tuning the following samples: ['emo_128_100', 'emo_128_13', 'emo_128_21', 'emo_128_42', 'emo_128_87']
05/27/2022 04:58:33 - INFO - __main__ - Fine-tuning the following samples: ['emo_128_100', 'emo_128_13', 'emo_128_21', 'emo_128_42', 'emo_128_87']
05/27/2022 04:58:37 - INFO - __main__ - Running ... prefix=emo_128_100, lr=0.5, bsz=8 ...
05/27/2022 04:58:38 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 04:58:38 - INFO - __main__ - Printing 3 examples
05/27/2022 04:58:38 - INFO - __main__ -  [emo] how cause yes am listening
05/27/2022 04:58:38 - INFO - __main__ - ['others']
05/27/2022 04:58:38 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/27/2022 04:58:38 - INFO - __main__ - ['others']
05/27/2022 04:58:38 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/27/2022 04:58:38 - INFO - __main__ - ['others']
05/27/2022 04:58:38 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:58:38 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 04:58:38 - INFO - __main__ - Printing 3 examples
05/27/2022 04:58:38 - INFO - __main__ -  [emo] how cause yes am listening
05/27/2022 04:58:38 - INFO - __main__ - ['others']
05/27/2022 04:58:38 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/27/2022 04:58:38 - INFO - __main__ - ['others']
05/27/2022 04:58:38 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/27/2022 04:58:38 - INFO - __main__ - ['others']
05/27/2022 04:58:38 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:58:38 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:58:38 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:58:39 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 04:58:39 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 04:58:39 - INFO - __main__ - Printing 3 examples
05/27/2022 04:58:39 - INFO - __main__ -  [emo] when when it comes to you never why
05/27/2022 04:58:39 - INFO - __main__ - ['others']
05/27/2022 04:58:39 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/27/2022 04:58:39 - INFO - __main__ - ['others']
05/27/2022 04:58:39 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/27/2022 04:58:39 - INFO - __main__ - ['others']
05/27/2022 04:58:39 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:58:39 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 04:58:39 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 04:58:39 - INFO - __main__ - Printing 3 examples
05/27/2022 04:58:39 - INFO - __main__ -  [emo] when when it comes to you never why
05/27/2022 04:58:39 - INFO - __main__ - ['others']
05/27/2022 04:58:39 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/27/2022 04:58:39 - INFO - __main__ - ['others']
05/27/2022 04:58:39 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/27/2022 04:58:39 - INFO - __main__ - ['others']
05/27/2022 04:58:39 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:58:39 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:58:39 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:58:40 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 04:58:40 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 04:58:57 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 04:58:57 - INFO - __main__ - task name: emo
05/27/2022 04:58:57 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 04:58:57 - INFO - __main__ - task name: emo
05/27/2022 04:58:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 04:58:58 - INFO - __main__ - Starting training!
05/27/2022 04:58:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 04:58:58 - INFO - __main__ - Starting training!
05/27/2022 04:59:02 - INFO - __main__ - Step 10 Global step 10 Train loss 6.51 on epoch=0
05/27/2022 04:59:04 - INFO - __main__ - Step 20 Global step 20 Train loss 2.58 on epoch=0
05/27/2022 04:59:06 - INFO - __main__ - Step 30 Global step 30 Train loss 1.37 on epoch=0
05/27/2022 04:59:09 - INFO - __main__ - Step 40 Global step 40 Train loss 1.02 on epoch=1
05/27/2022 04:59:11 - INFO - __main__ - Step 50 Global step 50 Train loss 1.05 on epoch=1
05/27/2022 04:59:18 - INFO - __main__ - Global step 50 Train loss 2.51 Classification-F1 0.1 on epoch=1
05/27/2022 04:59:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=1, global_step=50
05/27/2022 04:59:20 - INFO - __main__ - Step 60 Global step 60 Train loss 1.07 on epoch=1
05/27/2022 04:59:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.97 on epoch=2
05/27/2022 04:59:25 - INFO - __main__ - Step 80 Global step 80 Train loss 1.04 on epoch=2
05/27/2022 04:59:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.89 on epoch=2
05/27/2022 04:59:30 - INFO - __main__ - Step 100 Global step 100 Train loss 0.94 on epoch=3
05/27/2022 04:59:36 - INFO - __main__ - Global step 100 Train loss 0.98 Classification-F1 0.11216739235607161 on epoch=3
05/27/2022 04:59:37 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.11216739235607161 on epoch=3, global_step=100
05/27/2022 04:59:39 - INFO - __main__ - Step 110 Global step 110 Train loss 0.94 on epoch=3
05/27/2022 04:59:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.99 on epoch=3
05/27/2022 04:59:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.91 on epoch=4
05/27/2022 04:59:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.93 on epoch=4
05/27/2022 04:59:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.92 on epoch=4
05/27/2022 04:59:55 - INFO - __main__ - Global step 150 Train loss 0.94 Classification-F1 0.21009858779642954 on epoch=4
05/27/2022 04:59:55 - INFO - __main__ - Saving model with best Classification-F1: 0.11216739235607161 -> 0.21009858779642954 on epoch=4, global_step=150
05/27/2022 04:59:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.86 on epoch=4
05/27/2022 05:00:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.82 on epoch=5
05/27/2022 05:00:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.85 on epoch=5
05/27/2022 05:00:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.90 on epoch=5
05/27/2022 05:00:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.79 on epoch=6
05/27/2022 05:00:13 - INFO - __main__ - Global step 200 Train loss 0.84 Classification-F1 0.0989010989010989 on epoch=6
05/27/2022 05:00:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.89 on epoch=6
05/27/2022 05:00:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.96 on epoch=6
05/27/2022 05:00:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.86 on epoch=7
05/27/2022 05:00:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.92 on epoch=7
05/27/2022 05:00:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.87 on epoch=7
05/27/2022 05:00:32 - INFO - __main__ - Global step 250 Train loss 0.90 Classification-F1 0.12151278609059753 on epoch=7
05/27/2022 05:00:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.80 on epoch=8
05/27/2022 05:00:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.86 on epoch=8
05/27/2022 05:00:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.94 on epoch=8
05/27/2022 05:00:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.77 on epoch=9
05/27/2022 05:00:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.82 on epoch=9
05/27/2022 05:00:50 - INFO - __main__ - Global step 300 Train loss 0.84 Classification-F1 0.33656507975735583 on epoch=9
05/27/2022 05:00:50 - INFO - __main__ - Saving model with best Classification-F1: 0.21009858779642954 -> 0.33656507975735583 on epoch=9, global_step=300
05/27/2022 05:00:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.92 on epoch=9
05/27/2022 05:00:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.77 on epoch=9
05/27/2022 05:00:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.77 on epoch=10
05/27/2022 05:01:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.78 on epoch=10
05/27/2022 05:01:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.91 on epoch=10
05/27/2022 05:01:09 - INFO - __main__ - Global step 350 Train loss 0.83 Classification-F1 0.4170724355577797 on epoch=10
05/27/2022 05:01:09 - INFO - __main__ - Saving model with best Classification-F1: 0.33656507975735583 -> 0.4170724355577797 on epoch=10, global_step=350
05/27/2022 05:01:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.69 on epoch=11
05/27/2022 05:01:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.81 on epoch=11
05/27/2022 05:01:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.87 on epoch=11
05/27/2022 05:01:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.77 on epoch=12
05/27/2022 05:01:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.65 on epoch=12
05/27/2022 05:01:27 - INFO - __main__ - Global step 400 Train loss 0.76 Classification-F1 0.22826416953487763 on epoch=12
05/27/2022 05:01:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.72 on epoch=12
05/27/2022 05:01:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.69 on epoch=13
05/27/2022 05:01:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.74 on epoch=13
05/27/2022 05:01:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.76 on epoch=13
05/27/2022 05:01:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.71 on epoch=14
05/27/2022 05:01:46 - INFO - __main__ - Global step 450 Train loss 0.72 Classification-F1 0.47650434243176176 on epoch=14
05/27/2022 05:01:46 - INFO - __main__ - Saving model with best Classification-F1: 0.4170724355577797 -> 0.47650434243176176 on epoch=14, global_step=450
05/27/2022 05:01:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.66 on epoch=14
05/27/2022 05:01:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.73 on epoch=14
05/27/2022 05:01:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.63 on epoch=14
05/27/2022 05:01:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.65 on epoch=15
05/27/2022 05:01:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.60 on epoch=15
05/27/2022 05:02:04 - INFO - __main__ - Global step 500 Train loss 0.65 Classification-F1 0.6152718774154609 on epoch=15
05/27/2022 05:02:04 - INFO - __main__ - Saving model with best Classification-F1: 0.47650434243176176 -> 0.6152718774154609 on epoch=15, global_step=500
05/27/2022 05:02:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.71 on epoch=15
05/27/2022 05:02:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.52 on epoch=16
05/27/2022 05:02:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=16
05/27/2022 05:02:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.61 on epoch=16
05/27/2022 05:02:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.54 on epoch=17
05/27/2022 05:02:23 - INFO - __main__ - Global step 550 Train loss 0.56 Classification-F1 0.755292939023311 on epoch=17
05/27/2022 05:02:23 - INFO - __main__ - Saving model with best Classification-F1: 0.6152718774154609 -> 0.755292939023311 on epoch=17, global_step=550
05/27/2022 05:02:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=17
05/27/2022 05:02:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=17
05/27/2022 05:02:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=18
05/27/2022 05:02:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.54 on epoch=18
05/27/2022 05:02:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.53 on epoch=18
05/27/2022 05:02:41 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.7352246762916905 on epoch=18
05/27/2022 05:02:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=19
05/27/2022 05:02:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=19
05/27/2022 05:02:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=19
05/27/2022 05:02:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=19
05/27/2022 05:02:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.33 on epoch=20
05/27/2022 05:03:00 - INFO - __main__ - Global step 650 Train loss 0.36 Classification-F1 0.6983163914748349 on epoch=20
05/27/2022 05:03:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=20
05/27/2022 05:03:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=20
05/27/2022 05:03:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=21
05/27/2022 05:03:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=21
05/27/2022 05:03:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.30 on epoch=21
05/27/2022 05:03:19 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.7510092340087164 on epoch=21
05/27/2022 05:03:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=22
05/27/2022 05:03:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=22
05/27/2022 05:03:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=22
05/27/2022 05:03:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=23
05/27/2022 05:03:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=23
05/27/2022 05:03:38 - INFO - __main__ - Global step 750 Train loss 0.35 Classification-F1 0.7554903931105553 on epoch=23
05/27/2022 05:03:38 - INFO - __main__ - Saving model with best Classification-F1: 0.755292939023311 -> 0.7554903931105553 on epoch=23, global_step=750
05/27/2022 05:03:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.35 on epoch=23
05/27/2022 05:03:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.35 on epoch=24
05/27/2022 05:03:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.25 on epoch=24
05/27/2022 05:03:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=24
05/27/2022 05:03:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=24
05/27/2022 05:03:57 - INFO - __main__ - Global step 800 Train loss 0.30 Classification-F1 0.7847609243867726 on epoch=24
05/27/2022 05:03:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7554903931105553 -> 0.7847609243867726 on epoch=24, global_step=800
05/27/2022 05:03:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=25
05/27/2022 05:04:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=25
05/27/2022 05:04:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=25
05/27/2022 05:04:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.32 on epoch=26
05/27/2022 05:04:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.28 on epoch=26
05/27/2022 05:04:16 - INFO - __main__ - Global step 850 Train loss 0.33 Classification-F1 0.7678416329513785 on epoch=26
05/27/2022 05:04:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.26 on epoch=26
05/27/2022 05:04:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=27
05/27/2022 05:04:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.28 on epoch=27
05/27/2022 05:04:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=27
05/27/2022 05:04:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.30 on epoch=28
05/27/2022 05:04:35 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.8082960663481297 on epoch=28
05/27/2022 05:04:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7847609243867726 -> 0.8082960663481297 on epoch=28, global_step=900
05/27/2022 05:04:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.32 on epoch=28
05/27/2022 05:04:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.36 on epoch=28
05/27/2022 05:04:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=29
05/27/2022 05:04:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=29
05/27/2022 05:04:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=29
05/27/2022 05:04:54 - INFO - __main__ - Global step 950 Train loss 0.27 Classification-F1 0.7935776934316355 on epoch=29
05/27/2022 05:04:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=29
05/27/2022 05:04:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=30
05/27/2022 05:05:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=30
05/27/2022 05:05:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=30
05/27/2022 05:05:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.13 on epoch=31
05/27/2022 05:05:13 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.8110481707181508 on epoch=31
05/27/2022 05:05:13 - INFO - __main__ - Saving model with best Classification-F1: 0.8082960663481297 -> 0.8110481707181508 on epoch=31, global_step=1000
05/27/2022 05:05:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.26 on epoch=31
05/27/2022 05:05:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=31
05/27/2022 05:05:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=32
05/27/2022 05:05:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.28 on epoch=32
05/27/2022 05:05:25 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=32
05/27/2022 05:05:32 - INFO - __main__ - Global step 1050 Train loss 0.23 Classification-F1 0.7611440962208247 on epoch=32
05/27/2022 05:05:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=33
05/27/2022 05:05:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=33
05/27/2022 05:05:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=33
05/27/2022 05:05:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=34
05/27/2022 05:05:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.22 on epoch=34
05/27/2022 05:05:51 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.7928230239156658 on epoch=34
05/27/2022 05:05:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.24 on epoch=34
05/27/2022 05:05:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=34
05/27/2022 05:05:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=35
05/27/2022 05:06:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.15 on epoch=35
05/27/2022 05:06:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=35
05/27/2022 05:06:10 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.6131349692846879 on epoch=35
05/27/2022 05:06:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.22 on epoch=36
05/27/2022 05:06:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.15 on epoch=36
05/27/2022 05:06:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=36
05/27/2022 05:06:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.28 on epoch=37
05/27/2022 05:06:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.23 on epoch=37
05/27/2022 05:06:29 - INFO - __main__ - Global step 1200 Train loss 0.22 Classification-F1 0.7765212497397118 on epoch=37
05/27/2022 05:06:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.19 on epoch=37
05/27/2022 05:06:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.29 on epoch=38
05/27/2022 05:06:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.15 on epoch=38
05/27/2022 05:06:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.16 on epoch=38
05/27/2022 05:06:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=39
05/27/2022 05:06:48 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.6524347652347652 on epoch=39
05/27/2022 05:06:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.16 on epoch=39
05/27/2022 05:06:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=39
05/27/2022 05:06:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=39
05/27/2022 05:06:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=40
05/27/2022 05:07:00 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=40
05/27/2022 05:07:07 - INFO - __main__ - Global step 1300 Train loss 0.19 Classification-F1 0.6379214436053573 on epoch=40
05/27/2022 05:07:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=40
05/27/2022 05:07:12 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=41
05/27/2022 05:07:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=41
05/27/2022 05:07:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.17 on epoch=41
05/27/2022 05:07:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=42
05/27/2022 05:07:26 - INFO - __main__ - Global step 1350 Train loss 0.19 Classification-F1 0.6156319039491696 on epoch=42
05/27/2022 05:07:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.15 on epoch=42
05/27/2022 05:07:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=42
05/27/2022 05:07:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=43
05/27/2022 05:07:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=43
05/27/2022 05:07:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.22 on epoch=43
05/27/2022 05:07:44 - INFO - __main__ - Global step 1400 Train loss 0.18 Classification-F1 0.8118092212447051 on epoch=43
05/27/2022 05:07:44 - INFO - __main__ - Saving model with best Classification-F1: 0.8110481707181508 -> 0.8118092212447051 on epoch=43, global_step=1400
05/27/2022 05:07:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.17 on epoch=44
05/27/2022 05:07:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=44
05/27/2022 05:07:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=44
05/27/2022 05:07:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=44
05/27/2022 05:07:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.17 on epoch=45
05/27/2022 05:08:03 - INFO - __main__ - Global step 1450 Train loss 0.18 Classification-F1 0.8158354811143221 on epoch=45
05/27/2022 05:08:03 - INFO - __main__ - Saving model with best Classification-F1: 0.8118092212447051 -> 0.8158354811143221 on epoch=45, global_step=1450
05/27/2022 05:08:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=45
05/27/2022 05:08:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.20 on epoch=45
05/27/2022 05:08:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=46
05/27/2022 05:08:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=46
05/27/2022 05:08:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=46
05/27/2022 05:08:22 - INFO - __main__ - Global step 1500 Train loss 0.17 Classification-F1 0.8237818617949214 on epoch=46
05/27/2022 05:08:22 - INFO - __main__ - Saving model with best Classification-F1: 0.8158354811143221 -> 0.8237818617949214 on epoch=46, global_step=1500
05/27/2022 05:08:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.18 on epoch=47
05/27/2022 05:08:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.10 on epoch=47
05/27/2022 05:08:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=47
05/27/2022 05:08:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=48
05/27/2022 05:08:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.12 on epoch=48
05/27/2022 05:08:40 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.801584048047832 on epoch=48
05/27/2022 05:08:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.12 on epoch=48
05/27/2022 05:08:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=49
05/27/2022 05:08:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=49
05/27/2022 05:08:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.20 on epoch=49
05/27/2022 05:08:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=49
05/27/2022 05:08:59 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.8097766506423144 on epoch=49
05/27/2022 05:09:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=50
05/27/2022 05:09:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.11 on epoch=50
05/27/2022 05:09:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.16 on epoch=50
05/27/2022 05:09:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=51
05/27/2022 05:09:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.10 on epoch=51
05/27/2022 05:09:18 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.7824232937875487 on epoch=51
05/27/2022 05:09:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=51
05/27/2022 05:09:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.18 on epoch=52
05/27/2022 05:09:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.18 on epoch=52
05/27/2022 05:09:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=52
05/27/2022 05:09:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=53
05/27/2022 05:09:37 - INFO - __main__ - Global step 1700 Train loss 0.14 Classification-F1 0.818848843881457 on epoch=53
05/27/2022 05:09:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.13 on epoch=53
05/27/2022 05:09:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=53
05/27/2022 05:09:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=54
05/27/2022 05:09:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.14 on epoch=54
05/27/2022 05:09:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.13 on epoch=54
05/27/2022 05:09:55 - INFO - __main__ - Global step 1750 Train loss 0.15 Classification-F1 0.8270866611107254 on epoch=54
05/27/2022 05:09:55 - INFO - __main__ - Saving model with best Classification-F1: 0.8237818617949214 -> 0.8270866611107254 on epoch=54, global_step=1750
05/27/2022 05:09:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.13 on epoch=54
05/27/2022 05:10:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.12 on epoch=55
05/27/2022 05:10:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=55
05/27/2022 05:10:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=55
05/27/2022 05:10:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=56
05/27/2022 05:10:14 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.80525909086929 on epoch=56
05/27/2022 05:10:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=56
05/27/2022 05:10:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=56
05/27/2022 05:10:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.16 on epoch=57
05/27/2022 05:10:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=57
05/27/2022 05:10:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.09 on epoch=57
05/27/2022 05:10:33 - INFO - __main__ - Global step 1850 Train loss 0.12 Classification-F1 0.7970866692082856 on epoch=57
05/27/2022 05:10:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=58
05/27/2022 05:10:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.09 on epoch=58
05/27/2022 05:10:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=58
05/27/2022 05:10:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.15 on epoch=59
05/27/2022 05:10:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=59
05/27/2022 05:10:52 - INFO - __main__ - Global step 1900 Train loss 0.11 Classification-F1 0.8206994245317275 on epoch=59
05/27/2022 05:10:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=59
05/27/2022 05:10:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=59
05/27/2022 05:10:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=60
05/27/2022 05:11:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=60
05/27/2022 05:11:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=60
05/27/2022 05:11:10 - INFO - __main__ - Global step 1950 Train loss 0.11 Classification-F1 0.802665095501269 on epoch=60
05/27/2022 05:11:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.15 on epoch=61
05/27/2022 05:11:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=61
05/27/2022 05:11:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.15 on epoch=61
05/27/2022 05:11:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=62
05/27/2022 05:11:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.14 on epoch=62
05/27/2022 05:11:29 - INFO - __main__ - Global step 2000 Train loss 0.13 Classification-F1 0.7984016511005692 on epoch=62
05/27/2022 05:11:31 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=62
05/27/2022 05:11:34 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.10 on epoch=63
05/27/2022 05:11:36 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.09 on epoch=63
05/27/2022 05:11:39 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.08 on epoch=63
05/27/2022 05:11:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=64
05/27/2022 05:11:48 - INFO - __main__ - Global step 2050 Train loss 0.09 Classification-F1 0.8000760253439173 on epoch=64
05/27/2022 05:11:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.12 on epoch=64
05/27/2022 05:11:53 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.11 on epoch=64
05/27/2022 05:11:55 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.10 on epoch=64
05/27/2022 05:11:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=65
05/27/2022 05:12:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.06 on epoch=65
05/27/2022 05:12:07 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.8121122152200739 on epoch=65
05/27/2022 05:12:09 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=65
05/27/2022 05:12:12 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=66
05/27/2022 05:12:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=66
05/27/2022 05:12:16 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=66
05/27/2022 05:12:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.12 on epoch=67
05/27/2022 05:12:26 - INFO - __main__ - Global step 2150 Train loss 0.09 Classification-F1 0.7914928359559246 on epoch=67
05/27/2022 05:12:28 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.18 on epoch=67
05/27/2022 05:12:30 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=67
05/27/2022 05:12:33 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.07 on epoch=68
05/27/2022 05:12:35 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=68
05/27/2022 05:12:38 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.09 on epoch=68
05/27/2022 05:12:45 - INFO - __main__ - Global step 2200 Train loss 0.10 Classification-F1 0.8127236398073002 on epoch=68
05/27/2022 05:12:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.18 on epoch=69
05/27/2022 05:12:50 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.10 on epoch=69
05/27/2022 05:12:52 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=69
05/27/2022 05:12:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.10 on epoch=69
05/27/2022 05:12:57 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=70
05/27/2022 05:13:04 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.8105273632617382 on epoch=70
05/27/2022 05:13:06 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=70
05/27/2022 05:13:08 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=70
05/27/2022 05:13:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=71
05/27/2022 05:13:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.10 on epoch=71
05/27/2022 05:13:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.10 on epoch=71
05/27/2022 05:13:22 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.7889360390414718 on epoch=71
05/27/2022 05:13:25 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.11 on epoch=72
05/27/2022 05:13:27 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=72
05/27/2022 05:13:30 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=72
05/27/2022 05:13:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.12 on epoch=73
05/27/2022 05:13:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=73
05/27/2022 05:13:41 - INFO - __main__ - Global step 2350 Train loss 0.09 Classification-F1 0.8187656877173007 on epoch=73
05/27/2022 05:13:43 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=73
05/27/2022 05:13:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=74
05/27/2022 05:13:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=74
05/27/2022 05:13:51 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.12 on epoch=74
05/27/2022 05:13:53 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=74
05/27/2022 05:14:00 - INFO - __main__ - Global step 2400 Train loss 0.07 Classification-F1 0.8239818227142512 on epoch=74
05/27/2022 05:14:02 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=75
05/27/2022 05:14:05 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=75
05/27/2022 05:14:07 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=75
05/27/2022 05:14:09 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.07 on epoch=76
05/27/2022 05:14:12 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=76
05/27/2022 05:14:19 - INFO - __main__ - Global step 2450 Train loss 0.06 Classification-F1 0.818450788636101 on epoch=76
05/27/2022 05:14:21 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=76
05/27/2022 05:14:24 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=77
05/27/2022 05:14:26 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=77
05/27/2022 05:14:28 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.14 on epoch=77
05/27/2022 05:14:31 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.09 on epoch=78
05/27/2022 05:14:38 - INFO - __main__ - Global step 2500 Train loss 0.08 Classification-F1 0.8241847761711616 on epoch=78
05/27/2022 05:14:40 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.06 on epoch=78
05/27/2022 05:14:42 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.08 on epoch=78
05/27/2022 05:14:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.11 on epoch=79
05/27/2022 05:14:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=79
05/27/2022 05:14:49 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=79
05/27/2022 05:14:56 - INFO - __main__ - Global step 2550 Train loss 0.07 Classification-F1 0.8206874220637301 on epoch=79
05/27/2022 05:14:59 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=79
05/27/2022 05:15:01 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=80
05/27/2022 05:15:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=80
05/27/2022 05:15:06 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.10 on epoch=80
05/27/2022 05:15:08 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=81
05/27/2022 05:15:15 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.8216400939040777 on epoch=81
05/27/2022 05:15:17 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=81
05/27/2022 05:15:20 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=81
05/27/2022 05:15:22 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.09 on epoch=82
05/27/2022 05:15:25 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=82
05/27/2022 05:15:27 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.07 on epoch=82
05/27/2022 05:15:34 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.8015120372621456 on epoch=82
05/27/2022 05:15:36 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=83
05/27/2022 05:15:39 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=83
05/27/2022 05:15:41 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.10 on epoch=83
05/27/2022 05:15:43 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=84
05/27/2022 05:15:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=84
05/27/2022 05:15:53 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.8201655039249373 on epoch=84
05/27/2022 05:15:55 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=84
05/27/2022 05:15:57 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=84
05/27/2022 05:16:00 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.10 on epoch=85
05/27/2022 05:16:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=85
05/27/2022 05:16:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=85
05/27/2022 05:16:11 - INFO - __main__ - Global step 2750 Train loss 0.08 Classification-F1 0.8066316631813771 on epoch=85
05/27/2022 05:16:14 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=86
05/27/2022 05:16:16 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=86
05/27/2022 05:16:18 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.07 on epoch=86
05/27/2022 05:16:21 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=87
05/27/2022 05:16:23 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=87
05/27/2022 05:16:30 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.7952705961840759 on epoch=87
05/27/2022 05:16:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=87
05/27/2022 05:16:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=88
05/27/2022 05:16:37 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=88
05/27/2022 05:16:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.07 on epoch=88
05/27/2022 05:16:42 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=89
05/27/2022 05:16:49 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.8140403477710608 on epoch=89
05/27/2022 05:16:52 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=89
05/27/2022 05:16:55 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=89
05/27/2022 05:16:57 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=89
05/27/2022 05:17:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=90
05/27/2022 05:17:02 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=90
05/27/2022 05:17:10 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.8219801638054139 on epoch=90
05/27/2022 05:17:12 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=90
05/27/2022 05:17:15 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=91
05/27/2022 05:17:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=91
05/27/2022 05:17:20 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.07 on epoch=91
05/27/2022 05:17:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=92
05/27/2022 05:17:29 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.825522281793854 on epoch=92
05/27/2022 05:17:32 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=92
05/27/2022 05:17:34 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.12 on epoch=92
05/27/2022 05:17:37 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.08 on epoch=93
05/27/2022 05:17:39 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=93
05/27/2022 05:17:42 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.10 on epoch=93
05/27/2022 05:17:43 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 05:17:43 - INFO - __main__ - Printing 3 examples
05/27/2022 05:17:43 - INFO - __main__ -  [emo] how cause yes am listening
05/27/2022 05:17:43 - INFO - __main__ - ['others']
05/27/2022 05:17:43 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/27/2022 05:17:43 - INFO - __main__ - ['others']
05/27/2022 05:17:43 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/27/2022 05:17:43 - INFO - __main__ - ['others']
05/27/2022 05:17:43 - INFO - __main__ - Tokenizing Input ...
05/27/2022 05:17:43 - INFO - __main__ - Tokenizing Output ...
05/27/2022 05:17:44 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 05:17:44 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 05:17:44 - INFO - __main__ - Printing 3 examples
05/27/2022 05:17:44 - INFO - __main__ -  [emo] when when it comes to you never why
05/27/2022 05:17:44 - INFO - __main__ - ['others']
05/27/2022 05:17:44 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/27/2022 05:17:44 - INFO - __main__ - ['others']
05/27/2022 05:17:44 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/27/2022 05:17:44 - INFO - __main__ - ['others']
05/27/2022 05:17:44 - INFO - __main__ - Tokenizing Input ...
05/27/2022 05:17:44 - INFO - __main__ - Tokenizing Output ...
05/27/2022 05:17:45 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 05:17:49 - INFO - __main__ - Global step 3000 Train loss 0.08 Classification-F1 0.7865779124490548 on epoch=93
05/27/2022 05:17:49 - INFO - __main__ - save last model!
05/27/2022 05:17:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 05:17:49 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 05:17:49 - INFO - __main__ - Printing 3 examples
05/27/2022 05:17:49 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 05:17:49 - INFO - __main__ - ['others']
05/27/2022 05:17:49 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 05:17:49 - INFO - __main__ - ['others']
05/27/2022 05:17:49 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 05:17:49 - INFO - __main__ - ['others']
05/27/2022 05:17:49 - INFO - __main__ - Tokenizing Input ...
05/27/2022 05:17:51 - INFO - __main__ - Tokenizing Output ...
05/27/2022 05:17:57 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 05:18:01 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 05:18:01 - INFO - __main__ - task name: emo
05/27/2022 05:18:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 05:18:02 - INFO - __main__ - Starting training!
05/27/2022 05:19:10 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_100_0.5_8_predictions.txt
05/27/2022 05:19:10 - INFO - __main__ - Classification-F1 on test data: 0.4869
05/27/2022 05:19:10 - INFO - __main__ - prefix=emo_128_100, lr=0.5, bsz=8, dev_performance=0.8270866611107254, test_performance=0.48687883922505
05/27/2022 05:19:11 - INFO - __main__ - Running ... prefix=emo_128_100, lr=0.4, bsz=8 ...
05/27/2022 05:19:11 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 05:19:11 - INFO - __main__ - Printing 3 examples
05/27/2022 05:19:11 - INFO - __main__ -  [emo] how cause yes am listening
05/27/2022 05:19:11 - INFO - __main__ - ['others']
05/27/2022 05:19:11 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/27/2022 05:19:11 - INFO - __main__ - ['others']
05/27/2022 05:19:11 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/27/2022 05:19:11 - INFO - __main__ - ['others']
05/27/2022 05:19:11 - INFO - __main__ - Tokenizing Input ...
05/27/2022 05:19:12 - INFO - __main__ - Tokenizing Output ...
05/27/2022 05:19:12 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 05:19:12 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 05:19:12 - INFO - __main__ - Printing 3 examples
05/27/2022 05:19:12 - INFO - __main__ -  [emo] when when it comes to you never why
05/27/2022 05:19:12 - INFO - __main__ - ['others']
05/27/2022 05:19:12 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/27/2022 05:19:12 - INFO - __main__ - ['others']
05/27/2022 05:19:12 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/27/2022 05:19:12 - INFO - __main__ - ['others']
05/27/2022 05:19:12 - INFO - __main__ - Tokenizing Input ...
05/27/2022 05:19:12 - INFO - __main__ - Tokenizing Output ...
05/27/2022 05:19:13 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 05:19:28 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 05:19:28 - INFO - __main__ - task name: emo
05/27/2022 05:19:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 05:19:29 - INFO - __main__ - Starting training!
05/27/2022 05:19:32 - INFO - __main__ - Step 10 Global step 10 Train loss 6.84 on epoch=0
05/27/2022 05:19:35 - INFO - __main__ - Step 20 Global step 20 Train loss 2.90 on epoch=0
05/27/2022 05:19:37 - INFO - __main__ - Step 30 Global step 30 Train loss 1.41 on epoch=0
05/27/2022 05:19:40 - INFO - __main__ - Step 40 Global step 40 Train loss 1.10 on epoch=1
05/27/2022 05:19:42 - INFO - __main__ - Step 50 Global step 50 Train loss 1.07 on epoch=1
05/27/2022 05:19:49 - INFO - __main__ - Global step 50 Train loss 2.66 Classification-F1 0.10471177198368695 on epoch=1
05/27/2022 05:19:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10471177198368695 on epoch=1, global_step=50
05/27/2022 05:19:52 - INFO - __main__ - Step 60 Global step 60 Train loss 1.14 on epoch=1
05/27/2022 05:19:54 - INFO - __main__ - Step 70 Global step 70 Train loss 0.85 on epoch=2
05/27/2022 05:19:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.97 on epoch=2
05/27/2022 05:19:59 - INFO - __main__ - Step 90 Global step 90 Train loss 0.92 on epoch=2
05/27/2022 05:20:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.90 on epoch=3
05/27/2022 05:20:08 - INFO - __main__ - Global step 100 Train loss 0.95 Classification-F1 0.14423408607757413 on epoch=3
05/27/2022 05:20:08 - INFO - __main__ - Saving model with best Classification-F1: 0.10471177198368695 -> 0.14423408607757413 on epoch=3, global_step=100
05/27/2022 05:20:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.91 on epoch=3
05/27/2022 05:20:13 - INFO - __main__ - Step 120 Global step 120 Train loss 1.01 on epoch=3
05/27/2022 05:20:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.86 on epoch=4
05/27/2022 05:20:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.84 on epoch=4
05/27/2022 05:20:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.92 on epoch=4
05/27/2022 05:20:27 - INFO - __main__ - Global step 150 Train loss 0.91 Classification-F1 0.1561732186732187 on epoch=4
05/27/2022 05:20:27 - INFO - __main__ - Saving model with best Classification-F1: 0.14423408607757413 -> 0.1561732186732187 on epoch=4, global_step=150
05/27/2022 05:20:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.85 on epoch=4
05/27/2022 05:20:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.87 on epoch=5
05/27/2022 05:20:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.81 on epoch=5
05/27/2022 05:20:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.91 on epoch=5
05/27/2022 05:20:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.81 on epoch=6
05/27/2022 05:20:46 - INFO - __main__ - Global step 200 Train loss 0.85 Classification-F1 0.09952978056426331 on epoch=6
05/27/2022 05:20:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.81 on epoch=6
05/27/2022 05:20:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.90 on epoch=6
05/27/2022 05:20:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.78 on epoch=7
05/27/2022 05:20:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.92 on epoch=7
05/27/2022 05:20:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.84 on epoch=7
05/27/2022 05:21:05 - INFO - __main__ - Global step 250 Train loss 0.85 Classification-F1 0.11200107996220131 on epoch=7
05/27/2022 05:21:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.84 on epoch=8
05/27/2022 05:21:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.84 on epoch=8
05/27/2022 05:21:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.85 on epoch=8
05/27/2022 05:21:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.86 on epoch=9
05/27/2022 05:21:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.79 on epoch=9
05/27/2022 05:21:24 - INFO - __main__ - Global step 300 Train loss 0.84 Classification-F1 0.44313955381652237 on epoch=9
05/27/2022 05:21:24 - INFO - __main__ - Saving model with best Classification-F1: 0.1561732186732187 -> 0.44313955381652237 on epoch=9, global_step=300
05/27/2022 05:21:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.79 on epoch=9
05/27/2022 05:21:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.80 on epoch=9
05/27/2022 05:21:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.80 on epoch=10
05/27/2022 05:21:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.78 on epoch=10
05/27/2022 05:21:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.83 on epoch=10
05/27/2022 05:21:42 - INFO - __main__ - Global step 350 Train loss 0.80 Classification-F1 0.40607175764211423 on epoch=10
05/27/2022 05:21:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.80 on epoch=11
05/27/2022 05:21:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.80 on epoch=11
05/27/2022 05:21:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.77 on epoch=11
05/27/2022 05:21:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.79 on epoch=12
05/27/2022 05:21:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.75 on epoch=12
05/27/2022 05:22:01 - INFO - __main__ - Global step 400 Train loss 0.78 Classification-F1 0.17751622840866393 on epoch=12
05/27/2022 05:22:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.67 on epoch=12
05/27/2022 05:22:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.78 on epoch=13
05/27/2022 05:22:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.81 on epoch=13
05/27/2022 05:22:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.73 on epoch=13
05/27/2022 05:22:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.65 on epoch=14
05/27/2022 05:22:20 - INFO - __main__ - Global step 450 Train loss 0.73 Classification-F1 0.2929655733261046 on epoch=14
05/27/2022 05:22:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.71 on epoch=14
05/27/2022 05:22:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.74 on epoch=14
05/27/2022 05:22:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.74 on epoch=14
05/27/2022 05:22:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.65 on epoch=15
05/27/2022 05:22:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.62 on epoch=15
05/27/2022 05:22:38 - INFO - __main__ - Global step 500 Train loss 0.69 Classification-F1 0.303105237071661 on epoch=15
05/27/2022 05:22:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.76 on epoch=15
05/27/2022 05:22:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.59 on epoch=16
05/27/2022 05:22:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.58 on epoch=16
05/27/2022 05:22:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.66 on epoch=16
05/27/2022 05:22:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.67 on epoch=17
05/27/2022 05:22:57 - INFO - __main__ - Global step 550 Train loss 0.65 Classification-F1 0.7042873806790451 on epoch=17
05/27/2022 05:22:57 - INFO - __main__ - Saving model with best Classification-F1: 0.44313955381652237 -> 0.7042873806790451 on epoch=17, global_step=550
05/27/2022 05:22:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.59 on epoch=17
05/27/2022 05:23:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.55 on epoch=17
05/27/2022 05:23:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=18
05/27/2022 05:23:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.64 on epoch=18
05/27/2022 05:23:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.58 on epoch=18
05/27/2022 05:23:16 - INFO - __main__ - Global step 600 Train loss 0.57 Classification-F1 0.7116957605830202 on epoch=18
05/27/2022 05:23:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7042873806790451 -> 0.7116957605830202 on epoch=18, global_step=600
05/27/2022 05:23:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=19
05/27/2022 05:23:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.49 on epoch=19
05/27/2022 05:23:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.58 on epoch=19
05/27/2022 05:23:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=19
05/27/2022 05:23:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.51 on epoch=20
05/27/2022 05:23:34 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.6957555431147291 on epoch=20
05/27/2022 05:23:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.42 on epoch=20
05/27/2022 05:23:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.64 on epoch=20
05/27/2022 05:23:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.52 on epoch=21
05/27/2022 05:23:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=21
05/27/2022 05:23:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.59 on epoch=21
05/27/2022 05:23:53 - INFO - __main__ - Global step 700 Train loss 0.53 Classification-F1 0.7194942825072784 on epoch=21
05/27/2022 05:23:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7116957605830202 -> 0.7194942825072784 on epoch=21, global_step=700
05/27/2022 05:23:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=22
05/27/2022 05:23:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.53 on epoch=22
05/27/2022 05:24:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=22
05/27/2022 05:24:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=23
05/27/2022 05:24:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=23
05/27/2022 05:24:12 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.7432527841642496 on epoch=23
05/27/2022 05:24:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7194942825072784 -> 0.7432527841642496 on epoch=23, global_step=750
05/27/2022 05:24:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=23
05/27/2022 05:24:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.28 on epoch=24
05/27/2022 05:24:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=24
05/27/2022 05:24:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=24
05/27/2022 05:24:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=24
05/27/2022 05:24:31 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.7615793036263734 on epoch=24
05/27/2022 05:24:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7432527841642496 -> 0.7615793036263734 on epoch=24, global_step=800
05/27/2022 05:24:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.35 on epoch=25
05/27/2022 05:24:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=25
05/27/2022 05:24:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.51 on epoch=25
05/27/2022 05:24:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=26
05/27/2022 05:24:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=26
05/27/2022 05:24:49 - INFO - __main__ - Global step 850 Train loss 0.41 Classification-F1 0.7096551747916156 on epoch=26
05/27/2022 05:24:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=26
05/27/2022 05:24:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=27
05/27/2022 05:24:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=27
05/27/2022 05:24:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=27
05/27/2022 05:25:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.34 on epoch=28
05/27/2022 05:25:08 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.7234585776551523 on epoch=28
05/27/2022 05:25:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.29 on epoch=28
05/27/2022 05:25:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=28
05/27/2022 05:25:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.31 on epoch=29
05/27/2022 05:25:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=29
05/27/2022 05:25:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.47 on epoch=29
05/27/2022 05:25:27 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.7214001521595073 on epoch=29
05/27/2022 05:25:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.32 on epoch=29
05/27/2022 05:25:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.32 on epoch=30
05/27/2022 05:25:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.28 on epoch=30
05/27/2022 05:25:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=30
05/27/2022 05:25:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=31
05/27/2022 05:25:45 - INFO - __main__ - Global step 1000 Train loss 0.31 Classification-F1 0.6499092772589645 on epoch=31
05/27/2022 05:25:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=31
05/27/2022 05:25:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.32 on epoch=31
05/27/2022 05:25:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=32
05/27/2022 05:25:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.24 on epoch=32
05/27/2022 05:25:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.26 on epoch=32
05/27/2022 05:26:04 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.6415282127935247 on epoch=32
05/27/2022 05:26:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=33
05/27/2022 05:26:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=33
05/27/2022 05:26:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=33
05/27/2022 05:26:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.30 on epoch=34
05/27/2022 05:26:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=34
05/27/2022 05:26:23 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.778685291093545 on epoch=34
05/27/2022 05:26:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7615793036263734 -> 0.778685291093545 on epoch=34, global_step=1100
05/27/2022 05:26:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=34
05/27/2022 05:26:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=34
05/27/2022 05:26:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.34 on epoch=35
05/27/2022 05:26:32 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.27 on epoch=35
05/27/2022 05:26:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.31 on epoch=35
05/27/2022 05:26:41 - INFO - __main__ - Global step 1150 Train loss 0.29 Classification-F1 0.7715328997474646 on epoch=35
05/27/2022 05:26:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=36
05/27/2022 05:26:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=36
05/27/2022 05:26:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=36
05/27/2022 05:26:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.24 on epoch=37
05/27/2022 05:26:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.27 on epoch=37
05/27/2022 05:27:00 - INFO - __main__ - Global step 1200 Train loss 0.23 Classification-F1 0.7347738735483116 on epoch=37
05/27/2022 05:27:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.25 on epoch=37
05/27/2022 05:27:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.32 on epoch=38
05/27/2022 05:27:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.28 on epoch=38
05/27/2022 05:27:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.26 on epoch=38
05/27/2022 05:27:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=39
05/27/2022 05:27:19 - INFO - __main__ - Global step 1250 Train loss 0.27 Classification-F1 0.8159222745668123 on epoch=39
05/27/2022 05:27:19 - INFO - __main__ - Saving model with best Classification-F1: 0.778685291093545 -> 0.8159222745668123 on epoch=39, global_step=1250
05/27/2022 05:27:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.31 on epoch=39
05/27/2022 05:27:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.27 on epoch=39
05/27/2022 05:27:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=39
05/27/2022 05:27:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=40
05/27/2022 05:27:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=40
05/27/2022 05:27:37 - INFO - __main__ - Global step 1300 Train loss 0.26 Classification-F1 0.8116284218303021 on epoch=40
05/27/2022 05:27:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=40
05/27/2022 05:27:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=41
05/27/2022 05:27:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=41
05/27/2022 05:27:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.32 on epoch=41
05/27/2022 05:27:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=42
05/27/2022 05:27:56 - INFO - __main__ - Global step 1350 Train loss 0.24 Classification-F1 0.8126738043556934 on epoch=42
05/27/2022 05:27:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.27 on epoch=42
05/27/2022 05:28:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=42
05/27/2022 05:28:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=43
05/27/2022 05:28:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.26 on epoch=43
05/27/2022 05:28:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.28 on epoch=43
05/27/2022 05:28:15 - INFO - __main__ - Global step 1400 Train loss 0.27 Classification-F1 0.8009102646681794 on epoch=43
05/27/2022 05:28:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.23 on epoch=44
05/27/2022 05:28:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.23 on epoch=44
05/27/2022 05:28:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.24 on epoch=44
05/27/2022 05:28:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.20 on epoch=44
05/27/2022 05:28:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=45
05/27/2022 05:28:33 - INFO - __main__ - Global step 1450 Train loss 0.22 Classification-F1 0.7969090984150083 on epoch=45
05/27/2022 05:28:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=45
05/27/2022 05:28:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.26 on epoch=45
05/27/2022 05:28:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.24 on epoch=46
05/27/2022 05:28:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.22 on epoch=46
05/27/2022 05:28:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=46
05/27/2022 05:28:52 - INFO - __main__ - Global step 1500 Train loss 0.24 Classification-F1 0.7995561863023294 on epoch=46
05/27/2022 05:28:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=47
05/27/2022 05:28:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=47
05/27/2022 05:28:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=47
05/27/2022 05:29:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=48
05/27/2022 05:29:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=48
05/27/2022 05:29:11 - INFO - __main__ - Global step 1550 Train loss 0.23 Classification-F1 0.807571373007953 on epoch=48
05/27/2022 05:29:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=48
05/27/2022 05:29:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=49
05/27/2022 05:29:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=49
05/27/2022 05:29:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.16 on epoch=49
05/27/2022 05:29:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=49
05/27/2022 05:29:30 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.776762628581274 on epoch=49
05/27/2022 05:29:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.28 on epoch=50
05/27/2022 05:29:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=50
05/27/2022 05:29:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=50
05/27/2022 05:29:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=51
05/27/2022 05:29:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=51
05/27/2022 05:29:48 - INFO - __main__ - Global step 1650 Train loss 0.22 Classification-F1 0.8098159319436342 on epoch=51
05/27/2022 05:29:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.16 on epoch=51
05/27/2022 05:29:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.23 on epoch=52
05/27/2022 05:29:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=52
05/27/2022 05:29:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.25 on epoch=52
05/27/2022 05:30:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.17 on epoch=53
05/27/2022 05:30:07 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.793478683687809 on epoch=53
05/27/2022 05:30:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.16 on epoch=53
05/27/2022 05:30:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.21 on epoch=53
05/27/2022 05:30:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.27 on epoch=54
05/27/2022 05:30:17 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=54
05/27/2022 05:30:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=54
05/27/2022 05:30:26 - INFO - __main__ - Global step 1750 Train loss 0.20 Classification-F1 0.8029983632932853 on epoch=54
05/27/2022 05:30:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.16 on epoch=54
05/27/2022 05:30:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.22 on epoch=55
05/27/2022 05:30:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=55
05/27/2022 05:30:35 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=55
05/27/2022 05:30:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.17 on epoch=56
05/27/2022 05:30:45 - INFO - __main__ - Global step 1800 Train loss 0.17 Classification-F1 0.7884802382432897 on epoch=56
05/27/2022 05:30:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=56
05/27/2022 05:30:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.22 on epoch=56
05/27/2022 05:30:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.24 on epoch=57
05/27/2022 05:30:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=57
05/27/2022 05:30:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.17 on epoch=57
05/27/2022 05:31:03 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.7896669186536651 on epoch=57
05/27/2022 05:31:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.19 on epoch=58
05/27/2022 05:31:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.17 on epoch=58
05/27/2022 05:31:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=58
05/27/2022 05:31:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.22 on epoch=59
05/27/2022 05:31:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=59
05/27/2022 05:31:22 - INFO - __main__ - Global step 1900 Train loss 0.19 Classification-F1 0.8137804598343988 on epoch=59
05/27/2022 05:31:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=59
05/27/2022 05:31:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.18 on epoch=59
05/27/2022 05:31:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.26 on epoch=60
05/27/2022 05:31:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=60
05/27/2022 05:31:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=60
05/27/2022 05:31:41 - INFO - __main__ - Global step 1950 Train loss 0.17 Classification-F1 0.8198353488435993 on epoch=60
05/27/2022 05:31:41 - INFO - __main__ - Saving model with best Classification-F1: 0.8159222745668123 -> 0.8198353488435993 on epoch=60, global_step=1950
05/27/2022 05:31:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.14 on epoch=61
05/27/2022 05:31:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=61
05/27/2022 05:31:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.14 on epoch=61
05/27/2022 05:31:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=62
05/27/2022 05:31:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.13 on epoch=62
05/27/2022 05:32:00 - INFO - __main__ - Global step 2000 Train loss 0.12 Classification-F1 0.8094874377483072 on epoch=62
05/27/2022 05:32:02 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.18 on epoch=62
05/27/2022 05:32:04 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.16 on epoch=63
05/27/2022 05:32:07 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.13 on epoch=63
05/27/2022 05:32:09 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.18 on epoch=63
05/27/2022 05:32:12 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.19 on epoch=64
05/27/2022 05:32:18 - INFO - __main__ - Global step 2050 Train loss 0.17 Classification-F1 0.7861204823343219 on epoch=64
05/27/2022 05:32:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.17 on epoch=64
05/27/2022 05:32:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.20 on epoch=64
05/27/2022 05:32:25 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.12 on epoch=64
05/27/2022 05:32:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.23 on epoch=65
05/27/2022 05:32:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.09 on epoch=65
05/27/2022 05:32:37 - INFO - __main__ - Global step 2100 Train loss 0.16 Classification-F1 0.8029258216073881 on epoch=65
05/27/2022 05:32:39 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.18 on epoch=65
05/27/2022 05:32:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=66
05/27/2022 05:32:44 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.16 on epoch=66
05/27/2022 05:32:46 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=66
05/27/2022 05:32:49 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.11 on epoch=67
05/27/2022 05:32:56 - INFO - __main__ - Global step 2150 Train loss 0.14 Classification-F1 0.8129810526451374 on epoch=67
05/27/2022 05:32:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.18 on epoch=67
05/27/2022 05:33:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.16 on epoch=67
05/27/2022 05:33:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=68
05/27/2022 05:33:05 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.15 on epoch=68
05/27/2022 05:33:07 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.13 on epoch=68
05/27/2022 05:33:14 - INFO - __main__ - Global step 2200 Train loss 0.16 Classification-F1 0.8141452079039632 on epoch=68
05/27/2022 05:33:17 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.10 on epoch=69
05/27/2022 05:33:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.15 on epoch=69
05/27/2022 05:33:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.14 on epoch=69
05/27/2022 05:33:24 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.11 on epoch=69
05/27/2022 05:33:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.09 on epoch=70
05/27/2022 05:33:33 - INFO - __main__ - Global step 2250 Train loss 0.12 Classification-F1 0.820347512779771 on epoch=70
05/27/2022 05:33:33 - INFO - __main__ - Saving model with best Classification-F1: 0.8198353488435993 -> 0.820347512779771 on epoch=70, global_step=2250
05/27/2022 05:33:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.12 on epoch=70
05/27/2022 05:33:38 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=70
05/27/2022 05:33:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.14 on epoch=71
05/27/2022 05:33:43 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.12 on epoch=71
05/27/2022 05:33:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.10 on epoch=71
05/27/2022 05:33:52 - INFO - __main__ - Global step 2300 Train loss 0.11 Classification-F1 0.7807355501202085 on epoch=71
05/27/2022 05:33:54 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.13 on epoch=72
05/27/2022 05:33:57 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.14 on epoch=72
05/27/2022 05:33:59 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.10 on epoch=72
05/27/2022 05:34:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.10 on epoch=73
05/27/2022 05:34:04 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.12 on epoch=73
05/27/2022 05:34:11 - INFO - __main__ - Global step 2350 Train loss 0.12 Classification-F1 0.8245673425703639 on epoch=73
05/27/2022 05:34:11 - INFO - __main__ - Saving model with best Classification-F1: 0.820347512779771 -> 0.8245673425703639 on epoch=73, global_step=2350
05/27/2022 05:34:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=73
05/27/2022 05:34:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.09 on epoch=74
05/27/2022 05:34:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.12 on epoch=74
05/27/2022 05:34:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.10 on epoch=74
05/27/2022 05:34:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.10 on epoch=74
05/27/2022 05:34:30 - INFO - __main__ - Global step 2400 Train loss 0.10 Classification-F1 0.8160616091604334 on epoch=74
05/27/2022 05:34:33 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.08 on epoch=75
05/27/2022 05:34:35 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.19 on epoch=75
05/27/2022 05:34:38 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.09 on epoch=75
05/27/2022 05:34:40 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.11 on epoch=76
05/27/2022 05:34:43 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=76
05/27/2022 05:34:50 - INFO - __main__ - Global step 2450 Train loss 0.11 Classification-F1 0.8113467261904761 on epoch=76
05/27/2022 05:34:52 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.10 on epoch=76
05/27/2022 05:34:55 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.19 on epoch=77
05/27/2022 05:34:57 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=77
05/27/2022 05:35:00 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=77
05/27/2022 05:35:03 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.09 on epoch=78
05/27/2022 05:35:09 - INFO - __main__ - Global step 2500 Train loss 0.10 Classification-F1 0.8325607804608723 on epoch=78
05/27/2022 05:35:09 - INFO - __main__ - Saving model with best Classification-F1: 0.8245673425703639 -> 0.8325607804608723 on epoch=78, global_step=2500
05/27/2022 05:35:12 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.09 on epoch=78
05/27/2022 05:35:15 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=78
05/27/2022 05:35:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.18 on epoch=79
05/27/2022 05:35:20 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.19 on epoch=79
05/27/2022 05:35:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.13 on epoch=79
05/27/2022 05:35:29 - INFO - __main__ - Global step 2550 Train loss 0.13 Classification-F1 0.830619385466741 on epoch=79
05/27/2022 05:35:32 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=79
05/27/2022 05:35:34 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=80
05/27/2022 05:35:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.09 on epoch=80
05/27/2022 05:35:39 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.19 on epoch=80
05/27/2022 05:35:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=81
05/27/2022 05:35:48 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.8229278213692038 on epoch=81
05/27/2022 05:35:51 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.14 on epoch=81
05/27/2022 05:35:54 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=81
05/27/2022 05:35:56 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.17 on epoch=82
05/27/2022 05:35:59 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=82
05/27/2022 05:36:01 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.13 on epoch=82
05/27/2022 05:36:08 - INFO - __main__ - Global step 2650 Train loss 0.12 Classification-F1 0.8106193918005276 on epoch=82
05/27/2022 05:36:10 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=83
05/27/2022 05:36:13 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.15 on epoch=83
05/27/2022 05:36:16 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=83
05/27/2022 05:36:18 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.10 on epoch=84
05/27/2022 05:36:21 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.11 on epoch=84
05/27/2022 05:36:28 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.8258616208879136 on epoch=84
05/27/2022 05:36:30 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.10 on epoch=84
05/27/2022 05:36:33 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=84
05/27/2022 05:36:35 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.12 on epoch=85
05/27/2022 05:36:38 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.09 on epoch=85
05/27/2022 05:36:40 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=85
05/27/2022 05:36:47 - INFO - __main__ - Global step 2750 Train loss 0.09 Classification-F1 0.8119807633624112 on epoch=85
05/27/2022 05:36:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=86
05/27/2022 05:36:52 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.08 on epoch=86
05/27/2022 05:36:55 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=86
05/27/2022 05:36:57 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.15 on epoch=87
05/27/2022 05:37:00 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.16 on epoch=87
05/27/2022 05:37:07 - INFO - __main__ - Global step 2800 Train loss 0.12 Classification-F1 0.8159608740398613 on epoch=87
05/27/2022 05:37:09 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=87
05/27/2022 05:37:12 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=88
05/27/2022 05:37:14 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=88
05/27/2022 05:37:17 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.08 on epoch=88
05/27/2022 05:37:19 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=89
05/27/2022 05:37:26 - INFO - __main__ - Global step 2850 Train loss 0.07 Classification-F1 0.7926751170224047 on epoch=89
05/27/2022 05:37:29 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.07 on epoch=89
05/27/2022 05:37:31 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=89
05/27/2022 05:37:34 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=89
05/27/2022 05:37:36 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=90
05/27/2022 05:37:39 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=90
05/27/2022 05:37:46 - INFO - __main__ - Global step 2900 Train loss 0.06 Classification-F1 0.8059781036717638 on epoch=90
05/27/2022 05:37:48 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.15 on epoch=90
05/27/2022 05:37:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=91
05/27/2022 05:37:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=91
05/27/2022 05:37:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=91
05/27/2022 05:37:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=92
05/27/2022 05:38:05 - INFO - __main__ - Global step 2950 Train loss 0.08 Classification-F1 0.7956646281439487 on epoch=92
05/27/2022 05:38:08 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.11 on epoch=92
05/27/2022 05:38:10 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.09 on epoch=92
05/27/2022 05:38:13 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.09 on epoch=93
05/27/2022 05:38:15 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.15 on epoch=93
05/27/2022 05:38:18 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=93
05/27/2022 05:38:19 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 05:38:19 - INFO - __main__ - Printing 3 examples
05/27/2022 05:38:19 - INFO - __main__ -  [emo] how cause yes am listening
05/27/2022 05:38:19 - INFO - __main__ - ['others']
05/27/2022 05:38:19 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/27/2022 05:38:19 - INFO - __main__ - ['others']
05/27/2022 05:38:19 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/27/2022 05:38:19 - INFO - __main__ - ['others']
05/27/2022 05:38:19 - INFO - __main__ - Tokenizing Input ...
05/27/2022 05:38:19 - INFO - __main__ - Tokenizing Output ...
05/27/2022 05:38:20 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 05:38:20 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 05:38:20 - INFO - __main__ - Printing 3 examples
05/27/2022 05:38:20 - INFO - __main__ -  [emo] when when it comes to you never why
05/27/2022 05:38:20 - INFO - __main__ - ['others']
05/27/2022 05:38:20 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/27/2022 05:38:20 - INFO - __main__ - ['others']
05/27/2022 05:38:20 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/27/2022 05:38:20 - INFO - __main__ - ['others']
05/27/2022 05:38:20 - INFO - __main__ - Tokenizing Input ...
05/27/2022 05:38:20 - INFO - __main__ - Tokenizing Output ...
05/27/2022 05:38:21 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 05:38:25 - INFO - __main__ - Global step 3000 Train loss 0.10 Classification-F1 0.8323809190988037 on epoch=93
05/27/2022 05:38:25 - INFO - __main__ - save last model!
05/27/2022 05:38:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 05:38:25 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 05:38:25 - INFO - __main__ - Printing 3 examples
05/27/2022 05:38:25 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 05:38:25 - INFO - __main__ - ['others']
05/27/2022 05:38:25 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 05:38:25 - INFO - __main__ - ['others']
05/27/2022 05:38:25 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 05:38:25 - INFO - __main__ - ['others']
05/27/2022 05:38:25 - INFO - __main__ - Tokenizing Input ...
05/27/2022 05:38:27 - INFO - __main__ - Tokenizing Output ...
05/27/2022 05:38:32 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 05:38:39 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 05:38:39 - INFO - __main__ - task name: emo
05/27/2022 05:38:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 05:38:40 - INFO - __main__ - Starting training!
05/27/2022 05:39:44 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_100_0.4_8_predictions.txt
05/27/2022 05:39:44 - INFO - __main__ - Classification-F1 on test data: 0.5336
05/27/2022 05:39:45 - INFO - __main__ - prefix=emo_128_100, lr=0.4, bsz=8, dev_performance=0.8325607804608723, test_performance=0.5335784364578255
05/27/2022 05:39:45 - INFO - __main__ - Running ... prefix=emo_128_100, lr=0.3, bsz=8 ...
05/27/2022 05:39:46 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 05:39:46 - INFO - __main__ - Printing 3 examples
05/27/2022 05:39:46 - INFO - __main__ -  [emo] how cause yes am listening
05/27/2022 05:39:46 - INFO - __main__ - ['others']
05/27/2022 05:39:46 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/27/2022 05:39:46 - INFO - __main__ - ['others']
05/27/2022 05:39:46 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/27/2022 05:39:46 - INFO - __main__ - ['others']
05/27/2022 05:39:46 - INFO - __main__ - Tokenizing Input ...
05/27/2022 05:39:46 - INFO - __main__ - Tokenizing Output ...
05/27/2022 05:39:47 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 05:39:47 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 05:39:47 - INFO - __main__ - Printing 3 examples
05/27/2022 05:39:47 - INFO - __main__ -  [emo] when when it comes to you never why
05/27/2022 05:39:47 - INFO - __main__ - ['others']
05/27/2022 05:39:47 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/27/2022 05:39:47 - INFO - __main__ - ['others']
05/27/2022 05:39:47 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/27/2022 05:39:47 - INFO - __main__ - ['others']
05/27/2022 05:39:47 - INFO - __main__ - Tokenizing Input ...
05/27/2022 05:39:47 - INFO - __main__ - Tokenizing Output ...
05/27/2022 05:39:47 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 05:40:06 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 05:40:06 - INFO - __main__ - task name: emo
05/27/2022 05:40:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 05:40:07 - INFO - __main__ - Starting training!
05/27/2022 05:40:10 - INFO - __main__ - Step 10 Global step 10 Train loss 7.28 on epoch=0
05/27/2022 05:40:12 - INFO - __main__ - Step 20 Global step 20 Train loss 4.28 on epoch=0
05/27/2022 05:40:15 - INFO - __main__ - Step 30 Global step 30 Train loss 2.51 on epoch=0
05/27/2022 05:40:17 - INFO - __main__ - Step 40 Global step 40 Train loss 1.58 on epoch=1
05/27/2022 05:40:20 - INFO - __main__ - Step 50 Global step 50 Train loss 1.29 on epoch=1
05/27/2022 05:40:27 - INFO - __main__ - Global step 50 Train loss 3.39 Classification-F1 0.12365029606408917 on epoch=1
05/27/2022 05:40:27 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.12365029606408917 on epoch=1, global_step=50
05/27/2022 05:40:30 - INFO - __main__ - Step 60 Global step 60 Train loss 1.31 on epoch=1
05/27/2022 05:40:32 - INFO - __main__ - Step 70 Global step 70 Train loss 1.08 on epoch=2
05/27/2022 05:40:35 - INFO - __main__ - Step 80 Global step 80 Train loss 1.09 on epoch=2
05/27/2022 05:40:37 - INFO - __main__ - Step 90 Global step 90 Train loss 1.00 on epoch=2
05/27/2022 05:40:40 - INFO - __main__ - Step 100 Global step 100 Train loss 0.90 on epoch=3
05/27/2022 05:40:47 - INFO - __main__ - Global step 100 Train loss 1.08 Classification-F1 0.1 on epoch=3
05/27/2022 05:40:50 - INFO - __main__ - Step 110 Global step 110 Train loss 1.17 on epoch=3
05/27/2022 05:40:52 - INFO - __main__ - Step 120 Global step 120 Train loss 1.08 on epoch=3
05/27/2022 05:40:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.98 on epoch=4
05/27/2022 05:40:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.87 on epoch=4
05/27/2022 05:41:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.86 on epoch=4
05/27/2022 05:41:07 - INFO - __main__ - Global step 150 Train loss 0.99 Classification-F1 0.1463531700471999 on epoch=4
05/27/2022 05:41:07 - INFO - __main__ - Saving model with best Classification-F1: 0.12365029606408917 -> 0.1463531700471999 on epoch=4, global_step=150
05/27/2022 05:41:09 - INFO - __main__ - Step 160 Global step 160 Train loss 0.89 on epoch=4
05/27/2022 05:41:12 - INFO - __main__ - Step 170 Global step 170 Train loss 1.00 on epoch=5
05/27/2022 05:41:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.90 on epoch=5
05/27/2022 05:41:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.98 on epoch=5
05/27/2022 05:41:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.91 on epoch=6
05/27/2022 05:41:26 - INFO - __main__ - Global step 200 Train loss 0.94 Classification-F1 0.12022640179711931 on epoch=6
05/27/2022 05:41:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.94 on epoch=6
05/27/2022 05:41:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.99 on epoch=6
05/27/2022 05:41:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.93 on epoch=7
05/27/2022 05:41:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.86 on epoch=7
05/27/2022 05:41:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.87 on epoch=7
05/27/2022 05:41:46 - INFO - __main__ - Global step 250 Train loss 0.92 Classification-F1 0.16669226044226043 on epoch=7
05/27/2022 05:41:46 - INFO - __main__ - Saving model with best Classification-F1: 0.1463531700471999 -> 0.16669226044226043 on epoch=7, global_step=250
05/27/2022 05:41:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=8
05/27/2022 05:41:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.83 on epoch=8
05/27/2022 05:41:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.91 on epoch=8
05/27/2022 05:41:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.83 on epoch=9
05/27/2022 05:41:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.81 on epoch=9
05/27/2022 05:42:06 - INFO - __main__ - Global step 300 Train loss 0.85 Classification-F1 0.27293782167199887 on epoch=9
05/27/2022 05:42:06 - INFO - __main__ - Saving model with best Classification-F1: 0.16669226044226043 -> 0.27293782167199887 on epoch=9, global_step=300
05/27/2022 05:42:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.90 on epoch=9
05/27/2022 05:42:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.81 on epoch=9
05/27/2022 05:42:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.94 on epoch=10
05/27/2022 05:42:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.85 on epoch=10
05/27/2022 05:42:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.92 on epoch=10
05/27/2022 05:42:26 - INFO - __main__ - Global step 350 Train loss 0.88 Classification-F1 0.3622828219185674 on epoch=10
05/27/2022 05:42:26 - INFO - __main__ - Saving model with best Classification-F1: 0.27293782167199887 -> 0.3622828219185674 on epoch=10, global_step=350
05/27/2022 05:42:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.84 on epoch=11
05/27/2022 05:42:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.83 on epoch=11
05/27/2022 05:42:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.82 on epoch=11
05/27/2022 05:42:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.82 on epoch=12
05/27/2022 05:42:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.76 on epoch=12
05/27/2022 05:42:46 - INFO - __main__ - Global step 400 Train loss 0.82 Classification-F1 0.1842916342916343 on epoch=12
05/27/2022 05:42:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.85 on epoch=12
05/27/2022 05:42:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.87 on epoch=13
05/27/2022 05:42:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.91 on epoch=13
05/27/2022 05:42:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.77 on epoch=13
05/27/2022 05:42:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.86 on epoch=14
05/27/2022 05:43:06 - INFO - __main__ - Global step 450 Train loss 0.85 Classification-F1 0.31351694032457716 on epoch=14
05/27/2022 05:43:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.72 on epoch=14
05/27/2022 05:43:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.80 on epoch=14
05/27/2022 05:43:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.80 on epoch=14
05/27/2022 05:43:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.82 on epoch=15
05/27/2022 05:43:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.81 on epoch=15
05/27/2022 05:43:25 - INFO - __main__ - Global step 500 Train loss 0.79 Classification-F1 0.2515985684806891 on epoch=15
05/27/2022 05:43:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.85 on epoch=15
05/27/2022 05:43:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.78 on epoch=16
05/27/2022 05:43:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.74 on epoch=16
05/27/2022 05:43:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.79 on epoch=16
05/27/2022 05:43:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.77 on epoch=17
05/27/2022 05:43:45 - INFO - __main__ - Global step 550 Train loss 0.78 Classification-F1 0.4781407004955686 on epoch=17
05/27/2022 05:43:45 - INFO - __main__ - Saving model with best Classification-F1: 0.3622828219185674 -> 0.4781407004955686 on epoch=17, global_step=550
05/27/2022 05:43:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.74 on epoch=17
05/27/2022 05:43:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.75 on epoch=17
05/27/2022 05:43:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.84 on epoch=18
05/27/2022 05:43:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.79 on epoch=18
05/27/2022 05:43:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.87 on epoch=18
05/27/2022 05:44:05 - INFO - __main__ - Global step 600 Train loss 0.80 Classification-F1 0.5101828793249151 on epoch=18
05/27/2022 05:44:05 - INFO - __main__ - Saving model with best Classification-F1: 0.4781407004955686 -> 0.5101828793249151 on epoch=18, global_step=600
05/27/2022 05:44:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.69 on epoch=19
05/27/2022 05:44:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.75 on epoch=19
05/27/2022 05:44:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.73 on epoch=19
05/27/2022 05:44:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.65 on epoch=19
05/27/2022 05:44:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.64 on epoch=20
05/27/2022 05:44:25 - INFO - __main__ - Global step 650 Train loss 0.69 Classification-F1 0.4886500260968346 on epoch=20
05/27/2022 05:44:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.65 on epoch=20
05/27/2022 05:44:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.74 on epoch=20
05/27/2022 05:44:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.64 on epoch=21
05/27/2022 05:44:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.57 on epoch=21
05/27/2022 05:44:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.71 on epoch=21
05/27/2022 05:44:45 - INFO - __main__ - Global step 700 Train loss 0.66 Classification-F1 0.6551288501074635 on epoch=21
05/27/2022 05:44:45 - INFO - __main__ - Saving model with best Classification-F1: 0.5101828793249151 -> 0.6551288501074635 on epoch=21, global_step=700
05/27/2022 05:44:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.67 on epoch=22
05/27/2022 05:44:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.68 on epoch=22
05/27/2022 05:44:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.70 on epoch=22
05/27/2022 05:44:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.50 on epoch=23
05/27/2022 05:44:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.60 on epoch=23
05/27/2022 05:45:05 - INFO - __main__ - Global step 750 Train loss 0.63 Classification-F1 0.6055778596262021 on epoch=23
05/27/2022 05:45:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.62 on epoch=23
05/27/2022 05:45:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.61 on epoch=24
05/27/2022 05:45:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.53 on epoch=24
05/27/2022 05:45:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.65 on epoch=24
05/27/2022 05:45:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.58 on epoch=24
05/27/2022 05:45:25 - INFO - __main__ - Global step 800 Train loss 0.60 Classification-F1 0.6814454441915966 on epoch=24
05/27/2022 05:45:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6551288501074635 -> 0.6814454441915966 on epoch=24, global_step=800
05/27/2022 05:45:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.52 on epoch=25
05/27/2022 05:45:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.64 on epoch=25
05/27/2022 05:45:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.59 on epoch=25
05/27/2022 05:45:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.52 on epoch=26
05/27/2022 05:45:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.57 on epoch=26
05/27/2022 05:45:45 - INFO - __main__ - Global step 850 Train loss 0.57 Classification-F1 0.656257786746294 on epoch=26
05/27/2022 05:45:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.60 on epoch=26
05/27/2022 05:45:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.48 on epoch=27
05/27/2022 05:45:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.59 on epoch=27
05/27/2022 05:45:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.54 on epoch=27
05/27/2022 05:45:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.57 on epoch=28
05/27/2022 05:46:05 - INFO - __main__ - Global step 900 Train loss 0.55 Classification-F1 0.6600010640670175 on epoch=28
05/27/2022 05:46:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=28
05/27/2022 05:46:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.64 on epoch=28
05/27/2022 05:46:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.53 on epoch=29
05/27/2022 05:46:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.54 on epoch=29
05/27/2022 05:46:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.58 on epoch=29
05/27/2022 05:46:25 - INFO - __main__ - Global step 950 Train loss 0.55 Classification-F1 0.7267682531482582 on epoch=29
05/27/2022 05:46:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6814454441915966 -> 0.7267682531482582 on epoch=29, global_step=950
05/27/2022 05:46:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.48 on epoch=29
05/27/2022 05:46:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=30
05/27/2022 05:46:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.53 on epoch=30
05/27/2022 05:46:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.59 on epoch=30
05/27/2022 05:46:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=31
05/27/2022 05:46:45 - INFO - __main__ - Global step 1000 Train loss 0.51 Classification-F1 0.6860853854004945 on epoch=31
05/27/2022 05:46:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.56 on epoch=31
05/27/2022 05:46:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.60 on epoch=31
05/27/2022 05:46:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=32
05/27/2022 05:46:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.56 on epoch=32
05/27/2022 05:46:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.56 on epoch=32
05/27/2022 05:47:05 - INFO - __main__ - Global step 1050 Train loss 0.53 Classification-F1 0.6332573043960005 on epoch=32
05/27/2022 05:47:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.48 on epoch=33
05/27/2022 05:47:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.52 on epoch=33
05/27/2022 05:47:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.49 on epoch=33
05/27/2022 05:47:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=34
05/27/2022 05:47:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=34
05/27/2022 05:47:24 - INFO - __main__ - Global step 1100 Train loss 0.46 Classification-F1 0.6835485810695735 on epoch=34
05/27/2022 05:47:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.47 on epoch=34
05/27/2022 05:47:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=34
05/27/2022 05:47:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=35
05/27/2022 05:47:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=35
05/27/2022 05:47:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.50 on epoch=35
05/27/2022 05:47:44 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.7383263974994074 on epoch=35
05/27/2022 05:47:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7267682531482582 -> 0.7383263974994074 on epoch=35, global_step=1150
05/27/2022 05:47:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=36
05/27/2022 05:47:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.47 on epoch=36
05/27/2022 05:47:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=36
05/27/2022 05:47:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=37
05/27/2022 05:47:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=37
05/27/2022 05:48:04 - INFO - __main__ - Global step 1200 Train loss 0.43 Classification-F1 0.7176847408707834 on epoch=37
05/27/2022 05:48:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=37
05/27/2022 05:48:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=38
05/27/2022 05:48:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=38
05/27/2022 05:48:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=38
05/27/2022 05:48:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.37 on epoch=39
05/27/2022 05:48:24 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.6789908583306777 on epoch=39
05/27/2022 05:48:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=39
05/27/2022 05:48:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=39
05/27/2022 05:48:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=39
05/27/2022 05:48:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=40
05/27/2022 05:48:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.35 on epoch=40
05/27/2022 05:48:44 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.7351385262446228 on epoch=40
05/27/2022 05:48:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=40
05/27/2022 05:48:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.31 on epoch=41
05/27/2022 05:48:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=41
05/27/2022 05:48:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.33 on epoch=41
05/27/2022 05:48:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.29 on epoch=42
05/27/2022 05:49:04 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.7551081824220639 on epoch=42
05/27/2022 05:49:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7383263974994074 -> 0.7551081824220639 on epoch=42, global_step=1350
05/27/2022 05:49:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.30 on epoch=42
05/27/2022 05:49:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=42
05/27/2022 05:49:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.31 on epoch=43
05/27/2022 05:49:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=43
05/27/2022 05:49:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.49 on epoch=43
05/27/2022 05:49:23 - INFO - __main__ - Global step 1400 Train loss 0.38 Classification-F1 0.755343797546268 on epoch=43
05/27/2022 05:49:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7551081824220639 -> 0.755343797546268 on epoch=43, global_step=1400
05/27/2022 05:49:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=44
05/27/2022 05:49:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.29 on epoch=44
05/27/2022 05:49:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.43 on epoch=44
05/27/2022 05:49:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=44
05/27/2022 05:49:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=45
05/27/2022 05:49:43 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.7744983021939151 on epoch=45
05/27/2022 05:49:43 - INFO - __main__ - Saving model with best Classification-F1: 0.755343797546268 -> 0.7744983021939151 on epoch=45, global_step=1450
05/27/2022 05:49:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=45
05/27/2022 05:49:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=45
05/27/2022 05:49:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.30 on epoch=46
05/27/2022 05:49:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=46
05/27/2022 05:49:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=46
05/27/2022 05:50:03 - INFO - __main__ - Global step 1500 Train loss 0.36 Classification-F1 0.7528901731117816 on epoch=46
05/27/2022 05:50:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=47
05/27/2022 05:50:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.33 on epoch=47
05/27/2022 05:50:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.42 on epoch=47
05/27/2022 05:50:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.34 on epoch=48
05/27/2022 05:50:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=48
05/27/2022 05:50:23 - INFO - __main__ - Global step 1550 Train loss 0.34 Classification-F1 0.77245200978172 on epoch=48
05/27/2022 05:50:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.34 on epoch=48
05/27/2022 05:50:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=49
05/27/2022 05:50:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.31 on epoch=49
05/27/2022 05:50:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=49
05/27/2022 05:50:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.34 on epoch=49
05/27/2022 05:50:43 - INFO - __main__ - Global step 1600 Train loss 0.34 Classification-F1 0.7787687420686624 on epoch=49
05/27/2022 05:50:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7744983021939151 -> 0.7787687420686624 on epoch=49, global_step=1600
05/27/2022 05:50:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.33 on epoch=50
05/27/2022 05:50:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.29 on epoch=50
05/27/2022 05:50:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=50
05/27/2022 05:50:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.26 on epoch=51
05/27/2022 05:50:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.36 on epoch=51
05/27/2022 05:51:02 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.7537890800932165 on epoch=51
05/27/2022 05:51:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=51
05/27/2022 05:51:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.30 on epoch=52
05/27/2022 05:51:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.32 on epoch=52
05/27/2022 05:51:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=52
05/27/2022 05:51:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.27 on epoch=53
05/27/2022 05:51:22 - INFO - __main__ - Global step 1700 Train loss 0.31 Classification-F1 0.6985838965638691 on epoch=53
05/27/2022 05:51:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=53
05/27/2022 05:51:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.29 on epoch=53
05/27/2022 05:51:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=54
05/27/2022 05:51:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=54
05/27/2022 05:51:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=54
05/27/2022 05:51:42 - INFO - __main__ - Global step 1750 Train loss 0.26 Classification-F1 0.7688817388581235 on epoch=54
05/27/2022 05:51:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.33 on epoch=54
05/27/2022 05:51:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.32 on epoch=55
05/27/2022 05:51:50 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=55
05/27/2022 05:51:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=55
05/27/2022 05:51:55 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=56
05/27/2022 05:52:02 - INFO - __main__ - Global step 1800 Train loss 0.29 Classification-F1 0.739690427839347 on epoch=56
05/27/2022 05:52:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.34 on epoch=56
05/27/2022 05:52:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.29 on epoch=56
05/27/2022 05:52:10 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=57
05/27/2022 05:52:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.32 on epoch=57
05/27/2022 05:52:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.27 on epoch=57
05/27/2022 05:52:22 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.7486066820009848 on epoch=57
05/27/2022 05:52:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.30 on epoch=58
05/27/2022 05:52:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.30 on epoch=58
05/27/2022 05:52:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.26 on epoch=58
05/27/2022 05:52:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.20 on epoch=59
05/27/2022 05:52:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=59
05/27/2022 05:52:41 - INFO - __main__ - Global step 1900 Train loss 0.25 Classification-F1 0.8009928429707717 on epoch=59
05/27/2022 05:52:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7787687420686624 -> 0.8009928429707717 on epoch=59, global_step=1900
05/27/2022 05:52:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.33 on epoch=59
05/27/2022 05:52:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=59
05/27/2022 05:52:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.29 on epoch=60
05/27/2022 05:52:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.22 on epoch=60
05/27/2022 05:52:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.36 on epoch=60
05/27/2022 05:53:01 - INFO - __main__ - Global step 1950 Train loss 0.28 Classification-F1 0.7626588591952341 on epoch=60
05/27/2022 05:53:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=61
05/27/2022 05:53:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=61
05/27/2022 05:53:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.24 on epoch=61
05/27/2022 05:53:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.25 on epoch=62
05/27/2022 05:53:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=62
05/27/2022 05:53:21 - INFO - __main__ - Global step 2000 Train loss 0.24 Classification-F1 0.774316216161981 on epoch=62
05/27/2022 05:53:24 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.22 on epoch=62
05/27/2022 05:53:26 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.23 on epoch=63
05/27/2022 05:53:29 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.22 on epoch=63
05/27/2022 05:53:32 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.32 on epoch=63
05/27/2022 05:53:34 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.16 on epoch=64
05/27/2022 05:53:41 - INFO - __main__ - Global step 2050 Train loss 0.23 Classification-F1 0.7649470565271346 on epoch=64
05/27/2022 05:53:44 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.23 on epoch=64
05/27/2022 05:53:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.28 on epoch=64
05/27/2022 05:53:49 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.16 on epoch=64
05/27/2022 05:53:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.23 on epoch=65
05/27/2022 05:53:54 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.20 on epoch=65
05/27/2022 05:54:01 - INFO - __main__ - Global step 2100 Train loss 0.22 Classification-F1 0.7351488650113069 on epoch=65
05/27/2022 05:54:04 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.35 on epoch=65
05/27/2022 05:54:06 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.16 on epoch=66
05/27/2022 05:54:09 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.15 on epoch=66
05/27/2022 05:54:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.28 on epoch=66
05/27/2022 05:54:14 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.14 on epoch=67
05/27/2022 05:54:21 - INFO - __main__ - Global step 2150 Train loss 0.21 Classification-F1 0.7370344898567698 on epoch=67
05/27/2022 05:54:24 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.28 on epoch=67
05/27/2022 05:54:26 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.23 on epoch=67
05/27/2022 05:54:29 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.23 on epoch=68
05/27/2022 05:54:32 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.22 on epoch=68
05/27/2022 05:54:34 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.34 on epoch=68
05/27/2022 05:54:41 - INFO - __main__ - Global step 2200 Train loss 0.26 Classification-F1 0.7848770015180482 on epoch=68
05/27/2022 05:54:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.21 on epoch=69
05/27/2022 05:54:46 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.28 on epoch=69
05/27/2022 05:54:49 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.31 on epoch=69
05/27/2022 05:54:51 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.13 on epoch=69
05/27/2022 05:54:54 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.24 on epoch=70
05/27/2022 05:55:01 - INFO - __main__ - Global step 2250 Train loss 0.23 Classification-F1 0.7882342099626879 on epoch=70
05/27/2022 05:55:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.19 on epoch=70
05/27/2022 05:55:06 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.29 on epoch=70
05/27/2022 05:55:09 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.25 on epoch=71
05/27/2022 05:55:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.25 on epoch=71
05/27/2022 05:55:14 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.22 on epoch=71
05/27/2022 05:55:21 - INFO - __main__ - Global step 2300 Train loss 0.24 Classification-F1 0.7659193220303144 on epoch=71
05/27/2022 05:55:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.20 on epoch=72
05/27/2022 05:55:26 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.20 on epoch=72
05/27/2022 05:55:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.16 on epoch=72
05/27/2022 05:55:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.25 on epoch=73
05/27/2022 05:55:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=73
05/27/2022 05:55:41 - INFO - __main__ - Global step 2350 Train loss 0.21 Classification-F1 0.781570001643136 on epoch=73
05/27/2022 05:55:44 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.31 on epoch=73
05/27/2022 05:55:47 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.21 on epoch=74
05/27/2022 05:55:49 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.13 on epoch=74
05/27/2022 05:55:52 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.26 on epoch=74
05/27/2022 05:55:54 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.17 on epoch=74
05/27/2022 05:56:01 - INFO - __main__ - Global step 2400 Train loss 0.22 Classification-F1 0.7992327332534666 on epoch=74
05/27/2022 05:56:04 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.20 on epoch=75
05/27/2022 05:56:06 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.27 on epoch=75
05/27/2022 05:56:09 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.23 on epoch=75
05/27/2022 05:56:12 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.19 on epoch=76
05/27/2022 05:56:14 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.18 on epoch=76
05/27/2022 05:56:21 - INFO - __main__ - Global step 2450 Train loss 0.22 Classification-F1 0.6886087840221734 on epoch=76
05/27/2022 05:56:24 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.26 on epoch=76
05/27/2022 05:56:26 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.24 on epoch=77
05/27/2022 05:56:29 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.25 on epoch=77
05/27/2022 05:56:32 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.16 on epoch=77
05/27/2022 05:56:34 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.15 on epoch=78
05/27/2022 05:56:41 - INFO - __main__ - Global step 2500 Train loss 0.21 Classification-F1 0.79602121238261 on epoch=78
05/27/2022 05:56:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.14 on epoch=78
05/27/2022 05:56:46 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.19 on epoch=78
05/27/2022 05:56:49 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.21 on epoch=79
05/27/2022 05:56:52 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.19 on epoch=79
05/27/2022 05:56:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.21 on epoch=79
05/27/2022 05:57:01 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.7992084823377796 on epoch=79
05/27/2022 05:57:04 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.12 on epoch=79
05/27/2022 05:57:06 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.20 on epoch=80
05/27/2022 05:57:09 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.21 on epoch=80
05/27/2022 05:57:12 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.22 on epoch=80
05/27/2022 05:57:14 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.19 on epoch=81
05/27/2022 05:57:21 - INFO - __main__ - Global step 2600 Train loss 0.19 Classification-F1 0.774343384820032 on epoch=81
05/27/2022 05:57:24 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.20 on epoch=81
05/27/2022 05:57:27 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.23 on epoch=81
05/27/2022 05:57:29 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.23 on epoch=82
05/27/2022 05:57:32 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.13 on epoch=82
05/27/2022 05:57:34 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.25 on epoch=82
05/27/2022 05:57:41 - INFO - __main__ - Global step 2650 Train loss 0.21 Classification-F1 0.7709458357766326 on epoch=82
05/27/2022 05:57:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.17 on epoch=83
05/27/2022 05:57:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.19 on epoch=83
05/27/2022 05:57:49 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.24 on epoch=83
05/27/2022 05:57:52 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.19 on epoch=84
05/27/2022 05:57:54 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.18 on epoch=84
05/27/2022 05:58:01 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.7922620575689631 on epoch=84
05/27/2022 05:58:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.19 on epoch=84
05/27/2022 05:58:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=84
05/27/2022 05:58:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.17 on epoch=85
05/27/2022 05:58:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.15 on epoch=85
05/27/2022 05:58:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.22 on epoch=85
05/27/2022 05:58:21 - INFO - __main__ - Global step 2750 Train loss 0.16 Classification-F1 0.8055356711488827 on epoch=85
05/27/2022 05:58:21 - INFO - __main__ - Saving model with best Classification-F1: 0.8009928429707717 -> 0.8055356711488827 on epoch=85, global_step=2750
05/27/2022 05:58:24 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=86
05/27/2022 05:58:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.15 on epoch=86
05/27/2022 05:58:29 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=86
05/27/2022 05:58:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.21 on epoch=87
05/27/2022 05:58:34 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.19 on epoch=87
05/27/2022 05:58:41 - INFO - __main__ - Global step 2800 Train loss 0.18 Classification-F1 0.7760664753536921 on epoch=87
05/27/2022 05:58:44 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.11 on epoch=87
05/27/2022 05:58:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.22 on epoch=88
05/27/2022 05:58:49 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.16 on epoch=88
05/27/2022 05:58:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.19 on epoch=88
05/27/2022 05:58:54 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.13 on epoch=89
05/27/2022 05:59:01 - INFO - __main__ - Global step 2850 Train loss 0.16 Classification-F1 0.7983553371137244 on epoch=89
05/27/2022 05:59:04 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.20 on epoch=89
05/27/2022 05:59:07 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.17 on epoch=89
05/27/2022 05:59:09 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.19 on epoch=89
05/27/2022 05:59:12 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.17 on epoch=90
05/27/2022 05:59:14 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.15 on epoch=90
05/27/2022 05:59:21 - INFO - __main__ - Global step 2900 Train loss 0.18 Classification-F1 0.7494096629808439 on epoch=90
05/27/2022 05:59:24 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.16 on epoch=90
05/27/2022 05:59:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.18 on epoch=91
05/27/2022 05:59:29 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.12 on epoch=91
05/27/2022 05:59:32 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.17 on epoch=91
05/27/2022 05:59:34 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.15 on epoch=92
05/27/2022 05:59:41 - INFO - __main__ - Global step 2950 Train loss 0.16 Classification-F1 0.7666289219585153 on epoch=92
05/27/2022 05:59:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.18 on epoch=92
05/27/2022 05:59:47 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.12 on epoch=92
05/27/2022 05:59:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.19 on epoch=93
05/27/2022 05:59:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.15 on epoch=93
05/27/2022 05:59:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.16 on epoch=93
05/27/2022 05:59:56 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 05:59:56 - INFO - __main__ - Printing 3 examples
05/27/2022 05:59:56 - INFO - __main__ -  [emo] how cause yes am listening
05/27/2022 05:59:56 - INFO - __main__ - ['others']
05/27/2022 05:59:56 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/27/2022 05:59:56 - INFO - __main__ - ['others']
05/27/2022 05:59:56 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/27/2022 05:59:56 - INFO - __main__ - ['others']
05/27/2022 05:59:56 - INFO - __main__ - Tokenizing Input ...
05/27/2022 05:59:56 - INFO - __main__ - Tokenizing Output ...
05/27/2022 05:59:56 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 05:59:56 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 05:59:56 - INFO - __main__ - Printing 3 examples
05/27/2022 05:59:56 - INFO - __main__ -  [emo] when when it comes to you never why
05/27/2022 05:59:56 - INFO - __main__ - ['others']
05/27/2022 05:59:56 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/27/2022 05:59:56 - INFO - __main__ - ['others']
05/27/2022 05:59:56 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/27/2022 05:59:56 - INFO - __main__ - ['others']
05/27/2022 05:59:56 - INFO - __main__ - Tokenizing Input ...
05/27/2022 05:59:56 - INFO - __main__ - Tokenizing Output ...
05/27/2022 05:59:57 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 06:00:01 - INFO - __main__ - Global step 3000 Train loss 0.16 Classification-F1 0.7944586469183241 on epoch=93
05/27/2022 06:00:01 - INFO - __main__ - save last model!
05/27/2022 06:00:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 06:00:01 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 06:00:01 - INFO - __main__ - Printing 3 examples
05/27/2022 06:00:01 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 06:00:01 - INFO - __main__ - ['others']
05/27/2022 06:00:01 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 06:00:01 - INFO - __main__ - ['others']
05/27/2022 06:00:01 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 06:00:01 - INFO - __main__ - ['others']
05/27/2022 06:00:01 - INFO - __main__ - Tokenizing Input ...
05/27/2022 06:00:04 - INFO - __main__ - Tokenizing Output ...
05/27/2022 06:00:09 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 06:00:14 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 06:00:14 - INFO - __main__ - task name: emo
05/27/2022 06:00:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 06:00:15 - INFO - __main__ - Starting training!
05/27/2022 06:01:24 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_100_0.3_8_predictions.txt
05/27/2022 06:01:24 - INFO - __main__ - Classification-F1 on test data: 0.4991
05/27/2022 06:01:25 - INFO - __main__ - prefix=emo_128_100, lr=0.3, bsz=8, dev_performance=0.8055356711488827, test_performance=0.49909718174284573
05/27/2022 06:01:25 - INFO - __main__ - Running ... prefix=emo_128_100, lr=0.2, bsz=8 ...
05/27/2022 06:01:26 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 06:01:26 - INFO - __main__ - Printing 3 examples
05/27/2022 06:01:26 - INFO - __main__ -  [emo] how cause yes am listening
05/27/2022 06:01:26 - INFO - __main__ - ['others']
05/27/2022 06:01:26 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/27/2022 06:01:26 - INFO - __main__ - ['others']
05/27/2022 06:01:26 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/27/2022 06:01:26 - INFO - __main__ - ['others']
05/27/2022 06:01:26 - INFO - __main__ - Tokenizing Input ...
05/27/2022 06:01:26 - INFO - __main__ - Tokenizing Output ...
05/27/2022 06:01:26 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 06:01:26 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 06:01:26 - INFO - __main__ - Printing 3 examples
05/27/2022 06:01:26 - INFO - __main__ -  [emo] when when it comes to you never why
05/27/2022 06:01:26 - INFO - __main__ - ['others']
05/27/2022 06:01:26 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/27/2022 06:01:26 - INFO - __main__ - ['others']
05/27/2022 06:01:26 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/27/2022 06:01:26 - INFO - __main__ - ['others']
05/27/2022 06:01:26 - INFO - __main__ - Tokenizing Input ...
05/27/2022 06:01:27 - INFO - __main__ - Tokenizing Output ...
05/27/2022 06:01:27 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 06:01:43 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 06:01:43 - INFO - __main__ - task name: emo
05/27/2022 06:01:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 06:01:44 - INFO - __main__ - Starting training!
05/27/2022 06:01:47 - INFO - __main__ - Step 10 Global step 10 Train loss 7.63 on epoch=0
05/27/2022 06:01:49 - INFO - __main__ - Step 20 Global step 20 Train loss 5.64 on epoch=0
05/27/2022 06:01:52 - INFO - __main__ - Step 30 Global step 30 Train loss 3.42 on epoch=0
05/27/2022 06:01:54 - INFO - __main__ - Step 40 Global step 40 Train loss 2.13 on epoch=1
05/27/2022 06:01:57 - INFO - __main__ - Step 50 Global step 50 Train loss 1.61 on epoch=1
05/27/2022 06:02:04 - INFO - __main__ - Global step 50 Train loss 4.08 Classification-F1 0.16920341235605513 on epoch=1
05/27/2022 06:02:04 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16920341235605513 on epoch=1, global_step=50
05/27/2022 06:02:06 - INFO - __main__ - Step 60 Global step 60 Train loss 1.34 on epoch=1
05/27/2022 06:02:09 - INFO - __main__ - Step 70 Global step 70 Train loss 1.14 on epoch=2
05/27/2022 06:02:11 - INFO - __main__ - Step 80 Global step 80 Train loss 1.16 on epoch=2
05/27/2022 06:02:14 - INFO - __main__ - Step 90 Global step 90 Train loss 1.02 on epoch=2
05/27/2022 06:02:16 - INFO - __main__ - Step 100 Global step 100 Train loss 1.07 on epoch=3
05/27/2022 06:02:23 - INFO - __main__ - Global step 100 Train loss 1.14 Classification-F1 0.20491538487845043 on epoch=3
05/27/2022 06:02:23 - INFO - __main__ - Saving model with best Classification-F1: 0.16920341235605513 -> 0.20491538487845043 on epoch=3, global_step=100
05/27/2022 06:02:25 - INFO - __main__ - Step 110 Global step 110 Train loss 1.08 on epoch=3
05/27/2022 06:02:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.97 on epoch=3
05/27/2022 06:02:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.97 on epoch=4
05/27/2022 06:02:33 - INFO - __main__ - Step 140 Global step 140 Train loss 1.07 on epoch=4
05/27/2022 06:02:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.93 on epoch=4
05/27/2022 06:02:42 - INFO - __main__ - Global step 150 Train loss 1.00 Classification-F1 0.11830408284837098 on epoch=4
05/27/2022 06:02:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.94 on epoch=4
05/27/2022 06:02:47 - INFO - __main__ - Step 170 Global step 170 Train loss 1.03 on epoch=5
05/27/2022 06:02:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.93 on epoch=5
05/27/2022 06:02:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.99 on epoch=5
05/27/2022 06:02:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.86 on epoch=6
05/27/2022 06:03:02 - INFO - __main__ - Global step 200 Train loss 0.95 Classification-F1 0.2055363321799308 on epoch=6
05/27/2022 06:03:02 - INFO - __main__ - Saving model with best Classification-F1: 0.20491538487845043 -> 0.2055363321799308 on epoch=6, global_step=200
05/27/2022 06:03:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.94 on epoch=6
05/27/2022 06:03:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.95 on epoch=6
05/27/2022 06:03:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.86 on epoch=7
05/27/2022 06:03:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=7
05/27/2022 06:03:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.89 on epoch=7
05/27/2022 06:03:21 - INFO - __main__ - Global step 250 Train loss 0.90 Classification-F1 0.2222086513994911 on epoch=7
05/27/2022 06:03:21 - INFO - __main__ - Saving model with best Classification-F1: 0.2055363321799308 -> 0.2222086513994911 on epoch=7, global_step=250
05/27/2022 06:03:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.88 on epoch=8
05/27/2022 06:03:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.86 on epoch=8
05/27/2022 06:03:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.95 on epoch=8
05/27/2022 06:03:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.83 on epoch=9
05/27/2022 06:03:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.91 on epoch=9
05/27/2022 06:03:41 - INFO - __main__ - Global step 300 Train loss 0.89 Classification-F1 0.2672819409196502 on epoch=9
05/27/2022 06:03:41 - INFO - __main__ - Saving model with best Classification-F1: 0.2222086513994911 -> 0.2672819409196502 on epoch=9, global_step=300
05/27/2022 06:03:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.97 on epoch=9
05/27/2022 06:03:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.86 on epoch=9
05/27/2022 06:03:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.79 on epoch=10
05/27/2022 06:03:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.85 on epoch=10
05/27/2022 06:03:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.90 on epoch=10
05/27/2022 06:04:00 - INFO - __main__ - Global step 350 Train loss 0.87 Classification-F1 0.23853086433899778 on epoch=10
05/27/2022 06:04:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.84 on epoch=11
05/27/2022 06:04:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.90 on epoch=11
05/27/2022 06:04:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.83 on epoch=11
05/27/2022 06:04:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.86 on epoch=12
05/27/2022 06:04:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.78 on epoch=12
05/27/2022 06:04:19 - INFO - __main__ - Global step 400 Train loss 0.84 Classification-F1 0.12660272955562835 on epoch=12
05/27/2022 06:04:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.82 on epoch=12
05/27/2022 06:04:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.92 on epoch=13
05/27/2022 06:04:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.91 on epoch=13
05/27/2022 06:04:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.81 on epoch=13
05/27/2022 06:04:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.83 on epoch=14
05/27/2022 06:04:38 - INFO - __main__ - Global step 450 Train loss 0.86 Classification-F1 0.32469954548951696 on epoch=14
05/27/2022 06:04:38 - INFO - __main__ - Saving model with best Classification-F1: 0.2672819409196502 -> 0.32469954548951696 on epoch=14, global_step=450
05/27/2022 06:04:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.91 on epoch=14
05/27/2022 06:04:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.93 on epoch=14
05/27/2022 06:04:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.90 on epoch=14
05/27/2022 06:04:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.75 on epoch=15
05/27/2022 06:04:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.75 on epoch=15
05/27/2022 06:04:58 - INFO - __main__ - Global step 500 Train loss 0.85 Classification-F1 0.28180293932856404 on epoch=15
05/27/2022 06:05:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.84 on epoch=15
05/27/2022 06:05:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.79 on epoch=16
05/27/2022 06:05:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.83 on epoch=16
05/27/2022 06:05:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.83 on epoch=16
05/27/2022 06:05:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.72 on epoch=17
05/27/2022 06:05:17 - INFO - __main__ - Global step 550 Train loss 0.80 Classification-F1 0.5067968301277573 on epoch=17
05/27/2022 06:05:17 - INFO - __main__ - Saving model with best Classification-F1: 0.32469954548951696 -> 0.5067968301277573 on epoch=17, global_step=550
05/27/2022 06:05:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.80 on epoch=17
05/27/2022 06:05:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.79 on epoch=17
05/27/2022 06:05:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.84 on epoch=18
05/27/2022 06:05:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.80 on epoch=18
05/27/2022 06:05:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.71 on epoch=18
05/27/2022 06:05:36 - INFO - __main__ - Global step 600 Train loss 0.79 Classification-F1 0.5918001554247397 on epoch=18
05/27/2022 06:05:36 - INFO - __main__ - Saving model with best Classification-F1: 0.5067968301277573 -> 0.5918001554247397 on epoch=18, global_step=600
05/27/2022 06:05:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.68 on epoch=19
05/27/2022 06:05:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.79 on epoch=19
05/27/2022 06:05:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.81 on epoch=19
05/27/2022 06:05:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.71 on epoch=19
05/27/2022 06:05:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.65 on epoch=20
05/27/2022 06:05:56 - INFO - __main__ - Global step 650 Train loss 0.73 Classification-F1 0.391555070095977 on epoch=20
05/27/2022 06:05:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.71 on epoch=20
05/27/2022 06:06:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.65 on epoch=20
05/27/2022 06:06:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.68 on epoch=21
05/27/2022 06:06:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.66 on epoch=21
05/27/2022 06:06:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.72 on epoch=21
05/27/2022 06:06:15 - INFO - __main__ - Global step 700 Train loss 0.68 Classification-F1 0.6355088361064919 on epoch=21
05/27/2022 06:06:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5918001554247397 -> 0.6355088361064919 on epoch=21, global_step=700
05/27/2022 06:06:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.70 on epoch=22
05/27/2022 06:06:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.64 on epoch=22
05/27/2022 06:06:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.67 on epoch=22
05/27/2022 06:06:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.62 on epoch=23
05/27/2022 06:06:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.68 on epoch=23
05/27/2022 06:06:34 - INFO - __main__ - Global step 750 Train loss 0.66 Classification-F1 0.5724803252982046 on epoch=23
05/27/2022 06:06:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.65 on epoch=23
05/27/2022 06:06:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.52 on epoch=24
05/27/2022 06:06:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.62 on epoch=24
05/27/2022 06:06:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.64 on epoch=24
05/27/2022 06:06:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.57 on epoch=24
05/27/2022 06:06:53 - INFO - __main__ - Global step 800 Train loss 0.60 Classification-F1 0.6386152418684691 on epoch=24
05/27/2022 06:06:53 - INFO - __main__ - Saving model with best Classification-F1: 0.6355088361064919 -> 0.6386152418684691 on epoch=24, global_step=800
05/27/2022 06:06:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.68 on epoch=25
05/27/2022 06:06:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.72 on epoch=25
05/27/2022 06:07:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.67 on epoch=25
05/27/2022 06:07:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=26
05/27/2022 06:07:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.71 on epoch=26
05/27/2022 06:07:13 - INFO - __main__ - Global step 850 Train loss 0.66 Classification-F1 0.4996972106191557 on epoch=26
05/27/2022 06:07:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.51 on epoch=26
05/27/2022 06:07:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.60 on epoch=27
05/27/2022 06:07:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.59 on epoch=27
05/27/2022 06:07:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.52 on epoch=27
05/27/2022 06:07:25 - INFO - __main__ - Step 900 Global step 900 Train loss 0.58 on epoch=28
05/27/2022 06:07:32 - INFO - __main__ - Global step 900 Train loss 0.56 Classification-F1 0.47054180051463435 on epoch=28
05/27/2022 06:07:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.56 on epoch=28
05/27/2022 06:07:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.61 on epoch=28
05/27/2022 06:07:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.54 on epoch=29
05/27/2022 06:07:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.53 on epoch=29
05/27/2022 06:07:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.68 on epoch=29
05/27/2022 06:07:51 - INFO - __main__ - Global step 950 Train loss 0.58 Classification-F1 0.6624607335799131 on epoch=29
05/27/2022 06:07:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6386152418684691 -> 0.6624607335799131 on epoch=29, global_step=950
05/27/2022 06:07:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=29
05/27/2022 06:07:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.60 on epoch=30
05/27/2022 06:07:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.58 on epoch=30
05/27/2022 06:08:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.57 on epoch=30
05/27/2022 06:08:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.54 on epoch=31
05/27/2022 06:08:11 - INFO - __main__ - Global step 1000 Train loss 0.54 Classification-F1 0.5142729636966188 on epoch=31
05/27/2022 06:08:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.66 on epoch=31
05/27/2022 06:08:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.51 on epoch=31
05/27/2022 06:08:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=32
05/27/2022 06:08:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.54 on epoch=32
05/27/2022 06:08:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.56 on epoch=32
05/27/2022 06:08:30 - INFO - __main__ - Global step 1050 Train loss 0.54 Classification-F1 0.5611603169410503 on epoch=32
05/27/2022 06:08:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.47 on epoch=33
05/27/2022 06:08:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.53 on epoch=33
05/27/2022 06:08:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.59 on epoch=33
05/27/2022 06:08:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.55 on epoch=34
05/27/2022 06:08:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.46 on epoch=34
05/27/2022 06:08:49 - INFO - __main__ - Global step 1100 Train loss 0.52 Classification-F1 0.5620462034665211 on epoch=34
05/27/2022 06:08:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.66 on epoch=34
05/27/2022 06:08:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=34
05/27/2022 06:08:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.49 on epoch=35
05/27/2022 06:08:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.53 on epoch=35
05/27/2022 06:09:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.55 on epoch=35
05/27/2022 06:09:08 - INFO - __main__ - Global step 1150 Train loss 0.53 Classification-F1 0.6822805634623081 on epoch=35
05/27/2022 06:09:08 - INFO - __main__ - Saving model with best Classification-F1: 0.6624607335799131 -> 0.6822805634623081 on epoch=35, global_step=1150
05/27/2022 06:09:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=36
05/27/2022 06:09:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.55 on epoch=36
05/27/2022 06:09:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.54 on epoch=36
05/27/2022 06:09:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=37
05/27/2022 06:09:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.48 on epoch=37
05/27/2022 06:09:27 - INFO - __main__ - Global step 1200 Train loss 0.49 Classification-F1 0.558990485089821 on epoch=37
05/27/2022 06:09:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=37
05/27/2022 06:09:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=38
05/27/2022 06:09:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.48 on epoch=38
05/27/2022 06:09:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.48 on epoch=38
05/27/2022 06:09:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=39
05/27/2022 06:09:47 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.6342411837808031 on epoch=39
05/27/2022 06:09:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=39
05/27/2022 06:09:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.53 on epoch=39
05/27/2022 06:09:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=39
05/27/2022 06:09:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.44 on epoch=40
05/27/2022 06:09:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.46 on epoch=40
05/27/2022 06:10:06 - INFO - __main__ - Global step 1300 Train loss 0.45 Classification-F1 0.6013679975243331 on epoch=40
05/27/2022 06:10:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.54 on epoch=40
05/27/2022 06:10:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=41
05/27/2022 06:10:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.47 on epoch=41
05/27/2022 06:10:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.45 on epoch=41
05/27/2022 06:10:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.35 on epoch=42
05/27/2022 06:10:26 - INFO - __main__ - Global step 1350 Train loss 0.45 Classification-F1 0.6075616293461801 on epoch=42
05/27/2022 06:10:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.46 on epoch=42
05/27/2022 06:10:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=42
05/27/2022 06:10:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=43
05/27/2022 06:10:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=43
05/27/2022 06:10:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.45 on epoch=43
05/27/2022 06:10:46 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.7094247156532133 on epoch=43
05/27/2022 06:10:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6822805634623081 -> 0.7094247156532133 on epoch=43, global_step=1400
05/27/2022 06:10:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=44
05/27/2022 06:10:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=44
05/27/2022 06:10:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.54 on epoch=44
05/27/2022 06:10:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=44
05/27/2022 06:10:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=45
05/27/2022 06:11:06 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.7226226853245256 on epoch=45
05/27/2022 06:11:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7094247156532133 -> 0.7226226853245256 on epoch=45, global_step=1450
05/27/2022 06:11:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=45
05/27/2022 06:11:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=45
05/27/2022 06:11:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=46
05/27/2022 06:11:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=46
05/27/2022 06:11:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=46
05/27/2022 06:11:26 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.6892546501708414 on epoch=46
05/27/2022 06:11:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=47
05/27/2022 06:11:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=47
05/27/2022 06:11:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.48 on epoch=47
05/27/2022 06:11:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=48
05/27/2022 06:11:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=48
05/27/2022 06:11:46 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.7436466336806573 on epoch=48
05/27/2022 06:11:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7226226853245256 -> 0.7436466336806573 on epoch=48, global_step=1550
05/27/2022 06:11:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=48
05/27/2022 06:11:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=49
05/27/2022 06:11:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.33 on epoch=49
05/27/2022 06:11:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.40 on epoch=49
05/27/2022 06:11:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.27 on epoch=49
05/27/2022 06:12:06 - INFO - __main__ - Global step 1600 Train loss 0.36 Classification-F1 0.724231493886268 on epoch=49
05/27/2022 06:12:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=50
05/27/2022 06:12:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=50
05/27/2022 06:12:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=50
05/27/2022 06:12:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=51
05/27/2022 06:12:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=51
05/27/2022 06:12:26 - INFO - __main__ - Global step 1650 Train loss 0.34 Classification-F1 0.7219093753885116 on epoch=51
05/27/2022 06:12:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.35 on epoch=51
05/27/2022 06:12:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.32 on epoch=52
05/27/2022 06:12:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.46 on epoch=52
05/27/2022 06:12:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=52
05/27/2022 06:12:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=53
05/27/2022 06:12:46 - INFO - __main__ - Global step 1700 Train loss 0.36 Classification-F1 0.7108080160785786 on epoch=53
05/27/2022 06:12:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.25 on epoch=53
05/27/2022 06:12:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.42 on epoch=53
05/27/2022 06:12:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=54
05/27/2022 06:12:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.30 on epoch=54
05/27/2022 06:13:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=54
05/27/2022 06:13:07 - INFO - __main__ - Global step 1750 Train loss 0.34 Classification-F1 0.746487033634322 on epoch=54
05/27/2022 06:13:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7436466336806573 -> 0.746487033634322 on epoch=54, global_step=1750
05/27/2022 06:13:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=54
05/27/2022 06:13:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.36 on epoch=55
05/27/2022 06:13:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=55
05/27/2022 06:13:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.46 on epoch=55
05/27/2022 06:13:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.30 on epoch=56
05/27/2022 06:13:27 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.7312244709782421 on epoch=56
05/27/2022 06:13:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=56
05/27/2022 06:13:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=56
05/27/2022 06:13:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.29 on epoch=57
05/27/2022 06:13:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.35 on epoch=57
05/27/2022 06:13:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=57
05/27/2022 06:13:47 - INFO - __main__ - Global step 1850 Train loss 0.33 Classification-F1 0.6895069134237877 on epoch=57
05/27/2022 06:13:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=58
05/27/2022 06:13:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.35 on epoch=58
05/27/2022 06:13:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.29 on epoch=58
05/27/2022 06:13:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.22 on epoch=59
05/27/2022 06:14:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.29 on epoch=59
05/27/2022 06:14:07 - INFO - __main__ - Global step 1900 Train loss 0.30 Classification-F1 0.7490814704025439 on epoch=59
05/27/2022 06:14:07 - INFO - __main__ - Saving model with best Classification-F1: 0.746487033634322 -> 0.7490814704025439 on epoch=59, global_step=1900
05/27/2022 06:14:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=59
05/27/2022 06:14:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=59
05/27/2022 06:14:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.31 on epoch=60
05/27/2022 06:14:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.29 on epoch=60
05/27/2022 06:14:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.32 on epoch=60
05/27/2022 06:14:27 - INFO - __main__ - Global step 1950 Train loss 0.30 Classification-F1 0.696939948345376 on epoch=60
05/27/2022 06:14:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=61
05/27/2022 06:14:32 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.32 on epoch=61
05/27/2022 06:14:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=61
05/27/2022 06:14:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.26 on epoch=62
05/27/2022 06:14:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.30 on epoch=62
05/27/2022 06:14:47 - INFO - __main__ - Global step 2000 Train loss 0.29 Classification-F1 0.6725235189508374 on epoch=62
05/27/2022 06:14:50 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.29 on epoch=62
05/27/2022 06:14:53 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.26 on epoch=63
05/27/2022 06:14:55 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.30 on epoch=63
05/27/2022 06:14:58 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.27 on epoch=63
05/27/2022 06:15:00 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.22 on epoch=64
05/27/2022 06:15:07 - INFO - __main__ - Global step 2050 Train loss 0.27 Classification-F1 0.7846413777253842 on epoch=64
05/27/2022 06:15:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7490814704025439 -> 0.7846413777253842 on epoch=64, global_step=2050
05/27/2022 06:15:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.31 on epoch=64
05/27/2022 06:15:13 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.37 on epoch=64
05/27/2022 06:15:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.27 on epoch=64
05/27/2022 06:15:18 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.26 on epoch=65
05/27/2022 06:15:21 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.27 on epoch=65
05/27/2022 06:15:28 - INFO - __main__ - Global step 2100 Train loss 0.30 Classification-F1 0.6927461391831615 on epoch=65
05/27/2022 06:15:30 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.35 on epoch=65
05/27/2022 06:15:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.25 on epoch=66
05/27/2022 06:15:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.35 on epoch=66
05/27/2022 06:15:38 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.30 on epoch=66
05/27/2022 06:15:41 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.24 on epoch=67
05/27/2022 06:15:48 - INFO - __main__ - Global step 2150 Train loss 0.30 Classification-F1 0.7387394500832195 on epoch=67
05/27/2022 06:15:50 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.30 on epoch=67
05/27/2022 06:15:53 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.23 on epoch=67
05/27/2022 06:15:56 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.30 on epoch=68
05/27/2022 06:15:58 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.24 on epoch=68
05/27/2022 06:16:01 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.24 on epoch=68
05/27/2022 06:16:08 - INFO - __main__ - Global step 2200 Train loss 0.26 Classification-F1 0.7360076538691027 on epoch=68
05/27/2022 06:16:11 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.23 on epoch=69
05/27/2022 06:16:13 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.27 on epoch=69
05/27/2022 06:16:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.36 on epoch=69
05/27/2022 06:16:19 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.28 on epoch=69
05/27/2022 06:16:21 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.22 on epoch=70
05/27/2022 06:16:28 - INFO - __main__ - Global step 2250 Train loss 0.27 Classification-F1 0.7684138032434976 on epoch=70
05/27/2022 06:16:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.24 on epoch=70
05/27/2022 06:16:34 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.30 on epoch=70
05/27/2022 06:16:36 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.20 on epoch=71
05/27/2022 06:16:39 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.23 on epoch=71
05/27/2022 06:16:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.19 on epoch=71
05/27/2022 06:16:49 - INFO - __main__ - Global step 2300 Train loss 0.23 Classification-F1 0.7846130129184617 on epoch=71
05/27/2022 06:16:51 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.24 on epoch=72
05/27/2022 06:16:54 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.24 on epoch=72
05/27/2022 06:16:56 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.31 on epoch=72
05/27/2022 06:16:59 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.18 on epoch=73
05/27/2022 06:17:02 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=73
05/27/2022 06:17:09 - INFO - __main__ - Global step 2350 Train loss 0.24 Classification-F1 0.7364068417101526 on epoch=73
05/27/2022 06:17:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.26 on epoch=73
05/27/2022 06:17:14 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.24 on epoch=74
05/27/2022 06:17:17 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.21 on epoch=74
05/27/2022 06:17:19 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.29 on epoch=74
05/27/2022 06:17:22 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.17 on epoch=74
05/27/2022 06:17:29 - INFO - __main__ - Global step 2400 Train loss 0.23 Classification-F1 0.7945970521088164 on epoch=74
05/27/2022 06:17:29 - INFO - __main__ - Saving model with best Classification-F1: 0.7846413777253842 -> 0.7945970521088164 on epoch=74, global_step=2400
05/27/2022 06:17:31 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.17 on epoch=75
05/27/2022 06:17:34 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.19 on epoch=75
05/27/2022 06:17:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.26 on epoch=75
05/27/2022 06:17:39 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.10 on epoch=76
05/27/2022 06:17:42 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.26 on epoch=76
05/27/2022 06:17:49 - INFO - __main__ - Global step 2450 Train loss 0.20 Classification-F1 0.7835130066835869 on epoch=76
05/27/2022 06:17:52 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.34 on epoch=76
05/27/2022 06:17:54 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=77
05/27/2022 06:17:57 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.27 on epoch=77
05/27/2022 06:17:59 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.25 on epoch=77
05/27/2022 06:18:02 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.18 on epoch=78
05/27/2022 06:18:09 - INFO - __main__ - Global step 2500 Train loss 0.24 Classification-F1 0.7711858716089176 on epoch=78
05/27/2022 06:18:12 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.20 on epoch=78
05/27/2022 06:18:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.26 on epoch=78
05/27/2022 06:18:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.20 on epoch=79
05/27/2022 06:18:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.26 on epoch=79
05/27/2022 06:18:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.29 on epoch=79
05/27/2022 06:18:29 - INFO - __main__ - Global step 2550 Train loss 0.24 Classification-F1 0.7679242078549664 on epoch=79
05/27/2022 06:18:32 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.20 on epoch=79
05/27/2022 06:18:34 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.28 on epoch=80
05/27/2022 06:18:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=80
05/27/2022 06:18:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.31 on epoch=80
05/27/2022 06:18:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=81
05/27/2022 06:18:49 - INFO - __main__ - Global step 2600 Train loss 0.23 Classification-F1 0.770205981849903 on epoch=81
05/27/2022 06:18:52 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.25 on epoch=81
05/27/2022 06:18:54 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=81
05/27/2022 06:18:57 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.20 on epoch=82
05/27/2022 06:18:59 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.20 on epoch=82
05/27/2022 06:19:02 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.20 on epoch=82
05/27/2022 06:19:09 - INFO - __main__ - Global step 2650 Train loss 0.21 Classification-F1 0.7888043406338914 on epoch=82
05/27/2022 06:19:12 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.18 on epoch=83
05/27/2022 06:19:14 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.18 on epoch=83
05/27/2022 06:19:17 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.23 on epoch=83
05/27/2022 06:19:19 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.14 on epoch=84
05/27/2022 06:19:22 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.23 on epoch=84
05/27/2022 06:19:29 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.7702124570049098 on epoch=84
05/27/2022 06:19:32 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.33 on epoch=84
05/27/2022 06:19:34 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.15 on epoch=84
05/27/2022 06:19:37 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.20 on epoch=85
05/27/2022 06:19:39 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.26 on epoch=85
05/27/2022 06:19:42 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.19 on epoch=85
05/27/2022 06:19:49 - INFO - __main__ - Global step 2750 Train loss 0.22 Classification-F1 0.7766409443501262 on epoch=85
05/27/2022 06:19:52 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.22 on epoch=86
05/27/2022 06:19:54 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.27 on epoch=86
05/27/2022 06:19:57 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.18 on epoch=86
05/27/2022 06:20:00 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.18 on epoch=87
05/27/2022 06:20:02 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.18 on epoch=87
05/27/2022 06:20:09 - INFO - __main__ - Global step 2800 Train loss 0.21 Classification-F1 0.7754762849984136 on epoch=87
05/27/2022 06:20:12 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.20 on epoch=87
05/27/2022 06:20:15 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.17 on epoch=88
05/27/2022 06:20:17 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.15 on epoch=88
05/27/2022 06:20:20 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.17 on epoch=88
05/27/2022 06:20:23 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.17 on epoch=89
05/27/2022 06:20:30 - INFO - __main__ - Global step 2850 Train loss 0.17 Classification-F1 0.7798767333020165 on epoch=89
05/27/2022 06:20:32 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.18 on epoch=89
05/27/2022 06:20:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.31 on epoch=89
05/27/2022 06:20:37 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=89
05/27/2022 06:20:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.21 on epoch=90
05/27/2022 06:20:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.17 on epoch=90
05/27/2022 06:20:50 - INFO - __main__ - Global step 2900 Train loss 0.20 Classification-F1 0.735584472703533 on epoch=90
05/27/2022 06:20:52 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.31 on epoch=90
05/27/2022 06:20:55 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.11 on epoch=91
05/27/2022 06:20:58 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.23 on epoch=91
05/27/2022 06:21:00 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.20 on epoch=91
05/27/2022 06:21:03 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.13 on epoch=92
05/27/2022 06:21:10 - INFO - __main__ - Global step 2950 Train loss 0.20 Classification-F1 0.7549889698717892 on epoch=92
05/27/2022 06:21:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.14 on epoch=92
05/27/2022 06:21:15 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.14 on epoch=92
05/27/2022 06:21:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.25 on epoch=93
05/27/2022 06:21:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.13 on epoch=93
05/27/2022 06:21:23 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.28 on epoch=93
05/27/2022 06:21:24 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 06:21:24 - INFO - __main__ - Printing 3 examples
05/27/2022 06:21:24 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/27/2022 06:21:24 - INFO - __main__ - ['others']
05/27/2022 06:21:24 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/27/2022 06:21:24 - INFO - __main__ - ['others']
05/27/2022 06:21:24 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/27/2022 06:21:24 - INFO - __main__ - ['others']
05/27/2022 06:21:24 - INFO - __main__ - Tokenizing Input ...
05/27/2022 06:21:24 - INFO - __main__ - Tokenizing Output ...
05/27/2022 06:21:25 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 06:21:25 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 06:21:25 - INFO - __main__ - Printing 3 examples
05/27/2022 06:21:25 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/27/2022 06:21:25 - INFO - __main__ - ['others']
05/27/2022 06:21:25 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/27/2022 06:21:25 - INFO - __main__ - ['others']
05/27/2022 06:21:25 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/27/2022 06:21:25 - INFO - __main__ - ['others']
05/27/2022 06:21:25 - INFO - __main__ - Tokenizing Input ...
05/27/2022 06:21:25 - INFO - __main__ - Tokenizing Output ...
05/27/2022 06:21:25 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 06:21:30 - INFO - __main__ - Global step 3000 Train loss 0.19 Classification-F1 0.7987671297351605 on epoch=93
05/27/2022 06:21:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7945970521088164 -> 0.7987671297351605 on epoch=93, global_step=3000
05/27/2022 06:21:30 - INFO - __main__ - save last model!
05/27/2022 06:21:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 06:21:30 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 06:21:30 - INFO - __main__ - Printing 3 examples
05/27/2022 06:21:30 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 06:21:30 - INFO - __main__ - ['others']
05/27/2022 06:21:30 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 06:21:30 - INFO - __main__ - ['others']
05/27/2022 06:21:30 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 06:21:30 - INFO - __main__ - ['others']
05/27/2022 06:21:30 - INFO - __main__ - Tokenizing Input ...
05/27/2022 06:21:32 - INFO - __main__ - Tokenizing Output ...
05/27/2022 06:21:37 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 06:21:44 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 06:21:44 - INFO - __main__ - task name: emo
05/27/2022 06:21:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 06:21:45 - INFO - __main__ - Starting training!
05/27/2022 06:22:52 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_100_0.2_8_predictions.txt
05/27/2022 06:22:52 - INFO - __main__ - Classification-F1 on test data: 0.4997
05/27/2022 06:22:52 - INFO - __main__ - prefix=emo_128_100, lr=0.2, bsz=8, dev_performance=0.7987671297351605, test_performance=0.49971600104365393
05/27/2022 06:22:52 - INFO - __main__ - Running ... prefix=emo_128_13, lr=0.5, bsz=8 ...
05/27/2022 06:22:53 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 06:22:53 - INFO - __main__ - Printing 3 examples
05/27/2022 06:22:53 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/27/2022 06:22:53 - INFO - __main__ - ['others']
05/27/2022 06:22:53 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/27/2022 06:22:53 - INFO - __main__ - ['others']
05/27/2022 06:22:53 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/27/2022 06:22:53 - INFO - __main__ - ['others']
05/27/2022 06:22:53 - INFO - __main__ - Tokenizing Input ...
05/27/2022 06:22:53 - INFO - __main__ - Tokenizing Output ...
05/27/2022 06:22:54 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 06:22:54 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 06:22:54 - INFO - __main__ - Printing 3 examples
05/27/2022 06:22:54 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/27/2022 06:22:54 - INFO - __main__ - ['others']
05/27/2022 06:22:54 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/27/2022 06:22:54 - INFO - __main__ - ['others']
05/27/2022 06:22:54 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/27/2022 06:22:54 - INFO - __main__ - ['others']
05/27/2022 06:22:54 - INFO - __main__ - Tokenizing Input ...
05/27/2022 06:22:54 - INFO - __main__ - Tokenizing Output ...
05/27/2022 06:22:55 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 06:23:11 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 06:23:11 - INFO - __main__ - task name: emo
05/27/2022 06:23:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 06:23:12 - INFO - __main__ - Starting training!
05/27/2022 06:23:15 - INFO - __main__ - Step 10 Global step 10 Train loss 6.77 on epoch=0
05/27/2022 06:23:17 - INFO - __main__ - Step 20 Global step 20 Train loss 2.73 on epoch=0
05/27/2022 06:23:20 - INFO - __main__ - Step 30 Global step 30 Train loss 1.40 on epoch=0
05/27/2022 06:23:23 - INFO - __main__ - Step 40 Global step 40 Train loss 1.26 on epoch=1
05/27/2022 06:23:25 - INFO - __main__ - Step 50 Global step 50 Train loss 1.09 on epoch=1
05/27/2022 06:23:32 - INFO - __main__ - Global step 50 Train loss 2.65 Classification-F1 0.17292492298190534 on epoch=1
05/27/2022 06:23:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.17292492298190534 on epoch=1, global_step=50
05/27/2022 06:23:35 - INFO - __main__ - Step 60 Global step 60 Train loss 1.00 on epoch=1
05/27/2022 06:23:37 - INFO - __main__ - Step 70 Global step 70 Train loss 1.01 on epoch=2
05/27/2022 06:23:40 - INFO - __main__ - Step 80 Global step 80 Train loss 1.10 on epoch=2
05/27/2022 06:23:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.96 on epoch=2
05/27/2022 06:23:45 - INFO - __main__ - Step 100 Global step 100 Train loss 0.88 on epoch=3
05/27/2022 06:23:52 - INFO - __main__ - Global step 100 Train loss 0.99 Classification-F1 0.22986540741239964 on epoch=3
05/27/2022 06:23:52 - INFO - __main__ - Saving model with best Classification-F1: 0.17292492298190534 -> 0.22986540741239964 on epoch=3, global_step=100
05/27/2022 06:23:55 - INFO - __main__ - Step 110 Global step 110 Train loss 1.07 on epoch=3
05/27/2022 06:23:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.98 on epoch=3
05/27/2022 06:24:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.89 on epoch=4
05/27/2022 06:24:02 - INFO - __main__ - Step 140 Global step 140 Train loss 1.00 on epoch=4
05/27/2022 06:24:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.90 on epoch=4
05/27/2022 06:24:12 - INFO - __main__ - Global step 150 Train loss 0.97 Classification-F1 0.10284448256146368 on epoch=4
05/27/2022 06:24:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.93 on epoch=4
05/27/2022 06:24:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.87 on epoch=5
05/27/2022 06:24:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.94 on epoch=5
05/27/2022 06:24:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.93 on epoch=5
05/27/2022 06:24:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.96 on epoch=6
05/27/2022 06:24:32 - INFO - __main__ - Global step 200 Train loss 0.93 Classification-F1 0.12387690540062803 on epoch=6
05/27/2022 06:24:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.85 on epoch=6
05/27/2022 06:24:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.83 on epoch=6
05/27/2022 06:24:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.84 on epoch=7
05/27/2022 06:24:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.87 on epoch=7
05/27/2022 06:24:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.78 on epoch=7
05/27/2022 06:24:51 - INFO - __main__ - Global step 250 Train loss 0.84 Classification-F1 0.13648504273504272 on epoch=7
05/27/2022 06:24:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.79 on epoch=8
05/27/2022 06:24:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.89 on epoch=8
05/27/2022 06:24:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.87 on epoch=8
05/27/2022 06:25:02 - INFO - __main__ - Step 290 Global step 290 Train loss 0.82 on epoch=9
05/27/2022 06:25:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.86 on epoch=9
05/27/2022 06:25:11 - INFO - __main__ - Global step 300 Train loss 0.84 Classification-F1 0.20282292846552333 on epoch=9
05/27/2022 06:25:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.87 on epoch=9
05/27/2022 06:25:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.77 on epoch=9
05/27/2022 06:25:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.81 on epoch=10
05/27/2022 06:25:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.82 on epoch=10
05/27/2022 06:25:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.84 on epoch=10
05/27/2022 06:25:31 - INFO - __main__ - Global step 350 Train loss 0.82 Classification-F1 0.31767918177063686 on epoch=10
05/27/2022 06:25:31 - INFO - __main__ - Saving model with best Classification-F1: 0.22986540741239964 -> 0.31767918177063686 on epoch=10, global_step=350
05/27/2022 06:25:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.76 on epoch=11
05/27/2022 06:25:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.80 on epoch=11
05/27/2022 06:25:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.72 on epoch=11
05/27/2022 06:25:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.74 on epoch=12
05/27/2022 06:25:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.84 on epoch=12
05/27/2022 06:25:51 - INFO - __main__ - Global step 400 Train loss 0.77 Classification-F1 0.4245827716449798 on epoch=12
05/27/2022 06:25:51 - INFO - __main__ - Saving model with best Classification-F1: 0.31767918177063686 -> 0.4245827716449798 on epoch=12, global_step=400
05/27/2022 06:25:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.74 on epoch=12
05/27/2022 06:25:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.71 on epoch=13
05/27/2022 06:25:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.82 on epoch=13
05/27/2022 06:26:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.78 on epoch=13
05/27/2022 06:26:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.68 on epoch=14
05/27/2022 06:26:10 - INFO - __main__ - Global step 450 Train loss 0.74 Classification-F1 0.34809184309033636 on epoch=14
05/27/2022 06:26:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.72 on epoch=14
05/27/2022 06:26:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.67 on epoch=14
05/27/2022 06:26:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.64 on epoch=14
05/27/2022 06:26:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.67 on epoch=15
05/27/2022 06:26:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.56 on epoch=15
05/27/2022 06:26:30 - INFO - __main__ - Global step 500 Train loss 0.65 Classification-F1 0.6113168630772408 on epoch=15
05/27/2022 06:26:30 - INFO - __main__ - Saving model with best Classification-F1: 0.4245827716449798 -> 0.6113168630772408 on epoch=15, global_step=500
05/27/2022 06:26:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.67 on epoch=15
05/27/2022 06:26:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.49 on epoch=16
05/27/2022 06:26:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.56 on epoch=16
05/27/2022 06:26:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.62 on epoch=16
05/27/2022 06:26:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.53 on epoch=17
05/27/2022 06:26:50 - INFO - __main__ - Global step 550 Train loss 0.58 Classification-F1 0.6841135540184453 on epoch=17
05/27/2022 06:26:50 - INFO - __main__ - Saving model with best Classification-F1: 0.6113168630772408 -> 0.6841135540184453 on epoch=17, global_step=550
05/27/2022 06:26:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.59 on epoch=17
05/27/2022 06:26:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.59 on epoch=17
05/27/2022 06:26:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=18
05/27/2022 06:27:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=18
05/27/2022 06:27:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=18
05/27/2022 06:27:10 - INFO - __main__ - Global step 600 Train loss 0.54 Classification-F1 0.6804319751936968 on epoch=18
05/27/2022 06:27:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=19
05/27/2022 06:27:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.52 on epoch=19
05/27/2022 06:27:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=19
05/27/2022 06:27:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=19
05/27/2022 06:27:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=20
05/27/2022 06:27:29 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.7798660097944212 on epoch=20
05/27/2022 06:27:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6841135540184453 -> 0.7798660097944212 on epoch=20, global_step=650
05/27/2022 06:27:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=20
05/27/2022 06:27:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=20
05/27/2022 06:27:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=21
05/27/2022 06:27:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=21
05/27/2022 06:27:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=21
05/27/2022 06:27:49 - INFO - __main__ - Global step 700 Train loss 0.42 Classification-F1 0.6377738385713042 on epoch=21
05/27/2022 06:27:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=22
05/27/2022 06:27:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=22
05/27/2022 06:27:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.53 on epoch=22
05/27/2022 06:27:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.29 on epoch=23
05/27/2022 06:28:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=23
05/27/2022 06:28:09 - INFO - __main__ - Global step 750 Train loss 0.38 Classification-F1 0.7635168226926577 on epoch=23
05/27/2022 06:28:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=23
05/27/2022 06:28:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.29 on epoch=24
05/27/2022 06:28:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=24
05/27/2022 06:28:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=24
05/27/2022 06:28:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=24
05/27/2022 06:28:28 - INFO - __main__ - Global step 800 Train loss 0.36 Classification-F1 0.722964506703965 on epoch=24
05/27/2022 06:28:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=25
05/27/2022 06:28:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.32 on epoch=25
05/27/2022 06:28:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=25
05/27/2022 06:28:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=26
05/27/2022 06:28:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.32 on epoch=26
05/27/2022 06:28:48 - INFO - __main__ - Global step 850 Train loss 0.33 Classification-F1 0.7733491935037307 on epoch=26
05/27/2022 06:28:50 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=26
05/27/2022 06:28:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=27
05/27/2022 06:28:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=27
05/27/2022 06:28:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.32 on epoch=27
05/27/2022 06:29:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=28
05/27/2022 06:29:07 - INFO - __main__ - Global step 900 Train loss 0.31 Classification-F1 0.7045420372798052 on epoch=28
05/27/2022 06:29:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.32 on epoch=28
05/27/2022 06:29:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.33 on epoch=28
05/27/2022 06:29:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=29
05/27/2022 06:29:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.31 on epoch=29
05/27/2022 06:29:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.31 on epoch=29
05/27/2022 06:29:26 - INFO - __main__ - Global step 950 Train loss 0.29 Classification-F1 0.7824587095494342 on epoch=29
05/27/2022 06:29:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7798660097944212 -> 0.7824587095494342 on epoch=29, global_step=950
05/27/2022 06:29:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.16 on epoch=29
05/27/2022 06:29:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.29 on epoch=30
05/27/2022 06:29:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.29 on epoch=30
05/27/2022 06:29:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=30
05/27/2022 06:29:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=31
05/27/2022 06:29:45 - INFO - __main__ - Global step 1000 Train loss 0.25 Classification-F1 0.7624033406829185 on epoch=31
05/27/2022 06:29:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=31
05/27/2022 06:29:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=31
05/27/2022 06:29:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=32
05/27/2022 06:29:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=32
05/27/2022 06:29:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.30 on epoch=32
05/27/2022 06:30:04 - INFO - __main__ - Global step 1050 Train loss 0.25 Classification-F1 0.7445457827339539 on epoch=32
05/27/2022 06:30:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.17 on epoch=33
05/27/2022 06:30:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=33
05/27/2022 06:30:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=33
05/27/2022 06:30:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=34
05/27/2022 06:30:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.29 on epoch=34
05/27/2022 06:30:24 - INFO - __main__ - Global step 1100 Train loss 0.25 Classification-F1 0.7712459527255217 on epoch=34
05/27/2022 06:30:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=34
05/27/2022 06:30:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.17 on epoch=34
05/27/2022 06:30:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=35
05/27/2022 06:30:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.25 on epoch=35
05/27/2022 06:30:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=35
05/27/2022 06:30:43 - INFO - __main__ - Global step 1150 Train loss 0.23 Classification-F1 0.761588542548574 on epoch=35
05/27/2022 06:30:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.13 on epoch=36
05/27/2022 06:30:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=36
05/27/2022 06:30:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=36
05/27/2022 06:30:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=37
05/27/2022 06:30:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.24 on epoch=37
05/27/2022 06:31:02 - INFO - __main__ - Global step 1200 Train loss 0.22 Classification-F1 0.8054685556802474 on epoch=37
05/27/2022 06:31:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7824587095494342 -> 0.8054685556802474 on epoch=37, global_step=1200
05/27/2022 06:31:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=37
05/27/2022 06:31:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=38
05/27/2022 06:31:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.28 on epoch=38
05/27/2022 06:31:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.22 on epoch=38
05/27/2022 06:31:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.13 on epoch=39
05/27/2022 06:31:22 - INFO - __main__ - Global step 1250 Train loss 0.22 Classification-F1 0.7848760042357029 on epoch=39
05/27/2022 06:31:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=39
05/27/2022 06:31:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=39
05/27/2022 06:31:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.10 on epoch=39
05/27/2022 06:31:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=40
05/27/2022 06:31:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=40
05/27/2022 06:31:41 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.7949144593957123 on epoch=40
05/27/2022 06:31:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=40
05/27/2022 06:31:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=41
05/27/2022 06:31:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=41
05/27/2022 06:31:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=41
05/27/2022 06:31:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=42
05/27/2022 06:32:00 - INFO - __main__ - Global step 1350 Train loss 0.16 Classification-F1 0.7563057425421201 on epoch=42
05/27/2022 06:32:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=42
05/27/2022 06:32:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.15 on epoch=42
05/27/2022 06:32:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=43
05/27/2022 06:32:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.25 on epoch=43
05/27/2022 06:32:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.18 on epoch=43
05/27/2022 06:32:19 - INFO - __main__ - Global step 1400 Train loss 0.16 Classification-F1 0.7852320547912951 on epoch=43
05/27/2022 06:32:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=44
05/27/2022 06:32:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.28 on epoch=44
05/27/2022 06:32:27 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=44
05/27/2022 06:32:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.11 on epoch=44
05/27/2022 06:32:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=45
05/27/2022 06:32:38 - INFO - __main__ - Global step 1450 Train loss 0.18 Classification-F1 0.7792005229127665 on epoch=45
05/27/2022 06:32:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.14 on epoch=45
05/27/2022 06:32:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.19 on epoch=45
05/27/2022 06:32:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=46
05/27/2022 06:32:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=46
05/27/2022 06:32:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=46
05/27/2022 06:32:58 - INFO - __main__ - Global step 1500 Train loss 0.16 Classification-F1 0.7470495925464871 on epoch=46
05/27/2022 06:33:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=47
05/27/2022 06:33:03 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=47
05/27/2022 06:33:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.21 on epoch=47
05/27/2022 06:33:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=48
05/27/2022 06:33:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.28 on epoch=48
05/27/2022 06:33:17 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.7903582123921971 on epoch=48
05/27/2022 06:33:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.14 on epoch=48
05/27/2022 06:33:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=49
05/27/2022 06:33:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.21 on epoch=49
05/27/2022 06:33:27 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=49
05/27/2022 06:33:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=49
05/27/2022 06:33:36 - INFO - __main__ - Global step 1600 Train loss 0.14 Classification-F1 0.7899184319005239 on epoch=49
05/27/2022 06:33:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.09 on epoch=50
05/27/2022 06:33:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=50
05/27/2022 06:33:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=50
05/27/2022 06:33:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=51
05/27/2022 06:33:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=51
05/27/2022 06:33:55 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.8045345393709376 on epoch=51
05/27/2022 06:33:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=51
05/27/2022 06:34:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.14 on epoch=52
05/27/2022 06:34:03 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=52
05/27/2022 06:34:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=52
05/27/2022 06:34:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=53
05/27/2022 06:34:15 - INFO - __main__ - Global step 1700 Train loss 0.16 Classification-F1 0.7860770904006905 on epoch=53
05/27/2022 06:34:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=53
05/27/2022 06:34:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.20 on epoch=53
05/27/2022 06:34:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=54
05/27/2022 06:34:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.21 on epoch=54
05/27/2022 06:34:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.10 on epoch=54
05/27/2022 06:34:34 - INFO - __main__ - Global step 1750 Train loss 0.16 Classification-F1 0.7904671213936597 on epoch=54
05/27/2022 06:34:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=54
05/27/2022 06:34:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=55
05/27/2022 06:34:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.13 on epoch=55
05/27/2022 06:34:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.16 on epoch=55
05/27/2022 06:34:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=56
05/27/2022 06:34:53 - INFO - __main__ - Global step 1800 Train loss 0.10 Classification-F1 0.7942902633640633 on epoch=56
05/27/2022 06:34:55 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=56
05/27/2022 06:34:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.21 on epoch=56
05/27/2022 06:35:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=57
05/27/2022 06:35:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=57
05/27/2022 06:35:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=57
05/27/2022 06:35:12 - INFO - __main__ - Global step 1850 Train loss 0.14 Classification-F1 0.7923481388474928 on epoch=57
05/27/2022 06:35:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=58
05/27/2022 06:35:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.16 on epoch=58
05/27/2022 06:35:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=58
05/27/2022 06:35:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=59
05/27/2022 06:35:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=59
05/27/2022 06:35:31 - INFO - __main__ - Global step 1900 Train loss 0.11 Classification-F1 0.7972448861439678 on epoch=59
05/27/2022 06:35:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=59
05/27/2022 06:35:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=59
05/27/2022 06:35:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=60
05/27/2022 06:35:41 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=60
05/27/2022 06:35:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=60
05/27/2022 06:35:51 - INFO - __main__ - Global step 1950 Train loss 0.11 Classification-F1 0.7925022928051262 on epoch=60
05/27/2022 06:35:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.09 on epoch=61
05/27/2022 06:35:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=61
05/27/2022 06:35:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=61
05/27/2022 06:36:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=62
05/27/2022 06:36:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.15 on epoch=62
05/27/2022 06:36:10 - INFO - __main__ - Global step 2000 Train loss 0.11 Classification-F1 0.821172019062518 on epoch=62
05/27/2022 06:36:10 - INFO - __main__ - Saving model with best Classification-F1: 0.8054685556802474 -> 0.821172019062518 on epoch=62, global_step=2000
05/27/2022 06:36:12 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=62
05/27/2022 06:36:15 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.10 on epoch=63
05/27/2022 06:36:17 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.11 on epoch=63
05/27/2022 06:36:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.15 on epoch=63
05/27/2022 06:36:22 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=64
05/27/2022 06:36:29 - INFO - __main__ - Global step 2050 Train loss 0.10 Classification-F1 0.7961147320863992 on epoch=64
05/27/2022 06:36:31 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.13 on epoch=64
05/27/2022 06:36:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.09 on epoch=64
05/27/2022 06:36:36 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=64
05/27/2022 06:36:39 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.10 on epoch=65
05/27/2022 06:36:41 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.09 on epoch=65
05/27/2022 06:36:48 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.769844727724912 on epoch=65
05/27/2022 06:36:51 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=65
05/27/2022 06:36:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=66
05/27/2022 06:36:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.24 on epoch=66
05/27/2022 06:36:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=66
05/27/2022 06:37:01 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=67
05/27/2022 06:37:07 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.7789386689075083 on epoch=67
05/27/2022 06:37:10 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=67
05/27/2022 06:37:12 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=67
05/27/2022 06:37:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.07 on epoch=68
05/27/2022 06:37:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.16 on epoch=68
05/27/2022 06:37:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=68
05/27/2022 06:37:27 - INFO - __main__ - Global step 2200 Train loss 0.09 Classification-F1 0.8073937858666944 on epoch=68
05/27/2022 06:37:29 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=69
05/27/2022 06:37:32 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.09 on epoch=69
05/27/2022 06:37:34 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.16 on epoch=69
05/27/2022 06:37:37 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.10 on epoch=69
05/27/2022 06:37:39 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=70
05/27/2022 06:37:46 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.7808282467596943 on epoch=70
05/27/2022 06:37:48 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=70
05/27/2022 06:37:51 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.10 on epoch=70
05/27/2022 06:37:53 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=71
05/27/2022 06:37:56 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.07 on epoch=71
05/27/2022 06:37:58 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=71
05/27/2022 06:38:05 - INFO - __main__ - Global step 2300 Train loss 0.09 Classification-F1 0.7847529621074507 on epoch=71
05/27/2022 06:38:07 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.13 on epoch=72
05/27/2022 06:38:10 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.10 on epoch=72
05/27/2022 06:38:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.17 on epoch=72
05/27/2022 06:38:15 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=73
05/27/2022 06:38:17 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.17 on epoch=73
05/27/2022 06:38:24 - INFO - __main__ - Global step 2350 Train loss 0.12 Classification-F1 0.797872854190961 on epoch=73
05/27/2022 06:38:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=73
05/27/2022 06:38:29 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=74
05/27/2022 06:38:32 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.11 on epoch=74
05/27/2022 06:38:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=74
05/27/2022 06:38:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.15 on epoch=74
05/27/2022 06:38:44 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.7715028768737987 on epoch=74
05/27/2022 06:38:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.07 on epoch=75
05/27/2022 06:38:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.08 on epoch=75
05/27/2022 06:38:51 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.10 on epoch=75
05/27/2022 06:38:54 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=76
05/27/2022 06:38:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.12 on epoch=76
05/27/2022 06:39:03 - INFO - __main__ - Global step 2450 Train loss 0.09 Classification-F1 0.787395776052559 on epoch=76
05/27/2022 06:39:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=76
05/27/2022 06:39:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=77
05/27/2022 06:39:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.09 on epoch=77
05/27/2022 06:39:13 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.12 on epoch=77
05/27/2022 06:39:15 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=78
05/27/2022 06:39:22 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.7613955101077816 on epoch=78
05/27/2022 06:39:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.17 on epoch=78
05/27/2022 06:39:27 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.10 on epoch=78
05/27/2022 06:39:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=79
05/27/2022 06:39:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=79
05/27/2022 06:39:34 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.21 on epoch=79
05/27/2022 06:39:41 - INFO - __main__ - Global step 2550 Train loss 0.14 Classification-F1 0.8013372341784532 on epoch=79
05/27/2022 06:39:44 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=79
05/27/2022 06:39:46 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=80
05/27/2022 06:39:49 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=80
05/27/2022 06:39:51 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.11 on epoch=80
05/27/2022 06:39:54 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=81
05/27/2022 06:40:00 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.7871753615997611 on epoch=81
05/27/2022 06:40:03 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.15 on epoch=81
05/27/2022 06:40:05 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=81
05/27/2022 06:40:08 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=82
05/27/2022 06:40:10 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.13 on epoch=82
05/27/2022 06:40:13 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.14 on epoch=82
05/27/2022 06:40:20 - INFO - __main__ - Global step 2650 Train loss 0.11 Classification-F1 0.7764654604641459 on epoch=82
05/27/2022 06:40:22 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=83
05/27/2022 06:40:25 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.18 on epoch=83
05/27/2022 06:40:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.09 on epoch=83
05/27/2022 06:40:30 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=84
05/27/2022 06:40:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=84
05/27/2022 06:40:39 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.8025530725310551 on epoch=84
05/27/2022 06:40:41 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.10 on epoch=84
05/27/2022 06:40:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=84
05/27/2022 06:40:46 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.10 on epoch=85
05/27/2022 06:40:49 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=85
05/27/2022 06:40:51 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.12 on epoch=85
05/27/2022 06:40:58 - INFO - __main__ - Global step 2750 Train loss 0.08 Classification-F1 0.7973314134029794 on epoch=85
05/27/2022 06:41:00 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=86
05/27/2022 06:41:03 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.11 on epoch=86
05/27/2022 06:41:05 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=86
05/27/2022 06:41:08 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=87
05/27/2022 06:41:10 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.20 on epoch=87
05/27/2022 06:41:17 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.7911637719725492 on epoch=87
05/27/2022 06:41:20 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.11 on epoch=87
05/27/2022 06:41:22 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=88
05/27/2022 06:41:25 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.08 on epoch=88
05/27/2022 06:41:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=88
05/27/2022 06:41:30 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=89
05/27/2022 06:41:36 - INFO - __main__ - Global step 2850 Train loss 0.07 Classification-F1 0.7581070440391489 on epoch=89
05/27/2022 06:41:39 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.12 on epoch=89
05/27/2022 06:41:41 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=89
05/27/2022 06:41:44 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=89
05/27/2022 06:41:46 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=90
05/27/2022 06:41:49 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=90
05/27/2022 06:41:56 - INFO - __main__ - Global step 2900 Train loss 0.08 Classification-F1 0.7905461804486938 on epoch=90
05/27/2022 06:41:58 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.12 on epoch=90
05/27/2022 06:42:01 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=91
05/27/2022 06:42:03 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.10 on epoch=91
05/27/2022 06:42:06 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=91
05/27/2022 06:42:08 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=92
05/27/2022 06:42:15 - INFO - __main__ - Global step 2950 Train loss 0.08 Classification-F1 0.8009965923482139 on epoch=92
05/27/2022 06:42:17 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=92
05/27/2022 06:42:20 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.11 on epoch=92
05/27/2022 06:42:22 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=93
05/27/2022 06:42:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.10 on epoch=93
05/27/2022 06:42:27 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=93
05/27/2022 06:42:29 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 06:42:29 - INFO - __main__ - Printing 3 examples
05/27/2022 06:42:29 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/27/2022 06:42:29 - INFO - __main__ - ['others']
05/27/2022 06:42:29 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/27/2022 06:42:29 - INFO - __main__ - ['others']
05/27/2022 06:42:29 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/27/2022 06:42:29 - INFO - __main__ - ['others']
05/27/2022 06:42:29 - INFO - __main__ - Tokenizing Input ...
05/27/2022 06:42:29 - INFO - __main__ - Tokenizing Output ...
05/27/2022 06:42:29 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 06:42:29 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 06:42:29 - INFO - __main__ - Printing 3 examples
05/27/2022 06:42:29 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/27/2022 06:42:29 - INFO - __main__ - ['others']
05/27/2022 06:42:29 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/27/2022 06:42:29 - INFO - __main__ - ['others']
05/27/2022 06:42:29 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/27/2022 06:42:29 - INFO - __main__ - ['others']
05/27/2022 06:42:29 - INFO - __main__ - Tokenizing Input ...
05/27/2022 06:42:29 - INFO - __main__ - Tokenizing Output ...
05/27/2022 06:42:30 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 06:42:34 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.8104928452834463 on epoch=93
05/27/2022 06:42:34 - INFO - __main__ - save last model!
05/27/2022 06:42:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 06:42:34 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 06:42:34 - INFO - __main__ - Printing 3 examples
05/27/2022 06:42:34 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 06:42:34 - INFO - __main__ - ['others']
05/27/2022 06:42:34 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 06:42:34 - INFO - __main__ - ['others']
05/27/2022 06:42:34 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 06:42:34 - INFO - __main__ - ['others']
05/27/2022 06:42:34 - INFO - __main__ - Tokenizing Input ...
05/27/2022 06:42:36 - INFO - __main__ - Tokenizing Output ...
05/27/2022 06:42:42 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 06:42:47 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 06:42:47 - INFO - __main__ - task name: emo
05/27/2022 06:42:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 06:42:48 - INFO - __main__ - Starting training!
05/27/2022 06:43:54 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_13_0.5_8_predictions.txt
05/27/2022 06:43:54 - INFO - __main__ - Classification-F1 on test data: 0.5442
05/27/2022 06:43:54 - INFO - __main__ - prefix=emo_128_13, lr=0.5, bsz=8, dev_performance=0.821172019062518, test_performance=0.5442300052638367
05/27/2022 06:43:54 - INFO - __main__ - Running ... prefix=emo_128_13, lr=0.4, bsz=8 ...
05/27/2022 06:43:55 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 06:43:55 - INFO - __main__ - Printing 3 examples
05/27/2022 06:43:55 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/27/2022 06:43:55 - INFO - __main__ - ['others']
05/27/2022 06:43:55 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/27/2022 06:43:55 - INFO - __main__ - ['others']
05/27/2022 06:43:55 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/27/2022 06:43:55 - INFO - __main__ - ['others']
05/27/2022 06:43:55 - INFO - __main__ - Tokenizing Input ...
05/27/2022 06:43:55 - INFO - __main__ - Tokenizing Output ...
05/27/2022 06:43:56 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 06:43:56 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 06:43:56 - INFO - __main__ - Printing 3 examples
05/27/2022 06:43:56 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/27/2022 06:43:56 - INFO - __main__ - ['others']
05/27/2022 06:43:56 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/27/2022 06:43:56 - INFO - __main__ - ['others']
05/27/2022 06:43:56 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/27/2022 06:43:56 - INFO - __main__ - ['others']
05/27/2022 06:43:56 - INFO - __main__ - Tokenizing Input ...
05/27/2022 06:43:56 - INFO - __main__ - Tokenizing Output ...
05/27/2022 06:43:56 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 06:44:15 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 06:44:15 - INFO - __main__ - task name: emo
05/27/2022 06:44:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 06:44:16 - INFO - __main__ - Starting training!
05/27/2022 06:44:19 - INFO - __main__ - Step 10 Global step 10 Train loss 6.96 on epoch=0
05/27/2022 06:44:22 - INFO - __main__ - Step 20 Global step 20 Train loss 2.91 on epoch=0
05/27/2022 06:44:24 - INFO - __main__ - Step 30 Global step 30 Train loss 1.51 on epoch=0
05/27/2022 06:44:27 - INFO - __main__ - Step 40 Global step 40 Train loss 1.18 on epoch=1
05/27/2022 06:44:29 - INFO - __main__ - Step 50 Global step 50 Train loss 1.16 on epoch=1
05/27/2022 06:44:36 - INFO - __main__ - Global step 50 Train loss 2.74 Classification-F1 0.14572392035364795 on epoch=1
05/27/2022 06:44:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.14572392035364795 on epoch=1, global_step=50
05/27/2022 06:44:39 - INFO - __main__ - Step 60 Global step 60 Train loss 1.08 on epoch=1
05/27/2022 06:44:42 - INFO - __main__ - Step 70 Global step 70 Train loss 0.99 on epoch=2
05/27/2022 06:44:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.99 on epoch=2
05/27/2022 06:44:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.89 on epoch=2
05/27/2022 06:44:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.88 on epoch=3
05/27/2022 06:44:56 - INFO - __main__ - Global step 100 Train loss 0.96 Classification-F1 0.10810454540007429 on epoch=3
05/27/2022 06:44:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.98 on epoch=3
05/27/2022 06:45:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.91 on epoch=3
05/27/2022 06:45:04 - INFO - __main__ - Step 130 Global step 130 Train loss 0.94 on epoch=4
05/27/2022 06:45:06 - INFO - __main__ - Step 140 Global step 140 Train loss 0.95 on epoch=4
05/27/2022 06:45:09 - INFO - __main__ - Step 150 Global step 150 Train loss 0.88 on epoch=4
05/27/2022 06:45:16 - INFO - __main__ - Global step 150 Train loss 0.93 Classification-F1 0.14482587206336328 on epoch=4
05/27/2022 06:45:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.97 on epoch=4
05/27/2022 06:45:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.95 on epoch=5
05/27/2022 06:45:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.84 on epoch=5
05/27/2022 06:45:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.97 on epoch=5
05/27/2022 06:45:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.90 on epoch=6
05/27/2022 06:45:35 - INFO - __main__ - Global step 200 Train loss 0.93 Classification-F1 0.22445875391576753 on epoch=6
05/27/2022 06:45:35 - INFO - __main__ - Saving model with best Classification-F1: 0.14572392035364795 -> 0.22445875391576753 on epoch=6, global_step=200
05/27/2022 06:45:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.84 on epoch=6
05/27/2022 06:45:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.87 on epoch=6
05/27/2022 06:45:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.86 on epoch=7
05/27/2022 06:45:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=7
05/27/2022 06:45:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.95 on epoch=7
05/27/2022 06:45:55 - INFO - __main__ - Global step 250 Train loss 0.88 Classification-F1 0.27998983955154166 on epoch=7
05/27/2022 06:45:55 - INFO - __main__ - Saving model with best Classification-F1: 0.22445875391576753 -> 0.27998983955154166 on epoch=7, global_step=250
05/27/2022 06:45:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.84 on epoch=8
05/27/2022 06:46:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.90 on epoch=8
05/27/2022 06:46:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.85 on epoch=8
05/27/2022 06:46:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.74 on epoch=9
05/27/2022 06:46:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.87 on epoch=9
05/27/2022 06:46:15 - INFO - __main__ - Global step 300 Train loss 0.84 Classification-F1 0.1413543748269614 on epoch=9
05/27/2022 06:46:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.76 on epoch=9
05/27/2022 06:46:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.82 on epoch=9
05/27/2022 06:46:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.89 on epoch=10
05/27/2022 06:46:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.83 on epoch=10
05/27/2022 06:46:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.84 on epoch=10
05/27/2022 06:46:34 - INFO - __main__ - Global step 350 Train loss 0.83 Classification-F1 0.26104776361089965 on epoch=10
05/27/2022 06:46:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.72 on epoch=11
05/27/2022 06:46:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.74 on epoch=11
05/27/2022 06:46:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.78 on epoch=11
05/27/2022 06:46:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.82 on epoch=12
05/27/2022 06:46:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.85 on epoch=12
05/27/2022 06:46:54 - INFO - __main__ - Global step 400 Train loss 0.78 Classification-F1 0.11971594202898551 on epoch=12
05/27/2022 06:46:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.76 on epoch=12
05/27/2022 06:46:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.71 on epoch=13
05/27/2022 06:47:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.82 on epoch=13
05/27/2022 06:47:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.80 on epoch=13
05/27/2022 06:47:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.74 on epoch=14
05/27/2022 06:47:14 - INFO - __main__ - Global step 450 Train loss 0.77 Classification-F1 0.17952699946775041 on epoch=14
05/27/2022 06:47:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.80 on epoch=14
05/27/2022 06:47:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.75 on epoch=14
05/27/2022 06:47:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.77 on epoch=14
05/27/2022 06:47:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.72 on epoch=15
05/27/2022 06:47:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.72 on epoch=15
05/27/2022 06:47:34 - INFO - __main__ - Global step 500 Train loss 0.75 Classification-F1 0.2849058472887453 on epoch=15
05/27/2022 06:47:34 - INFO - __main__ - Saving model with best Classification-F1: 0.27998983955154166 -> 0.2849058472887453 on epoch=15, global_step=500
05/27/2022 06:47:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.80 on epoch=15
05/27/2022 06:47:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.65 on epoch=16
05/27/2022 06:47:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.77 on epoch=16
05/27/2022 06:47:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.65 on epoch=16
05/27/2022 06:47:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.72 on epoch=17
05/27/2022 06:47:53 - INFO - __main__ - Global step 550 Train loss 0.72 Classification-F1 0.5565849260373684 on epoch=17
05/27/2022 06:47:53 - INFO - __main__ - Saving model with best Classification-F1: 0.2849058472887453 -> 0.5565849260373684 on epoch=17, global_step=550
05/27/2022 06:47:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.72 on epoch=17
05/27/2022 06:47:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.73 on epoch=17
05/27/2022 06:48:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.61 on epoch=18
05/27/2022 06:48:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.75 on epoch=18
05/27/2022 06:48:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.74 on epoch=18
05/27/2022 06:48:13 - INFO - __main__ - Global step 600 Train loss 0.71 Classification-F1 0.5020146314913235 on epoch=18
05/27/2022 06:48:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.61 on epoch=19
05/27/2022 06:48:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.69 on epoch=19
05/27/2022 06:48:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.64 on epoch=19
05/27/2022 06:48:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.54 on epoch=19
05/27/2022 06:48:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.70 on epoch=20
05/27/2022 06:48:33 - INFO - __main__ - Global step 650 Train loss 0.64 Classification-F1 0.7186550297177484 on epoch=20
05/27/2022 06:48:33 - INFO - __main__ - Saving model with best Classification-F1: 0.5565849260373684 -> 0.7186550297177484 on epoch=20, global_step=650
05/27/2022 06:48:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=20
05/27/2022 06:48:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.58 on epoch=20
05/27/2022 06:48:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.54 on epoch=21
05/27/2022 06:48:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.52 on epoch=21
05/27/2022 06:48:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.60 on epoch=21
05/27/2022 06:48:53 - INFO - __main__ - Global step 700 Train loss 0.56 Classification-F1 0.7271681403831314 on epoch=21
05/27/2022 06:48:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7186550297177484 -> 0.7271681403831314 on epoch=21, global_step=700
05/27/2022 06:48:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.55 on epoch=22
05/27/2022 06:48:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.63 on epoch=22
05/27/2022 06:49:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=22
05/27/2022 06:49:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=23
05/27/2022 06:49:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.59 on epoch=23
05/27/2022 06:49:12 - INFO - __main__ - Global step 750 Train loss 0.54 Classification-F1 0.6984791224068903 on epoch=23
05/27/2022 06:49:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.51 on epoch=23
05/27/2022 06:49:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=24
05/27/2022 06:49:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.57 on epoch=24
05/27/2022 06:49:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.56 on epoch=24
05/27/2022 06:49:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.46 on epoch=24
05/27/2022 06:49:32 - INFO - __main__ - Global step 800 Train loss 0.51 Classification-F1 0.7140111230788375 on epoch=24
05/27/2022 06:49:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=25
05/27/2022 06:49:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.50 on epoch=25
05/27/2022 06:49:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=25
05/27/2022 06:49:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=26
05/27/2022 06:49:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.57 on epoch=26
05/27/2022 06:49:52 - INFO - __main__ - Global step 850 Train loss 0.49 Classification-F1 0.6990297135182867 on epoch=26
05/27/2022 06:49:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=26
05/27/2022 06:49:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=27
05/27/2022 06:49:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.53 on epoch=27
05/27/2022 06:50:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=27
05/27/2022 06:50:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=28
05/27/2022 06:50:11 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.6988747732241312 on epoch=28
05/27/2022 06:50:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.53 on epoch=28
05/27/2022 06:50:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.50 on epoch=28
05/27/2022 06:50:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.31 on epoch=29
05/27/2022 06:50:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.53 on epoch=29
05/27/2022 06:50:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=29
05/27/2022 06:50:31 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.7245951818535361 on epoch=29
05/27/2022 06:50:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.36 on epoch=29
05/27/2022 06:50:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=30
05/27/2022 06:50:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=30
05/27/2022 06:50:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=30
05/27/2022 06:50:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.30 on epoch=31
05/27/2022 06:50:51 - INFO - __main__ - Global step 1000 Train loss 0.36 Classification-F1 0.5564659386481197 on epoch=31
05/27/2022 06:50:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.48 on epoch=31
05/27/2022 06:50:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=31
05/27/2022 06:50:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.34 on epoch=32
05/27/2022 06:51:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.46 on epoch=32
05/27/2022 06:51:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.44 on epoch=32
05/27/2022 06:51:10 - INFO - __main__ - Global step 1050 Train loss 0.41 Classification-F1 0.6833355584748342 on epoch=32
05/27/2022 06:51:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.33 on epoch=33
05/27/2022 06:51:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.49 on epoch=33
05/27/2022 06:51:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=33
05/27/2022 06:51:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.28 on epoch=34
05/27/2022 06:51:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.40 on epoch=34
05/27/2022 06:51:30 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.7453385096964917 on epoch=34
05/27/2022 06:51:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7271681403831314 -> 0.7453385096964917 on epoch=34, global_step=1100
05/27/2022 06:51:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.32 on epoch=34
05/27/2022 06:51:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.23 on epoch=34
05/27/2022 06:51:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=35
05/27/2022 06:51:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=35
05/27/2022 06:51:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.34 on epoch=35
05/27/2022 06:51:50 - INFO - __main__ - Global step 1150 Train loss 0.34 Classification-F1 0.6128814154703524 on epoch=35
05/27/2022 06:51:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.31 on epoch=36
05/27/2022 06:51:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.44 on epoch=36
05/27/2022 06:51:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=36
05/27/2022 06:52:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=37
05/27/2022 06:52:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=37
05/27/2022 06:52:09 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.7905576400000937 on epoch=37
05/27/2022 06:52:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7453385096964917 -> 0.7905576400000937 on epoch=37, global_step=1200
05/27/2022 06:52:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.31 on epoch=37
05/27/2022 06:52:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.19 on epoch=38
05/27/2022 06:52:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.51 on epoch=38
05/27/2022 06:52:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.34 on epoch=38
05/27/2022 06:52:22 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=39
05/27/2022 06:52:29 - INFO - __main__ - Global step 1250 Train loss 0.31 Classification-F1 0.6602236963475314 on epoch=39
05/27/2022 06:52:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=39
05/27/2022 06:52:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=39
05/27/2022 06:52:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.31 on epoch=39
05/27/2022 06:52:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.33 on epoch=40
05/27/2022 06:52:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.35 on epoch=40
05/27/2022 06:52:48 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.7263149049157959 on epoch=40
05/27/2022 06:52:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.30 on epoch=40
05/27/2022 06:52:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.27 on epoch=41
05/27/2022 06:52:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=41
05/27/2022 06:52:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=41
05/27/2022 06:53:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.21 on epoch=42
05/27/2022 06:53:08 - INFO - __main__ - Global step 1350 Train loss 0.29 Classification-F1 0.6944529927078154 on epoch=42
05/27/2022 06:53:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.28 on epoch=42
05/27/2022 06:53:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=42
05/27/2022 06:53:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=43
05/27/2022 06:53:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=43
05/27/2022 06:53:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.28 on epoch=43
05/27/2022 06:53:28 - INFO - __main__ - Global step 1400 Train loss 0.31 Classification-F1 0.7332268976442005 on epoch=43
05/27/2022 06:53:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.23 on epoch=44
05/27/2022 06:53:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.33 on epoch=44
05/27/2022 06:53:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.33 on epoch=44
05/27/2022 06:53:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=44
05/27/2022 06:53:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=45
05/27/2022 06:53:48 - INFO - __main__ - Global step 1450 Train loss 0.28 Classification-F1 0.7762585861717712 on epoch=45
05/27/2022 06:53:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.22 on epoch=45
05/27/2022 06:53:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.26 on epoch=45
05/27/2022 06:53:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=46
05/27/2022 06:53:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=46
05/27/2022 06:54:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.30 on epoch=46
05/27/2022 06:54:07 - INFO - __main__ - Global step 1500 Train loss 0.27 Classification-F1 0.7582305118517265 on epoch=46
05/27/2022 06:54:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=47
05/27/2022 06:54:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.30 on epoch=47
05/27/2022 06:54:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=47
05/27/2022 06:54:18 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=48
05/27/2022 06:54:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=48
05/27/2022 06:54:27 - INFO - __main__ - Global step 1550 Train loss 0.31 Classification-F1 0.7875057091282016 on epoch=48
05/27/2022 06:54:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=48
05/27/2022 06:54:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=49
05/27/2022 06:54:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=49
05/27/2022 06:54:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.26 on epoch=49
05/27/2022 06:54:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.22 on epoch=49
05/27/2022 06:54:47 - INFO - __main__ - Global step 1600 Train loss 0.27 Classification-F1 0.7168477209116433 on epoch=49
05/27/2022 06:54:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=50
05/27/2022 06:54:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.24 on epoch=50
05/27/2022 06:54:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.26 on epoch=50
05/27/2022 06:54:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=51
05/27/2022 06:55:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.27 on epoch=51
05/27/2022 06:55:07 - INFO - __main__ - Global step 1650 Train loss 0.27 Classification-F1 0.7791169665511678 on epoch=51
05/27/2022 06:55:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.17 on epoch=51
05/27/2022 06:55:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.24 on epoch=52
05/27/2022 06:55:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.26 on epoch=52
05/27/2022 06:55:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.27 on epoch=52
05/27/2022 06:55:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=53
05/27/2022 06:55:26 - INFO - __main__ - Global step 1700 Train loss 0.22 Classification-F1 0.7948128607113432 on epoch=53
05/27/2022 06:55:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7905576400000937 -> 0.7948128607113432 on epoch=53, global_step=1700
05/27/2022 06:55:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=53
05/27/2022 06:55:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.29 on epoch=53
05/27/2022 06:55:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.19 on epoch=54
05/27/2022 06:55:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.30 on epoch=54
05/27/2022 06:55:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.23 on epoch=54
05/27/2022 06:55:46 - INFO - __main__ - Global step 1750 Train loss 0.26 Classification-F1 0.7761383338051921 on epoch=54
05/27/2022 06:55:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=54
05/27/2022 06:55:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.19 on epoch=55
05/27/2022 06:55:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=55
05/27/2022 06:55:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.18 on epoch=55
05/27/2022 06:55:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.16 on epoch=56
05/27/2022 06:56:06 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.7269394843490788 on epoch=56
05/27/2022 06:56:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.33 on epoch=56
05/27/2022 06:56:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=56
05/27/2022 06:56:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.27 on epoch=57
05/27/2022 06:56:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.25 on epoch=57
05/27/2022 06:56:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=57
05/27/2022 06:56:25 - INFO - __main__ - Global step 1850 Train loss 0.26 Classification-F1 0.7798010661251699 on epoch=57
05/27/2022 06:56:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.10 on epoch=58
05/27/2022 06:56:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=58
05/27/2022 06:56:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=58
05/27/2022 06:56:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=59
05/27/2022 06:56:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.25 on epoch=59
05/27/2022 06:56:45 - INFO - __main__ - Global step 1900 Train loss 0.21 Classification-F1 0.7739986158999429 on epoch=59
05/27/2022 06:56:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.24 on epoch=59
05/27/2022 06:56:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.15 on epoch=59
05/27/2022 06:56:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.22 on epoch=60
05/27/2022 06:56:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.26 on epoch=60
05/27/2022 06:56:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=60
05/27/2022 06:57:05 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.7457816691505217 on epoch=60
05/27/2022 06:57:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.15 on epoch=61
05/27/2022 06:57:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.24 on epoch=61
05/27/2022 06:57:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.16 on epoch=61
05/27/2022 06:57:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=62
05/27/2022 06:57:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.27 on epoch=62
05/27/2022 06:57:24 - INFO - __main__ - Global step 2000 Train loss 0.19 Classification-F1 0.7852718894565314 on epoch=62
05/27/2022 06:57:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.13 on epoch=62
05/27/2022 06:57:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.10 on epoch=63
05/27/2022 06:57:32 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=63
05/27/2022 06:57:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.18 on epoch=63
05/27/2022 06:57:37 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.11 on epoch=64
05/27/2022 06:57:44 - INFO - __main__ - Global step 2050 Train loss 0.14 Classification-F1 0.8005102926717249 on epoch=64
05/27/2022 06:57:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7948128607113432 -> 0.8005102926717249 on epoch=64, global_step=2050
05/27/2022 06:57:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.28 on epoch=64
05/27/2022 06:57:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.08 on epoch=64
05/27/2022 06:57:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.13 on epoch=64
05/27/2022 06:57:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.19 on epoch=65
05/27/2022 06:57:57 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.25 on epoch=65
05/27/2022 06:58:04 - INFO - __main__ - Global step 2100 Train loss 0.19 Classification-F1 0.7970065198032832 on epoch=65
05/27/2022 06:58:07 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.17 on epoch=65
05/27/2022 06:58:09 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=66
05/27/2022 06:58:12 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.22 on epoch=66
05/27/2022 06:58:14 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.20 on epoch=66
05/27/2022 06:58:17 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=67
05/27/2022 06:58:24 - INFO - __main__ - Global step 2150 Train loss 0.17 Classification-F1 0.7925452473055187 on epoch=67
05/27/2022 06:58:26 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.20 on epoch=67
05/27/2022 06:58:29 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.13 on epoch=67
05/27/2022 06:58:31 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.13 on epoch=68
05/27/2022 06:58:34 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.16 on epoch=68
05/27/2022 06:58:36 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.19 on epoch=68
05/27/2022 06:58:43 - INFO - __main__ - Global step 2200 Train loss 0.16 Classification-F1 0.7722885939093219 on epoch=68
05/27/2022 06:58:46 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.10 on epoch=69
05/27/2022 06:58:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=69
05/27/2022 06:58:51 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.16 on epoch=69
05/27/2022 06:58:53 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.12 on epoch=69
05/27/2022 06:58:56 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.19 on epoch=70
05/27/2022 06:59:03 - INFO - __main__ - Global step 2250 Train loss 0.16 Classification-F1 0.8092143388077474 on epoch=70
05/27/2022 06:59:03 - INFO - __main__ - Saving model with best Classification-F1: 0.8005102926717249 -> 0.8092143388077474 on epoch=70, global_step=2250
05/27/2022 06:59:06 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.19 on epoch=70
05/27/2022 06:59:08 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.26 on epoch=70
05/27/2022 06:59:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=71
05/27/2022 06:59:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.21 on epoch=71
05/27/2022 06:59:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.09 on epoch=71
05/27/2022 06:59:23 - INFO - __main__ - Global step 2300 Train loss 0.17 Classification-F1 0.7560011835574053 on epoch=71
05/27/2022 06:59:25 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=72
05/27/2022 06:59:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.20 on epoch=72
05/27/2022 06:59:30 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.09 on epoch=72
05/27/2022 06:59:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=73
05/27/2022 06:59:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.20 on epoch=73
05/27/2022 06:59:43 - INFO - __main__ - Global step 2350 Train loss 0.14 Classification-F1 0.8019306601775799 on epoch=73
05/27/2022 06:59:45 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=73
05/27/2022 06:59:48 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.10 on epoch=74
05/27/2022 06:59:50 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.26 on epoch=74
05/27/2022 06:59:53 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.12 on epoch=74
05/27/2022 06:59:55 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=74
05/27/2022 07:00:02 - INFO - __main__ - Global step 2400 Train loss 0.12 Classification-F1 0.7597704373424404 on epoch=74
05/27/2022 07:00:05 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.13 on epoch=75
05/27/2022 07:00:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=75
05/27/2022 07:00:10 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.16 on epoch=75
05/27/2022 07:00:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=76
05/27/2022 07:00:15 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.20 on epoch=76
05/27/2022 07:00:22 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.7644641614771926 on epoch=76
05/27/2022 07:00:25 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.10 on epoch=76
05/27/2022 07:00:27 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.11 on epoch=77
05/27/2022 07:00:30 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=77
05/27/2022 07:00:32 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.19 on epoch=77
05/27/2022 07:00:35 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.15 on epoch=78
05/27/2022 07:00:42 - INFO - __main__ - Global step 2500 Train loss 0.13 Classification-F1 0.7883778018251087 on epoch=78
05/27/2022 07:00:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.12 on epoch=78
05/27/2022 07:00:47 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.12 on epoch=78
05/27/2022 07:00:49 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=79
05/27/2022 07:00:52 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.21 on epoch=79
05/27/2022 07:00:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.13 on epoch=79
05/27/2022 07:01:01 - INFO - __main__ - Global step 2550 Train loss 0.13 Classification-F1 0.7565052263730694 on epoch=79
05/27/2022 07:01:04 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=79
05/27/2022 07:01:06 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.16 on epoch=80
05/27/2022 07:01:09 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.20 on epoch=80
05/27/2022 07:01:12 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.11 on epoch=80
05/27/2022 07:01:14 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.14 on epoch=81
05/27/2022 07:01:21 - INFO - __main__ - Global step 2600 Train loss 0.15 Classification-F1 0.7589143388856684 on epoch=81
05/27/2022 07:01:24 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.15 on epoch=81
05/27/2022 07:01:26 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.11 on epoch=81
05/27/2022 07:01:29 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.08 on epoch=82
05/27/2022 07:01:31 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.13 on epoch=82
05/27/2022 07:01:34 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.10 on epoch=82
05/27/2022 07:01:41 - INFO - __main__ - Global step 2650 Train loss 0.11 Classification-F1 0.7682042367417437 on epoch=82
05/27/2022 07:01:43 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=83
05/27/2022 07:01:46 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.16 on epoch=83
05/27/2022 07:01:48 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.19 on epoch=83
05/27/2022 07:01:51 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=84
05/27/2022 07:01:53 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.22 on epoch=84
05/27/2022 07:02:00 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.7724992415191505 on epoch=84
05/27/2022 07:02:03 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.15 on epoch=84
05/27/2022 07:02:05 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.11 on epoch=84
05/27/2022 07:02:08 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.18 on epoch=85
05/27/2022 07:02:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.14 on epoch=85
05/27/2022 07:02:13 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.15 on epoch=85
05/27/2022 07:02:20 - INFO - __main__ - Global step 2750 Train loss 0.15 Classification-F1 0.7866107885833284 on epoch=85
05/27/2022 07:02:22 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=86
05/27/2022 07:02:25 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=86
05/27/2022 07:02:28 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=86
05/27/2022 07:02:30 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.09 on epoch=87
05/27/2022 07:02:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.14 on epoch=87
05/27/2022 07:02:40 - INFO - __main__ - Global step 2800 Train loss 0.10 Classification-F1 0.8142388591800357 on epoch=87
05/27/2022 07:02:40 - INFO - __main__ - Saving model with best Classification-F1: 0.8092143388077474 -> 0.8142388591800357 on epoch=87, global_step=2800
05/27/2022 07:02:42 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.14 on epoch=87
05/27/2022 07:02:45 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=88
05/27/2022 07:02:48 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.16 on epoch=88
05/27/2022 07:02:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.12 on epoch=88
05/27/2022 07:02:53 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=89
05/27/2022 07:02:59 - INFO - __main__ - Global step 2850 Train loss 0.11 Classification-F1 0.7512140763116488 on epoch=89
05/27/2022 07:03:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.14 on epoch=89
05/27/2022 07:03:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.07 on epoch=89
05/27/2022 07:03:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=89
05/27/2022 07:03:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=90
05/27/2022 07:03:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=90
05/27/2022 07:03:19 - INFO - __main__ - Global step 2900 Train loss 0.08 Classification-F1 0.8031620591262485 on epoch=90
05/27/2022 07:03:22 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=90
05/27/2022 07:03:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=91
05/27/2022 07:03:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.10 on epoch=91
05/27/2022 07:03:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=91
05/27/2022 07:03:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.13 on epoch=92
05/27/2022 07:03:39 - INFO - __main__ - Global step 2950 Train loss 0.08 Classification-F1 0.7703823661718399 on epoch=92
05/27/2022 07:03:41 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=92
05/27/2022 07:03:44 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.08 on epoch=92
05/27/2022 07:03:46 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.10 on epoch=93
05/27/2022 07:03:49 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.13 on epoch=93
05/27/2022 07:03:51 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.13 on epoch=93
05/27/2022 07:03:53 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 07:03:53 - INFO - __main__ - Printing 3 examples
05/27/2022 07:03:53 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/27/2022 07:03:53 - INFO - __main__ - ['others']
05/27/2022 07:03:53 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/27/2022 07:03:53 - INFO - __main__ - ['others']
05/27/2022 07:03:53 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/27/2022 07:03:53 - INFO - __main__ - ['others']
05/27/2022 07:03:53 - INFO - __main__ - Tokenizing Input ...
05/27/2022 07:03:53 - INFO - __main__ - Tokenizing Output ...
05/27/2022 07:03:53 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 07:03:53 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 07:03:53 - INFO - __main__ - Printing 3 examples
05/27/2022 07:03:53 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/27/2022 07:03:53 - INFO - __main__ - ['others']
05/27/2022 07:03:53 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/27/2022 07:03:53 - INFO - __main__ - ['others']
05/27/2022 07:03:53 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/27/2022 07:03:53 - INFO - __main__ - ['others']
05/27/2022 07:03:53 - INFO - __main__ - Tokenizing Input ...
05/27/2022 07:03:53 - INFO - __main__ - Tokenizing Output ...
05/27/2022 07:03:54 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 07:03:58 - INFO - __main__ - Global step 3000 Train loss 0.11 Classification-F1 0.7952432576362737 on epoch=93
05/27/2022 07:03:58 - INFO - __main__ - save last model!
05/27/2022 07:03:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 07:03:58 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 07:03:58 - INFO - __main__ - Printing 3 examples
05/27/2022 07:03:58 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 07:03:58 - INFO - __main__ - ['others']
05/27/2022 07:03:58 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 07:03:58 - INFO - __main__ - ['others']
05/27/2022 07:03:58 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 07:03:58 - INFO - __main__ - ['others']
05/27/2022 07:03:58 - INFO - __main__ - Tokenizing Input ...
05/27/2022 07:04:00 - INFO - __main__ - Tokenizing Output ...
05/27/2022 07:04:06 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 07:04:12 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 07:04:12 - INFO - __main__ - task name: emo
05/27/2022 07:04:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 07:04:13 - INFO - __main__ - Starting training!
05/27/2022 07:05:20 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_13_0.4_8_predictions.txt
05/27/2022 07:05:20 - INFO - __main__ - Classification-F1 on test data: 0.4987
05/27/2022 07:05:20 - INFO - __main__ - prefix=emo_128_13, lr=0.4, bsz=8, dev_performance=0.8142388591800357, test_performance=0.4986858520137063
05/27/2022 07:05:20 - INFO - __main__ - Running ... prefix=emo_128_13, lr=0.3, bsz=8 ...
05/27/2022 07:05:21 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 07:05:21 - INFO - __main__ - Printing 3 examples
05/27/2022 07:05:21 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/27/2022 07:05:21 - INFO - __main__ - ['others']
05/27/2022 07:05:21 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/27/2022 07:05:21 - INFO - __main__ - ['others']
05/27/2022 07:05:21 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/27/2022 07:05:21 - INFO - __main__ - ['others']
05/27/2022 07:05:21 - INFO - __main__ - Tokenizing Input ...
05/27/2022 07:05:22 - INFO - __main__ - Tokenizing Output ...
05/27/2022 07:05:22 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 07:05:22 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 07:05:22 - INFO - __main__ - Printing 3 examples
05/27/2022 07:05:22 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/27/2022 07:05:22 - INFO - __main__ - ['others']
05/27/2022 07:05:22 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/27/2022 07:05:22 - INFO - __main__ - ['others']
05/27/2022 07:05:22 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/27/2022 07:05:22 - INFO - __main__ - ['others']
05/27/2022 07:05:22 - INFO - __main__ - Tokenizing Input ...
05/27/2022 07:05:22 - INFO - __main__ - Tokenizing Output ...
05/27/2022 07:05:23 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 07:05:38 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 07:05:38 - INFO - __main__ - task name: emo
05/27/2022 07:05:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 07:05:40 - INFO - __main__ - Starting training!
05/27/2022 07:05:42 - INFO - __main__ - Step 10 Global step 10 Train loss 6.89 on epoch=0
05/27/2022 07:05:45 - INFO - __main__ - Step 20 Global step 20 Train loss 3.27 on epoch=0
05/27/2022 07:05:47 - INFO - __main__ - Step 30 Global step 30 Train loss 1.85 on epoch=0
05/27/2022 07:05:50 - INFO - __main__ - Step 40 Global step 40 Train loss 1.21 on epoch=1
05/27/2022 07:05:52 - INFO - __main__ - Step 50 Global step 50 Train loss 1.16 on epoch=1
05/27/2022 07:05:59 - INFO - __main__ - Global step 50 Train loss 2.88 Classification-F1 0.19122905523212858 on epoch=1
05/27/2022 07:05:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.19122905523212858 on epoch=1, global_step=50
05/27/2022 07:06:02 - INFO - __main__ - Step 60 Global step 60 Train loss 1.06 on epoch=1
05/27/2022 07:06:04 - INFO - __main__ - Step 70 Global step 70 Train loss 0.97 on epoch=2
05/27/2022 07:06:07 - INFO - __main__ - Step 80 Global step 80 Train loss 1.02 on epoch=2
05/27/2022 07:06:09 - INFO - __main__ - Step 90 Global step 90 Train loss 0.95 on epoch=2
05/27/2022 07:06:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.98 on epoch=3
05/27/2022 07:06:18 - INFO - __main__ - Global step 100 Train loss 1.00 Classification-F1 0.17676987646499845 on epoch=3
05/27/2022 07:06:21 - INFO - __main__ - Step 110 Global step 110 Train loss 0.93 on epoch=3
05/27/2022 07:06:23 - INFO - __main__ - Step 120 Global step 120 Train loss 1.07 on epoch=3
05/27/2022 07:06:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.97 on epoch=4
05/27/2022 07:06:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.94 on epoch=4
05/27/2022 07:06:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.99 on epoch=4
05/27/2022 07:06:38 - INFO - __main__ - Global step 150 Train loss 0.98 Classification-F1 0.21067195251264986 on epoch=4
05/27/2022 07:06:38 - INFO - __main__ - Saving model with best Classification-F1: 0.19122905523212858 -> 0.21067195251264986 on epoch=4, global_step=150
05/27/2022 07:06:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.90 on epoch=4
05/27/2022 07:06:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.88 on epoch=5
05/27/2022 07:06:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.95 on epoch=5
05/27/2022 07:06:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.95 on epoch=5
05/27/2022 07:06:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.89 on epoch=6
05/27/2022 07:06:57 - INFO - __main__ - Global step 200 Train loss 0.91 Classification-F1 0.10867323019221753 on epoch=6
05/27/2022 07:06:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.91 on epoch=6
05/27/2022 07:07:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.85 on epoch=6
05/27/2022 07:07:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.89 on epoch=7
05/27/2022 07:07:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=7
05/27/2022 07:07:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.79 on epoch=7
05/27/2022 07:07:16 - INFO - __main__ - Global step 250 Train loss 0.86 Classification-F1 0.14096184169535947 on epoch=7
05/27/2022 07:07:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.77 on epoch=8
05/27/2022 07:07:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.94 on epoch=8
05/27/2022 07:07:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.85 on epoch=8
05/27/2022 07:07:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.80 on epoch=9
05/27/2022 07:07:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.91 on epoch=9
05/27/2022 07:07:36 - INFO - __main__ - Global step 300 Train loss 0.85 Classification-F1 0.21211272726164482 on epoch=9
05/27/2022 07:07:36 - INFO - __main__ - Saving model with best Classification-F1: 0.21067195251264986 -> 0.21211272726164482 on epoch=9, global_step=300
05/27/2022 07:07:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.84 on epoch=9
05/27/2022 07:07:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.83 on epoch=9
05/27/2022 07:07:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.81 on epoch=10
05/27/2022 07:07:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.79 on epoch=10
05/27/2022 07:07:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.89 on epoch=10
05/27/2022 07:07:55 - INFO - __main__ - Global step 350 Train loss 0.83 Classification-F1 0.3602490297212484 on epoch=10
05/27/2022 07:07:55 - INFO - __main__ - Saving model with best Classification-F1: 0.21211272726164482 -> 0.3602490297212484 on epoch=10, global_step=350
05/27/2022 07:07:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.80 on epoch=11
05/27/2022 07:08:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.91 on epoch=11
05/27/2022 07:08:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.87 on epoch=11
05/27/2022 07:08:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.83 on epoch=12
05/27/2022 07:08:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.77 on epoch=12
05/27/2022 07:08:14 - INFO - __main__ - Global step 400 Train loss 0.84 Classification-F1 0.23435241033196744 on epoch=12
05/27/2022 07:08:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.83 on epoch=12
05/27/2022 07:08:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.76 on epoch=13
05/27/2022 07:08:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.86 on epoch=13
05/27/2022 07:08:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.84 on epoch=13
05/27/2022 07:08:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.80 on epoch=14
05/27/2022 07:08:33 - INFO - __main__ - Global step 450 Train loss 0.82 Classification-F1 0.1466528485301194 on epoch=14
05/27/2022 07:08:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.87 on epoch=14
05/27/2022 07:08:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.80 on epoch=14
05/27/2022 07:08:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.78 on epoch=14
05/27/2022 07:08:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.86 on epoch=15
05/27/2022 07:08:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.80 on epoch=15
05/27/2022 07:08:52 - INFO - __main__ - Global step 500 Train loss 0.82 Classification-F1 0.2729477934968206 on epoch=15
05/27/2022 07:08:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.82 on epoch=15
05/27/2022 07:08:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.74 on epoch=16
05/27/2022 07:09:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.81 on epoch=16
05/27/2022 07:09:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.84 on epoch=16
05/27/2022 07:09:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=17
05/27/2022 07:09:12 - INFO - __main__ - Global step 550 Train loss 0.81 Classification-F1 0.1373872476001324 on epoch=17
05/27/2022 07:09:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.84 on epoch=17
05/27/2022 07:09:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.78 on epoch=17
05/27/2022 07:09:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.84 on epoch=18
05/27/2022 07:09:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.77 on epoch=18
05/27/2022 07:09:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.79 on epoch=18
05/27/2022 07:09:31 - INFO - __main__ - Global step 600 Train loss 0.80 Classification-F1 0.356010261143754 on epoch=18
05/27/2022 07:09:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.83 on epoch=19
05/27/2022 07:09:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.78 on epoch=19
05/27/2022 07:09:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.78 on epoch=19
05/27/2022 07:09:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.74 on epoch=19
05/27/2022 07:09:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.75 on epoch=20
05/27/2022 07:09:50 - INFO - __main__ - Global step 650 Train loss 0.78 Classification-F1 0.3862029053359864 on epoch=20
05/27/2022 07:09:50 - INFO - __main__ - Saving model with best Classification-F1: 0.3602490297212484 -> 0.3862029053359864 on epoch=20, global_step=650
05/27/2022 07:09:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.79 on epoch=20
05/27/2022 07:09:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.70 on epoch=20
05/27/2022 07:09:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.71 on epoch=21
05/27/2022 07:10:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.75 on epoch=21
05/27/2022 07:10:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.63 on epoch=21
05/27/2022 07:10:10 - INFO - __main__ - Global step 700 Train loss 0.71 Classification-F1 0.5164987054277927 on epoch=21
05/27/2022 07:10:10 - INFO - __main__ - Saving model with best Classification-F1: 0.3862029053359864 -> 0.5164987054277927 on epoch=21, global_step=700
05/27/2022 07:10:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.69 on epoch=22
05/27/2022 07:10:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.72 on epoch=22
05/27/2022 07:10:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.64 on epoch=22
05/27/2022 07:10:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.55 on epoch=23
05/27/2022 07:10:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.69 on epoch=23
05/27/2022 07:10:29 - INFO - __main__ - Global step 750 Train loss 0.66 Classification-F1 0.6281713664997434 on epoch=23
05/27/2022 07:10:29 - INFO - __main__ - Saving model with best Classification-F1: 0.5164987054277927 -> 0.6281713664997434 on epoch=23, global_step=750
05/27/2022 07:10:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.64 on epoch=23
05/27/2022 07:10:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.59 on epoch=24
05/27/2022 07:10:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.74 on epoch=24
05/27/2022 07:10:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.58 on epoch=24
05/27/2022 07:10:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.55 on epoch=24
05/27/2022 07:10:48 - INFO - __main__ - Global step 800 Train loss 0.62 Classification-F1 0.57042404293978 on epoch=24
05/27/2022 07:10:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.56 on epoch=25
05/27/2022 07:10:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.62 on epoch=25
05/27/2022 07:10:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.59 on epoch=25
05/27/2022 07:10:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.52 on epoch=26
05/27/2022 07:11:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.61 on epoch=26
05/27/2022 07:11:07 - INFO - __main__ - Global step 850 Train loss 0.58 Classification-F1 0.4636805076394118 on epoch=26
05/27/2022 07:11:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.63 on epoch=26
05/27/2022 07:11:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=27
05/27/2022 07:11:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.58 on epoch=27
05/27/2022 07:11:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.55 on epoch=27
05/27/2022 07:11:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.47 on epoch=28
05/27/2022 07:11:27 - INFO - __main__ - Global step 900 Train loss 0.55 Classification-F1 0.6235286798556676 on epoch=28
05/27/2022 07:11:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.52 on epoch=28
05/27/2022 07:11:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.52 on epoch=28
05/27/2022 07:11:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=29
05/27/2022 07:11:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.67 on epoch=29
05/27/2022 07:11:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.55 on epoch=29
05/27/2022 07:11:46 - INFO - __main__ - Global step 950 Train loss 0.54 Classification-F1 0.6404318348713367 on epoch=29
05/27/2022 07:11:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6281713664997434 -> 0.6404318348713367 on epoch=29, global_step=950
05/27/2022 07:11:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=29
05/27/2022 07:11:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=30
05/27/2022 07:11:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.45 on epoch=30
05/27/2022 07:11:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=30
05/27/2022 07:11:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=31
05/27/2022 07:12:06 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.693124458661543 on epoch=31
05/27/2022 07:12:06 - INFO - __main__ - Saving model with best Classification-F1: 0.6404318348713367 -> 0.693124458661543 on epoch=31, global_step=1000
05/27/2022 07:12:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=31
05/27/2022 07:12:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.53 on epoch=31
05/27/2022 07:12:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=32
05/27/2022 07:12:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.51 on epoch=32
05/27/2022 07:12:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=32
05/27/2022 07:12:25 - INFO - __main__ - Global step 1050 Train loss 0.46 Classification-F1 0.6363562874722535 on epoch=32
05/27/2022 07:12:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=33
05/27/2022 07:12:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.52 on epoch=33
05/27/2022 07:12:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.57 on epoch=33
05/27/2022 07:12:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.33 on epoch=34
05/27/2022 07:12:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.47 on epoch=34
05/27/2022 07:12:44 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.7023554206078478 on epoch=34
05/27/2022 07:12:44 - INFO - __main__ - Saving model with best Classification-F1: 0.693124458661543 -> 0.7023554206078478 on epoch=34, global_step=1100
05/27/2022 07:12:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.54 on epoch=34
05/27/2022 07:12:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=34
05/27/2022 07:12:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=35
05/27/2022 07:12:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.46 on epoch=35
05/27/2022 07:12:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=35
05/27/2022 07:13:04 - INFO - __main__ - Global step 1150 Train loss 0.45 Classification-F1 0.7223630761995138 on epoch=35
05/27/2022 07:13:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7023554206078478 -> 0.7223630761995138 on epoch=35, global_step=1150
05/27/2022 07:13:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.28 on epoch=36
05/27/2022 07:13:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.52 on epoch=36
05/27/2022 07:13:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=36
05/27/2022 07:13:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.33 on epoch=37
05/27/2022 07:13:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=37
05/27/2022 07:13:23 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.7844362629714347 on epoch=37
05/27/2022 07:13:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7223630761995138 -> 0.7844362629714347 on epoch=37, global_step=1200
05/27/2022 07:13:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=37
05/27/2022 07:13:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.30 on epoch=38
05/27/2022 07:13:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=38
05/27/2022 07:13:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=38
05/27/2022 07:13:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.27 on epoch=39
05/27/2022 07:13:42 - INFO - __main__ - Global step 1250 Train loss 0.35 Classification-F1 0.7014040784954181 on epoch=39
05/27/2022 07:13:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=39
05/27/2022 07:13:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=39
05/27/2022 07:13:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=39
05/27/2022 07:13:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.32 on epoch=40
05/27/2022 07:13:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.31 on epoch=40
05/27/2022 07:14:02 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.7362688233008243 on epoch=40
05/27/2022 07:14:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=40
05/27/2022 07:14:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=41
05/27/2022 07:14:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.38 on epoch=41
05/27/2022 07:14:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=41
05/27/2022 07:14:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.32 on epoch=42
05/27/2022 07:14:21 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.7738794250945452 on epoch=42
05/27/2022 07:14:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=42
05/27/2022 07:14:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=42
05/27/2022 07:14:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.26 on epoch=43
05/27/2022 07:14:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.42 on epoch=43
05/27/2022 07:14:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=43
05/27/2022 07:14:41 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.759503337696815 on epoch=43
05/27/2022 07:14:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=44
05/27/2022 07:14:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=44
05/27/2022 07:14:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.38 on epoch=44
05/27/2022 07:14:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.26 on epoch=44
05/27/2022 07:14:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.32 on epoch=45
05/27/2022 07:15:00 - INFO - __main__ - Global step 1450 Train loss 0.32 Classification-F1 0.7954670385143725 on epoch=45
05/27/2022 07:15:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7844362629714347 -> 0.7954670385143725 on epoch=45, global_step=1450
05/27/2022 07:15:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=45
05/27/2022 07:15:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=45
05/27/2022 07:15:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.28 on epoch=46
05/27/2022 07:15:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=46
05/27/2022 07:15:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.32 on epoch=46
05/27/2022 07:15:19 - INFO - __main__ - Global step 1500 Train loss 0.33 Classification-F1 0.7878345411701206 on epoch=46
05/27/2022 07:15:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.28 on epoch=47
05/27/2022 07:15:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=47
05/27/2022 07:15:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.36 on epoch=47
05/27/2022 07:15:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=48
05/27/2022 07:15:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.31 on epoch=48
05/27/2022 07:15:39 - INFO - __main__ - Global step 1550 Train loss 0.29 Classification-F1 0.7858950625208617 on epoch=48
05/27/2022 07:15:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=48
05/27/2022 07:15:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=49
05/27/2022 07:15:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.29 on epoch=49
05/27/2022 07:15:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.26 on epoch=49
05/27/2022 07:15:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.25 on epoch=49
05/27/2022 07:15:58 - INFO - __main__ - Global step 1600 Train loss 0.26 Classification-F1 0.8062059676750933 on epoch=49
05/27/2022 07:15:58 - INFO - __main__ - Saving model with best Classification-F1: 0.7954670385143725 -> 0.8062059676750933 on epoch=49, global_step=1600
05/27/2022 07:16:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.23 on epoch=50
05/27/2022 07:16:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=50
05/27/2022 07:16:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.25 on epoch=50
05/27/2022 07:16:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.28 on epoch=51
05/27/2022 07:16:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=51
05/27/2022 07:16:17 - INFO - __main__ - Global step 1650 Train loss 0.25 Classification-F1 0.792944767040942 on epoch=51
05/27/2022 07:16:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.25 on epoch=51
05/27/2022 07:16:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.27 on epoch=52
05/27/2022 07:16:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=52
05/27/2022 07:16:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.24 on epoch=52
05/27/2022 07:16:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.14 on epoch=53
05/27/2022 07:16:37 - INFO - __main__ - Global step 1700 Train loss 0.24 Classification-F1 0.7457057720744996 on epoch=53
05/27/2022 07:16:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=53
05/27/2022 07:16:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=53
05/27/2022 07:16:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.16 on epoch=54
05/27/2022 07:16:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.25 on epoch=54
05/27/2022 07:16:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.29 on epoch=54
05/27/2022 07:16:56 - INFO - __main__ - Global step 1750 Train loss 0.26 Classification-F1 0.8036978530497972 on epoch=54
05/27/2022 07:16:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=54
05/27/2022 07:17:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.22 on epoch=55
05/27/2022 07:17:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.26 on epoch=55
05/27/2022 07:17:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.26 on epoch=55
05/27/2022 07:17:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=56
05/27/2022 07:17:15 - INFO - __main__ - Global step 1800 Train loss 0.22 Classification-F1 0.7928761682727146 on epoch=56
05/27/2022 07:17:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.24 on epoch=56
05/27/2022 07:17:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=56
05/27/2022 07:17:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.15 on epoch=57
05/27/2022 07:17:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.25 on epoch=57
05/27/2022 07:17:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.25 on epoch=57
05/27/2022 07:17:35 - INFO - __main__ - Global step 1850 Train loss 0.23 Classification-F1 0.7797406736676417 on epoch=57
05/27/2022 07:17:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.15 on epoch=58
05/27/2022 07:17:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.24 on epoch=58
05/27/2022 07:17:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.25 on epoch=58
05/27/2022 07:17:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.12 on epoch=59
05/27/2022 07:17:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.21 on epoch=59
05/27/2022 07:17:54 - INFO - __main__ - Global step 1900 Train loss 0.19 Classification-F1 0.7914474385454872 on epoch=59
05/27/2022 07:17:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.29 on epoch=59
05/27/2022 07:17:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.15 on epoch=59
05/27/2022 07:18:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.17 on epoch=60
05/27/2022 07:18:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.17 on epoch=60
05/27/2022 07:18:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.26 on epoch=60
05/27/2022 07:18:13 - INFO - __main__ - Global step 1950 Train loss 0.21 Classification-F1 0.7980847233959266 on epoch=60
05/27/2022 07:18:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.25 on epoch=61
05/27/2022 07:18:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.26 on epoch=61
05/27/2022 07:18:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=61
05/27/2022 07:18:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=62
05/27/2022 07:18:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.18 on epoch=62
05/27/2022 07:18:33 - INFO - __main__ - Global step 2000 Train loss 0.22 Classification-F1 0.7891390800529755 on epoch=62
05/27/2022 07:18:35 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.24 on epoch=62
05/27/2022 07:18:38 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=63
05/27/2022 07:18:40 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.17 on epoch=63
05/27/2022 07:18:43 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.31 on epoch=63
05/27/2022 07:18:45 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.14 on epoch=64
05/27/2022 07:18:52 - INFO - __main__ - Global step 2050 Train loss 0.20 Classification-F1 0.8113181929492431 on epoch=64
05/27/2022 07:18:52 - INFO - __main__ - Saving model with best Classification-F1: 0.8062059676750933 -> 0.8113181929492431 on epoch=64, global_step=2050
05/27/2022 07:18:54 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.21 on epoch=64
05/27/2022 07:18:57 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.23 on epoch=64
05/27/2022 07:18:59 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.10 on epoch=64
05/27/2022 07:19:02 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.15 on epoch=65
05/27/2022 07:19:05 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.22 on epoch=65
05/27/2022 07:19:11 - INFO - __main__ - Global step 2100 Train loss 0.18 Classification-F1 0.7885193740130654 on epoch=65
05/27/2022 07:19:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.18 on epoch=65
05/27/2022 07:19:16 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=66
05/27/2022 07:19:19 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.26 on epoch=66
05/27/2022 07:19:21 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.18 on epoch=66
05/27/2022 07:19:24 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=67
05/27/2022 07:19:31 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.7904890411298747 on epoch=67
05/27/2022 07:19:33 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.12 on epoch=67
05/27/2022 07:19:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.17 on epoch=67
05/27/2022 07:19:38 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.08 on epoch=68
05/27/2022 07:19:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.21 on epoch=68
05/27/2022 07:19:43 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.25 on epoch=68
05/27/2022 07:19:50 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.7972215086908125 on epoch=68
05/27/2022 07:19:53 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.16 on epoch=69
05/27/2022 07:19:55 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=69
05/27/2022 07:19:58 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=69
05/27/2022 07:20:00 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.18 on epoch=69
05/27/2022 07:20:03 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.16 on epoch=70
05/27/2022 07:20:09 - INFO - __main__ - Global step 2250 Train loss 0.18 Classification-F1 0.775027001517594 on epoch=70
05/27/2022 07:20:12 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.20 on epoch=70
05/27/2022 07:20:15 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=70
05/27/2022 07:20:17 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.14 on epoch=71
05/27/2022 07:20:20 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.23 on epoch=71
05/27/2022 07:20:22 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.20 on epoch=71
05/27/2022 07:20:29 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.7865506045794038 on epoch=71
05/27/2022 07:20:31 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.10 on epoch=72
05/27/2022 07:20:34 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.23 on epoch=72
05/27/2022 07:20:36 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.23 on epoch=72
05/27/2022 07:20:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.10 on epoch=73
05/27/2022 07:20:42 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.16 on epoch=73
05/27/2022 07:20:48 - INFO - __main__ - Global step 2350 Train loss 0.16 Classification-F1 0.7970219276144637 on epoch=73
05/27/2022 07:20:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.24 on epoch=73
05/27/2022 07:20:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.18 on epoch=74
05/27/2022 07:20:56 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.17 on epoch=74
05/27/2022 07:20:58 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.25 on epoch=74
05/27/2022 07:21:01 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.18 on epoch=74
05/27/2022 07:21:08 - INFO - __main__ - Global step 2400 Train loss 0.20 Classification-F1 0.8098979698893832 on epoch=74
05/27/2022 07:21:10 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=75
05/27/2022 07:21:13 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.12 on epoch=75
05/27/2022 07:21:15 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=75
05/27/2022 07:21:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.07 on epoch=76
05/27/2022 07:21:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.21 on epoch=76
05/27/2022 07:21:27 - INFO - __main__ - Global step 2450 Train loss 0.12 Classification-F1 0.8015534194595886 on epoch=76
05/27/2022 07:21:30 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.19 on epoch=76
05/27/2022 07:21:32 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=77
05/27/2022 07:21:35 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.26 on epoch=77
05/27/2022 07:21:37 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.12 on epoch=77
05/27/2022 07:21:40 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.13 on epoch=78
05/27/2022 07:21:47 - INFO - __main__ - Global step 2500 Train loss 0.17 Classification-F1 0.7813103978742746 on epoch=78
05/27/2022 07:21:49 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.17 on epoch=78
05/27/2022 07:21:52 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.26 on epoch=78
05/27/2022 07:21:54 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.11 on epoch=79
05/27/2022 07:21:57 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.12 on epoch=79
05/27/2022 07:21:59 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.21 on epoch=79
05/27/2022 07:22:06 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.7964605597839397 on epoch=79
05/27/2022 07:22:08 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.14 on epoch=79
05/27/2022 07:22:11 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.14 on epoch=80
05/27/2022 07:22:13 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.14 on epoch=80
05/27/2022 07:22:16 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=80
05/27/2022 07:22:18 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.11 on epoch=81
05/27/2022 07:22:25 - INFO - __main__ - Global step 2600 Train loss 0.15 Classification-F1 0.7937175488391154 on epoch=81
05/27/2022 07:22:28 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.14 on epoch=81
05/27/2022 07:22:30 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.13 on epoch=81
05/27/2022 07:22:33 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.13 on epoch=82
05/27/2022 07:22:35 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.09 on epoch=82
05/27/2022 07:22:38 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.16 on epoch=82
05/27/2022 07:22:45 - INFO - __main__ - Global step 2650 Train loss 0.13 Classification-F1 0.7937062001818862 on epoch=82
05/27/2022 07:22:47 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=83
05/27/2022 07:22:50 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.11 on epoch=83
05/27/2022 07:22:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.14 on epoch=83
05/27/2022 07:22:55 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=84
05/27/2022 07:22:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.14 on epoch=84
05/27/2022 07:23:04 - INFO - __main__ - Global step 2700 Train loss 0.11 Classification-F1 0.8071346822482732 on epoch=84
05/27/2022 07:23:06 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.15 on epoch=84
05/27/2022 07:23:09 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=84
05/27/2022 07:23:11 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.11 on epoch=85
05/27/2022 07:23:14 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.18 on epoch=85
05/27/2022 07:23:16 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.12 on epoch=85
05/27/2022 07:23:23 - INFO - __main__ - Global step 2750 Train loss 0.13 Classification-F1 0.7796089801398058 on epoch=85
05/27/2022 07:23:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=86
05/27/2022 07:23:28 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.12 on epoch=86
05/27/2022 07:23:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=86
05/27/2022 07:23:33 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.11 on epoch=87
05/27/2022 07:23:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.14 on epoch=87
05/27/2022 07:23:43 - INFO - __main__ - Global step 2800 Train loss 0.10 Classification-F1 0.8018542307035654 on epoch=87
05/27/2022 07:23:45 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=87
05/27/2022 07:23:48 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=88
05/27/2022 07:23:50 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.15 on epoch=88
05/27/2022 07:23:53 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.18 on epoch=88
05/27/2022 07:23:55 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.10 on epoch=89
05/27/2022 07:24:02 - INFO - __main__ - Global step 2850 Train loss 0.11 Classification-F1 0.7788410415176257 on epoch=89
05/27/2022 07:24:05 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.13 on epoch=89
05/27/2022 07:24:07 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.11 on epoch=89
05/27/2022 07:24:10 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.11 on epoch=89
05/27/2022 07:24:12 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.17 on epoch=90
05/27/2022 07:24:15 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=90
05/27/2022 07:24:21 - INFO - __main__ - Global step 2900 Train loss 0.12 Classification-F1 0.8031468952990185 on epoch=90
05/27/2022 07:24:24 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.16 on epoch=90
05/27/2022 07:24:26 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.08 on epoch=91
05/27/2022 07:24:29 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.20 on epoch=91
05/27/2022 07:24:31 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=91
05/27/2022 07:24:34 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.10 on epoch=92
05/27/2022 07:24:41 - INFO - __main__ - Global step 2950 Train loss 0.13 Classification-F1 0.7891411795459052 on epoch=92
05/27/2022 07:24:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.11 on epoch=92
05/27/2022 07:24:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.11 on epoch=92
05/27/2022 07:24:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=93
05/27/2022 07:24:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.27 on epoch=93
05/27/2022 07:24:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=93
05/27/2022 07:24:55 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 07:24:55 - INFO - __main__ - Printing 3 examples
05/27/2022 07:24:55 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/27/2022 07:24:55 - INFO - __main__ - ['others']
05/27/2022 07:24:55 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/27/2022 07:24:55 - INFO - __main__ - ['others']
05/27/2022 07:24:55 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/27/2022 07:24:55 - INFO - __main__ - ['others']
05/27/2022 07:24:55 - INFO - __main__ - Tokenizing Input ...
05/27/2022 07:24:55 - INFO - __main__ - Tokenizing Output ...
05/27/2022 07:24:55 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 07:24:55 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 07:24:55 - INFO - __main__ - Printing 3 examples
05/27/2022 07:24:55 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/27/2022 07:24:55 - INFO - __main__ - ['others']
05/27/2022 07:24:55 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/27/2022 07:24:55 - INFO - __main__ - ['others']
05/27/2022 07:24:55 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/27/2022 07:24:55 - INFO - __main__ - ['others']
05/27/2022 07:24:55 - INFO - __main__ - Tokenizing Input ...
05/27/2022 07:24:56 - INFO - __main__ - Tokenizing Output ...
05/27/2022 07:24:56 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 07:25:00 - INFO - __main__ - Global step 3000 Train loss 0.13 Classification-F1 0.7610440146815718 on epoch=93
05/27/2022 07:25:00 - INFO - __main__ - save last model!
05/27/2022 07:25:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 07:25:00 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 07:25:00 - INFO - __main__ - Printing 3 examples
05/27/2022 07:25:00 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 07:25:00 - INFO - __main__ - ['others']
05/27/2022 07:25:00 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 07:25:00 - INFO - __main__ - ['others']
05/27/2022 07:25:00 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 07:25:00 - INFO - __main__ - ['others']
05/27/2022 07:25:00 - INFO - __main__ - Tokenizing Input ...
05/27/2022 07:25:03 - INFO - __main__ - Tokenizing Output ...
05/27/2022 07:25:08 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 07:25:13 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 07:25:13 - INFO - __main__ - task name: emo
05/27/2022 07:25:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 07:25:14 - INFO - __main__ - Starting training!
05/27/2022 07:26:20 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_13_0.3_8_predictions.txt
05/27/2022 07:26:20 - INFO - __main__ - Classification-F1 on test data: 0.4928
05/27/2022 07:26:20 - INFO - __main__ - prefix=emo_128_13, lr=0.3, bsz=8, dev_performance=0.8113181929492431, test_performance=0.4928154338794287
05/27/2022 07:26:20 - INFO - __main__ - Running ... prefix=emo_128_13, lr=0.2, bsz=8 ...
05/27/2022 07:26:21 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 07:26:21 - INFO - __main__ - Printing 3 examples
05/27/2022 07:26:21 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/27/2022 07:26:21 - INFO - __main__ - ['others']
05/27/2022 07:26:21 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/27/2022 07:26:21 - INFO - __main__ - ['others']
05/27/2022 07:26:21 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/27/2022 07:26:21 - INFO - __main__ - ['others']
05/27/2022 07:26:21 - INFO - __main__ - Tokenizing Input ...
05/27/2022 07:26:21 - INFO - __main__ - Tokenizing Output ...
05/27/2022 07:26:22 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 07:26:22 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 07:26:22 - INFO - __main__ - Printing 3 examples
05/27/2022 07:26:22 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/27/2022 07:26:22 - INFO - __main__ - ['others']
05/27/2022 07:26:22 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/27/2022 07:26:22 - INFO - __main__ - ['others']
05/27/2022 07:26:22 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/27/2022 07:26:22 - INFO - __main__ - ['others']
05/27/2022 07:26:22 - INFO - __main__ - Tokenizing Input ...
05/27/2022 07:26:22 - INFO - __main__ - Tokenizing Output ...
05/27/2022 07:26:23 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 07:26:41 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 07:26:41 - INFO - __main__ - task name: emo
05/27/2022 07:26:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 07:26:42 - INFO - __main__ - Starting training!
05/27/2022 07:26:45 - INFO - __main__ - Step 10 Global step 10 Train loss 7.59 on epoch=0
05/27/2022 07:26:48 - INFO - __main__ - Step 20 Global step 20 Train loss 5.34 on epoch=0
05/27/2022 07:26:50 - INFO - __main__ - Step 30 Global step 30 Train loss 3.67 on epoch=0
05/27/2022 07:26:53 - INFO - __main__ - Step 40 Global step 40 Train loss 2.12 on epoch=1
05/27/2022 07:26:55 - INFO - __main__ - Step 50 Global step 50 Train loss 1.60 on epoch=1
05/27/2022 07:27:02 - INFO - __main__ - Global step 50 Train loss 4.06 Classification-F1 0.10338724800750118 on epoch=1
05/27/2022 07:27:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10338724800750118 on epoch=1, global_step=50
05/27/2022 07:27:05 - INFO - __main__ - Step 60 Global step 60 Train loss 1.45 on epoch=1
05/27/2022 07:27:08 - INFO - __main__ - Step 70 Global step 70 Train loss 1.25 on epoch=2
05/27/2022 07:27:10 - INFO - __main__ - Step 80 Global step 80 Train loss 1.06 on epoch=2
05/27/2022 07:27:13 - INFO - __main__ - Step 90 Global step 90 Train loss 1.07 on epoch=2
05/27/2022 07:27:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.99 on epoch=3
05/27/2022 07:27:22 - INFO - __main__ - Global step 100 Train loss 1.16 Classification-F1 0.10350282215058661 on epoch=3
05/27/2022 07:27:22 - INFO - __main__ - Saving model with best Classification-F1: 0.10338724800750118 -> 0.10350282215058661 on epoch=3, global_step=100
05/27/2022 07:27:25 - INFO - __main__ - Step 110 Global step 110 Train loss 1.00 on epoch=3
05/27/2022 07:27:27 - INFO - __main__ - Step 120 Global step 120 Train loss 1.03 on epoch=3
05/27/2022 07:27:30 - INFO - __main__ - Step 130 Global step 130 Train loss 1.04 on epoch=4
05/27/2022 07:27:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.88 on epoch=4
05/27/2022 07:27:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.98 on epoch=4
05/27/2022 07:27:41 - INFO - __main__ - Global step 150 Train loss 0.99 Classification-F1 0.10741852487135506 on epoch=4
05/27/2022 07:27:41 - INFO - __main__ - Saving model with best Classification-F1: 0.10350282215058661 -> 0.10741852487135506 on epoch=4, global_step=150
05/27/2022 07:27:44 - INFO - __main__ - Step 160 Global step 160 Train loss 0.88 on epoch=4
05/27/2022 07:27:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.97 on epoch=5
05/27/2022 07:27:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.79 on epoch=5
05/27/2022 07:27:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.96 on epoch=5
05/27/2022 07:27:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.95 on epoch=6
05/27/2022 07:28:01 - INFO - __main__ - Global step 200 Train loss 0.91 Classification-F1 0.15743487035225048 on epoch=6
05/27/2022 07:28:01 - INFO - __main__ - Saving model with best Classification-F1: 0.10741852487135506 -> 0.15743487035225048 on epoch=6, global_step=200
05/27/2022 07:28:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.93 on epoch=6
05/27/2022 07:28:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.90 on epoch=6
05/27/2022 07:28:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.87 on epoch=7
05/27/2022 07:28:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=7
05/27/2022 07:28:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.88 on epoch=7
05/27/2022 07:28:21 - INFO - __main__ - Global step 250 Train loss 0.89 Classification-F1 0.19368267579829837 on epoch=7
05/27/2022 07:28:21 - INFO - __main__ - Saving model with best Classification-F1: 0.15743487035225048 -> 0.19368267579829837 on epoch=7, global_step=250
05/27/2022 07:28:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.94 on epoch=8
05/27/2022 07:28:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.94 on epoch=8
05/27/2022 07:28:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.92 on epoch=8
05/27/2022 07:28:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.88 on epoch=9
05/27/2022 07:28:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.91 on epoch=9
05/27/2022 07:28:40 - INFO - __main__ - Global step 300 Train loss 0.92 Classification-F1 0.1994726079001184 on epoch=9
05/27/2022 07:28:40 - INFO - __main__ - Saving model with best Classification-F1: 0.19368267579829837 -> 0.1994726079001184 on epoch=9, global_step=300
05/27/2022 07:28:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.88 on epoch=9
05/27/2022 07:28:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.91 on epoch=9
05/27/2022 07:28:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.91 on epoch=10
05/27/2022 07:28:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.90 on epoch=10
05/27/2022 07:28:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.89 on epoch=10
05/27/2022 07:29:00 - INFO - __main__ - Global step 350 Train loss 0.90 Classification-F1 0.20450819672131149 on epoch=10
05/27/2022 07:29:00 - INFO - __main__ - Saving model with best Classification-F1: 0.1994726079001184 -> 0.20450819672131149 on epoch=10, global_step=350
05/27/2022 07:29:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.83 on epoch=11
05/27/2022 07:29:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.83 on epoch=11
05/27/2022 07:29:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.82 on epoch=11
05/27/2022 07:29:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.91 on epoch=12
05/27/2022 07:29:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.87 on epoch=12
05/27/2022 07:29:19 - INFO - __main__ - Global step 400 Train loss 0.85 Classification-F1 0.2254601936374858 on epoch=12
05/27/2022 07:29:19 - INFO - __main__ - Saving model with best Classification-F1: 0.20450819672131149 -> 0.2254601936374858 on epoch=12, global_step=400
05/27/2022 07:29:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.85 on epoch=12
05/27/2022 07:29:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.81 on epoch=13
05/27/2022 07:29:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.92 on epoch=13
05/27/2022 07:29:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.89 on epoch=13
05/27/2022 07:29:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.74 on epoch=14
05/27/2022 07:29:39 - INFO - __main__ - Global step 450 Train loss 0.84 Classification-F1 0.10810454540007429 on epoch=14
05/27/2022 07:29:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.79 on epoch=14
05/27/2022 07:29:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.82 on epoch=14
05/27/2022 07:29:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.77 on epoch=14
05/27/2022 07:29:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.81 on epoch=15
05/27/2022 07:29:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.83 on epoch=15
05/27/2022 07:29:59 - INFO - __main__ - Global step 500 Train loss 0.80 Classification-F1 0.10403246351493978 on epoch=15
05/27/2022 07:30:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.87 on epoch=15
05/27/2022 07:30:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.82 on epoch=16
05/27/2022 07:30:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.85 on epoch=16
05/27/2022 07:30:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.85 on epoch=16
05/27/2022 07:30:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.80 on epoch=17
05/27/2022 07:30:18 - INFO - __main__ - Global step 550 Train loss 0.84 Classification-F1 0.29933877724464486 on epoch=17
05/27/2022 07:30:18 - INFO - __main__ - Saving model with best Classification-F1: 0.2254601936374858 -> 0.29933877724464486 on epoch=17, global_step=550
05/27/2022 07:30:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.88 on epoch=17
05/27/2022 07:30:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.81 on epoch=17
05/27/2022 07:30:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.80 on epoch=18
05/27/2022 07:30:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.87 on epoch=18
05/27/2022 07:30:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.86 on epoch=18
05/27/2022 07:30:37 - INFO - __main__ - Global step 600 Train loss 0.84 Classification-F1 0.24075517562068718 on epoch=18
05/27/2022 07:30:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.76 on epoch=19
05/27/2022 07:30:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.79 on epoch=19
05/27/2022 07:30:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.80 on epoch=19
05/27/2022 07:30:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.81 on epoch=19
05/27/2022 07:30:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.77 on epoch=20
05/27/2022 07:30:57 - INFO - __main__ - Global step 650 Train loss 0.78 Classification-F1 0.23066430148541583 on epoch=20
05/27/2022 07:31:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.73 on epoch=20
05/27/2022 07:31:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.81 on epoch=20
05/27/2022 07:31:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.78 on epoch=21
05/27/2022 07:31:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.77 on epoch=21
05/27/2022 07:31:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.80 on epoch=21
05/27/2022 07:31:17 - INFO - __main__ - Global step 700 Train loss 0.78 Classification-F1 0.4068561891186374 on epoch=21
05/27/2022 07:31:17 - INFO - __main__ - Saving model with best Classification-F1: 0.29933877724464486 -> 0.4068561891186374 on epoch=21, global_step=700
05/27/2022 07:31:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.74 on epoch=22
05/27/2022 07:31:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.80 on epoch=22
05/27/2022 07:31:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.71 on epoch=22
05/27/2022 07:31:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.69 on epoch=23
05/27/2022 07:31:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.82 on epoch=23
05/27/2022 07:31:36 - INFO - __main__ - Global step 750 Train loss 0.75 Classification-F1 0.5164082164688961 on epoch=23
05/27/2022 07:31:36 - INFO - __main__ - Saving model with best Classification-F1: 0.4068561891186374 -> 0.5164082164688961 on epoch=23, global_step=750
05/27/2022 07:31:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.69 on epoch=23
05/27/2022 07:31:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.73 on epoch=24
05/27/2022 07:31:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.75 on epoch=24
05/27/2022 07:31:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.67 on epoch=24
05/27/2022 07:31:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.68 on epoch=24
05/27/2022 07:31:56 - INFO - __main__ - Global step 800 Train loss 0.70 Classification-F1 0.45878989683048743 on epoch=24
05/27/2022 07:31:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.65 on epoch=25
05/27/2022 07:32:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.70 on epoch=25
05/27/2022 07:32:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.69 on epoch=25
05/27/2022 07:32:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.59 on epoch=26
05/27/2022 07:32:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.74 on epoch=26
05/27/2022 07:32:15 - INFO - __main__ - Global step 850 Train loss 0.68 Classification-F1 0.5740717166118512 on epoch=26
05/27/2022 07:32:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5164082164688961 -> 0.5740717166118512 on epoch=26, global_step=850
05/27/2022 07:32:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.76 on epoch=26
05/27/2022 07:32:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.60 on epoch=27
05/27/2022 07:32:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.64 on epoch=27
05/27/2022 07:32:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.71 on epoch=27
05/27/2022 07:32:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.58 on epoch=28
05/27/2022 07:32:35 - INFO - __main__ - Global step 900 Train loss 0.66 Classification-F1 0.583858505362542 on epoch=28
05/27/2022 07:32:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5740717166118512 -> 0.583858505362542 on epoch=28, global_step=900
05/27/2022 07:32:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.72 on epoch=28
05/27/2022 07:32:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.69 on epoch=28
05/27/2022 07:32:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.52 on epoch=29
05/27/2022 07:32:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.71 on epoch=29
05/27/2022 07:32:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.65 on epoch=29
05/27/2022 07:32:54 - INFO - __main__ - Global step 950 Train loss 0.66 Classification-F1 0.6184809651741683 on epoch=29
05/27/2022 07:32:54 - INFO - __main__ - Saving model with best Classification-F1: 0.583858505362542 -> 0.6184809651741683 on epoch=29, global_step=950
05/27/2022 07:32:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.53 on epoch=29
05/27/2022 07:32:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.60 on epoch=30
05/27/2022 07:33:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.57 on epoch=30
05/27/2022 07:33:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.64 on epoch=30
05/27/2022 07:33:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.52 on epoch=31
05/27/2022 07:33:14 - INFO - __main__ - Global step 1000 Train loss 0.57 Classification-F1 0.5549801699462524 on epoch=31
05/27/2022 07:33:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.64 on epoch=31
05/27/2022 07:33:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.61 on epoch=31
05/27/2022 07:33:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.55 on epoch=32
05/27/2022 07:33:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.67 on epoch=32
05/27/2022 07:33:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.52 on epoch=32
05/27/2022 07:33:33 - INFO - __main__ - Global step 1050 Train loss 0.60 Classification-F1 0.6103133233346716 on epoch=32
05/27/2022 07:33:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.47 on epoch=33
05/27/2022 07:33:38 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.68 on epoch=33
05/27/2022 07:33:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.61 on epoch=33
05/27/2022 07:33:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=34
05/27/2022 07:33:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.53 on epoch=34
05/27/2022 07:33:53 - INFO - __main__ - Global step 1100 Train loss 0.55 Classification-F1 0.6315406236625106 on epoch=34
05/27/2022 07:33:53 - INFO - __main__ - Saving model with best Classification-F1: 0.6184809651741683 -> 0.6315406236625106 on epoch=34, global_step=1100
05/27/2022 07:33:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.61 on epoch=34
05/27/2022 07:33:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.51 on epoch=34
05/27/2022 07:34:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.47 on epoch=35
05/27/2022 07:34:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.56 on epoch=35
05/27/2022 07:34:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.56 on epoch=35
05/27/2022 07:34:13 - INFO - __main__ - Global step 1150 Train loss 0.54 Classification-F1 0.6565055450058593 on epoch=35
05/27/2022 07:34:13 - INFO - __main__ - Saving model with best Classification-F1: 0.6315406236625106 -> 0.6565055450058593 on epoch=35, global_step=1150
05/27/2022 07:34:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=36
05/27/2022 07:34:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.72 on epoch=36
05/27/2022 07:34:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.53 on epoch=36
05/27/2022 07:34:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.50 on epoch=37
05/27/2022 07:34:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.62 on epoch=37
05/27/2022 07:34:32 - INFO - __main__ - Global step 1200 Train loss 0.56 Classification-F1 0.6638605217654093 on epoch=37
05/27/2022 07:34:32 - INFO - __main__ - Saving model with best Classification-F1: 0.6565055450058593 -> 0.6638605217654093 on epoch=37, global_step=1200
05/27/2022 07:34:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.56 on epoch=37
05/27/2022 07:34:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.46 on epoch=38
05/27/2022 07:34:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.65 on epoch=38
05/27/2022 07:34:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.55 on epoch=38
05/27/2022 07:34:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=39
05/27/2022 07:34:52 - INFO - __main__ - Global step 1250 Train loss 0.53 Classification-F1 0.5781079108394914 on epoch=39
05/27/2022 07:34:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.52 on epoch=39
05/27/2022 07:34:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.55 on epoch=39
05/27/2022 07:34:59 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=39
05/27/2022 07:35:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.60 on epoch=40
05/27/2022 07:35:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.55 on epoch=40
05/27/2022 07:35:11 - INFO - __main__ - Global step 1300 Train loss 0.52 Classification-F1 0.6464604236343368 on epoch=40
05/27/2022 07:35:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=40
05/27/2022 07:35:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.38 on epoch=41
05/27/2022 07:35:19 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.60 on epoch=41
05/27/2022 07:35:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.45 on epoch=41
05/27/2022 07:35:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=42
05/27/2022 07:35:31 - INFO - __main__ - Global step 1350 Train loss 0.47 Classification-F1 0.6920601362461827 on epoch=42
05/27/2022 07:35:31 - INFO - __main__ - Saving model with best Classification-F1: 0.6638605217654093 -> 0.6920601362461827 on epoch=42, global_step=1350
05/27/2022 07:35:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=42
05/27/2022 07:35:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.52 on epoch=42
05/27/2022 07:35:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=43
05/27/2022 07:35:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.53 on epoch=43
05/27/2022 07:35:43 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.52 on epoch=43
05/27/2022 07:35:50 - INFO - __main__ - Global step 1400 Train loss 0.47 Classification-F1 0.6963399870857832 on epoch=43
05/27/2022 07:35:50 - INFO - __main__ - Saving model with best Classification-F1: 0.6920601362461827 -> 0.6963399870857832 on epoch=43, global_step=1400
05/27/2022 07:35:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.39 on epoch=44
05/27/2022 07:35:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.49 on epoch=44
05/27/2022 07:35:58 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.48 on epoch=44
05/27/2022 07:36:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=44
05/27/2022 07:36:03 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=45
05/27/2022 07:36:10 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.7193580160598372 on epoch=45
05/27/2022 07:36:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6963399870857832 -> 0.7193580160598372 on epoch=45, global_step=1450
05/27/2022 07:36:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.49 on epoch=45
05/27/2022 07:36:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.45 on epoch=45
05/27/2022 07:36:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.40 on epoch=46
05/27/2022 07:36:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.49 on epoch=46
05/27/2022 07:36:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.45 on epoch=46
05/27/2022 07:36:29 - INFO - __main__ - Global step 1500 Train loss 0.46 Classification-F1 0.6927989422778225 on epoch=46
05/27/2022 07:36:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=47
05/27/2022 07:36:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.51 on epoch=47
05/27/2022 07:36:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=47
05/27/2022 07:36:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.30 on epoch=48
05/27/2022 07:36:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.53 on epoch=48
05/27/2022 07:36:49 - INFO - __main__ - Global step 1550 Train loss 0.44 Classification-F1 0.7309085686109926 on epoch=48
05/27/2022 07:36:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7193580160598372 -> 0.7309085686109926 on epoch=48, global_step=1550
05/27/2022 07:36:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.52 on epoch=48
05/27/2022 07:36:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=49
05/27/2022 07:36:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.46 on epoch=49
05/27/2022 07:36:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=49
05/27/2022 07:37:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=49
05/27/2022 07:37:09 - INFO - __main__ - Global step 1600 Train loss 0.41 Classification-F1 0.7307773263953249 on epoch=49
05/27/2022 07:37:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=50
05/27/2022 07:37:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=50
05/27/2022 07:37:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.47 on epoch=50
05/27/2022 07:37:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.37 on epoch=51
05/27/2022 07:37:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.48 on epoch=51
05/27/2022 07:37:29 - INFO - __main__ - Global step 1650 Train loss 0.43 Classification-F1 0.7377533410330994 on epoch=51
05/27/2022 07:37:29 - INFO - __main__ - Saving model with best Classification-F1: 0.7309085686109926 -> 0.7377533410330994 on epoch=51, global_step=1650
05/27/2022 07:37:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=51
05/27/2022 07:37:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.32 on epoch=52
05/27/2022 07:37:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.46 on epoch=52
05/27/2022 07:37:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=52
05/27/2022 07:37:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.29 on epoch=53
05/27/2022 07:37:49 - INFO - __main__ - Global step 1700 Train loss 0.36 Classification-F1 0.7152873388841509 on epoch=53
05/27/2022 07:37:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.46 on epoch=53
05/27/2022 07:37:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.51 on epoch=53
05/27/2022 07:37:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.35 on epoch=54
05/27/2022 07:37:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.41 on epoch=54
05/27/2022 07:38:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=54
05/27/2022 07:38:08 - INFO - __main__ - Global step 1750 Train loss 0.42 Classification-F1 0.7305123573135079 on epoch=54
05/27/2022 07:38:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=54
05/27/2022 07:38:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.46 on epoch=55
05/27/2022 07:38:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.49 on epoch=55
05/27/2022 07:38:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.44 on epoch=55
05/27/2022 07:38:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=56
05/27/2022 07:38:28 - INFO - __main__ - Global step 1800 Train loss 0.42 Classification-F1 0.7270825820708007 on epoch=56
05/27/2022 07:38:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.46 on epoch=56
05/27/2022 07:38:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=56
05/27/2022 07:38:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=57
05/27/2022 07:38:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=57
05/27/2022 07:38:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=57
05/27/2022 07:38:47 - INFO - __main__ - Global step 1850 Train loss 0.38 Classification-F1 0.7337649470580911 on epoch=57
05/27/2022 07:38:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.30 on epoch=58
05/27/2022 07:38:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=58
05/27/2022 07:38:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.40 on epoch=58
05/27/2022 07:38:58 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.26 on epoch=59
05/27/2022 07:39:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.35 on epoch=59
05/27/2022 07:39:07 - INFO - __main__ - Global step 1900 Train loss 0.35 Classification-F1 0.7372901754079502 on epoch=59
05/27/2022 07:39:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.33 on epoch=59
05/27/2022 07:39:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.33 on epoch=59
05/27/2022 07:39:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.31 on epoch=60
05/27/2022 07:39:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=60
05/27/2022 07:39:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.42 on epoch=60
05/27/2022 07:39:27 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.7155339513374595 on epoch=60
05/27/2022 07:39:29 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=61
05/27/2022 07:39:32 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=61
05/27/2022 07:39:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=61
05/27/2022 07:39:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.25 on epoch=62
05/27/2022 07:39:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=62
05/27/2022 07:39:47 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.7573382178661072 on epoch=62
05/27/2022 07:39:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7377533410330994 -> 0.7573382178661072 on epoch=62, global_step=2000
05/27/2022 07:39:49 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.37 on epoch=62
05/27/2022 07:39:52 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=63
05/27/2022 07:39:54 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.43 on epoch=63
05/27/2022 07:39:57 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.37 on epoch=63
05/27/2022 07:39:59 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.27 on epoch=64
05/27/2022 07:40:06 - INFO - __main__ - Global step 2050 Train loss 0.33 Classification-F1 0.7487402950594297 on epoch=64
05/27/2022 07:40:09 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.49 on epoch=64
05/27/2022 07:40:11 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.35 on epoch=64
05/27/2022 07:40:14 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.27 on epoch=64
05/27/2022 07:40:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.35 on epoch=65
05/27/2022 07:40:19 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.40 on epoch=65
05/27/2022 07:40:26 - INFO - __main__ - Global step 2100 Train loss 0.37 Classification-F1 0.695569634582585 on epoch=65
05/27/2022 07:40:29 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.35 on epoch=65
05/27/2022 07:40:31 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.28 on epoch=66
05/27/2022 07:40:34 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.37 on epoch=66
05/27/2022 07:40:36 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.26 on epoch=66
05/27/2022 07:40:39 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.38 on epoch=67
05/27/2022 07:40:46 - INFO - __main__ - Global step 2150 Train loss 0.33 Classification-F1 0.7394245454919612 on epoch=67
05/27/2022 07:40:48 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.40 on epoch=67
05/27/2022 07:40:51 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.25 on epoch=67
05/27/2022 07:40:53 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.25 on epoch=68
05/27/2022 07:40:56 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.39 on epoch=68
05/27/2022 07:40:59 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.28 on epoch=68
05/27/2022 07:41:06 - INFO - __main__ - Global step 2200 Train loss 0.32 Classification-F1 0.7035603695667186 on epoch=68
05/27/2022 07:41:08 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.20 on epoch=69
05/27/2022 07:41:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.36 on epoch=69
05/27/2022 07:41:13 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.35 on epoch=69
05/27/2022 07:41:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.27 on epoch=69
05/27/2022 07:41:18 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.24 on epoch=70
05/27/2022 07:41:25 - INFO - __main__ - Global step 2250 Train loss 0.28 Classification-F1 0.7605259638407925 on epoch=70
05/27/2022 07:41:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7573382178661072 -> 0.7605259638407925 on epoch=70, global_step=2250
05/27/2022 07:41:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.34 on epoch=70
05/27/2022 07:41:30 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.29 on epoch=70
05/27/2022 07:41:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.23 on epoch=71
05/27/2022 07:41:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.41 on epoch=71
05/27/2022 07:41:38 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.27 on epoch=71
05/27/2022 07:41:45 - INFO - __main__ - Global step 2300 Train loss 0.31 Classification-F1 0.7349153956564682 on epoch=71
05/27/2022 07:41:47 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.30 on epoch=72
05/27/2022 07:41:50 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.35 on epoch=72
05/27/2022 07:41:53 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.32 on epoch=72
05/27/2022 07:41:55 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.21 on epoch=73
05/27/2022 07:41:58 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.36 on epoch=73
05/27/2022 07:42:05 - INFO - __main__ - Global step 2350 Train loss 0.31 Classification-F1 0.7839177191344374 on epoch=73
05/27/2022 07:42:05 - INFO - __main__ - Saving model with best Classification-F1: 0.7605259638407925 -> 0.7839177191344374 on epoch=73, global_step=2350
05/27/2022 07:42:07 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.26 on epoch=73
05/27/2022 07:42:10 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.14 on epoch=74
05/27/2022 07:42:12 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.42 on epoch=74
05/27/2022 07:42:15 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.26 on epoch=74
05/27/2022 07:42:17 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.23 on epoch=74
05/27/2022 07:42:24 - INFO - __main__ - Global step 2400 Train loss 0.26 Classification-F1 0.775942922619687 on epoch=74
05/27/2022 07:42:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.28 on epoch=75
05/27/2022 07:42:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.33 on epoch=75
05/27/2022 07:42:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.31 on epoch=75
05/27/2022 07:42:34 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=76
05/27/2022 07:42:37 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.35 on epoch=76
05/27/2022 07:42:44 - INFO - __main__ - Global step 2450 Train loss 0.29 Classification-F1 0.7785769796052222 on epoch=76
05/27/2022 07:42:46 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.23 on epoch=76
05/27/2022 07:42:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.24 on epoch=77
05/27/2022 07:42:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.36 on epoch=77
05/27/2022 07:42:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.29 on epoch=77
05/27/2022 07:42:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.21 on epoch=78
05/27/2022 07:43:04 - INFO - __main__ - Global step 2500 Train loss 0.27 Classification-F1 0.7585783806300608 on epoch=78
05/27/2022 07:43:06 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.39 on epoch=78
05/27/2022 07:43:09 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.33 on epoch=78
05/27/2022 07:43:11 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.15 on epoch=79
05/27/2022 07:43:14 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.38 on epoch=79
05/27/2022 07:43:16 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.32 on epoch=79
05/27/2022 07:43:23 - INFO - __main__ - Global step 2550 Train loss 0.31 Classification-F1 0.7754257364998043 on epoch=79
05/27/2022 07:43:26 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.23 on epoch=79
05/27/2022 07:43:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.24 on epoch=80
05/27/2022 07:43:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.33 on epoch=80
05/27/2022 07:43:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.24 on epoch=80
05/27/2022 07:43:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.17 on epoch=81
05/27/2022 07:43:43 - INFO - __main__ - Global step 2600 Train loss 0.24 Classification-F1 0.7652541378876018 on epoch=81
05/27/2022 07:43:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.39 on epoch=81
05/27/2022 07:43:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.24 on epoch=81
05/27/2022 07:43:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.23 on epoch=82
05/27/2022 07:43:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.33 on epoch=82
05/27/2022 07:43:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.26 on epoch=82
05/27/2022 07:44:03 - INFO - __main__ - Global step 2650 Train loss 0.29 Classification-F1 0.7750465441518962 on epoch=82
05/27/2022 07:44:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.12 on epoch=83
05/27/2022 07:44:08 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.39 on epoch=83
05/27/2022 07:44:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.20 on epoch=83
05/27/2022 07:44:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.15 on epoch=84
05/27/2022 07:44:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.49 on epoch=84
05/27/2022 07:44:22 - INFO - __main__ - Global step 2700 Train loss 0.27 Classification-F1 0.7677450637943963 on epoch=84
05/27/2022 07:44:25 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.26 on epoch=84
05/27/2022 07:44:28 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.23 on epoch=84
05/27/2022 07:44:30 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.31 on epoch=85
05/27/2022 07:44:33 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.23 on epoch=85
05/27/2022 07:44:35 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.27 on epoch=85
05/27/2022 07:44:42 - INFO - __main__ - Global step 2750 Train loss 0.26 Classification-F1 0.7451694632052905 on epoch=85
05/27/2022 07:44:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=86
05/27/2022 07:44:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.40 on epoch=86
05/27/2022 07:44:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.31 on epoch=86
05/27/2022 07:44:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.22 on epoch=87
05/27/2022 07:44:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.31 on epoch=87
05/27/2022 07:45:02 - INFO - __main__ - Global step 2800 Train loss 0.28 Classification-F1 0.762530593045299 on epoch=87
05/27/2022 07:45:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.24 on epoch=87
05/27/2022 07:45:07 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.15 on epoch=88
05/27/2022 07:45:09 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.28 on epoch=88
05/27/2022 07:45:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.25 on epoch=88
05/27/2022 07:45:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.14 on epoch=89
05/27/2022 07:45:22 - INFO - __main__ - Global step 2850 Train loss 0.21 Classification-F1 0.7651944340361367 on epoch=89
05/27/2022 07:45:24 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.28 on epoch=89
05/27/2022 07:45:27 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.25 on epoch=89
05/27/2022 07:45:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.26 on epoch=89
05/27/2022 07:45:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.25 on epoch=90
05/27/2022 07:45:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.30 on epoch=90
05/27/2022 07:45:41 - INFO - __main__ - Global step 2900 Train loss 0.27 Classification-F1 0.7597443131513969 on epoch=90
05/27/2022 07:45:44 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.26 on epoch=90
05/27/2022 07:45:46 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.12 on epoch=91
05/27/2022 07:45:49 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.33 on epoch=91
05/27/2022 07:45:52 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.21 on epoch=91
05/27/2022 07:45:54 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.18 on epoch=92
05/27/2022 07:46:01 - INFO - __main__ - Global step 2950 Train loss 0.22 Classification-F1 0.7595480352319804 on epoch=92
05/27/2022 07:46:03 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.37 on epoch=92
05/27/2022 07:46:06 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.26 on epoch=92
05/27/2022 07:46:09 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.14 on epoch=93
05/27/2022 07:46:11 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.34 on epoch=93
05/27/2022 07:46:14 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.26 on epoch=93
05/27/2022 07:46:15 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 07:46:15 - INFO - __main__ - Printing 3 examples
05/27/2022 07:46:15 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/27/2022 07:46:15 - INFO - __main__ - ['sad']
05/27/2022 07:46:15 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/27/2022 07:46:15 - INFO - __main__ - ['sad']
05/27/2022 07:46:15 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/27/2022 07:46:15 - INFO - __main__ - ['sad']
05/27/2022 07:46:15 - INFO - __main__ - Tokenizing Input ...
05/27/2022 07:46:15 - INFO - __main__ - Tokenizing Output ...
05/27/2022 07:46:16 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 07:46:16 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 07:46:16 - INFO - __main__ - Printing 3 examples
05/27/2022 07:46:16 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/27/2022 07:46:16 - INFO - __main__ - ['sad']
05/27/2022 07:46:16 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/27/2022 07:46:16 - INFO - __main__ - ['sad']
05/27/2022 07:46:16 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/27/2022 07:46:16 - INFO - __main__ - ['sad']
05/27/2022 07:46:16 - INFO - __main__ - Tokenizing Input ...
05/27/2022 07:46:16 - INFO - __main__ - Tokenizing Output ...
05/27/2022 07:46:16 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 07:46:21 - INFO - __main__ - Global step 3000 Train loss 0.27 Classification-F1 0.7722012492674761 on epoch=93
05/27/2022 07:46:21 - INFO - __main__ - save last model!
05/27/2022 07:46:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 07:46:21 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 07:46:21 - INFO - __main__ - Printing 3 examples
05/27/2022 07:46:21 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 07:46:21 - INFO - __main__ - ['others']
05/27/2022 07:46:21 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 07:46:21 - INFO - __main__ - ['others']
05/27/2022 07:46:21 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 07:46:21 - INFO - __main__ - ['others']
05/27/2022 07:46:21 - INFO - __main__ - Tokenizing Input ...
05/27/2022 07:46:23 - INFO - __main__ - Tokenizing Output ...
05/27/2022 07:46:28 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 07:46:33 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 07:46:33 - INFO - __main__ - task name: emo
05/27/2022 07:46:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 07:46:34 - INFO - __main__ - Starting training!
05/27/2022 07:47:43 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_13_0.2_8_predictions.txt
05/27/2022 07:47:43 - INFO - __main__ - Classification-F1 on test data: 0.4490
05/27/2022 07:47:43 - INFO - __main__ - prefix=emo_128_13, lr=0.2, bsz=8, dev_performance=0.7839177191344374, test_performance=0.44902904431524604
05/27/2022 07:47:43 - INFO - __main__ - Running ... prefix=emo_128_21, lr=0.5, bsz=8 ...
05/27/2022 07:47:44 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 07:47:44 - INFO - __main__ - Printing 3 examples
05/27/2022 07:47:44 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/27/2022 07:47:44 - INFO - __main__ - ['sad']
05/27/2022 07:47:44 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/27/2022 07:47:44 - INFO - __main__ - ['sad']
05/27/2022 07:47:44 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/27/2022 07:47:44 - INFO - __main__ - ['sad']
05/27/2022 07:47:44 - INFO - __main__ - Tokenizing Input ...
05/27/2022 07:47:44 - INFO - __main__ - Tokenizing Output ...
05/27/2022 07:47:45 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 07:47:45 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 07:47:45 - INFO - __main__ - Printing 3 examples
05/27/2022 07:47:45 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/27/2022 07:47:45 - INFO - __main__ - ['sad']
05/27/2022 07:47:45 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/27/2022 07:47:45 - INFO - __main__ - ['sad']
05/27/2022 07:47:45 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/27/2022 07:47:45 - INFO - __main__ - ['sad']
05/27/2022 07:47:45 - INFO - __main__ - Tokenizing Input ...
05/27/2022 07:47:45 - INFO - __main__ - Tokenizing Output ...
05/27/2022 07:47:46 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 07:48:04 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 07:48:04 - INFO - __main__ - task name: emo
05/27/2022 07:48:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 07:48:05 - INFO - __main__ - Starting training!
05/27/2022 07:48:08 - INFO - __main__ - Step 10 Global step 10 Train loss 6.19 on epoch=0
05/27/2022 07:48:11 - INFO - __main__ - Step 20 Global step 20 Train loss 2.41 on epoch=0
05/27/2022 07:48:13 - INFO - __main__ - Step 30 Global step 30 Train loss 1.38 on epoch=0
05/27/2022 07:48:16 - INFO - __main__ - Step 40 Global step 40 Train loss 1.12 on epoch=1
05/27/2022 07:48:18 - INFO - __main__ - Step 50 Global step 50 Train loss 1.03 on epoch=1
05/27/2022 07:48:26 - INFO - __main__ - Global step 50 Train loss 2.43 Classification-F1 0.1072992700729927 on epoch=1
05/27/2022 07:48:26 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1072992700729927 on epoch=1, global_step=50
05/27/2022 07:48:28 - INFO - __main__ - Step 60 Global step 60 Train loss 1.07 on epoch=1
05/27/2022 07:48:31 - INFO - __main__ - Step 70 Global step 70 Train loss 0.98 on epoch=2
05/27/2022 07:48:33 - INFO - __main__ - Step 80 Global step 80 Train loss 0.97 on epoch=2
05/27/2022 07:48:36 - INFO - __main__ - Step 90 Global step 90 Train loss 0.99 on epoch=2
05/27/2022 07:48:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.98 on epoch=3
05/27/2022 07:48:45 - INFO - __main__ - Global step 100 Train loss 1.00 Classification-F1 0.27949965538648613 on epoch=3
05/27/2022 07:48:45 - INFO - __main__ - Saving model with best Classification-F1: 0.1072992700729927 -> 0.27949965538648613 on epoch=3, global_step=100
05/27/2022 07:48:48 - INFO - __main__ - Step 110 Global step 110 Train loss 0.91 on epoch=3
05/27/2022 07:48:51 - INFO - __main__ - Step 120 Global step 120 Train loss 0.91 on epoch=3
05/27/2022 07:48:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.95 on epoch=4
05/27/2022 07:48:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.89 on epoch=4
05/27/2022 07:48:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.84 on epoch=4
05/27/2022 07:49:05 - INFO - __main__ - Global step 150 Train loss 0.90 Classification-F1 0.1 on epoch=4
05/27/2022 07:49:08 - INFO - __main__ - Step 160 Global step 160 Train loss 0.90 on epoch=4
05/27/2022 07:49:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.89 on epoch=5
05/27/2022 07:49:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.99 on epoch=5
05/27/2022 07:49:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.86 on epoch=5
05/27/2022 07:49:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.86 on epoch=6
05/27/2022 07:49:25 - INFO - __main__ - Global step 200 Train loss 0.90 Classification-F1 0.10350282215058661 on epoch=6
05/27/2022 07:49:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.83 on epoch=6
05/27/2022 07:49:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.87 on epoch=6
05/27/2022 07:49:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.74 on epoch=7
05/27/2022 07:49:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.83 on epoch=7
05/27/2022 07:49:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.82 on epoch=7
05/27/2022 07:49:45 - INFO - __main__ - Global step 250 Train loss 0.82 Classification-F1 0.17863537710255017 on epoch=7
05/27/2022 07:49:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.90 on epoch=8
05/27/2022 07:49:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.86 on epoch=8
05/27/2022 07:49:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.88 on epoch=8
05/27/2022 07:49:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.92 on epoch=9
05/27/2022 07:49:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.85 on epoch=9
05/27/2022 07:50:04 - INFO - __main__ - Global step 300 Train loss 0.88 Classification-F1 0.1706629152587951 on epoch=9
05/27/2022 07:50:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.90 on epoch=9
05/27/2022 07:50:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.76 on epoch=9
05/27/2022 07:50:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.82 on epoch=10
05/27/2022 07:50:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.78 on epoch=10
05/27/2022 07:50:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.93 on epoch=10
05/27/2022 07:50:24 - INFO - __main__ - Global step 350 Train loss 0.84 Classification-F1 0.14718724532063568 on epoch=10
05/27/2022 07:50:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.83 on epoch=11
05/27/2022 07:50:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.81 on epoch=11
05/27/2022 07:50:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.86 on epoch=11
05/27/2022 07:50:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.72 on epoch=12
05/27/2022 07:50:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.85 on epoch=12
05/27/2022 07:50:44 - INFO - __main__ - Global step 400 Train loss 0.81 Classification-F1 0.2953369691264671 on epoch=12
05/27/2022 07:50:44 - INFO - __main__ - Saving model with best Classification-F1: 0.27949965538648613 -> 0.2953369691264671 on epoch=12, global_step=400
05/27/2022 07:50:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.86 on epoch=12
05/27/2022 07:50:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.80 on epoch=13
05/27/2022 07:50:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.72 on epoch=13
05/27/2022 07:50:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.76 on epoch=13
05/27/2022 07:50:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.82 on epoch=14
05/27/2022 07:51:04 - INFO - __main__ - Global step 450 Train loss 0.79 Classification-F1 0.2237914692864319 on epoch=14
05/27/2022 07:51:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.73 on epoch=14
05/27/2022 07:51:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.78 on epoch=14
05/27/2022 07:51:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.72 on epoch=14
05/27/2022 07:51:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.76 on epoch=15
05/27/2022 07:51:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.72 on epoch=15
05/27/2022 07:51:23 - INFO - __main__ - Global step 500 Train loss 0.74 Classification-F1 0.38321290523828727 on epoch=15
05/27/2022 07:51:24 - INFO - __main__ - Saving model with best Classification-F1: 0.2953369691264671 -> 0.38321290523828727 on epoch=15, global_step=500
05/27/2022 07:51:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.83 on epoch=15
05/27/2022 07:51:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.70 on epoch=16
05/27/2022 07:51:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.77 on epoch=16
05/27/2022 07:51:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.83 on epoch=16
05/27/2022 07:51:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.63 on epoch=17
05/27/2022 07:51:43 - INFO - __main__ - Global step 550 Train loss 0.75 Classification-F1 0.5601652613714263 on epoch=17
05/27/2022 07:51:43 - INFO - __main__ - Saving model with best Classification-F1: 0.38321290523828727 -> 0.5601652613714263 on epoch=17, global_step=550
05/27/2022 07:51:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.70 on epoch=17
05/27/2022 07:51:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.77 on epoch=17
05/27/2022 07:51:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.67 on epoch=18
05/27/2022 07:51:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.69 on epoch=18
05/27/2022 07:51:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.73 on epoch=18
05/27/2022 07:52:03 - INFO - __main__ - Global step 600 Train loss 0.71 Classification-F1 0.4609190519056373 on epoch=18
05/27/2022 07:52:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.68 on epoch=19
05/27/2022 07:52:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.63 on epoch=19
05/27/2022 07:52:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.71 on epoch=19
05/27/2022 07:52:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.63 on epoch=19
05/27/2022 07:52:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.61 on epoch=20
05/27/2022 07:52:23 - INFO - __main__ - Global step 650 Train loss 0.65 Classification-F1 0.324446670287977 on epoch=20
05/27/2022 07:52:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.53 on epoch=20
05/27/2022 07:52:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.61 on epoch=20
05/27/2022 07:52:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.59 on epoch=21
05/27/2022 07:52:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.56 on epoch=21
05/27/2022 07:52:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.68 on epoch=21
05/27/2022 07:52:43 - INFO - __main__ - Global step 700 Train loss 0.60 Classification-F1 0.5423659580348139 on epoch=21
05/27/2022 07:52:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.63 on epoch=22
05/27/2022 07:52:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=22
05/27/2022 07:52:50 - INFO - __main__ - Step 730 Global step 730 Train loss 0.59 on epoch=22
05/27/2022 07:52:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.60 on epoch=23
05/27/2022 07:52:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.57 on epoch=23
05/27/2022 07:53:03 - INFO - __main__ - Global step 750 Train loss 0.58 Classification-F1 0.6502195875101869 on epoch=23
05/27/2022 07:53:03 - INFO - __main__ - Saving model with best Classification-F1: 0.5601652613714263 -> 0.6502195875101869 on epoch=23, global_step=750
05/27/2022 07:53:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.62 on epoch=23
05/27/2022 07:53:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.51 on epoch=24
05/27/2022 07:53:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.52 on epoch=24
05/27/2022 07:53:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=24
05/27/2022 07:53:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.59 on epoch=24
05/27/2022 07:53:23 - INFO - __main__ - Global step 800 Train loss 0.54 Classification-F1 0.6037881955456946 on epoch=24
05/27/2022 07:53:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.56 on epoch=25
05/27/2022 07:53:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.58 on epoch=25
05/27/2022 07:53:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.55 on epoch=25
05/27/2022 07:53:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.55 on epoch=26
05/27/2022 07:53:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=26
05/27/2022 07:53:42 - INFO - __main__ - Global step 850 Train loss 0.53 Classification-F1 0.7331898526875368 on epoch=26
05/27/2022 07:53:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6502195875101869 -> 0.7331898526875368 on epoch=26, global_step=850
05/27/2022 07:53:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.48 on epoch=26
05/27/2022 07:53:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.56 on epoch=27
05/27/2022 07:53:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=27
05/27/2022 07:53:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.59 on epoch=27
05/27/2022 07:53:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=28
05/27/2022 07:54:02 - INFO - __main__ - Global step 900 Train loss 0.50 Classification-F1 0.7589802679696676 on epoch=28
05/27/2022 07:54:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7331898526875368 -> 0.7589802679696676 on epoch=28, global_step=900
05/27/2022 07:54:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=28
05/27/2022 07:54:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=28
05/27/2022 07:54:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.52 on epoch=29
05/27/2022 07:54:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=29
05/27/2022 07:54:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.48 on epoch=29
05/27/2022 07:54:22 - INFO - __main__ - Global step 950 Train loss 0.46 Classification-F1 0.6865358126721763 on epoch=29
05/27/2022 07:54:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.55 on epoch=29
05/27/2022 07:54:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=30
05/27/2022 07:54:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.48 on epoch=30
05/27/2022 07:54:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.48 on epoch=30
05/27/2022 07:54:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=31
05/27/2022 07:54:42 - INFO - __main__ - Global step 1000 Train loss 0.49 Classification-F1 0.640489468726056 on epoch=31
05/27/2022 07:54:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=31
05/27/2022 07:54:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=31
05/27/2022 07:54:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=32
05/27/2022 07:54:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.34 on epoch=32
05/27/2022 07:54:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.49 on epoch=32
05/27/2022 07:55:02 - INFO - __main__ - Global step 1050 Train loss 0.41 Classification-F1 0.7706788588056279 on epoch=32
05/27/2022 07:55:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7589802679696676 -> 0.7706788588056279 on epoch=32, global_step=1050
05/27/2022 07:55:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.43 on epoch=33
05/27/2022 07:55:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=33
05/27/2022 07:55:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=33
05/27/2022 07:55:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=34
05/27/2022 07:55:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.29 on epoch=34
05/27/2022 07:55:21 - INFO - __main__ - Global step 1100 Train loss 0.40 Classification-F1 0.6890985414241524 on epoch=34
05/27/2022 07:55:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.48 on epoch=34
05/27/2022 07:55:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=34
05/27/2022 07:55:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=35
05/27/2022 07:55:32 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.31 on epoch=35
05/27/2022 07:55:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=35
05/27/2022 07:55:41 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.7683262980399717 on epoch=35
05/27/2022 07:55:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=36
05/27/2022 07:55:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=36
05/27/2022 07:55:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=36
05/27/2022 07:55:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=37
05/27/2022 07:55:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.29 on epoch=37
05/27/2022 07:56:01 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.7393885656391314 on epoch=37
05/27/2022 07:56:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=37
05/27/2022 07:56:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=38
05/27/2022 07:56:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.29 on epoch=38
05/27/2022 07:56:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.37 on epoch=38
05/27/2022 07:56:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.27 on epoch=39
05/27/2022 07:56:21 - INFO - __main__ - Global step 1250 Train loss 0.31 Classification-F1 0.7532340200821644 on epoch=39
05/27/2022 07:56:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.29 on epoch=39
05/27/2022 07:56:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=39
05/27/2022 07:56:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=39
05/27/2022 07:56:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=40
05/27/2022 07:56:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.31 on epoch=40
05/27/2022 07:56:42 - INFO - __main__ - Global step 1300 Train loss 0.30 Classification-F1 0.7579178160016041 on epoch=40
05/27/2022 07:56:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.32 on epoch=40
05/27/2022 07:56:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.45 on epoch=41
05/27/2022 07:56:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.28 on epoch=41
05/27/2022 07:56:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=41
05/27/2022 07:56:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.29 on epoch=42
05/27/2022 07:57:01 - INFO - __main__ - Global step 1350 Train loss 0.33 Classification-F1 0.7564589607006584 on epoch=42
05/27/2022 07:57:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=42
05/27/2022 07:57:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.25 on epoch=42
05/27/2022 07:57:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.27 on epoch=43
05/27/2022 07:57:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.33 on epoch=43
05/27/2022 07:57:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=43
05/27/2022 07:57:21 - INFO - __main__ - Global step 1400 Train loss 0.28 Classification-F1 0.7701113458164763 on epoch=43
05/27/2022 07:57:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.28 on epoch=44
05/27/2022 07:57:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.25 on epoch=44
05/27/2022 07:57:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=44
05/27/2022 07:57:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.28 on epoch=44
05/27/2022 07:57:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.28 on epoch=45
05/27/2022 07:57:41 - INFO - __main__ - Global step 1450 Train loss 0.27 Classification-F1 0.7406663903279713 on epoch=45
05/27/2022 07:57:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.32 on epoch=45
05/27/2022 07:57:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.26 on epoch=45
05/27/2022 07:57:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=46
05/27/2022 07:57:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.27 on epoch=46
05/27/2022 07:57:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=46
05/27/2022 07:58:01 - INFO - __main__ - Global step 1500 Train loss 0.31 Classification-F1 0.7585528743394586 on epoch=46
05/27/2022 07:58:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=47
05/27/2022 07:58:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.33 on epoch=47
05/27/2022 07:58:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=47
05/27/2022 07:58:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=48
05/27/2022 07:58:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.23 on epoch=48
05/27/2022 07:58:21 - INFO - __main__ - Global step 1550 Train loss 0.29 Classification-F1 0.7768318772706149 on epoch=48
05/27/2022 07:58:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7706788588056279 -> 0.7768318772706149 on epoch=48, global_step=1550
05/27/2022 07:58:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.30 on epoch=48
05/27/2022 07:58:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.25 on epoch=49
05/27/2022 07:58:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=49
05/27/2022 07:58:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.32 on epoch=49
05/27/2022 07:58:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.25 on epoch=49
05/27/2022 07:58:40 - INFO - __main__ - Global step 1600 Train loss 0.27 Classification-F1 0.7623739528337173 on epoch=49
05/27/2022 07:58:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.26 on epoch=50
05/27/2022 07:58:46 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.37 on epoch=50
05/27/2022 07:58:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=50
05/27/2022 07:58:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=51
05/27/2022 07:58:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.26 on epoch=51
05/27/2022 07:59:00 - INFO - __main__ - Global step 1650 Train loss 0.27 Classification-F1 0.7475330076220064 on epoch=51
05/27/2022 07:59:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.28 on epoch=51
05/27/2022 07:59:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=52
05/27/2022 07:59:08 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.29 on epoch=52
05/27/2022 07:59:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.31 on epoch=52
05/27/2022 07:59:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.19 on epoch=53
05/27/2022 07:59:20 - INFO - __main__ - Global step 1700 Train loss 0.27 Classification-F1 0.7704783723415398 on epoch=53
05/27/2022 07:59:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=53
05/27/2022 07:59:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=53
05/27/2022 07:59:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.16 on epoch=54
05/27/2022 07:59:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.27 on epoch=54
05/27/2022 07:59:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=54
05/27/2022 07:59:41 - INFO - __main__ - Global step 1750 Train loss 0.25 Classification-F1 0.7794297024528619 on epoch=54
05/27/2022 07:59:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7768318772706149 -> 0.7794297024528619 on epoch=54, global_step=1750
05/27/2022 07:59:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=54
05/27/2022 07:59:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.26 on epoch=55
05/27/2022 07:59:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.23 on epoch=55
05/27/2022 07:59:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.21 on epoch=55
05/27/2022 07:59:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.18 on epoch=56
05/27/2022 08:00:00 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.7352882999978616 on epoch=56
05/27/2022 08:00:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.19 on epoch=56
05/27/2022 08:00:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.19 on epoch=56
05/27/2022 08:00:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.25 on epoch=57
05/27/2022 08:00:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=57
05/27/2022 08:00:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.24 on epoch=57
05/27/2022 08:00:20 - INFO - __main__ - Global step 1850 Train loss 0.22 Classification-F1 0.7914669159739581 on epoch=57
05/27/2022 08:00:20 - INFO - __main__ - Saving model with best Classification-F1: 0.7794297024528619 -> 0.7914669159739581 on epoch=57, global_step=1850
05/27/2022 08:00:23 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=58
05/27/2022 08:00:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.22 on epoch=58
05/27/2022 08:00:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.27 on epoch=58
05/27/2022 08:00:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=59
05/27/2022 08:00:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.25 on epoch=59
05/27/2022 08:00:40 - INFO - __main__ - Global step 1900 Train loss 0.21 Classification-F1 0.7752241307481897 on epoch=59
05/27/2022 08:00:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=59
05/27/2022 08:00:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.15 on epoch=59
05/27/2022 08:00:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.20 on epoch=60
05/27/2022 08:00:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.24 on epoch=60
05/27/2022 08:00:53 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.12 on epoch=60
05/27/2022 08:01:00 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.7796471279473015 on epoch=60
05/27/2022 08:01:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.24 on epoch=61
05/27/2022 08:01:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=61
05/27/2022 08:01:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.26 on epoch=61
05/27/2022 08:01:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=62
05/27/2022 08:01:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.18 on epoch=62
05/27/2022 08:01:20 - INFO - __main__ - Global step 2000 Train loss 0.21 Classification-F1 0.7866412741352232 on epoch=62
05/27/2022 08:01:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.22 on epoch=62
05/27/2022 08:01:25 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=63
05/27/2022 08:01:28 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.25 on epoch=63
05/27/2022 08:01:31 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.29 on epoch=63
05/27/2022 08:01:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.24 on epoch=64
05/27/2022 08:01:40 - INFO - __main__ - Global step 2050 Train loss 0.24 Classification-F1 0.7912805126854309 on epoch=64
05/27/2022 08:01:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.15 on epoch=64
05/27/2022 08:01:45 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.26 on epoch=64
05/27/2022 08:01:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.16 on epoch=64
05/27/2022 08:01:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.18 on epoch=65
05/27/2022 08:01:53 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.24 on epoch=65
05/27/2022 08:02:00 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.7825845992877581 on epoch=65
05/27/2022 08:02:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.11 on epoch=65
05/27/2022 08:02:05 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.17 on epoch=66
05/27/2022 08:02:08 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.17 on epoch=66
05/27/2022 08:02:11 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.17 on epoch=66
05/27/2022 08:02:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=67
05/27/2022 08:02:20 - INFO - __main__ - Global step 2150 Train loss 0.16 Classification-F1 0.7803715421475991 on epoch=67
05/27/2022 08:02:23 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.18 on epoch=67
05/27/2022 08:02:25 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.32 on epoch=67
05/27/2022 08:02:28 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=68
05/27/2022 08:02:30 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.22 on epoch=68
05/27/2022 08:02:33 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.24 on epoch=68
05/27/2022 08:02:40 - INFO - __main__ - Global step 2200 Train loss 0.21 Classification-F1 0.8016776079961502 on epoch=68
05/27/2022 08:02:40 - INFO - __main__ - Saving model with best Classification-F1: 0.7914669159739581 -> 0.8016776079961502 on epoch=68, global_step=2200
05/27/2022 08:02:43 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.12 on epoch=69
05/27/2022 08:02:45 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.14 on epoch=69
05/27/2022 08:02:48 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.24 on epoch=69
05/27/2022 08:02:50 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.16 on epoch=69
05/27/2022 08:02:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=70
05/27/2022 08:03:00 - INFO - __main__ - Global step 2250 Train loss 0.17 Classification-F1 0.7686850687783607 on epoch=70
05/27/2022 08:03:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.22 on epoch=70
05/27/2022 08:03:05 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.11 on epoch=70
05/27/2022 08:03:08 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.24 on epoch=71
05/27/2022 08:03:10 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.19 on epoch=71
05/27/2022 08:03:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.20 on epoch=71
05/27/2022 08:03:20 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.7819699326911591 on epoch=71
05/27/2022 08:03:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=72
05/27/2022 08:03:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.13 on epoch=72
05/27/2022 08:03:28 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.23 on epoch=72
05/27/2022 08:03:30 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.23 on epoch=73
05/27/2022 08:03:33 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.21 on epoch=73
05/27/2022 08:03:40 - INFO - __main__ - Global step 2350 Train loss 0.19 Classification-F1 0.798405885004594 on epoch=73
05/27/2022 08:03:43 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.21 on epoch=73
05/27/2022 08:03:45 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.21 on epoch=74
05/27/2022 08:03:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.10 on epoch=74
05/27/2022 08:03:50 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.23 on epoch=74
05/27/2022 08:03:53 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.13 on epoch=74
05/27/2022 08:04:00 - INFO - __main__ - Global step 2400 Train loss 0.17 Classification-F1 0.7969457762682954 on epoch=74
05/27/2022 08:04:03 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.21 on epoch=75
05/27/2022 08:04:05 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.19 on epoch=75
05/27/2022 08:04:08 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=75
05/27/2022 08:04:10 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.10 on epoch=76
05/27/2022 08:04:13 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.11 on epoch=76
05/27/2022 08:04:20 - INFO - __main__ - Global step 2450 Train loss 0.16 Classification-F1 0.8018135834441447 on epoch=76
05/27/2022 08:04:20 - INFO - __main__ - Saving model with best Classification-F1: 0.8016776079961502 -> 0.8018135834441447 on epoch=76, global_step=2450
05/27/2022 08:04:23 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.16 on epoch=76
05/27/2022 08:04:25 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.23 on epoch=77
05/27/2022 08:04:28 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.12 on epoch=77
05/27/2022 08:04:30 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.16 on epoch=77
05/27/2022 08:04:33 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.12 on epoch=78
05/27/2022 08:04:40 - INFO - __main__ - Global step 2500 Train loss 0.16 Classification-F1 0.790833806453167 on epoch=78
05/27/2022 08:04:43 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.11 on epoch=78
05/27/2022 08:04:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.12 on epoch=78
05/27/2022 08:04:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.11 on epoch=79
05/27/2022 08:04:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=79
05/27/2022 08:04:53 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.23 on epoch=79
05/27/2022 08:05:00 - INFO - __main__ - Global step 2550 Train loss 0.13 Classification-F1 0.787456933677864 on epoch=79
05/27/2022 08:05:02 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=79
05/27/2022 08:05:05 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.18 on epoch=80
05/27/2022 08:05:07 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.17 on epoch=80
05/27/2022 08:05:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.13 on epoch=80
05/27/2022 08:05:13 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.14 on epoch=81
05/27/2022 08:05:20 - INFO - __main__ - Global step 2600 Train loss 0.14 Classification-F1 0.7754112069341632 on epoch=81
05/27/2022 08:05:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.19 on epoch=81
05/27/2022 08:05:25 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=81
05/27/2022 08:05:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.13 on epoch=82
05/27/2022 08:05:30 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.05 on epoch=82
05/27/2022 08:05:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=82
05/27/2022 08:05:39 - INFO - __main__ - Global step 2650 Train loss 0.11 Classification-F1 0.799582249485304 on epoch=82
05/27/2022 08:05:42 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.14 on epoch=83
05/27/2022 08:05:44 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.07 on epoch=83
05/27/2022 08:05:47 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.13 on epoch=83
05/27/2022 08:05:49 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=84
05/27/2022 08:05:52 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=84
05/27/2022 08:05:59 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.7655172459943149 on epoch=84
05/27/2022 08:06:02 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.09 on epoch=84
05/27/2022 08:06:04 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=84
05/27/2022 08:06:07 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.24 on epoch=85
05/27/2022 08:06:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.11 on epoch=85
05/27/2022 08:06:12 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.07 on epoch=85
05/27/2022 08:06:19 - INFO - __main__ - Global step 2750 Train loss 0.12 Classification-F1 0.7781484730986545 on epoch=85
05/27/2022 08:06:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.16 on epoch=86
05/27/2022 08:06:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=86
05/27/2022 08:06:27 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.07 on epoch=86
05/27/2022 08:06:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.21 on epoch=87
05/27/2022 08:06:32 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.15 on epoch=87
05/27/2022 08:06:39 - INFO - __main__ - Global step 2800 Train loss 0.14 Classification-F1 0.7944919809147114 on epoch=87
05/27/2022 08:06:41 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.18 on epoch=87
05/27/2022 08:06:44 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=88
05/27/2022 08:06:46 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.09 on epoch=88
05/27/2022 08:06:49 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.17 on epoch=88
05/27/2022 08:06:51 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.09 on epoch=89
05/27/2022 08:06:58 - INFO - __main__ - Global step 2850 Train loss 0.12 Classification-F1 0.7867341075641056 on epoch=89
05/27/2022 08:07:01 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.14 on epoch=89
05/27/2022 08:07:04 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=89
05/27/2022 08:07:06 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.08 on epoch=89
05/27/2022 08:07:09 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.09 on epoch=90
05/27/2022 08:07:11 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=90
05/27/2022 08:07:19 - INFO - __main__ - Global step 2900 Train loss 0.09 Classification-F1 0.8032737109884707 on epoch=90
05/27/2022 08:07:19 - INFO - __main__ - Saving model with best Classification-F1: 0.8018135834441447 -> 0.8032737109884707 on epoch=90, global_step=2900
05/27/2022 08:07:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.06 on epoch=90
05/27/2022 08:07:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.20 on epoch=91
05/27/2022 08:07:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.07 on epoch=91
05/27/2022 08:07:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=91
05/27/2022 08:07:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.19 on epoch=92
05/27/2022 08:07:38 - INFO - __main__ - Global step 2950 Train loss 0.13 Classification-F1 0.7963339395497044 on epoch=92
05/27/2022 08:07:41 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=92
05/27/2022 08:07:43 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.17 on epoch=92
05/27/2022 08:07:46 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.09 on epoch=93
05/27/2022 08:07:49 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.12 on epoch=93
05/27/2022 08:07:51 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.15 on epoch=93
05/27/2022 08:07:52 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 08:07:52 - INFO - __main__ - Printing 3 examples
05/27/2022 08:07:52 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/27/2022 08:07:52 - INFO - __main__ - ['sad']
05/27/2022 08:07:52 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/27/2022 08:07:52 - INFO - __main__ - ['sad']
05/27/2022 08:07:52 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/27/2022 08:07:52 - INFO - __main__ - ['sad']
05/27/2022 08:07:52 - INFO - __main__ - Tokenizing Input ...
05/27/2022 08:07:52 - INFO - __main__ - Tokenizing Output ...
05/27/2022 08:07:53 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 08:07:53 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 08:07:53 - INFO - __main__ - Printing 3 examples
05/27/2022 08:07:53 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/27/2022 08:07:53 - INFO - __main__ - ['sad']
05/27/2022 08:07:53 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/27/2022 08:07:53 - INFO - __main__ - ['sad']
05/27/2022 08:07:53 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/27/2022 08:07:53 - INFO - __main__ - ['sad']
05/27/2022 08:07:53 - INFO - __main__ - Tokenizing Input ...
05/27/2022 08:07:53 - INFO - __main__ - Tokenizing Output ...
05/27/2022 08:07:54 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 08:07:58 - INFO - __main__ - Global step 3000 Train loss 0.12 Classification-F1 0.7556940827209236 on epoch=93
05/27/2022 08:07:58 - INFO - __main__ - save last model!
05/27/2022 08:07:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 08:07:58 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 08:07:58 - INFO - __main__ - Printing 3 examples
05/27/2022 08:07:58 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 08:07:58 - INFO - __main__ - ['others']
05/27/2022 08:07:58 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 08:07:58 - INFO - __main__ - ['others']
05/27/2022 08:07:58 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 08:07:58 - INFO - __main__ - ['others']
05/27/2022 08:07:58 - INFO - __main__ - Tokenizing Input ...
05/27/2022 08:08:00 - INFO - __main__ - Tokenizing Output ...
05/27/2022 08:08:06 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 08:08:10 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 08:08:10 - INFO - __main__ - task name: emo
05/27/2022 08:08:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 08:08:12 - INFO - __main__ - Starting training!
05/27/2022 08:09:21 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_21_0.5_8_predictions.txt
05/27/2022 08:09:21 - INFO - __main__ - Classification-F1 on test data: 0.4545
05/27/2022 08:09:21 - INFO - __main__ - prefix=emo_128_21, lr=0.5, bsz=8, dev_performance=0.8032737109884707, test_performance=0.4544901318276926
05/27/2022 08:09:21 - INFO - __main__ - Running ... prefix=emo_128_21, lr=0.4, bsz=8 ...
05/27/2022 08:09:22 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 08:09:22 - INFO - __main__ - Printing 3 examples
05/27/2022 08:09:22 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/27/2022 08:09:22 - INFO - __main__ - ['sad']
05/27/2022 08:09:22 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/27/2022 08:09:22 - INFO - __main__ - ['sad']
05/27/2022 08:09:22 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/27/2022 08:09:22 - INFO - __main__ - ['sad']
05/27/2022 08:09:22 - INFO - __main__ - Tokenizing Input ...
05/27/2022 08:09:22 - INFO - __main__ - Tokenizing Output ...
05/27/2022 08:09:23 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 08:09:23 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 08:09:23 - INFO - __main__ - Printing 3 examples
05/27/2022 08:09:23 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/27/2022 08:09:23 - INFO - __main__ - ['sad']
05/27/2022 08:09:23 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/27/2022 08:09:23 - INFO - __main__ - ['sad']
05/27/2022 08:09:23 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/27/2022 08:09:23 - INFO - __main__ - ['sad']
05/27/2022 08:09:23 - INFO - __main__ - Tokenizing Input ...
05/27/2022 08:09:23 - INFO - __main__ - Tokenizing Output ...
05/27/2022 08:09:23 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 08:09:42 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 08:09:42 - INFO - __main__ - task name: emo
05/27/2022 08:09:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 08:09:43 - INFO - __main__ - Starting training!
05/27/2022 08:09:46 - INFO - __main__ - Step 10 Global step 10 Train loss 6.30 on epoch=0
05/27/2022 08:09:48 - INFO - __main__ - Step 20 Global step 20 Train loss 2.99 on epoch=0
05/27/2022 08:09:51 - INFO - __main__ - Step 30 Global step 30 Train loss 1.75 on epoch=0
05/27/2022 08:09:53 - INFO - __main__ - Step 40 Global step 40 Train loss 1.34 on epoch=1
05/27/2022 08:09:56 - INFO - __main__ - Step 50 Global step 50 Train loss 1.14 on epoch=1
05/27/2022 08:10:03 - INFO - __main__ - Global step 50 Train loss 2.70 Classification-F1 0.1041596334699783 on epoch=1
05/27/2022 08:10:03 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1041596334699783 on epoch=1, global_step=50
05/27/2022 08:10:06 - INFO - __main__ - Step 60 Global step 60 Train loss 1.12 on epoch=1
05/27/2022 08:10:08 - INFO - __main__ - Step 70 Global step 70 Train loss 1.00 on epoch=2
05/27/2022 08:10:11 - INFO - __main__ - Step 80 Global step 80 Train loss 1.08 on epoch=2
05/27/2022 08:10:13 - INFO - __main__ - Step 90 Global step 90 Train loss 1.00 on epoch=2
05/27/2022 08:10:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.82 on epoch=3
05/27/2022 08:10:23 - INFO - __main__ - Global step 100 Train loss 1.00 Classification-F1 0.10555359979557738 on epoch=3
05/27/2022 08:10:23 - INFO - __main__ - Saving model with best Classification-F1: 0.1041596334699783 -> 0.10555359979557738 on epoch=3, global_step=100
05/27/2022 08:10:26 - INFO - __main__ - Step 110 Global step 110 Train loss 0.92 on epoch=3
05/27/2022 08:10:28 - INFO - __main__ - Step 120 Global step 120 Train loss 1.00 on epoch=3
05/27/2022 08:10:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.89 on epoch=4
05/27/2022 08:10:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.94 on epoch=4
05/27/2022 08:10:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.95 on epoch=4
05/27/2022 08:10:43 - INFO - __main__ - Global step 150 Train loss 0.94 Classification-F1 0.1774965800273598 on epoch=4
05/27/2022 08:10:43 - INFO - __main__ - Saving model with best Classification-F1: 0.10555359979557738 -> 0.1774965800273598 on epoch=4, global_step=150
05/27/2022 08:10:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.92 on epoch=4
05/27/2022 08:10:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.89 on epoch=5
05/27/2022 08:10:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.85 on epoch=5
05/27/2022 08:10:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.91 on epoch=5
05/27/2022 08:10:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.83 on epoch=6
05/27/2022 08:11:03 - INFO - __main__ - Global step 200 Train loss 0.88 Classification-F1 0.13397231096911608 on epoch=6
05/27/2022 08:11:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.91 on epoch=6
05/27/2022 08:11:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.91 on epoch=6
05/27/2022 08:11:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.82 on epoch=7
05/27/2022 08:11:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=7
05/27/2022 08:11:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.96 on epoch=7
05/27/2022 08:11:23 - INFO - __main__ - Global step 250 Train loss 0.90 Classification-F1 0.17041962778508554 on epoch=7
05/27/2022 08:11:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=8
05/27/2022 08:11:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.82 on epoch=8
05/27/2022 08:11:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.89 on epoch=8
05/27/2022 08:11:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.89 on epoch=9
05/27/2022 08:11:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.77 on epoch=9
05/27/2022 08:11:42 - INFO - __main__ - Global step 300 Train loss 0.84 Classification-F1 0.11252731175581289 on epoch=9
05/27/2022 08:11:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.84 on epoch=9
05/27/2022 08:11:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.75 on epoch=9
05/27/2022 08:11:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.83 on epoch=10
05/27/2022 08:11:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.81 on epoch=10
05/27/2022 08:11:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.96 on epoch=10
05/27/2022 08:12:02 - INFO - __main__ - Global step 350 Train loss 0.84 Classification-F1 0.2483051642577852 on epoch=10
05/27/2022 08:12:02 - INFO - __main__ - Saving model with best Classification-F1: 0.1774965800273598 -> 0.2483051642577852 on epoch=10, global_step=350
05/27/2022 08:12:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.77 on epoch=11
05/27/2022 08:12:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.89 on epoch=11
05/27/2022 08:12:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.87 on epoch=11
05/27/2022 08:12:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.87 on epoch=12
05/27/2022 08:12:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.74 on epoch=12
05/27/2022 08:12:22 - INFO - __main__ - Global step 400 Train loss 0.83 Classification-F1 0.2588218992348964 on epoch=12
05/27/2022 08:12:22 - INFO - __main__ - Saving model with best Classification-F1: 0.2483051642577852 -> 0.2588218992348964 on epoch=12, global_step=400
05/27/2022 08:12:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.83 on epoch=12
05/27/2022 08:12:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.76 on epoch=13
05/27/2022 08:12:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.85 on epoch=13
05/27/2022 08:12:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.78 on epoch=13
05/27/2022 08:12:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.76 on epoch=14
05/27/2022 08:12:42 - INFO - __main__ - Global step 450 Train loss 0.79 Classification-F1 0.2545089413510466 on epoch=14
05/27/2022 08:12:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.78 on epoch=14
05/27/2022 08:12:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.79 on epoch=14
05/27/2022 08:12:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.79 on epoch=14
05/27/2022 08:12:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.80 on epoch=15
05/27/2022 08:12:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.73 on epoch=15
05/27/2022 08:13:02 - INFO - __main__ - Global step 500 Train loss 0.78 Classification-F1 0.5076701950039215 on epoch=15
05/27/2022 08:13:02 - INFO - __main__ - Saving model with best Classification-F1: 0.2588218992348964 -> 0.5076701950039215 on epoch=15, global_step=500
05/27/2022 08:13:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.76 on epoch=15
05/27/2022 08:13:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.66 on epoch=16
05/27/2022 08:13:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.63 on epoch=16
05/27/2022 08:13:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.64 on epoch=16
05/27/2022 08:13:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.63 on epoch=17
05/27/2022 08:13:22 - INFO - __main__ - Global step 550 Train loss 0.66 Classification-F1 0.4943853868879351 on epoch=17
05/27/2022 08:13:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.55 on epoch=17
05/27/2022 08:13:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.65 on epoch=17
05/27/2022 08:13:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.61 on epoch=18
05/27/2022 08:13:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.61 on epoch=18
05/27/2022 08:13:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.65 on epoch=18
05/27/2022 08:13:41 - INFO - __main__ - Global step 600 Train loss 0.61 Classification-F1 0.502807267665978 on epoch=18
05/27/2022 08:13:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.72 on epoch=19
05/27/2022 08:13:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.55 on epoch=19
05/27/2022 08:13:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=19
05/27/2022 08:13:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.66 on epoch=19
05/27/2022 08:13:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.61 on epoch=20
05/27/2022 08:14:01 - INFO - __main__ - Global step 650 Train loss 0.61 Classification-F1 0.5321593164210963 on epoch=20
05/27/2022 08:14:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5076701950039215 -> 0.5321593164210963 on epoch=20, global_step=650
05/27/2022 08:14:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.58 on epoch=20
05/27/2022 08:14:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.55 on epoch=20
05/27/2022 08:14:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=21
05/27/2022 08:14:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.51 on epoch=21
05/27/2022 08:14:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.65 on epoch=21
05/27/2022 08:14:21 - INFO - __main__ - Global step 700 Train loss 0.55 Classification-F1 0.6234421140426769 on epoch=21
05/27/2022 08:14:21 - INFO - __main__ - Saving model with best Classification-F1: 0.5321593164210963 -> 0.6234421140426769 on epoch=21, global_step=700
05/27/2022 08:14:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.67 on epoch=22
05/27/2022 08:14:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=22
05/27/2022 08:14:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.52 on epoch=22
05/27/2022 08:14:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.61 on epoch=23
05/27/2022 08:14:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=23
05/27/2022 08:14:41 - INFO - __main__ - Global step 750 Train loss 0.54 Classification-F1 0.6592152797990558 on epoch=23
05/27/2022 08:14:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6234421140426769 -> 0.6592152797990558 on epoch=23, global_step=750
05/27/2022 08:14:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.57 on epoch=23
05/27/2022 08:14:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.52 on epoch=24
05/27/2022 08:14:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.55 on epoch=24
05/27/2022 08:14:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=24
05/27/2022 08:14:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.58 on epoch=24
05/27/2022 08:15:01 - INFO - __main__ - Global step 800 Train loss 0.53 Classification-F1 0.6370040045583153 on epoch=24
05/27/2022 08:15:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.54 on epoch=25
05/27/2022 08:15:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=25
05/27/2022 08:15:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.50 on epoch=25
05/27/2022 08:15:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.53 on epoch=26
05/27/2022 08:15:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.49 on epoch=26
05/27/2022 08:15:20 - INFO - __main__ - Global step 850 Train loss 0.50 Classification-F1 0.7054895488677501 on epoch=26
05/27/2022 08:15:20 - INFO - __main__ - Saving model with best Classification-F1: 0.6592152797990558 -> 0.7054895488677501 on epoch=26, global_step=850
05/27/2022 08:15:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=26
05/27/2022 08:15:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=27
05/27/2022 08:15:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=27
05/27/2022 08:15:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=27
05/27/2022 08:15:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.45 on epoch=28
05/27/2022 08:15:40 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.7168546290100345 on epoch=28
05/27/2022 08:15:40 - INFO - __main__ - Saving model with best Classification-F1: 0.7054895488677501 -> 0.7168546290100345 on epoch=28, global_step=900
05/27/2022 08:15:43 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=28
05/27/2022 08:15:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=28
05/27/2022 08:15:48 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=29
05/27/2022 08:15:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=29
05/27/2022 08:15:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=29
05/27/2022 08:16:00 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.7031787802840435 on epoch=29
05/27/2022 08:16:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=29
05/27/2022 08:16:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.46 on epoch=30
05/27/2022 08:16:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=30
05/27/2022 08:16:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=30
05/27/2022 08:16:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=31
05/27/2022 08:16:20 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.5640218627561528 on epoch=31
05/27/2022 08:16:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.33 on epoch=31
05/27/2022 08:16:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.44 on epoch=31
05/27/2022 08:16:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=32
05/27/2022 08:16:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.30 on epoch=32
05/27/2022 08:16:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.49 on epoch=32
05/27/2022 08:16:40 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.7046188926170136 on epoch=32
05/27/2022 08:16:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=33
05/27/2022 08:16:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=33
05/27/2022 08:16:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.49 on epoch=33
05/27/2022 08:16:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=34
05/27/2022 08:16:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=34
05/27/2022 08:17:00 - INFO - __main__ - Global step 1100 Train loss 0.40 Classification-F1 0.7577942569839231 on epoch=34
05/27/2022 08:17:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7168546290100345 -> 0.7577942569839231 on epoch=34, global_step=1100
05/27/2022 08:17:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=34
05/27/2022 08:17:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=34
05/27/2022 08:17:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.33 on epoch=35
05/27/2022 08:17:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=35
05/27/2022 08:17:13 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.34 on epoch=35
05/27/2022 08:17:20 - INFO - __main__ - Global step 1150 Train loss 0.36 Classification-F1 0.7341832646520147 on epoch=35
05/27/2022 08:17:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.34 on epoch=36
05/27/2022 08:17:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.26 on epoch=36
05/27/2022 08:17:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=36
05/27/2022 08:17:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=37
05/27/2022 08:17:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.36 on epoch=37
05/27/2022 08:17:39 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.7238981081016194 on epoch=37
05/27/2022 08:17:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=37
05/27/2022 08:17:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=38
05/27/2022 08:17:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.30 on epoch=38
05/27/2022 08:17:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=38
05/27/2022 08:17:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.49 on epoch=39
05/27/2022 08:17:59 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.7724391522100448 on epoch=39
05/27/2022 08:17:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7577942569839231 -> 0.7724391522100448 on epoch=39, global_step=1250
05/27/2022 08:18:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=39
05/27/2022 08:18:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=39
05/27/2022 08:18:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=39
05/27/2022 08:18:10 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=40
05/27/2022 08:18:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.30 on epoch=40
05/27/2022 08:18:19 - INFO - __main__ - Global step 1300 Train loss 0.31 Classification-F1 0.761659680781296 on epoch=40
05/27/2022 08:18:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.29 on epoch=40
05/27/2022 08:18:25 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.37 on epoch=41
05/27/2022 08:18:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=41
05/27/2022 08:18:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.37 on epoch=41
05/27/2022 08:18:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.32 on epoch=42
05/27/2022 08:18:39 - INFO - __main__ - Global step 1350 Train loss 0.31 Classification-F1 0.7489085906974009 on epoch=42
05/27/2022 08:18:42 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=42
05/27/2022 08:18:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.30 on epoch=42
05/27/2022 08:18:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.25 on epoch=43
05/27/2022 08:18:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.24 on epoch=43
05/27/2022 08:18:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=43
05/27/2022 08:18:59 - INFO - __main__ - Global step 1400 Train loss 0.26 Classification-F1 0.7461758514898612 on epoch=43
05/27/2022 08:19:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.31 on epoch=44
05/27/2022 08:19:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=44
05/27/2022 08:19:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.30 on epoch=44
05/27/2022 08:19:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.31 on epoch=44
05/27/2022 08:19:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.32 on epoch=45
05/27/2022 08:19:19 - INFO - __main__ - Global step 1450 Train loss 0.28 Classification-F1 0.7573849170009961 on epoch=45
05/27/2022 08:19:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.33 on epoch=45
05/27/2022 08:19:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=45
05/27/2022 08:19:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=46
05/27/2022 08:19:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=46
05/27/2022 08:19:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.33 on epoch=46
05/27/2022 08:19:39 - INFO - __main__ - Global step 1500 Train loss 0.28 Classification-F1 0.7527048444233628 on epoch=46
05/27/2022 08:19:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=47
05/27/2022 08:19:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=47
05/27/2022 08:19:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=47
05/27/2022 08:19:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=48
05/27/2022 08:19:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.21 on epoch=48
05/27/2022 08:20:00 - INFO - __main__ - Global step 1550 Train loss 0.26 Classification-F1 0.7755018409242651 on epoch=48
05/27/2022 08:20:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7724391522100448 -> 0.7755018409242651 on epoch=48, global_step=1550
05/27/2022 08:20:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.30 on epoch=48
05/27/2022 08:20:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.29 on epoch=49
05/27/2022 08:20:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=49
05/27/2022 08:20:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.22 on epoch=49
05/27/2022 08:20:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.37 on epoch=49
05/27/2022 08:20:19 - INFO - __main__ - Global step 1600 Train loss 0.29 Classification-F1 0.7155903852712928 on epoch=49
05/27/2022 08:20:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=50
05/27/2022 08:20:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=50
05/27/2022 08:20:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.26 on epoch=50
05/27/2022 08:20:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=51
05/27/2022 08:20:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=51
05/27/2022 08:20:39 - INFO - __main__ - Global step 1650 Train loss 0.26 Classification-F1 0.7536659049584746 on epoch=51
05/27/2022 08:20:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.25 on epoch=51
05/27/2022 08:20:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=52
05/27/2022 08:20:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=52
05/27/2022 08:20:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=52
05/27/2022 08:20:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.31 on epoch=53
05/27/2022 08:20:59 - INFO - __main__ - Global step 1700 Train loss 0.25 Classification-F1 0.7943298854087899 on epoch=53
05/27/2022 08:20:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7755018409242651 -> 0.7943298854087899 on epoch=53, global_step=1700
05/27/2022 08:21:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.28 on epoch=53
05/27/2022 08:21:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.28 on epoch=53
05/27/2022 08:21:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=54
05/27/2022 08:21:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.22 on epoch=54
05/27/2022 08:21:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=54
05/27/2022 08:21:19 - INFO - __main__ - Global step 1750 Train loss 0.24 Classification-F1 0.7478867666803364 on epoch=54
05/27/2022 08:21:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=54
05/27/2022 08:21:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.38 on epoch=55
05/27/2022 08:21:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.21 on epoch=55
05/27/2022 08:21:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.28 on epoch=55
05/27/2022 08:21:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=56
05/27/2022 08:21:39 - INFO - __main__ - Global step 1800 Train loss 0.30 Classification-F1 0.6773261214711803 on epoch=56
05/27/2022 08:21:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=56
05/27/2022 08:21:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=56
05/27/2022 08:21:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.27 on epoch=57
05/27/2022 08:21:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.28 on epoch=57
05/27/2022 08:21:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=57
05/27/2022 08:21:59 - INFO - __main__ - Global step 1850 Train loss 0.29 Classification-F1 0.7584106296940198 on epoch=57
05/27/2022 08:22:02 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.29 on epoch=58
05/27/2022 08:22:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.22 on epoch=58
05/27/2022 08:22:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=58
05/27/2022 08:22:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.24 on epoch=59
05/27/2022 08:22:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.20 on epoch=59
05/27/2022 08:22:19 - INFO - __main__ - Global step 1900 Train loss 0.25 Classification-F1 0.7769034401354917 on epoch=59
05/27/2022 08:22:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=59
05/27/2022 08:22:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=59
05/27/2022 08:22:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.24 on epoch=60
05/27/2022 08:22:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.23 on epoch=60
05/27/2022 08:22:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.33 on epoch=60
05/27/2022 08:22:39 - INFO - __main__ - Global step 1950 Train loss 0.24 Classification-F1 0.7447414187206627 on epoch=60
05/27/2022 08:22:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=61
05/27/2022 08:22:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.24 on epoch=61
05/27/2022 08:22:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.26 on epoch=61
05/27/2022 08:22:49 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.22 on epoch=62
05/27/2022 08:22:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.18 on epoch=62
05/27/2022 08:22:58 - INFO - __main__ - Global step 2000 Train loss 0.22 Classification-F1 0.7357986932413043 on epoch=62
05/27/2022 08:23:01 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.28 on epoch=62
05/27/2022 08:23:04 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.21 on epoch=63
05/27/2022 08:23:06 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.14 on epoch=63
05/27/2022 08:23:09 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.18 on epoch=63
05/27/2022 08:23:11 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.18 on epoch=64
05/27/2022 08:23:18 - INFO - __main__ - Global step 2050 Train loss 0.20 Classification-F1 0.7430449186446204 on epoch=64
05/27/2022 08:23:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.14 on epoch=64
05/27/2022 08:23:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.20 on epoch=64
05/27/2022 08:23:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=64
05/27/2022 08:23:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=65
05/27/2022 08:23:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.16 on epoch=65
05/27/2022 08:23:38 - INFO - __main__ - Global step 2100 Train loss 0.17 Classification-F1 0.7738628970891087 on epoch=65
05/27/2022 08:23:41 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.25 on epoch=65
05/27/2022 08:23:43 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.20 on epoch=66
05/27/2022 08:23:46 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.12 on epoch=66
05/27/2022 08:23:48 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.25 on epoch=66
05/27/2022 08:23:51 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.26 on epoch=67
05/27/2022 08:23:58 - INFO - __main__ - Global step 2150 Train loss 0.22 Classification-F1 0.7811629133087294 on epoch=67
05/27/2022 08:24:00 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.14 on epoch=67
05/27/2022 08:24:03 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.16 on epoch=67
05/27/2022 08:24:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=68
05/27/2022 08:24:08 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.17 on epoch=68
05/27/2022 08:24:11 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.22 on epoch=68
05/27/2022 08:24:18 - INFO - __main__ - Global step 2200 Train loss 0.18 Classification-F1 0.7698760341818074 on epoch=68
05/27/2022 08:24:20 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.23 on epoch=69
05/27/2022 08:24:23 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=69
05/27/2022 08:24:25 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.15 on epoch=69
05/27/2022 08:24:28 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.21 on epoch=69
05/27/2022 08:24:30 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.32 on epoch=70
05/27/2022 08:24:37 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.7764182459387177 on epoch=70
05/27/2022 08:24:40 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.18 on epoch=70
05/27/2022 08:24:42 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=70
05/27/2022 08:24:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.21 on epoch=71
05/27/2022 08:24:48 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=71
05/27/2022 08:24:50 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=71
05/27/2022 08:24:57 - INFO - __main__ - Global step 2300 Train loss 0.18 Classification-F1 0.7718420624251098 on epoch=71
05/27/2022 08:25:00 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.29 on epoch=72
05/27/2022 08:25:02 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.15 on epoch=72
05/27/2022 08:25:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.19 on epoch=72
05/27/2022 08:25:07 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.15 on epoch=73
05/27/2022 08:25:10 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=73
05/27/2022 08:25:17 - INFO - __main__ - Global step 2350 Train loss 0.18 Classification-F1 0.7712356185544791 on epoch=73
05/27/2022 08:25:20 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.16 on epoch=73
05/27/2022 08:25:22 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.23 on epoch=74
05/27/2022 08:25:25 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.20 on epoch=74
05/27/2022 08:25:27 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=74
05/27/2022 08:25:30 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.19 on epoch=74
05/27/2022 08:25:37 - INFO - __main__ - Global step 2400 Train loss 0.19 Classification-F1 0.750005298525466 on epoch=74
05/27/2022 08:25:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.21 on epoch=75
05/27/2022 08:25:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=75
05/27/2022 08:25:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=75
05/27/2022 08:25:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.16 on epoch=76
05/27/2022 08:25:50 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.10 on epoch=76
05/27/2022 08:25:57 - INFO - __main__ - Global step 2450 Train loss 0.14 Classification-F1 0.7727430442333967 on epoch=76
05/27/2022 08:25:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.19 on epoch=76
05/27/2022 08:26:02 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.24 on epoch=77
05/27/2022 08:26:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=77
05/27/2022 08:26:07 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.24 on epoch=77
05/27/2022 08:26:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.17 on epoch=78
05/27/2022 08:26:17 - INFO - __main__ - Global step 2500 Train loss 0.18 Classification-F1 0.7945763970053307 on epoch=78
05/27/2022 08:26:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7943298854087899 -> 0.7945763970053307 on epoch=78, global_step=2500
05/27/2022 08:26:19 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.09 on epoch=78
05/27/2022 08:26:22 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.15 on epoch=78
05/27/2022 08:26:24 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.11 on epoch=79
05/27/2022 08:26:27 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=79
05/27/2022 08:26:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.12 on epoch=79
05/27/2022 08:26:36 - INFO - __main__ - Global step 2550 Train loss 0.11 Classification-F1 0.7610888525505013 on epoch=79
05/27/2022 08:26:39 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.18 on epoch=79
05/27/2022 08:26:41 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.15 on epoch=80
05/27/2022 08:26:44 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.15 on epoch=80
05/27/2022 08:26:47 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=80
05/27/2022 08:26:49 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=81
05/27/2022 08:26:56 - INFO - __main__ - Global step 2600 Train loss 0.16 Classification-F1 0.7490706040962875 on epoch=81
05/27/2022 08:26:59 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.14 on epoch=81
05/27/2022 08:27:01 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=81
05/27/2022 08:27:04 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.15 on epoch=82
05/27/2022 08:27:06 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.16 on epoch=82
05/27/2022 08:27:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.11 on epoch=82
05/27/2022 08:27:16 - INFO - __main__ - Global step 2650 Train loss 0.15 Classification-F1 0.7477778788931183 on epoch=82
05/27/2022 08:27:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.20 on epoch=83
05/27/2022 08:27:21 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.13 on epoch=83
05/27/2022 08:27:24 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.13 on epoch=83
05/27/2022 08:27:26 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.12 on epoch=84
05/27/2022 08:27:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.11 on epoch=84
05/27/2022 08:27:36 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.7791402431744191 on epoch=84
05/27/2022 08:27:38 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.14 on epoch=84
05/27/2022 08:27:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.16 on epoch=84
05/27/2022 08:27:43 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.20 on epoch=85
05/27/2022 08:27:46 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.12 on epoch=85
05/27/2022 08:27:48 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.12 on epoch=85
05/27/2022 08:27:55 - INFO - __main__ - Global step 2750 Train loss 0.15 Classification-F1 0.7842084923246446 on epoch=85
05/27/2022 08:27:58 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.12 on epoch=86
05/27/2022 08:28:01 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.14 on epoch=86
05/27/2022 08:28:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.21 on epoch=86
05/27/2022 08:28:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=87
05/27/2022 08:28:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=87
05/27/2022 08:28:15 - INFO - __main__ - Global step 2800 Train loss 0.13 Classification-F1 0.7670171880698198 on epoch=87
05/27/2022 08:28:18 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.08 on epoch=87
05/27/2022 08:28:20 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.20 on epoch=88
05/27/2022 08:28:23 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.16 on epoch=88
05/27/2022 08:28:26 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.13 on epoch=88
05/27/2022 08:28:28 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.14 on epoch=89
05/27/2022 08:28:35 - INFO - __main__ - Global step 2850 Train loss 0.14 Classification-F1 0.7874577571391194 on epoch=89
05/27/2022 08:28:38 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.16 on epoch=89
05/27/2022 08:28:40 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=89
05/27/2022 08:28:43 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.14 on epoch=89
05/27/2022 08:28:45 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.18 on epoch=90
05/27/2022 08:28:48 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.11 on epoch=90
05/27/2022 08:28:55 - INFO - __main__ - Global step 2900 Train loss 0.13 Classification-F1 0.8100253000203617 on epoch=90
05/27/2022 08:28:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7945763970053307 -> 0.8100253000203617 on epoch=90, global_step=2900
05/27/2022 08:28:58 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=90
05/27/2022 08:29:00 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.17 on epoch=91
05/27/2022 08:29:03 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.10 on epoch=91
05/27/2022 08:29:05 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=91
05/27/2022 08:29:08 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.18 on epoch=92
05/27/2022 08:29:15 - INFO - __main__ - Global step 2950 Train loss 0.12 Classification-F1 0.7692873625960763 on epoch=92
05/27/2022 08:29:18 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.09 on epoch=92
05/27/2022 08:29:20 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.13 on epoch=92
05/27/2022 08:29:23 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.13 on epoch=93
05/27/2022 08:29:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.09 on epoch=93
05/27/2022 08:29:28 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.10 on epoch=93
05/27/2022 08:29:29 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 08:29:29 - INFO - __main__ - Printing 3 examples
05/27/2022 08:29:29 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/27/2022 08:29:29 - INFO - __main__ - ['sad']
05/27/2022 08:29:29 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/27/2022 08:29:29 - INFO - __main__ - ['sad']
05/27/2022 08:29:29 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/27/2022 08:29:29 - INFO - __main__ - ['sad']
05/27/2022 08:29:29 - INFO - __main__ - Tokenizing Input ...
05/27/2022 08:29:29 - INFO - __main__ - Tokenizing Output ...
05/27/2022 08:29:30 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 08:29:30 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 08:29:30 - INFO - __main__ - Printing 3 examples
05/27/2022 08:29:30 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/27/2022 08:29:30 - INFO - __main__ - ['sad']
05/27/2022 08:29:30 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/27/2022 08:29:30 - INFO - __main__ - ['sad']
05/27/2022 08:29:30 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/27/2022 08:29:30 - INFO - __main__ - ['sad']
05/27/2022 08:29:30 - INFO - __main__ - Tokenizing Input ...
05/27/2022 08:29:30 - INFO - __main__ - Tokenizing Output ...
05/27/2022 08:29:30 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 08:29:35 - INFO - __main__ - Global step 3000 Train loss 0.11 Classification-F1 0.8059295891309746 on epoch=93
05/27/2022 08:29:35 - INFO - __main__ - save last model!
05/27/2022 08:29:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 08:29:35 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 08:29:35 - INFO - __main__ - Printing 3 examples
05/27/2022 08:29:35 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 08:29:35 - INFO - __main__ - ['others']
05/27/2022 08:29:35 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 08:29:35 - INFO - __main__ - ['others']
05/27/2022 08:29:35 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 08:29:35 - INFO - __main__ - ['others']
05/27/2022 08:29:35 - INFO - __main__ - Tokenizing Input ...
05/27/2022 08:29:37 - INFO - __main__ - Tokenizing Output ...
05/27/2022 08:29:42 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 08:29:47 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 08:29:47 - INFO - __main__ - task name: emo
05/27/2022 08:29:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 08:29:48 - INFO - __main__ - Starting training!
05/27/2022 08:30:58 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_21_0.4_8_predictions.txt
05/27/2022 08:30:58 - INFO - __main__ - Classification-F1 on test data: 0.5170
05/27/2022 08:30:59 - INFO - __main__ - prefix=emo_128_21, lr=0.4, bsz=8, dev_performance=0.8100253000203617, test_performance=0.5169874980298335
05/27/2022 08:30:59 - INFO - __main__ - Running ... prefix=emo_128_21, lr=0.3, bsz=8 ...
05/27/2022 08:31:00 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 08:31:00 - INFO - __main__ - Printing 3 examples
05/27/2022 08:31:00 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/27/2022 08:31:00 - INFO - __main__ - ['sad']
05/27/2022 08:31:00 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/27/2022 08:31:00 - INFO - __main__ - ['sad']
05/27/2022 08:31:00 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/27/2022 08:31:00 - INFO - __main__ - ['sad']
05/27/2022 08:31:00 - INFO - __main__ - Tokenizing Input ...
05/27/2022 08:31:00 - INFO - __main__ - Tokenizing Output ...
05/27/2022 08:31:00 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 08:31:00 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 08:31:00 - INFO - __main__ - Printing 3 examples
05/27/2022 08:31:00 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/27/2022 08:31:00 - INFO - __main__ - ['sad']
05/27/2022 08:31:00 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/27/2022 08:31:00 - INFO - __main__ - ['sad']
05/27/2022 08:31:00 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/27/2022 08:31:00 - INFO - __main__ - ['sad']
05/27/2022 08:31:00 - INFO - __main__ - Tokenizing Input ...
05/27/2022 08:31:00 - INFO - __main__ - Tokenizing Output ...
05/27/2022 08:31:01 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 08:31:20 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 08:31:20 - INFO - __main__ - task name: emo
05/27/2022 08:31:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 08:31:21 - INFO - __main__ - Starting training!
05/27/2022 08:31:23 - INFO - __main__ - Step 10 Global step 10 Train loss 7.53 on epoch=0
05/27/2022 08:31:26 - INFO - __main__ - Step 20 Global step 20 Train loss 4.40 on epoch=0
05/27/2022 08:31:29 - INFO - __main__ - Step 30 Global step 30 Train loss 2.08 on epoch=0
05/27/2022 08:31:31 - INFO - __main__ - Step 40 Global step 40 Train loss 1.40 on epoch=1
05/27/2022 08:31:34 - INFO - __main__ - Step 50 Global step 50 Train loss 1.26 on epoch=1
05/27/2022 08:31:41 - INFO - __main__ - Global step 50 Train loss 3.33 Classification-F1 0.12645404096700347 on epoch=1
05/27/2022 08:31:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.12645404096700347 on epoch=1, global_step=50
05/27/2022 08:31:44 - INFO - __main__ - Step 60 Global step 60 Train loss 1.07 on epoch=1
05/27/2022 08:31:46 - INFO - __main__ - Step 70 Global step 70 Train loss 1.20 on epoch=2
05/27/2022 08:31:49 - INFO - __main__ - Step 80 Global step 80 Train loss 1.15 on epoch=2
05/27/2022 08:31:51 - INFO - __main__ - Step 90 Global step 90 Train loss 1.04 on epoch=2
05/27/2022 08:31:54 - INFO - __main__ - Step 100 Global step 100 Train loss 1.07 on epoch=3
05/27/2022 08:32:01 - INFO - __main__ - Global step 100 Train loss 1.11 Classification-F1 0.1849900759068629 on epoch=3
05/27/2022 08:32:01 - INFO - __main__ - Saving model with best Classification-F1: 0.12645404096700347 -> 0.1849900759068629 on epoch=3, global_step=100
05/27/2022 08:32:03 - INFO - __main__ - Step 110 Global step 110 Train loss 1.03 on epoch=3
05/27/2022 08:32:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.97 on epoch=3
05/27/2022 08:32:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.96 on epoch=4
05/27/2022 08:32:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.93 on epoch=4
05/27/2022 08:32:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.90 on epoch=4
05/27/2022 08:32:21 - INFO - __main__ - Global step 150 Train loss 0.96 Classification-F1 0.17706696959861518 on epoch=4
05/27/2022 08:32:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.97 on epoch=4
05/27/2022 08:32:26 - INFO - __main__ - Step 170 Global step 170 Train loss 1.08 on epoch=5
05/27/2022 08:32:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.91 on epoch=5
05/27/2022 08:32:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.97 on epoch=5
05/27/2022 08:32:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.93 on epoch=6
05/27/2022 08:32:40 - INFO - __main__ - Global step 200 Train loss 0.97 Classification-F1 0.2019295122796917 on epoch=6
05/27/2022 08:32:40 - INFO - __main__ - Saving model with best Classification-F1: 0.1849900759068629 -> 0.2019295122796917 on epoch=6, global_step=200
05/27/2022 08:32:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.92 on epoch=6
05/27/2022 08:32:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.90 on epoch=6
05/27/2022 08:32:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.95 on epoch=7
05/27/2022 08:32:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.87 on epoch=7
05/27/2022 08:32:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.84 on epoch=7
05/27/2022 08:33:00 - INFO - __main__ - Global step 250 Train loss 0.89 Classification-F1 0.18941810085759186 on epoch=7
05/27/2022 08:33:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=8
05/27/2022 08:33:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.87 on epoch=8
05/27/2022 08:33:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.81 on epoch=8
05/27/2022 08:33:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.87 on epoch=9
05/27/2022 08:33:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.92 on epoch=9
05/27/2022 08:33:20 - INFO - __main__ - Global step 300 Train loss 0.87 Classification-F1 0.1164454097737456 on epoch=9
05/27/2022 08:33:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.90 on epoch=9
05/27/2022 08:33:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.84 on epoch=9
05/27/2022 08:33:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.84 on epoch=10
05/27/2022 08:33:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.88 on epoch=10
05/27/2022 08:33:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.90 on epoch=10
05/27/2022 08:33:40 - INFO - __main__ - Global step 350 Train loss 0.87 Classification-F1 0.20849867724867727 on epoch=10
05/27/2022 08:33:40 - INFO - __main__ - Saving model with best Classification-F1: 0.2019295122796917 -> 0.20849867724867727 on epoch=10, global_step=350
05/27/2022 08:33:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.83 on epoch=11
05/27/2022 08:33:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.91 on epoch=11
05/27/2022 08:33:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.84 on epoch=11
05/27/2022 08:33:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.81 on epoch=12
05/27/2022 08:33:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.78 on epoch=12
05/27/2022 08:34:00 - INFO - __main__ - Global step 400 Train loss 0.83 Classification-F1 0.10800578731613214 on epoch=12
05/27/2022 08:34:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.90 on epoch=12
05/27/2022 08:34:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.79 on epoch=13
05/27/2022 08:34:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.80 on epoch=13
05/27/2022 08:34:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.87 on epoch=13
05/27/2022 08:34:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.76 on epoch=14
05/27/2022 08:34:19 - INFO - __main__ - Global step 450 Train loss 0.82 Classification-F1 0.19045466287780932 on epoch=14
05/27/2022 08:34:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.86 on epoch=14
05/27/2022 08:34:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.83 on epoch=14
05/27/2022 08:34:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.79 on epoch=14
05/27/2022 08:34:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.78 on epoch=15
05/27/2022 08:34:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.80 on epoch=15
05/27/2022 08:34:39 - INFO - __main__ - Global step 500 Train loss 0.81 Classification-F1 0.36780623262457174 on epoch=15
05/27/2022 08:34:39 - INFO - __main__ - Saving model with best Classification-F1: 0.20849867724867727 -> 0.36780623262457174 on epoch=15, global_step=500
05/27/2022 08:34:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.81 on epoch=15
05/27/2022 08:34:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.81 on epoch=16
05/27/2022 08:34:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.77 on epoch=16
05/27/2022 08:34:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.81 on epoch=16
05/27/2022 08:34:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.72 on epoch=17
05/27/2022 08:34:59 - INFO - __main__ - Global step 550 Train loss 0.78 Classification-F1 0.5188381559216235 on epoch=17
05/27/2022 08:34:59 - INFO - __main__ - Saving model with best Classification-F1: 0.36780623262457174 -> 0.5188381559216235 on epoch=17, global_step=550
05/27/2022 08:35:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.76 on epoch=17
05/27/2022 08:35:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.79 on epoch=17
05/27/2022 08:35:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.72 on epoch=18
05/27/2022 08:35:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.73 on epoch=18
05/27/2022 08:35:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.72 on epoch=18
05/27/2022 08:35:19 - INFO - __main__ - Global step 600 Train loss 0.75 Classification-F1 0.21026272239260285 on epoch=18
05/27/2022 08:35:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.79 on epoch=19
05/27/2022 08:35:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.67 on epoch=19
05/27/2022 08:35:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.76 on epoch=19
05/27/2022 08:35:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.76 on epoch=19
05/27/2022 08:35:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.66 on epoch=20
05/27/2022 08:35:39 - INFO - __main__ - Global step 650 Train loss 0.73 Classification-F1 0.31767723677844184 on epoch=20
05/27/2022 08:35:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.59 on epoch=20
05/27/2022 08:35:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.79 on epoch=20
05/27/2022 08:35:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.67 on epoch=21
05/27/2022 08:35:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.52 on epoch=21
05/27/2022 08:35:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.68 on epoch=21
05/27/2022 08:35:58 - INFO - __main__ - Global step 700 Train loss 0.65 Classification-F1 0.2730240827541619 on epoch=21
05/27/2022 08:36:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.74 on epoch=22
05/27/2022 08:36:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.57 on epoch=22
05/27/2022 08:36:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.57 on epoch=22
05/27/2022 08:36:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.61 on epoch=23
05/27/2022 08:36:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.69 on epoch=23
05/27/2022 08:36:18 - INFO - __main__ - Global step 750 Train loss 0.63 Classification-F1 0.5859794375669868 on epoch=23
05/27/2022 08:36:18 - INFO - __main__ - Saving model with best Classification-F1: 0.5188381559216235 -> 0.5859794375669868 on epoch=23, global_step=750
05/27/2022 08:36:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.60 on epoch=23
05/27/2022 08:36:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.63 on epoch=24
05/27/2022 08:36:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.51 on epoch=24
05/27/2022 08:36:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.64 on epoch=24
05/27/2022 08:36:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.58 on epoch=24
05/27/2022 08:36:38 - INFO - __main__ - Global step 800 Train loss 0.59 Classification-F1 0.3688054860417469 on epoch=24
05/27/2022 08:36:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.67 on epoch=25
05/27/2022 08:36:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.55 on epoch=25
05/27/2022 08:36:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.54 on epoch=25
05/27/2022 08:36:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.55 on epoch=26
05/27/2022 08:36:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=26
05/27/2022 08:36:58 - INFO - __main__ - Global step 850 Train loss 0.56 Classification-F1 0.6705223893366186 on epoch=26
05/27/2022 08:36:58 - INFO - __main__ - Saving model with best Classification-F1: 0.5859794375669868 -> 0.6705223893366186 on epoch=26, global_step=850
05/27/2022 08:37:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.61 on epoch=26
05/27/2022 08:37:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=27
05/27/2022 08:37:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.48 on epoch=27
05/27/2022 08:37:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.59 on epoch=27
05/27/2022 08:37:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=28
05/27/2022 08:37:18 - INFO - __main__ - Global step 900 Train loss 0.54 Classification-F1 0.6806392569173725 on epoch=28
05/27/2022 08:37:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6705223893366186 -> 0.6806392569173725 on epoch=28, global_step=900
05/27/2022 08:37:20 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=28
05/27/2022 08:37:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=28
05/27/2022 08:37:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.48 on epoch=29
05/27/2022 08:37:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.49 on epoch=29
05/27/2022 08:37:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.50 on epoch=29
05/27/2022 08:37:37 - INFO - __main__ - Global step 950 Train loss 0.47 Classification-F1 0.6800405496455378 on epoch=29
05/27/2022 08:37:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=29
05/27/2022 08:37:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=30
05/27/2022 08:37:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.54 on epoch=30
05/27/2022 08:37:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.54 on epoch=30
05/27/2022 08:37:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.43 on epoch=31
05/27/2022 08:37:57 - INFO - __main__ - Global step 1000 Train loss 0.48 Classification-F1 0.6152481482997949 on epoch=31
05/27/2022 08:38:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.49 on epoch=31
05/27/2022 08:38:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.55 on epoch=31
05/27/2022 08:38:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.52 on epoch=32
05/27/2022 08:38:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.39 on epoch=32
05/27/2022 08:38:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.48 on epoch=32
05/27/2022 08:38:17 - INFO - __main__ - Global step 1050 Train loss 0.48 Classification-F1 0.7096404133751999 on epoch=32
05/27/2022 08:38:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6806392569173725 -> 0.7096404133751999 on epoch=32, global_step=1050
05/27/2022 08:38:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.44 on epoch=33
05/27/2022 08:38:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=33
05/27/2022 08:38:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.48 on epoch=33
05/27/2022 08:38:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.48 on epoch=34
05/27/2022 08:38:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=34
05/27/2022 08:38:37 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.6952077944050196 on epoch=34
05/27/2022 08:38:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.47 on epoch=34
05/27/2022 08:38:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=34
05/27/2022 08:38:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=35
05/27/2022 08:38:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=35
05/27/2022 08:38:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=35
05/27/2022 08:38:57 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.6046981878604022 on epoch=35
05/27/2022 08:38:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=36
05/27/2022 08:39:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=36
05/27/2022 08:39:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=36
05/27/2022 08:39:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=37
05/27/2022 08:39:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.36 on epoch=37
05/27/2022 08:39:17 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.6755552624393909 on epoch=37
05/27/2022 08:39:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=37
05/27/2022 08:39:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=38
05/27/2022 08:39:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.28 on epoch=38
05/27/2022 08:39:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=38
05/27/2022 08:39:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.47 on epoch=39
05/27/2022 08:39:36 - INFO - __main__ - Global step 1250 Train loss 0.40 Classification-F1 0.7338253132324726 on epoch=39
05/27/2022 08:39:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7096404133751999 -> 0.7338253132324726 on epoch=39, global_step=1250
05/27/2022 08:39:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=39
05/27/2022 08:39:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.32 on epoch=39
05/27/2022 08:39:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=39
05/27/2022 08:39:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=40
05/27/2022 08:39:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=40
05/27/2022 08:39:56 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.7301795364687116 on epoch=40
05/27/2022 08:39:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=40
05/27/2022 08:40:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.45 on epoch=41
05/27/2022 08:40:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.32 on epoch=41
05/27/2022 08:40:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.46 on epoch=41
05/27/2022 08:40:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.46 on epoch=42
05/27/2022 08:40:16 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.6910133581743136 on epoch=42
05/27/2022 08:40:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.29 on epoch=42
05/27/2022 08:40:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=42
05/27/2022 08:40:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.31 on epoch=43
05/27/2022 08:40:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.26 on epoch=43
05/27/2022 08:40:29 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.28 on epoch=43
05/27/2022 08:40:36 - INFO - __main__ - Global step 1400 Train loss 0.32 Classification-F1 0.6498615742856815 on epoch=43
05/27/2022 08:40:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=44
05/27/2022 08:40:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.34 on epoch=44
05/27/2022 08:40:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.36 on epoch=44
05/27/2022 08:40:46 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.31 on epoch=44
05/27/2022 08:40:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=45
05/27/2022 08:40:56 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.7006283355372799 on epoch=45
05/27/2022 08:40:58 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=45
05/27/2022 08:41:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=45
05/27/2022 08:41:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.49 on epoch=46
05/27/2022 08:41:06 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.31 on epoch=46
05/27/2022 08:41:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=46
05/27/2022 08:41:16 - INFO - __main__ - Global step 1500 Train loss 0.36 Classification-F1 0.7236590478049676 on epoch=46
05/27/2022 08:41:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.38 on epoch=47
05/27/2022 08:41:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.27 on epoch=47
05/27/2022 08:41:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=47
05/27/2022 08:41:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.25 on epoch=48
05/27/2022 08:41:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.31 on epoch=48
05/27/2022 08:41:36 - INFO - __main__ - Global step 1550 Train loss 0.32 Classification-F1 0.7211752405936112 on epoch=48
05/27/2022 08:41:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=48
05/27/2022 08:41:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.34 on epoch=49
05/27/2022 08:41:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.30 on epoch=49
05/27/2022 08:41:46 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.33 on epoch=49
05/27/2022 08:41:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=49
05/27/2022 08:41:55 - INFO - __main__ - Global step 1600 Train loss 0.33 Classification-F1 0.7345972129695507 on epoch=49
05/27/2022 08:41:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7338253132324726 -> 0.7345972129695507 on epoch=49, global_step=1600
05/27/2022 08:41:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=50
05/27/2022 08:42:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=50
05/27/2022 08:42:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.28 on epoch=50
05/27/2022 08:42:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.29 on epoch=51
05/27/2022 08:42:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.17 on epoch=51
05/27/2022 08:42:15 - INFO - __main__ - Global step 1650 Train loss 0.26 Classification-F1 0.7581221365468355 on epoch=51
05/27/2022 08:42:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7345972129695507 -> 0.7581221365468355 on epoch=51, global_step=1650
05/27/2022 08:42:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.29 on epoch=51
05/27/2022 08:42:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.28 on epoch=52
05/27/2022 08:42:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=52
05/27/2022 08:42:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.36 on epoch=52
05/27/2022 08:42:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.22 on epoch=53
05/27/2022 08:42:35 - INFO - __main__ - Global step 1700 Train loss 0.28 Classification-F1 0.7854846130934819 on epoch=53
05/27/2022 08:42:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7581221365468355 -> 0.7854846130934819 on epoch=53, global_step=1700
05/27/2022 08:42:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.28 on epoch=53
05/27/2022 08:42:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=53
05/27/2022 08:42:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.29 on epoch=54
05/27/2022 08:42:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.26 on epoch=54
05/27/2022 08:42:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.31 on epoch=54
05/27/2022 08:42:55 - INFO - __main__ - Global step 1750 Train loss 0.28 Classification-F1 0.7612058061448768 on epoch=54
05/27/2022 08:42:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=54
05/27/2022 08:43:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.24 on epoch=55
05/27/2022 08:43:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.27 on epoch=55
05/27/2022 08:43:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=55
05/27/2022 08:43:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=56
05/27/2022 08:43:15 - INFO - __main__ - Global step 1800 Train loss 0.25 Classification-F1 0.7265827159923872 on epoch=56
05/27/2022 08:43:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.18 on epoch=56
05/27/2022 08:43:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.24 on epoch=56
05/27/2022 08:43:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=57
05/27/2022 08:43:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=57
05/27/2022 08:43:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.27 on epoch=57
05/27/2022 08:43:35 - INFO - __main__ - Global step 1850 Train loss 0.22 Classification-F1 0.787093583255542 on epoch=57
05/27/2022 08:43:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7854846130934819 -> 0.787093583255542 on epoch=57, global_step=1850
05/27/2022 08:43:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.21 on epoch=58
05/27/2022 08:43:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.18 on epoch=58
05/27/2022 08:43:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=58
05/27/2022 08:43:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.32 on epoch=59
05/27/2022 08:43:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.28 on epoch=59
05/27/2022 08:43:55 - INFO - __main__ - Global step 1900 Train loss 0.25 Classification-F1 0.7372606104074203 on epoch=59
05/27/2022 08:43:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=59
05/27/2022 08:44:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=59
05/27/2022 08:44:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.22 on epoch=60
05/27/2022 08:44:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.26 on epoch=60
05/27/2022 08:44:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=60
05/27/2022 08:44:14 - INFO - __main__ - Global step 1950 Train loss 0.23 Classification-F1 0.7255753498856947 on epoch=60
05/27/2022 08:44:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=61
05/27/2022 08:44:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.24 on epoch=61
05/27/2022 08:44:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.34 on epoch=61
05/27/2022 08:44:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.27 on epoch=62
05/27/2022 08:44:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.21 on epoch=62
05/27/2022 08:44:34 - INFO - __main__ - Global step 2000 Train loss 0.25 Classification-F1 0.7651439523234929 on epoch=62
05/27/2022 08:44:36 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.32 on epoch=62
05/27/2022 08:44:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=63
05/27/2022 08:44:41 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.26 on epoch=63
05/27/2022 08:44:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.23 on epoch=63
05/27/2022 08:44:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.20 on epoch=64
05/27/2022 08:44:54 - INFO - __main__ - Global step 2050 Train loss 0.24 Classification-F1 0.7624661401556522 on epoch=64
05/27/2022 08:44:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.18 on epoch=64
05/27/2022 08:44:59 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.21 on epoch=64
05/27/2022 08:45:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.25 on epoch=64
05/27/2022 08:45:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.24 on epoch=65
05/27/2022 08:45:06 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.20 on epoch=65
05/27/2022 08:45:13 - INFO - __main__ - Global step 2100 Train loss 0.22 Classification-F1 0.7596057829975957 on epoch=65
05/27/2022 08:45:16 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.28 on epoch=65
05/27/2022 08:45:18 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=66
05/27/2022 08:45:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.15 on epoch=66
05/27/2022 08:45:23 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.28 on epoch=66
05/27/2022 08:45:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=67
05/27/2022 08:45:33 - INFO - __main__ - Global step 2150 Train loss 0.20 Classification-F1 0.7548527164582537 on epoch=67
05/27/2022 08:45:36 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.21 on epoch=67
05/27/2022 08:45:38 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.16 on epoch=67
05/27/2022 08:45:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=68
05/27/2022 08:45:43 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.15 on epoch=68
05/27/2022 08:45:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.23 on epoch=68
05/27/2022 08:45:53 - INFO - __main__ - Global step 2200 Train loss 0.19 Classification-F1 0.7862898109376982 on epoch=68
05/27/2022 08:45:55 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=69
05/27/2022 08:45:58 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.17 on epoch=69
05/27/2022 08:46:00 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.30 on epoch=69
05/27/2022 08:46:03 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.27 on epoch=69
05/27/2022 08:46:05 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.15 on epoch=70
05/27/2022 08:46:12 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.7732660814562023 on epoch=70
05/27/2022 08:46:15 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.22 on epoch=70
05/27/2022 08:46:17 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.27 on epoch=70
05/27/2022 08:46:20 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.20 on epoch=71
05/27/2022 08:46:22 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.19 on epoch=71
05/27/2022 08:46:25 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.18 on epoch=71
05/27/2022 08:46:32 - INFO - __main__ - Global step 2300 Train loss 0.21 Classification-F1 0.7964838393152693 on epoch=71
05/27/2022 08:46:32 - INFO - __main__ - Saving model with best Classification-F1: 0.787093583255542 -> 0.7964838393152693 on epoch=71, global_step=2300
05/27/2022 08:46:34 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.15 on epoch=72
05/27/2022 08:46:37 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.20 on epoch=72
05/27/2022 08:46:39 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.23 on epoch=72
05/27/2022 08:46:42 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.21 on epoch=73
05/27/2022 08:46:44 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.19 on epoch=73
05/27/2022 08:46:51 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.8060706677398795 on epoch=73
05/27/2022 08:46:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7964838393152693 -> 0.8060706677398795 on epoch=73, global_step=2350
05/27/2022 08:46:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.13 on epoch=73
05/27/2022 08:46:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.17 on epoch=74
05/27/2022 08:46:59 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.11 on epoch=74
05/27/2022 08:47:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.11 on epoch=74
05/27/2022 08:47:04 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.18 on epoch=74
05/27/2022 08:47:11 - INFO - __main__ - Global step 2400 Train loss 0.14 Classification-F1 0.7824449324873339 on epoch=74
05/27/2022 08:47:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.20 on epoch=75
05/27/2022 08:47:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.25 on epoch=75
05/27/2022 08:47:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.16 on epoch=75
05/27/2022 08:47:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.15 on epoch=76
05/27/2022 08:47:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.11 on epoch=76
05/27/2022 08:47:31 - INFO - __main__ - Global step 2450 Train loss 0.18 Classification-F1 0.7826805915521571 on epoch=76
05/27/2022 08:47:33 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.20 on epoch=76
05/27/2022 08:47:36 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.13 on epoch=77
05/27/2022 08:47:39 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=77
05/27/2022 08:47:41 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.27 on epoch=77
05/27/2022 08:47:44 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.17 on epoch=78
05/27/2022 08:47:51 - INFO - __main__ - Global step 2500 Train loss 0.19 Classification-F1 0.7768572825995831 on epoch=78
05/27/2022 08:47:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.15 on epoch=78
05/27/2022 08:47:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.27 on epoch=78
05/27/2022 08:47:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.22 on epoch=79
05/27/2022 08:48:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.12 on epoch=79
05/27/2022 08:48:03 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.10 on epoch=79
05/27/2022 08:48:10 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.7896935762678409 on epoch=79
05/27/2022 08:48:13 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.19 on epoch=79
05/27/2022 08:48:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.24 on epoch=80
05/27/2022 08:48:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.17 on epoch=80
05/27/2022 08:48:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.11 on epoch=80
05/27/2022 08:48:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=81
05/27/2022 08:48:30 - INFO - __main__ - Global step 2600 Train loss 0.16 Classification-F1 0.7703173634728793 on epoch=81
05/27/2022 08:48:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.11 on epoch=81
05/27/2022 08:48:35 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.21 on epoch=81
05/27/2022 08:48:38 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.17 on epoch=82
05/27/2022 08:48:40 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=82
05/27/2022 08:48:43 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.10 on epoch=82
05/27/2022 08:48:50 - INFO - __main__ - Global step 2650 Train loss 0.13 Classification-F1 0.7801342132356052 on epoch=82
05/27/2022 08:48:52 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=83
05/27/2022 08:48:55 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=83
05/27/2022 08:48:57 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.18 on epoch=83
05/27/2022 08:49:00 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.19 on epoch=84
05/27/2022 08:49:02 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.17 on epoch=84
05/27/2022 08:49:10 - INFO - __main__ - Global step 2700 Train loss 0.15 Classification-F1 0.7832550726889522 on epoch=84
05/27/2022 08:49:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.11 on epoch=84
05/27/2022 08:49:15 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.16 on epoch=84
05/27/2022 08:49:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.18 on epoch=85
05/27/2022 08:49:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.12 on epoch=85
05/27/2022 08:49:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.14 on epoch=85
05/27/2022 08:49:29 - INFO - __main__ - Global step 2750 Train loss 0.14 Classification-F1 0.7979828559772664 on epoch=85
05/27/2022 08:49:32 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=86
05/27/2022 08:49:34 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.12 on epoch=86
05/27/2022 08:49:37 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.16 on epoch=86
05/27/2022 08:49:39 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.13 on epoch=87
05/27/2022 08:49:42 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.11 on epoch=87
05/27/2022 08:49:49 - INFO - __main__ - Global step 2800 Train loss 0.12 Classification-F1 0.7763398066107726 on epoch=87
05/27/2022 08:49:51 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.18 on epoch=87
05/27/2022 08:49:54 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=88
05/27/2022 08:49:56 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.13 on epoch=88
05/27/2022 08:49:59 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.16 on epoch=88
05/27/2022 08:50:02 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.16 on epoch=89
05/27/2022 08:50:09 - INFO - __main__ - Global step 2850 Train loss 0.15 Classification-F1 0.8075076053889613 on epoch=89
05/27/2022 08:50:09 - INFO - __main__ - Saving model with best Classification-F1: 0.8060706677398795 -> 0.8075076053889613 on epoch=89, global_step=2850
05/27/2022 08:50:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.13 on epoch=89
05/27/2022 08:50:14 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.21 on epoch=89
05/27/2022 08:50:16 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.17 on epoch=89
05/27/2022 08:50:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.14 on epoch=90
05/27/2022 08:50:21 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.17 on epoch=90
05/27/2022 08:50:28 - INFO - __main__ - Global step 2900 Train loss 0.16 Classification-F1 0.7719059075113606 on epoch=90
05/27/2022 08:50:31 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.11 on epoch=90
05/27/2022 08:50:33 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.16 on epoch=91
05/27/2022 08:50:36 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.19 on epoch=91
05/27/2022 08:50:38 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=91
05/27/2022 08:50:41 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.18 on epoch=92
05/27/2022 08:50:48 - INFO - __main__ - Global step 2950 Train loss 0.15 Classification-F1 0.7956067079633986 on epoch=92
05/27/2022 08:50:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.07 on epoch=92
05/27/2022 08:50:53 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.13 on epoch=92
05/27/2022 08:50:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=93
05/27/2022 08:50:58 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=93
05/27/2022 08:51:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.23 on epoch=93
05/27/2022 08:51:02 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 08:51:02 - INFO - __main__ - Printing 3 examples
05/27/2022 08:51:02 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/27/2022 08:51:02 - INFO - __main__ - ['sad']
05/27/2022 08:51:02 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/27/2022 08:51:02 - INFO - __main__ - ['sad']
05/27/2022 08:51:02 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/27/2022 08:51:02 - INFO - __main__ - ['sad']
05/27/2022 08:51:02 - INFO - __main__ - Tokenizing Input ...
05/27/2022 08:51:02 - INFO - __main__ - Tokenizing Output ...
05/27/2022 08:51:02 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 08:51:02 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 08:51:02 - INFO - __main__ - Printing 3 examples
05/27/2022 08:51:02 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/27/2022 08:51:02 - INFO - __main__ - ['sad']
05/27/2022 08:51:02 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/27/2022 08:51:02 - INFO - __main__ - ['sad']
05/27/2022 08:51:02 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/27/2022 08:51:02 - INFO - __main__ - ['sad']
05/27/2022 08:51:02 - INFO - __main__ - Tokenizing Input ...
05/27/2022 08:51:02 - INFO - __main__ - Tokenizing Output ...
05/27/2022 08:51:03 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 08:51:08 - INFO - __main__ - Global step 3000 Train loss 0.12 Classification-F1 0.7916390880566855 on epoch=93
05/27/2022 08:51:08 - INFO - __main__ - save last model!
05/27/2022 08:51:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 08:51:08 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 08:51:08 - INFO - __main__ - Printing 3 examples
05/27/2022 08:51:08 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 08:51:08 - INFO - __main__ - ['others']
05/27/2022 08:51:08 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 08:51:08 - INFO - __main__ - ['others']
05/27/2022 08:51:08 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 08:51:08 - INFO - __main__ - ['others']
05/27/2022 08:51:08 - INFO - __main__ - Tokenizing Input ...
05/27/2022 08:51:10 - INFO - __main__ - Tokenizing Output ...
05/27/2022 08:51:15 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 08:51:19 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 08:51:19 - INFO - __main__ - task name: emo
05/27/2022 08:51:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 08:51:20 - INFO - __main__ - Starting training!
05/27/2022 08:52:30 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_21_0.3_8_predictions.txt
05/27/2022 08:52:30 - INFO - __main__ - Classification-F1 on test data: 0.4799
05/27/2022 08:52:30 - INFO - __main__ - prefix=emo_128_21, lr=0.3, bsz=8, dev_performance=0.8075076053889613, test_performance=0.47988109394174167
05/27/2022 08:52:30 - INFO - __main__ - Running ... prefix=emo_128_21, lr=0.2, bsz=8 ...
05/27/2022 08:52:31 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 08:52:31 - INFO - __main__ - Printing 3 examples
05/27/2022 08:52:31 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/27/2022 08:52:31 - INFO - __main__ - ['sad']
05/27/2022 08:52:31 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/27/2022 08:52:31 - INFO - __main__ - ['sad']
05/27/2022 08:52:31 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/27/2022 08:52:31 - INFO - __main__ - ['sad']
05/27/2022 08:52:31 - INFO - __main__ - Tokenizing Input ...
05/27/2022 08:52:31 - INFO - __main__ - Tokenizing Output ...
05/27/2022 08:52:32 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 08:52:32 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 08:52:32 - INFO - __main__ - Printing 3 examples
05/27/2022 08:52:32 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/27/2022 08:52:32 - INFO - __main__ - ['sad']
05/27/2022 08:52:32 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/27/2022 08:52:32 - INFO - __main__ - ['sad']
05/27/2022 08:52:32 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/27/2022 08:52:32 - INFO - __main__ - ['sad']
05/27/2022 08:52:32 - INFO - __main__ - Tokenizing Input ...
05/27/2022 08:52:32 - INFO - __main__ - Tokenizing Output ...
05/27/2022 08:52:32 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 08:52:51 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 08:52:51 - INFO - __main__ - task name: emo
05/27/2022 08:52:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 08:52:52 - INFO - __main__ - Starting training!
05/27/2022 08:52:55 - INFO - __main__ - Step 10 Global step 10 Train loss 7.44 on epoch=0
05/27/2022 08:52:57 - INFO - __main__ - Step 20 Global step 20 Train loss 4.90 on epoch=0
05/27/2022 08:53:00 - INFO - __main__ - Step 30 Global step 30 Train loss 2.80 on epoch=0
05/27/2022 08:53:02 - INFO - __main__ - Step 40 Global step 40 Train loss 2.00 on epoch=1
05/27/2022 08:53:05 - INFO - __main__ - Step 50 Global step 50 Train loss 1.59 on epoch=1
05/27/2022 08:53:12 - INFO - __main__ - Global step 50 Train loss 3.75 Classification-F1 0.20600224656312585 on epoch=1
05/27/2022 08:53:12 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.20600224656312585 on epoch=1, global_step=50
05/27/2022 08:53:15 - INFO - __main__ - Step 60 Global step 60 Train loss 1.30 on epoch=1
05/27/2022 08:53:17 - INFO - __main__ - Step 70 Global step 70 Train loss 1.12 on epoch=2
05/27/2022 08:53:20 - INFO - __main__ - Step 80 Global step 80 Train loss 1.24 on epoch=2
05/27/2022 08:53:22 - INFO - __main__ - Step 90 Global step 90 Train loss 1.23 on epoch=2
05/27/2022 08:53:25 - INFO - __main__ - Step 100 Global step 100 Train loss 1.11 on epoch=3
05/27/2022 08:53:32 - INFO - __main__ - Global step 100 Train loss 1.20 Classification-F1 0.11648788890118131 on epoch=3
05/27/2022 08:53:35 - INFO - __main__ - Step 110 Global step 110 Train loss 1.07 on epoch=3
05/27/2022 08:53:37 - INFO - __main__ - Step 120 Global step 120 Train loss 1.11 on epoch=3
05/27/2022 08:53:40 - INFO - __main__ - Step 130 Global step 130 Train loss 1.01 on epoch=4
05/27/2022 08:53:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.87 on epoch=4
05/27/2022 08:53:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.98 on epoch=4
05/27/2022 08:53:52 - INFO - __main__ - Global step 150 Train loss 1.01 Classification-F1 0.15107297921440777 on epoch=4
05/27/2022 08:53:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.90 on epoch=4
05/27/2022 08:53:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.96 on epoch=5
05/27/2022 08:54:00 - INFO - __main__ - Step 180 Global step 180 Train loss 1.01 on epoch=5
05/27/2022 08:54:02 - INFO - __main__ - Step 190 Global step 190 Train loss 1.07 on epoch=5
05/27/2022 08:54:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.91 on epoch=6
05/27/2022 08:54:12 - INFO - __main__ - Global step 200 Train loss 0.97 Classification-F1 0.18936401086918753 on epoch=6
05/27/2022 08:54:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.99 on epoch=6
05/27/2022 08:54:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.97 on epoch=6
05/27/2022 08:54:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.88 on epoch=7
05/27/2022 08:54:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.95 on epoch=7
05/27/2022 08:54:25 - INFO - __main__ - Step 250 Global step 250 Train loss 1.01 on epoch=7
05/27/2022 08:54:32 - INFO - __main__ - Global step 250 Train loss 0.96 Classification-F1 0.22606456332218106 on epoch=7
05/27/2022 08:54:32 - INFO - __main__ - Saving model with best Classification-F1: 0.20600224656312585 -> 0.22606456332218106 on epoch=7, global_step=250
05/27/2022 08:54:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.92 on epoch=8
05/27/2022 08:54:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.94 on epoch=8
05/27/2022 08:54:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.97 on epoch=8
05/27/2022 08:54:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.96 on epoch=9
05/27/2022 08:54:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.88 on epoch=9
05/27/2022 08:54:52 - INFO - __main__ - Global step 300 Train loss 0.93 Classification-F1 0.14956962279785632 on epoch=9
05/27/2022 08:54:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.92 on epoch=9
05/27/2022 08:54:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.83 on epoch=9
05/27/2022 08:54:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.88 on epoch=10
05/27/2022 08:55:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.87 on epoch=10
05/27/2022 08:55:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.82 on epoch=10
05/27/2022 08:55:11 - INFO - __main__ - Global step 350 Train loss 0.87 Classification-F1 0.19691463998625214 on epoch=10
05/27/2022 08:55:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.77 on epoch=11
05/27/2022 08:55:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.92 on epoch=11
05/27/2022 08:55:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.83 on epoch=11
05/27/2022 08:55:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.92 on epoch=12
05/27/2022 08:55:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.96 on epoch=12
05/27/2022 08:55:31 - INFO - __main__ - Global step 400 Train loss 0.88 Classification-F1 0.11974336472095064 on epoch=12
05/27/2022 08:55:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.84 on epoch=12
05/27/2022 08:55:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.85 on epoch=13
05/27/2022 08:55:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.86 on epoch=13
05/27/2022 08:55:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.95 on epoch=13
05/27/2022 08:55:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.77 on epoch=14
05/27/2022 08:55:51 - INFO - __main__ - Global step 450 Train loss 0.85 Classification-F1 0.10800578731613214 on epoch=14
05/27/2022 08:55:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.85 on epoch=14
05/27/2022 08:55:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.86 on epoch=14
05/27/2022 08:55:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.77 on epoch=14
05/27/2022 08:56:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.82 on epoch=15
05/27/2022 08:56:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.88 on epoch=15
05/27/2022 08:56:11 - INFO - __main__ - Global step 500 Train loss 0.84 Classification-F1 0.16353925732789437 on epoch=15
05/27/2022 08:56:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.88 on epoch=15
05/27/2022 08:56:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.86 on epoch=16
05/27/2022 08:56:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.77 on epoch=16
05/27/2022 08:56:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.89 on epoch=16
05/27/2022 08:56:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.78 on epoch=17
05/27/2022 08:56:31 - INFO - __main__ - Global step 550 Train loss 0.84 Classification-F1 0.36719231068730884 on epoch=17
05/27/2022 08:56:31 - INFO - __main__ - Saving model with best Classification-F1: 0.22606456332218106 -> 0.36719231068730884 on epoch=17, global_step=550
05/27/2022 08:56:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.79 on epoch=17
05/27/2022 08:56:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.77 on epoch=17
05/27/2022 08:56:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.84 on epoch=18
05/27/2022 08:56:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.86 on epoch=18
05/27/2022 08:56:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.88 on epoch=18
05/27/2022 08:56:51 - INFO - __main__ - Global step 600 Train loss 0.83 Classification-F1 0.23672204125071972 on epoch=18
05/27/2022 08:56:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.86 on epoch=19
05/27/2022 08:56:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.81 on epoch=19
05/27/2022 08:56:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.90 on epoch=19
05/27/2022 08:57:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.78 on epoch=19
05/27/2022 08:57:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.82 on epoch=20
05/27/2022 08:57:11 - INFO - __main__ - Global step 650 Train loss 0.84 Classification-F1 0.29400421690530526 on epoch=20
05/27/2022 08:57:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.83 on epoch=20
05/27/2022 08:57:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.85 on epoch=20
05/27/2022 08:57:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.77 on epoch=21
05/27/2022 08:57:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.79 on epoch=21
05/27/2022 08:57:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.84 on epoch=21
05/27/2022 08:57:31 - INFO - __main__ - Global step 700 Train loss 0.81 Classification-F1 0.2547554250390832 on epoch=21
05/27/2022 08:57:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.80 on epoch=22
05/27/2022 08:57:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.76 on epoch=22
05/27/2022 08:57:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.82 on epoch=22
05/27/2022 08:57:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.77 on epoch=23
05/27/2022 08:57:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.80 on epoch=23
05/27/2022 08:57:50 - INFO - __main__ - Global step 750 Train loss 0.79 Classification-F1 0.2846818441691549 on epoch=23
05/27/2022 08:57:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.77 on epoch=23
05/27/2022 08:57:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.78 on epoch=24
05/27/2022 08:57:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.75 on epoch=24
05/27/2022 08:58:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.73 on epoch=24
05/27/2022 08:58:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.71 on epoch=24
05/27/2022 08:58:10 - INFO - __main__ - Global step 800 Train loss 0.75 Classification-F1 0.3122854334160686 on epoch=24
05/27/2022 08:58:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.77 on epoch=25
05/27/2022 08:58:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.80 on epoch=25
05/27/2022 08:58:18 - INFO - __main__ - Step 830 Global step 830 Train loss 1.57 on epoch=25
05/27/2022 08:58:20 - INFO - __main__ - Step 840 Global step 840 Train loss 1.56 on epoch=26
05/27/2022 08:58:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.77 on epoch=26
05/27/2022 08:58:30 - INFO - __main__ - Global step 850 Train loss 1.09 Classification-F1 0.5638504472462965 on epoch=26
05/27/2022 08:58:30 - INFO - __main__ - Saving model with best Classification-F1: 0.36719231068730884 -> 0.5638504472462965 on epoch=26, global_step=850
05/27/2022 08:58:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.82 on epoch=26
05/27/2022 08:58:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.66 on epoch=27
05/27/2022 08:58:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.69 on epoch=27
05/27/2022 08:58:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.74 on epoch=27
05/27/2022 08:58:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.84 on epoch=28
05/27/2022 08:58:50 - INFO - __main__ - Global step 900 Train loss 0.75 Classification-F1 0.4761854541334318 on epoch=28
05/27/2022 08:58:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.72 on epoch=28
05/27/2022 08:58:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.78 on epoch=28
05/27/2022 08:58:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.65 on epoch=29
05/27/2022 08:59:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.71 on epoch=29
05/27/2022 08:59:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.83 on epoch=29
05/27/2022 08:59:09 - INFO - __main__ - Global step 950 Train loss 0.74 Classification-F1 0.545349227987385 on epoch=29
05/27/2022 08:59:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.68 on epoch=29
05/27/2022 08:59:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.67 on epoch=30
05/27/2022 08:59:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.70 on epoch=30
05/27/2022 08:59:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.68 on epoch=30
05/27/2022 08:59:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.71 on epoch=31
05/27/2022 08:59:29 - INFO - __main__ - Global step 1000 Train loss 0.69 Classification-F1 0.4832959279159662 on epoch=31
05/27/2022 08:59:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.76 on epoch=31
05/27/2022 08:59:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.74 on epoch=31
05/27/2022 08:59:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.70 on epoch=32
05/27/2022 08:59:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.58 on epoch=32
05/27/2022 08:59:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.67 on epoch=32
05/27/2022 08:59:49 - INFO - __main__ - Global step 1050 Train loss 0.69 Classification-F1 0.5230039661359088 on epoch=32
05/27/2022 08:59:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.68 on epoch=33
05/27/2022 08:59:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.68 on epoch=33
05/27/2022 08:59:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.73 on epoch=33
05/27/2022 08:59:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.72 on epoch=34
05/27/2022 09:00:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.74 on epoch=34
05/27/2022 09:00:09 - INFO - __main__ - Global step 1100 Train loss 0.71 Classification-F1 0.5424767632586112 on epoch=34
05/27/2022 09:00:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.71 on epoch=34
05/27/2022 09:00:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.67 on epoch=34
05/27/2022 09:00:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.74 on epoch=35
05/27/2022 09:00:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.73 on epoch=35
05/27/2022 09:00:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.64 on epoch=35
05/27/2022 09:00:29 - INFO - __main__ - Global step 1150 Train loss 0.70 Classification-F1 0.4950661627735098 on epoch=35
05/27/2022 09:00:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.59 on epoch=36
05/27/2022 09:00:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.68 on epoch=36
05/27/2022 09:00:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.63 on epoch=36
05/27/2022 09:00:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.66 on epoch=37
05/27/2022 09:00:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.58 on epoch=37
05/27/2022 09:00:49 - INFO - __main__ - Global step 1200 Train loss 0.62 Classification-F1 0.4954403653767387 on epoch=37
05/27/2022 09:00:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.63 on epoch=37
05/27/2022 09:00:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.61 on epoch=38
05/27/2022 09:00:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.65 on epoch=38
05/27/2022 09:00:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.57 on epoch=38
05/27/2022 09:01:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.60 on epoch=39
05/27/2022 09:01:09 - INFO - __main__ - Global step 1250 Train loss 0.61 Classification-F1 0.5125755751469925 on epoch=39
05/27/2022 09:01:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.64 on epoch=39
05/27/2022 09:01:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.61 on epoch=39
05/27/2022 09:01:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.59 on epoch=39
05/27/2022 09:01:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.63 on epoch=40
05/27/2022 09:01:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.65 on epoch=40
05/27/2022 09:01:28 - INFO - __main__ - Global step 1300 Train loss 0.62 Classification-F1 0.5303811909379136 on epoch=40
05/27/2022 09:01:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.69 on epoch=40
05/27/2022 09:01:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.60 on epoch=41
05/27/2022 09:01:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.61 on epoch=41
05/27/2022 09:01:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.69 on epoch=41
05/27/2022 09:01:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.63 on epoch=42
05/27/2022 09:01:48 - INFO - __main__ - Global step 1350 Train loss 0.64 Classification-F1 0.5676282637804552 on epoch=42
05/27/2022 09:01:48 - INFO - __main__ - Saving model with best Classification-F1: 0.5638504472462965 -> 0.5676282637804552 on epoch=42, global_step=1350
05/27/2022 09:01:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.60 on epoch=42
05/27/2022 09:01:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.65 on epoch=42
05/27/2022 09:01:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.54 on epoch=43
05/27/2022 09:01:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.60 on epoch=43
05/27/2022 09:02:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.65 on epoch=43
05/27/2022 09:02:08 - INFO - __main__ - Global step 1400 Train loss 0.61 Classification-F1 0.5058266967484253 on epoch=43
05/27/2022 09:02:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.63 on epoch=44
05/27/2022 09:02:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.64 on epoch=44
05/27/2022 09:02:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.56 on epoch=44
05/27/2022 09:02:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.58 on epoch=44
05/27/2022 09:02:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.65 on epoch=45
05/27/2022 09:02:28 - INFO - __main__ - Global step 1450 Train loss 0.61 Classification-F1 0.5705785063607149 on epoch=45
05/27/2022 09:02:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5676282637804552 -> 0.5705785063607149 on epoch=45, global_step=1450
05/27/2022 09:02:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.64 on epoch=45
05/27/2022 09:02:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.71 on epoch=45
05/27/2022 09:02:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.57 on epoch=46
05/27/2022 09:02:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.57 on epoch=46
05/27/2022 09:02:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.63 on epoch=46
05/27/2022 09:02:48 - INFO - __main__ - Global step 1500 Train loss 0.62 Classification-F1 0.5475101440360528 on epoch=46
05/27/2022 09:02:50 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.55 on epoch=47
05/27/2022 09:02:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.57 on epoch=47
05/27/2022 09:02:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.58 on epoch=47
05/27/2022 09:02:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.58 on epoch=48
05/27/2022 09:03:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.67 on epoch=48
05/27/2022 09:03:07 - INFO - __main__ - Global step 1550 Train loss 0.59 Classification-F1 0.6000461689763752 on epoch=48
05/27/2022 09:03:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5705785063607149 -> 0.6000461689763752 on epoch=48, global_step=1550
05/27/2022 09:03:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.56 on epoch=48
05/27/2022 09:03:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.53 on epoch=49
05/27/2022 09:03:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.52 on epoch=49
05/27/2022 09:03:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.59 on epoch=49
05/27/2022 09:03:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.64 on epoch=49
05/27/2022 09:03:27 - INFO - __main__ - Global step 1600 Train loss 0.57 Classification-F1 0.5205489594600823 on epoch=49
05/27/2022 09:03:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.59 on epoch=50
05/27/2022 09:03:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.53 on epoch=50
05/27/2022 09:03:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.56 on epoch=50
05/27/2022 09:03:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.55 on epoch=51
05/27/2022 09:03:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.53 on epoch=51
05/27/2022 09:03:47 - INFO - __main__ - Global step 1650 Train loss 0.55 Classification-F1 0.5871455601226488 on epoch=51
05/27/2022 09:03:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.58 on epoch=51
05/27/2022 09:03:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.62 on epoch=52
05/27/2022 09:03:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.50 on epoch=52
05/27/2022 09:03:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.65 on epoch=52
05/27/2022 09:04:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.56 on epoch=53
05/27/2022 09:04:07 - INFO - __main__ - Global step 1700 Train loss 0.58 Classification-F1 0.5353629318325455 on epoch=53
05/27/2022 09:04:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.55 on epoch=53
05/27/2022 09:04:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.56 on epoch=53
05/27/2022 09:04:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.56 on epoch=54
05/27/2022 09:04:17 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.48 on epoch=54
05/27/2022 09:04:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.52 on epoch=54
05/27/2022 09:04:27 - INFO - __main__ - Global step 1750 Train loss 0.53 Classification-F1 0.5958482860381613 on epoch=54
05/27/2022 09:04:29 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.72 on epoch=54
05/27/2022 09:04:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.53 on epoch=55
05/27/2022 09:04:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.63 on epoch=55
05/27/2022 09:04:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.54 on epoch=55
05/27/2022 09:04:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.49 on epoch=56
05/27/2022 09:04:47 - INFO - __main__ - Global step 1800 Train loss 0.58 Classification-F1 0.5198679528768236 on epoch=56
05/27/2022 09:04:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.55 on epoch=56
05/27/2022 09:04:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.62 on epoch=56
05/27/2022 09:04:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.54 on epoch=57
05/27/2022 09:04:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.49 on epoch=57
05/27/2022 09:05:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.58 on epoch=57
05/27/2022 09:05:07 - INFO - __main__ - Global step 1850 Train loss 0.56 Classification-F1 0.6722873751964391 on epoch=57
05/27/2022 09:05:07 - INFO - __main__ - Saving model with best Classification-F1: 0.6000461689763752 -> 0.6722873751964391 on epoch=57, global_step=1850
05/27/2022 09:05:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.47 on epoch=58
05/27/2022 09:05:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=58
05/27/2022 09:05:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.53 on epoch=58
05/27/2022 09:05:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.63 on epoch=59
05/27/2022 09:05:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.47 on epoch=59
05/27/2022 09:05:26 - INFO - __main__ - Global step 1900 Train loss 0.50 Classification-F1 0.5548549398415623 on epoch=59
05/27/2022 09:05:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.43 on epoch=59
05/27/2022 09:05:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.43 on epoch=59
05/27/2022 09:05:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.44 on epoch=60
05/27/2022 09:05:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=60
05/27/2022 09:05:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.50 on epoch=60
05/27/2022 09:05:46 - INFO - __main__ - Global step 1950 Train loss 0.44 Classification-F1 0.5636686073549929 on epoch=60
05/27/2022 09:05:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.47 on epoch=61
05/27/2022 09:05:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.46 on epoch=61
05/27/2022 09:05:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.55 on epoch=61
05/27/2022 09:05:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.41 on epoch=62
05/27/2022 09:05:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.49 on epoch=62
05/27/2022 09:06:06 - INFO - __main__ - Global step 2000 Train loss 0.48 Classification-F1 0.6073699210179588 on epoch=62
05/27/2022 09:06:09 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.48 on epoch=62
05/27/2022 09:06:11 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.53 on epoch=63
05/27/2022 09:06:14 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.57 on epoch=63
05/27/2022 09:06:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.51 on epoch=63
05/27/2022 09:06:19 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.59 on epoch=64
05/27/2022 09:06:26 - INFO - __main__ - Global step 2050 Train loss 0.54 Classification-F1 0.6255091438869864 on epoch=64
05/27/2022 09:06:29 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.40 on epoch=64
05/27/2022 09:06:31 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.55 on epoch=64
05/27/2022 09:06:34 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.50 on epoch=64
05/27/2022 09:06:36 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.51 on epoch=65
05/27/2022 09:06:39 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.46 on epoch=65
05/27/2022 09:06:46 - INFO - __main__ - Global step 2100 Train loss 0.48 Classification-F1 0.5824046435706993 on epoch=65
05/27/2022 09:06:48 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.50 on epoch=65
05/27/2022 09:06:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.46 on epoch=66
05/27/2022 09:06:53 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.45 on epoch=66
05/27/2022 09:06:56 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.63 on epoch=66
05/27/2022 09:06:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.39 on epoch=67
05/27/2022 09:07:06 - INFO - __main__ - Global step 2150 Train loss 0.49 Classification-F1 0.652446843084882 on epoch=67
05/27/2022 09:07:08 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.44 on epoch=67
05/27/2022 09:07:11 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.49 on epoch=67
05/27/2022 09:07:13 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.45 on epoch=68
05/27/2022 09:07:16 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.47 on epoch=68
05/27/2022 09:07:18 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.47 on epoch=68
05/27/2022 09:07:25 - INFO - __main__ - Global step 2200 Train loss 0.46 Classification-F1 0.6195265620754038 on epoch=68
05/27/2022 09:07:28 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.52 on epoch=69
05/27/2022 09:07:31 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.41 on epoch=69
05/27/2022 09:07:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.45 on epoch=69
05/27/2022 09:07:36 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.50 on epoch=69
05/27/2022 09:07:38 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.55 on epoch=70
05/27/2022 09:07:45 - INFO - __main__ - Global step 2250 Train loss 0.49 Classification-F1 0.6482227337511606 on epoch=70
05/27/2022 09:07:48 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.47 on epoch=70
05/27/2022 09:07:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.49 on epoch=70
05/27/2022 09:07:53 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.46 on epoch=71
05/27/2022 09:07:56 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.41 on epoch=71
05/27/2022 09:07:58 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.60 on epoch=71
05/27/2022 09:08:05 - INFO - __main__ - Global step 2300 Train loss 0.48 Classification-F1 0.6320250656277554 on epoch=71
05/27/2022 09:08:08 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.49 on epoch=72
05/27/2022 09:08:10 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.51 on epoch=72
05/27/2022 09:08:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.50 on epoch=72
05/27/2022 09:08:15 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.52 on epoch=73
05/27/2022 09:08:18 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.48 on epoch=73
05/27/2022 09:08:25 - INFO - __main__ - Global step 2350 Train loss 0.50 Classification-F1 0.6448701982948559 on epoch=73
05/27/2022 09:08:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.49 on epoch=73
05/27/2022 09:08:30 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.40 on epoch=74
05/27/2022 09:08:32 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.40 on epoch=74
05/27/2022 09:08:35 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.40 on epoch=74
05/27/2022 09:08:38 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.47 on epoch=74
05/27/2022 09:08:45 - INFO - __main__ - Global step 2400 Train loss 0.43 Classification-F1 0.6083834291318051 on epoch=74
05/27/2022 09:08:47 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.43 on epoch=75
05/27/2022 09:08:50 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.36 on epoch=75
05/27/2022 09:08:52 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.55 on epoch=75
05/27/2022 09:08:55 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.38 on epoch=76
05/27/2022 09:08:57 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.38 on epoch=76
05/27/2022 09:09:04 - INFO - __main__ - Global step 2450 Train loss 0.42 Classification-F1 0.7188559004348478 on epoch=76
05/27/2022 09:09:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6722873751964391 -> 0.7188559004348478 on epoch=76, global_step=2450
05/27/2022 09:09:07 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.46 on epoch=76
05/27/2022 09:09:10 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.36 on epoch=77
05/27/2022 09:09:12 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.38 on epoch=77
05/27/2022 09:09:15 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.41 on epoch=77
05/27/2022 09:09:17 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.34 on epoch=78
05/27/2022 09:09:24 - INFO - __main__ - Global step 2500 Train loss 0.39 Classification-F1 0.7074978708192778 on epoch=78
05/27/2022 09:09:27 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.38 on epoch=78
05/27/2022 09:09:29 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.37 on epoch=78
05/27/2022 09:09:32 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.39 on epoch=79
05/27/2022 09:09:35 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.44 on epoch=79
05/27/2022 09:09:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.40 on epoch=79
05/27/2022 09:09:44 - INFO - __main__ - Global step 2550 Train loss 0.40 Classification-F1 0.6531391737417342 on epoch=79
05/27/2022 09:09:47 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.36 on epoch=79
05/27/2022 09:09:49 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.34 on epoch=80
05/27/2022 09:09:52 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.37 on epoch=80
05/27/2022 09:09:54 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.40 on epoch=80
05/27/2022 09:09:57 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.46 on epoch=81
05/27/2022 09:10:04 - INFO - __main__ - Global step 2600 Train loss 0.38 Classification-F1 0.6762522852054216 on epoch=81
05/27/2022 09:10:07 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.35 on epoch=81
05/27/2022 09:10:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.42 on epoch=81
05/27/2022 09:10:12 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.36 on epoch=82
05/27/2022 09:10:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.35 on epoch=82
05/27/2022 09:10:17 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.40 on epoch=82
05/27/2022 09:10:24 - INFO - __main__ - Global step 2650 Train loss 0.38 Classification-F1 0.6780406190469206 on epoch=82
05/27/2022 09:10:27 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.41 on epoch=83
05/27/2022 09:10:29 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.40 on epoch=83
05/27/2022 09:10:32 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.44 on epoch=83
05/27/2022 09:10:34 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.40 on epoch=84
05/27/2022 09:10:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.36 on epoch=84
05/27/2022 09:10:44 - INFO - __main__ - Global step 2700 Train loss 0.40 Classification-F1 0.7192529935677173 on epoch=84
05/27/2022 09:10:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7188559004348478 -> 0.7192529935677173 on epoch=84, global_step=2700
05/27/2022 09:10:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.44 on epoch=84
05/27/2022 09:10:49 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.45 on epoch=84
05/27/2022 09:10:52 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.42 on epoch=85
05/27/2022 09:10:54 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.38 on epoch=85
05/27/2022 09:10:57 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.44 on epoch=85
05/27/2022 09:11:04 - INFO - __main__ - Global step 2750 Train loss 0.43 Classification-F1 0.667993973908431 on epoch=85
05/27/2022 09:11:06 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.36 on epoch=86
05/27/2022 09:11:09 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.38 on epoch=86
05/27/2022 09:11:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.43 on epoch=86
05/27/2022 09:11:14 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.42 on epoch=87
05/27/2022 09:11:16 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.29 on epoch=87
05/27/2022 09:11:23 - INFO - __main__ - Global step 2800 Train loss 0.38 Classification-F1 0.648390607015081 on epoch=87
05/27/2022 09:11:26 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.44 on epoch=87
05/27/2022 09:11:29 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.30 on epoch=88
05/27/2022 09:11:31 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.34 on epoch=88
05/27/2022 09:11:34 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.38 on epoch=88
05/27/2022 09:11:36 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.34 on epoch=89
05/27/2022 09:11:43 - INFO - __main__ - Global step 2850 Train loss 0.36 Classification-F1 0.7059153927793541 on epoch=89
05/27/2022 09:11:46 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.30 on epoch=89
05/27/2022 09:11:48 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.38 on epoch=89
05/27/2022 09:11:51 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.35 on epoch=89
05/27/2022 09:11:54 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.33 on epoch=90
05/27/2022 09:11:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.35 on epoch=90
05/27/2022 09:12:03 - INFO - __main__ - Global step 2900 Train loss 0.34 Classification-F1 0.684806868621297 on epoch=90
05/27/2022 09:12:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.33 on epoch=90
05/27/2022 09:12:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.50 on epoch=91
05/27/2022 09:12:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.35 on epoch=91
05/27/2022 09:12:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.39 on epoch=91
05/27/2022 09:12:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.32 on epoch=92
05/27/2022 09:12:23 - INFO - __main__ - Global step 2950 Train loss 0.38 Classification-F1 0.713375605643769 on epoch=92
05/27/2022 09:12:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.35 on epoch=92
05/27/2022 09:12:28 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.39 on epoch=92
05/27/2022 09:12:31 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.33 on epoch=93
05/27/2022 09:12:33 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.25 on epoch=93
05/27/2022 09:12:36 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.37 on epoch=93
05/27/2022 09:12:37 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 09:12:37 - INFO - __main__ - Printing 3 examples
05/27/2022 09:12:37 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/27/2022 09:12:37 - INFO - __main__ - ['happy']
05/27/2022 09:12:37 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/27/2022 09:12:37 - INFO - __main__ - ['happy']
05/27/2022 09:12:37 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/27/2022 09:12:37 - INFO - __main__ - ['happy']
05/27/2022 09:12:37 - INFO - __main__ - Tokenizing Input ...
05/27/2022 09:12:37 - INFO - __main__ - Tokenizing Output ...
05/27/2022 09:12:38 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 09:12:38 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 09:12:38 - INFO - __main__ - Printing 3 examples
05/27/2022 09:12:38 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/27/2022 09:12:38 - INFO - __main__ - ['happy']
05/27/2022 09:12:38 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/27/2022 09:12:38 - INFO - __main__ - ['happy']
05/27/2022 09:12:38 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/27/2022 09:12:38 - INFO - __main__ - ['happy']
05/27/2022 09:12:38 - INFO - __main__ - Tokenizing Input ...
05/27/2022 09:12:38 - INFO - __main__ - Tokenizing Output ...
05/27/2022 09:12:38 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 09:12:43 - INFO - __main__ - Global step 3000 Train loss 0.34 Classification-F1 0.6816842023958282 on epoch=93
05/27/2022 09:12:43 - INFO - __main__ - save last model!
05/27/2022 09:12:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 09:12:43 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 09:12:43 - INFO - __main__ - Printing 3 examples
05/27/2022 09:12:43 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 09:12:43 - INFO - __main__ - ['others']
05/27/2022 09:12:43 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 09:12:43 - INFO - __main__ - ['others']
05/27/2022 09:12:43 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 09:12:43 - INFO - __main__ - ['others']
05/27/2022 09:12:43 - INFO - __main__ - Tokenizing Input ...
05/27/2022 09:12:45 - INFO - __main__ - Tokenizing Output ...
05/27/2022 09:12:50 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 09:12:55 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 09:12:55 - INFO - __main__ - task name: emo
05/27/2022 09:12:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 09:12:56 - INFO - __main__ - Starting training!
05/27/2022 09:14:05 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_21_0.2_8_predictions.txt
05/27/2022 09:14:05 - INFO - __main__ - Classification-F1 on test data: 0.4285
05/27/2022 09:14:06 - INFO - __main__ - prefix=emo_128_21, lr=0.2, bsz=8, dev_performance=0.7192529935677173, test_performance=0.42845449113927303
05/27/2022 09:14:06 - INFO - __main__ - Running ... prefix=emo_128_42, lr=0.5, bsz=8 ...
05/27/2022 09:14:06 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 09:14:06 - INFO - __main__ - Printing 3 examples
05/27/2022 09:14:06 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/27/2022 09:14:06 - INFO - __main__ - ['happy']
05/27/2022 09:14:06 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/27/2022 09:14:06 - INFO - __main__ - ['happy']
05/27/2022 09:14:06 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/27/2022 09:14:06 - INFO - __main__ - ['happy']
05/27/2022 09:14:06 - INFO - __main__ - Tokenizing Input ...
05/27/2022 09:14:07 - INFO - __main__ - Tokenizing Output ...
05/27/2022 09:14:07 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 09:14:07 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 09:14:07 - INFO - __main__ - Printing 3 examples
05/27/2022 09:14:07 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/27/2022 09:14:07 - INFO - __main__ - ['happy']
05/27/2022 09:14:07 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/27/2022 09:14:07 - INFO - __main__ - ['happy']
05/27/2022 09:14:07 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/27/2022 09:14:07 - INFO - __main__ - ['happy']
05/27/2022 09:14:07 - INFO - __main__ - Tokenizing Input ...
05/27/2022 09:14:07 - INFO - __main__ - Tokenizing Output ...
05/27/2022 09:14:08 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 09:14:27 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 09:14:27 - INFO - __main__ - task name: emo
05/27/2022 09:14:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 09:14:28 - INFO - __main__ - Starting training!
05/27/2022 09:14:30 - INFO - __main__ - Step 10 Global step 10 Train loss 6.06 on epoch=0
05/27/2022 09:14:33 - INFO - __main__ - Step 20 Global step 20 Train loss 2.09 on epoch=0
05/27/2022 09:14:35 - INFO - __main__ - Step 30 Global step 30 Train loss 1.39 on epoch=0
05/27/2022 09:14:38 - INFO - __main__ - Step 40 Global step 40 Train loss 1.10 on epoch=1
05/27/2022 09:14:40 - INFO - __main__ - Step 50 Global step 50 Train loss 1.06 on epoch=1
05/27/2022 09:14:48 - INFO - __main__ - Global step 50 Train loss 2.34 Classification-F1 0.1 on epoch=1
05/27/2022 09:14:48 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=1, global_step=50
05/27/2022 09:14:50 - INFO - __main__ - Step 60 Global step 60 Train loss 1.01 on epoch=1
05/27/2022 09:14:53 - INFO - __main__ - Step 70 Global step 70 Train loss 1.02 on epoch=2
05/27/2022 09:14:55 - INFO - __main__ - Step 80 Global step 80 Train loss 0.98 on epoch=2
05/27/2022 09:14:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.97 on epoch=2
05/27/2022 09:15:00 - INFO - __main__ - Step 100 Global step 100 Train loss 1.01 on epoch=3
05/27/2022 09:15:08 - INFO - __main__ - Global step 100 Train loss 1.00 Classification-F1 0.1210767218831735 on epoch=3
05/27/2022 09:15:08 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1210767218831735 on epoch=3, global_step=100
05/27/2022 09:15:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.92 on epoch=3
05/27/2022 09:15:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.98 on epoch=3
05/27/2022 09:15:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.99 on epoch=4
05/27/2022 09:15:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.89 on epoch=4
05/27/2022 09:15:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.90 on epoch=4
05/27/2022 09:15:27 - INFO - __main__ - Global step 150 Train loss 0.94 Classification-F1 0.18857493857493857 on epoch=4
05/27/2022 09:15:27 - INFO - __main__ - Saving model with best Classification-F1: 0.1210767218831735 -> 0.18857493857493857 on epoch=4, global_step=150
05/27/2022 09:15:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.91 on epoch=4
05/27/2022 09:15:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.92 on epoch=5
05/27/2022 09:15:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.94 on epoch=5
05/27/2022 09:15:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.92 on epoch=5
05/27/2022 09:15:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.81 on epoch=6
05/27/2022 09:15:47 - INFO - __main__ - Global step 200 Train loss 0.90 Classification-F1 0.20410532193129388 on epoch=6
05/27/2022 09:15:47 - INFO - __main__ - Saving model with best Classification-F1: 0.18857493857493857 -> 0.20410532193129388 on epoch=6, global_step=200
05/27/2022 09:15:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.88 on epoch=6
05/27/2022 09:15:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.97 on epoch=6
05/27/2022 09:15:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.82 on epoch=7
05/27/2022 09:15:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.83 on epoch=7
05/27/2022 09:16:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.83 on epoch=7
05/27/2022 09:16:07 - INFO - __main__ - Global step 250 Train loss 0.87 Classification-F1 0.21361783704337592 on epoch=7
05/27/2022 09:16:07 - INFO - __main__ - Saving model with best Classification-F1: 0.20410532193129388 -> 0.21361783704337592 on epoch=7, global_step=250
05/27/2022 09:16:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.81 on epoch=8
05/27/2022 09:16:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.80 on epoch=8
05/27/2022 09:16:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.76 on epoch=8
05/27/2022 09:16:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.85 on epoch=9
05/27/2022 09:16:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.72 on epoch=9
05/27/2022 09:16:27 - INFO - __main__ - Global step 300 Train loss 0.79 Classification-F1 0.3137794809044711 on epoch=9
05/27/2022 09:16:27 - INFO - __main__ - Saving model with best Classification-F1: 0.21361783704337592 -> 0.3137794809044711 on epoch=9, global_step=300
05/27/2022 09:16:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.80 on epoch=9
05/27/2022 09:16:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.82 on epoch=9
05/27/2022 09:16:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.87 on epoch=10
05/27/2022 09:16:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.75 on epoch=10
05/27/2022 09:16:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.87 on epoch=10
05/27/2022 09:16:47 - INFO - __main__ - Global step 350 Train loss 0.82 Classification-F1 0.46988789607083764 on epoch=10
05/27/2022 09:16:47 - INFO - __main__ - Saving model with best Classification-F1: 0.3137794809044711 -> 0.46988789607083764 on epoch=10, global_step=350
05/27/2022 09:16:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.73 on epoch=11
05/27/2022 09:16:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.81 on epoch=11
05/27/2022 09:16:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.79 on epoch=11
05/27/2022 09:16:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.71 on epoch=12
05/27/2022 09:16:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.73 on epoch=12
05/27/2022 09:17:06 - INFO - __main__ - Global step 400 Train loss 0.76 Classification-F1 0.30386666541405183 on epoch=12
05/27/2022 09:17:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.80 on epoch=12
05/27/2022 09:17:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.73 on epoch=13
05/27/2022 09:17:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.80 on epoch=13
05/27/2022 09:17:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.85 on epoch=13
05/27/2022 09:17:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.73 on epoch=14
05/27/2022 09:17:26 - INFO - __main__ - Global step 450 Train loss 0.78 Classification-F1 0.38999179685546503 on epoch=14
05/27/2022 09:17:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.82 on epoch=14
05/27/2022 09:17:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.74 on epoch=14
05/27/2022 09:17:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.70 on epoch=14
05/27/2022 09:17:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.70 on epoch=15
05/27/2022 09:17:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.73 on epoch=15
05/27/2022 09:17:46 - INFO - __main__ - Global step 500 Train loss 0.74 Classification-F1 0.4497207801902924 on epoch=15
05/27/2022 09:17:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.69 on epoch=15
05/27/2022 09:17:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.63 on epoch=16
05/27/2022 09:17:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.58 on epoch=16
05/27/2022 09:17:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.60 on epoch=16
05/27/2022 09:17:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.58 on epoch=17
05/27/2022 09:18:06 - INFO - __main__ - Global step 550 Train loss 0.61 Classification-F1 0.5961866738024136 on epoch=17
05/27/2022 09:18:06 - INFO - __main__ - Saving model with best Classification-F1: 0.46988789607083764 -> 0.5961866738024136 on epoch=17, global_step=550
05/27/2022 09:18:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=17
05/27/2022 09:18:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.66 on epoch=17
05/27/2022 09:18:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.51 on epoch=18
05/27/2022 09:18:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=18
05/27/2022 09:18:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=18
05/27/2022 09:18:27 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.6186400486886225 on epoch=18
05/27/2022 09:18:27 - INFO - __main__ - Saving model with best Classification-F1: 0.5961866738024136 -> 0.6186400486886225 on epoch=18, global_step=600
05/27/2022 09:18:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.51 on epoch=19
05/27/2022 09:18:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.55 on epoch=19
05/27/2022 09:18:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=19
05/27/2022 09:18:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=19
05/27/2022 09:18:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.51 on epoch=20
05/27/2022 09:18:47 - INFO - __main__ - Global step 650 Train loss 0.50 Classification-F1 0.6198805667925842 on epoch=20
05/27/2022 09:18:47 - INFO - __main__ - Saving model with best Classification-F1: 0.6186400486886225 -> 0.6198805667925842 on epoch=20, global_step=650
05/27/2022 09:18:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.51 on epoch=20
05/27/2022 09:18:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=20
05/27/2022 09:18:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=21
05/27/2022 09:18:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=21
05/27/2022 09:19:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.54 on epoch=21
05/27/2022 09:19:07 - INFO - __main__ - Global step 700 Train loss 0.49 Classification-F1 0.8122180813248652 on epoch=21
05/27/2022 09:19:07 - INFO - __main__ - Saving model with best Classification-F1: 0.6198805667925842 -> 0.8122180813248652 on epoch=21, global_step=700
05/27/2022 09:19:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.33 on epoch=22
05/27/2022 09:19:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=22
05/27/2022 09:19:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=22
05/27/2022 09:19:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.35 on epoch=23
05/27/2022 09:19:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=23
05/27/2022 09:19:27 - INFO - __main__ - Global step 750 Train loss 0.38 Classification-F1 0.7743247588019975 on epoch=23
05/27/2022 09:19:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=23
05/27/2022 09:19:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=24
05/27/2022 09:19:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.43 on epoch=24
05/27/2022 09:19:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=24
05/27/2022 09:19:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=24
05/27/2022 09:19:47 - INFO - __main__ - Global step 800 Train loss 0.41 Classification-F1 0.84043862940956 on epoch=24
05/27/2022 09:19:47 - INFO - __main__ - Saving model with best Classification-F1: 0.8122180813248652 -> 0.84043862940956 on epoch=24, global_step=800
05/27/2022 09:19:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.34 on epoch=25
05/27/2022 09:19:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=25
05/27/2022 09:19:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=25
05/27/2022 09:19:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=26
05/27/2022 09:19:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=26
05/27/2022 09:20:06 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.7926049048350131 on epoch=26
05/27/2022 09:20:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.26 on epoch=26
05/27/2022 09:20:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.32 on epoch=27
05/27/2022 09:20:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=27
05/27/2022 09:20:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.35 on epoch=27
05/27/2022 09:20:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=28
05/27/2022 09:20:26 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.7825396825396824 on epoch=28
05/27/2022 09:20:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.34 on epoch=28
05/27/2022 09:20:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.32 on epoch=28
05/27/2022 09:20:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=29
05/27/2022 09:20:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=29
05/27/2022 09:20:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=29
05/27/2022 09:20:46 - INFO - __main__ - Global step 950 Train loss 0.34 Classification-F1 0.8296968612710595 on epoch=29
05/27/2022 09:20:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=29
05/27/2022 09:20:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=30
05/27/2022 09:20:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.31 on epoch=30
05/27/2022 09:20:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=30
05/27/2022 09:20:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.30 on epoch=31
05/27/2022 09:21:06 - INFO - __main__ - Global step 1000 Train loss 0.28 Classification-F1 0.8259318986535608 on epoch=31
05/27/2022 09:21:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=31
05/27/2022 09:21:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.29 on epoch=31
05/27/2022 09:21:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=32
05/27/2022 09:21:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=32
05/27/2022 09:21:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=32
05/27/2022 09:21:26 - INFO - __main__ - Global step 1050 Train loss 0.28 Classification-F1 0.7341896677428684 on epoch=32
05/27/2022 09:21:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=33
05/27/2022 09:21:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=33
05/27/2022 09:21:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=33
05/27/2022 09:21:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.28 on epoch=34
05/27/2022 09:21:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=34
05/27/2022 09:21:46 - INFO - __main__ - Global step 1100 Train loss 0.28 Classification-F1 0.832650009306838 on epoch=34
05/27/2022 09:21:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.38 on epoch=34
05/27/2022 09:21:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.23 on epoch=34
05/27/2022 09:21:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.26 on epoch=35
05/27/2022 09:21:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.24 on epoch=35
05/27/2022 09:21:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=35
05/27/2022 09:22:06 - INFO - __main__ - Global step 1150 Train loss 0.26 Classification-F1 0.8317271369410854 on epoch=35
05/27/2022 09:22:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=36
05/27/2022 09:22:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=36
05/27/2022 09:22:13 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.22 on epoch=36
05/27/2022 09:22:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.24 on epoch=37
05/27/2022 09:22:18 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=37
05/27/2022 09:22:26 - INFO - __main__ - Global step 1200 Train loss 0.22 Classification-F1 0.8606091719728084 on epoch=37
05/27/2022 09:22:26 - INFO - __main__ - Saving model with best Classification-F1: 0.84043862940956 -> 0.8606091719728084 on epoch=37, global_step=1200
05/27/2022 09:22:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.23 on epoch=37
05/27/2022 09:22:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=38
05/27/2022 09:22:33 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=38
05/27/2022 09:22:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=38
05/27/2022 09:22:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=39
05/27/2022 09:22:46 - INFO - __main__ - Global step 1250 Train loss 0.19 Classification-F1 0.8206590391695783 on epoch=39
05/27/2022 09:22:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.25 on epoch=39
05/27/2022 09:22:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.12 on epoch=39
05/27/2022 09:22:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=39
05/27/2022 09:22:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=40
05/27/2022 09:22:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=40
05/27/2022 09:23:06 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.8476697762073385 on epoch=40
05/27/2022 09:23:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.17 on epoch=40
05/27/2022 09:23:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.23 on epoch=41
05/27/2022 09:23:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.27 on epoch=41
05/27/2022 09:23:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.13 on epoch=41
05/27/2022 09:23:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.15 on epoch=42
05/27/2022 09:23:26 - INFO - __main__ - Global step 1350 Train loss 0.19 Classification-F1 0.8367532717106194 on epoch=42
05/27/2022 09:23:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.27 on epoch=42
05/27/2022 09:23:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.26 on epoch=42
05/27/2022 09:23:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.12 on epoch=43
05/27/2022 09:23:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=43
05/27/2022 09:23:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.22 on epoch=43
05/27/2022 09:23:46 - INFO - __main__ - Global step 1400 Train loss 0.22 Classification-F1 0.8253433287436581 on epoch=43
05/27/2022 09:23:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=44
05/27/2022 09:23:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=44
05/27/2022 09:23:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.21 on epoch=44
05/27/2022 09:23:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=44
05/27/2022 09:23:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=45
05/27/2022 09:24:05 - INFO - __main__ - Global step 1450 Train loss 0.17 Classification-F1 0.8265186846465915 on epoch=45
05/27/2022 09:24:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=45
05/27/2022 09:24:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.17 on epoch=45
05/27/2022 09:24:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=46
05/27/2022 09:24:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=46
05/27/2022 09:24:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.26 on epoch=46
05/27/2022 09:24:25 - INFO - __main__ - Global step 1500 Train loss 0.17 Classification-F1 0.8516796153329375 on epoch=46
05/27/2022 09:24:27 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.19 on epoch=47
05/27/2022 09:24:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=47
05/27/2022 09:24:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=47
05/27/2022 09:24:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.24 on epoch=48
05/27/2022 09:24:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=48
05/27/2022 09:24:45 - INFO - __main__ - Global step 1550 Train loss 0.18 Classification-F1 0.846947136056771 on epoch=48
05/27/2022 09:24:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=48
05/27/2022 09:24:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=49
05/27/2022 09:24:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=49
05/27/2022 09:24:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=49
05/27/2022 09:24:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.09 on epoch=49
05/27/2022 09:25:04 - INFO - __main__ - Global step 1600 Train loss 0.13 Classification-F1 0.8654074559169419 on epoch=49
05/27/2022 09:25:04 - INFO - __main__ - Saving model with best Classification-F1: 0.8606091719728084 -> 0.8654074559169419 on epoch=49, global_step=1600
05/27/2022 09:25:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=50
05/27/2022 09:25:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=50
05/27/2022 09:25:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.16 on epoch=50
05/27/2022 09:25:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=51
05/27/2022 09:25:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=51
05/27/2022 09:25:25 - INFO - __main__ - Global step 1650 Train loss 0.15 Classification-F1 0.8423587551026414 on epoch=51
05/27/2022 09:25:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=51
05/27/2022 09:25:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.11 on epoch=52
05/27/2022 09:25:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.15 on epoch=52
05/27/2022 09:25:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=52
05/27/2022 09:25:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=53
05/27/2022 09:25:44 - INFO - __main__ - Global step 1700 Train loss 0.11 Classification-F1 0.841607682209186 on epoch=53
05/27/2022 09:25:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=53
05/27/2022 09:25:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.09 on epoch=53
05/27/2022 09:25:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=54
05/27/2022 09:25:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.15 on epoch=54
05/27/2022 09:25:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=54
05/27/2022 09:26:04 - INFO - __main__ - Global step 1750 Train loss 0.11 Classification-F1 0.8348324322506768 on epoch=54
05/27/2022 09:26:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.25 on epoch=54
05/27/2022 09:26:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.17 on epoch=55
05/27/2022 09:26:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=55
05/27/2022 09:26:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.14 on epoch=55
05/27/2022 09:26:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.15 on epoch=56
05/27/2022 09:26:24 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.8450319258138037 on epoch=56
05/27/2022 09:26:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.16 on epoch=56
05/27/2022 09:26:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.14 on epoch=56
05/27/2022 09:26:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=57
05/27/2022 09:26:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=57
05/27/2022 09:26:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=57
05/27/2022 09:26:44 - INFO - __main__ - Global step 1850 Train loss 0.16 Classification-F1 0.8147342557976957 on epoch=57
05/27/2022 09:26:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.14 on epoch=58
05/27/2022 09:26:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.09 on epoch=58
05/27/2022 09:26:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=58
05/27/2022 09:26:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.15 on epoch=59
05/27/2022 09:26:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=59
05/27/2022 09:27:04 - INFO - __main__ - Global step 1900 Train loss 0.13 Classification-F1 0.844463495634997 on epoch=59
05/27/2022 09:27:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=59
05/27/2022 09:27:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=59
05/27/2022 09:27:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=60
05/27/2022 09:27:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.17 on epoch=60
05/27/2022 09:27:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=60
05/27/2022 09:27:23 - INFO - __main__ - Global step 1950 Train loss 0.11 Classification-F1 0.828644620241078 on epoch=60
05/27/2022 09:27:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=61
05/27/2022 09:27:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=61
05/27/2022 09:27:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=61
05/27/2022 09:27:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=62
05/27/2022 09:27:36 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=62
05/27/2022 09:27:43 - INFO - __main__ - Global step 2000 Train loss 0.10 Classification-F1 0.8565669387716697 on epoch=62
05/27/2022 09:27:46 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=62
05/27/2022 09:27:48 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=63
05/27/2022 09:27:51 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.09 on epoch=63
05/27/2022 09:27:54 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.11 on epoch=63
05/27/2022 09:27:56 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.21 on epoch=64
05/27/2022 09:28:03 - INFO - __main__ - Global step 2050 Train loss 0.11 Classification-F1 0.8520617451054355 on epoch=64
05/27/2022 09:28:06 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.15 on epoch=64
05/27/2022 09:28:09 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.11 on epoch=64
05/27/2022 09:28:11 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.11 on epoch=64
05/27/2022 09:28:14 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=65
05/27/2022 09:28:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.11 on epoch=65
05/27/2022 09:28:23 - INFO - __main__ - Global step 2100 Train loss 0.11 Classification-F1 0.8622630423845702 on epoch=65
05/27/2022 09:28:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=65
05/27/2022 09:28:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.09 on epoch=66
05/27/2022 09:28:31 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.11 on epoch=66
05/27/2022 09:28:33 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=66
05/27/2022 09:28:36 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.12 on epoch=67
05/27/2022 09:28:43 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.8516041601192754 on epoch=67
05/27/2022 09:28:46 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.15 on epoch=67
05/27/2022 09:28:48 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.09 on epoch=67
05/27/2022 09:28:51 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.14 on epoch=68
05/27/2022 09:28:53 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=68
05/27/2022 09:28:56 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=68
05/27/2022 09:29:03 - INFO - __main__ - Global step 2200 Train loss 0.10 Classification-F1 0.8524453680776638 on epoch=68
05/27/2022 09:29:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=69
05/27/2022 09:29:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=69
05/27/2022 09:29:11 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.10 on epoch=69
05/27/2022 09:29:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.08 on epoch=69
05/27/2022 09:29:16 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.10 on epoch=70
05/27/2022 09:29:23 - INFO - __main__ - Global step 2250 Train loss 0.08 Classification-F1 0.8627590121354898 on epoch=70
05/27/2022 09:29:26 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=70
05/27/2022 09:29:28 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=70
05/27/2022 09:29:31 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=71
05/27/2022 09:29:33 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=71
05/27/2022 09:29:36 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=71
05/27/2022 09:29:43 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.8573648442158617 on epoch=71
05/27/2022 09:29:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.13 on epoch=72
05/27/2022 09:29:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=72
05/27/2022 09:29:51 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=72
05/27/2022 09:29:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=73
05/27/2022 09:29:56 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.15 on epoch=73
05/27/2022 09:30:03 - INFO - __main__ - Global step 2350 Train loss 0.09 Classification-F1 0.8359955474769916 on epoch=73
05/27/2022 09:30:05 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=73
05/27/2022 09:30:08 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.17 on epoch=74
05/27/2022 09:30:10 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.07 on epoch=74
05/27/2022 09:30:13 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=74
05/27/2022 09:30:16 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.09 on epoch=74
05/27/2022 09:30:23 - INFO - __main__ - Global step 2400 Train loss 0.09 Classification-F1 0.8667985720738566 on epoch=74
05/27/2022 09:30:23 - INFO - __main__ - Saving model with best Classification-F1: 0.8654074559169419 -> 0.8667985720738566 on epoch=74, global_step=2400
05/27/2022 09:30:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=75
05/27/2022 09:30:28 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.13 on epoch=75
05/27/2022 09:30:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.14 on epoch=75
05/27/2022 09:30:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.11 on epoch=76
05/27/2022 09:30:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=76
05/27/2022 09:30:43 - INFO - __main__ - Global step 2450 Train loss 0.11 Classification-F1 0.8538800005084781 on epoch=76
05/27/2022 09:30:45 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=76
05/27/2022 09:30:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=77
05/27/2022 09:30:50 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.09 on epoch=77
05/27/2022 09:30:53 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=77
05/27/2022 09:30:55 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=78
05/27/2022 09:31:02 - INFO - __main__ - Global step 2500 Train loss 0.09 Classification-F1 0.8589680291587594 on epoch=78
05/27/2022 09:31:05 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=78
05/27/2022 09:31:08 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=78
05/27/2022 09:31:10 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.07 on epoch=79
05/27/2022 09:31:13 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=79
05/27/2022 09:31:15 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.08 on epoch=79
05/27/2022 09:31:22 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.8484934536536852 on epoch=79
05/27/2022 09:31:25 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.11 on epoch=79
05/27/2022 09:31:27 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.17 on epoch=80
05/27/2022 09:31:30 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.06 on epoch=80
05/27/2022 09:31:32 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.07 on epoch=80
05/27/2022 09:31:35 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.12 on epoch=81
05/27/2022 09:31:42 - INFO - __main__ - Global step 2600 Train loss 0.11 Classification-F1 0.8553542305450611 on epoch=81
05/27/2022 09:31:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=81
05/27/2022 09:31:47 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.11 on epoch=81
05/27/2022 09:31:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=82
05/27/2022 09:31:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=82
05/27/2022 09:31:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=82
05/27/2022 09:32:02 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.8365813213734751 on epoch=82
05/27/2022 09:32:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=83
05/27/2022 09:32:07 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.09 on epoch=83
05/27/2022 09:32:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=83
05/27/2022 09:32:12 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=84
05/27/2022 09:32:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.08 on epoch=84
05/27/2022 09:32:22 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.8449673877815589 on epoch=84
05/27/2022 09:32:25 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.13 on epoch=84
05/27/2022 09:32:27 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=84
05/27/2022 09:32:30 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=85
05/27/2022 09:32:33 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=85
05/27/2022 09:32:35 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=85
05/27/2022 09:32:42 - INFO - __main__ - Global step 2750 Train loss 0.08 Classification-F1 0.8320228740540351 on epoch=85
05/27/2022 09:32:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.13 on epoch=86
05/27/2022 09:32:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.08 on epoch=86
05/27/2022 09:32:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=86
05/27/2022 09:32:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.09 on epoch=87
05/27/2022 09:32:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=87
05/27/2022 09:33:02 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.8598321715418309 on epoch=87
05/27/2022 09:33:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=87
05/27/2022 09:33:07 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.08 on epoch=88
05/27/2022 09:33:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.08 on epoch=88
05/27/2022 09:33:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=88
05/27/2022 09:33:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=89
05/27/2022 09:33:22 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.8322999832462827 on epoch=89
05/27/2022 09:33:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=89
05/27/2022 09:33:27 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=89
05/27/2022 09:33:30 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=89
05/27/2022 09:33:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=90
05/27/2022 09:33:35 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=90
05/27/2022 09:33:42 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.8440344795556025 on epoch=90
05/27/2022 09:33:45 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=90
05/27/2022 09:33:47 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=91
05/27/2022 09:33:50 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=91
05/27/2022 09:33:52 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=91
05/27/2022 09:33:55 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=92
05/27/2022 09:34:02 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.8486432112885826 on epoch=92
05/27/2022 09:34:05 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=92
05/27/2022 09:34:07 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=92
05/27/2022 09:34:10 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=93
05/27/2022 09:34:12 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=93
05/27/2022 09:34:15 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=93
05/27/2022 09:34:16 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 09:34:16 - INFO - __main__ - Printing 3 examples
05/27/2022 09:34:16 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/27/2022 09:34:16 - INFO - __main__ - ['happy']
05/27/2022 09:34:16 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/27/2022 09:34:16 - INFO - __main__ - ['happy']
05/27/2022 09:34:16 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/27/2022 09:34:16 - INFO - __main__ - ['happy']
05/27/2022 09:34:16 - INFO - __main__ - Tokenizing Input ...
05/27/2022 09:34:16 - INFO - __main__ - Tokenizing Output ...
05/27/2022 09:34:17 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 09:34:17 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 09:34:17 - INFO - __main__ - Printing 3 examples
05/27/2022 09:34:17 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/27/2022 09:34:17 - INFO - __main__ - ['happy']
05/27/2022 09:34:17 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/27/2022 09:34:17 - INFO - __main__ - ['happy']
05/27/2022 09:34:17 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/27/2022 09:34:17 - INFO - __main__ - ['happy']
05/27/2022 09:34:17 - INFO - __main__ - Tokenizing Input ...
05/27/2022 09:34:17 - INFO - __main__ - Tokenizing Output ...
05/27/2022 09:34:18 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 09:34:22 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.8408017630409627 on epoch=93
05/27/2022 09:34:22 - INFO - __main__ - save last model!
05/27/2022 09:34:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 09:34:22 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 09:34:22 - INFO - __main__ - Printing 3 examples
05/27/2022 09:34:22 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 09:34:22 - INFO - __main__ - ['others']
05/27/2022 09:34:22 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 09:34:22 - INFO - __main__ - ['others']
05/27/2022 09:34:22 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 09:34:22 - INFO - __main__ - ['others']
05/27/2022 09:34:22 - INFO - __main__ - Tokenizing Input ...
05/27/2022 09:34:25 - INFO - __main__ - Tokenizing Output ...
05/27/2022 09:34:30 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 09:34:34 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 09:34:34 - INFO - __main__ - task name: emo
05/27/2022 09:34:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 09:34:35 - INFO - __main__ - Starting training!
05/27/2022 09:35:47 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_42_0.5_8_predictions.txt
05/27/2022 09:35:47 - INFO - __main__ - Classification-F1 on test data: 0.4196
05/27/2022 09:35:48 - INFO - __main__ - prefix=emo_128_42, lr=0.5, bsz=8, dev_performance=0.8667985720738566, test_performance=0.4195654238555721
05/27/2022 09:35:48 - INFO - __main__ - Running ... prefix=emo_128_42, lr=0.4, bsz=8 ...
05/27/2022 09:35:49 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 09:35:49 - INFO - __main__ - Printing 3 examples
05/27/2022 09:35:49 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/27/2022 09:35:49 - INFO - __main__ - ['happy']
05/27/2022 09:35:49 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/27/2022 09:35:49 - INFO - __main__ - ['happy']
05/27/2022 09:35:49 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/27/2022 09:35:49 - INFO - __main__ - ['happy']
05/27/2022 09:35:49 - INFO - __main__ - Tokenizing Input ...
05/27/2022 09:35:49 - INFO - __main__ - Tokenizing Output ...
05/27/2022 09:35:49 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 09:35:49 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 09:35:49 - INFO - __main__ - Printing 3 examples
05/27/2022 09:35:49 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/27/2022 09:35:49 - INFO - __main__ - ['happy']
05/27/2022 09:35:49 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/27/2022 09:35:49 - INFO - __main__ - ['happy']
05/27/2022 09:35:49 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/27/2022 09:35:49 - INFO - __main__ - ['happy']
05/27/2022 09:35:49 - INFO - __main__ - Tokenizing Input ...
05/27/2022 09:35:50 - INFO - __main__ - Tokenizing Output ...
05/27/2022 09:35:50 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 09:36:09 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 09:36:09 - INFO - __main__ - task name: emo
05/27/2022 09:36:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 09:36:10 - INFO - __main__ - Starting training!
05/27/2022 09:36:13 - INFO - __main__ - Step 10 Global step 10 Train loss 6.31 on epoch=0
05/27/2022 09:36:15 - INFO - __main__ - Step 20 Global step 20 Train loss 2.87 on epoch=0
05/27/2022 09:36:18 - INFO - __main__ - Step 30 Global step 30 Train loss 1.49 on epoch=0
05/27/2022 09:36:20 - INFO - __main__ - Step 40 Global step 40 Train loss 1.30 on epoch=1
05/27/2022 09:36:23 - INFO - __main__ - Step 50 Global step 50 Train loss 1.21 on epoch=1
05/27/2022 09:36:30 - INFO - __main__ - Global step 50 Train loss 2.64 Classification-F1 0.1 on epoch=1
05/27/2022 09:36:30 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=1, global_step=50
05/27/2022 09:36:33 - INFO - __main__ - Step 60 Global step 60 Train loss 1.07 on epoch=1
05/27/2022 09:36:35 - INFO - __main__ - Step 70 Global step 70 Train loss 1.11 on epoch=2
05/27/2022 09:36:38 - INFO - __main__ - Step 80 Global step 80 Train loss 1.03 on epoch=2
05/27/2022 09:36:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.98 on epoch=2
05/27/2022 09:36:43 - INFO - __main__ - Step 100 Global step 100 Train loss 1.03 on epoch=3
05/27/2022 09:36:50 - INFO - __main__ - Global step 100 Train loss 1.04 Classification-F1 0.10819308045221665 on epoch=3
05/27/2022 09:36:50 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.10819308045221665 on epoch=3, global_step=100
05/27/2022 09:36:52 - INFO - __main__ - Step 110 Global step 110 Train loss 0.97 on epoch=3
05/27/2022 09:36:55 - INFO - __main__ - Step 120 Global step 120 Train loss 1.01 on epoch=3
05/27/2022 09:36:58 - INFO - __main__ - Step 130 Global step 130 Train loss 0.93 on epoch=4
05/27/2022 09:37:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.89 on epoch=4
05/27/2022 09:37:03 - INFO - __main__ - Step 150 Global step 150 Train loss 0.98 on epoch=4
05/27/2022 09:37:10 - INFO - __main__ - Global step 150 Train loss 0.95 Classification-F1 0.1 on epoch=4
05/27/2022 09:37:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.96 on epoch=4
05/27/2022 09:37:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.93 on epoch=5
05/27/2022 09:37:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.98 on epoch=5
05/27/2022 09:37:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.87 on epoch=5
05/27/2022 09:37:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.89 on epoch=6
05/27/2022 09:37:30 - INFO - __main__ - Global step 200 Train loss 0.93 Classification-F1 0.14119618963330202 on epoch=6
05/27/2022 09:37:30 - INFO - __main__ - Saving model with best Classification-F1: 0.10819308045221665 -> 0.14119618963330202 on epoch=6, global_step=200
05/27/2022 09:37:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.87 on epoch=6
05/27/2022 09:37:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.90 on epoch=6
05/27/2022 09:37:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.91 on epoch=7
05/27/2022 09:37:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.92 on epoch=7
05/27/2022 09:37:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.88 on epoch=7
05/27/2022 09:37:49 - INFO - __main__ - Global step 250 Train loss 0.90 Classification-F1 0.16217657829790624 on epoch=7
05/27/2022 09:37:49 - INFO - __main__ - Saving model with best Classification-F1: 0.14119618963330202 -> 0.16217657829790624 on epoch=7, global_step=250
05/27/2022 09:37:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.91 on epoch=8
05/27/2022 09:37:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.94 on epoch=8
05/27/2022 09:37:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.85 on epoch=8
05/27/2022 09:38:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.88 on epoch=9
05/27/2022 09:38:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.82 on epoch=9
05/27/2022 09:38:09 - INFO - __main__ - Global step 300 Train loss 0.88 Classification-F1 0.2458450266575083 on epoch=9
05/27/2022 09:38:09 - INFO - __main__ - Saving model with best Classification-F1: 0.16217657829790624 -> 0.2458450266575083 on epoch=9, global_step=300
05/27/2022 09:38:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.90 on epoch=9
05/27/2022 09:38:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.89 on epoch=9
05/27/2022 09:38:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.88 on epoch=10
05/27/2022 09:38:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.87 on epoch=10
05/27/2022 09:38:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.88 on epoch=10
05/27/2022 09:38:29 - INFO - __main__ - Global step 350 Train loss 0.89 Classification-F1 0.4233847634928555 on epoch=10
05/27/2022 09:38:29 - INFO - __main__ - Saving model with best Classification-F1: 0.2458450266575083 -> 0.4233847634928555 on epoch=10, global_step=350
05/27/2022 09:38:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.80 on epoch=11
05/27/2022 09:38:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.85 on epoch=11
05/27/2022 09:38:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.85 on epoch=11
05/27/2022 09:38:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.87 on epoch=12
05/27/2022 09:38:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.82 on epoch=12
05/27/2022 09:38:49 - INFO - __main__ - Global step 400 Train loss 0.84 Classification-F1 0.13040599419363286 on epoch=12
05/27/2022 09:38:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.82 on epoch=12
05/27/2022 09:38:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.86 on epoch=13
05/27/2022 09:38:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.76 on epoch=13
05/27/2022 09:38:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.83 on epoch=13
05/27/2022 09:39:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.88 on epoch=14
05/27/2022 09:39:09 - INFO - __main__ - Global step 450 Train loss 0.83 Classification-F1 0.1457606859558831 on epoch=14
05/27/2022 09:39:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.86 on epoch=14
05/27/2022 09:39:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.81 on epoch=14
05/27/2022 09:39:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.78 on epoch=14
05/27/2022 09:39:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.79 on epoch=15
05/27/2022 09:39:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.81 on epoch=15
05/27/2022 09:39:29 - INFO - __main__ - Global step 500 Train loss 0.81 Classification-F1 0.2273339455157637 on epoch=15
05/27/2022 09:39:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.88 on epoch=15
05/27/2022 09:39:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.81 on epoch=16
05/27/2022 09:39:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.81 on epoch=16
05/27/2022 09:39:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.79 on epoch=16
05/27/2022 09:39:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.71 on epoch=17
05/27/2022 09:39:49 - INFO - __main__ - Global step 550 Train loss 0.80 Classification-F1 0.296925106819564 on epoch=17
05/27/2022 09:39:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.72 on epoch=17
05/27/2022 09:39:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.87 on epoch=17
05/27/2022 09:39:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.70 on epoch=18
05/27/2022 09:39:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.76 on epoch=18
05/27/2022 09:40:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.74 on epoch=18
05/27/2022 09:40:08 - INFO - __main__ - Global step 600 Train loss 0.76 Classification-F1 0.2634510213369518 on epoch=18
05/27/2022 09:40:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.77 on epoch=19
05/27/2022 09:40:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.75 on epoch=19
05/27/2022 09:40:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.74 on epoch=19
05/27/2022 09:40:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.78 on epoch=19
05/27/2022 09:40:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.82 on epoch=20
05/27/2022 09:40:28 - INFO - __main__ - Global step 650 Train loss 0.77 Classification-F1 0.5701814222046651 on epoch=20
05/27/2022 09:40:28 - INFO - __main__ - Saving model with best Classification-F1: 0.4233847634928555 -> 0.5701814222046651 on epoch=20, global_step=650
05/27/2022 09:40:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.76 on epoch=20
05/27/2022 09:40:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.80 on epoch=20
05/27/2022 09:40:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.65 on epoch=21
05/27/2022 09:40:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.72 on epoch=21
05/27/2022 09:40:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.69 on epoch=21
05/27/2022 09:40:48 - INFO - __main__ - Global step 700 Train loss 0.72 Classification-F1 0.5334237061073102 on epoch=21
05/27/2022 09:40:51 - INFO - __main__ - Step 710 Global step 710 Train loss 0.69 on epoch=22
05/27/2022 09:40:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.65 on epoch=22
05/27/2022 09:40:56 - INFO - __main__ - Step 730 Global step 730 Train loss 0.71 on epoch=22
05/27/2022 09:40:58 - INFO - __main__ - Step 740 Global step 740 Train loss 0.66 on epoch=23
05/27/2022 09:41:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.59 on epoch=23
05/27/2022 09:41:08 - INFO - __main__ - Global step 750 Train loss 0.66 Classification-F1 0.4625484876723469 on epoch=23
05/27/2022 09:41:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.65 on epoch=23
05/27/2022 09:41:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.64 on epoch=24
05/27/2022 09:41:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.60 on epoch=24
05/27/2022 09:41:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.67 on epoch=24
05/27/2022 09:41:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.55 on epoch=24
05/27/2022 09:41:28 - INFO - __main__ - Global step 800 Train loss 0.62 Classification-F1 0.6835627708648355 on epoch=24
05/27/2022 09:41:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5701814222046651 -> 0.6835627708648355 on epoch=24, global_step=800
05/27/2022 09:41:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.62 on epoch=25
05/27/2022 09:41:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.60 on epoch=25
05/27/2022 09:41:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.59 on epoch=25
05/27/2022 09:41:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=26
05/27/2022 09:41:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.53 on epoch=26
05/27/2022 09:41:48 - INFO - __main__ - Global step 850 Train loss 0.56 Classification-F1 0.40452961138648935 on epoch=26
05/27/2022 09:41:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.50 on epoch=26
05/27/2022 09:41:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=27
05/27/2022 09:41:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.63 on epoch=27
05/27/2022 09:41:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.64 on epoch=27
05/27/2022 09:42:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.46 on epoch=28
05/27/2022 09:42:08 - INFO - __main__ - Global step 900 Train loss 0.53 Classification-F1 0.6341960163936842 on epoch=28
05/27/2022 09:42:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.55 on epoch=28
05/27/2022 09:42:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=28
05/27/2022 09:42:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.55 on epoch=29
05/27/2022 09:42:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.55 on epoch=29
05/27/2022 09:42:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=29
05/27/2022 09:42:28 - INFO - __main__ - Global step 950 Train loss 0.51 Classification-F1 0.7157534501602432 on epoch=29
05/27/2022 09:42:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6835627708648355 -> 0.7157534501602432 on epoch=29, global_step=950
05/27/2022 09:42:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=29
05/27/2022 09:42:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=30
05/27/2022 09:42:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.47 on epoch=30
05/27/2022 09:42:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.49 on epoch=30
05/27/2022 09:42:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=31
05/27/2022 09:42:48 - INFO - __main__ - Global step 1000 Train loss 0.45 Classification-F1 0.7924913092157186 on epoch=31
05/27/2022 09:42:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7157534501602432 -> 0.7924913092157186 on epoch=31, global_step=1000
05/27/2022 09:42:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.50 on epoch=31
05/27/2022 09:42:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=31
05/27/2022 09:42:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=32
05/27/2022 09:42:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=32
05/27/2022 09:43:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=32
05/27/2022 09:43:07 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.7174253800863766 on epoch=32
05/27/2022 09:43:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=33
05/27/2022 09:43:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=33
05/27/2022 09:43:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=33
05/27/2022 09:43:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.50 on epoch=34
05/27/2022 09:43:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.48 on epoch=34
05/27/2022 09:43:27 - INFO - __main__ - Global step 1100 Train loss 0.40 Classification-F1 0.7581180497475452 on epoch=34
05/27/2022 09:43:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=34
05/27/2022 09:43:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=34
05/27/2022 09:43:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.48 on epoch=35
05/27/2022 09:43:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=35
05/27/2022 09:43:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=35
05/27/2022 09:43:47 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.7225882259693353 on epoch=35
05/27/2022 09:43:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=36
05/27/2022 09:43:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=36
05/27/2022 09:43:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.36 on epoch=36
05/27/2022 09:43:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=37
05/27/2022 09:44:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.48 on epoch=37
05/27/2022 09:44:07 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.8055394053818012 on epoch=37
05/27/2022 09:44:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7924913092157186 -> 0.8055394053818012 on epoch=37, global_step=1200
05/27/2022 09:44:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.35 on epoch=37
05/27/2022 09:44:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.33 on epoch=38
05/27/2022 09:44:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=38
05/27/2022 09:44:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.33 on epoch=38
05/27/2022 09:44:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=39
05/27/2022 09:44:27 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.7297535644549371 on epoch=39
05/27/2022 09:44:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=39
05/27/2022 09:44:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.36 on epoch=39
05/27/2022 09:44:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=39
05/27/2022 09:44:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=40
05/27/2022 09:44:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.49 on epoch=40
05/27/2022 09:44:47 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.8086054680856176 on epoch=40
05/27/2022 09:44:47 - INFO - __main__ - Saving model with best Classification-F1: 0.8055394053818012 -> 0.8086054680856176 on epoch=40, global_step=1300
05/27/2022 09:44:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.29 on epoch=40
05/27/2022 09:44:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.31 on epoch=41
05/27/2022 09:44:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=41
05/27/2022 09:44:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=41
05/27/2022 09:45:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=42
05/27/2022 09:45:07 - INFO - __main__ - Global step 1350 Train loss 0.33 Classification-F1 0.7339302945080382 on epoch=42
05/27/2022 09:45:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.31 on epoch=42
05/27/2022 09:45:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.27 on epoch=42
05/27/2022 09:45:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.26 on epoch=43
05/27/2022 09:45:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.42 on epoch=43
05/27/2022 09:45:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.33 on epoch=43
05/27/2022 09:45:27 - INFO - __main__ - Global step 1400 Train loss 0.32 Classification-F1 0.7902200044250836 on epoch=43
05/27/2022 09:45:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=44
05/27/2022 09:45:32 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=44
05/27/2022 09:45:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.30 on epoch=44
05/27/2022 09:45:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=44
05/27/2022 09:45:39 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.35 on epoch=45
05/27/2022 09:45:47 - INFO - __main__ - Global step 1450 Train loss 0.34 Classification-F1 0.7778926692032068 on epoch=45
05/27/2022 09:45:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=45
05/27/2022 09:45:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=45
05/27/2022 09:45:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=46
05/27/2022 09:45:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.28 on epoch=46
05/27/2022 09:45:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=46
05/27/2022 09:46:06 - INFO - __main__ - Global step 1500 Train loss 0.26 Classification-F1 0.8288454260474232 on epoch=46
05/27/2022 09:46:06 - INFO - __main__ - Saving model with best Classification-F1: 0.8086054680856176 -> 0.8288454260474232 on epoch=46, global_step=1500
05/27/2022 09:46:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=47
05/27/2022 09:46:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.28 on epoch=47
05/27/2022 09:46:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=47
05/27/2022 09:46:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=48
05/27/2022 09:46:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.31 on epoch=48
05/27/2022 09:46:26 - INFO - __main__ - Global step 1550 Train loss 0.27 Classification-F1 0.7942345525703527 on epoch=48
05/27/2022 09:46:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.21 on epoch=48
05/27/2022 09:46:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=49
05/27/2022 09:46:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.27 on epoch=49
05/27/2022 09:46:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.29 on epoch=49
05/27/2022 09:46:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=49
05/27/2022 09:46:46 - INFO - __main__ - Global step 1600 Train loss 0.25 Classification-F1 0.841767663575813 on epoch=49
05/27/2022 09:46:46 - INFO - __main__ - Saving model with best Classification-F1: 0.8288454260474232 -> 0.841767663575813 on epoch=49, global_step=1600
05/27/2022 09:46:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.23 on epoch=50
05/27/2022 09:46:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.27 on epoch=50
05/27/2022 09:46:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.26 on epoch=50
05/27/2022 09:46:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=51
05/27/2022 09:46:59 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.27 on epoch=51
05/27/2022 09:47:06 - INFO - __main__ - Global step 1650 Train loss 0.26 Classification-F1 0.7372049031476997 on epoch=51
05/27/2022 09:47:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=51
05/27/2022 09:47:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.27 on epoch=52
05/27/2022 09:47:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=52
05/27/2022 09:47:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=52
05/27/2022 09:47:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=53
05/27/2022 09:47:26 - INFO - __main__ - Global step 1700 Train loss 0.22 Classification-F1 0.7725250432577419 on epoch=53
05/27/2022 09:47:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.32 on epoch=53
05/27/2022 09:47:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=53
05/27/2022 09:47:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.25 on epoch=54
05/27/2022 09:47:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.30 on epoch=54
05/27/2022 09:47:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=54
05/27/2022 09:47:46 - INFO - __main__ - Global step 1750 Train loss 0.25 Classification-F1 0.7763711788648605 on epoch=54
05/27/2022 09:47:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.23 on epoch=54
05/27/2022 09:47:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=55
05/27/2022 09:47:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.23 on epoch=55
05/27/2022 09:47:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.25 on epoch=55
05/27/2022 09:47:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=56
05/27/2022 09:48:06 - INFO - __main__ - Global step 1800 Train loss 0.23 Classification-F1 0.825387199644406 on epoch=56
05/27/2022 09:48:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=56
05/27/2022 09:48:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=56
05/27/2022 09:48:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.15 on epoch=57
05/27/2022 09:48:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.31 on epoch=57
05/27/2022 09:48:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.17 on epoch=57
05/27/2022 09:48:26 - INFO - __main__ - Global step 1850 Train loss 0.23 Classification-F1 0.7442159872478652 on epoch=57
05/27/2022 09:48:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.22 on epoch=58
05/27/2022 09:48:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.24 on epoch=58
05/27/2022 09:48:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=58
05/27/2022 09:48:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.20 on epoch=59
05/27/2022 09:48:39 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.20 on epoch=59
05/27/2022 09:48:46 - INFO - __main__ - Global step 1900 Train loss 0.21 Classification-F1 0.8309251855513214 on epoch=59
05/27/2022 09:48:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=59
05/27/2022 09:48:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.15 on epoch=59
05/27/2022 09:48:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=60
05/27/2022 09:48:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=60
05/27/2022 09:48:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=60
05/27/2022 09:49:06 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.8141797701597224 on epoch=60
05/27/2022 09:49:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=61
05/27/2022 09:49:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=61
05/27/2022 09:49:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=61
05/27/2022 09:49:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=62
05/27/2022 09:49:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=62
05/27/2022 09:49:25 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.8338879326498447 on epoch=62
05/27/2022 09:49:28 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.15 on epoch=62
05/27/2022 09:49:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=63
05/27/2022 09:49:33 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.19 on epoch=63
05/27/2022 09:49:36 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.11 on epoch=63
05/27/2022 09:49:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.26 on epoch=64
05/27/2022 09:49:45 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.8211621938827203 on epoch=64
05/27/2022 09:49:48 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.19 on epoch=64
05/27/2022 09:49:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.19 on epoch=64
05/27/2022 09:49:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.25 on epoch=64
05/27/2022 09:49:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.22 on epoch=65
05/27/2022 09:49:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.13 on epoch=65
05/27/2022 09:50:05 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.8274086698628648 on epoch=65
05/27/2022 09:50:08 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.15 on epoch=65
05/27/2022 09:50:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=66
05/27/2022 09:50:13 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.16 on epoch=66
05/27/2022 09:50:15 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.11 on epoch=66
05/27/2022 09:50:18 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.15 on epoch=67
05/27/2022 09:50:25 - INFO - __main__ - Global step 2150 Train loss 0.14 Classification-F1 0.8150104638015182 on epoch=67
05/27/2022 09:50:28 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.21 on epoch=67
05/27/2022 09:50:30 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.17 on epoch=67
05/27/2022 09:50:33 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.17 on epoch=68
05/27/2022 09:50:35 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.23 on epoch=68
05/27/2022 09:50:38 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=68
05/27/2022 09:50:45 - INFO - __main__ - Global step 2200 Train loss 0.19 Classification-F1 0.800517016937155 on epoch=68
05/27/2022 09:50:48 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.34 on epoch=69
05/27/2022 09:50:50 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.15 on epoch=69
05/27/2022 09:50:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=69
05/27/2022 09:50:55 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.18 on epoch=69
05/27/2022 09:50:58 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.10 on epoch=70
05/27/2022 09:51:05 - INFO - __main__ - Global step 2250 Train loss 0.19 Classification-F1 0.8271682559400648 on epoch=70
05/27/2022 09:51:08 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.16 on epoch=70
05/27/2022 09:51:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.10 on epoch=70
05/27/2022 09:51:13 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.11 on epoch=71
05/27/2022 09:51:16 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.14 on epoch=71
05/27/2022 09:51:18 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=71
05/27/2022 09:51:25 - INFO - __main__ - Global step 2300 Train loss 0.12 Classification-F1 0.8113981341886003 on epoch=71
05/27/2022 09:51:28 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.19 on epoch=72
05/27/2022 09:51:31 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.25 on epoch=72
05/27/2022 09:51:33 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.12 on epoch=72
05/27/2022 09:51:36 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=73
05/27/2022 09:51:38 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.16 on epoch=73
05/27/2022 09:51:45 - INFO - __main__ - Global step 2350 Train loss 0.17 Classification-F1 0.8575238373550055 on epoch=73
05/27/2022 09:51:45 - INFO - __main__ - Saving model with best Classification-F1: 0.841767663575813 -> 0.8575238373550055 on epoch=73, global_step=2350
05/27/2022 09:51:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=73
05/27/2022 09:51:51 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.11 on epoch=74
05/27/2022 09:51:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.20 on epoch=74
05/27/2022 09:51:56 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.09 on epoch=74
05/27/2022 09:51:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.25 on epoch=74
05/27/2022 09:52:05 - INFO - __main__ - Global step 2400 Train loss 0.15 Classification-F1 0.8496971680503544 on epoch=74
05/27/2022 09:52:08 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.14 on epoch=75
05/27/2022 09:52:10 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.24 on epoch=75
05/27/2022 09:52:13 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.16 on epoch=75
05/27/2022 09:52:16 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.11 on epoch=76
05/27/2022 09:52:18 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=76
05/27/2022 09:52:25 - INFO - __main__ - Global step 2450 Train loss 0.16 Classification-F1 0.8229389694821057 on epoch=76
05/27/2022 09:52:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.12 on epoch=76
05/27/2022 09:52:30 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.07 on epoch=77
05/27/2022 09:52:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=77
05/27/2022 09:52:35 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=77
05/27/2022 09:52:38 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=78
05/27/2022 09:52:45 - INFO - __main__ - Global step 2500 Train loss 0.10 Classification-F1 0.818266128527082 on epoch=78
05/27/2022 09:52:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.09 on epoch=78
05/27/2022 09:52:50 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.12 on epoch=78
05/27/2022 09:52:53 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.17 on epoch=79
05/27/2022 09:52:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=79
05/27/2022 09:52:58 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=79
05/27/2022 09:53:05 - INFO - __main__ - Global step 2550 Train loss 0.13 Classification-F1 0.8206716052403864 on epoch=79
05/27/2022 09:53:08 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.07 on epoch=79
05/27/2022 09:53:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.11 on epoch=80
05/27/2022 09:53:13 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.17 on epoch=80
05/27/2022 09:53:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.19 on epoch=80
05/27/2022 09:53:18 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.11 on epoch=81
05/27/2022 09:53:25 - INFO - __main__ - Global step 2600 Train loss 0.13 Classification-F1 0.8240968895734293 on epoch=81
05/27/2022 09:53:28 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.10 on epoch=81
05/27/2022 09:53:30 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.13 on epoch=81
05/27/2022 09:53:33 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.13 on epoch=82
05/27/2022 09:53:35 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=82
05/27/2022 09:53:38 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.16 on epoch=82
05/27/2022 09:53:45 - INFO - __main__ - Global step 2650 Train loss 0.13 Classification-F1 0.8094414274163377 on epoch=82
05/27/2022 09:53:48 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.14 on epoch=83
05/27/2022 09:53:50 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.11 on epoch=83
05/27/2022 09:53:53 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.12 on epoch=83
05/27/2022 09:53:55 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.14 on epoch=84
05/27/2022 09:53:58 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.13 on epoch=84
05/27/2022 09:54:05 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.8328918402569794 on epoch=84
05/27/2022 09:54:07 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.11 on epoch=84
05/27/2022 09:54:10 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=84
05/27/2022 09:54:13 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.09 on epoch=85
05/27/2022 09:54:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.14 on epoch=85
05/27/2022 09:54:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.13 on epoch=85
05/27/2022 09:54:25 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.8326533394603098 on epoch=85
05/27/2022 09:54:27 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.11 on epoch=86
05/27/2022 09:54:30 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.07 on epoch=86
05/27/2022 09:54:33 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.08 on epoch=86
05/27/2022 09:54:35 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.09 on epoch=87
05/27/2022 09:54:38 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.14 on epoch=87
05/27/2022 09:54:45 - INFO - __main__ - Global step 2800 Train loss 0.10 Classification-F1 0.850823831392937 on epoch=87
05/27/2022 09:54:47 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.12 on epoch=87
05/27/2022 09:54:50 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.09 on epoch=88
05/27/2022 09:54:53 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=88
05/27/2022 09:54:55 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=88
05/27/2022 09:54:58 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.09 on epoch=89
05/27/2022 09:55:05 - INFO - __main__ - Global step 2850 Train loss 0.09 Classification-F1 0.8519536044533681 on epoch=89
05/27/2022 09:55:07 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.14 on epoch=89
05/27/2022 09:55:10 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.22 on epoch=89
05/27/2022 09:55:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=89
05/27/2022 09:55:15 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.10 on epoch=90
05/27/2022 09:55:18 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=90
05/27/2022 09:55:25 - INFO - __main__ - Global step 2900 Train loss 0.12 Classification-F1 0.7934927496993371 on epoch=90
05/27/2022 09:55:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.07 on epoch=90
05/27/2022 09:55:30 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.11 on epoch=91
05/27/2022 09:55:33 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.16 on epoch=91
05/27/2022 09:55:35 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.08 on epoch=91
05/27/2022 09:55:38 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=92
05/27/2022 09:55:45 - INFO - __main__ - Global step 2950 Train loss 0.10 Classification-F1 0.8535914043594128 on epoch=92
05/27/2022 09:55:48 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=92
05/27/2022 09:55:50 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.07 on epoch=92
05/27/2022 09:55:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=93
05/27/2022 09:55:55 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.10 on epoch=93
05/27/2022 09:55:58 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=93
05/27/2022 09:55:59 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 09:55:59 - INFO - __main__ - Printing 3 examples
05/27/2022 09:55:59 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/27/2022 09:55:59 - INFO - __main__ - ['happy']
05/27/2022 09:55:59 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/27/2022 09:55:59 - INFO - __main__ - ['happy']
05/27/2022 09:55:59 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/27/2022 09:55:59 - INFO - __main__ - ['happy']
05/27/2022 09:55:59 - INFO - __main__ - Tokenizing Input ...
05/27/2022 09:55:59 - INFO - __main__ - Tokenizing Output ...
05/27/2022 09:56:00 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 09:56:00 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 09:56:00 - INFO - __main__ - Printing 3 examples
05/27/2022 09:56:00 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/27/2022 09:56:00 - INFO - __main__ - ['happy']
05/27/2022 09:56:00 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/27/2022 09:56:00 - INFO - __main__ - ['happy']
05/27/2022 09:56:00 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/27/2022 09:56:00 - INFO - __main__ - ['happy']
05/27/2022 09:56:00 - INFO - __main__ - Tokenizing Input ...
05/27/2022 09:56:00 - INFO - __main__ - Tokenizing Output ...
05/27/2022 09:56:00 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 09:56:05 - INFO - __main__ - Global step 3000 Train loss 0.08 Classification-F1 0.7691501371544891 on epoch=93
05/27/2022 09:56:05 - INFO - __main__ - save last model!
05/27/2022 09:56:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 09:56:05 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 09:56:05 - INFO - __main__ - Printing 3 examples
05/27/2022 09:56:05 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 09:56:05 - INFO - __main__ - ['others']
05/27/2022 09:56:05 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 09:56:05 - INFO - __main__ - ['others']
05/27/2022 09:56:05 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 09:56:05 - INFO - __main__ - ['others']
05/27/2022 09:56:05 - INFO - __main__ - Tokenizing Input ...
05/27/2022 09:56:07 - INFO - __main__ - Tokenizing Output ...
05/27/2022 09:56:13 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 09:56:17 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 09:56:17 - INFO - __main__ - task name: emo
05/27/2022 09:56:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 09:56:18 - INFO - __main__ - Starting training!
05/27/2022 09:57:29 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_42_0.4_8_predictions.txt
05/27/2022 09:57:29 - INFO - __main__ - Classification-F1 on test data: 0.3931
05/27/2022 09:57:29 - INFO - __main__ - prefix=emo_128_42, lr=0.4, bsz=8, dev_performance=0.8575238373550055, test_performance=0.39305252236633054
05/27/2022 09:57:29 - INFO - __main__ - Running ... prefix=emo_128_42, lr=0.3, bsz=8 ...
05/27/2022 09:57:30 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 09:57:30 - INFO - __main__ - Printing 3 examples
05/27/2022 09:57:30 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/27/2022 09:57:30 - INFO - __main__ - ['happy']
05/27/2022 09:57:30 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/27/2022 09:57:30 - INFO - __main__ - ['happy']
05/27/2022 09:57:30 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/27/2022 09:57:30 - INFO - __main__ - ['happy']
05/27/2022 09:57:30 - INFO - __main__ - Tokenizing Input ...
05/27/2022 09:57:30 - INFO - __main__ - Tokenizing Output ...
05/27/2022 09:57:31 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 09:57:31 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 09:57:31 - INFO - __main__ - Printing 3 examples
05/27/2022 09:57:31 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/27/2022 09:57:31 - INFO - __main__ - ['happy']
05/27/2022 09:57:31 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/27/2022 09:57:31 - INFO - __main__ - ['happy']
05/27/2022 09:57:31 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/27/2022 09:57:31 - INFO - __main__ - ['happy']
05/27/2022 09:57:31 - INFO - __main__ - Tokenizing Input ...
05/27/2022 09:57:31 - INFO - __main__ - Tokenizing Output ...
05/27/2022 09:57:31 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 09:57:47 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 09:57:47 - INFO - __main__ - task name: emo
05/27/2022 09:57:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 09:57:48 - INFO - __main__ - Starting training!
05/27/2022 09:57:51 - INFO - __main__ - Step 10 Global step 10 Train loss 7.33 on epoch=0
05/27/2022 09:57:53 - INFO - __main__ - Step 20 Global step 20 Train loss 3.87 on epoch=0
05/27/2022 09:57:56 - INFO - __main__ - Step 30 Global step 30 Train loss 2.06 on epoch=0
05/27/2022 09:57:58 - INFO - __main__ - Step 40 Global step 40 Train loss 1.40 on epoch=1
05/27/2022 09:58:01 - INFO - __main__ - Step 50 Global step 50 Train loss 1.22 on epoch=1
05/27/2022 09:58:08 - INFO - __main__ - Global step 50 Train loss 3.18 Classification-F1 0.1844686377144132 on epoch=1
05/27/2022 09:58:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1844686377144132 on epoch=1, global_step=50
05/27/2022 09:58:10 - INFO - __main__ - Step 60 Global step 60 Train loss 1.06 on epoch=1
05/27/2022 09:58:13 - INFO - __main__ - Step 70 Global step 70 Train loss 1.05 on epoch=2
05/27/2022 09:58:15 - INFO - __main__ - Step 80 Global step 80 Train loss 1.04 on epoch=2
05/27/2022 09:58:18 - INFO - __main__ - Step 90 Global step 90 Train loss 1.00 on epoch=2
05/27/2022 09:58:20 - INFO - __main__ - Step 100 Global step 100 Train loss 1.06 on epoch=3
05/27/2022 09:58:27 - INFO - __main__ - Global step 100 Train loss 1.04 Classification-F1 0.12174387732880579 on epoch=3
05/27/2022 09:58:29 - INFO - __main__ - Step 110 Global step 110 Train loss 1.00 on epoch=3
05/27/2022 09:58:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.91 on epoch=3
05/27/2022 09:58:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.95 on epoch=4
05/27/2022 09:58:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.92 on epoch=4
05/27/2022 09:58:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.99 on epoch=4
05/27/2022 09:58:46 - INFO - __main__ - Global step 150 Train loss 0.95 Classification-F1 0.10428775150694453 on epoch=4
05/27/2022 09:58:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.91 on epoch=4
05/27/2022 09:58:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.88 on epoch=5
05/27/2022 09:58:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.86 on epoch=5
05/27/2022 09:58:56 - INFO - __main__ - Step 190 Global step 190 Train loss 1.01 on epoch=5
05/27/2022 09:58:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.90 on epoch=6
05/27/2022 09:59:05 - INFO - __main__ - Global step 200 Train loss 0.91 Classification-F1 0.3173507215495045 on epoch=6
05/27/2022 09:59:05 - INFO - __main__ - Saving model with best Classification-F1: 0.1844686377144132 -> 0.3173507215495045 on epoch=6, global_step=200
05/27/2022 09:59:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.88 on epoch=6
05/27/2022 09:59:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.86 on epoch=6
05/27/2022 09:59:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.90 on epoch=7
05/27/2022 09:59:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.94 on epoch=7
05/27/2022 09:59:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.92 on epoch=7
05/27/2022 09:59:24 - INFO - __main__ - Global step 250 Train loss 0.90 Classification-F1 0.13276186734803205 on epoch=7
05/27/2022 09:59:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.82 on epoch=8
05/27/2022 09:59:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.96 on epoch=8
05/27/2022 09:59:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.92 on epoch=8
05/27/2022 09:59:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.91 on epoch=9
05/27/2022 09:59:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.82 on epoch=9
05/27/2022 09:59:42 - INFO - __main__ - Global step 300 Train loss 0.89 Classification-F1 0.2654838152763464 on epoch=9
05/27/2022 09:59:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.90 on epoch=9
05/27/2022 09:59:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.87 on epoch=9
05/27/2022 09:59:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.84 on epoch=10
05/27/2022 09:59:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.88 on epoch=10
05/27/2022 09:59:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.98 on epoch=10
05/27/2022 10:00:01 - INFO - __main__ - Global step 350 Train loss 0.89 Classification-F1 0.24393466893466892 on epoch=10
05/27/2022 10:00:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.84 on epoch=11
05/27/2022 10:00:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.86 on epoch=11
05/27/2022 10:00:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.90 on epoch=11
05/27/2022 10:00:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.84 on epoch=12
05/27/2022 10:00:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.81 on epoch=12
05/27/2022 10:00:20 - INFO - __main__ - Global step 400 Train loss 0.85 Classification-F1 0.35522265857829255 on epoch=12
05/27/2022 10:00:20 - INFO - __main__ - Saving model with best Classification-F1: 0.3173507215495045 -> 0.35522265857829255 on epoch=12, global_step=400
05/27/2022 10:00:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.89 on epoch=12
05/27/2022 10:00:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.87 on epoch=13
05/27/2022 10:00:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.83 on epoch=13
05/27/2022 10:00:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.84 on epoch=13
05/27/2022 10:00:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.77 on epoch=14
05/27/2022 10:00:38 - INFO - __main__ - Global step 450 Train loss 0.84 Classification-F1 0.2567385118153305 on epoch=14
05/27/2022 10:00:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.76 on epoch=14
05/27/2022 10:00:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.81 on epoch=14
05/27/2022 10:00:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.81 on epoch=14
05/27/2022 10:00:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.85 on epoch=15
05/27/2022 10:00:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.81 on epoch=15
05/27/2022 10:00:57 - INFO - __main__ - Global step 500 Train loss 0.81 Classification-F1 0.16076397671586418 on epoch=15
05/27/2022 10:01:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.80 on epoch=15
05/27/2022 10:01:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.78 on epoch=16
05/27/2022 10:01:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.77 on epoch=16
05/27/2022 10:01:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.75 on epoch=16
05/27/2022 10:01:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.71 on epoch=17
05/27/2022 10:01:16 - INFO - __main__ - Global step 550 Train loss 0.76 Classification-F1 0.5024517670875154 on epoch=17
05/27/2022 10:01:16 - INFO - __main__ - Saving model with best Classification-F1: 0.35522265857829255 -> 0.5024517670875154 on epoch=17, global_step=550
05/27/2022 10:01:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.73 on epoch=17
05/27/2022 10:01:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.77 on epoch=17
05/27/2022 10:01:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.69 on epoch=18
05/27/2022 10:01:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.76 on epoch=18
05/27/2022 10:01:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.76 on epoch=18
05/27/2022 10:01:35 - INFO - __main__ - Global step 600 Train loss 0.74 Classification-F1 0.4440742477335648 on epoch=18
05/27/2022 10:01:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.70 on epoch=19
05/27/2022 10:01:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.71 on epoch=19
05/27/2022 10:01:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.64 on epoch=19
05/27/2022 10:01:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.62 on epoch=19
05/27/2022 10:01:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.71 on epoch=20
05/27/2022 10:01:54 - INFO - __main__ - Global step 650 Train loss 0.68 Classification-F1 0.4843243934106181 on epoch=20
05/27/2022 10:01:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.65 on epoch=20
05/27/2022 10:01:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.79 on epoch=20
05/27/2022 10:02:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.65 on epoch=21
05/27/2022 10:02:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.59 on epoch=21
05/27/2022 10:02:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.62 on epoch=21
05/27/2022 10:02:12 - INFO - __main__ - Global step 700 Train loss 0.66 Classification-F1 0.648785796801413 on epoch=21
05/27/2022 10:02:12 - INFO - __main__ - Saving model with best Classification-F1: 0.5024517670875154 -> 0.648785796801413 on epoch=21, global_step=700
05/27/2022 10:02:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.60 on epoch=22
05/27/2022 10:02:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=22
05/27/2022 10:02:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.71 on epoch=22
05/27/2022 10:02:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.66 on epoch=23
05/27/2022 10:02:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.59 on epoch=23
05/27/2022 10:02:31 - INFO - __main__ - Global step 750 Train loss 0.62 Classification-F1 0.6508954377564441 on epoch=23
05/27/2022 10:02:31 - INFO - __main__ - Saving model with best Classification-F1: 0.648785796801413 -> 0.6508954377564441 on epoch=23, global_step=750
05/27/2022 10:02:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.54 on epoch=23
05/27/2022 10:02:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.63 on epoch=24
05/27/2022 10:02:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.50 on epoch=24
05/27/2022 10:02:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.56 on epoch=24
05/27/2022 10:02:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.62 on epoch=24
05/27/2022 10:02:50 - INFO - __main__ - Global step 800 Train loss 0.57 Classification-F1 0.6285785021079139 on epoch=24
05/27/2022 10:02:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=25
05/27/2022 10:02:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.53 on epoch=25
05/27/2022 10:02:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.52 on epoch=25
05/27/2022 10:03:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=26
05/27/2022 10:03:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.57 on epoch=26
05/27/2022 10:03:09 - INFO - __main__ - Global step 850 Train loss 0.52 Classification-F1 0.5289400516880487 on epoch=26
05/27/2022 10:03:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.50 on epoch=26
05/27/2022 10:03:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.56 on epoch=27
05/27/2022 10:03:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.50 on epoch=27
05/27/2022 10:03:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.67 on epoch=27
05/27/2022 10:03:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.45 on epoch=28
05/27/2022 10:03:28 - INFO - __main__ - Global step 900 Train loss 0.54 Classification-F1 0.6071937321937322 on epoch=28
05/27/2022 10:03:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.50 on epoch=28
05/27/2022 10:03:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.54 on epoch=28
05/27/2022 10:03:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.53 on epoch=29
05/27/2022 10:03:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.45 on epoch=29
05/27/2022 10:03:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.50 on epoch=29
05/27/2022 10:03:47 - INFO - __main__ - Global step 950 Train loss 0.51 Classification-F1 0.6084461311222399 on epoch=29
05/27/2022 10:03:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.53 on epoch=29
05/27/2022 10:03:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=30
05/27/2022 10:03:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.50 on epoch=30
05/27/2022 10:03:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.51 on epoch=30
05/27/2022 10:03:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=31
05/27/2022 10:04:05 - INFO - __main__ - Global step 1000 Train loss 0.48 Classification-F1 0.7795374850168901 on epoch=31
05/27/2022 10:04:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6508954377564441 -> 0.7795374850168901 on epoch=31, global_step=1000
05/27/2022 10:04:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.50 on epoch=31
05/27/2022 10:04:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.46 on epoch=31
05/27/2022 10:04:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=32
05/27/2022 10:04:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=32
05/27/2022 10:04:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.55 on epoch=32
05/27/2022 10:04:24 - INFO - __main__ - Global step 1050 Train loss 0.46 Classification-F1 0.6470580292671884 on epoch=32
05/27/2022 10:04:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.44 on epoch=33
05/27/2022 10:04:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=33
05/27/2022 10:04:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.52 on epoch=33
05/27/2022 10:04:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=34
05/27/2022 10:04:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=34
05/27/2022 10:04:43 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.7787296956037011 on epoch=34
05/27/2022 10:04:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=34
05/27/2022 10:04:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.49 on epoch=34
05/27/2022 10:04:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=35
05/27/2022 10:04:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=35
05/27/2022 10:04:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=35
05/27/2022 10:05:03 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.8135213689469338 on epoch=35
05/27/2022 10:05:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7795374850168901 -> 0.8135213689469338 on epoch=35, global_step=1150
05/27/2022 10:05:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=36
05/27/2022 10:05:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.49 on epoch=36
05/27/2022 10:05:10 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=36
05/27/2022 10:05:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=37
05/27/2022 10:05:15 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=37
05/27/2022 10:05:22 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.8155790845891091 on epoch=37
05/27/2022 10:05:22 - INFO - __main__ - Saving model with best Classification-F1: 0.8135213689469338 -> 0.8155790845891091 on epoch=37, global_step=1200
05/27/2022 10:05:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=37
05/27/2022 10:05:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=38
05/27/2022 10:05:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=38
05/27/2022 10:05:31 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.34 on epoch=38
05/27/2022 10:05:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=39
05/27/2022 10:05:41 - INFO - __main__ - Global step 1250 Train loss 0.41 Classification-F1 0.7854164075068942 on epoch=39
05/27/2022 10:05:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.38 on epoch=39
05/27/2022 10:05:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.40 on epoch=39
05/27/2022 10:05:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=39
05/27/2022 10:05:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=40
05/27/2022 10:05:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=40
05/27/2022 10:06:00 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.8237446798787413 on epoch=40
05/27/2022 10:06:00 - INFO - __main__ - Saving model with best Classification-F1: 0.8155790845891091 -> 0.8237446798787413 on epoch=40, global_step=1300
05/27/2022 10:06:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=40
05/27/2022 10:06:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=41
05/27/2022 10:06:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.47 on epoch=41
05/27/2022 10:06:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=41
05/27/2022 10:06:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.31 on epoch=42
05/27/2022 10:06:19 - INFO - __main__ - Global step 1350 Train loss 0.39 Classification-F1 0.7897666633714874 on epoch=42
05/27/2022 10:06:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=42
05/27/2022 10:06:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=42
05/27/2022 10:06:26 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.29 on epoch=43
05/27/2022 10:06:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.27 on epoch=43
05/27/2022 10:06:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.47 on epoch=43
05/27/2022 10:06:38 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.7722685823234426 on epoch=43
05/27/2022 10:06:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.49 on epoch=44
05/27/2022 10:06:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=44
05/27/2022 10:06:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=44
05/27/2022 10:06:48 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.34 on epoch=44
05/27/2022 10:06:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=45
05/27/2022 10:06:57 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.8269042508811897 on epoch=45
05/27/2022 10:06:57 - INFO - __main__ - Saving model with best Classification-F1: 0.8237446798787413 -> 0.8269042508811897 on epoch=45, global_step=1450
05/27/2022 10:07:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.47 on epoch=45
05/27/2022 10:07:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=45
05/27/2022 10:07:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=46
05/27/2022 10:07:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.33 on epoch=46
05/27/2022 10:07:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=46
05/27/2022 10:07:16 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.8278656015803193 on epoch=46
05/27/2022 10:07:16 - INFO - __main__ - Saving model with best Classification-F1: 0.8269042508811897 -> 0.8278656015803193 on epoch=46, global_step=1500
05/27/2022 10:07:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.28 on epoch=47
05/27/2022 10:07:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=47
05/27/2022 10:07:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=47
05/27/2022 10:07:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.33 on epoch=48
05/27/2022 10:07:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.31 on epoch=48
05/27/2022 10:07:35 - INFO - __main__ - Global step 1550 Train loss 0.32 Classification-F1 0.8303182338610107 on epoch=48
05/27/2022 10:07:35 - INFO - __main__ - Saving model with best Classification-F1: 0.8278656015803193 -> 0.8303182338610107 on epoch=48, global_step=1550
05/27/2022 10:07:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.34 on epoch=48
05/27/2022 10:07:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=49
05/27/2022 10:07:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=49
05/27/2022 10:07:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.30 on epoch=49
05/27/2022 10:07:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.37 on epoch=49
05/27/2022 10:07:54 - INFO - __main__ - Global step 1600 Train loss 0.36 Classification-F1 0.8482529991014236 on epoch=49
05/27/2022 10:07:54 - INFO - __main__ - Saving model with best Classification-F1: 0.8303182338610107 -> 0.8482529991014236 on epoch=49, global_step=1600
05/27/2022 10:07:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.35 on epoch=50
05/27/2022 10:07:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.31 on epoch=50
05/27/2022 10:08:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.31 on epoch=50
05/27/2022 10:08:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=51
05/27/2022 10:08:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.37 on epoch=51
05/27/2022 10:08:14 - INFO - __main__ - Global step 1650 Train loss 0.32 Classification-F1 0.8191399579949931 on epoch=51
05/27/2022 10:08:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.30 on epoch=51
05/27/2022 10:08:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.33 on epoch=52
05/27/2022 10:08:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.34 on epoch=52
05/27/2022 10:08:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=52
05/27/2022 10:08:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=53
05/27/2022 10:08:33 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.8288559631766813 on epoch=53
05/27/2022 10:08:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.27 on epoch=53
05/27/2022 10:08:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.22 on epoch=53
05/27/2022 10:08:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=54
05/27/2022 10:08:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.41 on epoch=54
05/27/2022 10:08:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=54
05/27/2022 10:08:52 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.8363720779716166 on epoch=54
05/27/2022 10:08:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.29 on epoch=54
05/27/2022 10:08:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=55
05/27/2022 10:08:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=55
05/27/2022 10:09:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=55
05/27/2022 10:09:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.27 on epoch=56
05/27/2022 10:09:11 - INFO - __main__ - Global step 1800 Train loss 0.29 Classification-F1 0.8220004830645425 on epoch=56
05/27/2022 10:09:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.31 on epoch=56
05/27/2022 10:09:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=56
05/27/2022 10:09:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.28 on epoch=57
05/27/2022 10:09:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.30 on epoch=57
05/27/2022 10:09:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.27 on epoch=57
05/27/2022 10:09:30 - INFO - __main__ - Global step 1850 Train loss 0.30 Classification-F1 0.768298664011188 on epoch=57
05/27/2022 10:09:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.33 on epoch=58
05/27/2022 10:09:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=58
05/27/2022 10:09:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.27 on epoch=58
05/27/2022 10:09:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.31 on epoch=59
05/27/2022 10:09:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.34 on epoch=59
05/27/2022 10:09:50 - INFO - __main__ - Global step 1900 Train loss 0.29 Classification-F1 0.8344181874619758 on epoch=59
05/27/2022 10:09:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.21 on epoch=59
05/27/2022 10:09:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.31 on epoch=59
05/27/2022 10:09:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.23 on epoch=60
05/27/2022 10:10:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.24 on epoch=60
05/27/2022 10:10:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.23 on epoch=60
05/27/2022 10:10:10 - INFO - __main__ - Global step 1950 Train loss 0.24 Classification-F1 0.8300063705989709 on epoch=60
05/27/2022 10:10:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.38 on epoch=61
05/27/2022 10:10:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.25 on epoch=61
05/27/2022 10:10:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.30 on epoch=61
05/27/2022 10:10:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=62
05/27/2022 10:10:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.22 on epoch=62
05/27/2022 10:10:30 - INFO - __main__ - Global step 2000 Train loss 0.27 Classification-F1 0.8421719916554015 on epoch=62
05/27/2022 10:10:32 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.35 on epoch=62
05/27/2022 10:10:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.34 on epoch=63
05/27/2022 10:10:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.33 on epoch=63
05/27/2022 10:10:40 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.22 on epoch=63
05/27/2022 10:10:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.24 on epoch=64
05/27/2022 10:10:50 - INFO - __main__ - Global step 2050 Train loss 0.30 Classification-F1 0.8579151714118491 on epoch=64
05/27/2022 10:10:50 - INFO - __main__ - Saving model with best Classification-F1: 0.8482529991014236 -> 0.8579151714118491 on epoch=64, global_step=2050
05/27/2022 10:10:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.25 on epoch=64
05/27/2022 10:10:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.22 on epoch=64
05/27/2022 10:10:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.25 on epoch=64
05/27/2022 10:11:00 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.26 on epoch=65
05/27/2022 10:11:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.23 on epoch=65
05/27/2022 10:11:10 - INFO - __main__ - Global step 2100 Train loss 0.24 Classification-F1 0.8058823855923063 on epoch=65
05/27/2022 10:11:12 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.22 on epoch=65
05/27/2022 10:11:15 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.42 on epoch=66
05/27/2022 10:11:17 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.22 on epoch=66
05/27/2022 10:11:20 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=66
05/27/2022 10:11:22 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.28 on epoch=67
05/27/2022 10:11:30 - INFO - __main__ - Global step 2150 Train loss 0.26 Classification-F1 0.8057050009384494 on epoch=67
05/27/2022 10:11:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.29 on epoch=67
05/27/2022 10:11:35 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.32 on epoch=67
05/27/2022 10:11:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.25 on epoch=68
05/27/2022 10:11:40 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.19 on epoch=68
05/27/2022 10:11:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.24 on epoch=68
05/27/2022 10:11:50 - INFO - __main__ - Global step 2200 Train loss 0.26 Classification-F1 0.804045209014524 on epoch=68
05/27/2022 10:11:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.30 on epoch=69
05/27/2022 10:11:55 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.29 on epoch=69
05/27/2022 10:11:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.28 on epoch=69
05/27/2022 10:12:00 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.19 on epoch=69
05/27/2022 10:12:02 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.29 on epoch=70
05/27/2022 10:12:09 - INFO - __main__ - Global step 2250 Train loss 0.27 Classification-F1 0.8324083473591539 on epoch=70
05/27/2022 10:12:12 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.21 on epoch=70
05/27/2022 10:12:14 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.23 on epoch=70
05/27/2022 10:12:17 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.21 on epoch=71
05/27/2022 10:12:20 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.27 on epoch=71
05/27/2022 10:12:22 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.22 on epoch=71
05/27/2022 10:12:29 - INFO - __main__ - Global step 2300 Train loss 0.23 Classification-F1 0.8257875956306692 on epoch=71
05/27/2022 10:12:32 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.23 on epoch=72
05/27/2022 10:12:34 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.18 on epoch=72
05/27/2022 10:12:37 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.26 on epoch=72
05/27/2022 10:12:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.18 on epoch=73
05/27/2022 10:12:42 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.16 on epoch=73
05/27/2022 10:12:49 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.8530067107628269 on epoch=73
05/27/2022 10:12:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.18 on epoch=73
05/27/2022 10:12:54 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.24 on epoch=74
05/27/2022 10:12:57 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.21 on epoch=74
05/27/2022 10:12:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.21 on epoch=74
05/27/2022 10:13:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.22 on epoch=74
05/27/2022 10:13:09 - INFO - __main__ - Global step 2400 Train loss 0.21 Classification-F1 0.8399785848646079 on epoch=74
05/27/2022 10:13:11 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.13 on epoch=75
05/27/2022 10:13:14 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.24 on epoch=75
05/27/2022 10:13:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.19 on epoch=75
05/27/2022 10:13:19 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.16 on epoch=76
05/27/2022 10:13:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.21 on epoch=76
05/27/2022 10:13:29 - INFO - __main__ - Global step 2450 Train loss 0.19 Classification-F1 0.8338451320868201 on epoch=76
05/27/2022 10:13:31 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=76
05/27/2022 10:13:34 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.19 on epoch=77
05/27/2022 10:13:36 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.13 on epoch=77
05/27/2022 10:13:39 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.21 on epoch=77
05/27/2022 10:13:41 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.22 on epoch=78
05/27/2022 10:13:49 - INFO - __main__ - Global step 2500 Train loss 0.19 Classification-F1 0.8154367231979833 on epoch=78
05/27/2022 10:13:51 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.25 on epoch=78
05/27/2022 10:13:54 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.16 on epoch=78
05/27/2022 10:13:56 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.26 on epoch=79
05/27/2022 10:13:59 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.23 on epoch=79
05/27/2022 10:14:01 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=79
05/27/2022 10:14:08 - INFO - __main__ - Global step 2550 Train loss 0.21 Classification-F1 0.8315437761144014 on epoch=79
05/27/2022 10:14:11 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.19 on epoch=79
05/27/2022 10:14:13 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.20 on epoch=80
05/27/2022 10:14:16 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.24 on epoch=80
05/27/2022 10:14:19 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.16 on epoch=80
05/27/2022 10:14:21 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.19 on epoch=81
05/27/2022 10:14:29 - INFO - __main__ - Global step 2600 Train loss 0.19 Classification-F1 0.8290638373680795 on epoch=81
05/27/2022 10:14:31 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=81
05/27/2022 10:14:34 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.19 on epoch=81
05/27/2022 10:14:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.24 on epoch=82
05/27/2022 10:14:39 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.19 on epoch=82
05/27/2022 10:14:41 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.13 on epoch=82
05/27/2022 10:14:49 - INFO - __main__ - Global step 2650 Train loss 0.18 Classification-F1 0.7874950284650952 on epoch=82
05/27/2022 10:14:51 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.22 on epoch=83
05/27/2022 10:14:54 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.17 on epoch=83
05/27/2022 10:14:56 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.17 on epoch=83
05/27/2022 10:14:59 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.23 on epoch=84
05/27/2022 10:15:02 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.22 on epoch=84
05/27/2022 10:15:09 - INFO - __main__ - Global step 2700 Train loss 0.20 Classification-F1 0.808806224310041 on epoch=84
05/27/2022 10:15:11 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.24 on epoch=84
05/27/2022 10:15:14 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.21 on epoch=84
05/27/2022 10:15:16 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.14 on epoch=85
05/27/2022 10:15:19 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.11 on epoch=85
05/27/2022 10:15:21 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.22 on epoch=85
05/27/2022 10:15:29 - INFO - __main__ - Global step 2750 Train loss 0.18 Classification-F1 0.8375099593177732 on epoch=85
05/27/2022 10:15:31 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=86
05/27/2022 10:15:34 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.21 on epoch=86
05/27/2022 10:15:36 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.20 on epoch=86
05/27/2022 10:15:39 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.14 on epoch=87
05/27/2022 10:15:41 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.19 on epoch=87
05/27/2022 10:15:48 - INFO - __main__ - Global step 2800 Train loss 0.18 Classification-F1 0.8474607659667277 on epoch=87
05/27/2022 10:15:51 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.15 on epoch=87
05/27/2022 10:15:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=88
05/27/2022 10:15:56 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.18 on epoch=88
05/27/2022 10:15:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.18 on epoch=88
05/27/2022 10:16:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.16 on epoch=89
05/27/2022 10:16:08 - INFO - __main__ - Global step 2850 Train loss 0.16 Classification-F1 0.8553153456151842 on epoch=89
05/27/2022 10:16:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.17 on epoch=89
05/27/2022 10:16:13 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.11 on epoch=89
05/27/2022 10:16:16 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.21 on epoch=89
05/27/2022 10:16:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.20 on epoch=90
05/27/2022 10:16:21 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.15 on epoch=90
05/27/2022 10:16:29 - INFO - __main__ - Global step 2900 Train loss 0.16 Classification-F1 0.8475100054328366 on epoch=90
05/27/2022 10:16:31 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.29 on epoch=90
05/27/2022 10:16:34 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=91
05/27/2022 10:16:36 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.19 on epoch=91
05/27/2022 10:16:39 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.17 on epoch=91
05/27/2022 10:16:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.23 on epoch=92
05/27/2022 10:16:49 - INFO - __main__ - Global step 2950 Train loss 0.20 Classification-F1 0.8326152858988712 on epoch=92
05/27/2022 10:16:51 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.17 on epoch=92
05/27/2022 10:16:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.16 on epoch=92
05/27/2022 10:16:57 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.17 on epoch=93
05/27/2022 10:16:59 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.16 on epoch=93
05/27/2022 10:17:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.15 on epoch=93
05/27/2022 10:17:03 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 10:17:03 - INFO - __main__ - Printing 3 examples
05/27/2022 10:17:03 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/27/2022 10:17:03 - INFO - __main__ - ['happy']
05/27/2022 10:17:03 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/27/2022 10:17:03 - INFO - __main__ - ['happy']
05/27/2022 10:17:03 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/27/2022 10:17:03 - INFO - __main__ - ['happy']
05/27/2022 10:17:03 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:17:03 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:17:04 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 10:17:04 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 10:17:04 - INFO - __main__ - Printing 3 examples
05/27/2022 10:17:04 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/27/2022 10:17:04 - INFO - __main__ - ['happy']
05/27/2022 10:17:04 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/27/2022 10:17:04 - INFO - __main__ - ['happy']
05/27/2022 10:17:04 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/27/2022 10:17:04 - INFO - __main__ - ['happy']
05/27/2022 10:17:04 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:17:04 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:17:04 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 10:17:09 - INFO - __main__ - Global step 3000 Train loss 0.16 Classification-F1 0.7962757179772476 on epoch=93
05/27/2022 10:17:09 - INFO - __main__ - save last model!
05/27/2022 10:17:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 10:17:09 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 10:17:09 - INFO - __main__ - Printing 3 examples
05/27/2022 10:17:09 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 10:17:09 - INFO - __main__ - ['others']
05/27/2022 10:17:09 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 10:17:09 - INFO - __main__ - ['others']
05/27/2022 10:17:09 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 10:17:09 - INFO - __main__ - ['others']
05/27/2022 10:17:09 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:17:11 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:17:16 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 10:17:21 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 10:17:21 - INFO - __main__ - task name: emo
05/27/2022 10:17:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 10:17:22 - INFO - __main__ - Starting training!
05/27/2022 10:18:31 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_42_0.3_8_predictions.txt
05/27/2022 10:18:31 - INFO - __main__ - Classification-F1 on test data: 0.4284
05/27/2022 10:18:32 - INFO - __main__ - prefix=emo_128_42, lr=0.3, bsz=8, dev_performance=0.8579151714118491, test_performance=0.4284135839111974
05/27/2022 10:18:32 - INFO - __main__ - Running ... prefix=emo_128_42, lr=0.2, bsz=8 ...
05/27/2022 10:18:32 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 10:18:32 - INFO - __main__ - Printing 3 examples
05/27/2022 10:18:32 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/27/2022 10:18:32 - INFO - __main__ - ['happy']
05/27/2022 10:18:32 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/27/2022 10:18:32 - INFO - __main__ - ['happy']
05/27/2022 10:18:32 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/27/2022 10:18:32 - INFO - __main__ - ['happy']
05/27/2022 10:18:32 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:18:33 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:18:33 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 10:18:33 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 10:18:33 - INFO - __main__ - Printing 3 examples
05/27/2022 10:18:33 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/27/2022 10:18:33 - INFO - __main__ - ['happy']
05/27/2022 10:18:33 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/27/2022 10:18:33 - INFO - __main__ - ['happy']
05/27/2022 10:18:33 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/27/2022 10:18:33 - INFO - __main__ - ['happy']
05/27/2022 10:18:33 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:18:33 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:18:34 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 10:18:49 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 10:18:49 - INFO - __main__ - task name: emo
05/27/2022 10:18:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 10:18:50 - INFO - __main__ - Starting training!
05/27/2022 10:18:53 - INFO - __main__ - Step 10 Global step 10 Train loss 7.33 on epoch=0
05/27/2022 10:18:56 - INFO - __main__ - Step 20 Global step 20 Train loss 5.21 on epoch=0
05/27/2022 10:18:58 - INFO - __main__ - Step 30 Global step 30 Train loss 3.21 on epoch=0
05/27/2022 10:19:01 - INFO - __main__ - Step 40 Global step 40 Train loss 2.02 on epoch=1
05/27/2022 10:19:03 - INFO - __main__ - Step 50 Global step 50 Train loss 1.52 on epoch=1
05/27/2022 10:19:10 - INFO - __main__ - Global step 50 Train loss 3.86 Classification-F1 0.14522556954520494 on epoch=1
05/27/2022 10:19:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.14522556954520494 on epoch=1, global_step=50
05/27/2022 10:19:12 - INFO - __main__ - Step 60 Global step 60 Train loss 1.36 on epoch=1
05/27/2022 10:19:15 - INFO - __main__ - Step 70 Global step 70 Train loss 1.21 on epoch=2
05/27/2022 10:19:17 - INFO - __main__ - Step 80 Global step 80 Train loss 1.11 on epoch=2
05/27/2022 10:19:20 - INFO - __main__ - Step 90 Global step 90 Train loss 1.06 on epoch=2
05/27/2022 10:19:22 - INFO - __main__ - Step 100 Global step 100 Train loss 0.93 on epoch=3
05/27/2022 10:19:29 - INFO - __main__ - Global step 100 Train loss 1.13 Classification-F1 0.18239318121318965 on epoch=3
05/27/2022 10:19:29 - INFO - __main__ - Saving model with best Classification-F1: 0.14522556954520494 -> 0.18239318121318965 on epoch=3, global_step=100
05/27/2022 10:19:32 - INFO - __main__ - Step 110 Global step 110 Train loss 1.04 on epoch=3
05/27/2022 10:19:34 - INFO - __main__ - Step 120 Global step 120 Train loss 1.03 on epoch=3
05/27/2022 10:19:37 - INFO - __main__ - Step 130 Global step 130 Train loss 0.97 on epoch=4
05/27/2022 10:19:39 - INFO - __main__ - Step 140 Global step 140 Train loss 0.94 on epoch=4
05/27/2022 10:19:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.91 on epoch=4
05/27/2022 10:19:49 - INFO - __main__ - Global step 150 Train loss 0.98 Classification-F1 0.22443545215657923 on epoch=4
05/27/2022 10:19:49 - INFO - __main__ - Saving model with best Classification-F1: 0.18239318121318965 -> 0.22443545215657923 on epoch=4, global_step=150
05/27/2022 10:19:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.92 on epoch=4
05/27/2022 10:19:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.96 on epoch=5
05/27/2022 10:19:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.96 on epoch=5
05/27/2022 10:19:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.82 on epoch=5
05/27/2022 10:20:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.98 on epoch=6
05/27/2022 10:20:08 - INFO - __main__ - Global step 200 Train loss 0.93 Classification-F1 0.17559244676891736 on epoch=6
05/27/2022 10:20:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.94 on epoch=6
05/27/2022 10:20:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.97 on epoch=6
05/27/2022 10:20:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.86 on epoch=7
05/27/2022 10:20:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.84 on epoch=7
05/27/2022 10:20:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.88 on epoch=7
05/27/2022 10:20:27 - INFO - __main__ - Global step 250 Train loss 0.90 Classification-F1 0.20243611226061176 on epoch=7
05/27/2022 10:20:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.87 on epoch=8
05/27/2022 10:20:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.89 on epoch=8
05/27/2022 10:20:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.87 on epoch=8
05/27/2022 10:20:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.91 on epoch=9
05/27/2022 10:20:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.85 on epoch=9
05/27/2022 10:20:47 - INFO - __main__ - Global step 300 Train loss 0.88 Classification-F1 0.11906404664038252 on epoch=9
05/27/2022 10:20:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.84 on epoch=9
05/27/2022 10:20:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.85 on epoch=9
05/27/2022 10:20:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.86 on epoch=10
05/27/2022 10:20:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.82 on epoch=10
05/27/2022 10:20:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.89 on epoch=10
05/27/2022 10:21:06 - INFO - __main__ - Global step 350 Train loss 0.85 Classification-F1 0.234944708886553 on epoch=10
05/27/2022 10:21:06 - INFO - __main__ - Saving model with best Classification-F1: 0.22443545215657923 -> 0.234944708886553 on epoch=10, global_step=350
05/27/2022 10:21:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.88 on epoch=11
05/27/2022 10:21:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.84 on epoch=11
05/27/2022 10:21:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.81 on epoch=11
05/27/2022 10:21:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.77 on epoch=12
05/27/2022 10:21:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.82 on epoch=12
05/27/2022 10:21:25 - INFO - __main__ - Global step 400 Train loss 0.82 Classification-F1 0.25563044354071446 on epoch=12
05/27/2022 10:21:25 - INFO - __main__ - Saving model with best Classification-F1: 0.234944708886553 -> 0.25563044354071446 on epoch=12, global_step=400
05/27/2022 10:21:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.94 on epoch=12
05/27/2022 10:21:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.86 on epoch=13
05/27/2022 10:21:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.87 on epoch=13
05/27/2022 10:21:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.83 on epoch=13
05/27/2022 10:21:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.82 on epoch=14
05/27/2022 10:21:44 - INFO - __main__ - Global step 450 Train loss 0.86 Classification-F1 0.21102800396331697 on epoch=14
05/27/2022 10:21:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.80 on epoch=14
05/27/2022 10:21:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.85 on epoch=14
05/27/2022 10:21:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.84 on epoch=14
05/27/2022 10:21:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.88 on epoch=15
05/27/2022 10:21:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.81 on epoch=15
05/27/2022 10:22:04 - INFO - __main__ - Global step 500 Train loss 0.84 Classification-F1 0.11523245456753849 on epoch=15
05/27/2022 10:22:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.82 on epoch=15
05/27/2022 10:22:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.87 on epoch=16
05/27/2022 10:22:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.83 on epoch=16
05/27/2022 10:22:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.80 on epoch=16
05/27/2022 10:22:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.75 on epoch=17
05/27/2022 10:22:23 - INFO - __main__ - Global step 550 Train loss 0.81 Classification-F1 0.27211029114674645 on epoch=17
05/27/2022 10:22:23 - INFO - __main__ - Saving model with best Classification-F1: 0.25563044354071446 -> 0.27211029114674645 on epoch=17, global_step=550
05/27/2022 10:22:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.72 on epoch=17
05/27/2022 10:22:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.82 on epoch=17
05/27/2022 10:22:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.76 on epoch=18
05/27/2022 10:22:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.80 on epoch=18
05/27/2022 10:22:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.79 on epoch=18
05/27/2022 10:22:42 - INFO - __main__ - Global step 600 Train loss 0.78 Classification-F1 0.22772735215378553 on epoch=18
05/27/2022 10:22:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.75 on epoch=19
05/27/2022 10:22:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.84 on epoch=19
05/27/2022 10:22:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.76 on epoch=19
05/27/2022 10:22:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.81 on epoch=19
05/27/2022 10:22:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.77 on epoch=20
05/27/2022 10:23:02 - INFO - __main__ - Global step 650 Train loss 0.78 Classification-F1 0.4072574653771693 on epoch=20
05/27/2022 10:23:02 - INFO - __main__ - Saving model with best Classification-F1: 0.27211029114674645 -> 0.4072574653771693 on epoch=20, global_step=650
05/27/2022 10:23:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.72 on epoch=20
05/27/2022 10:23:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.85 on epoch=20
05/27/2022 10:23:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.79 on epoch=21
05/27/2022 10:23:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.79 on epoch=21
05/27/2022 10:23:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.79 on epoch=21
05/27/2022 10:23:21 - INFO - __main__ - Global step 700 Train loss 0.79 Classification-F1 0.6335254720759703 on epoch=21
05/27/2022 10:23:21 - INFO - __main__ - Saving model with best Classification-F1: 0.4072574653771693 -> 0.6335254720759703 on epoch=21, global_step=700
05/27/2022 10:23:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.73 on epoch=22
05/27/2022 10:23:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.76 on epoch=22
05/27/2022 10:23:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.72 on epoch=22
05/27/2022 10:23:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.77 on epoch=23
05/27/2022 10:23:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.70 on epoch=23
05/27/2022 10:23:40 - INFO - __main__ - Global step 750 Train loss 0.74 Classification-F1 0.45482245071859073 on epoch=23
05/27/2022 10:23:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.64 on epoch=23
05/27/2022 10:23:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.68 on epoch=24
05/27/2022 10:23:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.72 on epoch=24
05/27/2022 10:23:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.70 on epoch=24
05/27/2022 10:23:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.70 on epoch=24
05/27/2022 10:24:00 - INFO - __main__ - Global step 800 Train loss 0.69 Classification-F1 0.5739398573419578 on epoch=24
05/27/2022 10:24:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.76 on epoch=25
05/27/2022 10:24:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.78 on epoch=25
05/27/2022 10:24:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.68 on epoch=25
05/27/2022 10:24:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.58 on epoch=26
05/27/2022 10:24:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.72 on epoch=26
05/27/2022 10:24:19 - INFO - __main__ - Global step 850 Train loss 0.70 Classification-F1 0.2900126453743553 on epoch=26
05/27/2022 10:24:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.67 on epoch=26
05/27/2022 10:24:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.68 on epoch=27
05/27/2022 10:24:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.69 on epoch=27
05/27/2022 10:24:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.71 on epoch=27
05/27/2022 10:24:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.63 on epoch=28
05/27/2022 10:24:38 - INFO - __main__ - Global step 900 Train loss 0.68 Classification-F1 0.4811653786312391 on epoch=28
05/27/2022 10:24:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.64 on epoch=28
05/27/2022 10:24:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.57 on epoch=28
05/27/2022 10:24:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.64 on epoch=29
05/27/2022 10:24:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.60 on epoch=29
05/27/2022 10:24:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.65 on epoch=29
05/27/2022 10:24:57 - INFO - __main__ - Global step 950 Train loss 0.62 Classification-F1 0.5572248428536815 on epoch=29
05/27/2022 10:25:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.56 on epoch=29
05/27/2022 10:25:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.59 on epoch=30
05/27/2022 10:25:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.57 on epoch=30
05/27/2022 10:25:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.63 on epoch=30
05/27/2022 10:25:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.54 on epoch=31
05/27/2022 10:25:17 - INFO - __main__ - Global step 1000 Train loss 0.58 Classification-F1 0.6253159193800185 on epoch=31
05/27/2022 10:25:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.63 on epoch=31
05/27/2022 10:25:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.57 on epoch=31
05/27/2022 10:25:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.56 on epoch=32
05/27/2022 10:25:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.58 on epoch=32
05/27/2022 10:25:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.59 on epoch=32
05/27/2022 10:25:36 - INFO - __main__ - Global step 1050 Train loss 0.59 Classification-F1 0.5632982845862673 on epoch=32
05/27/2022 10:25:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.52 on epoch=33
05/27/2022 10:25:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.63 on epoch=33
05/27/2022 10:25:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.50 on epoch=33
05/27/2022 10:25:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.54 on epoch=34
05/27/2022 10:25:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.55 on epoch=34
05/27/2022 10:25:55 - INFO - __main__ - Global step 1100 Train loss 0.55 Classification-F1 0.714979994005712 on epoch=34
05/27/2022 10:25:55 - INFO - __main__ - Saving model with best Classification-F1: 0.6335254720759703 -> 0.714979994005712 on epoch=34, global_step=1100
05/27/2022 10:25:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.58 on epoch=34
05/27/2022 10:26:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.56 on epoch=34
05/27/2022 10:26:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.57 on epoch=35
05/27/2022 10:26:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.55 on epoch=35
05/27/2022 10:26:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.58 on epoch=35
05/27/2022 10:26:14 - INFO - __main__ - Global step 1150 Train loss 0.57 Classification-F1 0.7878081369471776 on epoch=35
05/27/2022 10:26:14 - INFO - __main__ - Saving model with best Classification-F1: 0.714979994005712 -> 0.7878081369471776 on epoch=35, global_step=1150
05/27/2022 10:26:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.57 on epoch=36
05/27/2022 10:26:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.55 on epoch=36
05/27/2022 10:26:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.46 on epoch=36
05/27/2022 10:26:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.59 on epoch=37
05/27/2022 10:26:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.51 on epoch=37
05/27/2022 10:26:34 - INFO - __main__ - Global step 1200 Train loss 0.54 Classification-F1 0.7870559205995643 on epoch=37
05/27/2022 10:26:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.56 on epoch=37
05/27/2022 10:26:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=38
05/27/2022 10:26:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=38
05/27/2022 10:26:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=38
05/27/2022 10:26:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=39
05/27/2022 10:26:53 - INFO - __main__ - Global step 1250 Train loss 0.48 Classification-F1 0.6919850307596973 on epoch=39
05/27/2022 10:26:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.60 on epoch=39
05/27/2022 10:26:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=39
05/27/2022 10:27:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.46 on epoch=39
05/27/2022 10:27:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.52 on epoch=40
05/27/2022 10:27:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.46 on epoch=40
05/27/2022 10:27:12 - INFO - __main__ - Global step 1300 Train loss 0.49 Classification-F1 0.7298306864309033 on epoch=40
05/27/2022 10:27:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=40
05/27/2022 10:27:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.45 on epoch=41
05/27/2022 10:27:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.47 on epoch=41
05/27/2022 10:27:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.51 on epoch=41
05/27/2022 10:27:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.47 on epoch=42
05/27/2022 10:27:31 - INFO - __main__ - Global step 1350 Train loss 0.47 Classification-F1 0.6671246881773197 on epoch=42
05/27/2022 10:27:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.55 on epoch=42
05/27/2022 10:27:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.49 on epoch=42
05/27/2022 10:27:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=43
05/27/2022 10:27:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.59 on epoch=43
05/27/2022 10:27:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=43
05/27/2022 10:27:51 - INFO - __main__ - Global step 1400 Train loss 0.50 Classification-F1 0.7947087060836973 on epoch=43
05/27/2022 10:27:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7878081369471776 -> 0.7947087060836973 on epoch=43, global_step=1400
05/27/2022 10:27:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=44
05/27/2022 10:27:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=44
05/27/2022 10:27:58 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.48 on epoch=44
05/27/2022 10:28:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.42 on epoch=44
05/27/2022 10:28:03 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.37 on epoch=45
05/27/2022 10:28:10 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.7914707277686583 on epoch=45
05/27/2022 10:28:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.48 on epoch=45
05/27/2022 10:28:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=45
05/27/2022 10:28:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.49 on epoch=46
05/27/2022 10:28:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=46
05/27/2022 10:28:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.41 on epoch=46
05/27/2022 10:28:29 - INFO - __main__ - Global step 1500 Train loss 0.44 Classification-F1 0.7823657917661982 on epoch=46
05/27/2022 10:28:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=47
05/27/2022 10:28:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.42 on epoch=47
05/27/2022 10:28:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.36 on epoch=47
05/27/2022 10:28:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.44 on epoch=48
05/27/2022 10:28:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=48
05/27/2022 10:28:49 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.8011504623853859 on epoch=48
05/27/2022 10:28:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7947087060836973 -> 0.8011504623853859 on epoch=48, global_step=1550
05/27/2022 10:28:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.37 on epoch=48
05/27/2022 10:28:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.45 on epoch=49
05/27/2022 10:28:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.40 on epoch=49
05/27/2022 10:28:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.37 on epoch=49
05/27/2022 10:29:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.39 on epoch=49
05/27/2022 10:29:08 - INFO - __main__ - Global step 1600 Train loss 0.40 Classification-F1 0.8137109185245741 on epoch=49
05/27/2022 10:29:08 - INFO - __main__ - Saving model with best Classification-F1: 0.8011504623853859 -> 0.8137109185245741 on epoch=49, global_step=1600
05/27/2022 10:29:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=50
05/27/2022 10:29:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.41 on epoch=50
05/27/2022 10:29:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=50
05/27/2022 10:29:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=51
05/27/2022 10:29:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=51
05/27/2022 10:29:28 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.7601165621143129 on epoch=51
05/27/2022 10:29:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=51
05/27/2022 10:29:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=52
05/27/2022 10:29:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.44 on epoch=52
05/27/2022 10:29:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.44 on epoch=52
05/27/2022 10:29:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=53
05/27/2022 10:29:47 - INFO - __main__ - Global step 1700 Train loss 0.40 Classification-F1 0.8158620739425693 on epoch=53
05/27/2022 10:29:47 - INFO - __main__ - Saving model with best Classification-F1: 0.8137109185245741 -> 0.8158620739425693 on epoch=53, global_step=1700
05/27/2022 10:29:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.46 on epoch=53
05/27/2022 10:29:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.38 on epoch=53
05/27/2022 10:29:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.37 on epoch=54
05/27/2022 10:29:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=54
05/27/2022 10:30:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=54
05/27/2022 10:30:07 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.7786291719931504 on epoch=54
05/27/2022 10:30:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.31 on epoch=54
05/27/2022 10:30:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.33 on epoch=55
05/27/2022 10:30:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.38 on epoch=55
05/27/2022 10:30:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.31 on epoch=55
05/27/2022 10:30:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.47 on epoch=56
05/27/2022 10:30:26 - INFO - __main__ - Global step 1800 Train loss 0.36 Classification-F1 0.7962906826376344 on epoch=56
05/27/2022 10:30:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=56
05/27/2022 10:30:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.29 on epoch=56
05/27/2022 10:30:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.33 on epoch=57
05/27/2022 10:30:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=57
05/27/2022 10:30:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.22 on epoch=57
05/27/2022 10:30:46 - INFO - __main__ - Global step 1850 Train loss 0.32 Classification-F1 0.7156541891453941 on epoch=57
05/27/2022 10:30:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.28 on epoch=58
05/27/2022 10:30:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.30 on epoch=58
05/27/2022 10:30:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.24 on epoch=58
05/27/2022 10:30:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=59
05/27/2022 10:30:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=59
05/27/2022 10:31:05 - INFO - __main__ - Global step 1900 Train loss 0.32 Classification-F1 0.7958898617339577 on epoch=59
05/27/2022 10:31:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=59
05/27/2022 10:31:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.30 on epoch=59
05/27/2022 10:31:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.40 on epoch=60
05/27/2022 10:31:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=60
05/27/2022 10:31:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.31 on epoch=60
05/27/2022 10:31:25 - INFO - __main__ - Global step 1950 Train loss 0.34 Classification-F1 0.8094478365053381 on epoch=60
05/27/2022 10:31:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.34 on epoch=61
05/27/2022 10:31:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.31 on epoch=61
05/27/2022 10:31:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.29 on epoch=61
05/27/2022 10:31:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.26 on epoch=62
05/27/2022 10:31:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.28 on epoch=62
05/27/2022 10:31:44 - INFO - __main__ - Global step 2000 Train loss 0.30 Classification-F1 0.8106505524779791 on epoch=62
05/27/2022 10:31:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.29 on epoch=62
05/27/2022 10:31:49 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.27 on epoch=63
05/27/2022 10:31:52 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.36 on epoch=63
05/27/2022 10:31:55 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.23 on epoch=63
05/27/2022 10:31:57 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.30 on epoch=64
05/27/2022 10:32:04 - INFO - __main__ - Global step 2050 Train loss 0.29 Classification-F1 0.8090263792016872 on epoch=64
05/27/2022 10:32:06 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.27 on epoch=64
05/27/2022 10:32:09 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.26 on epoch=64
05/27/2022 10:32:12 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.33 on epoch=64
05/27/2022 10:32:14 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.31 on epoch=65
05/27/2022 10:32:17 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.27 on epoch=65
05/27/2022 10:32:23 - INFO - __main__ - Global step 2100 Train loss 0.29 Classification-F1 0.8196174757534466 on epoch=65
05/27/2022 10:32:24 - INFO - __main__ - Saving model with best Classification-F1: 0.8158620739425693 -> 0.8196174757534466 on epoch=65, global_step=2100
05/27/2022 10:32:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.26 on epoch=65
05/27/2022 10:32:29 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.30 on epoch=66
05/27/2022 10:32:31 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.25 on epoch=66
05/27/2022 10:32:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.29 on epoch=66
05/27/2022 10:32:36 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.26 on epoch=67
05/27/2022 10:32:43 - INFO - __main__ - Global step 2150 Train loss 0.27 Classification-F1 0.7580653470843057 on epoch=67
05/27/2022 10:32:46 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.31 on epoch=67
05/27/2022 10:32:48 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.23 on epoch=67
05/27/2022 10:32:51 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.26 on epoch=68
05/27/2022 10:32:53 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.28 on epoch=68
05/27/2022 10:32:56 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.26 on epoch=68
05/27/2022 10:33:03 - INFO - __main__ - Global step 2200 Train loss 0.27 Classification-F1 0.789241662285577 on epoch=68
05/27/2022 10:33:05 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.29 on epoch=69
05/27/2022 10:33:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.28 on epoch=69
05/27/2022 10:33:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.23 on epoch=69
05/27/2022 10:33:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.23 on epoch=69
05/27/2022 10:33:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.37 on epoch=70
05/27/2022 10:33:22 - INFO - __main__ - Global step 2250 Train loss 0.28 Classification-F1 0.7679061465760312 on epoch=70
05/27/2022 10:33:25 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.29 on epoch=70
05/27/2022 10:33:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.25 on epoch=70
05/27/2022 10:33:30 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.31 on epoch=71
05/27/2022 10:33:32 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.28 on epoch=71
05/27/2022 10:33:35 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.21 on epoch=71
05/27/2022 10:33:42 - INFO - __main__ - Global step 2300 Train loss 0.27 Classification-F1 0.8354374983194859 on epoch=71
05/27/2022 10:33:42 - INFO - __main__ - Saving model with best Classification-F1: 0.8196174757534466 -> 0.8354374983194859 on epoch=71, global_step=2300
05/27/2022 10:33:44 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=72
05/27/2022 10:33:47 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.31 on epoch=72
05/27/2022 10:33:49 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.21 on epoch=72
05/27/2022 10:33:52 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.24 on epoch=73
05/27/2022 10:33:54 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.32 on epoch=73
05/27/2022 10:34:01 - INFO - __main__ - Global step 2350 Train loss 0.26 Classification-F1 0.816119444281209 on epoch=73
05/27/2022 10:34:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.22 on epoch=73
05/27/2022 10:34:06 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.28 on epoch=74
05/27/2022 10:34:09 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.28 on epoch=74
05/27/2022 10:34:11 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.23 on epoch=74
05/27/2022 10:34:14 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.27 on epoch=74
05/27/2022 10:34:21 - INFO - __main__ - Global step 2400 Train loss 0.25 Classification-F1 0.8254770432566909 on epoch=74
05/27/2022 10:34:23 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.29 on epoch=75
05/27/2022 10:34:26 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.33 on epoch=75
05/27/2022 10:34:28 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.30 on epoch=75
05/27/2022 10:34:31 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.22 on epoch=76
05/27/2022 10:34:33 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=76
05/27/2022 10:34:40 - INFO - __main__ - Global step 2450 Train loss 0.26 Classification-F1 0.7928395595289235 on epoch=76
05/27/2022 10:34:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.21 on epoch=76
05/27/2022 10:34:45 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.24 on epoch=77
05/27/2022 10:34:48 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.30 on epoch=77
05/27/2022 10:34:50 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.20 on epoch=77
05/27/2022 10:34:53 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.18 on epoch=78
05/27/2022 10:35:00 - INFO - __main__ - Global step 2500 Train loss 0.23 Classification-F1 0.8322746810006736 on epoch=78
05/27/2022 10:35:02 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.26 on epoch=78
05/27/2022 10:35:05 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.34 on epoch=78
05/27/2022 10:35:08 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.21 on epoch=79
05/27/2022 10:35:10 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.19 on epoch=79
05/27/2022 10:35:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.20 on epoch=79
05/27/2022 10:35:20 - INFO - __main__ - Global step 2550 Train loss 0.24 Classification-F1 0.8186132069041314 on epoch=79
05/27/2022 10:35:22 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.27 on epoch=79
05/27/2022 10:35:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.30 on epoch=80
05/27/2022 10:35:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.24 on epoch=80
05/27/2022 10:35:30 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.19 on epoch=80
05/27/2022 10:35:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.23 on epoch=81
05/27/2022 10:35:40 - INFO - __main__ - Global step 2600 Train loss 0.24 Classification-F1 0.7958029485585693 on epoch=81
05/27/2022 10:35:42 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.24 on epoch=81
05/27/2022 10:35:45 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.17 on epoch=81
05/27/2022 10:35:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.21 on epoch=82
05/27/2022 10:35:50 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.25 on epoch=82
05/27/2022 10:35:53 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.24 on epoch=82
05/27/2022 10:36:00 - INFO - __main__ - Global step 2650 Train loss 0.22 Classification-F1 0.7322779854234521 on epoch=82
05/27/2022 10:36:02 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.20 on epoch=83
05/27/2022 10:36:05 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.25 on epoch=83
05/27/2022 10:36:08 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.18 on epoch=83
05/27/2022 10:36:10 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=84
05/27/2022 10:36:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.27 on epoch=84
05/27/2022 10:36:20 - INFO - __main__ - Global step 2700 Train loss 0.21 Classification-F1 0.8049056207154802 on epoch=84
05/27/2022 10:36:22 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.16 on epoch=84
05/27/2022 10:36:25 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.25 on epoch=84
05/27/2022 10:36:28 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.21 on epoch=85
05/27/2022 10:36:30 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.18 on epoch=85
05/27/2022 10:36:33 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.19 on epoch=85
05/27/2022 10:36:40 - INFO - __main__ - Global step 2750 Train loss 0.20 Classification-F1 0.8268690798798453 on epoch=85
05/27/2022 10:36:42 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.20 on epoch=86
05/27/2022 10:36:45 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.23 on epoch=86
05/27/2022 10:36:48 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=86
05/27/2022 10:36:50 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.19 on epoch=87
05/27/2022 10:36:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.27 on epoch=87
05/27/2022 10:37:00 - INFO - __main__ - Global step 2800 Train loss 0.21 Classification-F1 0.822452594457445 on epoch=87
05/27/2022 10:37:02 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.18 on epoch=87
05/27/2022 10:37:05 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=88
05/27/2022 10:37:08 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.25 on epoch=88
05/27/2022 10:37:10 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.17 on epoch=88
05/27/2022 10:37:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.15 on epoch=89
05/27/2022 10:37:20 - INFO - __main__ - Global step 2850 Train loss 0.18 Classification-F1 0.8296803821543223 on epoch=89
05/27/2022 10:37:22 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.27 on epoch=89
05/27/2022 10:37:25 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.12 on epoch=89
05/27/2022 10:37:27 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.17 on epoch=89
05/27/2022 10:37:30 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.20 on epoch=90
05/27/2022 10:37:33 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.18 on epoch=90
05/27/2022 10:37:40 - INFO - __main__ - Global step 2900 Train loss 0.19 Classification-F1 0.814480500668865 on epoch=90
05/27/2022 10:37:42 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.12 on epoch=90
05/27/2022 10:37:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.19 on epoch=91
05/27/2022 10:37:48 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.19 on epoch=91
05/27/2022 10:37:50 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.16 on epoch=91
05/27/2022 10:37:53 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.12 on epoch=92
05/27/2022 10:38:00 - INFO - __main__ - Global step 2950 Train loss 0.15 Classification-F1 0.803759788775577 on epoch=92
05/27/2022 10:38:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.16 on epoch=92
05/27/2022 10:38:05 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.18 on epoch=92
05/27/2022 10:38:08 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.16 on epoch=93
05/27/2022 10:38:10 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.16 on epoch=93
05/27/2022 10:38:13 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.17 on epoch=93
05/27/2022 10:38:14 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 10:38:14 - INFO - __main__ - Printing 3 examples
05/27/2022 10:38:14 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/27/2022 10:38:14 - INFO - __main__ - ['others']
05/27/2022 10:38:14 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/27/2022 10:38:14 - INFO - __main__ - ['others']
05/27/2022 10:38:14 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/27/2022 10:38:14 - INFO - __main__ - ['others']
05/27/2022 10:38:14 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:38:14 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:38:15 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 10:38:15 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 10:38:15 - INFO - __main__ - Printing 3 examples
05/27/2022 10:38:15 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/27/2022 10:38:15 - INFO - __main__ - ['others']
05/27/2022 10:38:15 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/27/2022 10:38:15 - INFO - __main__ - ['others']
05/27/2022 10:38:15 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/27/2022 10:38:15 - INFO - __main__ - ['others']
05/27/2022 10:38:15 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:38:15 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:38:15 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 10:38:20 - INFO - __main__ - Global step 3000 Train loss 0.16 Classification-F1 0.7319137018350694 on epoch=93
05/27/2022 10:38:20 - INFO - __main__ - save last model!
05/27/2022 10:38:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 10:38:20 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 10:38:20 - INFO - __main__ - Printing 3 examples
05/27/2022 10:38:20 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 10:38:20 - INFO - __main__ - ['others']
05/27/2022 10:38:20 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 10:38:20 - INFO - __main__ - ['others']
05/27/2022 10:38:20 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 10:38:20 - INFO - __main__ - ['others']
05/27/2022 10:38:20 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:38:22 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:38:27 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 10:38:32 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 10:38:32 - INFO - __main__ - task name: emo
05/27/2022 10:38:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 10:38:33 - INFO - __main__ - Starting training!
05/27/2022 10:39:39 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_42_0.2_8_predictions.txt
05/27/2022 10:39:39 - INFO - __main__ - Classification-F1 on test data: 0.3397
05/27/2022 10:39:40 - INFO - __main__ - prefix=emo_128_42, lr=0.2, bsz=8, dev_performance=0.8354374983194859, test_performance=0.3397395998694044
05/27/2022 10:39:40 - INFO - __main__ - Running ... prefix=emo_128_87, lr=0.5, bsz=8 ...
05/27/2022 10:39:41 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 10:39:41 - INFO - __main__ - Printing 3 examples
05/27/2022 10:39:41 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/27/2022 10:39:41 - INFO - __main__ - ['others']
05/27/2022 10:39:41 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/27/2022 10:39:41 - INFO - __main__ - ['others']
05/27/2022 10:39:41 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/27/2022 10:39:41 - INFO - __main__ - ['others']
05/27/2022 10:39:41 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:39:41 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:39:41 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 10:39:41 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 10:39:41 - INFO - __main__ - Printing 3 examples
05/27/2022 10:39:41 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/27/2022 10:39:41 - INFO - __main__ - ['others']
05/27/2022 10:39:41 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/27/2022 10:39:41 - INFO - __main__ - ['others']
05/27/2022 10:39:41 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/27/2022 10:39:41 - INFO - __main__ - ['others']
05/27/2022 10:39:41 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:39:41 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:39:42 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 10:39:57 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 10:39:57 - INFO - __main__ - task name: emo
05/27/2022 10:39:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 10:39:58 - INFO - __main__ - Starting training!
05/27/2022 10:40:01 - INFO - __main__ - Step 10 Global step 10 Train loss 6.61 on epoch=0
05/27/2022 10:40:04 - INFO - __main__ - Step 20 Global step 20 Train loss 2.52 on epoch=0
05/27/2022 10:40:06 - INFO - __main__ - Step 30 Global step 30 Train loss 1.49 on epoch=0
05/27/2022 10:40:09 - INFO - __main__ - Step 40 Global step 40 Train loss 1.02 on epoch=1
05/27/2022 10:40:11 - INFO - __main__ - Step 50 Global step 50 Train loss 1.18 on epoch=1
05/27/2022 10:40:18 - INFO - __main__ - Global step 50 Train loss 2.56 Classification-F1 0.1 on epoch=1
05/27/2022 10:40:19 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=1, global_step=50
05/27/2022 10:40:21 - INFO - __main__ - Step 60 Global step 60 Train loss 0.98 on epoch=1
05/27/2022 10:40:24 - INFO - __main__ - Step 70 Global step 70 Train loss 0.93 on epoch=2
05/27/2022 10:40:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.92 on epoch=2
05/27/2022 10:40:29 - INFO - __main__ - Step 90 Global step 90 Train loss 0.97 on epoch=2
05/27/2022 10:40:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.89 on epoch=3
05/27/2022 10:40:38 - INFO - __main__ - Global step 100 Train loss 0.94 Classification-F1 0.1835950119532209 on epoch=3
05/27/2022 10:40:38 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1835950119532209 on epoch=3, global_step=100
05/27/2022 10:40:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.92 on epoch=3
05/27/2022 10:40:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.93 on epoch=3
05/27/2022 10:40:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.93 on epoch=4
05/27/2022 10:40:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.90 on epoch=4
05/27/2022 10:40:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.92 on epoch=4
05/27/2022 10:40:57 - INFO - __main__ - Global step 150 Train loss 0.92 Classification-F1 0.174152256905457 on epoch=4
05/27/2022 10:41:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.83 on epoch=4
05/27/2022 10:41:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.98 on epoch=5
05/27/2022 10:41:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.89 on epoch=5
05/27/2022 10:41:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.93 on epoch=5
05/27/2022 10:41:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.95 on epoch=6
05/27/2022 10:41:17 - INFO - __main__ - Global step 200 Train loss 0.92 Classification-F1 0.2823556456500387 on epoch=6
05/27/2022 10:41:17 - INFO - __main__ - Saving model with best Classification-F1: 0.1835950119532209 -> 0.2823556456500387 on epoch=6, global_step=200
05/27/2022 10:41:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.78 on epoch=6
05/27/2022 10:41:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.85 on epoch=6
05/27/2022 10:41:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.91 on epoch=7
05/27/2022 10:41:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.96 on epoch=7
05/27/2022 10:41:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.82 on epoch=7
05/27/2022 10:41:36 - INFO - __main__ - Global step 250 Train loss 0.87 Classification-F1 0.2552672006483958 on epoch=7
05/27/2022 10:41:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.95 on epoch=8
05/27/2022 10:41:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.96 on epoch=8
05/27/2022 10:41:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.88 on epoch=8
05/27/2022 10:41:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.83 on epoch=9
05/27/2022 10:41:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.82 on epoch=9
05/27/2022 10:41:56 - INFO - __main__ - Global step 300 Train loss 0.89 Classification-F1 0.1 on epoch=9
05/27/2022 10:41:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.86 on epoch=9
05/27/2022 10:42:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.80 on epoch=9
05/27/2022 10:42:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.83 on epoch=10
05/27/2022 10:42:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.75 on epoch=10
05/27/2022 10:42:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.86 on epoch=10
05/27/2022 10:42:15 - INFO - __main__ - Global step 350 Train loss 0.82 Classification-F1 0.3016827463616634 on epoch=10
05/27/2022 10:42:15 - INFO - __main__ - Saving model with best Classification-F1: 0.2823556456500387 -> 0.3016827463616634 on epoch=10, global_step=350
05/27/2022 10:42:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.76 on epoch=11
05/27/2022 10:42:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.78 on epoch=11
05/27/2022 10:42:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.80 on epoch=11
05/27/2022 10:42:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.79 on epoch=12
05/27/2022 10:42:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.84 on epoch=12
05/27/2022 10:42:35 - INFO - __main__ - Global step 400 Train loss 0.79 Classification-F1 0.3170575959363977 on epoch=12
05/27/2022 10:42:35 - INFO - __main__ - Saving model with best Classification-F1: 0.3016827463616634 -> 0.3170575959363977 on epoch=12, global_step=400
05/27/2022 10:42:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.77 on epoch=12
05/27/2022 10:42:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.67 on epoch=13
05/27/2022 10:42:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.79 on epoch=13
05/27/2022 10:42:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.74 on epoch=13
05/27/2022 10:42:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.74 on epoch=14
05/27/2022 10:42:54 - INFO - __main__ - Global step 450 Train loss 0.74 Classification-F1 0.5409773341044477 on epoch=14
05/27/2022 10:42:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3170575959363977 -> 0.5409773341044477 on epoch=14, global_step=450
05/27/2022 10:42:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.73 on epoch=14
05/27/2022 10:42:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.76 on epoch=14
05/27/2022 10:43:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.66 on epoch=14
05/27/2022 10:43:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.70 on epoch=15
05/27/2022 10:43:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.71 on epoch=15
05/27/2022 10:43:14 - INFO - __main__ - Global step 500 Train loss 0.71 Classification-F1 0.5145530673299078 on epoch=15
05/27/2022 10:43:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.74 on epoch=15
05/27/2022 10:43:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.66 on epoch=16
05/27/2022 10:43:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.60 on epoch=16
05/27/2022 10:43:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.64 on epoch=16
05/27/2022 10:43:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.62 on epoch=17
05/27/2022 10:43:33 - INFO - __main__ - Global step 550 Train loss 0.65 Classification-F1 0.6629145126066178 on epoch=17
05/27/2022 10:43:33 - INFO - __main__ - Saving model with best Classification-F1: 0.5409773341044477 -> 0.6629145126066178 on epoch=17, global_step=550
05/27/2022 10:43:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.72 on epoch=17
05/27/2022 10:43:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.55 on epoch=17
05/27/2022 10:43:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.55 on epoch=18
05/27/2022 10:43:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.65 on epoch=18
05/27/2022 10:43:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.55 on epoch=18
05/27/2022 10:43:52 - INFO - __main__ - Global step 600 Train loss 0.60 Classification-F1 0.6191411444007597 on epoch=18
05/27/2022 10:43:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.52 on epoch=19
05/27/2022 10:43:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.54 on epoch=19
05/27/2022 10:44:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.56 on epoch=19
05/27/2022 10:44:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.62 on epoch=19
05/27/2022 10:44:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.64 on epoch=20
05/27/2022 10:44:12 - INFO - __main__ - Global step 650 Train loss 0.58 Classification-F1 0.7371361164029141 on epoch=20
05/27/2022 10:44:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6629145126066178 -> 0.7371361164029141 on epoch=20, global_step=650
05/27/2022 10:44:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.58 on epoch=20
05/27/2022 10:44:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.53 on epoch=20
05/27/2022 10:44:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.52 on epoch=21
05/27/2022 10:44:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=21
05/27/2022 10:44:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=21
05/27/2022 10:44:31 - INFO - __main__ - Global step 700 Train loss 0.51 Classification-F1 0.7518375415193294 on epoch=21
05/27/2022 10:44:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7371361164029141 -> 0.7518375415193294 on epoch=21, global_step=700
05/27/2022 10:44:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.50 on epoch=22
05/27/2022 10:44:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.55 on epoch=22
05/27/2022 10:44:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=22
05/27/2022 10:44:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=23
05/27/2022 10:44:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.57 on epoch=23
05/27/2022 10:44:51 - INFO - __main__ - Global step 750 Train loss 0.50 Classification-F1 0.7482708019426221 on epoch=23
05/27/2022 10:44:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=23
05/27/2022 10:44:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=24
05/27/2022 10:44:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.49 on epoch=24
05/27/2022 10:45:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=24
05/27/2022 10:45:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=24
05/27/2022 10:45:10 - INFO - __main__ - Global step 800 Train loss 0.42 Classification-F1 0.7155467204878483 on epoch=24
05/27/2022 10:45:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=25
05/27/2022 10:45:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=25
05/27/2022 10:45:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=25
05/27/2022 10:45:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=26
05/27/2022 10:45:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.32 on epoch=26
05/27/2022 10:45:30 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.8123738444135425 on epoch=26
05/27/2022 10:45:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7518375415193294 -> 0.8123738444135425 on epoch=26, global_step=850
05/27/2022 10:45:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=26
05/27/2022 10:45:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=27
05/27/2022 10:45:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.38 on epoch=27
05/27/2022 10:45:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.32 on epoch=27
05/27/2022 10:45:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=28
05/27/2022 10:45:49 - INFO - __main__ - Global step 900 Train loss 0.36 Classification-F1 0.8053299452481473 on epoch=28
05/27/2022 10:45:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=28
05/27/2022 10:45:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=28
05/27/2022 10:45:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.34 on epoch=29
05/27/2022 10:45:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=29
05/27/2022 10:46:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=29
05/27/2022 10:46:09 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.7651156072512473 on epoch=29
05/27/2022 10:46:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=29
05/27/2022 10:46:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.35 on epoch=30
05/27/2022 10:46:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.34 on epoch=30
05/27/2022 10:46:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=30
05/27/2022 10:46:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.30 on epoch=31
05/27/2022 10:46:28 - INFO - __main__ - Global step 1000 Train loss 0.34 Classification-F1 0.8144433900492263 on epoch=31
05/27/2022 10:46:28 - INFO - __main__ - Saving model with best Classification-F1: 0.8123738444135425 -> 0.8144433900492263 on epoch=31, global_step=1000
05/27/2022 10:46:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.30 on epoch=31
05/27/2022 10:46:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=31
05/27/2022 10:46:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.32 on epoch=32
05/27/2022 10:46:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=32
05/27/2022 10:46:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.29 on epoch=32
05/27/2022 10:46:48 - INFO - __main__ - Global step 1050 Train loss 0.30 Classification-F1 0.7885757286774863 on epoch=32
05/27/2022 10:46:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.30 on epoch=33
05/27/2022 10:46:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.29 on epoch=33
05/27/2022 10:46:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=33
05/27/2022 10:46:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=34
05/27/2022 10:47:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=34
05/27/2022 10:47:07 - INFO - __main__ - Global step 1100 Train loss 0.25 Classification-F1 0.8051488255314216 on epoch=34
05/27/2022 10:47:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=34
05/27/2022 10:47:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=34
05/27/2022 10:47:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.28 on epoch=35
05/27/2022 10:47:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=35
05/27/2022 10:47:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=35
05/27/2022 10:47:26 - INFO - __main__ - Global step 1150 Train loss 0.27 Classification-F1 0.8011832918059265 on epoch=35
05/27/2022 10:47:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.22 on epoch=36
05/27/2022 10:47:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=36
05/27/2022 10:47:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.27 on epoch=36
05/27/2022 10:47:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.28 on epoch=37
05/27/2022 10:47:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.27 on epoch=37
05/27/2022 10:47:46 - INFO - __main__ - Global step 1200 Train loss 0.26 Classification-F1 0.800817919065576 on epoch=37
05/27/2022 10:47:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=37
05/27/2022 10:47:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=38
05/27/2022 10:47:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.25 on epoch=38
05/27/2022 10:47:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=38
05/27/2022 10:47:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=39
05/27/2022 10:48:05 - INFO - __main__ - Global step 1250 Train loss 0.24 Classification-F1 0.7948514705919926 on epoch=39
05/27/2022 10:48:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.24 on epoch=39
05/27/2022 10:48:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.25 on epoch=39
05/27/2022 10:48:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.30 on epoch=39
05/27/2022 10:48:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.22 on epoch=40
05/27/2022 10:48:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.22 on epoch=40
05/27/2022 10:48:25 - INFO - __main__ - Global step 1300 Train loss 0.25 Classification-F1 0.8101180046102119 on epoch=40
05/27/2022 10:48:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.24 on epoch=40
05/27/2022 10:48:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=41
05/27/2022 10:48:33 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=41
05/27/2022 10:48:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=41
05/27/2022 10:48:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.29 on epoch=42
05/27/2022 10:48:45 - INFO - __main__ - Global step 1350 Train loss 0.22 Classification-F1 0.8235950461454429 on epoch=42
05/27/2022 10:48:45 - INFO - __main__ - Saving model with best Classification-F1: 0.8144433900492263 -> 0.8235950461454429 on epoch=42, global_step=1350
05/27/2022 10:48:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.21 on epoch=42
05/27/2022 10:48:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.25 on epoch=42
05/27/2022 10:48:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.22 on epoch=43
05/27/2022 10:48:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=43
05/27/2022 10:48:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=43
05/27/2022 10:49:04 - INFO - __main__ - Global step 1400 Train loss 0.23 Classification-F1 0.8251840647219911 on epoch=43
05/27/2022 10:49:04 - INFO - __main__ - Saving model with best Classification-F1: 0.8235950461454429 -> 0.8251840647219911 on epoch=43, global_step=1400
05/27/2022 10:49:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.32 on epoch=44
05/27/2022 10:49:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=44
05/27/2022 10:49:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=44
05/27/2022 10:49:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.24 on epoch=44
05/27/2022 10:49:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.18 on epoch=45
05/27/2022 10:49:24 - INFO - __main__ - Global step 1450 Train loss 0.23 Classification-F1 0.7844691068299494 on epoch=45
05/27/2022 10:49:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.41 on epoch=45
05/27/2022 10:49:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.51 on epoch=45
05/27/2022 10:49:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.23 on epoch=46
05/27/2022 10:49:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=46
05/27/2022 10:49:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.18 on epoch=46
05/27/2022 10:49:44 - INFO - __main__ - Global step 1500 Train loss 0.29 Classification-F1 0.8100958919354146 on epoch=46
05/27/2022 10:49:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=47
05/27/2022 10:49:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.17 on epoch=47
05/27/2022 10:49:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.28 on epoch=47
05/27/2022 10:49:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.26 on epoch=48
05/27/2022 10:49:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.21 on epoch=48
05/27/2022 10:50:03 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.7977189634444323 on epoch=48
05/27/2022 10:50:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.24 on epoch=48
05/27/2022 10:50:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=49
05/27/2022 10:50:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=49
05/27/2022 10:50:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.19 on epoch=49
05/27/2022 10:50:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=49
05/27/2022 10:50:23 - INFO - __main__ - Global step 1600 Train loss 0.20 Classification-F1 0.7871789221064405 on epoch=49
05/27/2022 10:50:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.19 on epoch=50
05/27/2022 10:50:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.34 on epoch=50
05/27/2022 10:50:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.34 on epoch=50
05/27/2022 10:50:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.23 on epoch=51
05/27/2022 10:50:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.28 on epoch=51
05/27/2022 10:50:43 - INFO - __main__ - Global step 1650 Train loss 0.28 Classification-F1 0.7675407671063219 on epoch=51
05/27/2022 10:50:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.28 on epoch=51
05/27/2022 10:50:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.27 on epoch=52
05/27/2022 10:50:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.24 on epoch=52
05/27/2022 10:50:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=52
05/27/2022 10:50:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.22 on epoch=53
05/27/2022 10:51:03 - INFO - __main__ - Global step 1700 Train loss 0.24 Classification-F1 0.7946551099414536 on epoch=53
05/27/2022 10:51:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.30 on epoch=53
05/27/2022 10:51:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.20 on epoch=53
05/27/2022 10:51:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.24 on epoch=54
05/27/2022 10:51:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.23 on epoch=54
05/27/2022 10:51:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=54
05/27/2022 10:51:23 - INFO - __main__ - Global step 1750 Train loss 0.22 Classification-F1 0.8095146806039678 on epoch=54
05/27/2022 10:51:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=54
05/27/2022 10:51:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=55
05/27/2022 10:51:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.17 on epoch=55
05/27/2022 10:51:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=55
05/27/2022 10:51:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=56
05/27/2022 10:51:43 - INFO - __main__ - Global step 1800 Train loss 0.16 Classification-F1 0.7954125607158803 on epoch=56
05/27/2022 10:51:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=56
05/27/2022 10:51:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=56
05/27/2022 10:51:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.16 on epoch=57
05/27/2022 10:51:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=57
05/27/2022 10:51:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=57
05/27/2022 10:52:03 - INFO - __main__ - Global step 1850 Train loss 0.16 Classification-F1 0.7910221271882287 on epoch=57
05/27/2022 10:52:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.22 on epoch=58
05/27/2022 10:52:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=58
05/27/2022 10:52:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=58
05/27/2022 10:52:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.14 on epoch=59
05/27/2022 10:52:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.17 on epoch=59
05/27/2022 10:52:23 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.7742339998968568 on epoch=59
05/27/2022 10:52:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=59
05/27/2022 10:52:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.18 on epoch=59
05/27/2022 10:52:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.16 on epoch=60
05/27/2022 10:52:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.22 on epoch=60
05/27/2022 10:52:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.20 on epoch=60
05/27/2022 10:52:43 - INFO - __main__ - Global step 1950 Train loss 0.18 Classification-F1 0.804353614889048 on epoch=60
05/27/2022 10:52:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.17 on epoch=61
05/27/2022 10:52:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=61
05/27/2022 10:52:51 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.27 on epoch=61
05/27/2022 10:52:53 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.11 on epoch=62
05/27/2022 10:52:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=62
05/27/2022 10:53:03 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.7981063373474486 on epoch=62
05/27/2022 10:53:06 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.11 on epoch=62
05/27/2022 10:53:08 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.21 on epoch=63
05/27/2022 10:53:11 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.28 on epoch=63
05/27/2022 10:53:13 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.15 on epoch=63
05/27/2022 10:53:16 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.16 on epoch=64
05/27/2022 10:53:23 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.7946567287880748 on epoch=64
05/27/2022 10:53:25 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.17 on epoch=64
05/27/2022 10:53:28 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.11 on epoch=64
05/27/2022 10:53:31 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.12 on epoch=64
05/27/2022 10:53:33 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=65
05/27/2022 10:53:36 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.18 on epoch=65
05/27/2022 10:53:43 - INFO - __main__ - Global step 2100 Train loss 0.13 Classification-F1 0.7900749198078734 on epoch=65
05/27/2022 10:53:45 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.17 on epoch=65
05/27/2022 10:53:48 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=66
05/27/2022 10:53:50 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.16 on epoch=66
05/27/2022 10:53:53 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.23 on epoch=66
05/27/2022 10:53:55 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.12 on epoch=67
05/27/2022 10:54:02 - INFO - __main__ - Global step 2150 Train loss 0.16 Classification-F1 0.8264165396284912 on epoch=67
05/27/2022 10:54:02 - INFO - __main__ - Saving model with best Classification-F1: 0.8251840647219911 -> 0.8264165396284912 on epoch=67, global_step=2150
05/27/2022 10:54:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.14 on epoch=67
05/27/2022 10:54:07 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=67
05/27/2022 10:54:10 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=68
05/27/2022 10:54:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.21 on epoch=68
05/27/2022 10:54:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.14 on epoch=68
05/27/2022 10:54:22 - INFO - __main__ - Global step 2200 Train loss 0.14 Classification-F1 0.8270429040308993 on epoch=68
05/27/2022 10:54:22 - INFO - __main__ - Saving model with best Classification-F1: 0.8264165396284912 -> 0.8270429040308993 on epoch=68, global_step=2200
05/27/2022 10:54:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.18 on epoch=69
05/27/2022 10:54:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.14 on epoch=69
05/27/2022 10:54:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.11 on epoch=69
05/27/2022 10:54:32 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.20 on epoch=69
05/27/2022 10:54:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.16 on epoch=70
05/27/2022 10:54:42 - INFO - __main__ - Global step 2250 Train loss 0.16 Classification-F1 0.8149763844022229 on epoch=70
05/27/2022 10:54:45 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=70
05/27/2022 10:54:47 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=70
05/27/2022 10:54:50 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.14 on epoch=71
05/27/2022 10:54:52 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.18 on epoch=71
05/27/2022 10:54:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.17 on epoch=71
05/27/2022 10:55:02 - INFO - __main__ - Global step 2300 Train loss 0.15 Classification-F1 0.8072917095070966 on epoch=71
05/27/2022 10:55:04 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.11 on epoch=72
05/27/2022 10:55:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.12 on epoch=72
05/27/2022 10:55:10 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.14 on epoch=72
05/27/2022 10:55:12 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.12 on epoch=73
05/27/2022 10:55:15 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=73
05/27/2022 10:55:22 - INFO - __main__ - Global step 2350 Train loss 0.12 Classification-F1 0.8285997895139685 on epoch=73
05/27/2022 10:55:22 - INFO - __main__ - Saving model with best Classification-F1: 0.8270429040308993 -> 0.8285997895139685 on epoch=73, global_step=2350
05/27/2022 10:55:25 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.09 on epoch=73
05/27/2022 10:55:27 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.14 on epoch=74
05/27/2022 10:55:30 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.12 on epoch=74
05/27/2022 10:55:32 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=74
05/27/2022 10:55:35 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=74
05/27/2022 10:55:42 - INFO - __main__ - Global step 2400 Train loss 0.14 Classification-F1 0.7983148713325047 on epoch=74
05/27/2022 10:55:44 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.15 on epoch=75
05/27/2022 10:55:47 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=75
05/27/2022 10:55:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.09 on epoch=75
05/27/2022 10:55:52 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.13 on epoch=76
05/27/2022 10:55:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.10 on epoch=76
05/27/2022 10:56:02 - INFO - __main__ - Global step 2450 Train loss 0.11 Classification-F1 0.823610629294393 on epoch=76
05/27/2022 10:56:04 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=76
05/27/2022 10:56:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.18 on epoch=77
05/27/2022 10:56:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.16 on epoch=77
05/27/2022 10:56:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.09 on epoch=77
05/27/2022 10:56:14 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=78
05/27/2022 10:56:21 - INFO - __main__ - Global step 2500 Train loss 0.12 Classification-F1 0.8249307091880443 on epoch=78
05/27/2022 10:56:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.11 on epoch=78
05/27/2022 10:56:27 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=78
05/27/2022 10:56:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.15 on epoch=79
05/27/2022 10:56:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.10 on epoch=79
05/27/2022 10:56:34 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=79
05/27/2022 10:56:41 - INFO - __main__ - Global step 2550 Train loss 0.10 Classification-F1 0.8276907631016754 on epoch=79
05/27/2022 10:56:44 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.20 on epoch=79
05/27/2022 10:56:46 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.13 on epoch=80
05/27/2022 10:56:49 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.11 on epoch=80
05/27/2022 10:56:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.23 on epoch=80
05/27/2022 10:56:54 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.14 on epoch=81
05/27/2022 10:57:01 - INFO - __main__ - Global step 2600 Train loss 0.16 Classification-F1 0.8335194347702013 on epoch=81
05/27/2022 10:57:01 - INFO - __main__ - Saving model with best Classification-F1: 0.8285997895139685 -> 0.8335194347702013 on epoch=81, global_step=2600
05/27/2022 10:57:04 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.09 on epoch=81
05/27/2022 10:57:07 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.12 on epoch=81
05/27/2022 10:57:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.21 on epoch=82
05/27/2022 10:57:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.12 on epoch=82
05/27/2022 10:57:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.16 on epoch=82
05/27/2022 10:57:21 - INFO - __main__ - Global step 2650 Train loss 0.14 Classification-F1 0.8258754564002885 on epoch=82
05/27/2022 10:57:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.12 on epoch=83
05/27/2022 10:57:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=83
05/27/2022 10:57:29 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=83
05/27/2022 10:57:32 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.18 on epoch=84
05/27/2022 10:57:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=84
05/27/2022 10:57:41 - INFO - __main__ - Global step 2700 Train loss 0.12 Classification-F1 0.8444821292352191 on epoch=84
05/27/2022 10:57:41 - INFO - __main__ - Saving model with best Classification-F1: 0.8335194347702013 -> 0.8444821292352191 on epoch=84, global_step=2700
05/27/2022 10:57:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=84
05/27/2022 10:57:46 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.14 on epoch=84
05/27/2022 10:57:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.11 on epoch=85
05/27/2022 10:57:52 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=85
05/27/2022 10:57:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=85
05/27/2022 10:58:01 - INFO - __main__ - Global step 2750 Train loss 0.10 Classification-F1 0.8367647802973757 on epoch=85
05/27/2022 10:58:04 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=86
05/27/2022 10:58:06 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=86
05/27/2022 10:58:09 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.12 on epoch=86
05/27/2022 10:58:12 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=87
05/27/2022 10:58:14 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.12 on epoch=87
05/27/2022 10:58:21 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.8442342616549656 on epoch=87
05/27/2022 10:58:24 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=87
05/27/2022 10:58:26 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=88
05/27/2022 10:58:29 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.13 on epoch=88
05/27/2022 10:58:32 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.07 on epoch=88
05/27/2022 10:58:34 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=89
05/27/2022 10:58:41 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.8215037045997007 on epoch=89
05/27/2022 10:58:44 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.15 on epoch=89
05/27/2022 10:58:46 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=89
05/27/2022 10:58:49 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.15 on epoch=89
05/27/2022 10:58:51 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=90
05/27/2022 10:58:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.10 on epoch=90
05/27/2022 10:59:01 - INFO - __main__ - Global step 2900 Train loss 0.10 Classification-F1 0.8219139579131407 on epoch=90
05/27/2022 10:59:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.09 on epoch=90
05/27/2022 10:59:06 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.11 on epoch=91
05/27/2022 10:59:09 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.07 on epoch=91
05/27/2022 10:59:11 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=91
05/27/2022 10:59:14 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.13 on epoch=92
05/27/2022 10:59:21 - INFO - __main__ - Global step 2950 Train loss 0.10 Classification-F1 0.8189290221621892 on epoch=92
05/27/2022 10:59:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.14 on epoch=92
05/27/2022 10:59:26 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.08 on epoch=92
05/27/2022 10:59:28 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=93
05/27/2022 10:59:31 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.12 on epoch=93
05/27/2022 10:59:34 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=93
05/27/2022 10:59:35 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 10:59:35 - INFO - __main__ - Printing 3 examples
05/27/2022 10:59:35 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/27/2022 10:59:35 - INFO - __main__ - ['others']
05/27/2022 10:59:35 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/27/2022 10:59:35 - INFO - __main__ - ['others']
05/27/2022 10:59:35 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/27/2022 10:59:35 - INFO - __main__ - ['others']
05/27/2022 10:59:35 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:59:35 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:59:35 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 10:59:35 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 10:59:35 - INFO - __main__ - Printing 3 examples
05/27/2022 10:59:35 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/27/2022 10:59:35 - INFO - __main__ - ['others']
05/27/2022 10:59:35 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/27/2022 10:59:35 - INFO - __main__ - ['others']
05/27/2022 10:59:35 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/27/2022 10:59:35 - INFO - __main__ - ['others']
05/27/2022 10:59:35 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:59:36 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:59:36 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 10:59:41 - INFO - __main__ - Global step 3000 Train loss 0.10 Classification-F1 0.8326273355502779 on epoch=93
05/27/2022 10:59:41 - INFO - __main__ - save last model!
05/27/2022 10:59:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 10:59:41 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 10:59:41 - INFO - __main__ - Printing 3 examples
05/27/2022 10:59:41 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 10:59:41 - INFO - __main__ - ['others']
05/27/2022 10:59:41 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 10:59:41 - INFO - __main__ - ['others']
05/27/2022 10:59:41 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 10:59:41 - INFO - __main__ - ['others']
05/27/2022 10:59:41 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:59:43 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:59:48 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 10:59:52 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 10:59:52 - INFO - __main__ - task name: emo
05/27/2022 10:59:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 10:59:54 - INFO - __main__ - Starting training!
05/27/2022 11:01:03 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_87_0.5_8_predictions.txt
05/27/2022 11:01:03 - INFO - __main__ - Classification-F1 on test data: 0.5695
05/27/2022 11:01:04 - INFO - __main__ - prefix=emo_128_87, lr=0.5, bsz=8, dev_performance=0.8444821292352191, test_performance=0.5694748524562181
05/27/2022 11:01:04 - INFO - __main__ - Running ... prefix=emo_128_87, lr=0.4, bsz=8 ...
05/27/2022 11:01:05 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 11:01:05 - INFO - __main__ - Printing 3 examples
05/27/2022 11:01:05 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/27/2022 11:01:05 - INFO - __main__ - ['others']
05/27/2022 11:01:05 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/27/2022 11:01:05 - INFO - __main__ - ['others']
05/27/2022 11:01:05 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/27/2022 11:01:05 - INFO - __main__ - ['others']
05/27/2022 11:01:05 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:01:05 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:01:05 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 11:01:05 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 11:01:05 - INFO - __main__ - Printing 3 examples
05/27/2022 11:01:05 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/27/2022 11:01:05 - INFO - __main__ - ['others']
05/27/2022 11:01:05 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/27/2022 11:01:05 - INFO - __main__ - ['others']
05/27/2022 11:01:05 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/27/2022 11:01:05 - INFO - __main__ - ['others']
05/27/2022 11:01:05 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:01:05 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:01:06 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 11:01:25 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 11:01:25 - INFO - __main__ - task name: emo
05/27/2022 11:01:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 11:01:26 - INFO - __main__ - Starting training!
05/27/2022 11:01:28 - INFO - __main__ - Step 10 Global step 10 Train loss 6.98 on epoch=0
05/27/2022 11:01:31 - INFO - __main__ - Step 20 Global step 20 Train loss 3.41 on epoch=0
05/27/2022 11:01:34 - INFO - __main__ - Step 30 Global step 30 Train loss 1.77 on epoch=0
05/27/2022 11:01:36 - INFO - __main__ - Step 40 Global step 40 Train loss 1.20 on epoch=1
05/27/2022 11:01:39 - INFO - __main__ - Step 50 Global step 50 Train loss 1.08 on epoch=1
05/27/2022 11:01:46 - INFO - __main__ - Global step 50 Train loss 2.89 Classification-F1 0.10015649452269171 on epoch=1
05/27/2022 11:01:46 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10015649452269171 on epoch=1, global_step=50
05/27/2022 11:01:48 - INFO - __main__ - Step 60 Global step 60 Train loss 1.19 on epoch=1
05/27/2022 11:01:51 - INFO - __main__ - Step 70 Global step 70 Train loss 1.14 on epoch=2
05/27/2022 11:01:53 - INFO - __main__ - Step 80 Global step 80 Train loss 1.07 on epoch=2
05/27/2022 11:01:56 - INFO - __main__ - Step 90 Global step 90 Train loss 0.98 on epoch=2
05/27/2022 11:01:58 - INFO - __main__ - Step 100 Global step 100 Train loss 0.96 on epoch=3
05/27/2022 11:02:05 - INFO - __main__ - Global step 100 Train loss 1.07 Classification-F1 0.1 on epoch=3
05/27/2022 11:02:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.95 on epoch=3
05/27/2022 11:02:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.92 on epoch=3
05/27/2022 11:02:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.94 on epoch=4
05/27/2022 11:02:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.90 on epoch=4
05/27/2022 11:02:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.93 on epoch=4
05/27/2022 11:02:25 - INFO - __main__ - Global step 150 Train loss 0.93 Classification-F1 0.15646917681801403 on epoch=4
05/27/2022 11:02:25 - INFO - __main__ - Saving model with best Classification-F1: 0.10015649452269171 -> 0.15646917681801403 on epoch=4, global_step=150
05/27/2022 11:02:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.92 on epoch=4
05/27/2022 11:02:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.88 on epoch=5
05/27/2022 11:02:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.91 on epoch=5
05/27/2022 11:02:36 - INFO - __main__ - Step 190 Global step 190 Train loss 1.00 on epoch=5
05/27/2022 11:02:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.89 on epoch=6
05/27/2022 11:02:45 - INFO - __main__ - Global step 200 Train loss 0.92 Classification-F1 0.17065495419665536 on epoch=6
05/27/2022 11:02:45 - INFO - __main__ - Saving model with best Classification-F1: 0.15646917681801403 -> 0.17065495419665536 on epoch=6, global_step=200
05/27/2022 11:02:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.80 on epoch=6
05/27/2022 11:02:50 - INFO - __main__ - Step 220 Global step 220 Train loss 0.90 on epoch=6
05/27/2022 11:02:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.89 on epoch=7
05/27/2022 11:02:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.87 on epoch=7
05/27/2022 11:02:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.95 on epoch=7
05/27/2022 11:03:05 - INFO - __main__ - Global step 250 Train loss 0.88 Classification-F1 0.10015649452269171 on epoch=7
05/27/2022 11:03:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.89 on epoch=8
05/27/2022 11:03:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.88 on epoch=8
05/27/2022 11:03:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.93 on epoch=8
05/27/2022 11:03:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.85 on epoch=9
05/27/2022 11:03:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.93 on epoch=9
05/27/2022 11:03:25 - INFO - __main__ - Global step 300 Train loss 0.90 Classification-F1 0.10015649452269171 on epoch=9
05/27/2022 11:03:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.95 on epoch=9
05/27/2022 11:03:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.81 on epoch=9
05/27/2022 11:03:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.86 on epoch=10
05/27/2022 11:03:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.82 on epoch=10
05/27/2022 11:03:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.92 on epoch=10
05/27/2022 11:03:45 - INFO - __main__ - Global step 350 Train loss 0.87 Classification-F1 0.10403246351493978 on epoch=10
05/27/2022 11:03:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.84 on epoch=11
05/27/2022 11:03:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.88 on epoch=11
05/27/2022 11:03:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.86 on epoch=11
05/27/2022 11:03:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=12
05/27/2022 11:03:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.87 on epoch=12
05/27/2022 11:04:04 - INFO - __main__ - Global step 400 Train loss 0.87 Classification-F1 0.14487501721525958 on epoch=12
05/27/2022 11:04:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.79 on epoch=12
05/27/2022 11:04:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.83 on epoch=13
05/27/2022 11:04:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.78 on epoch=13
05/27/2022 11:04:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.77 on epoch=13
05/27/2022 11:04:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.84 on epoch=14
05/27/2022 11:04:24 - INFO - __main__ - Global step 450 Train loss 0.80 Classification-F1 0.2702448210922787 on epoch=14
05/27/2022 11:04:24 - INFO - __main__ - Saving model with best Classification-F1: 0.17065495419665536 -> 0.2702448210922787 on epoch=14, global_step=450
05/27/2022 11:04:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.78 on epoch=14
05/27/2022 11:04:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.71 on epoch=14
05/27/2022 11:04:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.62 on epoch=14
05/27/2022 11:04:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.72 on epoch=15
05/27/2022 11:04:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.74 on epoch=15
05/27/2022 11:04:44 - INFO - __main__ - Global step 500 Train loss 0.72 Classification-F1 0.36719788858939795 on epoch=15
05/27/2022 11:04:44 - INFO - __main__ - Saving model with best Classification-F1: 0.2702448210922787 -> 0.36719788858939795 on epoch=15, global_step=500
05/27/2022 11:04:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.70 on epoch=15
05/27/2022 11:04:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.66 on epoch=16
05/27/2022 11:04:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.63 on epoch=16
05/27/2022 11:04:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.64 on epoch=16
05/27/2022 11:04:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.60 on epoch=17
05/27/2022 11:05:03 - INFO - __main__ - Global step 550 Train loss 0.65 Classification-F1 0.5897939742197917 on epoch=17
05/27/2022 11:05:03 - INFO - __main__ - Saving model with best Classification-F1: 0.36719788858939795 -> 0.5897939742197917 on epoch=17, global_step=550
05/27/2022 11:05:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.68 on epoch=17
05/27/2022 11:05:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.61 on epoch=17
05/27/2022 11:05:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.58 on epoch=18
05/27/2022 11:05:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.69 on epoch=18
05/27/2022 11:05:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.55 on epoch=18
05/27/2022 11:05:23 - INFO - __main__ - Global step 600 Train loss 0.62 Classification-F1 0.6642784676931018 on epoch=18
05/27/2022 11:05:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5897939742197917 -> 0.6642784676931018 on epoch=18, global_step=600
05/27/2022 11:05:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.57 on epoch=19
05/27/2022 11:05:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.56 on epoch=19
05/27/2022 11:05:31 - INFO - __main__ - Step 630 Global step 630 Train loss 0.58 on epoch=19
05/27/2022 11:05:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.52 on epoch=19
05/27/2022 11:05:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=20
05/27/2022 11:05:43 - INFO - __main__ - Global step 650 Train loss 0.55 Classification-F1 0.6929492138173505 on epoch=20
05/27/2022 11:05:43 - INFO - __main__ - Saving model with best Classification-F1: 0.6642784676931018 -> 0.6929492138173505 on epoch=20, global_step=650
05/27/2022 11:05:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.55 on epoch=20
05/27/2022 11:05:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.60 on epoch=20
05/27/2022 11:05:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.53 on epoch=21
05/27/2022 11:05:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.51 on epoch=21
05/27/2022 11:05:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.49 on epoch=21
05/27/2022 11:06:03 - INFO - __main__ - Global step 700 Train loss 0.54 Classification-F1 0.6980241785870142 on epoch=21
05/27/2022 11:06:03 - INFO - __main__ - Saving model with best Classification-F1: 0.6929492138173505 -> 0.6980241785870142 on epoch=21, global_step=700
05/27/2022 11:06:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.60 on epoch=22
05/27/2022 11:06:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.51 on epoch=22
05/27/2022 11:06:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=22
05/27/2022 11:06:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=23
05/27/2022 11:06:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.58 on epoch=23
05/27/2022 11:06:22 - INFO - __main__ - Global step 750 Train loss 0.51 Classification-F1 0.6944879755660833 on epoch=23
05/27/2022 11:06:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.54 on epoch=23
05/27/2022 11:06:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=24
05/27/2022 11:06:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.49 on epoch=24
05/27/2022 11:06:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.51 on epoch=24
05/27/2022 11:06:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=24
05/27/2022 11:06:42 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.666987240309985 on epoch=24
05/27/2022 11:06:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.53 on epoch=25
05/27/2022 11:06:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=25
05/27/2022 11:06:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.48 on epoch=25
05/27/2022 11:06:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=26
05/27/2022 11:06:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=26
05/27/2022 11:07:02 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.6785687336621122 on epoch=26
05/27/2022 11:07:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=26
05/27/2022 11:07:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=27
05/27/2022 11:07:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=27
05/27/2022 11:07:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=27
05/27/2022 11:07:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=28
05/27/2022 11:07:22 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.5892399012329665 on epoch=28
05/27/2022 11:07:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.51 on epoch=28
05/27/2022 11:07:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.36 on epoch=28
05/27/2022 11:07:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=29
05/27/2022 11:07:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=29
05/27/2022 11:07:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.34 on epoch=29
05/27/2022 11:07:42 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.752766658812691 on epoch=29
05/27/2022 11:07:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6980241785870142 -> 0.752766658812691 on epoch=29, global_step=950
05/27/2022 11:07:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=29
05/27/2022 11:07:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.46 on epoch=30
05/27/2022 11:07:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.33 on epoch=30
05/27/2022 11:07:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=30
05/27/2022 11:07:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=31
05/27/2022 11:08:01 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.7565163964003874 on epoch=31
05/27/2022 11:08:01 - INFO - __main__ - Saving model with best Classification-F1: 0.752766658812691 -> 0.7565163964003874 on epoch=31, global_step=1000
05/27/2022 11:08:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=31
05/27/2022 11:08:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.38 on epoch=31
05/27/2022 11:08:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=32
05/27/2022 11:08:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.42 on epoch=32
05/27/2022 11:08:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.29 on epoch=32
05/27/2022 11:08:21 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.7537137681662623 on epoch=32
05/27/2022 11:08:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=33
05/27/2022 11:08:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=33
05/27/2022 11:08:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=33
05/27/2022 11:08:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=34
05/27/2022 11:08:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=34
05/27/2022 11:08:41 - INFO - __main__ - Global step 1100 Train loss 0.37 Classification-F1 0.7598953939461937 on epoch=34
05/27/2022 11:08:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7565163964003874 -> 0.7598953939461937 on epoch=34, global_step=1100
05/27/2022 11:08:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.32 on epoch=34
05/27/2022 11:08:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.29 on epoch=34
05/27/2022 11:08:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=35
05/27/2022 11:08:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.30 on epoch=35
05/27/2022 11:08:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=35
05/27/2022 11:09:01 - INFO - __main__ - Global step 1150 Train loss 0.32 Classification-F1 0.7257672080911393 on epoch=35
05/27/2022 11:09:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=36
05/27/2022 11:09:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=36
05/27/2022 11:09:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=36
05/27/2022 11:09:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.33 on epoch=37
05/27/2022 11:09:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.33 on epoch=37
05/27/2022 11:09:21 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.7981146826904597 on epoch=37
05/27/2022 11:09:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7598953939461937 -> 0.7981146826904597 on epoch=37, global_step=1200
05/27/2022 11:09:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.27 on epoch=37
05/27/2022 11:09:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.30 on epoch=38
05/27/2022 11:09:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=38
05/27/2022 11:09:31 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.35 on epoch=38
05/27/2022 11:09:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=39
05/27/2022 11:09:41 - INFO - __main__ - Global step 1250 Train loss 0.31 Classification-F1 0.7561755936859663 on epoch=39
05/27/2022 11:09:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=39
05/27/2022 11:09:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.22 on epoch=39
05/27/2022 11:09:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.24 on epoch=39
05/27/2022 11:09:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.44 on epoch=40
05/27/2022 11:09:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.26 on epoch=40
05/27/2022 11:10:01 - INFO - __main__ - Global step 1300 Train loss 0.31 Classification-F1 0.7877743545975531 on epoch=40
05/27/2022 11:10:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.29 on epoch=40
05/27/2022 11:10:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.32 on epoch=41
05/27/2022 11:10:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.27 on epoch=41
05/27/2022 11:10:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.37 on epoch=41
05/27/2022 11:10:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.25 on epoch=42
05/27/2022 11:10:20 - INFO - __main__ - Global step 1350 Train loss 0.30 Classification-F1 0.7861137043829673 on epoch=42
05/27/2022 11:10:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.30 on epoch=42
05/27/2022 11:10:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=42
05/27/2022 11:10:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.28 on epoch=43
05/27/2022 11:10:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.33 on epoch=43
05/27/2022 11:10:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.38 on epoch=43
05/27/2022 11:10:40 - INFO - __main__ - Global step 1400 Train loss 0.30 Classification-F1 0.7882912773423723 on epoch=43
05/27/2022 11:10:42 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=44
05/27/2022 11:10:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=44
05/27/2022 11:10:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.23 on epoch=44
05/27/2022 11:10:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=44
05/27/2022 11:10:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=45
05/27/2022 11:10:59 - INFO - __main__ - Global step 1450 Train loss 0.27 Classification-F1 0.7940644436966372 on epoch=45
05/27/2022 11:11:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=45
05/27/2022 11:11:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.26 on epoch=45
05/27/2022 11:11:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.32 on epoch=46
05/27/2022 11:11:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.26 on epoch=46
05/27/2022 11:11:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.30 on epoch=46
05/27/2022 11:11:19 - INFO - __main__ - Global step 1500 Train loss 0.27 Classification-F1 0.759758651631626 on epoch=46
05/27/2022 11:11:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=47
05/27/2022 11:11:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.33 on epoch=47
05/27/2022 11:11:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.17 on epoch=47
05/27/2022 11:11:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=48
05/27/2022 11:11:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.23 on epoch=48
05/27/2022 11:11:39 - INFO - __main__ - Global step 1550 Train loss 0.24 Classification-F1 0.7705161592613913 on epoch=48
05/27/2022 11:11:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.31 on epoch=48
05/27/2022 11:11:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.28 on epoch=49
05/27/2022 11:11:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.29 on epoch=49
05/27/2022 11:11:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=49
05/27/2022 11:11:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=49
05/27/2022 11:11:59 - INFO - __main__ - Global step 1600 Train loss 0.26 Classification-F1 0.7398676072671048 on epoch=49
05/27/2022 11:12:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.26 on epoch=50
05/27/2022 11:12:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.24 on epoch=50
05/27/2022 11:12:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.22 on epoch=50
05/27/2022 11:12:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.21 on epoch=51
05/27/2022 11:12:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=51
05/27/2022 11:12:18 - INFO - __main__ - Global step 1650 Train loss 0.24 Classification-F1 0.7604804880869848 on epoch=51
05/27/2022 11:12:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.20 on epoch=51
05/27/2022 11:12:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.23 on epoch=52
05/27/2022 11:12:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.30 on epoch=52
05/27/2022 11:12:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=52
05/27/2022 11:12:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.19 on epoch=53
05/27/2022 11:12:38 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.7181977821449501 on epoch=53
05/27/2022 11:12:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.31 on epoch=53
05/27/2022 11:12:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.25 on epoch=53
05/27/2022 11:12:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=54
05/27/2022 11:12:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.29 on epoch=54
05/27/2022 11:12:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=54
05/27/2022 11:12:57 - INFO - __main__ - Global step 1750 Train loss 0.24 Classification-F1 0.8056103292265973 on epoch=54
05/27/2022 11:12:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7981146826904597 -> 0.8056103292265973 on epoch=54, global_step=1750
05/27/2022 11:13:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=54
05/27/2022 11:13:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.16 on epoch=55
05/27/2022 11:13:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=55
05/27/2022 11:13:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=55
05/27/2022 11:13:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=56
05/27/2022 11:13:17 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.822618291941575 on epoch=56
05/27/2022 11:13:17 - INFO - __main__ - Saving model with best Classification-F1: 0.8056103292265973 -> 0.822618291941575 on epoch=56, global_step=1800
05/27/2022 11:13:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=56
05/27/2022 11:13:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.30 on epoch=56
05/27/2022 11:13:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.22 on epoch=57
05/27/2022 11:13:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=57
05/27/2022 11:13:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=57
05/27/2022 11:13:36 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.7733622604154273 on epoch=57
05/27/2022 11:13:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.32 on epoch=58
05/27/2022 11:13:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.27 on epoch=58
05/27/2022 11:13:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=58
05/27/2022 11:13:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.22 on epoch=59
05/27/2022 11:13:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.26 on epoch=59
05/27/2022 11:13:56 - INFO - __main__ - Global step 1900 Train loss 0.26 Classification-F1 0.7995219388360953 on epoch=59
05/27/2022 11:13:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.21 on epoch=59
05/27/2022 11:14:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.16 on epoch=59
05/27/2022 11:14:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.29 on epoch=60
05/27/2022 11:14:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=60
05/27/2022 11:14:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.17 on epoch=60
05/27/2022 11:14:15 - INFO - __main__ - Global step 1950 Train loss 0.21 Classification-F1 0.8080316038672315 on epoch=60
05/27/2022 11:14:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=61
05/27/2022 11:14:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=61
05/27/2022 11:14:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.15 on epoch=61
05/27/2022 11:14:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.27 on epoch=62
05/27/2022 11:14:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.24 on epoch=62
05/27/2022 11:14:35 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.82235467783359 on epoch=62
05/27/2022 11:14:37 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.14 on epoch=62
05/27/2022 11:14:40 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.21 on epoch=63
05/27/2022 11:14:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.25 on epoch=63
05/27/2022 11:14:45 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.23 on epoch=63
05/27/2022 11:14:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.22 on epoch=64
05/27/2022 11:14:54 - INFO - __main__ - Global step 2050 Train loss 0.21 Classification-F1 0.7677125735167583 on epoch=64
05/27/2022 11:14:57 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.25 on epoch=64
05/27/2022 11:14:59 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.18 on epoch=64
05/27/2022 11:15:02 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=64
05/27/2022 11:15:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.15 on epoch=65
05/27/2022 11:15:07 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.16 on epoch=65
05/27/2022 11:15:14 - INFO - __main__ - Global step 2100 Train loss 0.18 Classification-F1 0.7993245484538054 on epoch=65
05/27/2022 11:15:16 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.13 on epoch=65
05/27/2022 11:15:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.25 on epoch=66
05/27/2022 11:15:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.20 on epoch=66
05/27/2022 11:15:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.20 on epoch=66
05/27/2022 11:15:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.22 on epoch=67
05/27/2022 11:15:33 - INFO - __main__ - Global step 2150 Train loss 0.20 Classification-F1 0.8093855779544722 on epoch=67
05/27/2022 11:15:36 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.19 on epoch=67
05/27/2022 11:15:38 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.16 on epoch=67
05/27/2022 11:15:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.15 on epoch=68
05/27/2022 11:15:43 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.21 on epoch=68
05/27/2022 11:15:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.24 on epoch=68
05/27/2022 11:15:53 - INFO - __main__ - Global step 2200 Train loss 0.19 Classification-F1 0.8328002445706215 on epoch=68
05/27/2022 11:15:53 - INFO - __main__ - Saving model with best Classification-F1: 0.822618291941575 -> 0.8328002445706215 on epoch=68, global_step=2200
05/27/2022 11:15:55 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.13 on epoch=69
05/27/2022 11:15:58 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.18 on epoch=69
05/27/2022 11:16:00 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.09 on epoch=69
05/27/2022 11:16:03 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.26 on epoch=69
05/27/2022 11:16:06 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.16 on epoch=70
05/27/2022 11:16:12 - INFO - __main__ - Global step 2250 Train loss 0.16 Classification-F1 0.8134878408725369 on epoch=70
05/27/2022 11:16:15 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.17 on epoch=70
05/27/2022 11:16:17 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.17 on epoch=70
05/27/2022 11:16:20 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.16 on epoch=71
05/27/2022 11:16:23 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.11 on epoch=71
05/27/2022 11:16:25 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.10 on epoch=71
05/27/2022 11:16:32 - INFO - __main__ - Global step 2300 Train loss 0.14 Classification-F1 0.8144257703081232 on epoch=71
05/27/2022 11:16:34 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.26 on epoch=72
05/27/2022 11:16:37 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.18 on epoch=72
05/27/2022 11:16:40 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.20 on epoch=72
05/27/2022 11:16:42 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.16 on epoch=73
05/27/2022 11:16:45 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.17 on epoch=73
05/27/2022 11:16:51 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.7838789038727426 on epoch=73
05/27/2022 11:16:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.16 on epoch=73
05/27/2022 11:16:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.11 on epoch=74
05/27/2022 11:16:59 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.14 on epoch=74
05/27/2022 11:17:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.26 on epoch=74
05/27/2022 11:17:04 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.13 on epoch=74
05/27/2022 11:17:11 - INFO - __main__ - Global step 2400 Train loss 0.16 Classification-F1 0.7775624843766029 on epoch=74
05/27/2022 11:17:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=75
05/27/2022 11:17:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=75
05/27/2022 11:17:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=75
05/27/2022 11:17:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.16 on epoch=76
05/27/2022 11:17:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.16 on epoch=76
05/27/2022 11:17:31 - INFO - __main__ - Global step 2450 Train loss 0.16 Classification-F1 0.7683749391464272 on epoch=76
05/27/2022 11:17:33 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=76
05/27/2022 11:17:36 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.25 on epoch=77
05/27/2022 11:17:38 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.17 on epoch=77
05/27/2022 11:17:41 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.14 on epoch=77
05/27/2022 11:17:43 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=78
05/27/2022 11:17:50 - INFO - __main__ - Global step 2500 Train loss 0.16 Classification-F1 0.7798616083033052 on epoch=78
05/27/2022 11:17:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.15 on epoch=78
05/27/2022 11:17:55 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.19 on epoch=78
05/27/2022 11:17:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=79
05/27/2022 11:18:00 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.21 on epoch=79
05/27/2022 11:18:03 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.13 on epoch=79
05/27/2022 11:18:10 - INFO - __main__ - Global step 2550 Train loss 0.15 Classification-F1 0.7978938319408 on epoch=79
05/27/2022 11:18:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.08 on epoch=79
05/27/2022 11:18:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.15 on epoch=80
05/27/2022 11:18:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.16 on epoch=80
05/27/2022 11:18:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.17 on epoch=80
05/27/2022 11:18:22 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.12 on epoch=81
05/27/2022 11:18:29 - INFO - __main__ - Global step 2600 Train loss 0.14 Classification-F1 0.8101094638805985 on epoch=81
05/27/2022 11:18:32 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.10 on epoch=81
05/27/2022 11:18:34 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.19 on epoch=81
05/27/2022 11:18:37 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.15 on epoch=82
05/27/2022 11:18:39 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=82
05/27/2022 11:18:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.13 on epoch=82
05/27/2022 11:18:48 - INFO - __main__ - Global step 2650 Train loss 0.14 Classification-F1 0.8154272248345525 on epoch=82
05/27/2022 11:18:51 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=83
05/27/2022 11:18:54 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=83
05/27/2022 11:18:56 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.20 on epoch=83
05/27/2022 11:18:59 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=84
05/27/2022 11:19:01 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.16 on epoch=84
05/27/2022 11:19:08 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.8175411323692776 on epoch=84
05/27/2022 11:19:10 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.07 on epoch=84
05/27/2022 11:19:13 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.14 on epoch=84
05/27/2022 11:19:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.25 on epoch=85
05/27/2022 11:19:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.11 on epoch=85
05/27/2022 11:19:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=85
05/27/2022 11:19:27 - INFO - __main__ - Global step 2750 Train loss 0.13 Classification-F1 0.8120578045631571 on epoch=85
05/27/2022 11:19:30 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.14 on epoch=86
05/27/2022 11:19:32 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.11 on epoch=86
05/27/2022 11:19:35 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.13 on epoch=86
05/27/2022 11:19:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.16 on epoch=87
05/27/2022 11:19:40 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.20 on epoch=87
05/27/2022 11:19:47 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.810032808642504 on epoch=87
05/27/2022 11:19:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=87
05/27/2022 11:19:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=88
05/27/2022 11:19:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.17 on epoch=88
05/27/2022 11:19:57 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=88
05/27/2022 11:19:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.11 on epoch=89
05/27/2022 11:20:06 - INFO - __main__ - Global step 2850 Train loss 0.11 Classification-F1 0.8073341750841752 on epoch=89
05/27/2022 11:20:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.11 on epoch=89
05/27/2022 11:20:11 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.13 on epoch=89
05/27/2022 11:20:14 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=89
05/27/2022 11:20:16 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.13 on epoch=90
05/27/2022 11:20:19 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.15 on epoch=90
05/27/2022 11:20:26 - INFO - __main__ - Global step 2900 Train loss 0.12 Classification-F1 0.79209324215733 on epoch=90
05/27/2022 11:20:28 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.17 on epoch=90
05/27/2022 11:20:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.16 on epoch=91
05/27/2022 11:20:33 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=91
05/27/2022 11:20:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.20 on epoch=91
05/27/2022 11:20:38 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=92
05/27/2022 11:20:45 - INFO - __main__ - Global step 2950 Train loss 0.14 Classification-F1 0.7725628118003773 on epoch=92
05/27/2022 11:20:48 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.23 on epoch=92
05/27/2022 11:20:50 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.07 on epoch=92
05/27/2022 11:20:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.15 on epoch=93
05/27/2022 11:20:55 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.11 on epoch=93
05/27/2022 11:20:58 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.13 on epoch=93
05/27/2022 11:20:59 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 11:20:59 - INFO - __main__ - Printing 3 examples
05/27/2022 11:20:59 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/27/2022 11:20:59 - INFO - __main__ - ['others']
05/27/2022 11:20:59 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/27/2022 11:20:59 - INFO - __main__ - ['others']
05/27/2022 11:20:59 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/27/2022 11:20:59 - INFO - __main__ - ['others']
05/27/2022 11:20:59 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:20:59 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:21:00 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 11:21:00 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 11:21:00 - INFO - __main__ - Printing 3 examples
05/27/2022 11:21:00 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/27/2022 11:21:00 - INFO - __main__ - ['others']
05/27/2022 11:21:00 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/27/2022 11:21:00 - INFO - __main__ - ['others']
05/27/2022 11:21:00 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/27/2022 11:21:00 - INFO - __main__ - ['others']
05/27/2022 11:21:00 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:21:00 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:21:00 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 11:21:04 - INFO - __main__ - Global step 3000 Train loss 0.14 Classification-F1 0.791102692824278 on epoch=93
05/27/2022 11:21:04 - INFO - __main__ - save last model!
05/27/2022 11:21:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 11:21:05 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 11:21:05 - INFO - __main__ - Printing 3 examples
05/27/2022 11:21:05 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 11:21:05 - INFO - __main__ - ['others']
05/27/2022 11:21:05 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 11:21:05 - INFO - __main__ - ['others']
05/27/2022 11:21:05 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 11:21:05 - INFO - __main__ - ['others']
05/27/2022 11:21:05 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:21:07 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:21:12 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 11:21:17 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 11:21:17 - INFO - __main__ - task name: emo
05/27/2022 11:21:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 11:21:18 - INFO - __main__ - Starting training!
05/27/2022 11:22:24 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_87_0.4_8_predictions.txt
05/27/2022 11:22:24 - INFO - __main__ - Classification-F1 on test data: 0.4332
05/27/2022 11:22:24 - INFO - __main__ - prefix=emo_128_87, lr=0.4, bsz=8, dev_performance=0.8328002445706215, test_performance=0.43321185700660214
05/27/2022 11:22:24 - INFO - __main__ - Running ... prefix=emo_128_87, lr=0.3, bsz=8 ...
05/27/2022 11:22:25 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 11:22:25 - INFO - __main__ - Printing 3 examples
05/27/2022 11:22:25 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/27/2022 11:22:25 - INFO - __main__ - ['others']
05/27/2022 11:22:25 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/27/2022 11:22:25 - INFO - __main__ - ['others']
05/27/2022 11:22:25 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/27/2022 11:22:25 - INFO - __main__ - ['others']
05/27/2022 11:22:25 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:22:25 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:22:26 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 11:22:26 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 11:22:26 - INFO - __main__ - Printing 3 examples
05/27/2022 11:22:26 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/27/2022 11:22:26 - INFO - __main__ - ['others']
05/27/2022 11:22:26 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/27/2022 11:22:26 - INFO - __main__ - ['others']
05/27/2022 11:22:26 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/27/2022 11:22:26 - INFO - __main__ - ['others']
05/27/2022 11:22:26 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:22:26 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:22:26 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 11:22:45 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 11:22:45 - INFO - __main__ - task name: emo
05/27/2022 11:22:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 11:22:46 - INFO - __main__ - Starting training!
05/27/2022 11:22:49 - INFO - __main__ - Step 10 Global step 10 Train loss 7.67 on epoch=0
05/27/2022 11:22:52 - INFO - __main__ - Step 20 Global step 20 Train loss 4.67 on epoch=0
05/27/2022 11:22:55 - INFO - __main__ - Step 30 Global step 30 Train loss 2.33 on epoch=0
05/27/2022 11:22:57 - INFO - __main__ - Step 40 Global step 40 Train loss 1.52 on epoch=1
05/27/2022 11:23:00 - INFO - __main__ - Step 50 Global step 50 Train loss 1.23 on epoch=1
05/27/2022 11:23:07 - INFO - __main__ - Global step 50 Train loss 3.48 Classification-F1 0.12024713266949293 on epoch=1
05/27/2022 11:23:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.12024713266949293 on epoch=1, global_step=50
05/27/2022 11:23:09 - INFO - __main__ - Step 60 Global step 60 Train loss 1.16 on epoch=1
05/27/2022 11:23:12 - INFO - __main__ - Step 70 Global step 70 Train loss 1.00 on epoch=2
05/27/2022 11:23:14 - INFO - __main__ - Step 80 Global step 80 Train loss 1.08 on epoch=2
05/27/2022 11:23:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.93 on epoch=2
05/27/2022 11:23:20 - INFO - __main__ - Step 100 Global step 100 Train loss 0.96 on epoch=3
05/27/2022 11:23:27 - INFO - __main__ - Global step 100 Train loss 1.02 Classification-F1 0.2870028155894479 on epoch=3
05/27/2022 11:23:27 - INFO - __main__ - Saving model with best Classification-F1: 0.12024713266949293 -> 0.2870028155894479 on epoch=3, global_step=100
05/27/2022 11:23:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.90 on epoch=3
05/27/2022 11:23:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.94 on epoch=3
05/27/2022 11:23:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.83 on epoch=4
05/27/2022 11:23:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.89 on epoch=4
05/27/2022 11:23:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.91 on epoch=4
05/27/2022 11:23:46 - INFO - __main__ - Global step 150 Train loss 0.89 Classification-F1 0.17418437246023452 on epoch=4
05/27/2022 11:23:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.95 on epoch=4
05/27/2022 11:23:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.89 on epoch=5
05/27/2022 11:23:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=5
05/27/2022 11:23:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.88 on epoch=5
05/27/2022 11:23:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.90 on epoch=6
05/27/2022 11:24:06 - INFO - __main__ - Global step 200 Train loss 0.90 Classification-F1 0.24984811358015938 on epoch=6
05/27/2022 11:24:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.91 on epoch=6
05/27/2022 11:24:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.88 on epoch=6
05/27/2022 11:24:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.88 on epoch=7
05/27/2022 11:24:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.93 on epoch=7
05/27/2022 11:24:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.96 on epoch=7
05/27/2022 11:24:26 - INFO - __main__ - Global step 250 Train loss 0.91 Classification-F1 0.4162671014589514 on epoch=7
05/27/2022 11:24:26 - INFO - __main__ - Saving model with best Classification-F1: 0.2870028155894479 -> 0.4162671014589514 on epoch=7, global_step=250
05/27/2022 11:24:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.93 on epoch=8
05/27/2022 11:24:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.83 on epoch=8
05/27/2022 11:24:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.84 on epoch=8
05/27/2022 11:24:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.80 on epoch=9
05/27/2022 11:24:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.93 on epoch=9
05/27/2022 11:24:46 - INFO - __main__ - Global step 300 Train loss 0.87 Classification-F1 0.31665963898108473 on epoch=9
05/27/2022 11:24:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.86 on epoch=9
05/27/2022 11:24:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.82 on epoch=9
05/27/2022 11:24:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.78 on epoch=10
05/27/2022 11:24:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.87 on epoch=10
05/27/2022 11:24:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.75 on epoch=10
05/27/2022 11:25:06 - INFO - __main__ - Global step 350 Train loss 0.82 Classification-F1 0.3880953560784707 on epoch=10
05/27/2022 11:25:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.85 on epoch=11
05/27/2022 11:25:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.83 on epoch=11
05/27/2022 11:25:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.81 on epoch=11
05/27/2022 11:25:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.82 on epoch=12
05/27/2022 11:25:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.78 on epoch=12
05/27/2022 11:25:25 - INFO - __main__ - Global step 400 Train loss 0.82 Classification-F1 0.39937885897724484 on epoch=12
05/27/2022 11:25:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.70 on epoch=12
05/27/2022 11:25:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.75 on epoch=13
05/27/2022 11:25:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.80 on epoch=13
05/27/2022 11:25:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.83 on epoch=13
05/27/2022 11:25:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.79 on epoch=14
05/27/2022 11:25:45 - INFO - __main__ - Global step 450 Train loss 0.77 Classification-F1 0.22472100842971876 on epoch=14
05/27/2022 11:25:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.79 on epoch=14
05/27/2022 11:25:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.76 on epoch=14
05/27/2022 11:25:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.71 on epoch=14
05/27/2022 11:25:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.75 on epoch=15
05/27/2022 11:25:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.66 on epoch=15
05/27/2022 11:26:05 - INFO - __main__ - Global step 500 Train loss 0.73 Classification-F1 0.37800004995304126 on epoch=15
05/27/2022 11:26:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.77 on epoch=15
05/27/2022 11:26:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.69 on epoch=16
05/27/2022 11:26:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.70 on epoch=16
05/27/2022 11:26:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.68 on epoch=16
05/27/2022 11:26:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.71 on epoch=17
05/27/2022 11:26:25 - INFO - __main__ - Global step 550 Train loss 0.71 Classification-F1 0.5117319316075515 on epoch=17
05/27/2022 11:26:25 - INFO - __main__ - Saving model with best Classification-F1: 0.4162671014589514 -> 0.5117319316075515 on epoch=17, global_step=550
05/27/2022 11:26:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.68 on epoch=17
05/27/2022 11:26:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.63 on epoch=17
05/27/2022 11:26:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.62 on epoch=18
05/27/2022 11:26:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.63 on epoch=18
05/27/2022 11:26:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.58 on epoch=18
05/27/2022 11:26:45 - INFO - __main__ - Global step 600 Train loss 0.63 Classification-F1 0.4964966125292588 on epoch=18
05/27/2022 11:26:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.63 on epoch=19
05/27/2022 11:26:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.64 on epoch=19
05/27/2022 11:26:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.61 on epoch=19
05/27/2022 11:26:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.59 on epoch=19
05/27/2022 11:26:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=20
05/27/2022 11:27:05 - INFO - __main__ - Global step 650 Train loss 0.60 Classification-F1 0.5000619475744652 on epoch=20
05/27/2022 11:27:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=20
05/27/2022 11:27:10 - INFO - __main__ - Step 670 Global step 670 Train loss 0.61 on epoch=20
05/27/2022 11:27:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.67 on epoch=21
05/27/2022 11:27:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.53 on epoch=21
05/27/2022 11:27:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=21
05/27/2022 11:27:24 - INFO - __main__ - Global step 700 Train loss 0.57 Classification-F1 0.6074273854954275 on epoch=21
05/27/2022 11:27:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5117319316075515 -> 0.6074273854954275 on epoch=21, global_step=700
05/27/2022 11:27:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.53 on epoch=22
05/27/2022 11:27:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.62 on epoch=22
05/27/2022 11:27:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=22
05/27/2022 11:27:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=23
05/27/2022 11:27:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.56 on epoch=23
05/27/2022 11:27:45 - INFO - __main__ - Global step 750 Train loss 0.53 Classification-F1 0.6376177502762181 on epoch=23
05/27/2022 11:27:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6074273854954275 -> 0.6376177502762181 on epoch=23, global_step=750
05/27/2022 11:27:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.57 on epoch=23
05/27/2022 11:27:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.54 on epoch=24
05/27/2022 11:27:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.55 on epoch=24
05/27/2022 11:27:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=24
05/27/2022 11:27:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=24
05/27/2022 11:28:04 - INFO - __main__ - Global step 800 Train loss 0.51 Classification-F1 0.5397908371012714 on epoch=24
05/27/2022 11:28:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.63 on epoch=25
05/27/2022 11:28:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=25
05/27/2022 11:28:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.52 on epoch=25
05/27/2022 11:28:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=26
05/27/2022 11:28:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.57 on epoch=26
05/27/2022 11:28:24 - INFO - __main__ - Global step 850 Train loss 0.52 Classification-F1 0.5551863861486624 on epoch=26
05/27/2022 11:28:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.46 on epoch=26
05/27/2022 11:28:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.51 on epoch=27
05/27/2022 11:28:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.55 on epoch=27
05/27/2022 11:28:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=27
05/27/2022 11:28:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.43 on epoch=28
05/27/2022 11:28:44 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.6287303936481736 on epoch=28
05/27/2022 11:28:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.59 on epoch=28
05/27/2022 11:28:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=28
05/27/2022 11:28:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.49 on epoch=29
05/27/2022 11:28:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.54 on epoch=29
05/27/2022 11:28:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=29
05/27/2022 11:29:04 - INFO - __main__ - Global step 950 Train loss 0.50 Classification-F1 0.674837183915961 on epoch=29
05/27/2022 11:29:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6376177502762181 -> 0.674837183915961 on epoch=29, global_step=950
05/27/2022 11:29:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=29
05/27/2022 11:29:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.56 on epoch=30
05/27/2022 11:29:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=30
05/27/2022 11:29:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=30
05/27/2022 11:29:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.49 on epoch=31
05/27/2022 11:29:24 - INFO - __main__ - Global step 1000 Train loss 0.46 Classification-F1 0.7521535870706528 on epoch=31
05/27/2022 11:29:24 - INFO - __main__ - Saving model with best Classification-F1: 0.674837183915961 -> 0.7521535870706528 on epoch=31, global_step=1000
05/27/2022 11:29:27 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.48 on epoch=31
05/27/2022 11:29:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=31
05/27/2022 11:29:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.50 on epoch=32
05/27/2022 11:29:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=32
05/27/2022 11:29:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=32
05/27/2022 11:29:44 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.7379511229259318 on epoch=32
05/27/2022 11:29:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=33
05/27/2022 11:29:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=33
05/27/2022 11:29:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=33
05/27/2022 11:29:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=34
05/27/2022 11:29:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.54 on epoch=34
05/27/2022 11:30:04 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.751120713820705 on epoch=34
05/27/2022 11:30:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=34
05/27/2022 11:30:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=34
05/27/2022 11:30:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=35
05/27/2022 11:30:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=35
05/27/2022 11:30:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=35
05/27/2022 11:30:24 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.7651222260489349 on epoch=35
05/27/2022 11:30:24 - INFO - __main__ - Saving model with best Classification-F1: 0.7521535870706528 -> 0.7651222260489349 on epoch=35, global_step=1150
05/27/2022 11:30:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.35 on epoch=36
05/27/2022 11:30:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=36
05/27/2022 11:30:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.36 on epoch=36
05/27/2022 11:30:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.33 on epoch=37
05/27/2022 11:30:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=37
05/27/2022 11:30:44 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.7715302144160439 on epoch=37
05/27/2022 11:30:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7651222260489349 -> 0.7715302144160439 on epoch=37, global_step=1200
05/27/2022 11:30:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.31 on epoch=37
05/27/2022 11:30:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=38
05/27/2022 11:30:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.43 on epoch=38
05/27/2022 11:30:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=38
05/27/2022 11:30:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.30 on epoch=39
05/27/2022 11:31:04 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.7217403151253518 on epoch=39
05/27/2022 11:31:07 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.38 on epoch=39
05/27/2022 11:31:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.24 on epoch=39
05/27/2022 11:31:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.24 on epoch=39
05/27/2022 11:31:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=40
05/27/2022 11:31:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=40
05/27/2022 11:31:24 - INFO - __main__ - Global step 1300 Train loss 0.33 Classification-F1 0.7312810089752291 on epoch=40
05/27/2022 11:31:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.32 on epoch=40
05/27/2022 11:31:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=41
05/27/2022 11:31:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.35 on epoch=41
05/27/2022 11:31:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=41
05/27/2022 11:31:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.40 on epoch=42
05/27/2022 11:31:44 - INFO - __main__ - Global step 1350 Train loss 0.34 Classification-F1 0.7857540141118202 on epoch=42
05/27/2022 11:31:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7715302144160439 -> 0.7857540141118202 on epoch=42, global_step=1350
05/27/2022 11:31:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=42
05/27/2022 11:31:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=42
05/27/2022 11:31:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.29 on epoch=43
05/27/2022 11:31:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.46 on epoch=43
05/27/2022 11:31:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.27 on epoch=43
05/27/2022 11:32:04 - INFO - __main__ - Global step 1400 Train loss 0.35 Classification-F1 0.7737842433878815 on epoch=43
05/27/2022 11:32:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.30 on epoch=44
05/27/2022 11:32:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.29 on epoch=44
05/27/2022 11:32:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.24 on epoch=44
05/27/2022 11:32:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.34 on epoch=44
05/27/2022 11:32:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=45
05/27/2022 11:32:24 - INFO - __main__ - Global step 1450 Train loss 0.29 Classification-F1 0.7873374660222663 on epoch=45
05/27/2022 11:32:24 - INFO - __main__ - Saving model with best Classification-F1: 0.7857540141118202 -> 0.7873374660222663 on epoch=45, global_step=1450
05/27/2022 11:32:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=45
05/27/2022 11:32:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=45
05/27/2022 11:32:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.25 on epoch=46
05/27/2022 11:32:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=46
05/27/2022 11:32:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.26 on epoch=46
05/27/2022 11:32:44 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.7602024172736972 on epoch=46
05/27/2022 11:32:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.31 on epoch=47
05/27/2022 11:32:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=47
05/27/2022 11:32:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=47
05/27/2022 11:32:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=48
05/27/2022 11:32:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.30 on epoch=48
05/27/2022 11:33:03 - INFO - __main__ - Global step 1550 Train loss 0.34 Classification-F1 0.8011058671682041 on epoch=48
05/27/2022 11:33:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7873374660222663 -> 0.8011058671682041 on epoch=48, global_step=1550
05/27/2022 11:33:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=48
05/27/2022 11:33:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.26 on epoch=49
05/27/2022 11:33:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=49
05/27/2022 11:33:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=49
05/27/2022 11:33:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.28 on epoch=49
05/27/2022 11:33:23 - INFO - __main__ - Global step 1600 Train loss 0.28 Classification-F1 0.6996255087755014 on epoch=49
05/27/2022 11:33:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.35 on epoch=50
05/27/2022 11:33:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=50
05/27/2022 11:33:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=50
05/27/2022 11:33:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=51
05/27/2022 11:33:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=51
05/27/2022 11:33:42 - INFO - __main__ - Global step 1650 Train loss 0.25 Classification-F1 0.7817365450384427 on epoch=51
05/27/2022 11:33:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.32 on epoch=51
05/27/2022 11:33:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=52
05/27/2022 11:33:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.34 on epoch=52
05/27/2022 11:33:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.29 on epoch=52
05/27/2022 11:33:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.27 on epoch=53
05/27/2022 11:34:01 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.7345600949077903 on epoch=53
05/27/2022 11:34:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=53
05/27/2022 11:34:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.32 on epoch=53
05/27/2022 11:34:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.23 on epoch=54
05/27/2022 11:34:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.28 on epoch=54
05/27/2022 11:34:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.28 on epoch=54
05/27/2022 11:34:21 - INFO - __main__ - Global step 1750 Train loss 0.29 Classification-F1 0.7871082721626201 on epoch=54
05/27/2022 11:34:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.27 on epoch=54
05/27/2022 11:34:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.24 on epoch=55
05/27/2022 11:34:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.20 on epoch=55
05/27/2022 11:34:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.32 on epoch=55
05/27/2022 11:34:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.18 on epoch=56
05/27/2022 11:34:40 - INFO - __main__ - Global step 1800 Train loss 0.24 Classification-F1 0.8304376705725149 on epoch=56
05/27/2022 11:34:40 - INFO - __main__ - Saving model with best Classification-F1: 0.8011058671682041 -> 0.8304376705725149 on epoch=56, global_step=1800
05/27/2022 11:34:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=56
05/27/2022 11:34:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=56
05/27/2022 11:34:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.22 on epoch=57
05/27/2022 11:34:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.25 on epoch=57
05/27/2022 11:34:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.24 on epoch=57
05/27/2022 11:35:00 - INFO - __main__ - Global step 1850 Train loss 0.25 Classification-F1 0.750930971183383 on epoch=57
05/27/2022 11:35:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=58
05/27/2022 11:35:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=58
05/27/2022 11:35:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.21 on epoch=58
05/27/2022 11:35:10 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=59
05/27/2022 11:35:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.27 on epoch=59
05/27/2022 11:35:19 - INFO - __main__ - Global step 1900 Train loss 0.24 Classification-F1 0.8094095947154908 on epoch=59
05/27/2022 11:35:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.18 on epoch=59
05/27/2022 11:35:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=59
05/27/2022 11:35:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.17 on epoch=60
05/27/2022 11:35:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=60
05/27/2022 11:35:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.24 on epoch=60
05/27/2022 11:35:39 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.8107704522000627 on epoch=60
05/27/2022 11:35:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.14 on epoch=61
05/27/2022 11:35:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=61
05/27/2022 11:35:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=61
05/27/2022 11:35:49 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.23 on epoch=62
05/27/2022 11:35:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.12 on epoch=62
05/27/2022 11:35:58 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.8152998849742529 on epoch=62
05/27/2022 11:36:01 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.26 on epoch=62
05/27/2022 11:36:03 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=63
05/27/2022 11:36:06 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.30 on epoch=63
05/27/2022 11:36:08 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=63
05/27/2022 11:36:11 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.24 on epoch=64
05/27/2022 11:36:18 - INFO - __main__ - Global step 2050 Train loss 0.24 Classification-F1 0.7916739949154337 on epoch=64
05/27/2022 11:36:20 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.31 on epoch=64
05/27/2022 11:36:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.23 on epoch=64
05/27/2022 11:36:25 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.22 on epoch=64
05/27/2022 11:36:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.19 on epoch=65
05/27/2022 11:36:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.11 on epoch=65
05/27/2022 11:36:37 - INFO - __main__ - Global step 2100 Train loss 0.21 Classification-F1 0.7977894211238615 on epoch=65
05/27/2022 11:36:40 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.15 on epoch=65
05/27/2022 11:36:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.17 on epoch=66
05/27/2022 11:36:45 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.24 on epoch=66
05/27/2022 11:36:47 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.22 on epoch=66
05/27/2022 11:36:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.25 on epoch=67
05/27/2022 11:36:57 - INFO - __main__ - Global step 2150 Train loss 0.21 Classification-F1 0.813790797295255 on epoch=67
05/27/2022 11:36:59 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.18 on epoch=67
05/27/2022 11:37:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=67
05/27/2022 11:37:04 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.15 on epoch=68
05/27/2022 11:37:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.27 on epoch=68
05/27/2022 11:37:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.22 on epoch=68
05/27/2022 11:37:16 - INFO - __main__ - Global step 2200 Train loss 0.19 Classification-F1 0.8331828861642365 on epoch=68
05/27/2022 11:37:16 - INFO - __main__ - Saving model with best Classification-F1: 0.8304376705725149 -> 0.8331828861642365 on epoch=68, global_step=2200
05/27/2022 11:37:19 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.19 on epoch=69
05/27/2022 11:37:21 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.26 on epoch=69
05/27/2022 11:37:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.25 on epoch=69
05/27/2022 11:37:26 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.22 on epoch=69
05/27/2022 11:37:29 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.27 on epoch=70
05/27/2022 11:37:35 - INFO - __main__ - Global step 2250 Train loss 0.24 Classification-F1 0.7884515929282092 on epoch=70
05/27/2022 11:37:38 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.18 on epoch=70
05/27/2022 11:37:40 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=70
05/27/2022 11:37:43 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.21 on epoch=71
05/27/2022 11:37:46 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.25 on epoch=71
05/27/2022 11:37:48 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.14 on epoch=71
05/27/2022 11:37:55 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.8118831389110166 on epoch=71
05/27/2022 11:37:57 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=72
05/27/2022 11:38:00 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.20 on epoch=72
05/27/2022 11:38:02 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.19 on epoch=72
05/27/2022 11:38:05 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.24 on epoch=73
05/27/2022 11:38:07 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=73
05/27/2022 11:38:14 - INFO - __main__ - Global step 2350 Train loss 0.21 Classification-F1 0.7434146315008958 on epoch=73
05/27/2022 11:38:17 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.28 on epoch=73
05/27/2022 11:38:19 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.15 on epoch=74
05/27/2022 11:38:22 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.26 on epoch=74
05/27/2022 11:38:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.10 on epoch=74
05/27/2022 11:38:27 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.26 on epoch=74
05/27/2022 11:38:34 - INFO - __main__ - Global step 2400 Train loss 0.21 Classification-F1 0.7855187521013446 on epoch=74
05/27/2022 11:38:37 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=75
05/27/2022 11:38:39 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.18 on epoch=75
05/27/2022 11:38:42 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.18 on epoch=75
05/27/2022 11:38:44 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.17 on epoch=76
05/27/2022 11:38:47 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=76
05/27/2022 11:38:54 - INFO - __main__ - Global step 2450 Train loss 0.17 Classification-F1 0.8308590033484968 on epoch=76
05/27/2022 11:38:56 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.14 on epoch=76
05/27/2022 11:38:59 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=77
05/27/2022 11:39:01 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=77
05/27/2022 11:39:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.23 on epoch=77
05/27/2022 11:39:06 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.18 on epoch=78
05/27/2022 11:39:13 - INFO - __main__ - Global step 2500 Train loss 0.19 Classification-F1 0.7881644287473275 on epoch=78
05/27/2022 11:39:16 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.19 on epoch=78
05/27/2022 11:39:18 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.15 on epoch=78
05/27/2022 11:39:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.13 on epoch=79
05/27/2022 11:39:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=79
05/27/2022 11:39:26 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.19 on epoch=79
05/27/2022 11:39:33 - INFO - __main__ - Global step 2550 Train loss 0.16 Classification-F1 0.7759273191598719 on epoch=79
05/27/2022 11:39:36 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.21 on epoch=79
05/27/2022 11:39:38 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.17 on epoch=80
05/27/2022 11:39:41 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.21 on epoch=80
05/27/2022 11:39:43 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.12 on epoch=80
05/27/2022 11:39:46 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.14 on epoch=81
05/27/2022 11:39:53 - INFO - __main__ - Global step 2600 Train loss 0.17 Classification-F1 0.8129174042616795 on epoch=81
05/27/2022 11:39:55 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.08 on epoch=81
05/27/2022 11:39:58 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.22 on epoch=81
05/27/2022 11:40:00 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.22 on epoch=82
05/27/2022 11:40:03 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=82
05/27/2022 11:40:05 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.20 on epoch=82
05/27/2022 11:40:12 - INFO - __main__ - Global step 2650 Train loss 0.17 Classification-F1 0.8185114949566895 on epoch=82
05/27/2022 11:40:14 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.13 on epoch=83
05/27/2022 11:40:17 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.11 on epoch=83
05/27/2022 11:40:19 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.16 on epoch=83
05/27/2022 11:40:22 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.18 on epoch=84
05/27/2022 11:40:25 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.24 on epoch=84
05/27/2022 11:40:31 - INFO - __main__ - Global step 2700 Train loss 0.16 Classification-F1 0.8124284339058621 on epoch=84
05/27/2022 11:40:34 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.16 on epoch=84
05/27/2022 11:40:37 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.10 on epoch=84
05/27/2022 11:40:39 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.12 on epoch=85
05/27/2022 11:40:42 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.20 on epoch=85
05/27/2022 11:40:44 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=85
05/27/2022 11:40:51 - INFO - __main__ - Global step 2750 Train loss 0.13 Classification-F1 0.7994666128304151 on epoch=85
05/27/2022 11:40:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.17 on epoch=86
05/27/2022 11:40:56 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.12 on epoch=86
05/27/2022 11:40:59 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.16 on epoch=86
05/27/2022 11:41:01 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=87
05/27/2022 11:41:04 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.19 on epoch=87
05/27/2022 11:41:11 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.8019824284016712 on epoch=87
05/27/2022 11:41:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.14 on epoch=87
05/27/2022 11:41:16 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.17 on epoch=88
05/27/2022 11:41:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.26 on epoch=88
05/27/2022 11:41:21 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.13 on epoch=88
05/27/2022 11:41:24 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.10 on epoch=89
05/27/2022 11:41:31 - INFO - __main__ - Global step 2850 Train loss 0.16 Classification-F1 0.7998482619630453 on epoch=89
05/27/2022 11:41:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.15 on epoch=89
05/27/2022 11:41:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=89
05/27/2022 11:41:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.15 on epoch=89
05/27/2022 11:41:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.15 on epoch=90
05/27/2022 11:41:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.20 on epoch=90
05/27/2022 11:41:51 - INFO - __main__ - Global step 2900 Train loss 0.15 Classification-F1 0.7765807311313317 on epoch=90
05/27/2022 11:41:53 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=90
05/27/2022 11:41:56 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.20 on epoch=91
05/27/2022 11:41:58 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.16 on epoch=91
05/27/2022 11:42:01 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=91
05/27/2022 11:42:03 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.13 on epoch=92
05/27/2022 11:42:10 - INFO - __main__ - Global step 2950 Train loss 0.14 Classification-F1 0.806368466169959 on epoch=92
05/27/2022 11:42:13 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.25 on epoch=92
05/27/2022 11:42:15 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=92
05/27/2022 11:42:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.08 on epoch=93
05/27/2022 11:42:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.13 on epoch=93
05/27/2022 11:42:23 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.14 on epoch=93
05/27/2022 11:42:24 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 11:42:24 - INFO - __main__ - Printing 3 examples
05/27/2022 11:42:24 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/27/2022 11:42:24 - INFO - __main__ - ['others']
05/27/2022 11:42:24 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/27/2022 11:42:24 - INFO - __main__ - ['others']
05/27/2022 11:42:24 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/27/2022 11:42:24 - INFO - __main__ - ['others']
05/27/2022 11:42:24 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:42:24 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:42:25 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 11:42:25 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 11:42:25 - INFO - __main__ - Printing 3 examples
05/27/2022 11:42:25 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/27/2022 11:42:25 - INFO - __main__ - ['others']
05/27/2022 11:42:25 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/27/2022 11:42:25 - INFO - __main__ - ['others']
05/27/2022 11:42:25 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/27/2022 11:42:25 - INFO - __main__ - ['others']
05/27/2022 11:42:25 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:42:25 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:42:26 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 11:42:30 - INFO - __main__ - Global step 3000 Train loss 0.13 Classification-F1 0.7593709480296473 on epoch=93
05/27/2022 11:42:30 - INFO - __main__ - save last model!
05/27/2022 11:42:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 11:42:30 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 11:42:30 - INFO - __main__ - Printing 3 examples
05/27/2022 11:42:30 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 11:42:30 - INFO - __main__ - ['others']
05/27/2022 11:42:30 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 11:42:30 - INFO - __main__ - ['others']
05/27/2022 11:42:30 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 11:42:30 - INFO - __main__ - ['others']
05/27/2022 11:42:30 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:42:32 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:42:37 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 11:42:44 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 11:42:44 - INFO - __main__ - task name: emo
05/27/2022 11:42:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 11:42:45 - INFO - __main__ - Starting training!
05/27/2022 11:43:51 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_87_0.3_8_predictions.txt
05/27/2022 11:43:51 - INFO - __main__ - Classification-F1 on test data: 0.4614
05/27/2022 11:43:52 - INFO - __main__ - prefix=emo_128_87, lr=0.3, bsz=8, dev_performance=0.8331828861642365, test_performance=0.4614235811638595
05/27/2022 11:43:52 - INFO - __main__ - Running ... prefix=emo_128_87, lr=0.2, bsz=8 ...
05/27/2022 11:43:53 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 11:43:53 - INFO - __main__ - Printing 3 examples
05/27/2022 11:43:53 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/27/2022 11:43:53 - INFO - __main__ - ['others']
05/27/2022 11:43:53 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/27/2022 11:43:53 - INFO - __main__ - ['others']
05/27/2022 11:43:53 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/27/2022 11:43:53 - INFO - __main__ - ['others']
05/27/2022 11:43:53 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:43:53 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:43:54 - INFO - __main__ - Loaded 512 examples from train data
05/27/2022 11:43:54 - INFO - __main__ - Start tokenizing ... 512 instances
05/27/2022 11:43:54 - INFO - __main__ - Printing 3 examples
05/27/2022 11:43:54 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/27/2022 11:43:54 - INFO - __main__ - ['others']
05/27/2022 11:43:54 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/27/2022 11:43:54 - INFO - __main__ - ['others']
05/27/2022 11:43:54 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/27/2022 11:43:54 - INFO - __main__ - ['others']
05/27/2022 11:43:54 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:43:54 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:43:54 - INFO - __main__ - Loaded 512 examples from dev data
05/27/2022 11:44:10 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 11:44:10 - INFO - __main__ - task name: emo
05/27/2022 11:44:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 11:44:11 - INFO - __main__ - Starting training!
05/27/2022 11:44:14 - INFO - __main__ - Step 10 Global step 10 Train loss 7.61 on epoch=0
05/27/2022 11:44:16 - INFO - __main__ - Step 20 Global step 20 Train loss 5.72 on epoch=0
05/27/2022 11:44:19 - INFO - __main__ - Step 30 Global step 30 Train loss 3.63 on epoch=0
05/27/2022 11:44:22 - INFO - __main__ - Step 40 Global step 40 Train loss 2.27 on epoch=1
05/27/2022 11:44:24 - INFO - __main__ - Step 50 Global step 50 Train loss 1.79 on epoch=1
05/27/2022 11:44:33 - INFO - __main__ - Global step 50 Train loss 4.21 Classification-F1 0.1280380448270912 on epoch=1
05/27/2022 11:44:33 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1280380448270912 on epoch=1, global_step=50
05/27/2022 11:44:35 - INFO - __main__ - Step 60 Global step 60 Train loss 1.32 on epoch=1
05/27/2022 11:44:38 - INFO - __main__ - Step 70 Global step 70 Train loss 1.30 on epoch=2
05/27/2022 11:44:40 - INFO - __main__ - Step 80 Global step 80 Train loss 1.16 on epoch=2
05/27/2022 11:44:43 - INFO - __main__ - Step 90 Global step 90 Train loss 1.06 on epoch=2
05/27/2022 11:44:45 - INFO - __main__ - Step 100 Global step 100 Train loss 1.16 on epoch=3
05/27/2022 11:44:52 - INFO - __main__ - Global step 100 Train loss 1.20 Classification-F1 0.11094948426342237 on epoch=3
05/27/2022 11:44:55 - INFO - __main__ - Step 110 Global step 110 Train loss 1.13 on epoch=3
05/27/2022 11:44:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.99 on epoch=3
05/27/2022 11:44:59 - INFO - __main__ - Step 130 Global step 130 Train loss 1.03 on epoch=4
05/27/2022 11:45:02 - INFO - __main__ - Step 140 Global step 140 Train loss 1.06 on epoch=4
05/27/2022 11:45:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.99 on epoch=4
05/27/2022 11:45:11 - INFO - __main__ - Global step 150 Train loss 1.04 Classification-F1 0.10340574955651138 on epoch=4
05/27/2022 11:45:14 - INFO - __main__ - Step 160 Global step 160 Train loss 1.18 on epoch=4
05/27/2022 11:45:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.89 on epoch=5
05/27/2022 11:45:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.89 on epoch=5
05/27/2022 11:45:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.91 on epoch=5
05/27/2022 11:45:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.93 on epoch=6
05/27/2022 11:45:31 - INFO - __main__ - Global step 200 Train loss 0.96 Classification-F1 0.2725651320813056 on epoch=6
05/27/2022 11:45:31 - INFO - __main__ - Saving model with best Classification-F1: 0.1280380448270912 -> 0.2725651320813056 on epoch=6, global_step=200
05/27/2022 11:45:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.98 on epoch=6
05/27/2022 11:45:36 - INFO - __main__ - Step 220 Global step 220 Train loss 1.03 on epoch=6
05/27/2022 11:45:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.96 on epoch=7
05/27/2022 11:45:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.94 on epoch=7
05/27/2022 11:45:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.96 on epoch=7
05/27/2022 11:45:50 - INFO - __main__ - Global step 250 Train loss 0.97 Classification-F1 0.18013805993892962 on epoch=7
05/27/2022 11:45:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.86 on epoch=8
05/27/2022 11:45:55 - INFO - __main__ - Step 270 Global step 270 Train loss 1.01 on epoch=8
05/27/2022 11:45:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.93 on epoch=8
05/27/2022 11:46:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.83 on epoch=9
05/27/2022 11:46:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.91 on epoch=9
05/27/2022 11:46:09 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.28583737312528407 on epoch=9
05/27/2022 11:46:09 - INFO - __main__ - Saving model with best Classification-F1: 0.2725651320813056 -> 0.28583737312528407 on epoch=9, global_step=300
05/27/2022 11:46:11 - INFO - __main__ - Step 310 Global step 310 Train loss 1.04 on epoch=9
05/27/2022 11:46:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.91 on epoch=9
05/27/2022 11:46:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.88 on epoch=10
05/27/2022 11:46:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.85 on epoch=10
05/27/2022 11:46:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.93 on epoch=10
05/27/2022 11:46:28 - INFO - __main__ - Global step 350 Train loss 0.92 Classification-F1 0.22699259883705072 on epoch=10
05/27/2022 11:46:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.79 on epoch=11
05/27/2022 11:46:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.89 on epoch=11
05/27/2022 11:46:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.89 on epoch=11
05/27/2022 11:46:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.92 on epoch=12
05/27/2022 11:46:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.89 on epoch=12
05/27/2022 11:46:47 - INFO - __main__ - Global step 400 Train loss 0.88 Classification-F1 0.3212280950614439 on epoch=12
05/27/2022 11:46:47 - INFO - __main__ - Saving model with best Classification-F1: 0.28583737312528407 -> 0.3212280950614439 on epoch=12, global_step=400
05/27/2022 11:46:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.81 on epoch=12
05/27/2022 11:46:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.85 on epoch=13
05/27/2022 11:46:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.95 on epoch=13
05/27/2022 11:46:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.89 on epoch=13
05/27/2022 11:47:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.83 on epoch=14
05/27/2022 11:47:07 - INFO - __main__ - Global step 450 Train loss 0.87 Classification-F1 0.1736549806304351 on epoch=14
05/27/2022 11:47:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.84 on epoch=14
05/27/2022 11:47:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.88 on epoch=14
05/27/2022 11:47:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.93 on epoch=14
05/27/2022 11:47:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.80 on epoch=15
05/27/2022 11:47:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.75 on epoch=15
05/27/2022 11:47:26 - INFO - __main__ - Global step 500 Train loss 0.84 Classification-F1 0.2095904019646224 on epoch=15
05/27/2022 11:47:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.84 on epoch=15
05/27/2022 11:47:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.87 on epoch=16
05/27/2022 11:47:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.88 on epoch=16
05/27/2022 11:47:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.85 on epoch=16
05/27/2022 11:47:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.86 on epoch=17
05/27/2022 11:47:45 - INFO - __main__ - Global step 550 Train loss 0.86 Classification-F1 0.43184490354723815 on epoch=17
05/27/2022 11:47:45 - INFO - __main__ - Saving model with best Classification-F1: 0.3212280950614439 -> 0.43184490354723815 on epoch=17, global_step=550
05/27/2022 11:47:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.77 on epoch=17
05/27/2022 11:47:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.80 on epoch=17
05/27/2022 11:47:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.82 on epoch=18
05/27/2022 11:47:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.76 on epoch=18
05/27/2022 11:47:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.84 on epoch=18
05/27/2022 11:48:04 - INFO - __main__ - Global step 600 Train loss 0.80 Classification-F1 0.3667403939434347 on epoch=18
05/27/2022 11:48:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.88 on epoch=19
05/27/2022 11:48:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.77 on epoch=19
05/27/2022 11:48:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.81 on epoch=19
05/27/2022 11:48:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.83 on epoch=19
05/27/2022 11:48:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.83 on epoch=20
05/27/2022 11:48:23 - INFO - __main__ - Global step 650 Train loss 0.82 Classification-F1 0.40654350599575434 on epoch=20
05/27/2022 11:48:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.70 on epoch=20
05/27/2022 11:48:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.86 on epoch=20
05/27/2022 11:48:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.79 on epoch=21
05/27/2022 11:48:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.74 on epoch=21
05/27/2022 11:48:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.83 on epoch=21
05/27/2022 11:48:43 - INFO - __main__ - Global step 700 Train loss 0.78 Classification-F1 0.4430655044717626 on epoch=21
05/27/2022 11:48:43 - INFO - __main__ - Saving model with best Classification-F1: 0.43184490354723815 -> 0.4430655044717626 on epoch=21, global_step=700
05/27/2022 11:48:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.78 on epoch=22
05/27/2022 11:48:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.70 on epoch=22
05/27/2022 11:48:50 - INFO - __main__ - Step 730 Global step 730 Train loss 0.68 on epoch=22
05/27/2022 11:48:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.70 on epoch=23
05/27/2022 11:48:55 - INFO - __main__ - Step 750 Global step 750 Train loss 0.80 on epoch=23
05/27/2022 11:49:02 - INFO - __main__ - Global step 750 Train loss 0.73 Classification-F1 0.49990964868144744 on epoch=23
05/27/2022 11:49:02 - INFO - __main__ - Saving model with best Classification-F1: 0.4430655044717626 -> 0.49990964868144744 on epoch=23, global_step=750
05/27/2022 11:49:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.71 on epoch=23
05/27/2022 11:49:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.73 on epoch=24
05/27/2022 11:49:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.76 on epoch=24
05/27/2022 11:49:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.70 on epoch=24
05/27/2022 11:49:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.72 on epoch=24
05/27/2022 11:49:21 - INFO - __main__ - Global step 800 Train loss 0.72 Classification-F1 0.4215880271867313 on epoch=24
05/27/2022 11:49:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.59 on epoch=25
05/27/2022 11:49:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.59 on epoch=25
05/27/2022 11:49:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.59 on epoch=25
05/27/2022 11:49:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.57 on epoch=26
05/27/2022 11:49:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.64 on epoch=26
05/27/2022 11:49:40 - INFO - __main__ - Global step 850 Train loss 0.59 Classification-F1 0.3759994475379091 on epoch=26
05/27/2022 11:49:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.67 on epoch=26
05/27/2022 11:49:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.65 on epoch=27
05/27/2022 11:49:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.59 on epoch=27
05/27/2022 11:49:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.59 on epoch=27
05/27/2022 11:49:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.59 on epoch=28
05/27/2022 11:49:59 - INFO - __main__ - Global step 900 Train loss 0.62 Classification-F1 0.584153475278984 on epoch=28
05/27/2022 11:49:59 - INFO - __main__ - Saving model with best Classification-F1: 0.49990964868144744 -> 0.584153475278984 on epoch=28, global_step=900
05/27/2022 11:50:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.70 on epoch=28
05/27/2022 11:50:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.64 on epoch=28
05/27/2022 11:50:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.61 on epoch=29
05/27/2022 11:50:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.64 on epoch=29
05/27/2022 11:50:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.62 on epoch=29
05/27/2022 11:50:19 - INFO - __main__ - Global step 950 Train loss 0.64 Classification-F1 0.5322194922194923 on epoch=29
05/27/2022 11:50:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.64 on epoch=29
05/27/2022 11:50:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.55 on epoch=30
05/27/2022 11:50:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.58 on epoch=30
05/27/2022 11:50:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.51 on epoch=30
05/27/2022 11:50:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.56 on epoch=31
05/27/2022 11:50:38 - INFO - __main__ - Global step 1000 Train loss 0.57 Classification-F1 0.6021331555397622 on epoch=31
05/27/2022 11:50:38 - INFO - __main__ - Saving model with best Classification-F1: 0.584153475278984 -> 0.6021331555397622 on epoch=31, global_step=1000
05/27/2022 11:50:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.50 on epoch=31
05/27/2022 11:50:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.57 on epoch=31
05/27/2022 11:50:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.61 on epoch=32
05/27/2022 11:50:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.66 on epoch=32
05/27/2022 11:50:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.52 on epoch=32
05/27/2022 11:50:57 - INFO - __main__ - Global step 1050 Train loss 0.57 Classification-F1 0.584605312280581 on epoch=32
05/27/2022 11:51:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.55 on epoch=33
05/27/2022 11:51:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.55 on epoch=33
05/27/2022 11:51:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.63 on epoch=33
05/27/2022 11:51:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.53 on epoch=34
05/27/2022 11:51:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.64 on epoch=34
05/27/2022 11:51:16 - INFO - __main__ - Global step 1100 Train loss 0.58 Classification-F1 0.6524424893588916 on epoch=34
05/27/2022 11:51:16 - INFO - __main__ - Saving model with best Classification-F1: 0.6021331555397622 -> 0.6524424893588916 on epoch=34, global_step=1100
05/27/2022 11:51:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.60 on epoch=34
05/27/2022 11:51:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.60 on epoch=34
05/27/2022 11:51:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.54 on epoch=35
05/27/2022 11:51:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.52 on epoch=35
05/27/2022 11:51:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.56 on epoch=35
05/27/2022 11:51:36 - INFO - __main__ - Global step 1150 Train loss 0.56 Classification-F1 0.6829231767361668 on epoch=35
05/27/2022 11:51:36 - INFO - __main__ - Saving model with best Classification-F1: 0.6524424893588916 -> 0.6829231767361668 on epoch=35, global_step=1150
05/27/2022 11:51:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.54 on epoch=36
05/27/2022 11:51:41 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.55 on epoch=36
05/27/2022 11:51:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.48 on epoch=36
05/27/2022 11:51:46 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.55 on epoch=37
05/27/2022 11:51:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.58 on epoch=37
05/27/2022 11:51:55 - INFO - __main__ - Global step 1200 Train loss 0.54 Classification-F1 0.7034593574084138 on epoch=37
05/27/2022 11:51:55 - INFO - __main__ - Saving model with best Classification-F1: 0.6829231767361668 -> 0.7034593574084138 on epoch=37, global_step=1200
05/27/2022 11:51:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=37
05/27/2022 11:52:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=38
05/27/2022 11:52:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.54 on epoch=38
05/27/2022 11:52:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.51 on epoch=38
05/27/2022 11:52:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.47 on epoch=39
05/27/2022 11:52:14 - INFO - __main__ - Global step 1250 Train loss 0.49 Classification-F1 0.6482897384305836 on epoch=39
05/27/2022 11:52:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.55 on epoch=39
05/27/2022 11:52:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=39
05/27/2022 11:52:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=39
05/27/2022 11:52:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=40
05/27/2022 11:52:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.48 on epoch=40
05/27/2022 11:52:33 - INFO - __main__ - Global step 1300 Train loss 0.45 Classification-F1 0.6592062337897622 on epoch=40
05/27/2022 11:52:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=40
05/27/2022 11:52:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=41
05/27/2022 11:52:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.47 on epoch=41
05/27/2022 11:52:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.53 on epoch=41
05/27/2022 11:52:46 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.55 on epoch=42
05/27/2022 11:52:53 - INFO - __main__ - Global step 1350 Train loss 0.49 Classification-F1 0.7125875554286348 on epoch=42
05/27/2022 11:52:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7034593574084138 -> 0.7125875554286348 on epoch=42, global_step=1350
05/27/2022 11:52:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=42
05/27/2022 11:52:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.47 on epoch=42
05/27/2022 11:53:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.34 on epoch=43
05/27/2022 11:53:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.58 on epoch=43
05/27/2022 11:53:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.46 on epoch=43
05/27/2022 11:53:12 - INFO - __main__ - Global step 1400 Train loss 0.46 Classification-F1 0.6935069190930379 on epoch=43
05/27/2022 11:53:14 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=44
05/27/2022 11:53:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.52 on epoch=44
05/27/2022 11:53:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.46 on epoch=44
05/27/2022 11:53:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=44
05/27/2022 11:53:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=45
05/27/2022 11:53:31 - INFO - __main__ - Global step 1450 Train loss 0.44 Classification-F1 0.7210565607860323 on epoch=45
05/27/2022 11:53:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7125875554286348 -> 0.7210565607860323 on epoch=45, global_step=1450
05/27/2022 11:53:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.46 on epoch=45
05/27/2022 11:53:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=45
05/27/2022 11:53:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.53 on epoch=46
05/27/2022 11:53:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=46
05/27/2022 11:53:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=46
05/27/2022 11:53:50 - INFO - __main__ - Global step 1500 Train loss 0.44 Classification-F1 0.7467279427044096 on epoch=46
05/27/2022 11:53:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7210565607860323 -> 0.7467279427044096 on epoch=46, global_step=1500
05/27/2022 11:53:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.48 on epoch=47
05/27/2022 11:53:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.46 on epoch=47
05/27/2022 11:53:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.42 on epoch=47
05/27/2022 11:54:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=48
05/27/2022 11:54:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.45 on epoch=48
05/27/2022 11:54:10 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.7660862217967668 on epoch=48
05/27/2022 11:54:10 - INFO - __main__ - Saving model with best Classification-F1: 0.7467279427044096 -> 0.7660862217967668 on epoch=48, global_step=1550
05/27/2022 11:54:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=48
05/27/2022 11:54:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.32 on epoch=49
05/27/2022 11:54:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.45 on epoch=49
05/27/2022 11:54:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.46 on epoch=49
05/27/2022 11:54:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.42 on epoch=49
05/27/2022 11:54:29 - INFO - __main__ - Global step 1600 Train loss 0.41 Classification-F1 0.6640743358685108 on epoch=49
05/27/2022 11:54:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=50
05/27/2022 11:54:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.42 on epoch=50
05/27/2022 11:54:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=50
05/27/2022 11:54:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.46 on epoch=51
05/27/2022 11:54:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=51
05/27/2022 11:54:48 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.6870888157894737 on epoch=51
05/27/2022 11:54:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.45 on epoch=51
05/27/2022 11:54:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.44 on epoch=52
05/27/2022 11:54:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.39 on epoch=52
05/27/2022 11:54:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=52
05/27/2022 11:55:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.41 on epoch=53
05/27/2022 11:55:07 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.6510257040909975 on epoch=53
05/27/2022 11:55:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=53
05/27/2022 11:55:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.45 on epoch=53
05/27/2022 11:55:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=54
05/27/2022 11:55:17 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=54
05/27/2022 11:55:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.40 on epoch=54
05/27/2022 11:55:27 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.7527051932182632 on epoch=54
05/27/2022 11:55:29 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=54
05/27/2022 11:55:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=55
05/27/2022 11:55:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.40 on epoch=55
05/27/2022 11:55:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=55
05/27/2022 11:55:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=56
05/27/2022 11:55:46 - INFO - __main__ - Global step 1800 Train loss 0.39 Classification-F1 0.7328964276751544 on epoch=56
05/27/2022 11:55:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=56
05/27/2022 11:55:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=56
05/27/2022 11:55:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=57
05/27/2022 11:55:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=57
05/27/2022 11:55:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.32 on epoch=57
05/27/2022 11:56:05 - INFO - __main__ - Global step 1850 Train loss 0.36 Classification-F1 0.7078472294062503 on epoch=57
05/27/2022 11:56:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.33 on epoch=58
05/27/2022 11:56:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.36 on epoch=58
05/27/2022 11:56:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.35 on epoch=58
05/27/2022 11:56:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.35 on epoch=59
05/27/2022 11:56:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=59
05/27/2022 11:56:24 - INFO - __main__ - Global step 1900 Train loss 0.36 Classification-F1 0.7596677679041354 on epoch=59
05/27/2022 11:56:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=59
05/27/2022 11:56:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.29 on epoch=59
05/27/2022 11:56:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=60
05/27/2022 11:56:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.29 on epoch=60
05/27/2022 11:56:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.31 on epoch=60
05/27/2022 11:56:44 - INFO - __main__ - Global step 1950 Train loss 0.32 Classification-F1 0.7668120739318514 on epoch=60
05/27/2022 11:56:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7660862217967668 -> 0.7668120739318514 on epoch=60, global_step=1950
05/27/2022 11:56:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=61
05/27/2022 11:56:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=61
05/27/2022 11:56:51 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.34 on epoch=61
05/27/2022 11:56:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=62
05/27/2022 11:56:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=62
05/27/2022 11:57:03 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.7418086725220697 on epoch=62
05/27/2022 11:57:05 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.33 on epoch=62
05/27/2022 11:57:08 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.27 on epoch=63
05/27/2022 11:57:10 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.48 on epoch=63
05/27/2022 11:57:13 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.34 on epoch=63
05/27/2022 11:57:15 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.32 on epoch=64
05/27/2022 11:57:22 - INFO - __main__ - Global step 2050 Train loss 0.35 Classification-F1 0.7139092329532591 on epoch=64
05/27/2022 11:57:25 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.44 on epoch=64
05/27/2022 11:57:27 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.28 on epoch=64
05/27/2022 11:57:30 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.34 on epoch=64
05/27/2022 11:57:32 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.30 on epoch=65
05/27/2022 11:57:35 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.30 on epoch=65
05/27/2022 11:57:41 - INFO - __main__ - Global step 2100 Train loss 0.33 Classification-F1 0.7507200609855718 on epoch=65
05/27/2022 11:57:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.35 on epoch=65
05/27/2022 11:57:46 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.32 on epoch=66
05/27/2022 11:57:49 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.29 on epoch=66
05/27/2022 11:57:51 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.34 on epoch=66
05/27/2022 11:57:54 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.32 on epoch=67
05/27/2022 11:58:01 - INFO - __main__ - Global step 2150 Train loss 0.32 Classification-F1 0.7600665736537838 on epoch=67
05/27/2022 11:58:03 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.29 on epoch=67
05/27/2022 11:58:06 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.27 on epoch=67
05/27/2022 11:58:08 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.26 on epoch=68
05/27/2022 11:58:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.33 on epoch=68
05/27/2022 11:58:13 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.30 on epoch=68
05/27/2022 11:58:20 - INFO - __main__ - Global step 2200 Train loss 0.29 Classification-F1 0.7738879118568753 on epoch=68
05/27/2022 11:58:20 - INFO - __main__ - Saving model with best Classification-F1: 0.7668120739318514 -> 0.7738879118568753 on epoch=68, global_step=2200
05/27/2022 11:58:22 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.27 on epoch=69
05/27/2022 11:58:25 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.27 on epoch=69
05/27/2022 11:58:27 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.28 on epoch=69
05/27/2022 11:58:30 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.32 on epoch=69
05/27/2022 11:58:32 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.35 on epoch=70
05/27/2022 11:58:39 - INFO - __main__ - Global step 2250 Train loss 0.30 Classification-F1 0.7585322172448044 on epoch=70
05/27/2022 11:58:42 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.39 on epoch=70
05/27/2022 11:58:44 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.25 on epoch=70
05/27/2022 11:58:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.23 on epoch=71
05/27/2022 11:58:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.24 on epoch=71
05/27/2022 11:58:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.36 on epoch=71
05/27/2022 11:58:58 - INFO - __main__ - Global step 2300 Train loss 0.29 Classification-F1 0.7654866133409788 on epoch=71
05/27/2022 11:59:01 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.24 on epoch=72
05/27/2022 11:59:03 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.36 on epoch=72
05/27/2022 11:59:06 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.31 on epoch=72
05/27/2022 11:59:08 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.31 on epoch=73
05/27/2022 11:59:11 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.38 on epoch=73
05/27/2022 11:59:18 - INFO - __main__ - Global step 2350 Train loss 0.32 Classification-F1 0.7472944315894003 on epoch=73
05/27/2022 11:59:20 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.31 on epoch=73
05/27/2022 11:59:23 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.26 on epoch=74
05/27/2022 11:59:25 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.28 on epoch=74
05/27/2022 11:59:28 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.26 on epoch=74
05/27/2022 11:59:30 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.27 on epoch=74
05/27/2022 11:59:37 - INFO - __main__ - Global step 2400 Train loss 0.28 Classification-F1 0.7380085827154993 on epoch=74
05/27/2022 11:59:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.40 on epoch=75
05/27/2022 11:59:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.34 on epoch=75
05/27/2022 11:59:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.20 on epoch=75
05/27/2022 11:59:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.39 on epoch=76
05/27/2022 11:59:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.28 on epoch=76
05/27/2022 11:59:56 - INFO - __main__ - Global step 2450 Train loss 0.32 Classification-F1 0.7618421000036915 on epoch=76
05/27/2022 11:59:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.23 on epoch=76
05/27/2022 12:00:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.25 on epoch=77
05/27/2022 12:00:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.33 on epoch=77
05/27/2022 12:00:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.23 on epoch=77
05/27/2022 12:00:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.21 on epoch=78
05/27/2022 12:00:16 - INFO - __main__ - Global step 2500 Train loss 0.25 Classification-F1 0.7272688240949206 on epoch=78
05/27/2022 12:00:18 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.25 on epoch=78
05/27/2022 12:00:21 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.23 on epoch=78
05/27/2022 12:00:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.26 on epoch=79
05/27/2022 12:00:26 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.28 on epoch=79
05/27/2022 12:00:28 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.17 on epoch=79
05/27/2022 12:00:35 - INFO - __main__ - Global step 2550 Train loss 0.24 Classification-F1 0.7854752214089822 on epoch=79
05/27/2022 12:00:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7738879118568753 -> 0.7854752214089822 on epoch=79, global_step=2550
05/27/2022 12:00:37 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.26 on epoch=79
05/27/2022 12:00:40 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.26 on epoch=80
05/27/2022 12:00:42 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.21 on epoch=80
05/27/2022 12:00:45 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.29 on epoch=80
05/27/2022 12:00:47 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.32 on epoch=81
05/27/2022 12:00:54 - INFO - __main__ - Global step 2600 Train loss 0.27 Classification-F1 0.7758383738643966 on epoch=81
05/27/2022 12:00:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.23 on epoch=81
05/27/2022 12:00:59 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.23 on epoch=81
05/27/2022 12:01:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.27 on epoch=82
05/27/2022 12:01:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.29 on epoch=82
05/27/2022 12:01:07 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.33 on epoch=82
05/27/2022 12:01:13 - INFO - __main__ - Global step 2650 Train loss 0.27 Classification-F1 0.7400726183206258 on epoch=82
05/27/2022 12:01:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.20 on epoch=83
05/27/2022 12:01:18 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.33 on epoch=83
05/27/2022 12:01:21 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.28 on epoch=83
05/27/2022 12:01:23 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.21 on epoch=84
05/27/2022 12:01:26 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.21 on epoch=84
05/27/2022 12:01:33 - INFO - __main__ - Global step 2700 Train loss 0.25 Classification-F1 0.7599622635929418 on epoch=84
05/27/2022 12:01:35 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.22 on epoch=84
05/27/2022 12:01:38 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.25 on epoch=84
05/27/2022 12:01:40 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.25 on epoch=85
05/27/2022 12:01:43 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.16 on epoch=85
05/27/2022 12:01:45 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.20 on epoch=85
05/27/2022 12:01:52 - INFO - __main__ - Global step 2750 Train loss 0.22 Classification-F1 0.7997874677152422 on epoch=85
05/27/2022 12:01:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7854752214089822 -> 0.7997874677152422 on epoch=85, global_step=2750
05/27/2022 12:01:55 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.28 on epoch=86
05/27/2022 12:01:57 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.23 on epoch=86
05/27/2022 12:02:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.19 on epoch=86
05/27/2022 12:02:02 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.22 on epoch=87
05/27/2022 12:02:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.18 on epoch=87
05/27/2022 12:02:12 - INFO - __main__ - Global step 2800 Train loss 0.22 Classification-F1 0.7839558043476036 on epoch=87
05/27/2022 12:02:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.24 on epoch=87
05/27/2022 12:02:17 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.26 on epoch=88
05/27/2022 12:02:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.24 on epoch=88
05/27/2022 12:02:22 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.22 on epoch=88
05/27/2022 12:02:24 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.22 on epoch=89
05/27/2022 12:02:31 - INFO - __main__ - Global step 2850 Train loss 0.24 Classification-F1 0.7348172753313347 on epoch=89
05/27/2022 12:02:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.25 on epoch=89
05/27/2022 12:02:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.23 on epoch=89
05/27/2022 12:02:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.19 on epoch=89
05/27/2022 12:02:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.21 on epoch=90
05/27/2022 12:02:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.27 on epoch=90
05/27/2022 12:02:50 - INFO - __main__ - Global step 2900 Train loss 0.23 Classification-F1 0.794189963738651 on epoch=90
05/27/2022 12:02:53 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.21 on epoch=90
05/27/2022 12:02:55 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.22 on epoch=91
05/27/2022 12:02:58 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.16 on epoch=91
05/27/2022 12:03:00 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.27 on epoch=91
05/27/2022 12:03:03 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.19 on epoch=92
05/27/2022 12:03:10 - INFO - __main__ - Global step 2950 Train loss 0.21 Classification-F1 0.7873616686260174 on epoch=92
05/27/2022 12:03:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.25 on epoch=92
05/27/2022 12:03:15 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.29 on epoch=92
05/27/2022 12:03:17 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.16 on epoch=93
05/27/2022 12:03:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.20 on epoch=93
05/27/2022 12:03:22 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.20 on epoch=93
05/27/2022 12:03:29 - INFO - __main__ - Global step 3000 Train loss 0.22 Classification-F1 0.7686414667912832 on epoch=93
05/27/2022 12:03:29 - INFO - __main__ - save last model!
05/27/2022 12:03:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 12:03:29 - INFO - __main__ - Start tokenizing ... 5509 instances
05/27/2022 12:03:29 - INFO - __main__ - Printing 3 examples
05/27/2022 12:03:29 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/27/2022 12:03:29 - INFO - __main__ - ['others']
05/27/2022 12:03:29 - INFO - __main__ -  [emo] what you like very little things ok
05/27/2022 12:03:29 - INFO - __main__ - ['others']
05/27/2022 12:03:29 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/27/2022 12:03:29 - INFO - __main__ - ['others']
05/27/2022 12:03:29 - INFO - __main__ - Tokenizing Input ...
05/27/2022 12:03:32 - INFO - __main__ - Tokenizing Output ...
05/27/2022 12:03:37 - INFO - __main__ - Loaded 5509 examples from test data
05/27/2022 12:04:51 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-emo/emo_128_87_0.2_8_predictions.txt
05/27/2022 12:04:51 - INFO - __main__ - Classification-F1 on test data: 0.4769
05/27/2022 12:04:52 - INFO - __main__ - prefix=emo_128_87, lr=0.2, bsz=8, dev_performance=0.7997874677152422, test_performance=0.47685515975682646
