05/29/2022 13:05:42 - INFO - __main__ - Namespace(task_dir='data_128/anli/', task_name='anli', identifier='T5-large-cls2cls-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls-down128shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
05/29/2022 13:05:42 - INFO - __main__ - models/T5-large-cls2cls-down128shot/singletask-anli
05/29/2022 13:05:42 - INFO - __main__ - Namespace(task_dir='data_128/anli/', task_name='anli', identifier='T5-large-cls2cls-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls-down128shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
05/29/2022 13:05:42 - INFO - __main__ - models/T5-large-cls2cls-down128shot/singletask-anli
05/29/2022 13:05:43 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/29/2022 13:05:43 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/29/2022 13:05:43 - INFO - __main__ - args.device: cuda:0
05/29/2022 13:05:43 - INFO - __main__ - Using 2 gpus
05/29/2022 13:05:43 - INFO - __main__ - args.device: cuda:1
05/29/2022 13:05:43 - INFO - __main__ - Using 2 gpus
05/29/2022 13:05:43 - INFO - __main__ - Fine-tuning the following samples: ['anli_128_100', 'anli_128_13', 'anli_128_21', 'anli_128_42', 'anli_128_87']
05/29/2022 13:05:43 - INFO - __main__ - Fine-tuning the following samples: ['anli_128_100', 'anli_128_13', 'anli_128_21', 'anli_128_42', 'anli_128_87']
05/29/2022 13:05:48 - INFO - __main__ - Running ... prefix=anli_128_100, lr=0.5, bsz=8 ...
05/29/2022 13:05:49 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 13:05:49 - INFO - __main__ - Printing 3 examples
05/29/2022 13:05:49 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/29/2022 13:05:49 - INFO - __main__ - ['neutral']
05/29/2022 13:05:49 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/29/2022 13:05:49 - INFO - __main__ - ['neutral']
05/29/2022 13:05:49 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/29/2022 13:05:49 - INFO - __main__ - ['neutral']
05/29/2022 13:05:49 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:05:49 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 13:05:49 - INFO - __main__ - Printing 3 examples
05/29/2022 13:05:49 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/29/2022 13:05:49 - INFO - __main__ - ['neutral']
05/29/2022 13:05:49 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/29/2022 13:05:49 - INFO - __main__ - ['neutral']
05/29/2022 13:05:49 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/29/2022 13:05:49 - INFO - __main__ - ['neutral']
05/29/2022 13:05:49 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:05:49 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:05:49 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:05:50 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 13:05:50 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 13:05:50 - INFO - __main__ - Printing 3 examples
05/29/2022 13:05:50 - INFO - __main__ -  [anli] premise: Nauset Regional High School an NEASC accredited high school located in North Eastham, Massachusetts. Nauset is inside the Cape Cod National Seashore, making it the only high school on the East Coast located within a National Park. The open campus is situated about a half-mile from Nauset Light. Nauset's colors are Black and Gold and the school's mascot is the Warrior. [SEP] hypothesis: Nauset Regional High School is a short walk from the Atlantic Ocean.
05/29/2022 13:05:50 - INFO - __main__ - ['neutral']
05/29/2022 13:05:50 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club is the oldest football club in England. 
05/29/2022 13:05:50 - INFO - __main__ - ['neutral']
05/29/2022 13:05:50 - INFO - __main__ -  [anli] premise: Brontë was a 2005 play by British playwright Polly Teale about the lives of the Brontë sisters, their brother Branwell and their father Patrick. It also featured characters from the sisters' novels such as Cathy and Heathcliff from "Wuthering Heights". [SEP] hypothesis: Bronte features characters from more than 3 other novels.
05/29/2022 13:05:50 - INFO - __main__ - ['neutral']
05/29/2022 13:05:50 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:05:50 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 13:05:50 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 13:05:50 - INFO - __main__ - Printing 3 examples
05/29/2022 13:05:50 - INFO - __main__ -  [anli] premise: Nauset Regional High School an NEASC accredited high school located in North Eastham, Massachusetts. Nauset is inside the Cape Cod National Seashore, making it the only high school on the East Coast located within a National Park. The open campus is situated about a half-mile from Nauset Light. Nauset's colors are Black and Gold and the school's mascot is the Warrior. [SEP] hypothesis: Nauset Regional High School is a short walk from the Atlantic Ocean.
05/29/2022 13:05:50 - INFO - __main__ - ['neutral']
05/29/2022 13:05:50 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club is the oldest football club in England. 
05/29/2022 13:05:50 - INFO - __main__ - ['neutral']
05/29/2022 13:05:50 - INFO - __main__ -  [anli] premise: Brontë was a 2005 play by British playwright Polly Teale about the lives of the Brontë sisters, their brother Branwell and their father Patrick. It also featured characters from the sisters' novels such as Cathy and Heathcliff from "Wuthering Heights". [SEP] hypothesis: Bronte features characters from more than 3 other novels.
05/29/2022 13:05:50 - INFO - __main__ - ['neutral']
05/29/2022 13:05:50 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:05:50 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:05:50 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:05:50 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 13:05:50 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 13:06:08 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 13:06:08 - INFO - __main__ - task name: anli
05/29/2022 13:06:08 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 13:06:08 - INFO - __main__ - task name: anli
05/29/2022 13:06:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 13:06:09 - INFO - __main__ - Starting training!
05/29/2022 13:06:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 13:06:09 - INFO - __main__ - Starting training!
05/29/2022 13:06:13 - INFO - __main__ - Step 10 Global step 10 Train loss 7.03 on epoch=0
05/29/2022 13:06:15 - INFO - __main__ - Step 20 Global step 20 Train loss 2.93 on epoch=0
05/29/2022 13:06:18 - INFO - __main__ - Step 30 Global step 30 Train loss 1.30 on epoch=1
05/29/2022 13:06:20 - INFO - __main__ - Step 40 Global step 40 Train loss 0.97 on epoch=1
05/29/2022 13:06:23 - INFO - __main__ - Step 50 Global step 50 Train loss 0.80 on epoch=2
05/29/2022 13:06:34 - INFO - __main__ - Global step 50 Train loss 2.61 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 13:06:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 13:06:37 - INFO - __main__ - Step 60 Global step 60 Train loss 0.74 on epoch=2
05/29/2022 13:06:39 - INFO - __main__ - Step 70 Global step 70 Train loss 0.67 on epoch=2
05/29/2022 13:06:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.65 on epoch=3
05/29/2022 13:06:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.66 on epoch=3
05/29/2022 13:06:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.69 on epoch=4
05/29/2022 13:06:58 - INFO - __main__ - Global step 100 Train loss 0.68 Classification-F1 0.17114127702362994 on epoch=4
05/29/2022 13:06:59 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.17114127702362994 on epoch=4, global_step=100
05/29/2022 13:07:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.59 on epoch=4
05/29/2022 13:07:04 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=4
05/29/2022 13:07:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.65 on epoch=5
05/29/2022 13:07:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=5
05/29/2022 13:07:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.58 on epoch=6
05/29/2022 13:07:23 - INFO - __main__ - Global step 150 Train loss 0.58 Classification-F1 0.2579570435994604 on epoch=6
05/29/2022 13:07:23 - INFO - __main__ - Saving model with best Classification-F1: 0.17114127702362994 -> 0.2579570435994604 on epoch=6, global_step=150
05/29/2022 13:07:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.63 on epoch=6
05/29/2022 13:07:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.55 on epoch=7
05/29/2022 13:07:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.60 on epoch=7
05/29/2022 13:07:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=7
05/29/2022 13:07:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.55 on epoch=8
05/29/2022 13:07:48 - INFO - __main__ - Global step 200 Train loss 0.57 Classification-F1 0.21436241173982365 on epoch=8
05/29/2022 13:07:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.63 on epoch=8
05/29/2022 13:07:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.60 on epoch=9
05/29/2022 13:07:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.53 on epoch=9
05/29/2022 13:07:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=9
05/29/2022 13:08:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.55 on epoch=10
05/29/2022 13:08:12 - INFO - __main__ - Global step 250 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=10
05/29/2022 13:08:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=10
05/29/2022 13:08:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.61 on epoch=11
05/29/2022 13:08:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.60 on epoch=11
05/29/2022 13:08:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=12
05/29/2022 13:08:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.59 on epoch=12
05/29/2022 13:08:36 - INFO - __main__ - Global step 300 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=12
05/29/2022 13:08:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=12
05/29/2022 13:08:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.53 on epoch=13
05/29/2022 13:08:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=13
05/29/2022 13:08:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=14
05/29/2022 13:08:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=14
05/29/2022 13:09:00 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.20982142857142852 on epoch=14
05/29/2022 13:09:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=14
05/29/2022 13:09:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.56 on epoch=15
05/29/2022 13:09:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.49 on epoch=15
05/29/2022 13:09:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.53 on epoch=16
05/29/2022 13:09:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.53 on epoch=16
05/29/2022 13:09:24 - INFO - __main__ - Global step 400 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=16
05/29/2022 13:09:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=17
05/29/2022 13:09:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.51 on epoch=17
05/29/2022 13:09:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=17
05/29/2022 13:09:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.59 on epoch=18
05/29/2022 13:09:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.53 on epoch=18
05/29/2022 13:09:48 - INFO - __main__ - Global step 450 Train loss 0.52 Classification-F1 0.1754553408096715 on epoch=18
05/29/2022 13:09:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.58 on epoch=19
05/29/2022 13:09:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.49 on epoch=19
05/29/2022 13:09:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=19
05/29/2022 13:09:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.50 on epoch=20
05/29/2022 13:10:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.54 on epoch=20
05/29/2022 13:10:12 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=20
05/29/2022 13:10:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.52 on epoch=21
05/29/2022 13:10:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.49 on epoch=21
05/29/2022 13:10:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=22
05/29/2022 13:10:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.57 on epoch=22
05/29/2022 13:10:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=22
05/29/2022 13:10:36 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.16568819308545338 on epoch=22
05/29/2022 13:10:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.51 on epoch=23
05/29/2022 13:10:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=23
05/29/2022 13:10:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=24
05/29/2022 13:10:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=24
05/29/2022 13:10:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.51 on epoch=24
05/29/2022 13:11:01 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.27562177227540113 on epoch=24
05/29/2022 13:11:01 - INFO - __main__ - Saving model with best Classification-F1: 0.2579570435994604 -> 0.27562177227540113 on epoch=24, global_step=600
05/29/2022 13:11:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.53 on epoch=25
05/29/2022 13:11:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.49 on epoch=25
05/29/2022 13:11:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=26
05/29/2022 13:11:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.56 on epoch=26
05/29/2022 13:11:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.48 on epoch=27
05/29/2022 13:11:25 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.18194444444444446 on epoch=27
05/29/2022 13:11:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=27
05/29/2022 13:11:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=27
05/29/2022 13:11:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=28
05/29/2022 13:11:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.50 on epoch=28
05/29/2022 13:11:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=29
05/29/2022 13:11:49 - INFO - __main__ - Global step 700 Train loss 0.49 Classification-F1 0.20499981154611466 on epoch=29
05/29/2022 13:11:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=29
05/29/2022 13:11:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=29
05/29/2022 13:11:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.50 on epoch=30
05/29/2022 13:12:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.51 on epoch=30
05/29/2022 13:12:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.46 on epoch=31
05/29/2022 13:12:14 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.16699282452707112 on epoch=31
05/29/2022 13:12:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.50 on epoch=31
05/29/2022 13:12:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=32
05/29/2022 13:12:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.50 on epoch=32
05/29/2022 13:12:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=32
05/29/2022 13:12:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=33
05/29/2022 13:12:38 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.16568819308545338 on epoch=33
05/29/2022 13:12:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=33
05/29/2022 13:12:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=34
05/29/2022 13:12:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=34
05/29/2022 13:12:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=34
05/29/2022 13:12:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=35
05/29/2022 13:13:02 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=35
05/29/2022 13:13:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=35
05/29/2022 13:13:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=36
05/29/2022 13:13:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.48 on epoch=36
05/29/2022 13:13:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=37
05/29/2022 13:13:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=37
05/29/2022 13:13:26 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.1637197118533071 on epoch=37
05/29/2022 13:13:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=37
05/29/2022 13:13:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.52 on epoch=38
05/29/2022 13:13:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=38
05/29/2022 13:13:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=39
05/29/2022 13:13:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=39
05/29/2022 13:13:51 - INFO - __main__ - Global step 950 Train loss 0.46 Classification-F1 0.16568819308545338 on epoch=39
05/29/2022 13:13:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=39
05/29/2022 13:13:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.47 on epoch=40
05/29/2022 13:13:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=40
05/29/2022 13:14:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.48 on epoch=41
05/29/2022 13:14:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=41
05/29/2022 13:14:15 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.19315673289183222 on epoch=41
05/29/2022 13:14:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.44 on epoch=42
05/29/2022 13:14:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.46 on epoch=42
05/29/2022 13:14:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.46 on epoch=42
05/29/2022 13:14:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=43
05/29/2022 13:14:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.48 on epoch=43
05/29/2022 13:14:40 - INFO - __main__ - Global step 1050 Train loss 0.46 Classification-F1 0.241986161986162 on epoch=43
05/29/2022 13:14:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.48 on epoch=44
05/29/2022 13:14:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=44
05/29/2022 13:14:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.49 on epoch=44
05/29/2022 13:14:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.49 on epoch=45
05/29/2022 13:14:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=45
05/29/2022 13:15:04 - INFO - __main__ - Global step 1100 Train loss 0.47 Classification-F1 0.26731614911034435 on epoch=45
05/29/2022 13:15:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=46
05/29/2022 13:15:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.48 on epoch=46
05/29/2022 13:15:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=47
05/29/2022 13:15:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=47
05/29/2022 13:15:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=47
05/29/2022 13:15:28 - INFO - __main__ - Global step 1150 Train loss 0.47 Classification-F1 0.17114127702362994 on epoch=47
05/29/2022 13:15:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=48
05/29/2022 13:15:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.47 on epoch=48
05/29/2022 13:15:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=49
05/29/2022 13:15:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=49
05/29/2022 13:15:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=49
05/29/2022 13:15:52 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.16568819308545338 on epoch=49
05/29/2022 13:15:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.49 on epoch=50
05/29/2022 13:15:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=50
05/29/2022 13:16:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=51
05/29/2022 13:16:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.48 on epoch=51
05/29/2022 13:16:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.47 on epoch=52
05/29/2022 13:16:17 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.2536536536536537 on epoch=52
05/29/2022 13:16:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=52
05/29/2022 13:16:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=52
05/29/2022 13:16:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.46 on epoch=53
05/29/2022 13:16:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.44 on epoch=53
05/29/2022 13:16:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.46 on epoch=54
05/29/2022 13:16:41 - INFO - __main__ - Global step 1300 Train loss 0.45 Classification-F1 0.2285845693917443 on epoch=54
05/29/2022 13:16:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.44 on epoch=54
05/29/2022 13:16:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.45 on epoch=54
05/29/2022 13:16:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.47 on epoch=55
05/29/2022 13:16:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=55
05/29/2022 13:16:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.47 on epoch=56
05/29/2022 13:17:06 - INFO - __main__ - Global step 1350 Train loss 0.45 Classification-F1 0.21200422548737155 on epoch=56
05/29/2022 13:17:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=56
05/29/2022 13:17:11 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=57
05/29/2022 13:17:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.50 on epoch=57
05/29/2022 13:17:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.44 on epoch=57
05/29/2022 13:17:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.50 on epoch=58
05/29/2022 13:17:30 - INFO - __main__ - Global step 1400 Train loss 0.45 Classification-F1 0.23064936883939982 on epoch=58
05/29/2022 13:17:33 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.47 on epoch=58
05/29/2022 13:17:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=59
05/29/2022 13:17:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.45 on epoch=59
05/29/2022 13:17:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.52 on epoch=59
05/29/2022 13:17:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.47 on epoch=60
05/29/2022 13:17:54 - INFO - __main__ - Global step 1450 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=60
05/29/2022 13:17:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=60
05/29/2022 13:18:00 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=61
05/29/2022 13:18:02 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.50 on epoch=61
05/29/2022 13:18:05 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.45 on epoch=62
05/29/2022 13:18:07 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.45 on epoch=62
05/29/2022 13:18:18 - INFO - __main__ - Global step 1500 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=62
05/29/2022 13:18:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.41 on epoch=62
05/29/2022 13:18:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.44 on epoch=63
05/29/2022 13:18:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.45 on epoch=63
05/29/2022 13:18:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.45 on epoch=64
05/29/2022 13:18:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.47 on epoch=64
05/29/2022 13:18:43 - INFO - __main__ - Global step 1550 Train loss 0.45 Classification-F1 0.2666535536323105 on epoch=64
05/29/2022 13:18:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.44 on epoch=64
05/29/2022 13:18:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.49 on epoch=65
05/29/2022 13:18:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=65
05/29/2022 13:18:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=66
05/29/2022 13:18:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.49 on epoch=66
05/29/2022 13:19:08 - INFO - __main__ - Global step 1600 Train loss 0.46 Classification-F1 0.19265097379851479 on epoch=66
05/29/2022 13:19:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.44 on epoch=67
05/29/2022 13:19:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=67
05/29/2022 13:19:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=67
05/29/2022 13:19:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.46 on epoch=68
05/29/2022 13:19:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.44 on epoch=68
05/29/2022 13:19:32 - INFO - __main__ - Global step 1650 Train loss 0.44 Classification-F1 0.1757055360294227 on epoch=68
05/29/2022 13:19:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.47 on epoch=69
05/29/2022 13:19:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.43 on epoch=69
05/29/2022 13:19:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.46 on epoch=69
05/29/2022 13:19:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.47 on epoch=70
05/29/2022 13:19:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.41 on epoch=70
05/29/2022 13:19:57 - INFO - __main__ - Global step 1700 Train loss 0.45 Classification-F1 0.17275422888459993 on epoch=70
05/29/2022 13:19:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.45 on epoch=71
05/29/2022 13:20:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=71
05/29/2022 13:20:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=72
05/29/2022 13:20:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.44 on epoch=72
05/29/2022 13:20:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.45 on epoch=72
05/29/2022 13:20:21 - INFO - __main__ - Global step 1750 Train loss 0.44 Classification-F1 0.20005399568034557 on epoch=72
05/29/2022 13:20:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.48 on epoch=73
05/29/2022 13:20:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.47 on epoch=73
05/29/2022 13:20:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.45 on epoch=74
05/29/2022 13:20:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.45 on epoch=74
05/29/2022 13:20:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.49 on epoch=74
05/29/2022 13:20:45 - INFO - __main__ - Global step 1800 Train loss 0.47 Classification-F1 0.24371859296482412 on epoch=74
05/29/2022 13:20:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.43 on epoch=75
05/29/2022 13:20:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=75
05/29/2022 13:20:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.40 on epoch=76
05/29/2022 13:20:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.50 on epoch=76
05/29/2022 13:20:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=77
05/29/2022 13:21:10 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.23872484035085662 on epoch=77
05/29/2022 13:21:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.49 on epoch=77
05/29/2022 13:21:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.44 on epoch=77
05/29/2022 13:21:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=78
05/29/2022 13:21:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.41 on epoch=78
05/29/2022 13:21:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.46 on epoch=79
05/29/2022 13:21:34 - INFO - __main__ - Global step 1900 Train loss 0.45 Classification-F1 0.16601307189542483 on epoch=79
05/29/2022 13:21:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.45 on epoch=79
05/29/2022 13:21:39 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.44 on epoch=79
05/29/2022 13:21:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.53 on epoch=80
05/29/2022 13:21:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.43 on epoch=80
05/29/2022 13:21:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=81
05/29/2022 13:21:59 - INFO - __main__ - Global step 1950 Train loss 0.45 Classification-F1 0.21260176717984333 on epoch=81
05/29/2022 13:22:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.46 on epoch=81
05/29/2022 13:22:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.41 on epoch=82
05/29/2022 13:22:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.50 on epoch=82
05/29/2022 13:22:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.44 on epoch=82
05/29/2022 13:22:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.43 on epoch=83
05/29/2022 13:22:23 - INFO - __main__ - Global step 2000 Train loss 0.45 Classification-F1 0.22789685388393213 on epoch=83
05/29/2022 13:22:26 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.47 on epoch=83
05/29/2022 13:22:29 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.46 on epoch=84
05/29/2022 13:22:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.46 on epoch=84
05/29/2022 13:22:34 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.48 on epoch=84
05/29/2022 13:22:36 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.43 on epoch=85
05/29/2022 13:22:48 - INFO - __main__ - Global step 2050 Train loss 0.46 Classification-F1 0.168310322156476 on epoch=85
05/29/2022 13:22:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.43 on epoch=85
05/29/2022 13:22:53 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.44 on epoch=86
05/29/2022 13:22:55 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.47 on epoch=86
05/29/2022 13:22:58 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.43 on epoch=87
05/29/2022 13:23:01 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.45 on epoch=87
05/29/2022 13:23:12 - INFO - __main__ - Global step 2100 Train loss 0.44 Classification-F1 0.18586425079091173 on epoch=87
05/29/2022 13:23:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.44 on epoch=87
05/29/2022 13:23:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.43 on epoch=88
05/29/2022 13:23:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.44 on epoch=88
05/29/2022 13:23:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.44 on epoch=89
05/29/2022 13:23:25 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.43 on epoch=89
05/29/2022 13:23:36 - INFO - __main__ - Global step 2150 Train loss 0.43 Classification-F1 0.21303030303030304 on epoch=89
05/29/2022 13:23:39 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.48 on epoch=89
05/29/2022 13:23:41 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.43 on epoch=90
05/29/2022 13:23:44 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.43 on epoch=90
05/29/2022 13:23:47 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.42 on epoch=91
05/29/2022 13:23:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.46 on epoch=91
05/29/2022 13:24:01 - INFO - __main__ - Global step 2200 Train loss 0.44 Classification-F1 0.21233316865053534 on epoch=91
05/29/2022 13:24:03 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.43 on epoch=92
05/29/2022 13:24:06 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.46 on epoch=92
05/29/2022 13:24:09 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.42 on epoch=92
05/29/2022 13:24:11 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.43 on epoch=93
05/29/2022 13:24:14 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.46 on epoch=93
05/29/2022 13:24:25 - INFO - __main__ - Global step 2250 Train loss 0.44 Classification-F1 0.20075757575757577 on epoch=93
05/29/2022 13:24:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.43 on epoch=94
05/29/2022 13:24:31 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.43 on epoch=94
05/29/2022 13:24:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.45 on epoch=94
05/29/2022 13:24:36 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.46 on epoch=95
05/29/2022 13:24:39 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.43 on epoch=95
05/29/2022 13:24:50 - INFO - __main__ - Global step 2300 Train loss 0.44 Classification-F1 0.21409897508118672 on epoch=95
05/29/2022 13:24:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.39 on epoch=96
05/29/2022 13:24:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.46 on epoch=96
05/29/2022 13:24:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.41 on epoch=97
05/29/2022 13:25:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.48 on epoch=97
05/29/2022 13:25:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.42 on epoch=97
05/29/2022 13:25:15 - INFO - __main__ - Global step 2350 Train loss 0.43 Classification-F1 0.298049082294464 on epoch=97
05/29/2022 13:25:15 - INFO - __main__ - Saving model with best Classification-F1: 0.27562177227540113 -> 0.298049082294464 on epoch=97, global_step=2350
05/29/2022 13:25:18 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.42 on epoch=98
05/29/2022 13:25:20 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.45 on epoch=98
05/29/2022 13:25:23 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.45 on epoch=99
05/29/2022 13:25:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.42 on epoch=99
05/29/2022 13:25:28 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.43 on epoch=99
05/29/2022 13:25:39 - INFO - __main__ - Global step 2400 Train loss 0.43 Classification-F1 0.16699282452707112 on epoch=99
05/29/2022 13:25:42 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.48 on epoch=100
05/29/2022 13:25:44 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.43 on epoch=100
05/29/2022 13:25:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.43 on epoch=101
05/29/2022 13:25:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.47 on epoch=101
05/29/2022 13:25:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.42 on epoch=102
05/29/2022 13:26:04 - INFO - __main__ - Global step 2450 Train loss 0.44 Classification-F1 0.1855186509475055 on epoch=102
05/29/2022 13:26:06 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.44 on epoch=102
05/29/2022 13:26:09 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.46 on epoch=102
05/29/2022 13:26:12 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.42 on epoch=103
05/29/2022 13:26:14 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.43 on epoch=103
05/29/2022 13:26:17 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.45 on epoch=104
05/29/2022 13:26:28 - INFO - __main__ - Global step 2500 Train loss 0.44 Classification-F1 0.2634920634920635 on epoch=104
05/29/2022 13:26:31 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.47 on epoch=104
05/29/2022 13:26:34 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.44 on epoch=104
05/29/2022 13:26:36 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.42 on epoch=105
05/29/2022 13:26:39 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.43 on epoch=105
05/29/2022 13:26:41 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.41 on epoch=106
05/29/2022 13:26:54 - INFO - __main__ - Global step 2550 Train loss 0.43 Classification-F1 0.2319242798820773 on epoch=106
05/29/2022 13:26:56 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.42 on epoch=106
05/29/2022 13:26:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.37 on epoch=107
05/29/2022 13:27:01 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.51 on epoch=107
05/29/2022 13:27:04 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.43 on epoch=107
05/29/2022 13:27:07 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.41 on epoch=108
05/29/2022 13:27:18 - INFO - __main__ - Global step 2600 Train loss 0.43 Classification-F1 0.2616183995357915 on epoch=108
05/29/2022 13:27:21 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.42 on epoch=108
05/29/2022 13:27:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.42 on epoch=109
05/29/2022 13:27:26 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.44 on epoch=109
05/29/2022 13:27:28 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.40 on epoch=109
05/29/2022 13:27:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.42 on epoch=110
05/29/2022 13:27:42 - INFO - __main__ - Global step 2650 Train loss 0.42 Classification-F1 0.1673254281949934 on epoch=110
05/29/2022 13:27:45 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.42 on epoch=110
05/29/2022 13:27:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.41 on epoch=111
05/29/2022 13:27:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.45 on epoch=111
05/29/2022 13:27:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.39 on epoch=112
05/29/2022 13:27:55 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.46 on epoch=112
05/29/2022 13:28:07 - INFO - __main__ - Global step 2700 Train loss 0.42 Classification-F1 0.2903614841240246 on epoch=112
05/29/2022 13:28:10 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.42 on epoch=112
05/29/2022 13:28:12 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.45 on epoch=113
05/29/2022 13:28:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.43 on epoch=113
05/29/2022 13:28:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.43 on epoch=114
05/29/2022 13:28:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.41 on epoch=114
05/29/2022 13:28:31 - INFO - __main__ - Global step 2750 Train loss 0.43 Classification-F1 0.20635888605899724 on epoch=114
05/29/2022 13:28:34 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.42 on epoch=114
05/29/2022 13:28:37 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.43 on epoch=115
05/29/2022 13:28:39 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.45 on epoch=115
05/29/2022 13:28:42 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.46 on epoch=116
05/29/2022 13:28:45 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.45 on epoch=116
05/29/2022 13:28:56 - INFO - __main__ - Global step 2800 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=116
05/29/2022 13:28:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.42 on epoch=117
05/29/2022 13:29:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.47 on epoch=117
05/29/2022 13:29:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.41 on epoch=117
05/29/2022 13:29:07 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.47 on epoch=118
05/29/2022 13:29:09 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.47 on epoch=118
05/29/2022 13:29:21 - INFO - __main__ - Global step 2850 Train loss 0.45 Classification-F1 0.1721607831834019 on epoch=118
05/29/2022 13:29:24 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.47 on epoch=119
05/29/2022 13:29:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.45 on epoch=119
05/29/2022 13:29:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.45 on epoch=119
05/29/2022 13:29:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.43 on epoch=120
05/29/2022 13:29:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.45 on epoch=120
05/29/2022 13:29:46 - INFO - __main__ - Global step 2900 Train loss 0.45 Classification-F1 0.321951962239732 on epoch=120
05/29/2022 13:29:46 - INFO - __main__ - Saving model with best Classification-F1: 0.298049082294464 -> 0.321951962239732 on epoch=120, global_step=2900
05/29/2022 13:29:48 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.47 on epoch=121
05/29/2022 13:29:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.44 on epoch=121
05/29/2022 13:29:54 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.42 on epoch=122
05/29/2022 13:29:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.52 on epoch=122
05/29/2022 13:29:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.39 on epoch=122
05/29/2022 13:30:10 - INFO - __main__ - Global step 2950 Train loss 0.45 Classification-F1 0.29381534644692536 on epoch=122
05/29/2022 13:30:13 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.46 on epoch=123
05/29/2022 13:30:15 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.47 on epoch=123
05/29/2022 13:30:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.46 on epoch=124
05/29/2022 13:30:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.47 on epoch=124
05/29/2022 13:30:23 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.42 on epoch=124
05/29/2022 13:30:24 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 13:30:24 - INFO - __main__ - Printing 3 examples
05/29/2022 13:30:24 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/29/2022 13:30:24 - INFO - __main__ - ['neutral']
05/29/2022 13:30:24 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/29/2022 13:30:24 - INFO - __main__ - ['neutral']
05/29/2022 13:30:24 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/29/2022 13:30:24 - INFO - __main__ - ['neutral']
05/29/2022 13:30:24 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:30:25 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:30:25 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 13:30:25 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 13:30:25 - INFO - __main__ - Printing 3 examples
05/29/2022 13:30:25 - INFO - __main__ -  [anli] premise: Nauset Regional High School an NEASC accredited high school located in North Eastham, Massachusetts. Nauset is inside the Cape Cod National Seashore, making it the only high school on the East Coast located within a National Park. The open campus is situated about a half-mile from Nauset Light. Nauset's colors are Black and Gold and the school's mascot is the Warrior. [SEP] hypothesis: Nauset Regional High School is a short walk from the Atlantic Ocean.
05/29/2022 13:30:25 - INFO - __main__ - ['neutral']
05/29/2022 13:30:25 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club is the oldest football club in England. 
05/29/2022 13:30:25 - INFO - __main__ - ['neutral']
05/29/2022 13:30:25 - INFO - __main__ -  [anli] premise: Brontë was a 2005 play by British playwright Polly Teale about the lives of the Brontë sisters, their brother Branwell and their father Patrick. It also featured characters from the sisters' novels such as Cathy and Heathcliff from "Wuthering Heights". [SEP] hypothesis: Bronte features characters from more than 3 other novels.
05/29/2022 13:30:25 - INFO - __main__ - ['neutral']
05/29/2022 13:30:25 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:30:25 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:30:26 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 13:30:34 - INFO - __main__ - Global step 3000 Train loss 0.46 Classification-F1 0.17281694359411726 on epoch=124
05/29/2022 13:30:34 - INFO - __main__ - save last model!
05/29/2022 13:30:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 13:30:34 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 13:30:34 - INFO - __main__ - Printing 3 examples
05/29/2022 13:30:34 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 13:30:34 - INFO - __main__ - ['contradiction']
05/29/2022 13:30:34 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 13:30:34 - INFO - __main__ - ['entailment']
05/29/2022 13:30:34 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 13:30:34 - INFO - __main__ - ['contradiction']
05/29/2022 13:30:34 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:30:35 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:30:36 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 13:30:41 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 13:30:41 - INFO - __main__ - task name: anli
05/29/2022 13:30:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 13:30:42 - INFO - __main__ - Starting training!
05/29/2022 13:31:05 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_100_0.5_8_predictions.txt
05/29/2022 13:31:05 - INFO - __main__ - Classification-F1 on test data: 0.1752
05/29/2022 13:31:06 - INFO - __main__ - prefix=anli_128_100, lr=0.5, bsz=8, dev_performance=0.321951962239732, test_performance=0.17521695387250447
05/29/2022 13:31:06 - INFO - __main__ - Running ... prefix=anli_128_100, lr=0.4, bsz=8 ...
05/29/2022 13:31:07 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 13:31:07 - INFO - __main__ - Printing 3 examples
05/29/2022 13:31:07 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/29/2022 13:31:07 - INFO - __main__ - ['neutral']
05/29/2022 13:31:07 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/29/2022 13:31:07 - INFO - __main__ - ['neutral']
05/29/2022 13:31:07 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/29/2022 13:31:07 - INFO - __main__ - ['neutral']
05/29/2022 13:31:07 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:31:07 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:31:07 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 13:31:07 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 13:31:07 - INFO - __main__ - Printing 3 examples
05/29/2022 13:31:07 - INFO - __main__ -  [anli] premise: Nauset Regional High School an NEASC accredited high school located in North Eastham, Massachusetts. Nauset is inside the Cape Cod National Seashore, making it the only high school on the East Coast located within a National Park. The open campus is situated about a half-mile from Nauset Light. Nauset's colors are Black and Gold and the school's mascot is the Warrior. [SEP] hypothesis: Nauset Regional High School is a short walk from the Atlantic Ocean.
05/29/2022 13:31:07 - INFO - __main__ - ['neutral']
05/29/2022 13:31:07 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club is the oldest football club in England. 
05/29/2022 13:31:07 - INFO - __main__ - ['neutral']
05/29/2022 13:31:07 - INFO - __main__ -  [anli] premise: Brontë was a 2005 play by British playwright Polly Teale about the lives of the Brontë sisters, their brother Branwell and their father Patrick. It also featured characters from the sisters' novels such as Cathy and Heathcliff from "Wuthering Heights". [SEP] hypothesis: Bronte features characters from more than 3 other novels.
05/29/2022 13:31:07 - INFO - __main__ - ['neutral']
05/29/2022 13:31:07 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:31:07 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:31:08 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 13:31:24 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 13:31:24 - INFO - __main__ - task name: anli
05/29/2022 13:31:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 13:31:25 - INFO - __main__ - Starting training!
05/29/2022 13:31:28 - INFO - __main__ - Step 10 Global step 10 Train loss 6.47 on epoch=0
05/29/2022 13:31:31 - INFO - __main__ - Step 20 Global step 20 Train loss 2.55 on epoch=0
05/29/2022 13:31:33 - INFO - __main__ - Step 30 Global step 30 Train loss 1.35 on epoch=1
05/29/2022 13:31:36 - INFO - __main__ - Step 40 Global step 40 Train loss 0.98 on epoch=1
05/29/2022 13:31:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.73 on epoch=2
05/29/2022 13:31:47 - INFO - __main__ - Global step 50 Train loss 2.41 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 13:31:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 13:31:50 - INFO - __main__ - Step 60 Global step 60 Train loss 1.51 on epoch=2
05/29/2022 13:31:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.73 on epoch=2
05/29/2022 13:31:55 - INFO - __main__ - Step 80 Global step 80 Train loss 0.66 on epoch=3
05/29/2022 13:31:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.67 on epoch=3
05/29/2022 13:32:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.72 on epoch=4
05/29/2022 13:32:08 - INFO - __main__ - Global step 100 Train loss 0.86 Classification-F1 0.25523588569219025 on epoch=4
05/29/2022 13:32:08 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.25523588569219025 on epoch=4, global_step=100
05/29/2022 13:32:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.63 on epoch=4
05/29/2022 13:32:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=4
05/29/2022 13:32:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.64 on epoch=5
05/29/2022 13:32:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.56 on epoch=5
05/29/2022 13:32:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.60 on epoch=6
05/29/2022 13:32:30 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=6
05/29/2022 13:32:32 - INFO - __main__ - Step 160 Global step 160 Train loss 0.62 on epoch=6
05/29/2022 13:32:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=7
05/29/2022 13:32:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.66 on epoch=7
05/29/2022 13:32:40 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=7
05/29/2022 13:32:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.63 on epoch=8
05/29/2022 13:32:51 - INFO - __main__ - Global step 200 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=8
05/29/2022 13:32:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.55 on epoch=8
05/29/2022 13:32:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=9
05/29/2022 13:32:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=9
05/29/2022 13:33:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.57 on epoch=9
05/29/2022 13:33:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=10
05/29/2022 13:33:12 - INFO - __main__ - Global step 250 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=10
05/29/2022 13:33:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.53 on epoch=10
05/29/2022 13:33:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.53 on epoch=11
05/29/2022 13:33:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.61 on epoch=11
05/29/2022 13:33:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=12
05/29/2022 13:33:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.58 on epoch=12
05/29/2022 13:33:33 - INFO - __main__ - Global step 300 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=12
05/29/2022 13:33:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=12
05/29/2022 13:33:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.61 on epoch=13
05/29/2022 13:33:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.59 on epoch=13
05/29/2022 13:33:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.55 on epoch=14
05/29/2022 13:33:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=14
05/29/2022 13:33:54 - INFO - __main__ - Global step 350 Train loss 0.55 Classification-F1 0.2578882127563669 on epoch=14
05/29/2022 13:33:54 - INFO - __main__ - Saving model with best Classification-F1: 0.25523588569219025 -> 0.2578882127563669 on epoch=14, global_step=350
05/29/2022 13:33:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.55 on epoch=14
05/29/2022 13:33:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.59 on epoch=15
05/29/2022 13:34:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.53 on epoch=15
05/29/2022 13:34:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=16
05/29/2022 13:34:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.52 on epoch=16
05/29/2022 13:34:15 - INFO - __main__ - Global step 400 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=16
05/29/2022 13:34:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.54 on epoch=17
05/29/2022 13:34:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.51 on epoch=17
05/29/2022 13:34:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.51 on epoch=17
05/29/2022 13:34:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.55 on epoch=18
05/29/2022 13:34:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=18
05/29/2022 13:34:36 - INFO - __main__ - Global step 450 Train loss 0.52 Classification-F1 0.2555555555555556 on epoch=18
05/29/2022 13:34:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.53 on epoch=19
05/29/2022 13:34:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.53 on epoch=19
05/29/2022 13:34:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.53 on epoch=19
05/29/2022 13:34:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=20
05/29/2022 13:34:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.50 on epoch=20
05/29/2022 13:34:57 - INFO - __main__ - Global step 500 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=20
05/29/2022 13:34:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=21
05/29/2022 13:35:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.54 on epoch=21
05/29/2022 13:35:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=22
05/29/2022 13:35:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.52 on epoch=22
05/29/2022 13:35:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.49 on epoch=22
05/29/2022 13:35:21 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=22
05/29/2022 13:35:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.55 on epoch=23
05/29/2022 13:35:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.51 on epoch=23
05/29/2022 13:35:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=24
05/29/2022 13:35:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=24
05/29/2022 13:35:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=24
05/29/2022 13:35:42 - INFO - __main__ - Global step 600 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=24
05/29/2022 13:35:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.53 on epoch=25
05/29/2022 13:35:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=25
05/29/2022 13:35:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.49 on epoch=26
05/29/2022 13:35:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.57 on epoch=26
05/29/2022 13:35:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=27
05/29/2022 13:36:02 - INFO - __main__ - Global step 650 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=27
05/29/2022 13:36:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.54 on epoch=27
05/29/2022 13:36:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=27
05/29/2022 13:36:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=28
05/29/2022 13:36:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=28
05/29/2022 13:36:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=29
05/29/2022 13:36:23 - INFO - __main__ - Global step 700 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=29
05/29/2022 13:36:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=29
05/29/2022 13:36:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.49 on epoch=29
05/29/2022 13:36:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.52 on epoch=30
05/29/2022 13:36:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=30
05/29/2022 13:36:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.50 on epoch=31
05/29/2022 13:36:44 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=31
05/29/2022 13:36:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=31
05/29/2022 13:36:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.49 on epoch=32
05/29/2022 13:36:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=32
05/29/2022 13:36:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=32
05/29/2022 13:36:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=33
05/29/2022 13:37:08 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=33
05/29/2022 13:37:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=33
05/29/2022 13:37:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=34
05/29/2022 13:37:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=34
05/29/2022 13:37:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.54 on epoch=34
05/29/2022 13:37:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=35
05/29/2022 13:37:29 - INFO - __main__ - Global step 850 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=35
05/29/2022 13:37:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.51 on epoch=35
05/29/2022 13:37:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.49 on epoch=36
05/29/2022 13:37:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=36
05/29/2022 13:37:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=37
05/29/2022 13:37:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.51 on epoch=37
05/29/2022 13:37:53 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.23440794829294131 on epoch=37
05/29/2022 13:37:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.41 on epoch=37
05/29/2022 13:37:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=38
05/29/2022 13:38:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.48 on epoch=38
05/29/2022 13:38:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.54 on epoch=39
05/29/2022 13:38:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.54 on epoch=39
05/29/2022 13:38:15 - INFO - __main__ - Global step 950 Train loss 0.50 Classification-F1 0.17753330888644422 on epoch=39
05/29/2022 13:38:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=39
05/29/2022 13:38:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.51 on epoch=40
05/29/2022 13:38:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.42 on epoch=40
05/29/2022 13:38:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=41
05/29/2022 13:38:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.50 on epoch=41
05/29/2022 13:38:35 - INFO - __main__ - Global step 1000 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=41
05/29/2022 13:38:38 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=42
05/29/2022 13:38:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.52 on epoch=42
05/29/2022 13:38:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.47 on epoch=42
05/29/2022 13:38:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.46 on epoch=43
05/29/2022 13:38:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.44 on epoch=43
05/29/2022 13:38:56 - INFO - __main__ - Global step 1050 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=43
05/29/2022 13:38:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.48 on epoch=44
05/29/2022 13:39:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=44
05/29/2022 13:39:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=44
05/29/2022 13:39:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=45
05/29/2022 13:39:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=45
05/29/2022 13:39:20 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.16699282452707112 on epoch=45
05/29/2022 13:39:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.48 on epoch=46
05/29/2022 13:39:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.48 on epoch=46
05/29/2022 13:39:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=47
05/29/2022 13:39:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.46 on epoch=47
05/29/2022 13:39:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=47
05/29/2022 13:39:45 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.3127707910250668 on epoch=47
05/29/2022 13:39:45 - INFO - __main__ - Saving model with best Classification-F1: 0.2578882127563669 -> 0.3127707910250668 on epoch=47, global_step=1150
05/29/2022 13:39:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.50 on epoch=48
05/29/2022 13:39:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.50 on epoch=48
05/29/2022 13:39:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=49
05/29/2022 13:39:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.48 on epoch=49
05/29/2022 13:39:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.47 on epoch=49
05/29/2022 13:40:09 - INFO - __main__ - Global step 1200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=49
05/29/2022 13:40:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.51 on epoch=50
05/29/2022 13:40:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.41 on epoch=50
05/29/2022 13:40:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.50 on epoch=51
05/29/2022 13:40:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.49 on epoch=51
05/29/2022 13:40:22 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=52
05/29/2022 13:40:34 - INFO - __main__ - Global step 1250 Train loss 0.46 Classification-F1 0.267989417989418 on epoch=52
05/29/2022 13:40:36 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.50 on epoch=52
05/29/2022 13:40:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=52
05/29/2022 13:40:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.51 on epoch=53
05/29/2022 13:40:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=53
05/29/2022 13:40:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.51 on epoch=54
05/29/2022 13:40:57 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.20952421364549711 on epoch=54
05/29/2022 13:40:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=54
05/29/2022 13:41:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=54
05/29/2022 13:41:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.50 on epoch=55
05/29/2022 13:41:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=55
05/29/2022 13:41:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=56
05/29/2022 13:41:21 - INFO - __main__ - Global step 1350 Train loss 0.45 Classification-F1 0.17114127702362994 on epoch=56
05/29/2022 13:41:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.48 on epoch=56
05/29/2022 13:41:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.44 on epoch=57
05/29/2022 13:41:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.47 on epoch=57
05/29/2022 13:41:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=57
05/29/2022 13:41:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.45 on epoch=58
05/29/2022 13:41:44 - INFO - __main__ - Global step 1400 Train loss 0.45 Classification-F1 0.194252700738574 on epoch=58
05/29/2022 13:41:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=58
05/29/2022 13:41:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.46 on epoch=59
05/29/2022 13:41:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.44 on epoch=59
05/29/2022 13:41:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.48 on epoch=59
05/29/2022 13:41:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.50 on epoch=60
05/29/2022 13:42:06 - INFO - __main__ - Global step 1450 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=60
05/29/2022 13:42:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=60
05/29/2022 13:42:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.51 on epoch=61
05/29/2022 13:42:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.48 on epoch=61
05/29/2022 13:42:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=62
05/29/2022 13:42:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.49 on epoch=62
05/29/2022 13:42:30 - INFO - __main__ - Global step 1500 Train loss 0.47 Classification-F1 0.16237623762376238 on epoch=62
05/29/2022 13:42:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=62
05/29/2022 13:42:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.44 on epoch=63
05/29/2022 13:42:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.46 on epoch=63
05/29/2022 13:42:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.44 on epoch=64
05/29/2022 13:42:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.47 on epoch=64
05/29/2022 13:42:55 - INFO - __main__ - Global step 1550 Train loss 0.45 Classification-F1 0.23799148567940728 on epoch=64
05/29/2022 13:42:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.54 on epoch=64
05/29/2022 13:43:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.84 on epoch=65
05/29/2022 13:43:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.59 on epoch=65
05/29/2022 13:43:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.46 on epoch=66
05/29/2022 13:43:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.46 on epoch=66
05/29/2022 13:43:20 - INFO - __main__ - Global step 1600 Train loss 0.58 Classification-F1 0.26808593148809673 on epoch=66
05/29/2022 13:43:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.45 on epoch=67
05/29/2022 13:43:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.46 on epoch=67
05/29/2022 13:43:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=67
05/29/2022 13:43:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=68
05/29/2022 13:43:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=68
05/29/2022 13:43:44 - INFO - __main__ - Global step 1650 Train loss 0.44 Classification-F1 0.28032240564209876 on epoch=68
05/29/2022 13:43:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=69
05/29/2022 13:43:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.47 on epoch=69
05/29/2022 13:43:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.46 on epoch=69
05/29/2022 13:43:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.45 on epoch=70
05/29/2022 13:43:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.46 on epoch=70
05/29/2022 13:44:09 - INFO - __main__ - Global step 1700 Train loss 0.46 Classification-F1 0.29398688692224545 on epoch=70
05/29/2022 13:44:11 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.46 on epoch=71
05/29/2022 13:44:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.50 on epoch=71
05/29/2022 13:44:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.44 on epoch=72
05/29/2022 13:44:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.48 on epoch=72
05/29/2022 13:44:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=72
05/29/2022 13:44:34 - INFO - __main__ - Global step 1750 Train loss 0.46 Classification-F1 0.21585724762195355 on epoch=72
05/29/2022 13:44:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.43 on epoch=73
05/29/2022 13:44:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.47 on epoch=73
05/29/2022 13:44:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.50 on epoch=74
05/29/2022 13:44:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.43 on epoch=74
05/29/2022 13:44:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.44 on epoch=74
05/29/2022 13:44:58 - INFO - __main__ - Global step 1800 Train loss 0.45 Classification-F1 0.2437118437118437 on epoch=74
05/29/2022 13:45:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.46 on epoch=75
05/29/2022 13:45:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.44 on epoch=75
05/29/2022 13:45:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.41 on epoch=76
05/29/2022 13:45:09 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=76
05/29/2022 13:45:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.43 on epoch=77
05/29/2022 13:45:23 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.21787301980538695 on epoch=77
05/29/2022 13:45:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.44 on epoch=77
05/29/2022 13:45:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=77
05/29/2022 13:45:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=78
05/29/2022 13:45:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.46 on epoch=78
05/29/2022 13:45:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=79
05/29/2022 13:45:48 - INFO - __main__ - Global step 1900 Train loss 0.44 Classification-F1 0.24425072644250725 on epoch=79
05/29/2022 13:45:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.42 on epoch=79
05/29/2022 13:45:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.42 on epoch=79
05/29/2022 13:45:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.45 on epoch=80
05/29/2022 13:45:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.41 on epoch=80
05/29/2022 13:46:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.50 on epoch=81
05/29/2022 13:46:13 - INFO - __main__ - Global step 1950 Train loss 0.44 Classification-F1 0.18559004348478028 on epoch=81
05/29/2022 13:46:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.47 on epoch=81
05/29/2022 13:46:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=82
05/29/2022 13:46:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.47 on epoch=82
05/29/2022 13:46:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.42 on epoch=82
05/29/2022 13:46:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.47 on epoch=83
05/29/2022 13:46:38 - INFO - __main__ - Global step 2000 Train loss 0.45 Classification-F1 0.2037447314384524 on epoch=83
05/29/2022 13:46:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.42 on epoch=83
05/29/2022 13:46:43 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.40 on epoch=84
05/29/2022 13:46:46 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.46 on epoch=84
05/29/2022 13:46:48 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.42 on epoch=84
05/29/2022 13:46:51 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.46 on epoch=85
05/29/2022 13:47:02 - INFO - __main__ - Global step 2050 Train loss 0.43 Classification-F1 0.23689888150797708 on epoch=85
05/29/2022 13:47:05 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.43 on epoch=85
05/29/2022 13:47:08 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.41 on epoch=86
05/29/2022 13:47:10 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.45 on epoch=86
05/29/2022 13:47:13 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.45 on epoch=87
05/29/2022 13:47:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.44 on epoch=87
05/29/2022 13:47:27 - INFO - __main__ - Global step 2100 Train loss 0.44 Classification-F1 0.20768109716002184 on epoch=87
05/29/2022 13:47:30 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.42 on epoch=87
05/29/2022 13:47:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.41 on epoch=88
05/29/2022 13:47:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.46 on epoch=88
05/29/2022 13:47:38 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.48 on epoch=89
05/29/2022 13:47:41 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.40 on epoch=89
05/29/2022 13:47:52 - INFO - __main__ - Global step 2150 Train loss 0.44 Classification-F1 0.25321593291404615 on epoch=89
05/29/2022 13:47:55 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.39 on epoch=89
05/29/2022 13:47:58 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.44 on epoch=90
05/29/2022 13:48:00 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.41 on epoch=90
05/29/2022 13:48:03 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.42 on epoch=91
05/29/2022 13:48:06 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.47 on epoch=91
05/29/2022 13:48:17 - INFO - __main__ - Global step 2200 Train loss 0.43 Classification-F1 0.18716476477198293 on epoch=91
05/29/2022 13:48:20 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.42 on epoch=92
05/29/2022 13:48:23 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.44 on epoch=92
05/29/2022 13:48:25 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.44 on epoch=92
05/29/2022 13:48:28 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.45 on epoch=93
05/29/2022 13:48:31 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.46 on epoch=93
05/29/2022 13:48:42 - INFO - __main__ - Global step 2250 Train loss 0.44 Classification-F1 0.18819143819143816 on epoch=93
05/29/2022 13:48:45 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.44 on epoch=94
05/29/2022 13:48:47 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.46 on epoch=94
05/29/2022 13:48:50 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.42 on epoch=94
05/29/2022 13:48:53 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.43 on epoch=95
05/29/2022 13:48:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.43 on epoch=95
05/29/2022 13:49:07 - INFO - __main__ - Global step 2300 Train loss 0.44 Classification-F1 0.24865020989441278 on epoch=95
05/29/2022 13:49:10 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.45 on epoch=96
05/29/2022 13:49:12 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.48 on epoch=96
05/29/2022 13:49:15 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.45 on epoch=97
05/29/2022 13:49:18 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.45 on epoch=97
05/29/2022 13:49:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.42 on epoch=97
05/29/2022 13:49:32 - INFO - __main__ - Global step 2350 Train loss 0.45 Classification-F1 0.24782778213091983 on epoch=97
05/29/2022 13:49:34 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.42 on epoch=98
05/29/2022 13:49:37 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.48 on epoch=98
05/29/2022 13:49:39 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.45 on epoch=99
05/29/2022 13:49:42 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.45 on epoch=99
05/29/2022 13:49:45 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.45 on epoch=99
05/29/2022 13:49:56 - INFO - __main__ - Global step 2400 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=99
05/29/2022 13:49:58 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.42 on epoch=100
05/29/2022 13:50:01 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.42 on epoch=100
05/29/2022 13:50:04 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.41 on epoch=101
05/29/2022 13:50:06 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.46 on epoch=101
05/29/2022 13:50:09 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.40 on epoch=102
05/29/2022 13:50:21 - INFO - __main__ - Global step 2450 Train loss 0.42 Classification-F1 0.2379655301003616 on epoch=102
05/29/2022 13:50:23 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.46 on epoch=102
05/29/2022 13:50:26 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.44 on epoch=102
05/29/2022 13:50:28 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.43 on epoch=103
05/29/2022 13:50:31 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.48 on epoch=103
05/29/2022 13:50:34 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.43 on epoch=104
05/29/2022 13:50:45 - INFO - __main__ - Global step 2500 Train loss 0.45 Classification-F1 0.2904178809033118 on epoch=104
05/29/2022 13:50:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.43 on epoch=104
05/29/2022 13:50:51 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.43 on epoch=104
05/29/2022 13:50:53 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.43 on epoch=105
05/29/2022 13:50:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.43 on epoch=105
05/29/2022 13:50:59 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.45 on epoch=106
05/29/2022 13:51:10 - INFO - __main__ - Global step 2550 Train loss 0.43 Classification-F1 0.22079308613217538 on epoch=106
05/29/2022 13:51:13 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.45 on epoch=106
05/29/2022 13:51:16 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.42 on epoch=107
05/29/2022 13:51:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.46 on epoch=107
05/29/2022 13:51:21 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.40 on epoch=107
05/29/2022 13:51:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.46 on epoch=108
05/29/2022 13:51:35 - INFO - __main__ - Global step 2600 Train loss 0.44 Classification-F1 0.25925337905778 on epoch=108
05/29/2022 13:51:38 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.45 on epoch=108
05/29/2022 13:51:40 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.47 on epoch=109
05/29/2022 13:51:43 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.42 on epoch=109
05/29/2022 13:51:46 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.46 on epoch=109
05/29/2022 13:51:49 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.46 on epoch=110
05/29/2022 13:52:00 - INFO - __main__ - Global step 2650 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=110
05/29/2022 13:52:02 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.47 on epoch=110
05/29/2022 13:52:05 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.44 on epoch=111
05/29/2022 13:52:08 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.44 on epoch=111
05/29/2022 13:52:10 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.49 on epoch=112
05/29/2022 13:52:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.47 on epoch=112
05/29/2022 13:52:25 - INFO - __main__ - Global step 2700 Train loss 0.46 Classification-F1 0.24503112442172958 on epoch=112
05/29/2022 13:52:27 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.42 on epoch=112
05/29/2022 13:52:30 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.47 on epoch=113
05/29/2022 13:52:32 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.46 on epoch=113
05/29/2022 13:52:35 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.44 on epoch=114
05/29/2022 13:52:38 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.42 on epoch=114
05/29/2022 13:52:49 - INFO - __main__ - Global step 2750 Train loss 0.44 Classification-F1 0.3169866665744974 on epoch=114
05/29/2022 13:52:49 - INFO - __main__ - Saving model with best Classification-F1: 0.3127707910250668 -> 0.3169866665744974 on epoch=114, global_step=2750
05/29/2022 13:52:52 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.43 on epoch=114
05/29/2022 13:52:55 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.45 on epoch=115
05/29/2022 13:52:57 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.41 on epoch=115
05/29/2022 13:53:00 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.45 on epoch=116
05/29/2022 13:53:02 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.45 on epoch=116
05/29/2022 13:53:14 - INFO - __main__ - Global step 2800 Train loss 0.44 Classification-F1 0.2625815183954719 on epoch=116
05/29/2022 13:53:16 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.38 on epoch=117
05/29/2022 13:53:19 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.45 on epoch=117
05/29/2022 13:53:22 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.37 on epoch=117
05/29/2022 13:53:24 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.49 on epoch=118
05/29/2022 13:53:27 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.43 on epoch=118
05/29/2022 13:53:38 - INFO - __main__ - Global step 2850 Train loss 0.43 Classification-F1 0.24225177412935875 on epoch=118
05/29/2022 13:53:41 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.44 on epoch=119
05/29/2022 13:53:44 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.47 on epoch=119
05/29/2022 13:53:46 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.44 on epoch=119
05/29/2022 13:53:49 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.48 on epoch=120
05/29/2022 13:53:52 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.45 on epoch=120
05/29/2022 13:54:02 - INFO - __main__ - Global step 2900 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=120
05/29/2022 13:54:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.46 on epoch=121
05/29/2022 13:54:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.43 on epoch=121
05/29/2022 13:54:10 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.40 on epoch=122
05/29/2022 13:54:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.47 on epoch=122
05/29/2022 13:54:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.46 on epoch=122
05/29/2022 13:54:27 - INFO - __main__ - Global step 2950 Train loss 0.44 Classification-F1 0.16917019987886128 on epoch=122
05/29/2022 13:54:29 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.46 on epoch=123
05/29/2022 13:54:32 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.49 on epoch=123
05/29/2022 13:54:35 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.43 on epoch=124
05/29/2022 13:54:37 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.42 on epoch=124
05/29/2022 13:54:40 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.49 on epoch=124
05/29/2022 13:54:41 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 13:54:41 - INFO - __main__ - Printing 3 examples
05/29/2022 13:54:41 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/29/2022 13:54:41 - INFO - __main__ - ['neutral']
05/29/2022 13:54:41 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/29/2022 13:54:41 - INFO - __main__ - ['neutral']
05/29/2022 13:54:41 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/29/2022 13:54:41 - INFO - __main__ - ['neutral']
05/29/2022 13:54:41 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:54:41 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:54:42 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 13:54:42 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 13:54:42 - INFO - __main__ - Printing 3 examples
05/29/2022 13:54:42 - INFO - __main__ -  [anli] premise: Nauset Regional High School an NEASC accredited high school located in North Eastham, Massachusetts. Nauset is inside the Cape Cod National Seashore, making it the only high school on the East Coast located within a National Park. The open campus is situated about a half-mile from Nauset Light. Nauset's colors are Black and Gold and the school's mascot is the Warrior. [SEP] hypothesis: Nauset Regional High School is a short walk from the Atlantic Ocean.
05/29/2022 13:54:42 - INFO - __main__ - ['neutral']
05/29/2022 13:54:42 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club is the oldest football club in England. 
05/29/2022 13:54:42 - INFO - __main__ - ['neutral']
05/29/2022 13:54:42 - INFO - __main__ -  [anli] premise: Brontë was a 2005 play by British playwright Polly Teale about the lives of the Brontë sisters, their brother Branwell and their father Patrick. It also featured characters from the sisters' novels such as Cathy and Heathcliff from "Wuthering Heights". [SEP] hypothesis: Bronte features characters from more than 3 other novels.
05/29/2022 13:54:42 - INFO - __main__ - ['neutral']
05/29/2022 13:54:42 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:54:42 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:54:42 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 13:54:51 - INFO - __main__ - Global step 3000 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=124
05/29/2022 13:54:51 - INFO - __main__ - save last model!
05/29/2022 13:54:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 13:54:51 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 13:54:51 - INFO - __main__ - Printing 3 examples
05/29/2022 13:54:51 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 13:54:51 - INFO - __main__ - ['contradiction']
05/29/2022 13:54:51 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 13:54:51 - INFO - __main__ - ['entailment']
05/29/2022 13:54:51 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 13:54:51 - INFO - __main__ - ['contradiction']
05/29/2022 13:54:51 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:54:52 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:54:53 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 13:54:58 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 13:54:58 - INFO - __main__ - task name: anli
05/29/2022 13:54:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 13:54:59 - INFO - __main__ - Starting training!
05/29/2022 13:55:22 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_100_0.4_8_predictions.txt
05/29/2022 13:55:22 - INFO - __main__ - Classification-F1 on test data: 0.1665
05/29/2022 13:55:22 - INFO - __main__ - prefix=anli_128_100, lr=0.4, bsz=8, dev_performance=0.3169866665744974, test_performance=0.16654163540885222
05/29/2022 13:55:22 - INFO - __main__ - Running ... prefix=anli_128_100, lr=0.3, bsz=8 ...
05/29/2022 13:55:23 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 13:55:23 - INFO - __main__ - Printing 3 examples
05/29/2022 13:55:23 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/29/2022 13:55:23 - INFO - __main__ - ['neutral']
05/29/2022 13:55:23 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/29/2022 13:55:23 - INFO - __main__ - ['neutral']
05/29/2022 13:55:23 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/29/2022 13:55:23 - INFO - __main__ - ['neutral']
05/29/2022 13:55:23 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:55:23 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:55:24 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 13:55:24 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 13:55:24 - INFO - __main__ - Printing 3 examples
05/29/2022 13:55:24 - INFO - __main__ -  [anli] premise: Nauset Regional High School an NEASC accredited high school located in North Eastham, Massachusetts. Nauset is inside the Cape Cod National Seashore, making it the only high school on the East Coast located within a National Park. The open campus is situated about a half-mile from Nauset Light. Nauset's colors are Black and Gold and the school's mascot is the Warrior. [SEP] hypothesis: Nauset Regional High School is a short walk from the Atlantic Ocean.
05/29/2022 13:55:24 - INFO - __main__ - ['neutral']
05/29/2022 13:55:24 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club is the oldest football club in England. 
05/29/2022 13:55:24 - INFO - __main__ - ['neutral']
05/29/2022 13:55:24 - INFO - __main__ -  [anli] premise: Brontë was a 2005 play by British playwright Polly Teale about the lives of the Brontë sisters, their brother Branwell and their father Patrick. It also featured characters from the sisters' novels such as Cathy and Heathcliff from "Wuthering Heights". [SEP] hypothesis: Bronte features characters from more than 3 other novels.
05/29/2022 13:55:24 - INFO - __main__ - ['neutral']
05/29/2022 13:55:24 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:55:24 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:55:24 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 13:55:40 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 13:55:40 - INFO - __main__ - task name: anli
05/29/2022 13:55:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 13:55:41 - INFO - __main__ - Starting training!
05/29/2022 13:55:44 - INFO - __main__ - Step 10 Global step 10 Train loss 6.76 on epoch=0
05/29/2022 13:55:47 - INFO - __main__ - Step 20 Global step 20 Train loss 3.31 on epoch=0
05/29/2022 13:55:49 - INFO - __main__ - Step 30 Global step 30 Train loss 1.69 on epoch=1
05/29/2022 13:55:52 - INFO - __main__ - Step 40 Global step 40 Train loss 1.15 on epoch=1
05/29/2022 13:55:55 - INFO - __main__ - Step 50 Global step 50 Train loss 0.88 on epoch=2
05/29/2022 13:56:05 - INFO - __main__ - Global step 50 Train loss 2.76 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 13:56:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 13:56:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.89 on epoch=2
05/29/2022 13:56:10 - INFO - __main__ - Step 70 Global step 70 Train loss 0.73 on epoch=2
05/29/2022 13:56:13 - INFO - __main__ - Step 80 Global step 80 Train loss 0.64 on epoch=3
05/29/2022 13:56:15 - INFO - __main__ - Step 90 Global step 90 Train loss 0.62 on epoch=3
05/29/2022 13:56:18 - INFO - __main__ - Step 100 Global step 100 Train loss 0.65 on epoch=4
05/29/2022 13:56:29 - INFO - __main__ - Global step 100 Train loss 0.70 Classification-F1 0.16666666666666666 on epoch=4
05/29/2022 13:56:32 - INFO - __main__ - Step 110 Global step 110 Train loss 0.69 on epoch=4
05/29/2022 13:56:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.62 on epoch=4
05/29/2022 13:56:37 - INFO - __main__ - Step 130 Global step 130 Train loss 0.64 on epoch=5
05/29/2022 13:56:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.64 on epoch=5
05/29/2022 13:56:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.58 on epoch=6
05/29/2022 13:56:54 - INFO - __main__ - Global step 150 Train loss 0.63 Classification-F1 0.23172389587598716 on epoch=6
05/29/2022 13:56:54 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.23172389587598716 on epoch=6, global_step=150
05/29/2022 13:56:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.59 on epoch=6
05/29/2022 13:56:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.56 on epoch=7
05/29/2022 13:57:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.66 on epoch=7
05/29/2022 13:57:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.54 on epoch=7
05/29/2022 13:57:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=8
05/29/2022 13:57:19 - INFO - __main__ - Global step 200 Train loss 0.57 Classification-F1 0.252194071866203 on epoch=8
05/29/2022 13:57:19 - INFO - __main__ - Saving model with best Classification-F1: 0.23172389587598716 -> 0.252194071866203 on epoch=8, global_step=200
05/29/2022 13:57:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=8
05/29/2022 13:57:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.58 on epoch=9
05/29/2022 13:57:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.53 on epoch=9
05/29/2022 13:57:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.55 on epoch=9
05/29/2022 13:57:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.56 on epoch=10
05/29/2022 13:57:43 - INFO - __main__ - Global step 250 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=10
05/29/2022 13:57:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=10
05/29/2022 13:57:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=11
05/29/2022 13:57:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.56 on epoch=11
05/29/2022 13:57:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=12
05/29/2022 13:57:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.59 on epoch=12
05/29/2022 13:58:07 - INFO - __main__ - Global step 300 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=12
05/29/2022 13:58:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=12
05/29/2022 13:58:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.54 on epoch=13
05/29/2022 13:58:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.52 on epoch=13
05/29/2022 13:58:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.58 on epoch=14
05/29/2022 13:58:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=14
05/29/2022 13:58:31 - INFO - __main__ - Global step 350 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=14
05/29/2022 13:58:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=14
05/29/2022 13:58:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=15
05/29/2022 13:58:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=15
05/29/2022 13:58:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=16
05/29/2022 13:58:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.56 on epoch=16
05/29/2022 13:58:56 - INFO - __main__ - Global step 400 Train loss 0.51 Classification-F1 0.23731192223258465 on epoch=16
05/29/2022 13:58:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=17
05/29/2022 13:59:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=17
05/29/2022 13:59:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.49 on epoch=17
05/29/2022 13:59:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.54 on epoch=18
05/29/2022 13:59:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=18
05/29/2022 13:59:21 - INFO - __main__ - Global step 450 Train loss 0.50 Classification-F1 0.2417979850167915 on epoch=18
05/29/2022 13:59:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=19
05/29/2022 13:59:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=19
05/29/2022 13:59:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.49 on epoch=19
05/29/2022 13:59:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.50 on epoch=20
05/29/2022 13:59:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=20
05/29/2022 13:59:45 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.16864295125164688 on epoch=20
05/29/2022 13:59:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=21
05/29/2022 13:59:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.58 on epoch=21
05/29/2022 13:59:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=22
05/29/2022 13:59:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.54 on epoch=22
05/29/2022 13:59:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.48 on epoch=22
05/29/2022 14:00:09 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=22
05/29/2022 14:00:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=23
05/29/2022 14:00:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=23
05/29/2022 14:00:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=24
05/29/2022 14:00:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=24
05/29/2022 14:00:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=24
05/29/2022 14:00:33 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.17230055257310287 on epoch=24
05/29/2022 14:00:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.53 on epoch=25
05/29/2022 14:00:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=25
05/29/2022 14:00:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=26
05/29/2022 14:00:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=26
05/29/2022 14:00:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=27
05/29/2022 14:00:58 - INFO - __main__ - Global step 650 Train loss 0.47 Classification-F1 0.264372306209878 on epoch=27
05/29/2022 14:00:58 - INFO - __main__ - Saving model with best Classification-F1: 0.252194071866203 -> 0.264372306209878 on epoch=27, global_step=650
05/29/2022 14:01:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.52 on epoch=27
05/29/2022 14:01:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=27
05/29/2022 14:01:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.54 on epoch=28
05/29/2022 14:01:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.53 on epoch=28
05/29/2022 14:01:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=29
05/29/2022 14:01:23 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.2724681126197614 on epoch=29
05/29/2022 14:01:23 - INFO - __main__ - Saving model with best Classification-F1: 0.264372306209878 -> 0.2724681126197614 on epoch=29, global_step=700
05/29/2022 14:01:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=29
05/29/2022 14:01:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.48 on epoch=29
05/29/2022 14:01:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.50 on epoch=30
05/29/2022 14:01:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=30
05/29/2022 14:01:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=31
05/29/2022 14:01:47 - INFO - __main__ - Global step 750 Train loss 0.47 Classification-F1 0.28125 on epoch=31
05/29/2022 14:01:47 - INFO - __main__ - Saving model with best Classification-F1: 0.2724681126197614 -> 0.28125 on epoch=31, global_step=750
05/29/2022 14:01:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=31
05/29/2022 14:01:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.47 on epoch=32
05/29/2022 14:01:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.50 on epoch=32
05/29/2022 14:01:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=32
05/29/2022 14:02:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.52 on epoch=33
05/29/2022 14:02:12 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.24787921149828315 on epoch=33
05/29/2022 14:02:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.51 on epoch=33
05/29/2022 14:02:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.50 on epoch=34
05/29/2022 14:02:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.48 on epoch=34
05/29/2022 14:02:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=34
05/29/2022 14:02:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=35
05/29/2022 14:02:36 - INFO - __main__ - Global step 850 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=35
05/29/2022 14:02:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=35
05/29/2022 14:02:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.49 on epoch=36
05/29/2022 14:02:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.48 on epoch=36
05/29/2022 14:02:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=37
05/29/2022 14:02:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.49 on epoch=37
05/29/2022 14:03:01 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.23441295546558705 on epoch=37
05/29/2022 14:03:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=37
05/29/2022 14:03:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=38
05/29/2022 14:03:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.51 on epoch=38
05/29/2022 14:03:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=39
05/29/2022 14:03:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.47 on epoch=39
05/29/2022 14:03:25 - INFO - __main__ - Global step 950 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=39
05/29/2022 14:03:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=39
05/29/2022 14:03:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.51 on epoch=40
05/29/2022 14:03:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=40
05/29/2022 14:03:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.49 on epoch=41
05/29/2022 14:03:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.49 on epoch=41
05/29/2022 14:03:50 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.2237363831647945 on epoch=41
05/29/2022 14:03:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=42
05/29/2022 14:03:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.49 on epoch=42
05/29/2022 14:03:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=42
05/29/2022 14:04:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.50 on epoch=43
05/29/2022 14:04:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=43
05/29/2022 14:04:15 - INFO - __main__ - Global step 1050 Train loss 0.46 Classification-F1 0.2609338409409056 on epoch=43
05/29/2022 14:04:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=44
05/29/2022 14:04:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.47 on epoch=44
05/29/2022 14:04:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=44
05/29/2022 14:04:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.50 on epoch=45
05/29/2022 14:04:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=45
05/29/2022 14:04:39 - INFO - __main__ - Global step 1100 Train loss 0.47 Classification-F1 0.19296196760985496 on epoch=45
05/29/2022 14:04:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=46
05/29/2022 14:04:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=46
05/29/2022 14:04:47 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=47
05/29/2022 14:04:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.47 on epoch=47
05/29/2022 14:04:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=47
05/29/2022 14:05:03 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.16568819308545338 on epoch=47
05/29/2022 14:05:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=48
05/29/2022 14:05:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=48
05/29/2022 14:05:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=49
05/29/2022 14:05:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=49
05/29/2022 14:05:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.48 on epoch=49
05/29/2022 14:05:28 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.16633922724296005 on epoch=49
05/29/2022 14:05:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=50
05/29/2022 14:05:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=50
05/29/2022 14:05:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=51
05/29/2022 14:05:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.52 on epoch=51
05/29/2022 14:05:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=52
05/29/2022 14:05:52 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.22470288762423593 on epoch=52
05/29/2022 14:05:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.54 on epoch=52
05/29/2022 14:05:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=52
05/29/2022 14:06:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.50 on epoch=53
05/29/2022 14:06:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=53
05/29/2022 14:06:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=54
05/29/2022 14:06:17 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.2662324553129664 on epoch=54
05/29/2022 14:06:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.45 on epoch=54
05/29/2022 14:06:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.47 on epoch=54
05/29/2022 14:06:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.47 on epoch=55
05/29/2022 14:06:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=55
05/29/2022 14:06:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.52 on epoch=56
05/29/2022 14:06:42 - INFO - __main__ - Global step 1350 Train loss 0.48 Classification-F1 0.27098552382582936 on epoch=56
05/29/2022 14:06:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.50 on epoch=56
05/29/2022 14:06:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=57
05/29/2022 14:06:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.51 on epoch=57
05/29/2022 14:06:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.44 on epoch=57
05/29/2022 14:06:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.45 on epoch=58
05/29/2022 14:07:06 - INFO - __main__ - Global step 1400 Train loss 0.46 Classification-F1 0.1721607831834019 on epoch=58
05/29/2022 14:07:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=58
05/29/2022 14:07:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=59
05/29/2022 14:07:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.45 on epoch=59
05/29/2022 14:07:17 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=59
05/29/2022 14:07:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=60
05/29/2022 14:07:31 - INFO - __main__ - Global step 1450 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=60
05/29/2022 14:07:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=60
05/29/2022 14:07:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=61
05/29/2022 14:07:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.49 on epoch=61
05/29/2022 14:07:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=62
05/29/2022 14:07:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.46 on epoch=62
05/29/2022 14:07:55 - INFO - __main__ - Global step 1500 Train loss 0.44 Classification-F1 0.2694324078532071 on epoch=62
05/29/2022 14:07:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=62
05/29/2022 14:08:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.50 on epoch=63
05/29/2022 14:08:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.44 on epoch=63
05/29/2022 14:08:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=64
05/29/2022 14:08:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.47 on epoch=64
05/29/2022 14:08:19 - INFO - __main__ - Global step 1550 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=64
05/29/2022 14:08:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.45 on epoch=64
05/29/2022 14:08:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.44 on epoch=65
05/29/2022 14:08:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=65
05/29/2022 14:08:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=66
05/29/2022 14:08:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.48 on epoch=66
05/29/2022 14:08:44 - INFO - __main__ - Global step 1600 Train loss 0.44 Classification-F1 0.27686311434973343 on epoch=66
05/29/2022 14:08:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.41 on epoch=67
05/29/2022 14:08:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.47 on epoch=67
05/29/2022 14:08:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=67
05/29/2022 14:08:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.46 on epoch=68
05/29/2022 14:08:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.44 on epoch=68
05/29/2022 14:09:08 - INFO - __main__ - Global step 1650 Train loss 0.43 Classification-F1 0.30783771187959735 on epoch=68
05/29/2022 14:09:08 - INFO - __main__ - Saving model with best Classification-F1: 0.28125 -> 0.30783771187959735 on epoch=68, global_step=1650
05/29/2022 14:09:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.45 on epoch=69
05/29/2022 14:09:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.46 on epoch=69
05/29/2022 14:09:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.50 on epoch=69
05/29/2022 14:09:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.48 on epoch=70
05/29/2022 14:09:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.49 on epoch=70
05/29/2022 14:09:33 - INFO - __main__ - Global step 1700 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=70
05/29/2022 14:09:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.47 on epoch=71
05/29/2022 14:09:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.48 on epoch=71
05/29/2022 14:09:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.46 on epoch=72
05/29/2022 14:09:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.44 on epoch=72
05/29/2022 14:09:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.44 on epoch=72
05/29/2022 14:09:57 - INFO - __main__ - Global step 1750 Train loss 0.46 Classification-F1 0.1775476492457625 on epoch=72
05/29/2022 14:10:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.44 on epoch=73
05/29/2022 14:10:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.41 on epoch=73
05/29/2022 14:10:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.45 on epoch=74
05/29/2022 14:10:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.49 on epoch=74
05/29/2022 14:10:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=74
05/29/2022 14:10:21 - INFO - __main__ - Global step 1800 Train loss 0.45 Classification-F1 0.16601307189542483 on epoch=74
05/29/2022 14:10:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=75
05/29/2022 14:10:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.45 on epoch=75
05/29/2022 14:10:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.43 on epoch=76
05/29/2022 14:10:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.46 on epoch=76
05/29/2022 14:10:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=77
05/29/2022 14:10:46 - INFO - __main__ - Global step 1850 Train loss 0.44 Classification-F1 0.1900035311246874 on epoch=77
05/29/2022 14:10:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.46 on epoch=77
05/29/2022 14:10:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=77
05/29/2022 14:10:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.46 on epoch=78
05/29/2022 14:10:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=78
05/29/2022 14:10:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=79
05/29/2022 14:11:10 - INFO - __main__ - Global step 1900 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=79
05/29/2022 14:11:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.46 on epoch=79
05/29/2022 14:11:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.47 on epoch=79
05/29/2022 14:11:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.44 on epoch=80
05/29/2022 14:11:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.44 on epoch=80
05/29/2022 14:11:24 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.42 on epoch=81
05/29/2022 14:11:35 - INFO - __main__ - Global step 1950 Train loss 0.45 Classification-F1 0.18684932293686007 on epoch=81
05/29/2022 14:11:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.50 on epoch=81
05/29/2022 14:11:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=82
05/29/2022 14:11:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.49 on epoch=82
05/29/2022 14:11:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.46 on epoch=82
05/29/2022 14:11:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.45 on epoch=83
05/29/2022 14:12:00 - INFO - __main__ - Global step 2000 Train loss 0.47 Classification-F1 0.22758077226162332 on epoch=83
05/29/2022 14:12:02 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.44 on epoch=83
05/29/2022 14:12:05 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.49 on epoch=84
05/29/2022 14:12:08 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.45 on epoch=84
05/29/2022 14:12:10 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.42 on epoch=84
05/29/2022 14:12:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.50 on epoch=85
05/29/2022 14:12:24 - INFO - __main__ - Global step 2050 Train loss 0.46 Classification-F1 0.16568819308545338 on epoch=85
05/29/2022 14:12:27 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.42 on epoch=85
05/29/2022 14:12:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.43 on epoch=86
05/29/2022 14:12:32 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.45 on epoch=86
05/29/2022 14:12:35 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.43 on epoch=87
05/29/2022 14:12:37 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.47 on epoch=87
05/29/2022 14:12:49 - INFO - __main__ - Global step 2100 Train loss 0.44 Classification-F1 0.2545791869907738 on epoch=87
05/29/2022 14:12:52 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.43 on epoch=87
05/29/2022 14:12:54 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.48 on epoch=88
05/29/2022 14:12:57 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.42 on epoch=88
05/29/2022 14:12:59 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.43 on epoch=89
05/29/2022 14:13:02 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.44 on epoch=89
05/29/2022 14:13:13 - INFO - __main__ - Global step 2150 Train loss 0.44 Classification-F1 0.16732026143790854 on epoch=89
05/29/2022 14:13:16 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.44 on epoch=89
05/29/2022 14:13:18 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.45 on epoch=90
05/29/2022 14:13:21 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.44 on epoch=90
05/29/2022 14:13:24 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.45 on epoch=91
05/29/2022 14:13:26 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.47 on epoch=91
05/29/2022 14:13:38 - INFO - __main__ - Global step 2200 Train loss 0.45 Classification-F1 0.1721607831834019 on epoch=91
05/29/2022 14:13:41 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.43 on epoch=92
05/29/2022 14:13:43 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.46 on epoch=92
05/29/2022 14:13:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.41 on epoch=92
05/29/2022 14:13:49 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.49 on epoch=93
05/29/2022 14:13:51 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.49 on epoch=93
05/29/2022 14:14:03 - INFO - __main__ - Global step 2250 Train loss 0.46 Classification-F1 0.1775766716943188 on epoch=93
05/29/2022 14:14:06 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.44 on epoch=94
05/29/2022 14:14:08 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.45 on epoch=94
05/29/2022 14:14:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.46 on epoch=94
05/29/2022 14:14:14 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.47 on epoch=95
05/29/2022 14:14:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.46 on epoch=95
05/29/2022 14:14:27 - INFO - __main__ - Global step 2300 Train loss 0.46 Classification-F1 0.2517453100634753 on epoch=95
05/29/2022 14:14:30 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.46 on epoch=96
05/29/2022 14:14:33 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.46 on epoch=96
05/29/2022 14:14:35 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.42 on epoch=97
05/29/2022 14:14:38 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.45 on epoch=97
05/29/2022 14:14:40 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.41 on epoch=97
05/29/2022 14:14:52 - INFO - __main__ - Global step 2350 Train loss 0.44 Classification-F1 0.2802853600195793 on epoch=97
05/29/2022 14:14:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.45 on epoch=98
05/29/2022 14:14:57 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.45 on epoch=98
05/29/2022 14:15:00 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.46 on epoch=99
05/29/2022 14:15:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.43 on epoch=99
05/29/2022 14:15:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.46 on epoch=99
05/29/2022 14:15:16 - INFO - __main__ - Global step 2400 Train loss 0.45 Classification-F1 0.24181894015398317 on epoch=99
05/29/2022 14:15:19 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.42 on epoch=100
05/29/2022 14:15:22 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.49 on epoch=100
05/29/2022 14:15:24 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.43 on epoch=101
05/29/2022 14:15:27 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.44 on epoch=101
05/29/2022 14:15:29 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.40 on epoch=102
05/29/2022 14:15:41 - INFO - __main__ - Global step 2450 Train loss 0.44 Classification-F1 0.19999999999999998 on epoch=102
05/29/2022 14:15:44 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.42 on epoch=102
05/29/2022 14:15:46 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.37 on epoch=102
05/29/2022 14:15:49 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.44 on epoch=103
05/29/2022 14:15:52 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.43 on epoch=103
05/29/2022 14:15:54 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.45 on epoch=104
05/29/2022 14:16:05 - INFO - __main__ - Global step 2500 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=104
05/29/2022 14:16:08 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.42 on epoch=104
05/29/2022 14:16:11 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.42 on epoch=104
05/29/2022 14:16:13 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.48 on epoch=105
05/29/2022 14:16:16 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.45 on epoch=105
05/29/2022 14:16:18 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.44 on epoch=106
05/29/2022 14:16:30 - INFO - __main__ - Global step 2550 Train loss 0.44 Classification-F1 0.21315842890490208 on epoch=106
05/29/2022 14:16:33 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.47 on epoch=106
05/29/2022 14:16:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.42 on epoch=107
05/29/2022 14:16:38 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.42 on epoch=107
05/29/2022 14:16:41 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.45 on epoch=107
05/29/2022 14:16:43 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.49 on epoch=108
05/29/2022 14:16:55 - INFO - __main__ - Global step 2600 Train loss 0.45 Classification-F1 0.22053269727688332 on epoch=108
05/29/2022 14:16:58 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.45 on epoch=108
05/29/2022 14:17:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.46 on epoch=109
05/29/2022 14:17:03 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.44 on epoch=109
05/29/2022 14:17:06 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.43 on epoch=109
05/29/2022 14:17:08 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.50 on epoch=110
05/29/2022 14:17:20 - INFO - __main__ - Global step 2650 Train loss 0.45 Classification-F1 0.19949624823620946 on epoch=110
05/29/2022 14:17:22 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.42 on epoch=110
05/29/2022 14:17:25 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.43 on epoch=111
05/29/2022 14:17:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.43 on epoch=111
05/29/2022 14:17:30 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.40 on epoch=112
05/29/2022 14:17:33 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.45 on epoch=112
05/29/2022 14:17:44 - INFO - __main__ - Global step 2700 Train loss 0.43 Classification-F1 0.2973615756224452 on epoch=112
05/29/2022 14:17:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.46 on epoch=112
05/29/2022 14:17:49 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.44 on epoch=113
05/29/2022 14:17:52 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.44 on epoch=113
05/29/2022 14:17:55 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.48 on epoch=114
05/29/2022 14:17:57 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.48 on epoch=114
05/29/2022 14:18:09 - INFO - __main__ - Global step 2750 Train loss 0.46 Classification-F1 0.33548334620904224 on epoch=114
05/29/2022 14:18:09 - INFO - __main__ - Saving model with best Classification-F1: 0.30783771187959735 -> 0.33548334620904224 on epoch=114, global_step=2750
05/29/2022 14:18:12 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.41 on epoch=114
05/29/2022 14:18:14 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.44 on epoch=115
05/29/2022 14:18:17 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.41 on epoch=115
05/29/2022 14:18:19 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.41 on epoch=116
05/29/2022 14:18:22 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.45 on epoch=116
05/29/2022 14:18:34 - INFO - __main__ - Global step 2800 Train loss 0.43 Classification-F1 0.2053428496542269 on epoch=116
05/29/2022 14:18:37 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.38 on epoch=117
05/29/2022 14:18:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.46 on epoch=117
05/29/2022 14:18:42 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.42 on epoch=117
05/29/2022 14:18:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.45 on epoch=118
05/29/2022 14:18:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.43 on epoch=118
05/29/2022 14:18:59 - INFO - __main__ - Global step 2850 Train loss 0.43 Classification-F1 0.19631618453721345 on epoch=118
05/29/2022 14:19:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.45 on epoch=119
05/29/2022 14:19:04 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.42 on epoch=119
05/29/2022 14:19:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.42 on epoch=119
05/29/2022 14:19:09 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.42 on epoch=120
05/29/2022 14:19:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.43 on epoch=120
05/29/2022 14:19:24 - INFO - __main__ - Global step 2900 Train loss 0.43 Classification-F1 0.32132731571029444 on epoch=120
05/29/2022 14:19:26 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.44 on epoch=121
05/29/2022 14:19:29 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.48 on epoch=121
05/29/2022 14:19:32 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.44 on epoch=122
05/29/2022 14:19:34 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.46 on epoch=122
05/29/2022 14:19:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.44 on epoch=122
05/29/2022 14:19:48 - INFO - __main__ - Global step 2950 Train loss 0.45 Classification-F1 0.21538731807497827 on epoch=122
05/29/2022 14:19:51 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.43 on epoch=123
05/29/2022 14:19:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.44 on epoch=123
05/29/2022 14:19:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.47 on epoch=124
05/29/2022 14:19:59 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.43 on epoch=124
05/29/2022 14:20:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.43 on epoch=124
05/29/2022 14:20:03 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 14:20:03 - INFO - __main__ - Printing 3 examples
05/29/2022 14:20:03 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/29/2022 14:20:03 - INFO - __main__ - ['neutral']
05/29/2022 14:20:03 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/29/2022 14:20:03 - INFO - __main__ - ['neutral']
05/29/2022 14:20:03 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/29/2022 14:20:03 - INFO - __main__ - ['neutral']
05/29/2022 14:20:03 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:20:03 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:20:04 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 14:20:04 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 14:20:04 - INFO - __main__ - Printing 3 examples
05/29/2022 14:20:04 - INFO - __main__ -  [anli] premise: Nauset Regional High School an NEASC accredited high school located in North Eastham, Massachusetts. Nauset is inside the Cape Cod National Seashore, making it the only high school on the East Coast located within a National Park. The open campus is situated about a half-mile from Nauset Light. Nauset's colors are Black and Gold and the school's mascot is the Warrior. [SEP] hypothesis: Nauset Regional High School is a short walk from the Atlantic Ocean.
05/29/2022 14:20:04 - INFO - __main__ - ['neutral']
05/29/2022 14:20:04 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club is the oldest football club in England. 
05/29/2022 14:20:04 - INFO - __main__ - ['neutral']
05/29/2022 14:20:04 - INFO - __main__ -  [anli] premise: Brontë was a 2005 play by British playwright Polly Teale about the lives of the Brontë sisters, their brother Branwell and their father Patrick. It also featured characters from the sisters' novels such as Cathy and Heathcliff from "Wuthering Heights". [SEP] hypothesis: Bronte features characters from more than 3 other novels.
05/29/2022 14:20:04 - INFO - __main__ - ['neutral']
05/29/2022 14:20:04 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:20:04 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:20:04 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 14:20:13 - INFO - __main__ - Global step 3000 Train loss 0.44 Classification-F1 0.16568819308545338 on epoch=124
05/29/2022 14:20:13 - INFO - __main__ - save last model!
05/29/2022 14:20:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 14:20:13 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 14:20:13 - INFO - __main__ - Printing 3 examples
05/29/2022 14:20:13 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 14:20:13 - INFO - __main__ - ['contradiction']
05/29/2022 14:20:13 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 14:20:13 - INFO - __main__ - ['entailment']
05/29/2022 14:20:13 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 14:20:13 - INFO - __main__ - ['contradiction']
05/29/2022 14:20:13 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:20:14 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:20:15 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 14:20:23 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 14:20:23 - INFO - __main__ - task name: anli
05/29/2022 14:20:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 14:20:24 - INFO - __main__ - Starting training!
05/29/2022 14:20:45 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_100_0.3_8_predictions.txt
05/29/2022 14:20:45 - INFO - __main__ - Classification-F1 on test data: 0.1683
05/29/2022 14:20:45 - INFO - __main__ - prefix=anli_128_100, lr=0.3, bsz=8, dev_performance=0.33548334620904224, test_performance=0.16827513625034285
05/29/2022 14:20:45 - INFO - __main__ - Running ... prefix=anli_128_100, lr=0.2, bsz=8 ...
05/29/2022 14:20:46 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 14:20:46 - INFO - __main__ - Printing 3 examples
05/29/2022 14:20:46 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/29/2022 14:20:46 - INFO - __main__ - ['neutral']
05/29/2022 14:20:46 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/29/2022 14:20:46 - INFO - __main__ - ['neutral']
05/29/2022 14:20:46 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/29/2022 14:20:46 - INFO - __main__ - ['neutral']
05/29/2022 14:20:46 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:20:46 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:20:47 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 14:20:47 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 14:20:47 - INFO - __main__ - Printing 3 examples
05/29/2022 14:20:47 - INFO - __main__ -  [anli] premise: Nauset Regional High School an NEASC accredited high school located in North Eastham, Massachusetts. Nauset is inside the Cape Cod National Seashore, making it the only high school on the East Coast located within a National Park. The open campus is situated about a half-mile from Nauset Light. Nauset's colors are Black and Gold and the school's mascot is the Warrior. [SEP] hypothesis: Nauset Regional High School is a short walk from the Atlantic Ocean.
05/29/2022 14:20:47 - INFO - __main__ - ['neutral']
05/29/2022 14:20:47 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club is the oldest football club in England. 
05/29/2022 14:20:47 - INFO - __main__ - ['neutral']
05/29/2022 14:20:47 - INFO - __main__ -  [anli] premise: Brontë was a 2005 play by British playwright Polly Teale about the lives of the Brontë sisters, their brother Branwell and their father Patrick. It also featured characters from the sisters' novels such as Cathy and Heathcliff from "Wuthering Heights". [SEP] hypothesis: Bronte features characters from more than 3 other novels.
05/29/2022 14:20:47 - INFO - __main__ - ['neutral']
05/29/2022 14:20:47 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:20:47 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:20:47 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 14:21:03 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 14:21:03 - INFO - __main__ - task name: anli
05/29/2022 14:21:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 14:21:04 - INFO - __main__ - Starting training!
05/29/2022 14:21:07 - INFO - __main__ - Step 10 Global step 10 Train loss 7.38 on epoch=0
05/29/2022 14:21:10 - INFO - __main__ - Step 20 Global step 20 Train loss 4.87 on epoch=0
05/29/2022 14:21:13 - INFO - __main__ - Step 30 Global step 30 Train loss 2.88 on epoch=1
05/29/2022 14:21:15 - INFO - __main__ - Step 40 Global step 40 Train loss 1.97 on epoch=1
05/29/2022 14:21:18 - INFO - __main__ - Step 50 Global step 50 Train loss 1.40 on epoch=2
05/29/2022 14:21:27 - INFO - __main__ - Global step 50 Train loss 3.70 Classification-F1 0.16601307189542483 on epoch=2
05/29/2022 14:21:27 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16601307189542483 on epoch=2, global_step=50
05/29/2022 14:21:29 - INFO - __main__ - Step 60 Global step 60 Train loss 1.22 on epoch=2
05/29/2022 14:21:32 - INFO - __main__ - Step 70 Global step 70 Train loss 1.08 on epoch=2
05/29/2022 14:21:35 - INFO - __main__ - Step 80 Global step 80 Train loss 0.82 on epoch=3
05/29/2022 14:21:37 - INFO - __main__ - Step 90 Global step 90 Train loss 0.80 on epoch=3
05/29/2022 14:21:40 - INFO - __main__ - Step 100 Global step 100 Train loss 0.78 on epoch=4
05/29/2022 14:21:48 - INFO - __main__ - Global step 100 Train loss 0.94 Classification-F1 0.18093327827266828 on epoch=4
05/29/2022 14:21:48 - INFO - __main__ - Saving model with best Classification-F1: 0.16601307189542483 -> 0.18093327827266828 on epoch=4, global_step=100
05/29/2022 14:21:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.69 on epoch=4
05/29/2022 14:21:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.70 on epoch=4
05/29/2022 14:21:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.63 on epoch=5
05/29/2022 14:21:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.72 on epoch=5
05/29/2022 14:22:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.61 on epoch=6
05/29/2022 14:22:09 - INFO - __main__ - Global step 150 Train loss 0.67 Classification-F1 0.16666666666666666 on epoch=6
05/29/2022 14:22:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.63 on epoch=6
05/29/2022 14:22:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.60 on epoch=7
05/29/2022 14:22:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.72 on epoch=7
05/29/2022 14:22:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=7
05/29/2022 14:22:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.55 on epoch=8
05/29/2022 14:22:31 - INFO - __main__ - Global step 200 Train loss 0.61 Classification-F1 0.2906172488251033 on epoch=8
05/29/2022 14:22:31 - INFO - __main__ - Saving model with best Classification-F1: 0.18093327827266828 -> 0.2906172488251033 on epoch=8, global_step=200
05/29/2022 14:22:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=8
05/29/2022 14:22:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.57 on epoch=9
05/29/2022 14:22:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.70 on epoch=9
05/29/2022 14:22:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=9
05/29/2022 14:22:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.60 on epoch=10
05/29/2022 14:22:52 - INFO - __main__ - Global step 250 Train loss 0.58 Classification-F1 0.16666666666666666 on epoch=10
05/29/2022 14:22:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=10
05/29/2022 14:22:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=11
05/29/2022 14:23:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.58 on epoch=11
05/29/2022 14:23:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=12
05/29/2022 14:23:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.52 on epoch=12
05/29/2022 14:23:15 - INFO - __main__ - Global step 300 Train loss 0.52 Classification-F1 0.19543297879854937 on epoch=12
05/29/2022 14:23:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.56 on epoch=12
05/29/2022 14:23:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.52 on epoch=13
05/29/2022 14:23:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.52 on epoch=13
05/29/2022 14:23:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=14
05/29/2022 14:23:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.59 on epoch=14
05/29/2022 14:23:39 - INFO - __main__ - Global step 350 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=14
05/29/2022 14:23:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=14
05/29/2022 14:23:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.53 on epoch=15
05/29/2022 14:23:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=15
05/29/2022 14:23:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=16
05/29/2022 14:23:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.54 on epoch=16
05/29/2022 14:24:02 - INFO - __main__ - Global step 400 Train loss 0.52 Classification-F1 0.16601307189542483 on epoch=16
05/29/2022 14:24:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.54 on epoch=17
05/29/2022 14:24:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.54 on epoch=17
05/29/2022 14:24:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=17
05/29/2022 14:24:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=18
05/29/2022 14:24:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.56 on epoch=18
05/29/2022 14:24:23 - INFO - __main__ - Global step 450 Train loss 0.53 Classification-F1 0.16568819308545338 on epoch=18
05/29/2022 14:24:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=19
05/29/2022 14:24:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.51 on epoch=19
05/29/2022 14:24:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.51 on epoch=19
05/29/2022 14:24:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.54 on epoch=20
05/29/2022 14:24:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.50 on epoch=20
05/29/2022 14:24:44 - INFO - __main__ - Global step 500 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=20
05/29/2022 14:24:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=21
05/29/2022 14:24:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.54 on epoch=21
05/29/2022 14:24:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.52 on epoch=22
05/29/2022 14:24:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.53 on epoch=22
05/29/2022 14:24:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.49 on epoch=22
05/29/2022 14:25:05 - INFO - __main__ - Global step 550 Train loss 0.52 Classification-F1 0.16601307189542483 on epoch=22
05/29/2022 14:25:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=23
05/29/2022 14:25:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.51 on epoch=23
05/29/2022 14:25:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=24
05/29/2022 14:25:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=24
05/29/2022 14:25:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.52 on epoch=24
05/29/2022 14:25:26 - INFO - __main__ - Global step 600 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=24
05/29/2022 14:25:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.56 on epoch=25
05/29/2022 14:25:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=25
05/29/2022 14:25:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=26
05/29/2022 14:25:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.54 on epoch=26
05/29/2022 14:25:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=27
05/29/2022 14:25:48 - INFO - __main__ - Global step 650 Train loss 0.50 Classification-F1 0.18137254901960784 on epoch=27
05/29/2022 14:25:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.56 on epoch=27
05/29/2022 14:25:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=27
05/29/2022 14:25:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=28
05/29/2022 14:25:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=28
05/29/2022 14:26:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.56 on epoch=29
05/29/2022 14:26:09 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.16568819308545338 on epoch=29
05/29/2022 14:26:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.53 on epoch=29
05/29/2022 14:26:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.49 on epoch=29
05/29/2022 14:26:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=30
05/29/2022 14:26:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.48 on epoch=30
05/29/2022 14:26:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.46 on epoch=31
05/29/2022 14:26:32 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.16699282452707112 on epoch=31
05/29/2022 14:26:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.55 on epoch=31
05/29/2022 14:26:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=32
05/29/2022 14:26:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.46 on epoch=32
05/29/2022 14:26:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.54 on epoch=32
05/29/2022 14:26:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.50 on epoch=33
05/29/2022 14:26:54 - INFO - __main__ - Global step 800 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=33
05/29/2022 14:26:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.51 on epoch=33
05/29/2022 14:26:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=34
05/29/2022 14:27:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=34
05/29/2022 14:27:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=34
05/29/2022 14:27:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.46 on epoch=35
05/29/2022 14:27:15 - INFO - __main__ - Global step 850 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=35
05/29/2022 14:27:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.46 on epoch=35
05/29/2022 14:27:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.48 on epoch=36
05/29/2022 14:27:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.52 on epoch=36
05/29/2022 14:27:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.47 on epoch=37
05/29/2022 14:27:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.55 on epoch=37
05/29/2022 14:27:36 - INFO - __main__ - Global step 900 Train loss 0.50 Classification-F1 0.2120047345364913 on epoch=37
05/29/2022 14:27:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.50 on epoch=37
05/29/2022 14:27:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=38
05/29/2022 14:27:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.51 on epoch=38
05/29/2022 14:27:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=39
05/29/2022 14:27:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.46 on epoch=39
05/29/2022 14:27:58 - INFO - __main__ - Global step 950 Train loss 0.49 Classification-F1 0.1852566234342091 on epoch=39
05/29/2022 14:28:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.48 on epoch=39
05/29/2022 14:28:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=40
05/29/2022 14:28:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.45 on epoch=40
05/29/2022 14:28:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=41
05/29/2022 14:28:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=41
05/29/2022 14:28:20 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.16568819308545338 on epoch=41
05/29/2022 14:28:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.47 on epoch=42
05/29/2022 14:28:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.49 on epoch=42
05/29/2022 14:28:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=42
05/29/2022 14:28:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.50 on epoch=43
05/29/2022 14:28:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.49 on epoch=43
05/29/2022 14:28:44 - INFO - __main__ - Global step 1050 Train loss 0.48 Classification-F1 0.20153854856043893 on epoch=43
05/29/2022 14:28:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=44
05/29/2022 14:28:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.47 on epoch=44
05/29/2022 14:28:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.47 on epoch=44
05/29/2022 14:28:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=45
05/29/2022 14:28:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.47 on epoch=45
05/29/2022 14:29:08 - INFO - __main__ - Global step 1100 Train loss 0.46 Classification-F1 0.1738394526714039 on epoch=45
05/29/2022 14:29:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.50 on epoch=46
05/29/2022 14:29:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.49 on epoch=46
05/29/2022 14:29:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=47
05/29/2022 14:29:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=47
05/29/2022 14:29:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=47
05/29/2022 14:29:32 - INFO - __main__ - Global step 1150 Train loss 0.47 Classification-F1 0.3082842605884823 on epoch=47
05/29/2022 14:29:32 - INFO - __main__ - Saving model with best Classification-F1: 0.2906172488251033 -> 0.3082842605884823 on epoch=47, global_step=1150
05/29/2022 14:29:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.50 on epoch=48
05/29/2022 14:29:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=48
05/29/2022 14:29:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.51 on epoch=49
05/29/2022 14:29:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.47 on epoch=49
05/29/2022 14:29:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=49
05/29/2022 14:29:56 - INFO - __main__ - Global step 1200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=49
05/29/2022 14:29:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=50
05/29/2022 14:30:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.51 on epoch=50
05/29/2022 14:30:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.48 on epoch=51
05/29/2022 14:30:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.53 on epoch=51
05/29/2022 14:30:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=52
05/29/2022 14:30:20 - INFO - __main__ - Global step 1250 Train loss 0.48 Classification-F1 0.16568819308545338 on epoch=52
05/29/2022 14:30:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.50 on epoch=52
05/29/2022 14:30:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=52
05/29/2022 14:30:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=53
05/29/2022 14:30:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.48 on epoch=53
05/29/2022 14:30:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.52 on epoch=54
05/29/2022 14:30:45 - INFO - __main__ - Global step 1300 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=54
05/29/2022 14:30:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.49 on epoch=54
05/29/2022 14:30:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.46 on epoch=54
05/29/2022 14:30:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.49 on epoch=55
05/29/2022 14:30:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=55
05/29/2022 14:30:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.51 on epoch=56
05/29/2022 14:31:09 - INFO - __main__ - Global step 1350 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=56
05/29/2022 14:31:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.50 on epoch=56
05/29/2022 14:31:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.44 on epoch=57
05/29/2022 14:31:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.49 on epoch=57
05/29/2022 14:31:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.47 on epoch=57
05/29/2022 14:31:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.48 on epoch=58
05/29/2022 14:31:34 - INFO - __main__ - Global step 1400 Train loss 0.48 Classification-F1 0.24868334430546413 on epoch=58
05/29/2022 14:31:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=58
05/29/2022 14:31:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=59
05/29/2022 14:31:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.48 on epoch=59
05/29/2022 14:31:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=59
05/29/2022 14:31:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.43 on epoch=60
05/29/2022 14:31:57 - INFO - __main__ - Global step 1450 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=60
05/29/2022 14:32:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=60
05/29/2022 14:32:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.47 on epoch=61
05/29/2022 14:32:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.45 on epoch=61
05/29/2022 14:32:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.44 on epoch=62
05/29/2022 14:32:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.50 on epoch=62
05/29/2022 14:32:20 - INFO - __main__ - Global step 1500 Train loss 0.46 Classification-F1 0.18595146871008938 on epoch=62
05/29/2022 14:32:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=62
05/29/2022 14:32:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.48 on epoch=63
05/29/2022 14:32:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.49 on epoch=63
05/29/2022 14:32:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.48 on epoch=64
05/29/2022 14:32:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=64
05/29/2022 14:32:44 - INFO - __main__ - Global step 1550 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=64
05/29/2022 14:32:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.45 on epoch=64
05/29/2022 14:32:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.44 on epoch=65
05/29/2022 14:32:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.46 on epoch=65
05/29/2022 14:32:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.45 on epoch=66
05/29/2022 14:32:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=66
05/29/2022 14:33:09 - INFO - __main__ - Global step 1600 Train loss 0.45 Classification-F1 0.17204590281513357 on epoch=66
05/29/2022 14:33:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=67
05/29/2022 14:33:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.49 on epoch=67
05/29/2022 14:33:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=67
05/29/2022 14:33:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.43 on epoch=68
05/29/2022 14:33:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.47 on epoch=68
05/29/2022 14:33:34 - INFO - __main__ - Global step 1650 Train loss 0.46 Classification-F1 0.22495544058684658 on epoch=68
05/29/2022 14:33:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.47 on epoch=69
05/29/2022 14:33:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.54 on epoch=69
05/29/2022 14:33:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.46 on epoch=69
05/29/2022 14:33:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.47 on epoch=70
05/29/2022 14:33:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.42 on epoch=70
05/29/2022 14:33:58 - INFO - __main__ - Global step 1700 Train loss 0.47 Classification-F1 0.29534345253408795 on epoch=70
05/29/2022 14:34:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=71
05/29/2022 14:34:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.50 on epoch=71
05/29/2022 14:34:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.42 on epoch=72
05/29/2022 14:34:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.47 on epoch=72
05/29/2022 14:34:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.44 on epoch=72
05/29/2022 14:34:23 - INFO - __main__ - Global step 1750 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=72
05/29/2022 14:34:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.50 on epoch=73
05/29/2022 14:34:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=73
05/29/2022 14:34:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.44 on epoch=74
05/29/2022 14:34:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.44 on epoch=74
05/29/2022 14:34:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=74
05/29/2022 14:34:47 - INFO - __main__ - Global step 1800 Train loss 0.45 Classification-F1 0.16568819308545338 on epoch=74
05/29/2022 14:34:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.42 on epoch=75
05/29/2022 14:34:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.46 on epoch=75
05/29/2022 14:34:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.41 on epoch=76
05/29/2022 14:34:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.49 on epoch=76
05/29/2022 14:35:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=77
05/29/2022 14:35:12 - INFO - __main__ - Global step 1850 Train loss 0.44 Classification-F1 0.2645356523782942 on epoch=77
05/29/2022 14:35:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.49 on epoch=77
05/29/2022 14:35:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=77
05/29/2022 14:35:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.46 on epoch=78
05/29/2022 14:35:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.45 on epoch=78
05/29/2022 14:35:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=79
05/29/2022 14:35:32 - INFO - __main__ - Global step 1900 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=79
05/29/2022 14:35:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.43 on epoch=79
05/29/2022 14:35:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.46 on epoch=79
05/29/2022 14:35:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.52 on epoch=80
05/29/2022 14:35:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=80
05/29/2022 14:35:46 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.44 on epoch=81
05/29/2022 14:35:57 - INFO - __main__ - Global step 1950 Train loss 0.45 Classification-F1 0.27214611872146116 on epoch=81
05/29/2022 14:36:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.46 on epoch=81
05/29/2022 14:36:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.46 on epoch=82
05/29/2022 14:36:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=82
05/29/2022 14:36:08 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.46 on epoch=82
05/29/2022 14:36:10 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.48 on epoch=83
05/29/2022 14:36:22 - INFO - __main__ - Global step 2000 Train loss 0.46 Classification-F1 0.21043083900226758 on epoch=83
05/29/2022 14:36:24 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.47 on epoch=83
05/29/2022 14:36:27 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.42 on epoch=84
05/29/2022 14:36:30 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.41 on epoch=84
05/29/2022 14:36:32 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.41 on epoch=84
05/29/2022 14:36:35 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.48 on epoch=85
05/29/2022 14:36:46 - INFO - __main__ - Global step 2050 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=85
05/29/2022 14:36:49 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.44 on epoch=85
05/29/2022 14:36:51 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.51 on epoch=86
05/29/2022 14:36:54 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.47 on epoch=86
05/29/2022 14:36:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.39 on epoch=87
05/29/2022 14:36:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.49 on epoch=87
05/29/2022 14:37:11 - INFO - __main__ - Global step 2100 Train loss 0.46 Classification-F1 0.32170443935149823 on epoch=87
05/29/2022 14:37:11 - INFO - __main__ - Saving model with best Classification-F1: 0.3082842605884823 -> 0.32170443935149823 on epoch=87, global_step=2100
05/29/2022 14:37:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.44 on epoch=87
05/29/2022 14:37:16 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.46 on epoch=88
05/29/2022 14:37:19 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.45 on epoch=88
05/29/2022 14:37:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.50 on epoch=89
05/29/2022 14:37:24 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.46 on epoch=89
05/29/2022 14:37:35 - INFO - __main__ - Global step 2150 Train loss 0.46 Classification-F1 0.16568819308545338 on epoch=89
05/29/2022 14:37:38 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.45 on epoch=89
05/29/2022 14:37:41 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.44 on epoch=90
05/29/2022 14:37:43 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.49 on epoch=90
05/29/2022 14:37:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.47 on epoch=91
05/29/2022 14:37:48 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.47 on epoch=91
05/29/2022 14:38:00 - INFO - __main__ - Global step 2200 Train loss 0.46 Classification-F1 0.24313654423613915 on epoch=91
05/29/2022 14:38:03 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.46 on epoch=92
05/29/2022 14:38:05 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.51 on epoch=92
05/29/2022 14:38:08 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.41 on epoch=92
05/29/2022 14:38:11 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.45 on epoch=93
05/29/2022 14:38:13 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.45 on epoch=93
05/29/2022 14:38:25 - INFO - __main__ - Global step 2250 Train loss 0.46 Classification-F1 0.19064132994120628 on epoch=93
05/29/2022 14:38:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.47 on epoch=94
05/29/2022 14:38:30 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.46 on epoch=94
05/29/2022 14:38:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.52 on epoch=94
05/29/2022 14:38:36 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.49 on epoch=95
05/29/2022 14:38:38 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.45 on epoch=95
05/29/2022 14:38:50 - INFO - __main__ - Global step 2300 Train loss 0.48 Classification-F1 0.2761963921633976 on epoch=95
05/29/2022 14:38:52 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.42 on epoch=96
05/29/2022 14:38:55 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.45 on epoch=96
05/29/2022 14:38:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.44 on epoch=97
05/29/2022 14:39:00 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.47 on epoch=97
05/29/2022 14:39:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.42 on epoch=97
05/29/2022 14:39:14 - INFO - __main__ - Global step 2350 Train loss 0.44 Classification-F1 0.32573548515577505 on epoch=97
05/29/2022 14:39:14 - INFO - __main__ - Saving model with best Classification-F1: 0.32170443935149823 -> 0.32573548515577505 on epoch=97, global_step=2350
05/29/2022 14:39:17 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.46 on epoch=98
05/29/2022 14:39:19 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.41 on epoch=98
05/29/2022 14:39:22 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.45 on epoch=99
05/29/2022 14:39:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.45 on epoch=99
05/29/2022 14:39:27 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.43 on epoch=99
05/29/2022 14:39:38 - INFO - __main__ - Global step 2400 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=99
05/29/2022 14:39:41 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.42 on epoch=100
05/29/2022 14:39:44 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.44 on epoch=100
05/29/2022 14:39:46 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.42 on epoch=101
05/29/2022 14:39:49 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.47 on epoch=101
05/29/2022 14:39:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.45 on epoch=102
05/29/2022 14:40:03 - INFO - __main__ - Global step 2450 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=102
05/29/2022 14:40:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.46 on epoch=102
05/29/2022 14:40:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.43 on epoch=102
05/29/2022 14:40:11 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.48 on epoch=103
05/29/2022 14:40:13 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.45 on epoch=103
05/29/2022 14:40:16 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.41 on epoch=104
05/29/2022 14:40:28 - INFO - __main__ - Global step 2500 Train loss 0.44 Classification-F1 0.2769787834469003 on epoch=104
05/29/2022 14:40:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.47 on epoch=104
05/29/2022 14:40:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.43 on epoch=104
05/29/2022 14:40:36 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.44 on epoch=105
05/29/2022 14:40:38 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.41 on epoch=105
05/29/2022 14:40:41 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.42 on epoch=106
05/29/2022 14:40:53 - INFO - __main__ - Global step 2550 Train loss 0.43 Classification-F1 0.3028804474626952 on epoch=106
05/29/2022 14:40:55 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.49 on epoch=106
05/29/2022 14:40:58 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.43 on epoch=107
05/29/2022 14:41:01 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.49 on epoch=107
05/29/2022 14:41:03 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.43 on epoch=107
05/29/2022 14:41:06 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.41 on epoch=108
05/29/2022 14:41:18 - INFO - __main__ - Global step 2600 Train loss 0.45 Classification-F1 0.2648749154834347 on epoch=108
05/29/2022 14:41:20 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.48 on epoch=108
05/29/2022 14:41:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.43 on epoch=109
05/29/2022 14:41:26 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.42 on epoch=109
05/29/2022 14:41:28 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.41 on epoch=109
05/29/2022 14:41:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.44 on epoch=110
05/29/2022 14:41:42 - INFO - __main__ - Global step 2650 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=110
05/29/2022 14:41:45 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.43 on epoch=110
05/29/2022 14:41:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.44 on epoch=111
05/29/2022 14:41:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.49 on epoch=111
05/29/2022 14:41:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.44 on epoch=112
05/29/2022 14:41:55 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.48 on epoch=112
05/29/2022 14:42:07 - INFO - __main__ - Global step 2700 Train loss 0.46 Classification-F1 0.1741221741221741 on epoch=112
05/29/2022 14:42:10 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.46 on epoch=112
05/29/2022 14:42:12 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.43 on epoch=113
05/29/2022 14:42:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.46 on epoch=113
05/29/2022 14:42:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.45 on epoch=114
05/29/2022 14:42:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.47 on epoch=114
05/29/2022 14:42:31 - INFO - __main__ - Global step 2750 Train loss 0.46 Classification-F1 0.21839149948483919 on epoch=114
05/29/2022 14:42:34 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.41 on epoch=114
05/29/2022 14:42:37 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.44 on epoch=115
05/29/2022 14:42:39 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.44 on epoch=115
05/29/2022 14:42:42 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.46 on epoch=116
05/29/2022 14:42:45 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.52 on epoch=116
05/29/2022 14:42:57 - INFO - __main__ - Global step 2800 Train loss 0.45 Classification-F1 0.22351361124024996 on epoch=116
05/29/2022 14:42:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.41 on epoch=117
05/29/2022 14:43:02 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.45 on epoch=117
05/29/2022 14:43:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.45 on epoch=117
05/29/2022 14:43:07 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.45 on epoch=118
05/29/2022 14:43:10 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.46 on epoch=118
05/29/2022 14:43:21 - INFO - __main__ - Global step 2850 Train loss 0.44 Classification-F1 0.2240390131956397 on epoch=118
05/29/2022 14:43:24 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.44 on epoch=119
05/29/2022 14:43:27 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.41 on epoch=119
05/29/2022 14:43:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.40 on epoch=119
05/29/2022 14:43:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.51 on epoch=120
05/29/2022 14:43:35 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.43 on epoch=120
05/29/2022 14:43:46 - INFO - __main__ - Global step 2900 Train loss 0.44 Classification-F1 0.27496625988983286 on epoch=120
05/29/2022 14:43:49 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.51 on epoch=121
05/29/2022 14:43:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.47 on epoch=121
05/29/2022 14:43:54 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.43 on epoch=122
05/29/2022 14:43:57 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.49 on epoch=122
05/29/2022 14:43:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.43 on epoch=122
05/29/2022 14:44:11 - INFO - __main__ - Global step 2950 Train loss 0.46 Classification-F1 0.24320773994453596 on epoch=122
05/29/2022 14:44:13 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.38 on epoch=123
05/29/2022 14:44:16 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.42 on epoch=123
05/29/2022 14:44:19 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.47 on epoch=124
05/29/2022 14:44:21 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.44 on epoch=124
05/29/2022 14:44:24 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.44 on epoch=124
05/29/2022 14:44:25 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 14:44:25 - INFO - __main__ - Printing 3 examples
05/29/2022 14:44:25 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/29/2022 14:44:25 - INFO - __main__ - ['contradiction']
05/29/2022 14:44:25 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/29/2022 14:44:25 - INFO - __main__ - ['contradiction']
05/29/2022 14:44:25 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/29/2022 14:44:25 - INFO - __main__ - ['contradiction']
05/29/2022 14:44:25 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:44:26 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:44:26 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 14:44:26 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 14:44:26 - INFO - __main__ - Printing 3 examples
05/29/2022 14:44:26 - INFO - __main__ -  [anli] premise: Dexter Alexander Nottage (born November 14, 1970) is a former American football defensive end in the National Football League (NFL) for the Washington Redskins and the Kansas City Chiefs. He played college football at Florida A&M University and was selected in the sixth round of the 1994 NFL Draft. He played high school football at Hollywood Hills High School. [SEP] hypothesis: Dexter Alexander Nottage (born November 14, 1970) is a former power forward basketball in the NBA.
05/29/2022 14:44:26 - INFO - __main__ - ['contradiction']
05/29/2022 14:44:26 - INFO - __main__ -  [anli] premise: A grasshopper is a sweet, mint-flavored, after-dinner drink. The name of the drink derives from its green color, which comes from crème de menthe. The drink reputedly originated at Tujague's, a landmark bar in the French Quarter of New Orleans, Louisiana, and was invented by its owner, Philip Guichet. The drink gained popularity during the 1950s and 1960s throughout the American South. [SEP] hypothesis: The drink is white
05/29/2022 14:44:26 - INFO - __main__ - ['contradiction']
05/29/2022 14:44:26 - INFO - __main__ -  [anli] premise: Denis Hale Johnson (July 1, 1949 – May 24, 2017) was an American writer best known for his short story collection "Jesus' Son" (1992) and his novel "Tree of Smoke" (2007), which won the National Book Award for Fiction. He also wrote plays, poetry, journalism, and non-fiction. [SEP] hypothesis: Denis Hale Johnson (July 1, 1949 – May 24, 2010) was an American writer best known for his short story collection "Jesus' Son"
05/29/2022 14:44:26 - INFO - __main__ - ['contradiction']
05/29/2022 14:44:26 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:44:26 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:44:27 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 14:44:35 - INFO - __main__ - Global step 3000 Train loss 0.43 Classification-F1 0.17146743237116516 on epoch=124
05/29/2022 14:44:35 - INFO - __main__ - save last model!
05/29/2022 14:44:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 14:44:35 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 14:44:35 - INFO - __main__ - Printing 3 examples
05/29/2022 14:44:35 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 14:44:35 - INFO - __main__ - ['contradiction']
05/29/2022 14:44:35 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 14:44:35 - INFO - __main__ - ['entailment']
05/29/2022 14:44:35 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 14:44:35 - INFO - __main__ - ['contradiction']
05/29/2022 14:44:35 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:44:36 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:44:37 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 14:44:43 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 14:44:43 - INFO - __main__ - task name: anli
05/29/2022 14:44:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 14:44:43 - INFO - __main__ - Starting training!
05/29/2022 14:45:06 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_100_0.2_8_predictions.txt
05/29/2022 14:45:06 - INFO - __main__ - Classification-F1 on test data: 0.1826
05/29/2022 14:45:06 - INFO - __main__ - prefix=anli_128_100, lr=0.2, bsz=8, dev_performance=0.32573548515577505, test_performance=0.18256774583451127
05/29/2022 14:45:06 - INFO - __main__ - Running ... prefix=anli_128_13, lr=0.5, bsz=8 ...
05/29/2022 14:45:07 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 14:45:07 - INFO - __main__ - Printing 3 examples
05/29/2022 14:45:07 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/29/2022 14:45:07 - INFO - __main__ - ['contradiction']
05/29/2022 14:45:07 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/29/2022 14:45:07 - INFO - __main__ - ['contradiction']
05/29/2022 14:45:07 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/29/2022 14:45:07 - INFO - __main__ - ['contradiction']
05/29/2022 14:45:07 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:45:07 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:45:07 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 14:45:07 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 14:45:07 - INFO - __main__ - Printing 3 examples
05/29/2022 14:45:07 - INFO - __main__ -  [anli] premise: Dexter Alexander Nottage (born November 14, 1970) is a former American football defensive end in the National Football League (NFL) for the Washington Redskins and the Kansas City Chiefs. He played college football at Florida A&M University and was selected in the sixth round of the 1994 NFL Draft. He played high school football at Hollywood Hills High School. [SEP] hypothesis: Dexter Alexander Nottage (born November 14, 1970) is a former power forward basketball in the NBA.
05/29/2022 14:45:07 - INFO - __main__ - ['contradiction']
05/29/2022 14:45:07 - INFO - __main__ -  [anli] premise: A grasshopper is a sweet, mint-flavored, after-dinner drink. The name of the drink derives from its green color, which comes from crème de menthe. The drink reputedly originated at Tujague's, a landmark bar in the French Quarter of New Orleans, Louisiana, and was invented by its owner, Philip Guichet. The drink gained popularity during the 1950s and 1960s throughout the American South. [SEP] hypothesis: The drink is white
05/29/2022 14:45:07 - INFO - __main__ - ['contradiction']
05/29/2022 14:45:07 - INFO - __main__ -  [anli] premise: Denis Hale Johnson (July 1, 1949 – May 24, 2017) was an American writer best known for his short story collection "Jesus' Son" (1992) and his novel "Tree of Smoke" (2007), which won the National Book Award for Fiction. He also wrote plays, poetry, journalism, and non-fiction. [SEP] hypothesis: Denis Hale Johnson (July 1, 1949 – May 24, 2010) was an American writer best known for his short story collection "Jesus' Son"
05/29/2022 14:45:07 - INFO - __main__ - ['contradiction']
05/29/2022 14:45:07 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:45:08 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:45:08 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 14:45:24 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 14:45:24 - INFO - __main__ - task name: anli
05/29/2022 14:45:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 14:45:25 - INFO - __main__ - Starting training!
05/29/2022 14:45:28 - INFO - __main__ - Step 10 Global step 10 Train loss 6.07 on epoch=0
05/29/2022 14:45:31 - INFO - __main__ - Step 20 Global step 20 Train loss 2.08 on epoch=0
05/29/2022 14:45:33 - INFO - __main__ - Step 30 Global step 30 Train loss 1.11 on epoch=1
05/29/2022 14:45:36 - INFO - __main__ - Step 40 Global step 40 Train loss 0.90 on epoch=1
05/29/2022 14:45:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.75 on epoch=2
05/29/2022 14:45:47 - INFO - __main__ - Global step 50 Train loss 2.18 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 14:45:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 14:45:50 - INFO - __main__ - Step 60 Global step 60 Train loss 0.77 on epoch=2
05/29/2022 14:45:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.68 on epoch=2
05/29/2022 14:45:55 - INFO - __main__ - Step 80 Global step 80 Train loss 0.71 on epoch=3
05/29/2022 14:45:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.61 on epoch=3
05/29/2022 14:46:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.65 on epoch=4
05/29/2022 14:46:08 - INFO - __main__ - Global step 100 Train loss 0.69 Classification-F1 0.16568819308545338 on epoch=4
05/29/2022 14:46:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.65 on epoch=4
05/29/2022 14:46:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.61 on epoch=4
05/29/2022 14:46:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.67 on epoch=5
05/29/2022 14:46:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.55 on epoch=5
05/29/2022 14:46:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.61 on epoch=6
05/29/2022 14:46:33 - INFO - __main__ - Global step 150 Train loss 0.62 Classification-F1 0.2527742930569652 on epoch=6
05/29/2022 14:46:33 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2527742930569652 on epoch=6, global_step=150
05/29/2022 14:46:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=6
05/29/2022 14:46:38 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=7
05/29/2022 14:46:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.54 on epoch=7
05/29/2022 14:46:43 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=7
05/29/2022 14:46:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.63 on epoch=8
05/29/2022 14:46:57 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=8
05/29/2022 14:47:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=8
05/29/2022 14:47:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.58 on epoch=9
05/29/2022 14:47:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.53 on epoch=9
05/29/2022 14:47:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.53 on epoch=9
05/29/2022 14:47:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=10
05/29/2022 14:47:21 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=10
05/29/2022 14:47:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.52 on epoch=10
05/29/2022 14:47:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.54 on epoch=11
05/29/2022 14:47:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=11
05/29/2022 14:47:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=12
05/29/2022 14:47:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=12
05/29/2022 14:47:46 - INFO - __main__ - Global step 300 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=12
05/29/2022 14:47:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=12
05/29/2022 14:47:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.55 on epoch=13
05/29/2022 14:47:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=13
05/29/2022 14:47:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=14
05/29/2022 14:47:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.52 on epoch=14
05/29/2022 14:48:10 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=14
05/29/2022 14:48:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=14
05/29/2022 14:48:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=15
05/29/2022 14:48:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.49 on epoch=15
05/29/2022 14:48:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.51 on epoch=16
05/29/2022 14:48:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=16
05/29/2022 14:48:35 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.247960493545595 on epoch=16
05/29/2022 14:48:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.51 on epoch=17
05/29/2022 14:48:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.56 on epoch=17
05/29/2022 14:48:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=17
05/29/2022 14:48:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.58 on epoch=18
05/29/2022 14:48:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=18
05/29/2022 14:48:59 - INFO - __main__ - Global step 450 Train loss 0.52 Classification-F1 0.16568819308545338 on epoch=18
05/29/2022 14:49:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=19
05/29/2022 14:49:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.55 on epoch=19
05/29/2022 14:49:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=19
05/29/2022 14:49:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=20
05/29/2022 14:49:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=20
05/29/2022 14:49:24 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=20
05/29/2022 14:49:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.52 on epoch=21
05/29/2022 14:49:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.56 on epoch=21
05/29/2022 14:49:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=22
05/29/2022 14:49:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.51 on epoch=22
05/29/2022 14:49:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=22
05/29/2022 14:49:48 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=22
05/29/2022 14:49:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=23
05/29/2022 14:49:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.49 on epoch=23
05/29/2022 14:49:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=24
05/29/2022 14:49:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=24
05/29/2022 14:50:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=24
05/29/2022 14:50:14 - INFO - __main__ - Global step 600 Train loss 0.48 Classification-F1 0.21813754557727005 on epoch=24
05/29/2022 14:50:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.55 on epoch=25
05/29/2022 14:50:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=25
05/29/2022 14:50:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=26
05/29/2022 14:50:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.49 on epoch=26
05/29/2022 14:50:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.47 on epoch=27
05/29/2022 14:50:38 - INFO - __main__ - Global step 650 Train loss 0.48 Classification-F1 0.27130368761270046 on epoch=27
05/29/2022 14:50:38 - INFO - __main__ - Saving model with best Classification-F1: 0.2527742930569652 -> 0.27130368761270046 on epoch=27, global_step=650
05/29/2022 14:50:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.51 on epoch=27
05/29/2022 14:50:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.49 on epoch=27
05/29/2022 14:50:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=28
05/29/2022 14:50:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.50 on epoch=28
05/29/2022 14:50:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.53 on epoch=29
05/29/2022 14:51:03 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.2846419555608216 on epoch=29
05/29/2022 14:51:03 - INFO - __main__ - Saving model with best Classification-F1: 0.27130368761270046 -> 0.2846419555608216 on epoch=29, global_step=700
05/29/2022 14:51:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=29
05/29/2022 14:51:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.50 on epoch=29
05/29/2022 14:51:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=30
05/29/2022 14:51:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.48 on epoch=30
05/29/2022 14:51:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=31
05/29/2022 14:51:28 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.2771677525697039 on epoch=31
05/29/2022 14:51:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=31
05/29/2022 14:51:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=32
05/29/2022 14:51:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.55 on epoch=32
05/29/2022 14:51:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=32
05/29/2022 14:51:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.51 on epoch=33
05/29/2022 14:51:52 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.23979591836734696 on epoch=33
05/29/2022 14:51:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=33
05/29/2022 14:51:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.51 on epoch=34
05/29/2022 14:52:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.49 on epoch=34
05/29/2022 14:52:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=34
05/29/2022 14:52:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=35
05/29/2022 14:52:16 - INFO - __main__ - Global step 850 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=35
05/29/2022 14:52:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.48 on epoch=35
05/29/2022 14:52:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.49 on epoch=36
05/29/2022 14:52:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.48 on epoch=36
05/29/2022 14:52:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=37
05/29/2022 14:52:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.46 on epoch=37
05/29/2022 14:52:41 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.18291616051030557 on epoch=37
05/29/2022 14:52:43 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=37
05/29/2022 14:52:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.50 on epoch=38
05/29/2022 14:52:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=38
05/29/2022 14:52:51 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=39
05/29/2022 14:52:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=39
05/29/2022 14:53:05 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=39
05/29/2022 14:53:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.48 on epoch=39
05/29/2022 14:53:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.50 on epoch=40
05/29/2022 14:53:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=40
05/29/2022 14:53:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=41
05/29/2022 14:53:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=41
05/29/2022 14:53:30 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.25644364241456874 on epoch=41
05/29/2022 14:53:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=42
05/29/2022 14:53:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.49 on epoch=42
05/29/2022 14:53:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.47 on epoch=42
05/29/2022 14:53:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=43
05/29/2022 14:53:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.46 on epoch=43
05/29/2022 14:53:54 - INFO - __main__ - Global step 1050 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=43
05/29/2022 14:53:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.47 on epoch=44
05/29/2022 14:53:59 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=44
05/29/2022 14:54:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.52 on epoch=44
05/29/2022 14:54:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.48 on epoch=45
05/29/2022 14:54:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.46 on epoch=45
05/29/2022 14:54:18 - INFO - __main__ - Global step 1100 Train loss 0.48 Classification-F1 0.1913218414105093 on epoch=45
05/29/2022 14:54:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.47 on epoch=46
05/29/2022 14:54:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=46
05/29/2022 14:54:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=47
05/29/2022 14:54:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.49 on epoch=47
05/29/2022 14:54:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=47
05/29/2022 14:54:42 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=47
05/29/2022 14:54:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=48
05/29/2022 14:54:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.47 on epoch=48
05/29/2022 14:54:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.46 on epoch=49
05/29/2022 14:54:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.48 on epoch=49
05/29/2022 14:54:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.43 on epoch=49
05/29/2022 14:55:06 - INFO - __main__ - Global step 1200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=49
05/29/2022 14:55:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.46 on epoch=50
05/29/2022 14:55:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.44 on epoch=50
05/29/2022 14:55:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=51
05/29/2022 14:55:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.48 on epoch=51
05/29/2022 14:55:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=52
05/29/2022 14:55:31 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.2386569280904355 on epoch=52
05/29/2022 14:55:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.47 on epoch=52
05/29/2022 14:55:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.40 on epoch=52
05/29/2022 14:55:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.47 on epoch=53
05/29/2022 14:55:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=53
05/29/2022 14:55:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.48 on epoch=54
05/29/2022 14:55:56 - INFO - __main__ - Global step 1300 Train loss 0.45 Classification-F1 0.24667305518698704 on epoch=54
05/29/2022 14:55:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=54
05/29/2022 14:56:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=54
05/29/2022 14:56:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.49 on epoch=55
05/29/2022 14:56:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.44 on epoch=55
05/29/2022 14:56:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.49 on epoch=56
05/29/2022 14:56:20 - INFO - __main__ - Global step 1350 Train loss 0.46 Classification-F1 0.2415982215689104 on epoch=56
05/29/2022 14:56:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.50 on epoch=56
05/29/2022 14:56:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=57
05/29/2022 14:56:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.49 on epoch=57
05/29/2022 14:56:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=57
05/29/2022 14:56:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.47 on epoch=58
05/29/2022 14:56:45 - INFO - __main__ - Global step 1400 Train loss 0.45 Classification-F1 0.23090277777777776 on epoch=58
05/29/2022 14:56:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=58
05/29/2022 14:56:50 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=59
05/29/2022 14:56:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=59
05/29/2022 14:56:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=59
05/29/2022 14:56:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.51 on epoch=60
05/29/2022 14:57:09 - INFO - __main__ - Global step 1450 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=60
05/29/2022 14:57:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=60
05/29/2022 14:57:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=61
05/29/2022 14:57:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.42 on epoch=61
05/29/2022 14:57:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=62
05/29/2022 14:57:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.48 on epoch=62
05/29/2022 14:57:34 - INFO - __main__ - Global step 1500 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=62
05/29/2022 14:57:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=62
05/29/2022 14:57:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.44 on epoch=63
05/29/2022 14:57:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.44 on epoch=63
05/29/2022 14:57:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.48 on epoch=64
05/29/2022 14:57:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.44 on epoch=64
05/29/2022 14:57:58 - INFO - __main__ - Global step 1550 Train loss 0.45 Classification-F1 0.26536913101609255 on epoch=64
05/29/2022 14:58:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=64
05/29/2022 14:58:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.46 on epoch=65
05/29/2022 14:58:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.44 on epoch=65
05/29/2022 14:58:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.45 on epoch=66
05/29/2022 14:58:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.47 on epoch=66
05/29/2022 14:58:23 - INFO - __main__ - Global step 1600 Train loss 0.45 Classification-F1 0.22021346765005692 on epoch=66
05/29/2022 14:58:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.44 on epoch=67
05/29/2022 14:58:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.51 on epoch=67
05/29/2022 14:58:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=67
05/29/2022 14:58:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.42 on epoch=68
05/29/2022 14:58:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.47 on epoch=68
05/29/2022 14:58:49 - INFO - __main__ - Global step 1650 Train loss 0.45 Classification-F1 0.23614880665854146 on epoch=68
05/29/2022 14:58:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.46 on epoch=69
05/29/2022 14:58:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=69
05/29/2022 14:58:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.39 on epoch=69
05/29/2022 14:59:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.46 on epoch=70
05/29/2022 14:59:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.41 on epoch=70
05/29/2022 14:59:14 - INFO - __main__ - Global step 1700 Train loss 0.43 Classification-F1 0.23175808293190134 on epoch=70
05/29/2022 14:59:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.46 on epoch=71
05/29/2022 14:59:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.48 on epoch=71
05/29/2022 14:59:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.42 on epoch=72
05/29/2022 14:59:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.48 on epoch=72
05/29/2022 14:59:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.45 on epoch=72
05/29/2022 14:59:39 - INFO - __main__ - Global step 1750 Train loss 0.46 Classification-F1 0.1966058454883257 on epoch=72
05/29/2022 14:59:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.46 on epoch=73
05/29/2022 14:59:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.42 on epoch=73
05/29/2022 14:59:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.48 on epoch=74
05/29/2022 14:59:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.46 on epoch=74
05/29/2022 14:59:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=74
05/29/2022 15:00:05 - INFO - __main__ - Global step 1800 Train loss 0.46 Classification-F1 0.27395389719237245 on epoch=74
05/29/2022 15:00:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.43 on epoch=75
05/29/2022 15:00:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.43 on epoch=75
05/29/2022 15:00:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.49 on epoch=76
05/29/2022 15:00:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=76
05/29/2022 15:00:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=77
05/29/2022 15:00:30 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.2644851444851445 on epoch=77
05/29/2022 15:00:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=77
05/29/2022 15:00:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.44 on epoch=77
05/29/2022 15:00:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.48 on epoch=78
05/29/2022 15:00:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.42 on epoch=78
05/29/2022 15:00:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=79
05/29/2022 15:00:54 - INFO - __main__ - Global step 1900 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=79
05/29/2022 15:00:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.45 on epoch=79
05/29/2022 15:01:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.44 on epoch=79
05/29/2022 15:01:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.49 on epoch=80
05/29/2022 15:01:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.39 on epoch=80
05/29/2022 15:01:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=81
05/29/2022 15:01:20 - INFO - __main__ - Global step 1950 Train loss 0.43 Classification-F1 0.2969511330440455 on epoch=81
05/29/2022 15:01:20 - INFO - __main__ - Saving model with best Classification-F1: 0.2846419555608216 -> 0.2969511330440455 on epoch=81, global_step=1950
05/29/2022 15:01:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=81
05/29/2022 15:01:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=82
05/29/2022 15:01:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.48 on epoch=82
05/29/2022 15:01:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.48 on epoch=82
05/29/2022 15:01:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.45 on epoch=83
05/29/2022 15:01:45 - INFO - __main__ - Global step 2000 Train loss 0.45 Classification-F1 0.2541691487872603 on epoch=83
05/29/2022 15:01:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.45 on epoch=83
05/29/2022 15:01:50 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.43 on epoch=84
05/29/2022 15:01:53 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.47 on epoch=84
05/29/2022 15:01:56 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.44 on epoch=84
05/29/2022 15:01:58 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.42 on epoch=85
05/29/2022 15:02:10 - INFO - __main__ - Global step 2050 Train loss 0.44 Classification-F1 0.24568956201887127 on epoch=85
05/29/2022 15:02:13 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.42 on epoch=85
05/29/2022 15:02:15 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.46 on epoch=86
05/29/2022 15:02:18 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.45 on epoch=86
05/29/2022 15:02:21 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.38 on epoch=87
05/29/2022 15:02:23 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.54 on epoch=87
05/29/2022 15:02:35 - INFO - __main__ - Global step 2100 Train loss 0.45 Classification-F1 0.2413850026825141 on epoch=87
05/29/2022 15:02:38 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.40 on epoch=87
05/29/2022 15:02:41 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.48 on epoch=88
05/29/2022 15:02:43 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.44 on epoch=88
05/29/2022 15:02:46 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.41 on epoch=89
05/29/2022 15:02:48 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.39 on epoch=89
05/29/2022 15:03:00 - INFO - __main__ - Global step 2150 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=89
05/29/2022 15:03:03 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.44 on epoch=89
05/29/2022 15:03:05 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.43 on epoch=90
05/29/2022 15:03:08 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.46 on epoch=90
05/29/2022 15:03:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.42 on epoch=91
05/29/2022 15:03:13 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.46 on epoch=91
05/29/2022 15:03:25 - INFO - __main__ - Global step 2200 Train loss 0.44 Classification-F1 0.270305603775325 on epoch=91
05/29/2022 15:03:28 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.40 on epoch=92
05/29/2022 15:03:31 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.48 on epoch=92
05/29/2022 15:03:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.42 on epoch=92
05/29/2022 15:03:36 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.44 on epoch=93
05/29/2022 15:03:39 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.43 on epoch=93
05/29/2022 15:03:50 - INFO - __main__ - Global step 2250 Train loss 0.43 Classification-F1 0.2639388723726073 on epoch=93
05/29/2022 15:03:53 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.45 on epoch=94
05/29/2022 15:03:56 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.43 on epoch=94
05/29/2022 15:03:58 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.45 on epoch=94
05/29/2022 15:04:01 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.44 on epoch=95
05/29/2022 15:04:03 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.41 on epoch=95
05/29/2022 15:04:15 - INFO - __main__ - Global step 2300 Train loss 0.44 Classification-F1 0.25626609972639164 on epoch=95
05/29/2022 15:04:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.40 on epoch=96
05/29/2022 15:04:20 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.47 on epoch=96
05/29/2022 15:04:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.39 on epoch=97
05/29/2022 15:04:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.50 on epoch=97
05/29/2022 15:04:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.37 on epoch=97
05/29/2022 15:04:40 - INFO - __main__ - Global step 2350 Train loss 0.42 Classification-F1 0.30401692108702977 on epoch=97
05/29/2022 15:04:40 - INFO - __main__ - Saving model with best Classification-F1: 0.2969511330440455 -> 0.30401692108702977 on epoch=97, global_step=2350
05/29/2022 15:04:42 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.42 on epoch=98
05/29/2022 15:04:45 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.45 on epoch=98
05/29/2022 15:04:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.42 on epoch=99
05/29/2022 15:04:50 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.42 on epoch=99
05/29/2022 15:04:53 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.45 on epoch=99
05/29/2022 15:05:05 - INFO - __main__ - Global step 2400 Train loss 0.43 Classification-F1 0.37450279744732934 on epoch=99
05/29/2022 15:05:05 - INFO - __main__ - Saving model with best Classification-F1: 0.30401692108702977 -> 0.37450279744732934 on epoch=99, global_step=2400
05/29/2022 15:05:08 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.44 on epoch=100
05/29/2022 15:05:10 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.43 on epoch=100
05/29/2022 15:05:13 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.42 on epoch=101
05/29/2022 15:05:16 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.46 on epoch=101
05/29/2022 15:05:18 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.39 on epoch=102
05/29/2022 15:05:37 - INFO - __main__ - Global step 2450 Train loss 0.43 Classification-F1 0.28051721507814237 on epoch=102
05/29/2022 15:05:40 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.47 on epoch=102
05/29/2022 15:05:42 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.34 on epoch=102
05/29/2022 15:05:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.47 on epoch=103
05/29/2022 15:05:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.45 on epoch=103
05/29/2022 15:05:50 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.44 on epoch=104
05/29/2022 15:06:02 - INFO - __main__ - Global step 2500 Train loss 0.43 Classification-F1 0.25292333314879406 on epoch=104
05/29/2022 15:06:05 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.45 on epoch=104
05/29/2022 15:06:08 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.42 on epoch=104
05/29/2022 15:06:10 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.43 on epoch=105
05/29/2022 15:06:13 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.38 on epoch=105
05/29/2022 15:06:16 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.42 on epoch=106
05/29/2022 15:06:33 - INFO - __main__ - Global step 2550 Train loss 0.42 Classification-F1 0.39309723838091243 on epoch=106
05/29/2022 15:06:33 - INFO - __main__ - Saving model with best Classification-F1: 0.37450279744732934 -> 0.39309723838091243 on epoch=106, global_step=2550
05/29/2022 15:06:36 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.47 on epoch=106
05/29/2022 15:06:39 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.39 on epoch=107
05/29/2022 15:06:41 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.43 on epoch=107
05/29/2022 15:06:44 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.38 on epoch=107
05/29/2022 15:06:47 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.44 on epoch=108
05/29/2022 15:07:03 - INFO - __main__ - Global step 2600 Train loss 0.42 Classification-F1 0.19721470019342358 on epoch=108
05/29/2022 15:07:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.43 on epoch=108
05/29/2022 15:07:08 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.42 on epoch=109
05/29/2022 15:07:11 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.40 on epoch=109
05/29/2022 15:07:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.42 on epoch=109
05/29/2022 15:07:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.42 on epoch=110
05/29/2022 15:07:28 - INFO - __main__ - Global step 2650 Train loss 0.42 Classification-F1 0.3331174919795315 on epoch=110
05/29/2022 15:07:30 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.37 on epoch=110
05/29/2022 15:07:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.42 on epoch=111
05/29/2022 15:07:36 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.40 on epoch=111
05/29/2022 15:07:38 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.42 on epoch=112
05/29/2022 15:07:41 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.44 on epoch=112
05/29/2022 15:07:53 - INFO - __main__ - Global step 2700 Train loss 0.41 Classification-F1 0.24233013363448144 on epoch=112
05/29/2022 15:07:56 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.37 on epoch=112
05/29/2022 15:07:59 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.42 on epoch=113
05/29/2022 15:08:01 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.37 on epoch=113
05/29/2022 15:08:04 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.41 on epoch=114
05/29/2022 15:08:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.45 on epoch=114
05/29/2022 15:08:18 - INFO - __main__ - Global step 2750 Train loss 0.40 Classification-F1 0.2367708017389546 on epoch=114
05/29/2022 15:08:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.37 on epoch=114
05/29/2022 15:08:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.39 on epoch=115
05/29/2022 15:08:26 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.43 on epoch=115
05/29/2022 15:08:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.40 on epoch=116
05/29/2022 15:08:32 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.38 on epoch=116
05/29/2022 15:08:47 - INFO - __main__ - Global step 2800 Train loss 0.39 Classification-F1 0.3631759714040055 on epoch=116
05/29/2022 15:08:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.39 on epoch=117
05/29/2022 15:08:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.35 on epoch=117
05/29/2022 15:08:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.34 on epoch=117
05/29/2022 15:08:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.39 on epoch=118
05/29/2022 15:09:00 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.44 on epoch=118
05/29/2022 15:09:12 - INFO - __main__ - Global step 2850 Train loss 0.38 Classification-F1 0.3146111763465371 on epoch=118
05/29/2022 15:09:15 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.40 on epoch=119
05/29/2022 15:09:18 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.39 on epoch=119
05/29/2022 15:09:20 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.40 on epoch=119
05/29/2022 15:09:23 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.41 on epoch=120
05/29/2022 15:09:26 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.38 on epoch=120
05/29/2022 15:09:38 - INFO - __main__ - Global step 2900 Train loss 0.40 Classification-F1 0.3191796233742395 on epoch=120
05/29/2022 15:09:41 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.36 on epoch=121
05/29/2022 15:09:43 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.40 on epoch=121
05/29/2022 15:09:46 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.38 on epoch=122
05/29/2022 15:09:49 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.42 on epoch=122
05/29/2022 15:09:51 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.34 on epoch=122
05/29/2022 15:10:03 - INFO - __main__ - Global step 2950 Train loss 0.38 Classification-F1 0.2763157894736842 on epoch=122
05/29/2022 15:10:06 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.35 on epoch=123
05/29/2022 15:10:08 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.44 on epoch=123
05/29/2022 15:10:11 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.34 on epoch=124
05/29/2022 15:10:14 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.41 on epoch=124
05/29/2022 15:10:16 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.36 on epoch=124
05/29/2022 15:10:18 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 15:10:18 - INFO - __main__ - Printing 3 examples
05/29/2022 15:10:18 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/29/2022 15:10:18 - INFO - __main__ - ['contradiction']
05/29/2022 15:10:18 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/29/2022 15:10:18 - INFO - __main__ - ['contradiction']
05/29/2022 15:10:18 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/29/2022 15:10:18 - INFO - __main__ - ['contradiction']
05/29/2022 15:10:18 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:10:18 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:10:19 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 15:10:19 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 15:10:19 - INFO - __main__ - Printing 3 examples
05/29/2022 15:10:19 - INFO - __main__ -  [anli] premise: Dexter Alexander Nottage (born November 14, 1970) is a former American football defensive end in the National Football League (NFL) for the Washington Redskins and the Kansas City Chiefs. He played college football at Florida A&M University and was selected in the sixth round of the 1994 NFL Draft. He played high school football at Hollywood Hills High School. [SEP] hypothesis: Dexter Alexander Nottage (born November 14, 1970) is a former power forward basketball in the NBA.
05/29/2022 15:10:19 - INFO - __main__ - ['contradiction']
05/29/2022 15:10:19 - INFO - __main__ -  [anli] premise: A grasshopper is a sweet, mint-flavored, after-dinner drink. The name of the drink derives from its green color, which comes from crème de menthe. The drink reputedly originated at Tujague's, a landmark bar in the French Quarter of New Orleans, Louisiana, and was invented by its owner, Philip Guichet. The drink gained popularity during the 1950s and 1960s throughout the American South. [SEP] hypothesis: The drink is white
05/29/2022 15:10:19 - INFO - __main__ - ['contradiction']
05/29/2022 15:10:19 - INFO - __main__ -  [anli] premise: Denis Hale Johnson (July 1, 1949 – May 24, 2017) was an American writer best known for his short story collection "Jesus' Son" (1992) and his novel "Tree of Smoke" (2007), which won the National Book Award for Fiction. He also wrote plays, poetry, journalism, and non-fiction. [SEP] hypothesis: Denis Hale Johnson (July 1, 1949 – May 24, 2010) was an American writer best known for his short story collection "Jesus' Son"
05/29/2022 15:10:19 - INFO - __main__ - ['contradiction']
05/29/2022 15:10:19 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:10:19 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:10:19 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 15:10:29 - INFO - __main__ - Global step 3000 Train loss 0.38 Classification-F1 0.3938047032519097 on epoch=124
05/29/2022 15:10:29 - INFO - __main__ - Saving model with best Classification-F1: 0.39309723838091243 -> 0.3938047032519097 on epoch=124, global_step=3000
05/29/2022 15:10:29 - INFO - __main__ - save last model!
05/29/2022 15:10:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 15:10:29 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 15:10:29 - INFO - __main__ - Printing 3 examples
05/29/2022 15:10:29 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 15:10:29 - INFO - __main__ - ['contradiction']
05/29/2022 15:10:29 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 15:10:29 - INFO - __main__ - ['entailment']
05/29/2022 15:10:29 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 15:10:29 - INFO - __main__ - ['contradiction']
05/29/2022 15:10:29 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:10:29 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:10:30 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 15:10:35 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 15:10:35 - INFO - __main__ - task name: anli
05/29/2022 15:10:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 15:10:36 - INFO - __main__ - Starting training!
05/29/2022 15:11:02 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_13_0.5_8_predictions.txt
05/29/2022 15:11:02 - INFO - __main__ - Classification-F1 on test data: 0.3128
05/29/2022 15:11:02 - INFO - __main__ - prefix=anli_128_13, lr=0.5, bsz=8, dev_performance=0.3938047032519097, test_performance=0.3127701789935832
05/29/2022 15:11:02 - INFO - __main__ - Running ... prefix=anli_128_13, lr=0.4, bsz=8 ...
05/29/2022 15:11:03 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 15:11:03 - INFO - __main__ - Printing 3 examples
05/29/2022 15:11:03 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/29/2022 15:11:03 - INFO - __main__ - ['contradiction']
05/29/2022 15:11:03 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/29/2022 15:11:03 - INFO - __main__ - ['contradiction']
05/29/2022 15:11:03 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/29/2022 15:11:03 - INFO - __main__ - ['contradiction']
05/29/2022 15:11:03 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:11:03 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:11:04 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 15:11:04 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 15:11:04 - INFO - __main__ - Printing 3 examples
05/29/2022 15:11:04 - INFO - __main__ -  [anli] premise: Dexter Alexander Nottage (born November 14, 1970) is a former American football defensive end in the National Football League (NFL) for the Washington Redskins and the Kansas City Chiefs. He played college football at Florida A&M University and was selected in the sixth round of the 1994 NFL Draft. He played high school football at Hollywood Hills High School. [SEP] hypothesis: Dexter Alexander Nottage (born November 14, 1970) is a former power forward basketball in the NBA.
05/29/2022 15:11:04 - INFO - __main__ - ['contradiction']
05/29/2022 15:11:04 - INFO - __main__ -  [anli] premise: A grasshopper is a sweet, mint-flavored, after-dinner drink. The name of the drink derives from its green color, which comes from crème de menthe. The drink reputedly originated at Tujague's, a landmark bar in the French Quarter of New Orleans, Louisiana, and was invented by its owner, Philip Guichet. The drink gained popularity during the 1950s and 1960s throughout the American South. [SEP] hypothesis: The drink is white
05/29/2022 15:11:04 - INFO - __main__ - ['contradiction']
05/29/2022 15:11:04 - INFO - __main__ -  [anli] premise: Denis Hale Johnson (July 1, 1949 – May 24, 2017) was an American writer best known for his short story collection "Jesus' Son" (1992) and his novel "Tree of Smoke" (2007), which won the National Book Award for Fiction. He also wrote plays, poetry, journalism, and non-fiction. [SEP] hypothesis: Denis Hale Johnson (July 1, 1949 – May 24, 2010) was an American writer best known for his short story collection "Jesus' Son"
05/29/2022 15:11:04 - INFO - __main__ - ['contradiction']
05/29/2022 15:11:04 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:11:04 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:11:04 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 15:11:23 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 15:11:23 - INFO - __main__ - task name: anli
05/29/2022 15:11:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 15:11:24 - INFO - __main__ - Starting training!
05/29/2022 15:11:27 - INFO - __main__ - Step 10 Global step 10 Train loss 6.43 on epoch=0
05/29/2022 15:11:30 - INFO - __main__ - Step 20 Global step 20 Train loss 2.49 on epoch=0
05/29/2022 15:11:33 - INFO - __main__ - Step 30 Global step 30 Train loss 1.06 on epoch=1
05/29/2022 15:11:35 - INFO - __main__ - Step 40 Global step 40 Train loss 0.83 on epoch=1
05/29/2022 15:11:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.73 on epoch=2
05/29/2022 15:11:46 - INFO - __main__ - Global step 50 Train loss 2.31 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 15:11:46 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 15:11:49 - INFO - __main__ - Step 60 Global step 60 Train loss 0.71 on epoch=2
05/29/2022 15:11:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=2
05/29/2022 15:11:54 - INFO - __main__ - Step 80 Global step 80 Train loss 0.63 on epoch=3
05/29/2022 15:11:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.57 on epoch=3
05/29/2022 15:12:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.63 on epoch=4
05/29/2022 15:12:08 - INFO - __main__ - Global step 100 Train loss 0.62 Classification-F1 0.16568819308545338 on epoch=4
05/29/2022 15:12:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.59 on epoch=4
05/29/2022 15:12:13 - INFO - __main__ - Step 120 Global step 120 Train loss 5.17 on epoch=4
05/29/2022 15:12:16 - INFO - __main__ - Step 130 Global step 130 Train loss 1.84 on epoch=5
05/29/2022 15:12:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=5
05/29/2022 15:12:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.61 on epoch=6
05/29/2022 15:12:30 - INFO - __main__ - Global step 150 Train loss 1.75 Classification-F1 0.24182210820760322 on epoch=6
05/29/2022 15:12:30 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.24182210820760322 on epoch=6, global_step=150
05/29/2022 15:12:32 - INFO - __main__ - Step 160 Global step 160 Train loss 0.57 on epoch=6
05/29/2022 15:12:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.55 on epoch=7
05/29/2022 15:12:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.58 on epoch=7
05/29/2022 15:12:40 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=7
05/29/2022 15:12:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.60 on epoch=8
05/29/2022 15:12:51 - INFO - __main__ - Global step 200 Train loss 0.56 Classification-F1 0.2747968116055471 on epoch=8
05/29/2022 15:12:51 - INFO - __main__ - Saving model with best Classification-F1: 0.24182210820760322 -> 0.2747968116055471 on epoch=8, global_step=200
05/29/2022 15:12:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=8
05/29/2022 15:12:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.59 on epoch=9
05/29/2022 15:12:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=9
05/29/2022 15:13:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.56 on epoch=9
05/29/2022 15:13:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.52 on epoch=10
05/29/2022 15:13:13 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.17244846656611368 on epoch=10
05/29/2022 15:13:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=10
05/29/2022 15:13:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.53 on epoch=11
05/29/2022 15:13:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=11
05/29/2022 15:13:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=12
05/29/2022 15:13:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.57 on epoch=12
05/29/2022 15:13:35 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.25719532881916696 on epoch=12
05/29/2022 15:13:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.53 on epoch=12
05/29/2022 15:13:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.52 on epoch=13
05/29/2022 15:13:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.52 on epoch=13
05/29/2022 15:13:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.56 on epoch=14
05/29/2022 15:13:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=14
05/29/2022 15:13:57 - INFO - __main__ - Global step 350 Train loss 0.53 Classification-F1 0.1681011296395912 on epoch=14
05/29/2022 15:14:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.55 on epoch=14
05/29/2022 15:14:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.56 on epoch=15
05/29/2022 15:14:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=15
05/29/2022 15:14:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.58 on epoch=16
05/29/2022 15:14:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.52 on epoch=16
05/29/2022 15:14:19 - INFO - __main__ - Global step 400 Train loss 0.54 Classification-F1 0.17433647054294896 on epoch=16
05/29/2022 15:14:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.50 on epoch=17
05/29/2022 15:14:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=17
05/29/2022 15:14:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.51 on epoch=17
05/29/2022 15:14:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=18
05/29/2022 15:14:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=18
05/29/2022 15:14:44 - INFO - __main__ - Global step 450 Train loss 0.50 Classification-F1 0.1927004347386513 on epoch=18
05/29/2022 15:14:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=19
05/29/2022 15:14:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.51 on epoch=19
05/29/2022 15:14:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.47 on epoch=19
05/29/2022 15:14:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.50 on epoch=20
05/29/2022 15:14:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.49 on epoch=20
05/29/2022 15:15:05 - INFO - __main__ - Global step 500 Train loss 0.49 Classification-F1 0.16568819308545338 on epoch=20
05/29/2022 15:15:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.51 on epoch=21
05/29/2022 15:15:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=21
05/29/2022 15:15:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=22
05/29/2022 15:15:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=22
05/29/2022 15:15:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=22
05/29/2022 15:15:26 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.16600790513833993 on epoch=22
05/29/2022 15:15:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.51 on epoch=23
05/29/2022 15:15:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.50 on epoch=23
05/29/2022 15:15:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.47 on epoch=24
05/29/2022 15:15:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.53 on epoch=24
05/29/2022 15:15:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=24
05/29/2022 15:15:50 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.1721607831834019 on epoch=24
05/29/2022 15:15:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.53 on epoch=25
05/29/2022 15:15:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.49 on epoch=25
05/29/2022 15:15:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=26
05/29/2022 15:16:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.54 on epoch=26
05/29/2022 15:16:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.50 on epoch=27
05/29/2022 15:16:12 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.16568819308545338 on epoch=27
05/29/2022 15:16:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=27
05/29/2022 15:16:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=27
05/29/2022 15:16:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.50 on epoch=28
05/29/2022 15:16:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=28
05/29/2022 15:16:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=29
05/29/2022 15:16:33 - INFO - __main__ - Global step 700 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=29
05/29/2022 15:16:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=29
05/29/2022 15:16:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=29
05/29/2022 15:16:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=30
05/29/2022 15:16:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=30
05/29/2022 15:16:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=31
05/29/2022 15:16:55 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.20839782442814284 on epoch=31
05/29/2022 15:16:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.50 on epoch=31
05/29/2022 15:17:00 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=32
05/29/2022 15:17:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.50 on epoch=32
05/29/2022 15:17:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=32
05/29/2022 15:17:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.51 on epoch=33
05/29/2022 15:17:19 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=33
05/29/2022 15:17:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=33
05/29/2022 15:17:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.52 on epoch=34
05/29/2022 15:17:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.53 on epoch=34
05/29/2022 15:17:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=34
05/29/2022 15:17:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.53 on epoch=35
05/29/2022 15:17:41 - INFO - __main__ - Global step 850 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=35
05/29/2022 15:17:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.46 on epoch=35
05/29/2022 15:17:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.48 on epoch=36
05/29/2022 15:17:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.49 on epoch=36
05/29/2022 15:17:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=37
05/29/2022 15:17:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.52 on epoch=37
05/29/2022 15:18:05 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=37
05/29/2022 15:18:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=37
05/29/2022 15:18:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.50 on epoch=38
05/29/2022 15:18:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.48 on epoch=38
05/29/2022 15:18:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=39
05/29/2022 15:18:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.49 on epoch=39
05/29/2022 15:18:29 - INFO - __main__ - Global step 950 Train loss 0.47 Classification-F1 0.16568819308545338 on epoch=39
05/29/2022 15:18:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=39
05/29/2022 15:18:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.53 on epoch=40
05/29/2022 15:18:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.45 on epoch=40
05/29/2022 15:18:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.48 on epoch=41
05/29/2022 15:18:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.48 on epoch=41
05/29/2022 15:18:54 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.24161345900476336 on epoch=41
05/29/2022 15:18:57 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=42
05/29/2022 15:18:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.50 on epoch=42
05/29/2022 15:19:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.46 on epoch=42
05/29/2022 15:19:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.47 on epoch=43
05/29/2022 15:19:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.54 on epoch=43
05/29/2022 15:19:19 - INFO - __main__ - Global step 1050 Train loss 0.48 Classification-F1 0.22278040121906292 on epoch=43
05/29/2022 15:19:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.51 on epoch=44
05/29/2022 15:19:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.52 on epoch=44
05/29/2022 15:19:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.50 on epoch=44
05/29/2022 15:19:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.53 on epoch=45
05/29/2022 15:19:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.46 on epoch=45
05/29/2022 15:19:43 - INFO - __main__ - Global step 1100 Train loss 0.50 Classification-F1 0.1650294695481336 on epoch=45
05/29/2022 15:19:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.44 on epoch=46
05/29/2022 15:19:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.51 on epoch=46
05/29/2022 15:19:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=47
05/29/2022 15:19:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.55 on epoch=47
05/29/2022 15:19:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=47
05/29/2022 15:20:08 - INFO - __main__ - Global step 1150 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=47
05/29/2022 15:20:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=48
05/29/2022 15:20:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.44 on epoch=48
05/29/2022 15:20:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.51 on epoch=49
05/29/2022 15:20:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.51 on epoch=49
05/29/2022 15:20:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.49 on epoch=49
05/29/2022 15:20:32 - INFO - __main__ - Global step 1200 Train loss 0.48 Classification-F1 0.23412733652974502 on epoch=49
05/29/2022 15:20:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.49 on epoch=50
05/29/2022 15:20:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=50
05/29/2022 15:20:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.50 on epoch=51
05/29/2022 15:20:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.50 on epoch=51
05/29/2022 15:20:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=52
05/29/2022 15:20:57 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.24840175488457028 on epoch=52
05/29/2022 15:21:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=52
05/29/2022 15:21:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.47 on epoch=52
05/29/2022 15:21:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=53
05/29/2022 15:21:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.48 on epoch=53
05/29/2022 15:21:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.48 on epoch=54
05/29/2022 15:21:22 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.16568819308545338 on epoch=54
05/29/2022 15:21:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.49 on epoch=54
05/29/2022 15:21:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.49 on epoch=54
05/29/2022 15:21:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.49 on epoch=55
05/29/2022 15:21:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.44 on epoch=55
05/29/2022 15:21:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=56
05/29/2022 15:21:46 - INFO - __main__ - Global step 1350 Train loss 0.48 Classification-F1 0.24560963710141248 on epoch=56
05/29/2022 15:21:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.49 on epoch=56
05/29/2022 15:21:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.42 on epoch=57
05/29/2022 15:21:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.47 on epoch=57
05/29/2022 15:21:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.42 on epoch=57
05/29/2022 15:21:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=58
05/29/2022 15:22:11 - INFO - __main__ - Global step 1400 Train loss 0.44 Classification-F1 0.16568819308545338 on epoch=58
05/29/2022 15:22:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.49 on epoch=58
05/29/2022 15:22:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=59
05/29/2022 15:22:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.44 on epoch=59
05/29/2022 15:22:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.46 on epoch=59
05/29/2022 15:22:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.47 on epoch=60
05/29/2022 15:22:35 - INFO - __main__ - Global step 1450 Train loss 0.46 Classification-F1 0.25275655658741275 on epoch=60
05/29/2022 15:22:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.42 on epoch=60
05/29/2022 15:22:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.45 on epoch=61
05/29/2022 15:22:43 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.46 on epoch=61
05/29/2022 15:22:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.47 on epoch=62
05/29/2022 15:22:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.51 on epoch=62
05/29/2022 15:23:00 - INFO - __main__ - Global step 1500 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=62
05/29/2022 15:23:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=62
05/29/2022 15:23:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=63
05/29/2022 15:23:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.47 on epoch=63
05/29/2022 15:23:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.46 on epoch=64
05/29/2022 15:23:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.49 on epoch=64
05/29/2022 15:23:24 - INFO - __main__ - Global step 1550 Train loss 0.46 Classification-F1 0.16568819308545338 on epoch=64
05/29/2022 15:23:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.52 on epoch=64
05/29/2022 15:23:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.50 on epoch=65
05/29/2022 15:23:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.45 on epoch=65
05/29/2022 15:23:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=66
05/29/2022 15:23:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.48 on epoch=66
05/29/2022 15:23:49 - INFO - __main__ - Global step 1600 Train loss 0.47 Classification-F1 0.22037004881248676 on epoch=66
05/29/2022 15:23:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=67
05/29/2022 15:23:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.53 on epoch=67
05/29/2022 15:23:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=67
05/29/2022 15:23:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.49 on epoch=68
05/29/2022 15:24:02 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.46 on epoch=68
05/29/2022 15:24:13 - INFO - __main__ - Global step 1650 Train loss 0.47 Classification-F1 0.26766425495092666 on epoch=68
05/29/2022 15:24:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=69
05/29/2022 15:24:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.42 on epoch=69
05/29/2022 15:24:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=69
05/29/2022 15:24:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.54 on epoch=70
05/29/2022 15:24:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.44 on epoch=70
05/29/2022 15:24:38 - INFO - __main__ - Global step 1700 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=70
05/29/2022 15:24:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.46 on epoch=71
05/29/2022 15:24:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=71
05/29/2022 15:24:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.42 on epoch=72
05/29/2022 15:24:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.51 on epoch=72
05/29/2022 15:24:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.43 on epoch=72
05/29/2022 15:25:02 - INFO - __main__ - Global step 1750 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=72
05/29/2022 15:25:05 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.47 on epoch=73
05/29/2022 15:25:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.42 on epoch=73
05/29/2022 15:25:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.49 on epoch=74
05/29/2022 15:25:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.51 on epoch=74
05/29/2022 15:25:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.48 on epoch=74
05/29/2022 15:25:27 - INFO - __main__ - Global step 1800 Train loss 0.47 Classification-F1 0.28300541345864394 on epoch=74
05/29/2022 15:25:27 - INFO - __main__ - Saving model with best Classification-F1: 0.2747968116055471 -> 0.28300541345864394 on epoch=74, global_step=1800
05/29/2022 15:25:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.42 on epoch=75
05/29/2022 15:25:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.42 on epoch=75
05/29/2022 15:25:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.44 on epoch=76
05/29/2022 15:25:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.46 on epoch=76
05/29/2022 15:25:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=77
05/29/2022 15:25:51 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.16568819308545338 on epoch=77
05/29/2022 15:25:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.51 on epoch=77
05/29/2022 15:25:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.45 on epoch=77
05/29/2022 15:25:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.51 on epoch=78
05/29/2022 15:26:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.48 on epoch=78
05/29/2022 15:26:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=79
05/29/2022 15:26:16 - INFO - __main__ - Global step 1900 Train loss 0.48 Classification-F1 0.16568819308545338 on epoch=79
05/29/2022 15:26:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.46 on epoch=79
05/29/2022 15:26:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.42 on epoch=79
05/29/2022 15:26:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.46 on epoch=80
05/29/2022 15:26:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=80
05/29/2022 15:26:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.44 on epoch=81
05/29/2022 15:26:40 - INFO - __main__ - Global step 1950 Train loss 0.44 Classification-F1 0.23857081837537863 on epoch=81
05/29/2022 15:26:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.45 on epoch=81
05/29/2022 15:26:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.44 on epoch=82
05/29/2022 15:26:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.51 on epoch=82
05/29/2022 15:26:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.46 on epoch=82
05/29/2022 15:26:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.48 on epoch=83
05/29/2022 15:27:05 - INFO - __main__ - Global step 2000 Train loss 0.47 Classification-F1 0.16568819308545338 on epoch=83
05/29/2022 15:27:07 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.44 on epoch=83
05/29/2022 15:27:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.48 on epoch=84
05/29/2022 15:27:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.46 on epoch=84
05/29/2022 15:27:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.43 on epoch=84
05/29/2022 15:27:18 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.47 on epoch=85
05/29/2022 15:27:29 - INFO - __main__ - Global step 2050 Train loss 0.46 Classification-F1 0.17676767676767677 on epoch=85
05/29/2022 15:27:32 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.48 on epoch=85
05/29/2022 15:27:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.43 on epoch=86
05/29/2022 15:27:37 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.44 on epoch=86
05/29/2022 15:27:40 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.47 on epoch=87
05/29/2022 15:27:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.47 on epoch=87
05/29/2022 15:27:54 - INFO - __main__ - Global step 2100 Train loss 0.46 Classification-F1 0.1757055360294227 on epoch=87
05/29/2022 15:27:56 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.42 on epoch=87
05/29/2022 15:27:59 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.44 on epoch=88
05/29/2022 15:28:01 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.45 on epoch=88
05/29/2022 15:28:04 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.48 on epoch=89
05/29/2022 15:28:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.47 on epoch=89
05/29/2022 15:28:18 - INFO - __main__ - Global step 2150 Train loss 0.45 Classification-F1 0.22387669801462906 on epoch=89
05/29/2022 15:28:21 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.45 on epoch=89
05/29/2022 15:28:23 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.45 on epoch=90
05/29/2022 15:28:26 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.42 on epoch=90
05/29/2022 15:28:29 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.43 on epoch=91
05/29/2022 15:28:31 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.47 on epoch=91
05/29/2022 15:28:43 - INFO - __main__ - Global step 2200 Train loss 0.44 Classification-F1 0.17564194997504298 on epoch=91
05/29/2022 15:28:46 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.45 on epoch=92
05/29/2022 15:28:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.49 on epoch=92
05/29/2022 15:28:51 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.41 on epoch=92
05/29/2022 15:28:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.46 on epoch=93
05/29/2022 15:28:56 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.42 on epoch=93
05/29/2022 15:29:08 - INFO - __main__ - Global step 2250 Train loss 0.44 Classification-F1 0.25708896008623533 on epoch=93
05/29/2022 15:29:11 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.44 on epoch=94
05/29/2022 15:29:13 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.46 on epoch=94
05/29/2022 15:29:16 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.43 on epoch=94
05/29/2022 15:29:18 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.47 on epoch=95
05/29/2022 15:29:21 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.44 on epoch=95
05/29/2022 15:29:33 - INFO - __main__ - Global step 2300 Train loss 0.45 Classification-F1 0.25225855971535766 on epoch=95
05/29/2022 15:29:35 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.43 on epoch=96
05/29/2022 15:29:38 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.48 on epoch=96
05/29/2022 15:29:41 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.44 on epoch=97
05/29/2022 15:29:43 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.46 on epoch=97
05/29/2022 15:29:46 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.38 on epoch=97
05/29/2022 15:29:57 - INFO - __main__ - Global step 2350 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=97
05/29/2022 15:30:00 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.50 on epoch=98
05/29/2022 15:30:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.45 on epoch=98
05/29/2022 15:30:05 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.45 on epoch=99
05/29/2022 15:30:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.47 on epoch=99
05/29/2022 15:30:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.43 on epoch=99
05/29/2022 15:30:22 - INFO - __main__ - Global step 2400 Train loss 0.46 Classification-F1 0.25378974606218524 on epoch=99
05/29/2022 15:30:24 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.50 on epoch=100
05/29/2022 15:30:27 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.37 on epoch=100
05/29/2022 15:30:29 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.48 on epoch=101
05/29/2022 15:30:32 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.45 on epoch=101
05/29/2022 15:30:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.47 on epoch=102
05/29/2022 15:30:46 - INFO - __main__ - Global step 2450 Train loss 0.45 Classification-F1 0.23999509713795428 on epoch=102
05/29/2022 15:30:49 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.50 on epoch=102
05/29/2022 15:30:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.43 on epoch=102
05/29/2022 15:30:54 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.45 on epoch=103
05/29/2022 15:30:57 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.40 on epoch=103
05/29/2022 15:30:59 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.42 on epoch=104
05/29/2022 15:31:10 - INFO - __main__ - Global step 2500 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=104
05/29/2022 15:31:13 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.49 on epoch=104
05/29/2022 15:31:16 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.43 on epoch=104
05/29/2022 15:31:18 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.48 on epoch=105
05/29/2022 15:31:21 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.44 on epoch=105
05/29/2022 15:31:23 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.46 on epoch=106
05/29/2022 15:31:35 - INFO - __main__ - Global step 2550 Train loss 0.46 Classification-F1 0.25778490156172557 on epoch=106
05/29/2022 15:31:38 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.46 on epoch=106
05/29/2022 15:31:40 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.44 on epoch=107
05/29/2022 15:31:43 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.43 on epoch=107
05/29/2022 15:31:45 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.41 on epoch=107
05/29/2022 15:31:48 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.47 on epoch=108
05/29/2022 15:32:00 - INFO - __main__ - Global step 2600 Train loss 0.44 Classification-F1 0.2340878606781708 on epoch=108
05/29/2022 15:32:02 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.41 on epoch=108
05/29/2022 15:32:05 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.43 on epoch=109
05/29/2022 15:32:08 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.41 on epoch=109
05/29/2022 15:32:10 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.45 on epoch=109
05/29/2022 15:32:13 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.47 on epoch=110
05/29/2022 15:32:24 - INFO - __main__ - Global step 2650 Train loss 0.43 Classification-F1 0.20362589480236537 on epoch=110
05/29/2022 15:32:27 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.42 on epoch=110
05/29/2022 15:32:30 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.48 on epoch=111
05/29/2022 15:32:32 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.45 on epoch=111
05/29/2022 15:32:35 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.44 on epoch=112
05/29/2022 15:32:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.51 on epoch=112
05/29/2022 15:32:49 - INFO - __main__ - Global step 2700 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=112
05/29/2022 15:32:51 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.41 on epoch=112
05/29/2022 15:32:54 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.48 on epoch=113
05/29/2022 15:32:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.49 on epoch=113
05/29/2022 15:32:59 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.49 on epoch=114
05/29/2022 15:33:02 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.41 on epoch=114
05/29/2022 15:33:13 - INFO - __main__ - Global step 2750 Train loss 0.46 Classification-F1 0.21062801932367148 on epoch=114
05/29/2022 15:33:16 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.41 on epoch=114
05/29/2022 15:33:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.44 on epoch=115
05/29/2022 15:33:21 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.44 on epoch=115
05/29/2022 15:33:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.41 on epoch=116
05/29/2022 15:33:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.46 on epoch=116
05/29/2022 15:33:38 - INFO - __main__ - Global step 2800 Train loss 0.43 Classification-F1 0.2363014631625919 on epoch=116
05/29/2022 15:33:41 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.39 on epoch=117
05/29/2022 15:33:43 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.44 on epoch=117
05/29/2022 15:33:46 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.42 on epoch=117
05/29/2022 15:33:48 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.41 on epoch=118
05/29/2022 15:33:51 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.44 on epoch=118
05/29/2022 15:34:03 - INFO - __main__ - Global step 2850 Train loss 0.42 Classification-F1 0.26849282951037473 on epoch=118
05/29/2022 15:34:05 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.47 on epoch=119
05/29/2022 15:34:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.45 on epoch=119
05/29/2022 15:34:10 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.39 on epoch=119
05/29/2022 15:34:13 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.42 on epoch=120
05/29/2022 15:34:16 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.41 on epoch=120
05/29/2022 15:34:27 - INFO - __main__ - Global step 2900 Train loss 0.43 Classification-F1 0.1689231248054777 on epoch=120
05/29/2022 15:34:29 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.45 on epoch=121
05/29/2022 15:34:32 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.44 on epoch=121
05/29/2022 15:34:35 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.41 on epoch=122
05/29/2022 15:34:37 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.46 on epoch=122
05/29/2022 15:34:40 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.41 on epoch=122
05/29/2022 15:34:51 - INFO - __main__ - Global step 2950 Train loss 0.43 Classification-F1 0.256792802567928 on epoch=122
05/29/2022 15:34:54 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.46 on epoch=123
05/29/2022 15:34:57 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.39 on epoch=123
05/29/2022 15:34:59 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.45 on epoch=124
05/29/2022 15:35:02 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.42 on epoch=124
05/29/2022 15:35:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.39 on epoch=124
05/29/2022 15:35:06 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 15:35:06 - INFO - __main__ - Printing 3 examples
05/29/2022 15:35:06 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/29/2022 15:35:06 - INFO - __main__ - ['contradiction']
05/29/2022 15:35:06 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/29/2022 15:35:06 - INFO - __main__ - ['contradiction']
05/29/2022 15:35:06 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/29/2022 15:35:06 - INFO - __main__ - ['contradiction']
05/29/2022 15:35:06 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:35:06 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:35:06 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 15:35:06 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 15:35:06 - INFO - __main__ - Printing 3 examples
05/29/2022 15:35:06 - INFO - __main__ -  [anli] premise: Dexter Alexander Nottage (born November 14, 1970) is a former American football defensive end in the National Football League (NFL) for the Washington Redskins and the Kansas City Chiefs. He played college football at Florida A&M University and was selected in the sixth round of the 1994 NFL Draft. He played high school football at Hollywood Hills High School. [SEP] hypothesis: Dexter Alexander Nottage (born November 14, 1970) is a former power forward basketball in the NBA.
05/29/2022 15:35:06 - INFO - __main__ - ['contradiction']
05/29/2022 15:35:06 - INFO - __main__ -  [anli] premise: A grasshopper is a sweet, mint-flavored, after-dinner drink. The name of the drink derives from its green color, which comes from crème de menthe. The drink reputedly originated at Tujague's, a landmark bar in the French Quarter of New Orleans, Louisiana, and was invented by its owner, Philip Guichet. The drink gained popularity during the 1950s and 1960s throughout the American South. [SEP] hypothesis: The drink is white
05/29/2022 15:35:06 - INFO - __main__ - ['contradiction']
05/29/2022 15:35:06 - INFO - __main__ -  [anli] premise: Denis Hale Johnson (July 1, 1949 – May 24, 2017) was an American writer best known for his short story collection "Jesus' Son" (1992) and his novel "Tree of Smoke" (2007), which won the National Book Award for Fiction. He also wrote plays, poetry, journalism, and non-fiction. [SEP] hypothesis: Denis Hale Johnson (July 1, 1949 – May 24, 2010) was an American writer best known for his short story collection "Jesus' Son"
05/29/2022 15:35:06 - INFO - __main__ - ['contradiction']
05/29/2022 15:35:06 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:35:07 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:35:07 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 15:35:16 - INFO - __main__ - Global step 3000 Train loss 0.42 Classification-F1 0.24957970971831345 on epoch=124
05/29/2022 15:35:16 - INFO - __main__ - save last model!
05/29/2022 15:35:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 15:35:16 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 15:35:16 - INFO - __main__ - Printing 3 examples
05/29/2022 15:35:16 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 15:35:16 - INFO - __main__ - ['contradiction']
05/29/2022 15:35:16 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 15:35:16 - INFO - __main__ - ['entailment']
05/29/2022 15:35:16 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 15:35:16 - INFO - __main__ - ['contradiction']
05/29/2022 15:35:16 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:35:17 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:35:18 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 15:35:22 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 15:35:22 - INFO - __main__ - task name: anli
05/29/2022 15:35:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 15:35:23 - INFO - __main__ - Starting training!
05/29/2022 15:35:49 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_13_0.4_8_predictions.txt
05/29/2022 15:35:49 - INFO - __main__ - Classification-F1 on test data: 0.2477
05/29/2022 15:35:50 - INFO - __main__ - prefix=anli_128_13, lr=0.4, bsz=8, dev_performance=0.28300541345864394, test_performance=0.2476573480258418
05/29/2022 15:35:50 - INFO - __main__ - Running ... prefix=anli_128_13, lr=0.3, bsz=8 ...
05/29/2022 15:35:51 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 15:35:51 - INFO - __main__ - Printing 3 examples
05/29/2022 15:35:51 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/29/2022 15:35:51 - INFO - __main__ - ['contradiction']
05/29/2022 15:35:51 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/29/2022 15:35:51 - INFO - __main__ - ['contradiction']
05/29/2022 15:35:51 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/29/2022 15:35:51 - INFO - __main__ - ['contradiction']
05/29/2022 15:35:51 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:35:51 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:35:51 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 15:35:51 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 15:35:51 - INFO - __main__ - Printing 3 examples
05/29/2022 15:35:51 - INFO - __main__ -  [anli] premise: Dexter Alexander Nottage (born November 14, 1970) is a former American football defensive end in the National Football League (NFL) for the Washington Redskins and the Kansas City Chiefs. He played college football at Florida A&M University and was selected in the sixth round of the 1994 NFL Draft. He played high school football at Hollywood Hills High School. [SEP] hypothesis: Dexter Alexander Nottage (born November 14, 1970) is a former power forward basketball in the NBA.
05/29/2022 15:35:51 - INFO - __main__ - ['contradiction']
05/29/2022 15:35:51 - INFO - __main__ -  [anli] premise: A grasshopper is a sweet, mint-flavored, after-dinner drink. The name of the drink derives from its green color, which comes from crème de menthe. The drink reputedly originated at Tujague's, a landmark bar in the French Quarter of New Orleans, Louisiana, and was invented by its owner, Philip Guichet. The drink gained popularity during the 1950s and 1960s throughout the American South. [SEP] hypothesis: The drink is white
05/29/2022 15:35:51 - INFO - __main__ - ['contradiction']
05/29/2022 15:35:51 - INFO - __main__ -  [anli] premise: Denis Hale Johnson (July 1, 1949 – May 24, 2017) was an American writer best known for his short story collection "Jesus' Son" (1992) and his novel "Tree of Smoke" (2007), which won the National Book Award for Fiction. He also wrote plays, poetry, journalism, and non-fiction. [SEP] hypothesis: Denis Hale Johnson (July 1, 1949 – May 24, 2010) was an American writer best known for his short story collection "Jesus' Son"
05/29/2022 15:35:51 - INFO - __main__ - ['contradiction']
05/29/2022 15:35:51 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:35:52 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:35:52 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 15:36:08 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 15:36:08 - INFO - __main__ - task name: anli
05/29/2022 15:36:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 15:36:09 - INFO - __main__ - Starting training!
05/29/2022 15:36:12 - INFO - __main__ - Step 10 Global step 10 Train loss 6.94 on epoch=0
05/29/2022 15:36:15 - INFO - __main__ - Step 20 Global step 20 Train loss 3.30 on epoch=0
05/29/2022 15:36:18 - INFO - __main__ - Step 30 Global step 30 Train loss 1.70 on epoch=1
05/29/2022 15:36:20 - INFO - __main__ - Step 40 Global step 40 Train loss 1.16 on epoch=1
05/29/2022 15:36:23 - INFO - __main__ - Step 50 Global step 50 Train loss 0.93 on epoch=2
05/29/2022 15:36:31 - INFO - __main__ - Global step 50 Train loss 2.81 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 15:36:31 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 15:36:33 - INFO - __main__ - Step 60 Global step 60 Train loss 0.84 on epoch=2
05/29/2022 15:36:36 - INFO - __main__ - Step 70 Global step 70 Train loss 0.71 on epoch=2
05/29/2022 15:36:39 - INFO - __main__ - Step 80 Global step 80 Train loss 0.77 on epoch=3
05/29/2022 15:36:41 - INFO - __main__ - Step 90 Global step 90 Train loss 0.59 on epoch=3
05/29/2022 15:36:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.62 on epoch=4
05/29/2022 15:36:52 - INFO - __main__ - Global step 100 Train loss 0.71 Classification-F1 0.16666666666666666 on epoch=4
05/29/2022 15:36:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.66 on epoch=4
05/29/2022 15:36:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.57 on epoch=4
05/29/2022 15:36:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.64 on epoch=5
05/29/2022 15:37:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=5
05/29/2022 15:37:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.65 on epoch=6
05/29/2022 15:37:13 - INFO - __main__ - Global step 150 Train loss 0.61 Classification-F1 0.16601307189542483 on epoch=6
05/29/2022 15:37:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.63 on epoch=6
05/29/2022 15:37:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=7
05/29/2022 15:37:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.62 on epoch=7
05/29/2022 15:37:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.57 on epoch=7
05/29/2022 15:37:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.56 on epoch=8
05/29/2022 15:37:33 - INFO - __main__ - Global step 200 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=8
05/29/2022 15:37:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.60 on epoch=8
05/29/2022 15:37:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.63 on epoch=9
05/29/2022 15:37:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.55 on epoch=9
05/29/2022 15:37:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=9
05/29/2022 15:37:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.55 on epoch=10
05/29/2022 15:37:54 - INFO - __main__ - Global step 250 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=10
05/29/2022 15:37:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=10
05/29/2022 15:37:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=11
05/29/2022 15:38:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=11
05/29/2022 15:38:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=12
05/29/2022 15:38:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=12
05/29/2022 15:38:16 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=12
05/29/2022 15:38:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=12
05/29/2022 15:38:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.55 on epoch=13
05/29/2022 15:38:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.56 on epoch=13
05/29/2022 15:38:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.57 on epoch=14
05/29/2022 15:38:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=14
05/29/2022 15:38:37 - INFO - __main__ - Global step 350 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=14
05/29/2022 15:38:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=14
05/29/2022 15:38:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=15
05/29/2022 15:38:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=15
05/29/2022 15:38:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=16
05/29/2022 15:38:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.57 on epoch=16
05/29/2022 15:38:58 - INFO - __main__ - Global step 400 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=16
05/29/2022 15:39:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.51 on epoch=17
05/29/2022 15:39:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.54 on epoch=17
05/29/2022 15:39:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=17
05/29/2022 15:39:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.51 on epoch=18
05/29/2022 15:39:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.54 on epoch=18
05/29/2022 15:39:22 - INFO - __main__ - Global step 450 Train loss 0.51 Classification-F1 0.2014168244812747 on epoch=18
05/29/2022 15:39:22 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2014168244812747 on epoch=18, global_step=450
05/29/2022 15:39:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.55 on epoch=19
05/29/2022 15:39:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.58 on epoch=19
05/29/2022 15:39:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.56 on epoch=19
05/29/2022 15:39:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=20
05/29/2022 15:39:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.51 on epoch=20
05/29/2022 15:39:43 - INFO - __main__ - Global step 500 Train loss 0.53 Classification-F1 0.16535433070866143 on epoch=20
05/29/2022 15:39:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=21
05/29/2022 15:39:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.46 on epoch=21
05/29/2022 15:39:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=22
05/29/2022 15:39:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=22
05/29/2022 15:39:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=22
05/29/2022 15:40:04 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=22
05/29/2022 15:40:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=23
05/29/2022 15:40:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=23
05/29/2022 15:40:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.54 on epoch=24
05/29/2022 15:40:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=24
05/29/2022 15:40:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=24
05/29/2022 15:40:29 - INFO - __main__ - Global step 600 Train loss 0.50 Classification-F1 0.25229776720031594 on epoch=24
05/29/2022 15:40:29 - INFO - __main__ - Saving model with best Classification-F1: 0.2014168244812747 -> 0.25229776720031594 on epoch=24, global_step=600
05/29/2022 15:40:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.54 on epoch=25
05/29/2022 15:40:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=25
05/29/2022 15:40:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=26
05/29/2022 15:40:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=26
05/29/2022 15:40:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=27
05/29/2022 15:40:53 - INFO - __main__ - Global step 650 Train loss 0.48 Classification-F1 0.27912548667265646 on epoch=27
05/29/2022 15:40:53 - INFO - __main__ - Saving model with best Classification-F1: 0.25229776720031594 -> 0.27912548667265646 on epoch=27, global_step=650
05/29/2022 15:40:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.52 on epoch=27
05/29/2022 15:40:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=27
05/29/2022 15:41:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=28
05/29/2022 15:41:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.54 on epoch=28
05/29/2022 15:41:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=29
05/29/2022 15:41:17 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=29
05/29/2022 15:41:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=29
05/29/2022 15:41:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.51 on epoch=29
05/29/2022 15:41:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.48 on epoch=30
05/29/2022 15:41:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=30
05/29/2022 15:41:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=31
05/29/2022 15:41:41 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.1721607831834019 on epoch=31
05/29/2022 15:41:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=31
05/29/2022 15:41:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=32
05/29/2022 15:41:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.50 on epoch=32
05/29/2022 15:41:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=32
05/29/2022 15:41:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=33
05/29/2022 15:42:05 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=33
05/29/2022 15:42:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=33
05/29/2022 15:42:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.50 on epoch=34
05/29/2022 15:42:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=34
05/29/2022 15:42:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=34
05/29/2022 15:42:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.51 on epoch=35
05/29/2022 15:42:29 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.20094459531079248 on epoch=35
05/29/2022 15:42:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=35
05/29/2022 15:42:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.48 on epoch=36
05/29/2022 15:42:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=36
05/29/2022 15:42:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=37
05/29/2022 15:42:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.51 on epoch=37
05/29/2022 15:42:54 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.22061234306132263 on epoch=37
05/29/2022 15:42:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=37
05/29/2022 15:42:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.47 on epoch=38
05/29/2022 15:43:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=38
05/29/2022 15:43:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=39
05/29/2022 15:43:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.47 on epoch=39
05/29/2022 15:43:18 - INFO - __main__ - Global step 950 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=39
05/29/2022 15:43:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=39
05/29/2022 15:43:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=40
05/29/2022 15:43:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=40
05/29/2022 15:43:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.49 on epoch=41
05/29/2022 15:43:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.50 on epoch=41
05/29/2022 15:43:43 - INFO - __main__ - Global step 1000 Train loss 0.46 Classification-F1 0.24569303182910374 on epoch=41
05/29/2022 15:43:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=42
05/29/2022 15:43:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.52 on epoch=42
05/29/2022 15:43:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=42
05/29/2022 15:43:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.46 on epoch=43
05/29/2022 15:43:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.46 on epoch=43
05/29/2022 15:44:07 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.2304043338526097 on epoch=43
05/29/2022 15:44:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=44
05/29/2022 15:44:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=44
05/29/2022 15:44:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=44
05/29/2022 15:44:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.47 on epoch=45
05/29/2022 15:44:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=45
05/29/2022 15:44:32 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.17142828576713307 on epoch=45
05/29/2022 15:44:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=46
05/29/2022 15:44:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.48 on epoch=46
05/29/2022 15:44:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=47
05/29/2022 15:44:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.50 on epoch=47
05/29/2022 15:44:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.46 on epoch=47
05/29/2022 15:44:56 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.19282764862265966 on epoch=47
05/29/2022 15:44:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=48
05/29/2022 15:45:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.47 on epoch=48
05/29/2022 15:45:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=49
05/29/2022 15:45:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.50 on epoch=49
05/29/2022 15:45:09 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=49
05/29/2022 15:45:20 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.25593897833784013 on epoch=49
05/29/2022 15:45:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.46 on epoch=50
05/29/2022 15:45:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.44 on epoch=50
05/29/2022 15:45:28 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.43 on epoch=51
05/29/2022 15:45:31 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=51
05/29/2022 15:45:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=52
05/29/2022 15:45:45 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.1874734253970015 on epoch=52
05/29/2022 15:45:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.49 on epoch=52
05/29/2022 15:45:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=52
05/29/2022 15:45:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.50 on epoch=53
05/29/2022 15:45:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=53
05/29/2022 15:45:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=54
05/29/2022 15:46:10 - INFO - __main__ - Global step 1300 Train loss 0.45 Classification-F1 0.16436554898093358 on epoch=54
05/29/2022 15:46:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.44 on epoch=54
05/29/2022 15:46:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=54
05/29/2022 15:46:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.47 on epoch=55
05/29/2022 15:46:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=55
05/29/2022 15:46:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=56
05/29/2022 15:46:35 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.26057269868645044 on epoch=56
05/29/2022 15:46:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.45 on epoch=56
05/29/2022 15:46:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.43 on epoch=57
05/29/2022 15:46:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=57
05/29/2022 15:46:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.44 on epoch=57
05/29/2022 15:46:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.42 on epoch=58
05/29/2022 15:46:59 - INFO - __main__ - Global step 1400 Train loss 0.44 Classification-F1 0.203215532001504 on epoch=58
05/29/2022 15:47:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=58
05/29/2022 15:47:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.46 on epoch=59
05/29/2022 15:47:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.47 on epoch=59
05/29/2022 15:47:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=59
05/29/2022 15:47:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.47 on epoch=60
05/29/2022 15:47:24 - INFO - __main__ - Global step 1450 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=60
05/29/2022 15:47:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.45 on epoch=60
05/29/2022 15:47:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.47 on epoch=61
05/29/2022 15:47:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=61
05/29/2022 15:47:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=62
05/29/2022 15:47:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.49 on epoch=62
05/29/2022 15:47:48 - INFO - __main__ - Global step 1500 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=62
05/29/2022 15:47:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=62
05/29/2022 15:47:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.44 on epoch=63
05/29/2022 15:47:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.42 on epoch=63
05/29/2022 15:47:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=64
05/29/2022 15:48:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=64
05/29/2022 15:48:14 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.24161614797897124 on epoch=64
05/29/2022 15:48:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=64
05/29/2022 15:48:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.47 on epoch=65
05/29/2022 15:48:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.44 on epoch=65
05/29/2022 15:48:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=66
05/29/2022 15:48:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=66
05/29/2022 15:48:39 - INFO - __main__ - Global step 1600 Train loss 0.44 Classification-F1 0.2749267828015643 on epoch=66
05/29/2022 15:48:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=67
05/29/2022 15:48:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.49 on epoch=67
05/29/2022 15:48:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=67
05/29/2022 15:48:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.53 on epoch=68
05/29/2022 15:48:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.45 on epoch=68
05/29/2022 15:49:03 - INFO - __main__ - Global step 1650 Train loss 0.45 Classification-F1 0.1970341579564753 on epoch=68
05/29/2022 15:49:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=69
05/29/2022 15:49:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=69
05/29/2022 15:49:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=69
05/29/2022 15:49:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.44 on epoch=70
05/29/2022 15:49:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=70
05/29/2022 15:49:28 - INFO - __main__ - Global step 1700 Train loss 0.44 Classification-F1 0.19512586179252844 on epoch=70
05/29/2022 15:49:31 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.48 on epoch=71
05/29/2022 15:49:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=71
05/29/2022 15:49:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=72
05/29/2022 15:49:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.46 on epoch=72
05/29/2022 15:49:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.45 on epoch=72
05/29/2022 15:49:53 - INFO - __main__ - Global step 1750 Train loss 0.44 Classification-F1 0.2395571181840612 on epoch=72
05/29/2022 15:49:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.48 on epoch=73
05/29/2022 15:49:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=73
05/29/2022 15:50:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.50 on epoch=74
05/29/2022 15:50:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.46 on epoch=74
05/29/2022 15:50:06 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=74
05/29/2022 15:50:17 - INFO - __main__ - Global step 1800 Train loss 0.47 Classification-F1 0.26049922299922296 on epoch=74
05/29/2022 15:50:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=75
05/29/2022 15:50:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.42 on epoch=75
05/29/2022 15:50:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.43 on epoch=76
05/29/2022 15:50:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.47 on epoch=76
05/29/2022 15:50:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=77
05/29/2022 15:50:41 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.17894578998031277 on epoch=77
05/29/2022 15:50:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.47 on epoch=77
05/29/2022 15:50:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=77
05/29/2022 15:50:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.43 on epoch=78
05/29/2022 15:50:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=78
05/29/2022 15:50:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.52 on epoch=79
05/29/2022 15:51:06 - INFO - __main__ - Global step 1900 Train loss 0.45 Classification-F1 0.1751478369125428 on epoch=79
05/29/2022 15:51:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.45 on epoch=79
05/29/2022 15:51:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.42 on epoch=79
05/29/2022 15:51:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.46 on epoch=80
05/29/2022 15:51:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.41 on epoch=80
05/29/2022 15:51:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=81
05/29/2022 15:51:31 - INFO - __main__ - Global step 1950 Train loss 0.43 Classification-F1 0.3562773388555008 on epoch=81
05/29/2022 15:51:31 - INFO - __main__ - Saving model with best Classification-F1: 0.27912548667265646 -> 0.3562773388555008 on epoch=81, global_step=1950
05/29/2022 15:51:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.42 on epoch=81
05/29/2022 15:51:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.40 on epoch=82
05/29/2022 15:51:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.45 on epoch=82
05/29/2022 15:51:42 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.41 on epoch=82
05/29/2022 15:51:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.44 on epoch=83
05/29/2022 15:51:56 - INFO - __main__ - Global step 2000 Train loss 0.43 Classification-F1 0.2870983789924995 on epoch=83
05/29/2022 15:51:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.43 on epoch=83
05/29/2022 15:52:01 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.46 on epoch=84
05/29/2022 15:52:04 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.44 on epoch=84
05/29/2022 15:52:07 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.41 on epoch=84
05/29/2022 15:52:09 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.44 on epoch=85
05/29/2022 15:52:21 - INFO - __main__ - Global step 2050 Train loss 0.43 Classification-F1 0.33561438593766185 on epoch=85
05/29/2022 15:52:23 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.38 on epoch=85
05/29/2022 15:52:26 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.44 on epoch=86
05/29/2022 15:52:29 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.43 on epoch=86
05/29/2022 15:52:31 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.40 on epoch=87
05/29/2022 15:52:34 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.44 on epoch=87
05/29/2022 15:52:46 - INFO - __main__ - Global step 2100 Train loss 0.42 Classification-F1 0.30585629921259844 on epoch=87
05/29/2022 15:52:48 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.39 on epoch=87
05/29/2022 15:52:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.41 on epoch=88
05/29/2022 15:52:54 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.46 on epoch=88
05/29/2022 15:52:56 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.47 on epoch=89
05/29/2022 15:52:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.42 on epoch=89
05/29/2022 15:53:10 - INFO - __main__ - Global step 2150 Train loss 0.43 Classification-F1 0.23923921021708175 on epoch=89
05/29/2022 15:53:12 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.42 on epoch=89
05/29/2022 15:53:15 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.44 on epoch=90
05/29/2022 15:53:18 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.40 on epoch=90
05/29/2022 15:53:20 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.41 on epoch=91
05/29/2022 15:53:23 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.43 on epoch=91
05/29/2022 15:53:33 - INFO - __main__ - Global step 2200 Train loss 0.42 Classification-F1 0.2956000899775681 on epoch=91
05/29/2022 15:53:35 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.43 on epoch=92
05/29/2022 15:53:38 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.43 on epoch=92
05/29/2022 15:53:41 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.39 on epoch=92
05/29/2022 15:53:43 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.43 on epoch=93
05/29/2022 15:53:46 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.42 on epoch=93
05/29/2022 15:53:57 - INFO - __main__ - Global step 2250 Train loss 0.42 Classification-F1 0.178080012725682 on epoch=93
05/29/2022 15:54:00 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.45 on epoch=94
05/29/2022 15:54:02 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.45 on epoch=94
05/29/2022 15:54:05 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.38 on epoch=94
05/29/2022 15:54:08 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.46 on epoch=95
05/29/2022 15:54:10 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.44 on epoch=95
05/29/2022 15:54:22 - INFO - __main__ - Global step 2300 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=95
05/29/2022 15:54:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.46 on epoch=96
05/29/2022 15:54:27 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.46 on epoch=96
05/29/2022 15:54:30 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.38 on epoch=97
05/29/2022 15:54:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.41 on epoch=97
05/29/2022 15:54:35 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.36 on epoch=97
05/29/2022 15:54:46 - INFO - __main__ - Global step 2350 Train loss 0.41 Classification-F1 0.1948313816896979 on epoch=97
05/29/2022 15:54:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.44 on epoch=98
05/29/2022 15:54:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.43 on epoch=98
05/29/2022 15:54:54 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.41 on epoch=99
05/29/2022 15:54:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.42 on epoch=99
05/29/2022 15:54:59 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.44 on epoch=99
05/29/2022 15:55:11 - INFO - __main__ - Global step 2400 Train loss 0.43 Classification-F1 0.3323618070722601 on epoch=99
05/29/2022 15:55:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.48 on epoch=100
05/29/2022 15:55:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.39 on epoch=100
05/29/2022 15:55:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.42 on epoch=101
05/29/2022 15:55:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.45 on epoch=101
05/29/2022 15:55:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.37 on epoch=102
05/29/2022 15:55:36 - INFO - __main__ - Global step 2450 Train loss 0.42 Classification-F1 0.18631603728630153 on epoch=102
05/29/2022 15:55:38 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.43 on epoch=102
05/29/2022 15:55:41 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.37 on epoch=102
05/29/2022 15:55:44 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.50 on epoch=103
05/29/2022 15:55:46 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.39 on epoch=103
05/29/2022 15:55:49 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.41 on epoch=104
05/29/2022 15:56:00 - INFO - __main__ - Global step 2500 Train loss 0.42 Classification-F1 0.3284753486809642 on epoch=104
05/29/2022 15:56:03 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.40 on epoch=104
05/29/2022 15:56:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.40 on epoch=104
05/29/2022 15:56:08 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.42 on epoch=105
05/29/2022 15:56:11 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.43 on epoch=105
05/29/2022 15:56:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.41 on epoch=106
05/29/2022 15:56:26 - INFO - __main__ - Global step 2550 Train loss 0.41 Classification-F1 0.3273729100947334 on epoch=106
05/29/2022 15:56:28 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.42 on epoch=106
05/29/2022 15:56:31 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.39 on epoch=107
05/29/2022 15:56:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.45 on epoch=107
05/29/2022 15:56:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.36 on epoch=107
05/29/2022 15:56:39 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.45 on epoch=108
05/29/2022 15:56:51 - INFO - __main__ - Global step 2600 Train loss 0.42 Classification-F1 0.31392445222232457 on epoch=108
05/29/2022 15:56:54 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.44 on epoch=108
05/29/2022 15:56:57 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.41 on epoch=109
05/29/2022 15:56:59 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.39 on epoch=109
05/29/2022 15:57:02 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.40 on epoch=109
05/29/2022 15:57:05 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.45 on epoch=110
05/29/2022 15:57:16 - INFO - __main__ - Global step 2650 Train loss 0.42 Classification-F1 0.3318770405694808 on epoch=110
05/29/2022 15:57:19 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.41 on epoch=110
05/29/2022 15:57:21 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.44 on epoch=111
05/29/2022 15:57:24 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.45 on epoch=111
05/29/2022 15:57:27 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.41 on epoch=112
05/29/2022 15:57:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.46 on epoch=112
05/29/2022 15:57:41 - INFO - __main__ - Global step 2700 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=112
05/29/2022 15:57:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.38 on epoch=112
05/29/2022 15:57:47 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.47 on epoch=113
05/29/2022 15:57:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.41 on epoch=113
05/29/2022 15:57:52 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.38 on epoch=114
05/29/2022 15:57:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.44 on epoch=114
05/29/2022 15:58:06 - INFO - __main__ - Global step 2750 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=114
05/29/2022 15:58:09 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.43 on epoch=114
05/29/2022 15:58:12 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.45 on epoch=115
05/29/2022 15:58:15 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.40 on epoch=115
05/29/2022 15:58:17 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.41 on epoch=116
05/29/2022 15:58:20 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.44 on epoch=116
05/29/2022 15:58:32 - INFO - __main__ - Global step 2800 Train loss 0.43 Classification-F1 0.28392687559354224 on epoch=116
05/29/2022 15:58:35 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.39 on epoch=117
05/29/2022 15:58:38 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.45 on epoch=117
05/29/2022 15:58:40 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.39 on epoch=117
05/29/2022 15:58:43 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.43 on epoch=118
05/29/2022 15:58:46 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.44 on epoch=118
05/29/2022 15:58:58 - INFO - __main__ - Global step 2850 Train loss 0.42 Classification-F1 0.26497210092188256 on epoch=118
05/29/2022 15:59:00 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.42 on epoch=119
05/29/2022 15:59:03 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.42 on epoch=119
05/29/2022 15:59:06 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.38 on epoch=119
05/29/2022 15:59:08 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.43 on epoch=120
05/29/2022 15:59:11 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.40 on epoch=120
05/29/2022 15:59:23 - INFO - __main__ - Global step 2900 Train loss 0.41 Classification-F1 0.3219012317372973 on epoch=120
05/29/2022 15:59:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.36 on epoch=121
05/29/2022 15:59:29 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.41 on epoch=121
05/29/2022 15:59:32 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.39 on epoch=122
05/29/2022 15:59:35 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.43 on epoch=122
05/29/2022 15:59:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.36 on epoch=122
05/29/2022 15:59:49 - INFO - __main__ - Global step 2950 Train loss 0.39 Classification-F1 0.1721607831834019 on epoch=122
05/29/2022 15:59:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.38 on epoch=123
05/29/2022 15:59:55 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.40 on epoch=123
05/29/2022 15:59:57 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.44 on epoch=124
05/29/2022 16:00:00 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.39 on epoch=124
05/29/2022 16:00:03 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.38 on epoch=124
05/29/2022 16:00:04 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 16:00:04 - INFO - __main__ - Printing 3 examples
05/29/2022 16:00:04 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/29/2022 16:00:04 - INFO - __main__ - ['contradiction']
05/29/2022 16:00:04 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/29/2022 16:00:04 - INFO - __main__ - ['contradiction']
05/29/2022 16:00:04 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/29/2022 16:00:04 - INFO - __main__ - ['contradiction']
05/29/2022 16:00:04 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:00:04 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:00:05 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 16:00:05 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 16:00:05 - INFO - __main__ - Printing 3 examples
05/29/2022 16:00:05 - INFO - __main__ -  [anli] premise: Dexter Alexander Nottage (born November 14, 1970) is a former American football defensive end in the National Football League (NFL) for the Washington Redskins and the Kansas City Chiefs. He played college football at Florida A&M University and was selected in the sixth round of the 1994 NFL Draft. He played high school football at Hollywood Hills High School. [SEP] hypothesis: Dexter Alexander Nottage (born November 14, 1970) is a former power forward basketball in the NBA.
05/29/2022 16:00:05 - INFO - __main__ - ['contradiction']
05/29/2022 16:00:05 - INFO - __main__ -  [anli] premise: A grasshopper is a sweet, mint-flavored, after-dinner drink. The name of the drink derives from its green color, which comes from crème de menthe. The drink reputedly originated at Tujague's, a landmark bar in the French Quarter of New Orleans, Louisiana, and was invented by its owner, Philip Guichet. The drink gained popularity during the 1950s and 1960s throughout the American South. [SEP] hypothesis: The drink is white
05/29/2022 16:00:05 - INFO - __main__ - ['contradiction']
05/29/2022 16:00:05 - INFO - __main__ -  [anli] premise: Denis Hale Johnson (July 1, 1949 – May 24, 2017) was an American writer best known for his short story collection "Jesus' Son" (1992) and his novel "Tree of Smoke" (2007), which won the National Book Award for Fiction. He also wrote plays, poetry, journalism, and non-fiction. [SEP] hypothesis: Denis Hale Johnson (July 1, 1949 – May 24, 2010) was an American writer best known for his short story collection "Jesus' Son"
05/29/2022 16:00:05 - INFO - __main__ - ['contradiction']
05/29/2022 16:00:05 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:00:05 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:00:05 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 16:00:14 - INFO - __main__ - Global step 3000 Train loss 0.40 Classification-F1 0.3416619080950977 on epoch=124
05/29/2022 16:00:14 - INFO - __main__ - save last model!
05/29/2022 16:00:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 16:00:15 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 16:00:15 - INFO - __main__ - Printing 3 examples
05/29/2022 16:00:15 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 16:00:15 - INFO - __main__ - ['contradiction']
05/29/2022 16:00:15 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 16:00:15 - INFO - __main__ - ['entailment']
05/29/2022 16:00:15 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 16:00:15 - INFO - __main__ - ['contradiction']
05/29/2022 16:00:15 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:00:15 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:00:16 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 16:00:24 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 16:00:24 - INFO - __main__ - task name: anli
05/29/2022 16:00:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 16:00:25 - INFO - __main__ - Starting training!
05/29/2022 16:00:47 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_13_0.3_8_predictions.txt
05/29/2022 16:00:47 - INFO - __main__ - Classification-F1 on test data: 0.2782
05/29/2022 16:00:47 - INFO - __main__ - prefix=anli_128_13, lr=0.3, bsz=8, dev_performance=0.3562773388555008, test_performance=0.27819114596471484
05/29/2022 16:00:47 - INFO - __main__ - Running ... prefix=anli_128_13, lr=0.2, bsz=8 ...
05/29/2022 16:00:48 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 16:00:48 - INFO - __main__ - Printing 3 examples
05/29/2022 16:00:48 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/29/2022 16:00:48 - INFO - __main__ - ['contradiction']
05/29/2022 16:00:48 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/29/2022 16:00:48 - INFO - __main__ - ['contradiction']
05/29/2022 16:00:48 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/29/2022 16:00:48 - INFO - __main__ - ['contradiction']
05/29/2022 16:00:48 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:00:48 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:00:49 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 16:00:49 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 16:00:49 - INFO - __main__ - Printing 3 examples
05/29/2022 16:00:49 - INFO - __main__ -  [anli] premise: Dexter Alexander Nottage (born November 14, 1970) is a former American football defensive end in the National Football League (NFL) for the Washington Redskins and the Kansas City Chiefs. He played college football at Florida A&M University and was selected in the sixth round of the 1994 NFL Draft. He played high school football at Hollywood Hills High School. [SEP] hypothesis: Dexter Alexander Nottage (born November 14, 1970) is a former power forward basketball in the NBA.
05/29/2022 16:00:49 - INFO - __main__ - ['contradiction']
05/29/2022 16:00:49 - INFO - __main__ -  [anli] premise: A grasshopper is a sweet, mint-flavored, after-dinner drink. The name of the drink derives from its green color, which comes from crème de menthe. The drink reputedly originated at Tujague's, a landmark bar in the French Quarter of New Orleans, Louisiana, and was invented by its owner, Philip Guichet. The drink gained popularity during the 1950s and 1960s throughout the American South. [SEP] hypothesis: The drink is white
05/29/2022 16:00:49 - INFO - __main__ - ['contradiction']
05/29/2022 16:00:49 - INFO - __main__ -  [anli] premise: Denis Hale Johnson (July 1, 1949 – May 24, 2017) was an American writer best known for his short story collection "Jesus' Son" (1992) and his novel "Tree of Smoke" (2007), which won the National Book Award for Fiction. He also wrote plays, poetry, journalism, and non-fiction. [SEP] hypothesis: Denis Hale Johnson (July 1, 1949 – May 24, 2010) was an American writer best known for his short story collection "Jesus' Son"
05/29/2022 16:00:49 - INFO - __main__ - ['contradiction']
05/29/2022 16:00:49 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:00:49 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:00:49 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 16:01:05 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 16:01:05 - INFO - __main__ - task name: anli
05/29/2022 16:01:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 16:01:06 - INFO - __main__ - Starting training!
05/29/2022 16:01:10 - INFO - __main__ - Step 10 Global step 10 Train loss 7.38 on epoch=0
05/29/2022 16:01:12 - INFO - __main__ - Step 20 Global step 20 Train loss 4.91 on epoch=0
05/29/2022 16:01:15 - INFO - __main__ - Step 30 Global step 30 Train loss 2.69 on epoch=1
05/29/2022 16:01:18 - INFO - __main__ - Step 40 Global step 40 Train loss 1.76 on epoch=1
05/29/2022 16:01:20 - INFO - __main__ - Step 50 Global step 50 Train loss 1.21 on epoch=2
05/29/2022 16:01:29 - INFO - __main__ - Global step 50 Train loss 3.59 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 16:01:29 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 16:01:32 - INFO - __main__ - Step 60 Global step 60 Train loss 1.04 on epoch=2
05/29/2022 16:01:34 - INFO - __main__ - Step 70 Global step 70 Train loss 0.94 on epoch=2
05/29/2022 16:01:37 - INFO - __main__ - Step 80 Global step 80 Train loss 0.96 on epoch=3
05/29/2022 16:01:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.72 on epoch=3
05/29/2022 16:01:42 - INFO - __main__ - Step 100 Global step 100 Train loss 0.78 on epoch=4
05/29/2022 16:01:52 - INFO - __main__ - Global step 100 Train loss 0.89 Classification-F1 0.16666666666666666 on epoch=4
05/29/2022 16:01:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.65 on epoch=4
05/29/2022 16:01:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.75 on epoch=4
05/29/2022 16:02:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.72 on epoch=5
05/29/2022 16:02:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.65 on epoch=5
05/29/2022 16:02:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.56 on epoch=6
05/29/2022 16:02:14 - INFO - __main__ - Global step 150 Train loss 0.66 Classification-F1 0.1772756769512123 on epoch=6
05/29/2022 16:02:14 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1772756769512123 on epoch=6, global_step=150
05/29/2022 16:02:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.61 on epoch=6
05/29/2022 16:02:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.61 on epoch=7
05/29/2022 16:02:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.57 on epoch=7
05/29/2022 16:02:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=7
05/29/2022 16:02:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.63 on epoch=8
05/29/2022 16:02:37 - INFO - __main__ - Global step 200 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=8
05/29/2022 16:02:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.55 on epoch=8
05/29/2022 16:02:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.56 on epoch=9
05/29/2022 16:02:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.62 on epoch=9
05/29/2022 16:02:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=9
05/29/2022 16:02:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.61 on epoch=10
05/29/2022 16:02:59 - INFO - __main__ - Global step 250 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=10
05/29/2022 16:03:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.55 on epoch=10
05/29/2022 16:03:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.58 on epoch=11
05/29/2022 16:03:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.60 on epoch=11
05/29/2022 16:03:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=12
05/29/2022 16:03:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.53 on epoch=12
05/29/2022 16:03:20 - INFO - __main__ - Global step 300 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=12
05/29/2022 16:03:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.57 on epoch=12
05/29/2022 16:03:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.59 on epoch=13
05/29/2022 16:03:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=13
05/29/2022 16:03:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.57 on epoch=14
05/29/2022 16:03:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.56 on epoch=14
05/29/2022 16:03:41 - INFO - __main__ - Global step 350 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=14
05/29/2022 16:03:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=14
05/29/2022 16:03:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.58 on epoch=15
05/29/2022 16:03:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.53 on epoch=15
05/29/2022 16:03:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.54 on epoch=16
05/29/2022 16:03:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.59 on epoch=16
05/29/2022 16:04:07 - INFO - __main__ - Global step 400 Train loss 0.55 Classification-F1 0.2717952287695035 on epoch=16
05/29/2022 16:04:07 - INFO - __main__ - Saving model with best Classification-F1: 0.1772756769512123 -> 0.2717952287695035 on epoch=16, global_step=400
05/29/2022 16:04:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.52 on epoch=17
05/29/2022 16:04:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.55 on epoch=17
05/29/2022 16:04:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=17
05/29/2022 16:04:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.56 on epoch=18
05/29/2022 16:04:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=18
05/29/2022 16:04:31 - INFO - __main__ - Global step 450 Train loss 0.50 Classification-F1 0.2038354949562133 on epoch=18
05/29/2022 16:04:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=19
05/29/2022 16:04:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.54 on epoch=19
05/29/2022 16:04:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.51 on epoch=19
05/29/2022 16:04:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.51 on epoch=20
05/29/2022 16:04:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.54 on epoch=20
05/29/2022 16:04:56 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.16568819308545338 on epoch=20
05/29/2022 16:04:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=21
05/29/2022 16:05:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.56 on epoch=21
05/29/2022 16:05:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=22
05/29/2022 16:05:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.57 on epoch=22
05/29/2022 16:05:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=22
05/29/2022 16:05:20 - INFO - __main__ - Global step 550 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=22
05/29/2022 16:05:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=23
05/29/2022 16:05:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.51 on epoch=23
05/29/2022 16:05:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=24
05/29/2022 16:05:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.52 on epoch=24
05/29/2022 16:05:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.55 on epoch=24
05/29/2022 16:05:44 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.23690252292973835 on epoch=24
05/29/2022 16:05:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.58 on epoch=25
05/29/2022 16:05:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.49 on epoch=25
05/29/2022 16:05:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=26
05/29/2022 16:05:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.56 on epoch=26
05/29/2022 16:05:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=27
05/29/2022 16:06:09 - INFO - __main__ - Global step 650 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=27
05/29/2022 16:06:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.54 on epoch=27
05/29/2022 16:06:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=27
05/29/2022 16:06:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.53 on epoch=28
05/29/2022 16:06:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=28
05/29/2022 16:06:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.55 on epoch=29
05/29/2022 16:06:34 - INFO - __main__ - Global step 700 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=29
05/29/2022 16:06:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.49 on epoch=29
05/29/2022 16:06:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=29
05/29/2022 16:06:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=30
05/29/2022 16:06:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.49 on epoch=30
05/29/2022 16:06:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=31
05/29/2022 16:06:58 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.2219393954462372 on epoch=31
05/29/2022 16:07:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=31
05/29/2022 16:07:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=32
05/29/2022 16:07:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.50 on epoch=32
05/29/2022 16:07:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=32
05/29/2022 16:07:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.51 on epoch=33
05/29/2022 16:07:23 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.1721607831834019 on epoch=33
05/29/2022 16:07:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=33
05/29/2022 16:07:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.52 on epoch=34
05/29/2022 16:07:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=34
05/29/2022 16:07:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=34
05/29/2022 16:07:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.52 on epoch=35
05/29/2022 16:07:47 - INFO - __main__ - Global step 850 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=35
05/29/2022 16:07:50 - INFO - __main__ - Step 860 Global step 860 Train loss 0.51 on epoch=35
05/29/2022 16:07:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=36
05/29/2022 16:07:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=36
05/29/2022 16:07:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=37
05/29/2022 16:08:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.54 on epoch=37
05/29/2022 16:08:10 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=37
05/29/2022 16:08:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=37
05/29/2022 16:08:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=38
05/29/2022 16:08:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.52 on epoch=38
05/29/2022 16:08:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.52 on epoch=39
05/29/2022 16:08:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=39
05/29/2022 16:08:33 - INFO - __main__ - Global step 950 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=39
05/29/2022 16:08:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.47 on epoch=39
05/29/2022 16:08:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.47 on epoch=40
05/29/2022 16:08:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=40
05/29/2022 16:08:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.49 on epoch=41
05/29/2022 16:08:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.48 on epoch=41
05/29/2022 16:08:54 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.17938720203528566 on epoch=41
05/29/2022 16:08:57 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=42
05/29/2022 16:08:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=42
05/29/2022 16:09:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=42
05/29/2022 16:09:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.47 on epoch=43
05/29/2022 16:09:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=43
05/29/2022 16:09:19 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.22491510134776607 on epoch=43
05/29/2022 16:09:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.47 on epoch=44
05/29/2022 16:09:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.49 on epoch=44
05/29/2022 16:09:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=44
05/29/2022 16:09:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.50 on epoch=45
05/29/2022 16:09:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=45
05/29/2022 16:09:42 - INFO - __main__ - Global step 1100 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=45
05/29/2022 16:09:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.51 on epoch=46
05/29/2022 16:09:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.47 on epoch=46
05/29/2022 16:09:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=47
05/29/2022 16:09:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.52 on epoch=47
05/29/2022 16:09:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.47 on epoch=47
05/29/2022 16:10:07 - INFO - __main__ - Global step 1150 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=47
05/29/2022 16:10:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=48
05/29/2022 16:10:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=48
05/29/2022 16:10:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.49 on epoch=49
05/29/2022 16:10:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=49
05/29/2022 16:10:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.49 on epoch=49
05/29/2022 16:10:28 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.19431913771536416 on epoch=49
05/29/2022 16:10:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.50 on epoch=50
05/29/2022 16:10:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=50
05/29/2022 16:10:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=51
05/29/2022 16:10:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=51
05/29/2022 16:10:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=52
05/29/2022 16:10:52 - INFO - __main__ - Global step 1250 Train loss 0.46 Classification-F1 0.1757055360294227 on epoch=52
05/29/2022 16:10:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.47 on epoch=52
05/29/2022 16:10:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=52
05/29/2022 16:11:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.47 on epoch=53
05/29/2022 16:11:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=53
05/29/2022 16:11:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.52 on epoch=54
05/29/2022 16:11:17 - INFO - __main__ - Global step 1300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=54
05/29/2022 16:11:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.49 on epoch=54
05/29/2022 16:11:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.45 on epoch=54
05/29/2022 16:11:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.52 on epoch=55
05/29/2022 16:11:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.44 on epoch=55
05/29/2022 16:11:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=56
05/29/2022 16:11:40 - INFO - __main__ - Global step 1350 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=56
05/29/2022 16:11:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=56
05/29/2022 16:11:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=57
05/29/2022 16:11:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.49 on epoch=57
05/29/2022 16:11:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.47 on epoch=57
05/29/2022 16:11:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=58
05/29/2022 16:12:02 - INFO - __main__ - Global step 1400 Train loss 0.45 Classification-F1 0.18193384223918574 on epoch=58
05/29/2022 16:12:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.48 on epoch=58
05/29/2022 16:12:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=59
05/29/2022 16:12:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.44 on epoch=59
05/29/2022 16:12:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.47 on epoch=59
05/29/2022 16:12:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.46 on epoch=60
05/29/2022 16:12:25 - INFO - __main__ - Global step 1450 Train loss 0.46 Classification-F1 0.23516605151239642 on epoch=60
05/29/2022 16:12:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.47 on epoch=60
05/29/2022 16:12:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.45 on epoch=61
05/29/2022 16:12:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.45 on epoch=61
05/29/2022 16:12:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.46 on epoch=62
05/29/2022 16:12:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.49 on epoch=62
05/29/2022 16:12:49 - INFO - __main__ - Global step 1500 Train loss 0.47 Classification-F1 0.22771258592154117 on epoch=62
05/29/2022 16:12:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=62
05/29/2022 16:12:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.54 on epoch=63
05/29/2022 16:12:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.46 on epoch=63
05/29/2022 16:13:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=64
05/29/2022 16:13:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.50 on epoch=64
05/29/2022 16:13:13 - INFO - __main__ - Global step 1550 Train loss 0.48 Classification-F1 0.16535433070866143 on epoch=64
05/29/2022 16:13:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.49 on epoch=64
05/29/2022 16:13:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.50 on epoch=65
05/29/2022 16:13:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.46 on epoch=65
05/29/2022 16:13:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.46 on epoch=66
05/29/2022 16:13:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.49 on epoch=66
05/29/2022 16:13:38 - INFO - __main__ - Global step 1600 Train loss 0.48 Classification-F1 0.32449834688926044 on epoch=66
05/29/2022 16:13:38 - INFO - __main__ - Saving model with best Classification-F1: 0.2717952287695035 -> 0.32449834688926044 on epoch=66, global_step=1600
05/29/2022 16:13:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=67
05/29/2022 16:13:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.47 on epoch=67
05/29/2022 16:13:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=67
05/29/2022 16:13:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.47 on epoch=68
05/29/2022 16:13:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.46 on epoch=68
05/29/2022 16:14:01 - INFO - __main__ - Global step 1650 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=68
05/29/2022 16:14:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.49 on epoch=69
05/29/2022 16:14:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=69
05/29/2022 16:14:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.49 on epoch=69
05/29/2022 16:14:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.45 on epoch=70
05/29/2022 16:14:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.45 on epoch=70
05/29/2022 16:14:25 - INFO - __main__ - Global step 1700 Train loss 0.47 Classification-F1 0.2696650870252018 on epoch=70
05/29/2022 16:14:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.45 on epoch=71
05/29/2022 16:14:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.48 on epoch=71
05/29/2022 16:14:33 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=72
05/29/2022 16:14:35 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.48 on epoch=72
05/29/2022 16:14:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.44 on epoch=72
05/29/2022 16:14:50 - INFO - __main__ - Global step 1750 Train loss 0.45 Classification-F1 0.2774249998506865 on epoch=72
05/29/2022 16:14:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.51 on epoch=73
05/29/2022 16:14:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=73
05/29/2022 16:14:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.45 on epoch=74
05/29/2022 16:15:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.43 on epoch=74
05/29/2022 16:15:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.52 on epoch=74
05/29/2022 16:15:14 - INFO - __main__ - Global step 1800 Train loss 0.47 Classification-F1 0.19871260198864182 on epoch=74
05/29/2022 16:15:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.49 on epoch=75
05/29/2022 16:15:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.43 on epoch=75
05/29/2022 16:15:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.41 on epoch=76
05/29/2022 16:15:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.50 on epoch=76
05/29/2022 16:15:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.43 on epoch=77
05/29/2022 16:15:38 - INFO - __main__ - Global step 1850 Train loss 0.45 Classification-F1 0.18291616051030557 on epoch=77
05/29/2022 16:15:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.44 on epoch=77
05/29/2022 16:15:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.41 on epoch=77
05/29/2022 16:15:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.46 on epoch=78
05/29/2022 16:15:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.45 on epoch=78
05/29/2022 16:15:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=79
05/29/2022 16:16:03 - INFO - __main__ - Global step 1900 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=79
05/29/2022 16:16:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.51 on epoch=79
05/29/2022 16:16:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=79
05/29/2022 16:16:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.47 on epoch=80
05/29/2022 16:16:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.45 on epoch=80
05/29/2022 16:16:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.45 on epoch=81
05/29/2022 16:16:27 - INFO - __main__ - Global step 1950 Train loss 0.47 Classification-F1 0.17142884783754894 on epoch=81
05/29/2022 16:16:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.46 on epoch=81
05/29/2022 16:16:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=82
05/29/2022 16:16:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=82
05/29/2022 16:16:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.38 on epoch=82
05/29/2022 16:16:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.50 on epoch=83
05/29/2022 16:16:51 - INFO - __main__ - Global step 2000 Train loss 0.45 Classification-F1 0.24273214063699075 on epoch=83
05/29/2022 16:16:54 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.48 on epoch=83
05/29/2022 16:16:57 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.46 on epoch=84
05/29/2022 16:16:59 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.45 on epoch=84
05/29/2022 16:17:02 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.43 on epoch=84
05/29/2022 16:17:05 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.47 on epoch=85
05/29/2022 16:17:16 - INFO - __main__ - Global step 2050 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=85
05/29/2022 16:17:18 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.45 on epoch=85
05/29/2022 16:17:21 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.44 on epoch=86
05/29/2022 16:17:24 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.47 on epoch=86
05/29/2022 16:17:26 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.43 on epoch=87
05/29/2022 16:17:29 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.47 on epoch=87
05/29/2022 16:17:40 - INFO - __main__ - Global step 2100 Train loss 0.45 Classification-F1 0.17142828576713307 on epoch=87
05/29/2022 16:17:43 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.45 on epoch=87
05/29/2022 16:17:45 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.42 on epoch=88
05/29/2022 16:17:48 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.45 on epoch=88
05/29/2022 16:17:51 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.48 on epoch=89
05/29/2022 16:17:53 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.46 on epoch=89
05/29/2022 16:18:04 - INFO - __main__ - Global step 2150 Train loss 0.45 Classification-F1 0.3090364469267408 on epoch=89
05/29/2022 16:18:07 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.44 on epoch=89
05/29/2022 16:18:10 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.49 on epoch=90
05/29/2022 16:18:12 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.42 on epoch=90
05/29/2022 16:18:15 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.40 on epoch=91
05/29/2022 16:18:18 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.47 on epoch=91
05/29/2022 16:18:29 - INFO - __main__ - Global step 2200 Train loss 0.44 Classification-F1 0.2591082046214918 on epoch=91
05/29/2022 16:18:32 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.40 on epoch=92
05/29/2022 16:18:35 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.47 on epoch=92
05/29/2022 16:18:37 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.40 on epoch=92
05/29/2022 16:18:40 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.47 on epoch=93
05/29/2022 16:18:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.47 on epoch=93
05/29/2022 16:18:54 - INFO - __main__ - Global step 2250 Train loss 0.44 Classification-F1 0.25847376254260884 on epoch=93
05/29/2022 16:18:57 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.47 on epoch=94
05/29/2022 16:18:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.46 on epoch=94
05/29/2022 16:19:02 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.46 on epoch=94
05/29/2022 16:19:05 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.44 on epoch=95
05/29/2022 16:19:07 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.43 on epoch=95
05/29/2022 16:19:19 - INFO - __main__ - Global step 2300 Train loss 0.45 Classification-F1 0.2528837523799081 on epoch=95
05/29/2022 16:19:22 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.42 on epoch=96
05/29/2022 16:19:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.48 on epoch=96
05/29/2022 16:19:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.43 on epoch=97
05/29/2022 16:19:30 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.47 on epoch=97
05/29/2022 16:19:33 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.41 on epoch=97
05/29/2022 16:19:44 - INFO - __main__ - Global step 2350 Train loss 0.44 Classification-F1 0.28070646736153915 on epoch=97
05/29/2022 16:19:47 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.48 on epoch=98
05/29/2022 16:19:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.41 on epoch=98
05/29/2022 16:19:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.42 on epoch=99
05/29/2022 16:19:55 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.46 on epoch=99
05/29/2022 16:19:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.43 on epoch=99
05/29/2022 16:20:10 - INFO - __main__ - Global step 2400 Train loss 0.44 Classification-F1 0.2575385515635444 on epoch=99
05/29/2022 16:20:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.48 on epoch=100
05/29/2022 16:20:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.41 on epoch=100
05/29/2022 16:20:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.46 on epoch=101
05/29/2022 16:20:20 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.47 on epoch=101
05/29/2022 16:20:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.39 on epoch=102
05/29/2022 16:20:35 - INFO - __main__ - Global step 2450 Train loss 0.44 Classification-F1 0.21873233014135884 on epoch=102
05/29/2022 16:20:38 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.51 on epoch=102
05/29/2022 16:20:40 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.45 on epoch=102
05/29/2022 16:20:43 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.47 on epoch=103
05/29/2022 16:20:46 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.43 on epoch=103
05/29/2022 16:20:48 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.48 on epoch=104
05/29/2022 16:21:00 - INFO - __main__ - Global step 2500 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=104
05/29/2022 16:21:02 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.47 on epoch=104
05/29/2022 16:21:05 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.46 on epoch=104
05/29/2022 16:21:08 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.45 on epoch=105
05/29/2022 16:21:10 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.43 on epoch=105
05/29/2022 16:21:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.47 on epoch=106
05/29/2022 16:21:25 - INFO - __main__ - Global step 2550 Train loss 0.46 Classification-F1 0.2723985854045217 on epoch=106
05/29/2022 16:21:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.41 on epoch=106
05/29/2022 16:21:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.40 on epoch=107
05/29/2022 16:21:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.44 on epoch=107
05/29/2022 16:21:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.45 on epoch=107
05/29/2022 16:21:38 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.48 on epoch=108
05/29/2022 16:21:50 - INFO - __main__ - Global step 2600 Train loss 0.44 Classification-F1 0.22599143622408 on epoch=108
05/29/2022 16:21:53 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.39 on epoch=108
05/29/2022 16:21:55 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.47 on epoch=109
05/29/2022 16:21:58 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.45 on epoch=109
05/29/2022 16:22:01 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.43 on epoch=109
05/29/2022 16:22:03 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.42 on epoch=110
05/29/2022 16:22:15 - INFO - __main__ - Global step 2650 Train loss 0.43 Classification-F1 0.16732026143790854 on epoch=110
05/29/2022 16:22:17 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.40 on epoch=110
05/29/2022 16:22:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.43 on epoch=111
05/29/2022 16:22:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.46 on epoch=111
05/29/2022 16:22:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.40 on epoch=112
05/29/2022 16:22:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.47 on epoch=112
05/29/2022 16:22:37 - INFO - __main__ - Global step 2700 Train loss 0.43 Classification-F1 0.17244846656611368 on epoch=112
05/29/2022 16:22:40 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.39 on epoch=112
05/29/2022 16:22:42 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.45 on epoch=113
05/29/2022 16:22:45 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.45 on epoch=113
05/29/2022 16:22:48 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.42 on epoch=114
05/29/2022 16:22:50 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.46 on epoch=114
05/29/2022 16:23:02 - INFO - __main__ - Global step 2750 Train loss 0.43 Classification-F1 0.21088180963066216 on epoch=114
05/29/2022 16:23:05 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.43 on epoch=114
05/29/2022 16:23:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.41 on epoch=115
05/29/2022 16:23:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.42 on epoch=115
05/29/2022 16:23:13 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.41 on epoch=116
05/29/2022 16:23:15 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.47 on epoch=116
05/29/2022 16:23:27 - INFO - __main__ - Global step 2800 Train loss 0.43 Classification-F1 0.272944810225512 on epoch=116
05/29/2022 16:23:30 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.39 on epoch=117
05/29/2022 16:23:33 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.48 on epoch=117
05/29/2022 16:23:35 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.43 on epoch=117
05/29/2022 16:23:38 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.45 on epoch=118
05/29/2022 16:23:41 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.44 on epoch=118
05/29/2022 16:23:52 - INFO - __main__ - Global step 2850 Train loss 0.44 Classification-F1 0.20289710289710292 on epoch=118
05/29/2022 16:23:55 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.47 on epoch=119
05/29/2022 16:23:57 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.41 on epoch=119
05/29/2022 16:24:00 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.44 on epoch=119
05/29/2022 16:24:03 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.46 on epoch=120
05/29/2022 16:24:05 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.41 on epoch=120
05/29/2022 16:24:17 - INFO - __main__ - Global step 2900 Train loss 0.44 Classification-F1 0.23282033990321196 on epoch=120
05/29/2022 16:24:19 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.44 on epoch=121
05/29/2022 16:24:22 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.43 on epoch=121
05/29/2022 16:24:25 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.37 on epoch=122
05/29/2022 16:24:27 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.48 on epoch=122
05/29/2022 16:24:30 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.38 on epoch=122
05/29/2022 16:24:42 - INFO - __main__ - Global step 2950 Train loss 0.42 Classification-F1 0.24724695283518808 on epoch=122
05/29/2022 16:24:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.48 on epoch=123
05/29/2022 16:24:47 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.43 on epoch=123
05/29/2022 16:24:50 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.46 on epoch=124
05/29/2022 16:24:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.43 on epoch=124
05/29/2022 16:24:55 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.46 on epoch=124
05/29/2022 16:24:56 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 16:24:56 - INFO - __main__ - Printing 3 examples
05/29/2022 16:24:56 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/29/2022 16:24:56 - INFO - __main__ - ['entailment']
05/29/2022 16:24:56 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/29/2022 16:24:56 - INFO - __main__ - ['entailment']
05/29/2022 16:24:56 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/29/2022 16:24:56 - INFO - __main__ - ['entailment']
05/29/2022 16:24:56 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:24:56 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:24:57 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 16:24:57 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 16:24:57 - INFO - __main__ - Printing 3 examples
05/29/2022 16:24:57 - INFO - __main__ -  [anli] premise: Terenzo is a "comune" (municipality) in the Province of Parma in the Italian region Emilia-Romagna, located about 100 km west of Bologna and about 30 km southwest of Parma. As of 31 December 2004, it had a population of 1,250 and an area of 72.4 km2 . [SEP] hypothesis: Terenzo has a small population.
05/29/2022 16:24:57 - INFO - __main__ - ['entailment']
05/29/2022 16:24:57 - INFO - __main__ -  [anli] premise: Brett McLaughlin, known professionally as Leland, is an American singer, songwriter, record producer, composer and lecturer. Based in Los Angeles, California, he has worked closely with a range of popular artists, including Troye Sivan, Daya, Capital Cities, Andy Grammer, Hilary Duff and Allie X. [SEP] hypothesis: Leland has worked with at least 6 popular artists.
05/29/2022 16:24:57 - INFO - __main__ - ['entailment']
05/29/2022 16:24:57 - INFO - __main__ -  [anli] premise: Amazon Fire TV refers to two digital media players and microconsoles developed by Amazon.com. It is a small network appliance and entertainment device designed to stream digital audio/video content to a high-definition television. The device also allows users to play video games with the included remote, via a mobile app, or with an optional game controller. [SEP] hypothesis: Amazon Fire TV refers to the digital media players and micro consoles developed by amazon.com
05/29/2022 16:24:57 - INFO - __main__ - ['entailment']
05/29/2022 16:24:57 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:24:57 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:24:57 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 16:25:06 - INFO - __main__ - Global step 3000 Train loss 0.45 Classification-F1 0.1647058823529412 on epoch=124
05/29/2022 16:25:06 - INFO - __main__ - save last model!
05/29/2022 16:25:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 16:25:06 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 16:25:06 - INFO - __main__ - Printing 3 examples
05/29/2022 16:25:06 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 16:25:06 - INFO - __main__ - ['contradiction']
05/29/2022 16:25:06 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 16:25:06 - INFO - __main__ - ['entailment']
05/29/2022 16:25:06 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 16:25:06 - INFO - __main__ - ['contradiction']
05/29/2022 16:25:06 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:25:07 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:25:08 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 16:25:16 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 16:25:16 - INFO - __main__ - task name: anli
05/29/2022 16:25:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 16:25:17 - INFO - __main__ - Starting training!
05/29/2022 16:25:37 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_13_0.2_8_predictions.txt
05/29/2022 16:25:37 - INFO - __main__ - Classification-F1 on test data: 0.1684
05/29/2022 16:25:37 - INFO - __main__ - prefix=anli_128_13, lr=0.2, bsz=8, dev_performance=0.32449834688926044, test_performance=0.168394279467214
05/29/2022 16:25:37 - INFO - __main__ - Running ... prefix=anli_128_21, lr=0.5, bsz=8 ...
05/29/2022 16:25:38 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 16:25:38 - INFO - __main__ - Printing 3 examples
05/29/2022 16:25:38 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/29/2022 16:25:38 - INFO - __main__ - ['entailment']
05/29/2022 16:25:38 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/29/2022 16:25:38 - INFO - __main__ - ['entailment']
05/29/2022 16:25:38 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/29/2022 16:25:38 - INFO - __main__ - ['entailment']
05/29/2022 16:25:38 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:25:39 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:25:39 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 16:25:39 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 16:25:39 - INFO - __main__ - Printing 3 examples
05/29/2022 16:25:39 - INFO - __main__ -  [anli] premise: Terenzo is a "comune" (municipality) in the Province of Parma in the Italian region Emilia-Romagna, located about 100 km west of Bologna and about 30 km southwest of Parma. As of 31 December 2004, it had a population of 1,250 and an area of 72.4 km2 . [SEP] hypothesis: Terenzo has a small population.
05/29/2022 16:25:39 - INFO - __main__ - ['entailment']
05/29/2022 16:25:39 - INFO - __main__ -  [anli] premise: Brett McLaughlin, known professionally as Leland, is an American singer, songwriter, record producer, composer and lecturer. Based in Los Angeles, California, he has worked closely with a range of popular artists, including Troye Sivan, Daya, Capital Cities, Andy Grammer, Hilary Duff and Allie X. [SEP] hypothesis: Leland has worked with at least 6 popular artists.
05/29/2022 16:25:39 - INFO - __main__ - ['entailment']
05/29/2022 16:25:39 - INFO - __main__ -  [anli] premise: Amazon Fire TV refers to two digital media players and microconsoles developed by Amazon.com. It is a small network appliance and entertainment device designed to stream digital audio/video content to a high-definition television. The device also allows users to play video games with the included remote, via a mobile app, or with an optional game controller. [SEP] hypothesis: Amazon Fire TV refers to the digital media players and micro consoles developed by amazon.com
05/29/2022 16:25:39 - INFO - __main__ - ['entailment']
05/29/2022 16:25:39 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:25:39 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:25:40 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 16:25:56 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 16:25:56 - INFO - __main__ - task name: anli
05/29/2022 16:25:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 16:25:57 - INFO - __main__ - Starting training!
05/29/2022 16:26:00 - INFO - __main__ - Step 10 Global step 10 Train loss 6.03 on epoch=0
05/29/2022 16:26:02 - INFO - __main__ - Step 20 Global step 20 Train loss 2.32 on epoch=0
05/29/2022 16:26:05 - INFO - __main__ - Step 30 Global step 30 Train loss 1.17 on epoch=1
05/29/2022 16:26:08 - INFO - __main__ - Step 40 Global step 40 Train loss 0.91 on epoch=1
05/29/2022 16:26:10 - INFO - __main__ - Step 50 Global step 50 Train loss 0.73 on epoch=2
05/29/2022 16:26:19 - INFO - __main__ - Global step 50 Train loss 2.23 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 16:26:19 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 16:26:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.82 on epoch=2
05/29/2022 16:26:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.62 on epoch=2
05/29/2022 16:26:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.67 on epoch=3
05/29/2022 16:26:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.60 on epoch=3
05/29/2022 16:26:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.59 on epoch=4
05/29/2022 16:26:43 - INFO - __main__ - Global step 100 Train loss 0.66 Classification-F1 0.21138211382113822 on epoch=4
05/29/2022 16:26:43 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.21138211382113822 on epoch=4, global_step=100
05/29/2022 16:26:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.65 on epoch=4
05/29/2022 16:26:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.56 on epoch=4
05/29/2022 16:26:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.58 on epoch=5
05/29/2022 16:26:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=5
05/29/2022 16:26:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.63 on epoch=6
05/29/2022 16:27:07 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.17489304486004156 on epoch=6
05/29/2022 16:27:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.59 on epoch=6
05/29/2022 16:27:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.53 on epoch=7
05/29/2022 16:27:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.58 on epoch=7
05/29/2022 16:27:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=7
05/29/2022 16:27:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.55 on epoch=8
05/29/2022 16:27:32 - INFO - __main__ - Global step 200 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=8
05/29/2022 16:27:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=8
05/29/2022 16:27:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=9
05/29/2022 16:27:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.59 on epoch=9
05/29/2022 16:27:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.53 on epoch=9
05/29/2022 16:27:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.55 on epoch=10
05/29/2022 16:27:56 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=10
05/29/2022 16:27:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=10
05/29/2022 16:28:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=11
05/29/2022 16:28:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=11
05/29/2022 16:28:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=12
05/29/2022 16:28:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=12
05/29/2022 16:28:21 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.3402087287407505 on epoch=12
05/29/2022 16:28:21 - INFO - __main__ - Saving model with best Classification-F1: 0.21138211382113822 -> 0.3402087287407505 on epoch=12, global_step=300
05/29/2022 16:28:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=12
05/29/2022 16:28:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=13
05/29/2022 16:28:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=13
05/29/2022 16:28:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=14
05/29/2022 16:28:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=14
05/29/2022 16:28:43 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.17659563749937032 on epoch=14
05/29/2022 16:28:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=14
05/29/2022 16:28:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=15
05/29/2022 16:28:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=15
05/29/2022 16:28:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=16
05/29/2022 16:28:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=16
05/29/2022 16:29:07 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.17659563749937032 on epoch=16
05/29/2022 16:29:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.51 on epoch=17
05/29/2022 16:29:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.54 on epoch=17
05/29/2022 16:29:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=17
05/29/2022 16:29:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=18
05/29/2022 16:29:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=18
05/29/2022 16:29:32 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.34957663255360627 on epoch=18
05/29/2022 16:29:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3402087287407505 -> 0.34957663255360627 on epoch=18, global_step=450
05/29/2022 16:29:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=19
05/29/2022 16:29:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=19
05/29/2022 16:29:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.50 on epoch=19
05/29/2022 16:29:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.52 on epoch=20
05/29/2022 16:29:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=20
05/29/2022 16:29:56 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.17464063083790535 on epoch=20
05/29/2022 16:29:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.54 on epoch=21
05/29/2022 16:30:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.53 on epoch=21
05/29/2022 16:30:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=22
05/29/2022 16:30:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.49 on epoch=22
05/29/2022 16:30:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=22
05/29/2022 16:30:21 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.31086296625047305 on epoch=22
05/29/2022 16:30:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=23
05/29/2022 16:30:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=23
05/29/2022 16:30:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=24
05/29/2022 16:30:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=24
05/29/2022 16:30:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=24
05/29/2022 16:30:45 - INFO - __main__ - Global step 600 Train loss 0.48 Classification-F1 0.3177938196555218 on epoch=24
05/29/2022 16:30:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=25
05/29/2022 16:30:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=25
05/29/2022 16:30:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.49 on epoch=26
05/29/2022 16:30:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=26
05/29/2022 16:30:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.47 on epoch=27
05/29/2022 16:31:09 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=27
05/29/2022 16:31:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=27
05/29/2022 16:31:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=27
05/29/2022 16:31:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.45 on epoch=28
05/29/2022 16:31:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=28
05/29/2022 16:31:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=29
05/29/2022 16:31:34 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.2992257065427797 on epoch=29
05/29/2022 16:31:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=29
05/29/2022 16:31:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=29
05/29/2022 16:31:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.45 on epoch=30
05/29/2022 16:31:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=30
05/29/2022 16:31:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.50 on epoch=31
05/29/2022 16:31:55 - INFO - __main__ - Global step 750 Train loss 0.46 Classification-F1 0.1965714285714286 on epoch=31
05/29/2022 16:31:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.51 on epoch=31
05/29/2022 16:32:00 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=32
05/29/2022 16:32:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=32
05/29/2022 16:32:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=32
05/29/2022 16:32:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=33
05/29/2022 16:32:19 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=33
05/29/2022 16:32:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=33
05/29/2022 16:32:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.45 on epoch=34
05/29/2022 16:32:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.48 on epoch=34
05/29/2022 16:32:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=34
05/29/2022 16:32:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.50 on epoch=35
05/29/2022 16:32:43 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=35
05/29/2022 16:32:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=35
05/29/2022 16:32:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.53 on epoch=36
05/29/2022 16:32:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.51 on epoch=36
05/29/2022 16:32:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.50 on epoch=37
05/29/2022 16:32:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.53 on epoch=37
05/29/2022 16:33:08 - INFO - __main__ - Global step 900 Train loss 0.50 Classification-F1 0.16731450922870064 on epoch=37
05/29/2022 16:33:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=37
05/29/2022 16:33:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.46 on epoch=38
05/29/2022 16:33:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=38
05/29/2022 16:33:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.49 on epoch=39
05/29/2022 16:33:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=39
05/29/2022 16:33:32 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.23819764464925755 on epoch=39
05/29/2022 16:33:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=39
05/29/2022 16:33:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.50 on epoch=40
05/29/2022 16:33:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=40
05/29/2022 16:33:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=41
05/29/2022 16:33:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.50 on epoch=41
05/29/2022 16:33:57 - INFO - __main__ - Global step 1000 Train loss 0.46 Classification-F1 0.27595573368769244 on epoch=41
05/29/2022 16:33:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.48 on epoch=42
05/29/2022 16:34:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.49 on epoch=42
05/29/2022 16:34:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=42
05/29/2022 16:34:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.50 on epoch=43
05/29/2022 16:34:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=43
05/29/2022 16:34:21 - INFO - __main__ - Global step 1050 Train loss 0.46 Classification-F1 0.1927333536029188 on epoch=43
05/29/2022 16:34:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=44
05/29/2022 16:34:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=44
05/29/2022 16:34:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=44
05/29/2022 16:34:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=45
05/29/2022 16:34:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=45
05/29/2022 16:34:46 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.25503175058525196 on epoch=45
05/29/2022 16:34:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=46
05/29/2022 16:34:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=46
05/29/2022 16:34:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.48 on epoch=47
05/29/2022 16:34:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=47
05/29/2022 16:34:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=47
05/29/2022 16:35:11 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.26696855619800647 on epoch=47
05/29/2022 16:35:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=48
05/29/2022 16:35:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=48
05/29/2022 16:35:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.48 on epoch=49
05/29/2022 16:35:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.47 on epoch=49
05/29/2022 16:35:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=49
05/29/2022 16:35:35 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.1721607831834019 on epoch=49
05/29/2022 16:35:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.46 on epoch=50
05/29/2022 16:35:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.44 on epoch=50
05/29/2022 16:35:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.49 on epoch=51
05/29/2022 16:35:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.48 on epoch=51
05/29/2022 16:35:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.47 on epoch=52
05/29/2022 16:35:57 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=52
05/29/2022 16:35:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.49 on epoch=52
05/29/2022 16:36:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=52
05/29/2022 16:36:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=53
05/29/2022 16:36:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.44 on epoch=53
05/29/2022 16:36:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=54
05/29/2022 16:36:23 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.23973374592201627 on epoch=54
05/29/2022 16:36:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=54
05/29/2022 16:36:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=54
05/29/2022 16:36:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.46 on epoch=55
05/29/2022 16:36:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.44 on epoch=55
05/29/2022 16:36:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=56
05/29/2022 16:36:48 - INFO - __main__ - Global step 1350 Train loss 0.44 Classification-F1 0.22083333333333333 on epoch=56
05/29/2022 16:36:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.50 on epoch=56
05/29/2022 16:36:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=57
05/29/2022 16:36:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=57
05/29/2022 16:36:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.44 on epoch=57
05/29/2022 16:37:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.46 on epoch=58
05/29/2022 16:37:13 - INFO - __main__ - Global step 1400 Train loss 0.47 Classification-F1 0.2263522299224351 on epoch=58
05/29/2022 16:37:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=58
05/29/2022 16:37:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=59
05/29/2022 16:37:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.45 on epoch=59
05/29/2022 16:37:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=59
05/29/2022 16:37:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.50 on epoch=60
05/29/2022 16:37:36 - INFO - __main__ - Global step 1450 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=60
05/29/2022 16:37:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.39 on epoch=60
05/29/2022 16:37:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.47 on epoch=61
05/29/2022 16:37:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.45 on epoch=61
05/29/2022 16:37:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.46 on epoch=62
05/29/2022 16:37:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.45 on epoch=62
05/29/2022 16:38:00 - INFO - __main__ - Global step 1500 Train loss 0.44 Classification-F1 0.20960141370508542 on epoch=62
05/29/2022 16:38:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.41 on epoch=62
05/29/2022 16:38:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=63
05/29/2022 16:38:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=63
05/29/2022 16:38:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.43 on epoch=64
05/29/2022 16:38:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.47 on epoch=64
05/29/2022 16:38:21 - INFO - __main__ - Global step 1550 Train loss 0.43 Classification-F1 0.17146743237116516 on epoch=64
05/29/2022 16:38:24 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.44 on epoch=64
05/29/2022 16:38:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.47 on epoch=65
05/29/2022 16:38:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=65
05/29/2022 16:38:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.51 on epoch=66
05/29/2022 16:38:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=66
05/29/2022 16:38:46 - INFO - __main__ - Global step 1600 Train loss 0.46 Classification-F1 0.3277435992400705 on epoch=66
05/29/2022 16:38:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.41 on epoch=67
05/29/2022 16:38:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.44 on epoch=67
05/29/2022 16:38:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=67
05/29/2022 16:38:56 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.46 on epoch=68
05/29/2022 16:38:59 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.40 on epoch=68
05/29/2022 16:39:09 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.22759036909980304 on epoch=68
05/29/2022 16:39:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=69
05/29/2022 16:39:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.49 on epoch=69
05/29/2022 16:39:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.42 on epoch=69
05/29/2022 16:39:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.48 on epoch=70
05/29/2022 16:39:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=70
05/29/2022 16:39:34 - INFO - __main__ - Global step 1700 Train loss 0.44 Classification-F1 0.2846528702001063 on epoch=70
05/29/2022 16:39:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.49 on epoch=71
05/29/2022 16:39:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.47 on epoch=71
05/29/2022 16:39:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.46 on epoch=72
05/29/2022 16:39:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.44 on epoch=72
05/29/2022 16:39:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=72
05/29/2022 16:39:56 - INFO - __main__ - Global step 1750 Train loss 0.45 Classification-F1 0.24403883969174753 on epoch=72
05/29/2022 16:39:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.47 on epoch=73
05/29/2022 16:40:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=73
05/29/2022 16:40:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.45 on epoch=74
05/29/2022 16:40:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=74
05/29/2022 16:40:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=74
05/29/2022 16:40:17 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.28378090366581415 on epoch=74
05/29/2022 16:40:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.49 on epoch=75
05/29/2022 16:40:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.45 on epoch=75
05/29/2022 16:40:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.45 on epoch=76
05/29/2022 16:40:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.46 on epoch=76
05/29/2022 16:40:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=77
05/29/2022 16:40:41 - INFO - __main__ - Global step 1850 Train loss 0.45 Classification-F1 0.17114127702362994 on epoch=77
05/29/2022 16:40:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=77
05/29/2022 16:40:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=77
05/29/2022 16:40:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.48 on epoch=78
05/29/2022 16:40:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.49 on epoch=78
05/29/2022 16:40:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.46 on epoch=79
05/29/2022 16:41:05 - INFO - __main__ - Global step 1900 Train loss 0.45 Classification-F1 0.3474174627115803 on epoch=79
05/29/2022 16:41:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.45 on epoch=79
05/29/2022 16:41:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.42 on epoch=79
05/29/2022 16:41:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.44 on epoch=80
05/29/2022 16:41:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=80
05/29/2022 16:41:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.48 on epoch=81
05/29/2022 16:41:26 - INFO - __main__ - Global step 1950 Train loss 0.44 Classification-F1 0.3008350369913888 on epoch=81
05/29/2022 16:41:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.48 on epoch=81
05/29/2022 16:41:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.46 on epoch=82
05/29/2022 16:41:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.43 on epoch=82
05/29/2022 16:41:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.41 on epoch=82
05/29/2022 16:41:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.47 on epoch=83
05/29/2022 16:41:48 - INFO - __main__ - Global step 2000 Train loss 0.45 Classification-F1 0.1721607831834019 on epoch=83
05/29/2022 16:41:51 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.41 on epoch=83
05/29/2022 16:41:54 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.60 on epoch=84
05/29/2022 16:41:56 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.46 on epoch=84
05/29/2022 16:41:59 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.43 on epoch=84
05/29/2022 16:42:02 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.46 on epoch=85
05/29/2022 16:42:12 - INFO - __main__ - Global step 2050 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=85
05/29/2022 16:42:15 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.41 on epoch=85
05/29/2022 16:42:17 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.46 on epoch=86
05/29/2022 16:42:20 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.45 on epoch=86
05/29/2022 16:42:23 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.44 on epoch=87
05/29/2022 16:42:25 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.43 on epoch=87
05/29/2022 16:42:36 - INFO - __main__ - Global step 2100 Train loss 0.44 Classification-F1 0.20324471710974779 on epoch=87
05/29/2022 16:42:39 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.41 on epoch=87
05/29/2022 16:42:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.43 on epoch=88
05/29/2022 16:42:44 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.43 on epoch=88
05/29/2022 16:42:47 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.41 on epoch=89
05/29/2022 16:42:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.42 on epoch=89
05/29/2022 16:43:01 - INFO - __main__ - Global step 2150 Train loss 0.42 Classification-F1 0.23980121643673047 on epoch=89
05/29/2022 16:43:04 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.42 on epoch=89
05/29/2022 16:43:06 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.48 on epoch=90
05/29/2022 16:43:09 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.40 on epoch=90
05/29/2022 16:43:12 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.44 on epoch=91
05/29/2022 16:43:14 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.49 on epoch=91
05/29/2022 16:43:24 - INFO - __main__ - Global step 2200 Train loss 0.45 Classification-F1 0.18921363952679193 on epoch=91
05/29/2022 16:43:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.43 on epoch=92
05/29/2022 16:43:30 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.46 on epoch=92
05/29/2022 16:43:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.43 on epoch=92
05/29/2022 16:43:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.45 on epoch=93
05/29/2022 16:43:37 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.46 on epoch=93
05/29/2022 16:43:46 - INFO - __main__ - Global step 2250 Train loss 0.44 Classification-F1 0.17647058823529413 on epoch=93
05/29/2022 16:43:49 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.42 on epoch=94
05/29/2022 16:43:52 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.41 on epoch=94
05/29/2022 16:43:54 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.41 on epoch=94
05/29/2022 16:43:57 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.42 on epoch=95
05/29/2022 16:44:00 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.44 on epoch=95
05/29/2022 16:44:11 - INFO - __main__ - Global step 2300 Train loss 0.42 Classification-F1 0.23905756918797413 on epoch=95
05/29/2022 16:44:14 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.45 on epoch=96
05/29/2022 16:44:16 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.42 on epoch=96
05/29/2022 16:44:19 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.39 on epoch=97
05/29/2022 16:44:22 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.40 on epoch=97
05/29/2022 16:44:24 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.42 on epoch=97
05/29/2022 16:44:36 - INFO - __main__ - Global step 2350 Train loss 0.42 Classification-F1 0.2669040128056522 on epoch=97
05/29/2022 16:44:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.45 on epoch=98
05/29/2022 16:44:41 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.41 on epoch=98
05/29/2022 16:44:44 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.43 on epoch=99
05/29/2022 16:44:46 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.44 on epoch=99
05/29/2022 16:44:49 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.44 on epoch=99
05/29/2022 16:45:00 - INFO - __main__ - Global step 2400 Train loss 0.44 Classification-F1 0.18182343091402506 on epoch=99
05/29/2022 16:45:03 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.43 on epoch=100
05/29/2022 16:45:05 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.44 on epoch=100
05/29/2022 16:45:08 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.47 on epoch=101
05/29/2022 16:45:10 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.45 on epoch=101
05/29/2022 16:45:13 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.42 on epoch=102
05/29/2022 16:45:24 - INFO - __main__ - Global step 2450 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=102
05/29/2022 16:45:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.41 on epoch=102
05/29/2022 16:45:30 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.43 on epoch=102
05/29/2022 16:45:32 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.41 on epoch=103
05/29/2022 16:45:35 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.41 on epoch=103
05/29/2022 16:45:38 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.41 on epoch=104
05/29/2022 16:45:49 - INFO - __main__ - Global step 2500 Train loss 0.42 Classification-F1 0.27099311701081613 on epoch=104
05/29/2022 16:45:52 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.41 on epoch=104
05/29/2022 16:45:54 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.42 on epoch=104
05/29/2022 16:45:57 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.43 on epoch=105
05/29/2022 16:46:00 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.44 on epoch=105
05/29/2022 16:46:02 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.45 on epoch=106
05/29/2022 16:46:14 - INFO - __main__ - Global step 2550 Train loss 0.43 Classification-F1 0.3848827280199829 on epoch=106
05/29/2022 16:46:14 - INFO - __main__ - Saving model with best Classification-F1: 0.34957663255360627 -> 0.3848827280199829 on epoch=106, global_step=2550
05/29/2022 16:46:16 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.41 on epoch=106
05/29/2022 16:46:19 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.46 on epoch=107
05/29/2022 16:46:22 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.48 on epoch=107
05/29/2022 16:46:24 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.43 on epoch=107
05/29/2022 16:46:27 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.47 on epoch=108
05/29/2022 16:46:38 - INFO - __main__ - Global step 2600 Train loss 0.45 Classification-F1 0.19535546148672708 on epoch=108
05/29/2022 16:46:40 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.43 on epoch=108
05/29/2022 16:46:43 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.41 on epoch=109
05/29/2022 16:46:46 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.44 on epoch=109
05/29/2022 16:46:48 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.41 on epoch=109
05/29/2022 16:46:51 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.45 on epoch=110
05/29/2022 16:47:00 - INFO - __main__ - Global step 2650 Train loss 0.43 Classification-F1 0.16601307189542483 on epoch=110
05/29/2022 16:47:02 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.40 on epoch=110
05/29/2022 16:47:05 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.47 on epoch=111
05/29/2022 16:47:07 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.42 on epoch=111
05/29/2022 16:47:10 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.44 on epoch=112
05/29/2022 16:47:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.43 on epoch=112
05/29/2022 16:47:24 - INFO - __main__ - Global step 2700 Train loss 0.43 Classification-F1 0.22453437900829332 on epoch=112
05/29/2022 16:47:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.42 on epoch=112
05/29/2022 16:47:29 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.39 on epoch=113
05/29/2022 16:47:32 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.43 on epoch=113
05/29/2022 16:47:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.44 on epoch=114
05/29/2022 16:47:37 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.47 on epoch=114
05/29/2022 16:47:48 - INFO - __main__ - Global step 2750 Train loss 0.43 Classification-F1 0.25449957313215704 on epoch=114
05/29/2022 16:47:51 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.40 on epoch=114
05/29/2022 16:47:53 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.45 on epoch=115
05/29/2022 16:47:56 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.38 on epoch=115
05/29/2022 16:47:59 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.45 on epoch=116
05/29/2022 16:48:01 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.45 on epoch=116
05/29/2022 16:48:13 - INFO - __main__ - Global step 2800 Train loss 0.43 Classification-F1 0.36369829026512096 on epoch=116
05/29/2022 16:48:15 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.45 on epoch=117
05/29/2022 16:48:18 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.44 on epoch=117
05/29/2022 16:48:21 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.43 on epoch=117
05/29/2022 16:48:23 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.49 on epoch=118
05/29/2022 16:48:26 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.44 on epoch=118
05/29/2022 16:48:38 - INFO - __main__ - Global step 2850 Train loss 0.45 Classification-F1 0.22499294491905897 on epoch=118
05/29/2022 16:48:40 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.43 on epoch=119
05/29/2022 16:48:43 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.45 on epoch=119
05/29/2022 16:48:46 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.43 on epoch=119
05/29/2022 16:48:48 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.42 on epoch=120
05/29/2022 16:48:51 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.42 on epoch=120
05/29/2022 16:49:02 - INFO - __main__ - Global step 2900 Train loss 0.43 Classification-F1 0.25750244090855445 on epoch=120
05/29/2022 16:49:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.46 on epoch=121
05/29/2022 16:49:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.46 on epoch=121
05/29/2022 16:49:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.43 on epoch=122
05/29/2022 16:49:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.46 on epoch=122
05/29/2022 16:49:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.41 on epoch=122
05/29/2022 16:49:25 - INFO - __main__ - Global step 2950 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=122
05/29/2022 16:49:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.44 on epoch=123
05/29/2022 16:49:31 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.41 on epoch=123
05/29/2022 16:49:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.44 on epoch=124
05/29/2022 16:49:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.47 on epoch=124
05/29/2022 16:49:39 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.43 on epoch=124
05/29/2022 16:49:40 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 16:49:40 - INFO - __main__ - Printing 3 examples
05/29/2022 16:49:40 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/29/2022 16:49:40 - INFO - __main__ - ['entailment']
05/29/2022 16:49:40 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/29/2022 16:49:40 - INFO - __main__ - ['entailment']
05/29/2022 16:49:40 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/29/2022 16:49:40 - INFO - __main__ - ['entailment']
05/29/2022 16:49:40 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:49:40 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:49:41 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 16:49:41 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 16:49:41 - INFO - __main__ - Printing 3 examples
05/29/2022 16:49:41 - INFO - __main__ -  [anli] premise: Terenzo is a "comune" (municipality) in the Province of Parma in the Italian region Emilia-Romagna, located about 100 km west of Bologna and about 30 km southwest of Parma. As of 31 December 2004, it had a population of 1,250 and an area of 72.4 km2 . [SEP] hypothesis: Terenzo has a small population.
05/29/2022 16:49:41 - INFO - __main__ - ['entailment']
05/29/2022 16:49:41 - INFO - __main__ -  [anli] premise: Brett McLaughlin, known professionally as Leland, is an American singer, songwriter, record producer, composer and lecturer. Based in Los Angeles, California, he has worked closely with a range of popular artists, including Troye Sivan, Daya, Capital Cities, Andy Grammer, Hilary Duff and Allie X. [SEP] hypothesis: Leland has worked with at least 6 popular artists.
05/29/2022 16:49:41 - INFO - __main__ - ['entailment']
05/29/2022 16:49:41 - INFO - __main__ -  [anli] premise: Amazon Fire TV refers to two digital media players and microconsoles developed by Amazon.com. It is a small network appliance and entertainment device designed to stream digital audio/video content to a high-definition television. The device also allows users to play video games with the included remote, via a mobile app, or with an optional game controller. [SEP] hypothesis: Amazon Fire TV refers to the digital media players and micro consoles developed by amazon.com
05/29/2022 16:49:41 - INFO - __main__ - ['entailment']
05/29/2022 16:49:41 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:49:41 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:49:41 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 16:49:48 - INFO - __main__ - Global step 3000 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=124
05/29/2022 16:49:48 - INFO - __main__ - save last model!
05/29/2022 16:49:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 16:49:48 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 16:49:48 - INFO - __main__ - Printing 3 examples
05/29/2022 16:49:48 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 16:49:48 - INFO - __main__ - ['contradiction']
05/29/2022 16:49:48 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 16:49:48 - INFO - __main__ - ['entailment']
05/29/2022 16:49:48 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 16:49:48 - INFO - __main__ - ['contradiction']
05/29/2022 16:49:48 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:49:49 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:49:50 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 16:50:00 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 16:50:00 - INFO - __main__ - task name: anli
05/29/2022 16:50:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 16:50:01 - INFO - __main__ - Starting training!
05/29/2022 16:50:14 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_21_0.5_8_predictions.txt
05/29/2022 16:50:14 - INFO - __main__ - Classification-F1 on test data: 0.1665
05/29/2022 16:50:14 - INFO - __main__ - prefix=anli_128_21, lr=0.5, bsz=8, dev_performance=0.3848827280199829, test_performance=0.16654163540885222
05/29/2022 16:50:14 - INFO - __main__ - Running ... prefix=anli_128_21, lr=0.4, bsz=8 ...
05/29/2022 16:50:15 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 16:50:15 - INFO - __main__ - Printing 3 examples
05/29/2022 16:50:15 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/29/2022 16:50:15 - INFO - __main__ - ['entailment']
05/29/2022 16:50:15 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/29/2022 16:50:15 - INFO - __main__ - ['entailment']
05/29/2022 16:50:15 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/29/2022 16:50:15 - INFO - __main__ - ['entailment']
05/29/2022 16:50:15 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:50:16 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:50:16 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 16:50:16 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 16:50:16 - INFO - __main__ - Printing 3 examples
05/29/2022 16:50:16 - INFO - __main__ -  [anli] premise: Terenzo is a "comune" (municipality) in the Province of Parma in the Italian region Emilia-Romagna, located about 100 km west of Bologna and about 30 km southwest of Parma. As of 31 December 2004, it had a population of 1,250 and an area of 72.4 km2 . [SEP] hypothesis: Terenzo has a small population.
05/29/2022 16:50:16 - INFO - __main__ - ['entailment']
05/29/2022 16:50:16 - INFO - __main__ -  [anli] premise: Brett McLaughlin, known professionally as Leland, is an American singer, songwriter, record producer, composer and lecturer. Based in Los Angeles, California, he has worked closely with a range of popular artists, including Troye Sivan, Daya, Capital Cities, Andy Grammer, Hilary Duff and Allie X. [SEP] hypothesis: Leland has worked with at least 6 popular artists.
05/29/2022 16:50:16 - INFO - __main__ - ['entailment']
05/29/2022 16:50:16 - INFO - __main__ -  [anli] premise: Amazon Fire TV refers to two digital media players and microconsoles developed by Amazon.com. It is a small network appliance and entertainment device designed to stream digital audio/video content to a high-definition television. The device also allows users to play video games with the included remote, via a mobile app, or with an optional game controller. [SEP] hypothesis: Amazon Fire TV refers to the digital media players and micro consoles developed by amazon.com
05/29/2022 16:50:16 - INFO - __main__ - ['entailment']
05/29/2022 16:50:16 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:50:16 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:50:17 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 16:50:32 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 16:50:32 - INFO - __main__ - task name: anli
05/29/2022 16:50:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 16:50:33 - INFO - __main__ - Starting training!
05/29/2022 16:50:36 - INFO - __main__ - Step 10 Global step 10 Train loss 6.71 on epoch=0
05/29/2022 16:50:39 - INFO - __main__ - Step 20 Global step 20 Train loss 2.91 on epoch=0
05/29/2022 16:50:42 - INFO - __main__ - Step 30 Global step 30 Train loss 1.36 on epoch=1
05/29/2022 16:50:44 - INFO - __main__ - Step 40 Global step 40 Train loss 0.99 on epoch=1
05/29/2022 16:50:47 - INFO - __main__ - Step 50 Global step 50 Train loss 0.82 on epoch=2
05/29/2022 16:50:58 - INFO - __main__ - Global step 50 Train loss 2.56 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 16:50:58 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 16:51:01 - INFO - __main__ - Step 60 Global step 60 Train loss 0.81 on epoch=2
05/29/2022 16:51:03 - INFO - __main__ - Step 70 Global step 70 Train loss 0.69 on epoch=2
05/29/2022 16:51:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.70 on epoch=3
05/29/2022 16:51:09 - INFO - __main__ - Step 90 Global step 90 Train loss 0.68 on epoch=3
05/29/2022 16:51:11 - INFO - __main__ - Step 100 Global step 100 Train loss 0.61 on epoch=4
05/29/2022 16:51:22 - INFO - __main__ - Global step 100 Train loss 0.70 Classification-F1 0.2541230199458047 on epoch=4
05/29/2022 16:51:22 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2541230199458047 on epoch=4, global_step=100
05/29/2022 16:51:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.60 on epoch=4
05/29/2022 16:51:27 - INFO - __main__ - Step 120 Global step 120 Train loss 0.55 on epoch=4
05/29/2022 16:51:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.68 on epoch=5
05/29/2022 16:51:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=5
05/29/2022 16:51:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.57 on epoch=6
05/29/2022 16:51:43 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.18115144847818113 on epoch=6
05/29/2022 16:51:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=6
05/29/2022 16:51:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.60 on epoch=7
05/29/2022 16:51:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.61 on epoch=7
05/29/2022 16:51:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=7
05/29/2022 16:51:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.61 on epoch=8
05/29/2022 16:52:07 - INFO - __main__ - Global step 200 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=8
05/29/2022 16:52:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=8
05/29/2022 16:52:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=9
05/29/2022 16:52:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.63 on epoch=9
05/29/2022 16:52:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.56 on epoch=9
05/29/2022 16:52:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=10
05/29/2022 16:52:29 - INFO - __main__ - Global step 250 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=10
05/29/2022 16:52:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=10
05/29/2022 16:52:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.53 on epoch=11
05/29/2022 16:52:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=11
05/29/2022 16:52:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.56 on epoch=12
05/29/2022 16:52:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=12
05/29/2022 16:52:53 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.25083625018432426 on epoch=12
05/29/2022 16:52:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=12
05/29/2022 16:52:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.48 on epoch=13
05/29/2022 16:53:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.53 on epoch=13
05/29/2022 16:53:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=14
05/29/2022 16:53:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=14
05/29/2022 16:53:18 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.24711415695966546 on epoch=14
05/29/2022 16:53:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=14
05/29/2022 16:53:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.54 on epoch=15
05/29/2022 16:53:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=15
05/29/2022 16:53:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.51 on epoch=16
05/29/2022 16:53:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=16
05/29/2022 16:53:42 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=16
05/29/2022 16:53:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.53 on epoch=17
05/29/2022 16:53:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=17
05/29/2022 16:53:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.49 on epoch=17
05/29/2022 16:53:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.57 on epoch=18
05/29/2022 16:53:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=18
05/29/2022 16:54:06 - INFO - __main__ - Global step 450 Train loss 0.51 Classification-F1 0.3368512233107899 on epoch=18
05/29/2022 16:54:06 - INFO - __main__ - Saving model with best Classification-F1: 0.2541230199458047 -> 0.3368512233107899 on epoch=18, global_step=450
05/29/2022 16:54:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=19
05/29/2022 16:54:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.53 on epoch=19
05/29/2022 16:54:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.57 on epoch=19
05/29/2022 16:54:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=20
05/29/2022 16:54:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=20
05/29/2022 16:54:31 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.24851190476190474 on epoch=20
05/29/2022 16:54:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=21
05/29/2022 16:54:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.54 on epoch=21
05/29/2022 16:54:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.54 on epoch=22
05/29/2022 16:54:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=22
05/29/2022 16:54:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=22
05/29/2022 16:54:56 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.2650512894257501 on epoch=22
05/29/2022 16:54:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.50 on epoch=23
05/29/2022 16:55:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.49 on epoch=23
05/29/2022 16:55:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=24
05/29/2022 16:55:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=24
05/29/2022 16:55:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.44 on epoch=24
05/29/2022 16:55:21 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.3052154734075571 on epoch=24
05/29/2022 16:55:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=25
05/29/2022 16:55:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=25
05/29/2022 16:55:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=26
05/29/2022 16:55:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=26
05/29/2022 16:55:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=27
05/29/2022 16:55:45 - INFO - __main__ - Global step 650 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=27
05/29/2022 16:55:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=27
05/29/2022 16:55:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=27
05/29/2022 16:55:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=28
05/29/2022 16:55:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.50 on epoch=28
05/29/2022 16:55:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=29
05/29/2022 16:56:10 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.2091601955708944 on epoch=29
05/29/2022 16:56:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.50 on epoch=29
05/29/2022 16:56:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.49 on epoch=29
05/29/2022 16:56:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.52 on epoch=30
05/29/2022 16:56:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=30
05/29/2022 16:56:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.51 on epoch=31
05/29/2022 16:56:35 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.26986480266050156 on epoch=31
05/29/2022 16:56:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.48 on epoch=31
05/29/2022 16:56:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=32
05/29/2022 16:56:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=32
05/29/2022 16:56:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=32
05/29/2022 16:56:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=33
05/29/2022 16:56:59 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.22825791384743352 on epoch=33
05/29/2022 16:57:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=33
05/29/2022 16:57:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=34
05/29/2022 16:57:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.47 on epoch=34
05/29/2022 16:57:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=34
05/29/2022 16:57:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=35
05/29/2022 16:57:23 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=35
05/29/2022 16:57:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.48 on epoch=35
05/29/2022 16:57:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=36
05/29/2022 16:57:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.49 on epoch=36
05/29/2022 16:57:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=37
05/29/2022 16:57:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.49 on epoch=37
05/29/2022 16:57:48 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.18840913420071007 on epoch=37
05/29/2022 16:57:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=37
05/29/2022 16:57:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.46 on epoch=38
05/29/2022 16:57:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.47 on epoch=38
05/29/2022 16:57:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=39
05/29/2022 16:58:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.49 on epoch=39
05/29/2022 16:58:13 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.2589967679754192 on epoch=39
05/29/2022 16:58:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.48 on epoch=39
05/29/2022 16:58:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=40
05/29/2022 16:58:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=40
05/29/2022 16:58:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.47 on epoch=41
05/29/2022 16:58:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=41
05/29/2022 16:58:38 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.19872393401805166 on epoch=41
05/29/2022 16:58:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.49 on epoch=42
05/29/2022 16:58:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.50 on epoch=42
05/29/2022 16:58:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=42
05/29/2022 16:58:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=43
05/29/2022 16:58:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.44 on epoch=43
05/29/2022 16:59:03 - INFO - __main__ - Global step 1050 Train loss 0.46 Classification-F1 0.21219047619047618 on epoch=43
05/29/2022 16:59:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.43 on epoch=44
05/29/2022 16:59:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=44
05/29/2022 16:59:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=44
05/29/2022 16:59:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.50 on epoch=45
05/29/2022 16:59:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=45
05/29/2022 16:59:28 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.27361583478873674 on epoch=45
05/29/2022 16:59:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.50 on epoch=46
05/29/2022 16:59:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=46
05/29/2022 16:59:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=47
05/29/2022 16:59:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=47
05/29/2022 16:59:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=47
05/29/2022 16:59:53 - INFO - __main__ - Global step 1150 Train loss 0.47 Classification-F1 0.23887746017131886 on epoch=47
05/29/2022 16:59:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=48
05/29/2022 16:59:58 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=48
05/29/2022 17:00:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=49
05/29/2022 17:00:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=49
05/29/2022 17:00:06 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=49
05/29/2022 17:00:17 - INFO - __main__ - Global step 1200 Train loss 0.45 Classification-F1 0.2614873809776645 on epoch=49
05/29/2022 17:00:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=50
05/29/2022 17:00:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=50
05/29/2022 17:00:25 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.50 on epoch=51
05/29/2022 17:00:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=51
05/29/2022 17:00:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.42 on epoch=52
05/29/2022 17:00:41 - INFO - __main__ - Global step 1250 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=52
05/29/2022 17:00:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.48 on epoch=52
05/29/2022 17:00:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=52
05/29/2022 17:00:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.47 on epoch=53
05/29/2022 17:00:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.44 on epoch=53
05/29/2022 17:00:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=54
05/29/2022 17:01:06 - INFO - __main__ - Global step 1300 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=54
05/29/2022 17:01:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=54
05/29/2022 17:01:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.45 on epoch=54
05/29/2022 17:01:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=55
05/29/2022 17:01:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.46 on epoch=55
05/29/2022 17:01:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=56
05/29/2022 17:01:31 - INFO - __main__ - Global step 1350 Train loss 0.46 Classification-F1 0.2806958299130173 on epoch=56
05/29/2022 17:01:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.51 on epoch=56
05/29/2022 17:01:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.48 on epoch=57
05/29/2022 17:01:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.49 on epoch=57
05/29/2022 17:01:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=57
05/29/2022 17:01:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.47 on epoch=58
05/29/2022 17:01:55 - INFO - __main__ - Global step 1400 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=58
05/29/2022 17:01:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.41 on epoch=58
05/29/2022 17:02:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=59
05/29/2022 17:02:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.46 on epoch=59
05/29/2022 17:02:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.48 on epoch=59
05/29/2022 17:02:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.44 on epoch=60
05/29/2022 17:02:19 - INFO - __main__ - Global step 1450 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=60
05/29/2022 17:02:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.39 on epoch=60
05/29/2022 17:02:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.47 on epoch=61
05/29/2022 17:02:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.45 on epoch=61
05/29/2022 17:02:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=62
05/29/2022 17:02:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.48 on epoch=62
05/29/2022 17:02:44 - INFO - __main__ - Global step 1500 Train loss 0.44 Classification-F1 0.18457719220947807 on epoch=62
05/29/2022 17:02:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.46 on epoch=62
05/29/2022 17:02:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.44 on epoch=63
05/29/2022 17:02:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.44 on epoch=63
05/29/2022 17:02:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.50 on epoch=64
05/29/2022 17:02:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.46 on epoch=64
05/29/2022 17:03:09 - INFO - __main__ - Global step 1550 Train loss 0.46 Classification-F1 0.26267287550167107 on epoch=64
05/29/2022 17:03:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.44 on epoch=64
05/29/2022 17:03:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.50 on epoch=65
05/29/2022 17:03:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.39 on epoch=65
05/29/2022 17:03:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=66
05/29/2022 17:03:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.47 on epoch=66
05/29/2022 17:03:33 - INFO - __main__ - Global step 1600 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=66
05/29/2022 17:03:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.48 on epoch=67
05/29/2022 17:03:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.51 on epoch=67
05/29/2022 17:03:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=67
05/29/2022 17:03:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=68
05/29/2022 17:03:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.44 on epoch=68
05/29/2022 17:03:58 - INFO - __main__ - Global step 1650 Train loss 0.46 Classification-F1 0.31172064238809316 on epoch=68
05/29/2022 17:04:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=69
05/29/2022 17:04:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=69
05/29/2022 17:04:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.39 on epoch=69
05/29/2022 17:04:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.47 on epoch=70
05/29/2022 17:04:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.44 on epoch=70
05/29/2022 17:04:23 - INFO - __main__ - Global step 1700 Train loss 0.43 Classification-F1 0.18882922416613737 on epoch=70
05/29/2022 17:04:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.44 on epoch=71
05/29/2022 17:04:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.50 on epoch=71
05/29/2022 17:04:31 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.43 on epoch=72
05/29/2022 17:04:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=72
05/29/2022 17:04:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=72
05/29/2022 17:04:48 - INFO - __main__ - Global step 1750 Train loss 0.44 Classification-F1 0.2568929397631033 on epoch=72
05/29/2022 17:04:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.46 on epoch=73
05/29/2022 17:04:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.45 on epoch=73
05/29/2022 17:04:56 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.43 on epoch=74
05/29/2022 17:04:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.49 on epoch=74
05/29/2022 17:05:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.44 on epoch=74
05/29/2022 17:05:13 - INFO - __main__ - Global step 1800 Train loss 0.46 Classification-F1 0.19772906368651053 on epoch=74
05/29/2022 17:05:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.42 on epoch=75
05/29/2022 17:05:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=75
05/29/2022 17:05:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.50 on epoch=76
05/29/2022 17:05:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.47 on epoch=76
05/29/2022 17:05:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.43 on epoch=77
05/29/2022 17:05:37 - INFO - __main__ - Global step 1850 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=77
05/29/2022 17:05:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.49 on epoch=77
05/29/2022 17:05:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=77
05/29/2022 17:05:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=78
05/29/2022 17:05:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.44 on epoch=78
05/29/2022 17:05:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.42 on epoch=79
05/29/2022 17:06:02 - INFO - __main__ - Global step 1900 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=79
05/29/2022 17:06:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.47 on epoch=79
05/29/2022 17:06:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.44 on epoch=79
05/29/2022 17:06:10 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.48 on epoch=80
05/29/2022 17:06:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.44 on epoch=80
05/29/2022 17:06:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.44 on epoch=81
05/29/2022 17:06:26 - INFO - __main__ - Global step 1950 Train loss 0.45 Classification-F1 0.22617723396366332 on epoch=81
05/29/2022 17:06:29 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.45 on epoch=81
05/29/2022 17:06:32 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.45 on epoch=82
05/29/2022 17:06:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.44 on epoch=82
05/29/2022 17:06:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=82
05/29/2022 17:06:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.43 on epoch=83
05/29/2022 17:06:51 - INFO - __main__ - Global step 2000 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=83
05/29/2022 17:06:53 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.43 on epoch=83
05/29/2022 17:06:56 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.47 on epoch=84
05/29/2022 17:06:59 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.43 on epoch=84
05/29/2022 17:07:01 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.48 on epoch=84
05/29/2022 17:07:04 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.44 on epoch=85
05/29/2022 17:07:15 - INFO - __main__ - Global step 2050 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=85
05/29/2022 17:07:18 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.42 on epoch=85
05/29/2022 17:07:20 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.48 on epoch=86
05/29/2022 17:07:23 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.44 on epoch=86
05/29/2022 17:07:26 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.48 on epoch=87
05/29/2022 17:07:28 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.47 on epoch=87
05/29/2022 17:07:40 - INFO - __main__ - Global step 2100 Train loss 0.45 Classification-F1 0.1932427725531174 on epoch=87
05/29/2022 17:07:43 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.42 on epoch=87
05/29/2022 17:07:45 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.45 on epoch=88
05/29/2022 17:07:48 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.45 on epoch=88
05/29/2022 17:07:51 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.43 on epoch=89
05/29/2022 17:07:53 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.42 on epoch=89
05/29/2022 17:08:05 - INFO - __main__ - Global step 2150 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=89
05/29/2022 17:08:08 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.44 on epoch=89
05/29/2022 17:08:10 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.44 on epoch=90
05/29/2022 17:08:13 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.39 on epoch=90
05/29/2022 17:08:16 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.46 on epoch=91
05/29/2022 17:08:18 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.44 on epoch=91
05/29/2022 17:08:30 - INFO - __main__ - Global step 2200 Train loss 0.43 Classification-F1 0.1775766716943188 on epoch=91
05/29/2022 17:08:33 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.45 on epoch=92
05/29/2022 17:08:35 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.47 on epoch=92
05/29/2022 17:08:38 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.40 on epoch=92
05/29/2022 17:08:41 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.45 on epoch=93
05/29/2022 17:08:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.46 on epoch=93
05/29/2022 17:08:55 - INFO - __main__ - Global step 2250 Train loss 0.45 Classification-F1 0.1721607831834019 on epoch=93
05/29/2022 17:08:58 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.43 on epoch=94
05/29/2022 17:09:01 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.48 on epoch=94
05/29/2022 17:09:03 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.46 on epoch=94
05/29/2022 17:09:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.46 on epoch=95
05/29/2022 17:09:09 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.41 on epoch=95
05/29/2022 17:09:21 - INFO - __main__ - Global step 2300 Train loss 0.45 Classification-F1 0.27595274539817105 on epoch=95
05/29/2022 17:09:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.45 on epoch=96
05/29/2022 17:09:26 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.45 on epoch=96
05/29/2022 17:09:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.47 on epoch=97
05/29/2022 17:09:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.45 on epoch=97
05/29/2022 17:09:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.44 on epoch=97
05/29/2022 17:09:46 - INFO - __main__ - Global step 2350 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=97
05/29/2022 17:09:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.43 on epoch=98
05/29/2022 17:09:51 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.45 on epoch=98
05/29/2022 17:09:54 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.42 on epoch=99
05/29/2022 17:09:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.42 on epoch=99
05/29/2022 17:10:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.39 on epoch=99
05/29/2022 17:10:12 - INFO - __main__ - Global step 2400 Train loss 0.42 Classification-F1 0.1721607831834019 on epoch=99
05/29/2022 17:10:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.42 on epoch=100
05/29/2022 17:10:17 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.45 on epoch=100
05/29/2022 17:10:20 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.44 on epoch=101
05/29/2022 17:10:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.42 on epoch=101
05/29/2022 17:10:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.41 on epoch=102
05/29/2022 17:10:37 - INFO - __main__ - Global step 2450 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=102
05/29/2022 17:10:39 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.45 on epoch=102
05/29/2022 17:10:42 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.41 on epoch=102
05/29/2022 17:10:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.45 on epoch=103
05/29/2022 17:10:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.42 on epoch=103
05/29/2022 17:10:50 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.46 on epoch=104
05/29/2022 17:11:02 - INFO - __main__ - Global step 2500 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=104
05/29/2022 17:11:05 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.52 on epoch=104
05/29/2022 17:11:07 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.43 on epoch=104
05/29/2022 17:11:10 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.46 on epoch=105
05/29/2022 17:11:13 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.42 on epoch=105
05/29/2022 17:11:16 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.47 on epoch=106
05/29/2022 17:11:28 - INFO - __main__ - Global step 2550 Train loss 0.46 Classification-F1 0.24405896402808272 on epoch=106
05/29/2022 17:11:30 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.46 on epoch=106
05/29/2022 17:11:33 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.45 on epoch=107
05/29/2022 17:11:36 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.46 on epoch=107
05/29/2022 17:11:38 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.40 on epoch=107
05/29/2022 17:11:41 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.40 on epoch=108
05/29/2022 17:11:53 - INFO - __main__ - Global step 2600 Train loss 0.43 Classification-F1 0.20653818136370483 on epoch=108
05/29/2022 17:11:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.40 on epoch=108
05/29/2022 17:11:58 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.44 on epoch=109
05/29/2022 17:12:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.41 on epoch=109
05/29/2022 17:12:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.42 on epoch=109
05/29/2022 17:12:06 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.45 on epoch=110
05/29/2022 17:12:18 - INFO - __main__ - Global step 2650 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=110
05/29/2022 17:12:20 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.41 on epoch=110
05/29/2022 17:12:23 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.43 on epoch=111
05/29/2022 17:12:26 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.44 on epoch=111
05/29/2022 17:12:29 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.46 on epoch=112
05/29/2022 17:12:31 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.45 on epoch=112
05/29/2022 17:12:43 - INFO - __main__ - Global step 2700 Train loss 0.44 Classification-F1 0.24520393728440285 on epoch=112
05/29/2022 17:12:46 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.43 on epoch=112
05/29/2022 17:12:48 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.42 on epoch=113
05/29/2022 17:12:51 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.42 on epoch=113
05/29/2022 17:12:54 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.43 on epoch=114
05/29/2022 17:12:57 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.44 on epoch=114
05/29/2022 17:13:09 - INFO - __main__ - Global step 2750 Train loss 0.43 Classification-F1 0.16699282452707112 on epoch=114
05/29/2022 17:13:11 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.43 on epoch=114
05/29/2022 17:13:14 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.44 on epoch=115
05/29/2022 17:13:17 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.39 on epoch=115
05/29/2022 17:13:19 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.44 on epoch=116
05/29/2022 17:13:22 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.47 on epoch=116
05/29/2022 17:13:34 - INFO - __main__ - Global step 2800 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=116
05/29/2022 17:13:37 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.45 on epoch=117
05/29/2022 17:13:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.40 on epoch=117
05/29/2022 17:13:42 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.41 on epoch=117
05/29/2022 17:13:45 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.47 on epoch=118
05/29/2022 17:13:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.41 on epoch=118
05/29/2022 17:13:59 - INFO - __main__ - Global step 2850 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=118
05/29/2022 17:14:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.45 on epoch=119
05/29/2022 17:14:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.45 on epoch=119
05/29/2022 17:14:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.42 on epoch=119
05/29/2022 17:14:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.46 on epoch=120
05/29/2022 17:14:13 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.42 on epoch=120
05/29/2022 17:14:25 - INFO - __main__ - Global step 2900 Train loss 0.44 Classification-F1 0.1952333444185789 on epoch=120
05/29/2022 17:14:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.43 on epoch=121
05/29/2022 17:14:30 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.47 on epoch=121
05/29/2022 17:14:33 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.41 on epoch=122
05/29/2022 17:14:35 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.41 on epoch=122
05/29/2022 17:14:38 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.40 on epoch=122
05/29/2022 17:14:50 - INFO - __main__ - Global step 2950 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=122
05/29/2022 17:14:53 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.44 on epoch=123
05/29/2022 17:14:55 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.41 on epoch=123
05/29/2022 17:14:58 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.42 on epoch=124
05/29/2022 17:15:01 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.42 on epoch=124
05/29/2022 17:15:04 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.41 on epoch=124
05/29/2022 17:15:05 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 17:15:05 - INFO - __main__ - Printing 3 examples
05/29/2022 17:15:05 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/29/2022 17:15:05 - INFO - __main__ - ['entailment']
05/29/2022 17:15:05 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/29/2022 17:15:05 - INFO - __main__ - ['entailment']
05/29/2022 17:15:05 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/29/2022 17:15:05 - INFO - __main__ - ['entailment']
05/29/2022 17:15:05 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:15:05 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:15:05 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 17:15:05 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 17:15:05 - INFO - __main__ - Printing 3 examples
05/29/2022 17:15:05 - INFO - __main__ -  [anli] premise: Terenzo is a "comune" (municipality) in the Province of Parma in the Italian region Emilia-Romagna, located about 100 km west of Bologna and about 30 km southwest of Parma. As of 31 December 2004, it had a population of 1,250 and an area of 72.4 km2 . [SEP] hypothesis: Terenzo has a small population.
05/29/2022 17:15:05 - INFO - __main__ - ['entailment']
05/29/2022 17:15:05 - INFO - __main__ -  [anli] premise: Brett McLaughlin, known professionally as Leland, is an American singer, songwriter, record producer, composer and lecturer. Based in Los Angeles, California, he has worked closely with a range of popular artists, including Troye Sivan, Daya, Capital Cities, Andy Grammer, Hilary Duff and Allie X. [SEP] hypothesis: Leland has worked with at least 6 popular artists.
05/29/2022 17:15:05 - INFO - __main__ - ['entailment']
05/29/2022 17:15:05 - INFO - __main__ -  [anli] premise: Amazon Fire TV refers to two digital media players and microconsoles developed by Amazon.com. It is a small network appliance and entertainment device designed to stream digital audio/video content to a high-definition television. The device also allows users to play video games with the included remote, via a mobile app, or with an optional game controller. [SEP] hypothesis: Amazon Fire TV refers to the digital media players and micro consoles developed by amazon.com
05/29/2022 17:15:05 - INFO - __main__ - ['entailment']
05/29/2022 17:15:05 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:15:06 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:15:06 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 17:15:16 - INFO - __main__ - Global step 3000 Train loss 0.42 Classification-F1 0.20817694621902436 on epoch=124
05/29/2022 17:15:16 - INFO - __main__ - save last model!
05/29/2022 17:15:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 17:15:16 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 17:15:16 - INFO - __main__ - Printing 3 examples
05/29/2022 17:15:16 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 17:15:16 - INFO - __main__ - ['contradiction']
05/29/2022 17:15:16 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 17:15:16 - INFO - __main__ - ['entailment']
05/29/2022 17:15:16 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 17:15:16 - INFO - __main__ - ['contradiction']
05/29/2022 17:15:16 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:15:16 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:15:17 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 17:15:25 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 17:15:25 - INFO - __main__ - task name: anli
05/29/2022 17:15:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 17:15:26 - INFO - __main__ - Starting training!
05/29/2022 17:15:48 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_21_0.4_8_predictions.txt
05/29/2022 17:15:48 - INFO - __main__ - Classification-F1 on test data: 0.1906
05/29/2022 17:15:49 - INFO - __main__ - prefix=anli_128_21, lr=0.4, bsz=8, dev_performance=0.3368512233107899, test_performance=0.19062389462344845
05/29/2022 17:15:49 - INFO - __main__ - Running ... prefix=anli_128_21, lr=0.3, bsz=8 ...
05/29/2022 17:15:50 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 17:15:50 - INFO - __main__ - Printing 3 examples
05/29/2022 17:15:50 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/29/2022 17:15:50 - INFO - __main__ - ['entailment']
05/29/2022 17:15:50 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/29/2022 17:15:50 - INFO - __main__ - ['entailment']
05/29/2022 17:15:50 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/29/2022 17:15:50 - INFO - __main__ - ['entailment']
05/29/2022 17:15:50 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:15:50 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:15:50 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 17:15:50 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 17:15:50 - INFO - __main__ - Printing 3 examples
05/29/2022 17:15:50 - INFO - __main__ -  [anli] premise: Terenzo is a "comune" (municipality) in the Province of Parma in the Italian region Emilia-Romagna, located about 100 km west of Bologna and about 30 km southwest of Parma. As of 31 December 2004, it had a population of 1,250 and an area of 72.4 km2 . [SEP] hypothesis: Terenzo has a small population.
05/29/2022 17:15:50 - INFO - __main__ - ['entailment']
05/29/2022 17:15:50 - INFO - __main__ -  [anli] premise: Brett McLaughlin, known professionally as Leland, is an American singer, songwriter, record producer, composer and lecturer. Based in Los Angeles, California, he has worked closely with a range of popular artists, including Troye Sivan, Daya, Capital Cities, Andy Grammer, Hilary Duff and Allie X. [SEP] hypothesis: Leland has worked with at least 6 popular artists.
05/29/2022 17:15:50 - INFO - __main__ - ['entailment']
05/29/2022 17:15:50 - INFO - __main__ -  [anli] premise: Amazon Fire TV refers to two digital media players and microconsoles developed by Amazon.com. It is a small network appliance and entertainment device designed to stream digital audio/video content to a high-definition television. The device also allows users to play video games with the included remote, via a mobile app, or with an optional game controller. [SEP] hypothesis: Amazon Fire TV refers to the digital media players and micro consoles developed by amazon.com
05/29/2022 17:15:50 - INFO - __main__ - ['entailment']
05/29/2022 17:15:50 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:15:51 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:15:51 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 17:16:10 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 17:16:10 - INFO - __main__ - task name: anli
05/29/2022 17:16:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 17:16:11 - INFO - __main__ - Starting training!
05/29/2022 17:16:14 - INFO - __main__ - Step 10 Global step 10 Train loss 6.70 on epoch=0
05/29/2022 17:16:16 - INFO - __main__ - Step 20 Global step 20 Train loss 3.22 on epoch=0
05/29/2022 17:16:19 - INFO - __main__ - Step 30 Global step 30 Train loss 1.49 on epoch=1
05/29/2022 17:16:22 - INFO - __main__ - Step 40 Global step 40 Train loss 1.21 on epoch=1
05/29/2022 17:16:24 - INFO - __main__ - Step 50 Global step 50 Train loss 0.93 on epoch=2
05/29/2022 17:16:34 - INFO - __main__ - Global step 50 Train loss 2.71 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 17:16:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 17:16:37 - INFO - __main__ - Step 60 Global step 60 Train loss 0.83 on epoch=2
05/29/2022 17:16:40 - INFO - __main__ - Step 70 Global step 70 Train loss 0.78 on epoch=2
05/29/2022 17:16:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.62 on epoch=3
05/29/2022 17:16:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.64 on epoch=3
05/29/2022 17:16:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.58 on epoch=4
05/29/2022 17:16:59 - INFO - __main__ - Global step 100 Train loss 0.69 Classification-F1 0.3637954988998633 on epoch=4
05/29/2022 17:16:59 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.3637954988998633 on epoch=4, global_step=100
05/29/2022 17:17:02 - INFO - __main__ - Step 110 Global step 110 Train loss 0.74 on epoch=4
05/29/2022 17:17:05 - INFO - __main__ - Step 120 Global step 120 Train loss 0.68 on epoch=4
05/29/2022 17:17:07 - INFO - __main__ - Step 130 Global step 130 Train loss 0.57 on epoch=5
05/29/2022 17:17:10 - INFO - __main__ - Step 140 Global step 140 Train loss 0.60 on epoch=5
05/29/2022 17:17:13 - INFO - __main__ - Step 150 Global step 150 Train loss 0.66 on epoch=6
05/29/2022 17:17:24 - INFO - __main__ - Global step 150 Train loss 0.65 Classification-F1 0.16666666666666666 on epoch=6
05/29/2022 17:17:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.59 on epoch=6
05/29/2022 17:17:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=7
05/29/2022 17:17:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.60 on epoch=7
05/29/2022 17:17:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.55 on epoch=7
05/29/2022 17:17:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.58 on epoch=8
05/29/2022 17:17:46 - INFO - __main__ - Global step 200 Train loss 0.57 Classification-F1 0.1650294695481336 on epoch=8
05/29/2022 17:17:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=8
05/29/2022 17:17:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.59 on epoch=9
05/29/2022 17:17:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.58 on epoch=9
05/29/2022 17:17:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.55 on epoch=9
05/29/2022 17:17:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.68 on epoch=10
05/29/2022 17:18:12 - INFO - __main__ - Global step 250 Train loss 0.58 Classification-F1 0.25799853473314666 on epoch=10
05/29/2022 17:18:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.94 on epoch=10
05/29/2022 17:18:17 - INFO - __main__ - Step 270 Global step 270 Train loss 1.11 on epoch=11
05/29/2022 17:18:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.74 on epoch=11
05/29/2022 17:18:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.59 on epoch=12
05/29/2022 17:18:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.59 on epoch=12
05/29/2022 17:18:37 - INFO - __main__ - Global step 300 Train loss 0.79 Classification-F1 0.26283499513008207 on epoch=12
05/29/2022 17:18:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=12
05/29/2022 17:18:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.52 on epoch=13
05/29/2022 17:18:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.58 on epoch=13
05/29/2022 17:18:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.55 on epoch=14
05/29/2022 17:18:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.57 on epoch=14
05/29/2022 17:19:02 - INFO - __main__ - Global step 350 Train loss 0.54 Classification-F1 0.2634647712760676 on epoch=14
05/29/2022 17:19:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=14
05/29/2022 17:19:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=15
05/29/2022 17:19:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=15
05/29/2022 17:19:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.63 on epoch=16
05/29/2022 17:19:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.56 on epoch=16
05/29/2022 17:19:27 - INFO - __main__ - Global step 400 Train loss 0.54 Classification-F1 0.2537426024500898 on epoch=16
05/29/2022 17:19:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=17
05/29/2022 17:19:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.52 on epoch=17
05/29/2022 17:19:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=17
05/29/2022 17:19:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.56 on epoch=18
05/29/2022 17:19:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.52 on epoch=18
05/29/2022 17:19:52 - INFO - __main__ - Global step 450 Train loss 0.51 Classification-F1 0.258975353866177 on epoch=18
05/29/2022 17:19:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=19
05/29/2022 17:19:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.60 on epoch=19
05/29/2022 17:20:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=19
05/29/2022 17:20:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.53 on epoch=20
05/29/2022 17:20:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.48 on epoch=20
05/29/2022 17:20:18 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.25703618200732475 on epoch=20
05/29/2022 17:20:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.49 on epoch=21
05/29/2022 17:20:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.56 on epoch=21
05/29/2022 17:20:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=22
05/29/2022 17:20:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.56 on epoch=22
05/29/2022 17:20:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=22
05/29/2022 17:20:43 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.26700709098698583 on epoch=22
05/29/2022 17:20:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.52 on epoch=23
05/29/2022 17:20:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=23
05/29/2022 17:20:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=24
05/29/2022 17:20:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.63 on epoch=24
05/29/2022 17:20:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=24
05/29/2022 17:21:07 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.27355042818960346 on epoch=24
05/29/2022 17:21:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.56 on epoch=25
05/29/2022 17:21:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.53 on epoch=25
05/29/2022 17:21:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=26
05/29/2022 17:21:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=26
05/29/2022 17:21:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.54 on epoch=27
05/29/2022 17:21:32 - INFO - __main__ - Global step 650 Train loss 0.53 Classification-F1 0.3184459336175295 on epoch=27
05/29/2022 17:21:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=27
05/29/2022 17:21:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=27
05/29/2022 17:21:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=28
05/29/2022 17:21:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.49 on epoch=28
05/29/2022 17:21:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.56 on epoch=29
05/29/2022 17:21:57 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.27708256534850434 on epoch=29
05/29/2022 17:22:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.54 on epoch=29
05/29/2022 17:22:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.48 on epoch=29
05/29/2022 17:22:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=30
05/29/2022 17:22:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.51 on epoch=30
05/29/2022 17:22:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=31
05/29/2022 17:22:22 - INFO - __main__ - Global step 750 Train loss 0.50 Classification-F1 0.33640850688862695 on epoch=31
05/29/2022 17:22:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=31
05/29/2022 17:22:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.51 on epoch=32
05/29/2022 17:22:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.49 on epoch=32
05/29/2022 17:22:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=32
05/29/2022 17:22:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.51 on epoch=33
05/29/2022 17:22:46 - INFO - __main__ - Global step 800 Train loss 0.50 Classification-F1 0.28014518961134266 on epoch=33
05/29/2022 17:22:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.50 on epoch=33
05/29/2022 17:22:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=34
05/29/2022 17:22:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.56 on epoch=34
05/29/2022 17:22:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=34
05/29/2022 17:23:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.50 on epoch=35
05/29/2022 17:23:11 - INFO - __main__ - Global step 850 Train loss 0.50 Classification-F1 0.1637197118533071 on epoch=35
05/29/2022 17:23:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.51 on epoch=35
05/29/2022 17:23:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.49 on epoch=36
05/29/2022 17:23:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.53 on epoch=36
05/29/2022 17:23:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=37
05/29/2022 17:23:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.52 on epoch=37
05/29/2022 17:23:35 - INFO - __main__ - Global step 900 Train loss 0.50 Classification-F1 0.2615495077259002 on epoch=37
05/29/2022 17:23:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=37
05/29/2022 17:23:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.50 on epoch=38
05/29/2022 17:23:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=38
05/29/2022 17:23:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.49 on epoch=39
05/29/2022 17:23:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.54 on epoch=39
05/29/2022 17:24:01 - INFO - __main__ - Global step 950 Train loss 0.49 Classification-F1 0.293841642228739 on epoch=39
05/29/2022 17:24:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=39
05/29/2022 17:24:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=40
05/29/2022 17:24:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.48 on epoch=40
05/29/2022 17:24:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.49 on epoch=41
05/29/2022 17:24:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.50 on epoch=41
05/29/2022 17:24:26 - INFO - __main__ - Global step 1000 Train loss 0.48 Classification-F1 0.21026364579810863 on epoch=41
05/29/2022 17:24:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.52 on epoch=42
05/29/2022 17:24:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=42
05/29/2022 17:24:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.51 on epoch=42
05/29/2022 17:24:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.50 on epoch=43
05/29/2022 17:24:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=43
05/29/2022 17:24:51 - INFO - __main__ - Global step 1050 Train loss 0.48 Classification-F1 0.3020305593121127 on epoch=43
05/29/2022 17:24:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.50 on epoch=44
05/29/2022 17:24:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.53 on epoch=44
05/29/2022 17:24:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=44
05/29/2022 17:25:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.50 on epoch=45
05/29/2022 17:25:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.47 on epoch=45
05/29/2022 17:25:16 - INFO - __main__ - Global step 1100 Train loss 0.48 Classification-F1 0.2133333333333333 on epoch=45
05/29/2022 17:25:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.51 on epoch=46
05/29/2022 17:25:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.49 on epoch=46
05/29/2022 17:25:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=47
05/29/2022 17:25:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.52 on epoch=47
05/29/2022 17:25:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.51 on epoch=47
05/29/2022 17:25:41 - INFO - __main__ - Global step 1150 Train loss 0.50 Classification-F1 0.1717171717171717 on epoch=47
05/29/2022 17:25:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.47 on epoch=48
05/29/2022 17:25:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=48
05/29/2022 17:25:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.49 on epoch=49
05/29/2022 17:25:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.48 on epoch=49
05/29/2022 17:25:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.48 on epoch=49
05/29/2022 17:26:05 - INFO - __main__ - Global step 1200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=49
05/29/2022 17:26:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.49 on epoch=50
05/29/2022 17:26:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.47 on epoch=50
05/29/2022 17:26:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=51
05/29/2022 17:26:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.49 on epoch=51
05/29/2022 17:26:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=52
05/29/2022 17:26:30 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.17244846656611368 on epoch=52
05/29/2022 17:26:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.52 on epoch=52
05/29/2022 17:26:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.46 on epoch=52
05/29/2022 17:26:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=53
05/29/2022 17:26:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.49 on epoch=53
05/29/2022 17:26:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.50 on epoch=54
05/29/2022 17:26:55 - INFO - __main__ - Global step 1300 Train loss 0.49 Classification-F1 0.28728596952532287 on epoch=54
05/29/2022 17:26:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.49 on epoch=54
05/29/2022 17:27:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.46 on epoch=54
05/29/2022 17:27:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.54 on epoch=55
05/29/2022 17:27:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.46 on epoch=55
05/29/2022 17:27:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.49 on epoch=56
05/29/2022 17:27:20 - INFO - __main__ - Global step 1350 Train loss 0.49 Classification-F1 0.258396702894034 on epoch=56
05/29/2022 17:27:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.53 on epoch=56
05/29/2022 17:27:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.50 on epoch=57
05/29/2022 17:27:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.57 on epoch=57
05/29/2022 17:27:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.46 on epoch=57
05/29/2022 17:27:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.48 on epoch=58
05/29/2022 17:27:46 - INFO - __main__ - Global step 1400 Train loss 0.51 Classification-F1 0.281992337164751 on epoch=58
05/29/2022 17:27:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=58
05/29/2022 17:27:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.48 on epoch=59
05/29/2022 17:27:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.51 on epoch=59
05/29/2022 17:27:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.48 on epoch=59
05/29/2022 17:27:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.50 on epoch=60
05/29/2022 17:28:10 - INFO - __main__ - Global step 1450 Train loss 0.48 Classification-F1 0.1650294695481336 on epoch=60
05/29/2022 17:28:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.46 on epoch=60
05/29/2022 17:28:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.50 on epoch=61
05/29/2022 17:28:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.49 on epoch=61
05/29/2022 17:28:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=62
05/29/2022 17:28:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.49 on epoch=62
05/29/2022 17:28:35 - INFO - __main__ - Global step 1500 Train loss 0.47 Classification-F1 0.17542096424996964 on epoch=62
05/29/2022 17:28:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.48 on epoch=62
05/29/2022 17:28:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=63
05/29/2022 17:28:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.47 on epoch=63
05/29/2022 17:28:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=64
05/29/2022 17:28:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.54 on epoch=64
05/29/2022 17:29:00 - INFO - __main__ - Global step 1550 Train loss 0.48 Classification-F1 0.2605894562058946 on epoch=64
05/29/2022 17:29:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.48 on epoch=64
05/29/2022 17:29:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.54 on epoch=65
05/29/2022 17:29:08 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.44 on epoch=65
05/29/2022 17:29:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.46 on epoch=66
05/29/2022 17:29:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=66
05/29/2022 17:29:26 - INFO - __main__ - Global step 1600 Train loss 0.47 Classification-F1 0.3137219702695893 on epoch=66
05/29/2022 17:29:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.48 on epoch=67
05/29/2022 17:29:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.50 on epoch=67
05/29/2022 17:29:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=67
05/29/2022 17:29:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.49 on epoch=68
05/29/2022 17:29:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.48 on epoch=68
05/29/2022 17:29:50 - INFO - __main__ - Global step 1650 Train loss 0.48 Classification-F1 0.1885434487640847 on epoch=68
05/29/2022 17:29:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.49 on epoch=69
05/29/2022 17:29:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.42 on epoch=69
05/29/2022 17:29:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.48 on epoch=69
05/29/2022 17:30:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.45 on epoch=70
05/29/2022 17:30:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=70
05/29/2022 17:30:15 - INFO - __main__ - Global step 1700 Train loss 0.45 Classification-F1 0.26753600869342026 on epoch=70
05/29/2022 17:30:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.45 on epoch=71
05/29/2022 17:30:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.48 on epoch=71
05/29/2022 17:30:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.47 on epoch=72
05/29/2022 17:30:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.46 on epoch=72
05/29/2022 17:30:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.46 on epoch=72
05/29/2022 17:30:40 - INFO - __main__ - Global step 1750 Train loss 0.46 Classification-F1 0.2336449089179973 on epoch=72
05/29/2022 17:30:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.49 on epoch=73
05/29/2022 17:30:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.45 on epoch=73
05/29/2022 17:30:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.47 on epoch=74
05/29/2022 17:30:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.49 on epoch=74
05/29/2022 17:30:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.43 on epoch=74
05/29/2022 17:31:05 - INFO - __main__ - Global step 1800 Train loss 0.46 Classification-F1 0.19636617749825294 on epoch=74
05/29/2022 17:31:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=75
05/29/2022 17:31:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.45 on epoch=75
05/29/2022 17:31:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.47 on epoch=76
05/29/2022 17:31:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.49 on epoch=76
05/29/2022 17:31:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.50 on epoch=77
05/29/2022 17:31:29 - INFO - __main__ - Global step 1850 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=77
05/29/2022 17:31:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.46 on epoch=77
05/29/2022 17:31:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.40 on epoch=77
05/29/2022 17:31:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=78
05/29/2022 17:31:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=78
05/29/2022 17:31:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.47 on epoch=79
05/29/2022 17:31:53 - INFO - __main__ - Global step 1900 Train loss 0.44 Classification-F1 0.2609809062975878 on epoch=79
05/29/2022 17:31:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.47 on epoch=79
05/29/2022 17:31:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=79
05/29/2022 17:32:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.45 on epoch=80
05/29/2022 17:32:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.44 on epoch=80
05/29/2022 17:32:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.51 on epoch=81
05/29/2022 17:32:18 - INFO - __main__ - Global step 1950 Train loss 0.46 Classification-F1 0.2586177840967081 on epoch=81
05/29/2022 17:32:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.45 on epoch=81
05/29/2022 17:32:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.45 on epoch=82
05/29/2022 17:32:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.48 on epoch=82
05/29/2022 17:32:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.47 on epoch=82
05/29/2022 17:32:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.48 on epoch=83
05/29/2022 17:32:42 - INFO - __main__ - Global step 2000 Train loss 0.47 Classification-F1 0.1967085402668225 on epoch=83
05/29/2022 17:32:45 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.45 on epoch=83
05/29/2022 17:32:48 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.50 on epoch=84
05/29/2022 17:32:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.47 on epoch=84
05/29/2022 17:32:53 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.46 on epoch=84
05/29/2022 17:32:56 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.46 on epoch=85
05/29/2022 17:33:07 - INFO - __main__ - Global step 2050 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=85
05/29/2022 17:33:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.43 on epoch=85
05/29/2022 17:33:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.43 on epoch=86
05/29/2022 17:33:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.53 on epoch=86
05/29/2022 17:33:18 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.47 on epoch=87
05/29/2022 17:33:20 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.47 on epoch=87
05/29/2022 17:33:32 - INFO - __main__ - Global step 2100 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=87
05/29/2022 17:33:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.41 on epoch=87
05/29/2022 17:33:37 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.48 on epoch=88
05/29/2022 17:33:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.45 on epoch=88
05/29/2022 17:33:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.45 on epoch=89
05/29/2022 17:33:45 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.46 on epoch=89
05/29/2022 17:33:57 - INFO - __main__ - Global step 2150 Train loss 0.45 Classification-F1 0.23901303856645664 on epoch=89
05/29/2022 17:34:00 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.45 on epoch=89
05/29/2022 17:34:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.46 on epoch=90
05/29/2022 17:34:05 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.45 on epoch=90
05/29/2022 17:34:08 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.45 on epoch=91
05/29/2022 17:34:10 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.47 on epoch=91
05/29/2022 17:34:22 - INFO - __main__ - Global step 2200 Train loss 0.46 Classification-F1 0.17621317687324287 on epoch=91
05/29/2022 17:34:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.46 on epoch=92
05/29/2022 17:34:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.44 on epoch=92
05/29/2022 17:34:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.42 on epoch=92
05/29/2022 17:34:32 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.50 on epoch=93
05/29/2022 17:34:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.45 on epoch=93
05/29/2022 17:34:47 - INFO - __main__ - Global step 2250 Train loss 0.46 Classification-F1 0.29088352027610004 on epoch=93
05/29/2022 17:34:49 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.47 on epoch=94
05/29/2022 17:34:52 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.53 on epoch=94
05/29/2022 17:34:55 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.46 on epoch=94
05/29/2022 17:34:58 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.45 on epoch=95
05/29/2022 17:35:00 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.44 on epoch=95
05/29/2022 17:35:12 - INFO - __main__ - Global step 2300 Train loss 0.47 Classification-F1 0.23696057524904454 on epoch=95
05/29/2022 17:35:14 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.50 on epoch=96
05/29/2022 17:35:17 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.49 on epoch=96
05/29/2022 17:35:20 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.45 on epoch=97
05/29/2022 17:35:22 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.48 on epoch=97
05/29/2022 17:35:25 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.44 on epoch=97
05/29/2022 17:35:37 - INFO - __main__ - Global step 2350 Train loss 0.47 Classification-F1 0.2746723059576433 on epoch=97
05/29/2022 17:35:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.51 on epoch=98
05/29/2022 17:35:42 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.45 on epoch=98
05/29/2022 17:35:45 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.46 on epoch=99
05/29/2022 17:35:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.45 on epoch=99
05/29/2022 17:35:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.40 on epoch=99
05/29/2022 17:36:02 - INFO - __main__ - Global step 2400 Train loss 0.45 Classification-F1 0.3082798413763052 on epoch=99
05/29/2022 17:36:05 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.48 on epoch=100
05/29/2022 17:36:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.39 on epoch=100
05/29/2022 17:36:10 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.47 on epoch=101
05/29/2022 17:36:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.46 on epoch=101
05/29/2022 17:36:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.49 on epoch=102
05/29/2022 17:36:27 - INFO - __main__ - Global step 2450 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=102
05/29/2022 17:36:30 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.43 on epoch=102
05/29/2022 17:36:33 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.46 on epoch=102
05/29/2022 17:36:35 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.40 on epoch=103
05/29/2022 17:36:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.44 on epoch=103
05/29/2022 17:36:41 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.44 on epoch=104
05/29/2022 17:36:52 - INFO - __main__ - Global step 2500 Train loss 0.43 Classification-F1 0.26441372247823863 on epoch=104
05/29/2022 17:36:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.49 on epoch=104
05/29/2022 17:36:58 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.43 on epoch=104
05/29/2022 17:37:00 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.47 on epoch=105
05/29/2022 17:37:03 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.43 on epoch=105
05/29/2022 17:37:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.50 on epoch=106
05/29/2022 17:37:18 - INFO - __main__ - Global step 2550 Train loss 0.46 Classification-F1 0.2869511698779992 on epoch=106
05/29/2022 17:37:20 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.44 on epoch=106
05/29/2022 17:37:23 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.44 on epoch=107
05/29/2022 17:37:26 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.48 on epoch=107
05/29/2022 17:37:28 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.46 on epoch=107
05/29/2022 17:37:31 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.46 on epoch=108
05/29/2022 17:37:43 - INFO - __main__ - Global step 2600 Train loss 0.46 Classification-F1 0.3459342897423692 on epoch=108
05/29/2022 17:37:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.47 on epoch=108
05/29/2022 17:37:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.46 on epoch=109
05/29/2022 17:37:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.48 on epoch=109
05/29/2022 17:37:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.42 on epoch=109
05/29/2022 17:37:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.48 on epoch=110
05/29/2022 17:38:07 - INFO - __main__ - Global step 2650 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=110
05/29/2022 17:38:10 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.42 on epoch=110
05/29/2022 17:38:13 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.43 on epoch=111
05/29/2022 17:38:15 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.45 on epoch=111
05/29/2022 17:38:18 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.45 on epoch=112
05/29/2022 17:38:21 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.48 on epoch=112
05/29/2022 17:38:32 - INFO - __main__ - Global step 2700 Train loss 0.45 Classification-F1 0.1791673884173451 on epoch=112
05/29/2022 17:38:35 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.44 on epoch=112
05/29/2022 17:38:37 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.46 on epoch=113
05/29/2022 17:38:40 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.44 on epoch=113
05/29/2022 17:38:43 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.44 on epoch=114
05/29/2022 17:38:45 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.45 on epoch=114
05/29/2022 17:38:57 - INFO - __main__ - Global step 2750 Train loss 0.45 Classification-F1 0.20146353903275027 on epoch=114
05/29/2022 17:39:00 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.44 on epoch=114
05/29/2022 17:39:02 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.48 on epoch=115
05/29/2022 17:39:05 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.42 on epoch=115
05/29/2022 17:39:08 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.46 on epoch=116
05/29/2022 17:39:10 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.43 on epoch=116
05/29/2022 17:39:22 - INFO - __main__ - Global step 2800 Train loss 0.45 Classification-F1 0.2042774326984539 on epoch=116
05/29/2022 17:39:25 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.47 on epoch=117
05/29/2022 17:39:28 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.46 on epoch=117
05/29/2022 17:39:30 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.46 on epoch=117
05/29/2022 17:39:33 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.46 on epoch=118
05/29/2022 17:39:35 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.43 on epoch=118
05/29/2022 17:39:47 - INFO - __main__ - Global step 2850 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=118
05/29/2022 17:39:49 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.42 on epoch=119
05/29/2022 17:39:52 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.44 on epoch=119
05/29/2022 17:39:55 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.44 on epoch=119
05/29/2022 17:39:57 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.53 on epoch=120
05/29/2022 17:40:00 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.42 on epoch=120
05/29/2022 17:40:11 - INFO - __main__ - Global step 2900 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=120
05/29/2022 17:40:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.41 on epoch=121
05/29/2022 17:40:17 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.42 on epoch=121
05/29/2022 17:40:19 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.44 on epoch=122
05/29/2022 17:40:22 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.46 on epoch=122
05/29/2022 17:40:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.40 on epoch=122
05/29/2022 17:40:36 - INFO - __main__ - Global step 2950 Train loss 0.42 Classification-F1 0.26268389550819493 on epoch=122
05/29/2022 17:40:39 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.46 on epoch=123
05/29/2022 17:40:41 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.45 on epoch=123
05/29/2022 17:40:44 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.46 on epoch=124
05/29/2022 17:40:47 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.48 on epoch=124
05/29/2022 17:40:49 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.40 on epoch=124
05/29/2022 17:40:50 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 17:40:50 - INFO - __main__ - Printing 3 examples
05/29/2022 17:40:50 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/29/2022 17:40:50 - INFO - __main__ - ['entailment']
05/29/2022 17:40:50 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/29/2022 17:40:50 - INFO - __main__ - ['entailment']
05/29/2022 17:40:50 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/29/2022 17:40:50 - INFO - __main__ - ['entailment']
05/29/2022 17:40:50 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:40:51 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:40:51 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 17:40:51 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 17:40:51 - INFO - __main__ - Printing 3 examples
05/29/2022 17:40:51 - INFO - __main__ -  [anli] premise: Terenzo is a "comune" (municipality) in the Province of Parma in the Italian region Emilia-Romagna, located about 100 km west of Bologna and about 30 km southwest of Parma. As of 31 December 2004, it had a population of 1,250 and an area of 72.4 km2 . [SEP] hypothesis: Terenzo has a small population.
05/29/2022 17:40:51 - INFO - __main__ - ['entailment']
05/29/2022 17:40:51 - INFO - __main__ -  [anli] premise: Brett McLaughlin, known professionally as Leland, is an American singer, songwriter, record producer, composer and lecturer. Based in Los Angeles, California, he has worked closely with a range of popular artists, including Troye Sivan, Daya, Capital Cities, Andy Grammer, Hilary Duff and Allie X. [SEP] hypothesis: Leland has worked with at least 6 popular artists.
05/29/2022 17:40:51 - INFO - __main__ - ['entailment']
05/29/2022 17:40:51 - INFO - __main__ -  [anli] premise: Amazon Fire TV refers to two digital media players and microconsoles developed by Amazon.com. It is a small network appliance and entertainment device designed to stream digital audio/video content to a high-definition television. The device also allows users to play video games with the included remote, via a mobile app, or with an optional game controller. [SEP] hypothesis: Amazon Fire TV refers to the digital media players and micro consoles developed by amazon.com
05/29/2022 17:40:51 - INFO - __main__ - ['entailment']
05/29/2022 17:40:51 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:40:51 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:40:52 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 17:41:01 - INFO - __main__ - Global step 3000 Train loss 0.45 Classification-F1 0.32729326092606775 on epoch=124
05/29/2022 17:41:01 - INFO - __main__ - save last model!
05/29/2022 17:41:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 17:41:02 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 17:41:02 - INFO - __main__ - Printing 3 examples
05/29/2022 17:41:02 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 17:41:02 - INFO - __main__ - ['contradiction']
05/29/2022 17:41:02 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 17:41:02 - INFO - __main__ - ['entailment']
05/29/2022 17:41:02 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 17:41:02 - INFO - __main__ - ['contradiction']
05/29/2022 17:41:02 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:41:02 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:41:03 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 17:41:10 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 17:41:10 - INFO - __main__ - task name: anli
05/29/2022 17:41:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 17:41:11 - INFO - __main__ - Starting training!
05/29/2022 17:41:35 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_21_0.3_8_predictions.txt
05/29/2022 17:41:35 - INFO - __main__ - Classification-F1 on test data: 0.2986
05/29/2022 17:41:36 - INFO - __main__ - prefix=anli_128_21, lr=0.3, bsz=8, dev_performance=0.3637954988998633, test_performance=0.29863597624263766
05/29/2022 17:41:36 - INFO - __main__ - Running ... prefix=anli_128_21, lr=0.2, bsz=8 ...
05/29/2022 17:41:37 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 17:41:37 - INFO - __main__ - Printing 3 examples
05/29/2022 17:41:37 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/29/2022 17:41:37 - INFO - __main__ - ['entailment']
05/29/2022 17:41:37 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/29/2022 17:41:37 - INFO - __main__ - ['entailment']
05/29/2022 17:41:37 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/29/2022 17:41:37 - INFO - __main__ - ['entailment']
05/29/2022 17:41:37 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:41:37 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:41:37 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 17:41:37 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 17:41:37 - INFO - __main__ - Printing 3 examples
05/29/2022 17:41:37 - INFO - __main__ -  [anli] premise: Terenzo is a "comune" (municipality) in the Province of Parma in the Italian region Emilia-Romagna, located about 100 km west of Bologna and about 30 km southwest of Parma. As of 31 December 2004, it had a population of 1,250 and an area of 72.4 km2 . [SEP] hypothesis: Terenzo has a small population.
05/29/2022 17:41:37 - INFO - __main__ - ['entailment']
05/29/2022 17:41:37 - INFO - __main__ -  [anli] premise: Brett McLaughlin, known professionally as Leland, is an American singer, songwriter, record producer, composer and lecturer. Based in Los Angeles, California, he has worked closely with a range of popular artists, including Troye Sivan, Daya, Capital Cities, Andy Grammer, Hilary Duff and Allie X. [SEP] hypothesis: Leland has worked with at least 6 popular artists.
05/29/2022 17:41:37 - INFO - __main__ - ['entailment']
05/29/2022 17:41:37 - INFO - __main__ -  [anli] premise: Amazon Fire TV refers to two digital media players and microconsoles developed by Amazon.com. It is a small network appliance and entertainment device designed to stream digital audio/video content to a high-definition television. The device also allows users to play video games with the included remote, via a mobile app, or with an optional game controller. [SEP] hypothesis: Amazon Fire TV refers to the digital media players and micro consoles developed by amazon.com
05/29/2022 17:41:37 - INFO - __main__ - ['entailment']
05/29/2022 17:41:37 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:41:38 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:41:38 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 17:41:54 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 17:41:54 - INFO - __main__ - task name: anli
05/29/2022 17:41:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 17:41:55 - INFO - __main__ - Starting training!
05/29/2022 17:41:58 - INFO - __main__ - Step 10 Global step 10 Train loss 6.99 on epoch=0
05/29/2022 17:42:01 - INFO - __main__ - Step 20 Global step 20 Train loss 4.10 on epoch=0
05/29/2022 17:42:04 - INFO - __main__ - Step 30 Global step 30 Train loss 2.45 on epoch=1
05/29/2022 17:42:06 - INFO - __main__ - Step 40 Global step 40 Train loss 1.44 on epoch=1
05/29/2022 17:42:09 - INFO - __main__ - Step 50 Global step 50 Train loss 1.10 on epoch=2
05/29/2022 17:42:22 - INFO - __main__ - Global step 50 Train loss 3.22 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 17:42:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 17:42:24 - INFO - __main__ - Step 60 Global step 60 Train loss 0.86 on epoch=2
05/29/2022 17:42:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.75 on epoch=2
05/29/2022 17:42:30 - INFO - __main__ - Step 80 Global step 80 Train loss 0.76 on epoch=3
05/29/2022 17:42:33 - INFO - __main__ - Step 90 Global step 90 Train loss 0.66 on epoch=3
05/29/2022 17:42:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.68 on epoch=4
05/29/2022 17:42:48 - INFO - __main__ - Global step 100 Train loss 0.74 Classification-F1 0.20872667692613947 on epoch=4
05/29/2022 17:42:48 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.20872667692613947 on epoch=4, global_step=100
05/29/2022 17:42:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.76 on epoch=4
05/29/2022 17:42:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=4
05/29/2022 17:42:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.64 on epoch=5
05/29/2022 17:42:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.60 on epoch=5
05/29/2022 17:43:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.69 on epoch=6
05/29/2022 17:43:13 - INFO - __main__ - Global step 150 Train loss 0.65 Classification-F1 0.1637197118533071 on epoch=6
05/29/2022 17:43:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.62 on epoch=6
05/29/2022 17:43:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.60 on epoch=7
05/29/2022 17:43:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.72 on epoch=7
05/29/2022 17:43:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.57 on epoch=7
05/29/2022 17:43:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.63 on epoch=8
05/29/2022 17:43:38 - INFO - __main__ - Global step 200 Train loss 0.63 Classification-F1 0.26425743976370786 on epoch=8
05/29/2022 17:43:38 - INFO - __main__ - Saving model with best Classification-F1: 0.20872667692613947 -> 0.26425743976370786 on epoch=8, global_step=200
05/29/2022 17:43:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.59 on epoch=8
05/29/2022 17:43:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.63 on epoch=9
05/29/2022 17:43:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.55 on epoch=9
05/29/2022 17:43:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=9
05/29/2022 17:43:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=10
05/29/2022 17:44:00 - INFO - __main__ - Global step 250 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=10
05/29/2022 17:44:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.52 on epoch=10
05/29/2022 17:44:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.55 on epoch=11
05/29/2022 17:44:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.63 on epoch=11
05/29/2022 17:44:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=12
05/29/2022 17:44:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.60 on epoch=12
05/29/2022 17:44:27 - INFO - __main__ - Global step 300 Train loss 0.57 Classification-F1 0.2845591041280118 on epoch=12
05/29/2022 17:44:27 - INFO - __main__ - Saving model with best Classification-F1: 0.26425743976370786 -> 0.2845591041280118 on epoch=12, global_step=300
05/29/2022 17:44:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.53 on epoch=12
05/29/2022 17:44:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.59 on epoch=13
05/29/2022 17:44:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.53 on epoch=13
05/29/2022 17:44:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=14
05/29/2022 17:44:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.57 on epoch=14
05/29/2022 17:44:53 - INFO - __main__ - Global step 350 Train loss 0.53 Classification-F1 0.29066974341527313 on epoch=14
05/29/2022 17:44:53 - INFO - __main__ - Saving model with best Classification-F1: 0.2845591041280118 -> 0.29066974341527313 on epoch=14, global_step=350
05/29/2022 17:44:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=14
05/29/2022 17:44:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.56 on epoch=15
05/29/2022 17:45:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=15
05/29/2022 17:45:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.59 on epoch=16
05/29/2022 17:45:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.55 on epoch=16
05/29/2022 17:45:20 - INFO - __main__ - Global step 400 Train loss 0.54 Classification-F1 0.22406787945439824 on epoch=16
05/29/2022 17:45:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.55 on epoch=17
05/29/2022 17:45:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.57 on epoch=17
05/29/2022 17:45:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.53 on epoch=17
05/29/2022 17:45:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.57 on epoch=18
05/29/2022 17:45:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=18
05/29/2022 17:45:47 - INFO - __main__ - Global step 450 Train loss 0.54 Classification-F1 0.24333333333333332 on epoch=18
05/29/2022 17:45:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=19
05/29/2022 17:45:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.59 on epoch=19
05/29/2022 17:45:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.50 on epoch=19
05/29/2022 17:45:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.50 on epoch=20
05/29/2022 17:46:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.51 on epoch=20
05/29/2022 17:46:12 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.20270270270270271 on epoch=20
05/29/2022 17:46:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.51 on epoch=21
05/29/2022 17:46:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.54 on epoch=21
05/29/2022 17:46:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=22
05/29/2022 17:46:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.56 on epoch=22
05/29/2022 17:46:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.50 on epoch=22
05/29/2022 17:46:38 - INFO - __main__ - Global step 550 Train loss 0.52 Classification-F1 0.29553001277139207 on epoch=22
05/29/2022 17:46:38 - INFO - __main__ - Saving model with best Classification-F1: 0.29066974341527313 -> 0.29553001277139207 on epoch=22, global_step=550
05/29/2022 17:46:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.51 on epoch=23
05/29/2022 17:46:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.50 on epoch=23
05/29/2022 17:46:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=24
05/29/2022 17:46:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.53 on epoch=24
05/29/2022 17:46:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=24
05/29/2022 17:47:03 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.1756228796375973 on epoch=24
05/29/2022 17:47:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.53 on epoch=25
05/29/2022 17:47:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.52 on epoch=25
05/29/2022 17:47:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.56 on epoch=26
05/29/2022 17:47:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.52 on epoch=26
05/29/2022 17:47:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.48 on epoch=27
05/29/2022 17:47:28 - INFO - __main__ - Global step 650 Train loss 0.52 Classification-F1 0.17651734429130608 on epoch=27
05/29/2022 17:47:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.51 on epoch=27
05/29/2022 17:47:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=27
05/29/2022 17:47:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=28
05/29/2022 17:47:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=28
05/29/2022 17:47:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.49 on epoch=29
05/29/2022 17:47:53 - INFO - __main__ - Global step 700 Train loss 0.48 Classification-F1 0.33729593079922027 on epoch=29
05/29/2022 17:47:53 - INFO - __main__ - Saving model with best Classification-F1: 0.29553001277139207 -> 0.33729593079922027 on epoch=29, global_step=700
05/29/2022 17:47:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.50 on epoch=29
05/29/2022 17:47:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.48 on epoch=29
05/29/2022 17:48:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.52 on epoch=30
05/29/2022 17:48:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.50 on epoch=30
05/29/2022 17:48:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=31
05/29/2022 17:48:17 - INFO - __main__ - Global step 750 Train loss 0.50 Classification-F1 0.22142869552878333 on epoch=31
05/29/2022 17:48:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.52 on epoch=31
05/29/2022 17:48:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=32
05/29/2022 17:48:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.50 on epoch=32
05/29/2022 17:48:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=32
05/29/2022 17:48:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=33
05/29/2022 17:48:42 - INFO - __main__ - Global step 800 Train loss 0.49 Classification-F1 0.24875427503984357 on epoch=33
05/29/2022 17:48:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=33
05/29/2022 17:48:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=34
05/29/2022 17:48:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.49 on epoch=34
05/29/2022 17:48:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.55 on epoch=34
05/29/2022 17:48:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.49 on epoch=35
05/29/2022 17:49:06 - INFO - __main__ - Global step 850 Train loss 0.49 Classification-F1 0.1721607831834019 on epoch=35
05/29/2022 17:49:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.47 on epoch=35
05/29/2022 17:49:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.52 on epoch=36
05/29/2022 17:49:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=36
05/29/2022 17:49:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.51 on epoch=37
05/29/2022 17:49:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.47 on epoch=37
05/29/2022 17:49:31 - INFO - __main__ - Global step 900 Train loss 0.49 Classification-F1 0.19959818308874913 on epoch=37
05/29/2022 17:49:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=37
05/29/2022 17:49:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=38
05/29/2022 17:49:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=38
05/29/2022 17:49:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=39
05/29/2022 17:49:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.48 on epoch=39
05/29/2022 17:49:55 - INFO - __main__ - Global step 950 Train loss 0.47 Classification-F1 0.25950259502595024 on epoch=39
05/29/2022 17:49:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.48 on epoch=39
05/29/2022 17:50:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.47 on epoch=40
05/29/2022 17:50:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=40
05/29/2022 17:50:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.52 on epoch=41
05/29/2022 17:50:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.49 on epoch=41
05/29/2022 17:50:20 - INFO - __main__ - Global step 1000 Train loss 0.48 Classification-F1 0.27091191846998874 on epoch=41
05/29/2022 17:50:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=42
05/29/2022 17:50:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=42
05/29/2022 17:50:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.46 on epoch=42
05/29/2022 17:50:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.46 on epoch=43
05/29/2022 17:50:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.44 on epoch=43
05/29/2022 17:50:45 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.2673375889769128 on epoch=43
05/29/2022 17:50:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.48 on epoch=44
05/29/2022 17:50:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.50 on epoch=44
05/29/2022 17:50:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.50 on epoch=44
05/29/2022 17:50:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.50 on epoch=45
05/29/2022 17:50:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.46 on epoch=45
05/29/2022 17:51:10 - INFO - __main__ - Global step 1100 Train loss 0.49 Classification-F1 0.2585062603975134 on epoch=45
05/29/2022 17:51:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.56 on epoch=46
05/29/2022 17:51:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.49 on epoch=46
05/29/2022 17:51:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=47
05/29/2022 17:51:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.47 on epoch=47
05/29/2022 17:51:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.45 on epoch=47
05/29/2022 17:51:35 - INFO - __main__ - Global step 1150 Train loss 0.49 Classification-F1 0.19871454073142372 on epoch=47
05/29/2022 17:51:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.47 on epoch=48
05/29/2022 17:51:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.46 on epoch=48
05/29/2022 17:51:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.47 on epoch=49
05/29/2022 17:51:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.47 on epoch=49
05/29/2022 17:51:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.43 on epoch=49
05/29/2022 17:51:59 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.16699282452707112 on epoch=49
05/29/2022 17:52:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.51 on epoch=50
05/29/2022 17:52:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.44 on epoch=50
05/29/2022 17:52:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.49 on epoch=51
05/29/2022 17:52:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=51
05/29/2022 17:52:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.49 on epoch=52
05/29/2022 17:52:23 - INFO - __main__ - Global step 1250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=52
05/29/2022 17:52:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.51 on epoch=52
05/29/2022 17:52:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=52
05/29/2022 17:52:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.53 on epoch=53
05/29/2022 17:52:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.46 on epoch=53
05/29/2022 17:52:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=54
05/29/2022 17:52:48 - INFO - __main__ - Global step 1300 Train loss 0.47 Classification-F1 0.29221679216307944 on epoch=54
05/29/2022 17:52:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=54
05/29/2022 17:52:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.45 on epoch=54
05/29/2022 17:52:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=55
05/29/2022 17:52:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=55
05/29/2022 17:53:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=56
05/29/2022 17:53:13 - INFO - __main__ - Global step 1350 Train loss 0.44 Classification-F1 0.38209253620435923 on epoch=56
05/29/2022 17:53:13 - INFO - __main__ - Saving model with best Classification-F1: 0.33729593079922027 -> 0.38209253620435923 on epoch=56, global_step=1350
05/29/2022 17:53:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.48 on epoch=56
05/29/2022 17:53:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.44 on epoch=57
05/29/2022 17:53:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=57
05/29/2022 17:53:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=57
05/29/2022 17:53:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.48 on epoch=58
05/29/2022 17:53:37 - INFO - __main__ - Global step 1400 Train loss 0.46 Classification-F1 0.2936227301721268 on epoch=58
05/29/2022 17:53:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.46 on epoch=58
05/29/2022 17:53:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=59
05/29/2022 17:53:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.47 on epoch=59
05/29/2022 17:53:48 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=59
05/29/2022 17:53:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.46 on epoch=60
05/29/2022 17:54:01 - INFO - __main__ - Global step 1450 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=60
05/29/2022 17:54:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.38 on epoch=60
05/29/2022 17:54:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.46 on epoch=61
05/29/2022 17:54:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=61
05/29/2022 17:54:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.45 on epoch=62
05/29/2022 17:54:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.44 on epoch=62
05/29/2022 17:54:25 - INFO - __main__ - Global step 1500 Train loss 0.43 Classification-F1 0.1699547732551033 on epoch=62
05/29/2022 17:54:27 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.47 on epoch=62
05/29/2022 17:54:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.48 on epoch=63
05/29/2022 17:54:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.45 on epoch=63
05/29/2022 17:54:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.46 on epoch=64
05/29/2022 17:54:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.46 on epoch=64
05/29/2022 17:54:49 - INFO - __main__ - Global step 1550 Train loss 0.47 Classification-F1 0.31580236159504455 on epoch=64
05/29/2022 17:54:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.46 on epoch=64
05/29/2022 17:54:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.47 on epoch=65
05/29/2022 17:54:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=65
05/29/2022 17:54:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.47 on epoch=66
05/29/2022 17:55:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=66
05/29/2022 17:55:13 - INFO - __main__ - Global step 1600 Train loss 0.45 Classification-F1 0.19586611395358614 on epoch=66
05/29/2022 17:55:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.41 on epoch=67
05/29/2022 17:55:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.41 on epoch=67
05/29/2022 17:55:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=67
05/29/2022 17:55:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=68
05/29/2022 17:55:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.45 on epoch=68
05/29/2022 17:55:36 - INFO - __main__ - Global step 1650 Train loss 0.43 Classification-F1 0.2544747942562561 on epoch=68
05/29/2022 17:55:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.46 on epoch=69
05/29/2022 17:55:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.47 on epoch=69
05/29/2022 17:55:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=69
05/29/2022 17:55:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.45 on epoch=70
05/29/2022 17:55:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=70
05/29/2022 17:56:00 - INFO - __main__ - Global step 1700 Train loss 0.44 Classification-F1 0.18418797895815087 on epoch=70
05/29/2022 17:56:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.48 on epoch=71
05/29/2022 17:56:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.48 on epoch=71
05/29/2022 17:56:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.52 on epoch=72
05/29/2022 17:56:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.48 on epoch=72
05/29/2022 17:56:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=72
05/29/2022 17:56:24 - INFO - __main__ - Global step 1750 Train loss 0.48 Classification-F1 0.24179811358619308 on epoch=72
05/29/2022 17:56:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.45 on epoch=73
05/29/2022 17:56:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.42 on epoch=73
05/29/2022 17:56:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=74
05/29/2022 17:56:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.50 on epoch=74
05/29/2022 17:56:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.44 on epoch=74
05/29/2022 17:56:48 - INFO - __main__ - Global step 1800 Train loss 0.45 Classification-F1 0.18365020800150542 on epoch=74
05/29/2022 17:56:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.48 on epoch=75
05/29/2022 17:56:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.39 on epoch=75
05/29/2022 17:56:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.48 on epoch=76
05/29/2022 17:56:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.46 on epoch=76
05/29/2022 17:57:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.49 on epoch=77
05/29/2022 17:57:12 - INFO - __main__ - Global step 1850 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=77
05/29/2022 17:57:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.46 on epoch=77
05/29/2022 17:57:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.44 on epoch=77
05/29/2022 17:57:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.40 on epoch=78
05/29/2022 17:57:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=78
05/29/2022 17:57:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=79
05/29/2022 17:57:37 - INFO - __main__ - Global step 1900 Train loss 0.43 Classification-F1 0.326361503260003 on epoch=79
05/29/2022 17:57:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.46 on epoch=79
05/29/2022 17:57:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.43 on epoch=79
05/29/2022 17:57:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.45 on epoch=80
05/29/2022 17:57:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=80
05/29/2022 17:57:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.49 on epoch=81
05/29/2022 17:58:00 - INFO - __main__ - Global step 1950 Train loss 0.44 Classification-F1 0.23438812083973373 on epoch=81
05/29/2022 17:58:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.46 on epoch=81
05/29/2022 17:58:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.44 on epoch=82
05/29/2022 17:58:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.47 on epoch=82
05/29/2022 17:58:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=82
05/29/2022 17:58:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.52 on epoch=83
05/29/2022 17:58:23 - INFO - __main__ - Global step 2000 Train loss 0.47 Classification-F1 0.26606940582844196 on epoch=83
05/29/2022 17:58:25 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.37 on epoch=83
05/29/2022 17:58:28 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.43 on epoch=84
05/29/2022 17:58:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.42 on epoch=84
05/29/2022 17:58:33 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.43 on epoch=84
05/29/2022 17:58:36 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.50 on epoch=85
05/29/2022 17:58:46 - INFO - __main__ - Global step 2050 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=85
05/29/2022 17:58:49 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.43 on epoch=85
05/29/2022 17:58:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.47 on epoch=86
05/29/2022 17:58:54 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.48 on epoch=86
05/29/2022 17:58:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.46 on epoch=87
05/29/2022 17:59:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.44 on epoch=87
05/29/2022 17:59:10 - INFO - __main__ - Global step 2100 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=87
05/29/2022 17:59:13 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.39 on epoch=87
05/29/2022 17:59:16 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.49 on epoch=88
05/29/2022 17:59:18 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.45 on epoch=88
05/29/2022 17:59:21 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.43 on epoch=89
05/29/2022 17:59:23 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.48 on epoch=89
05/29/2022 17:59:35 - INFO - __main__ - Global step 2150 Train loss 0.45 Classification-F1 0.29716932264102075 on epoch=89
05/29/2022 17:59:38 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.44 on epoch=89
05/29/2022 17:59:40 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.47 on epoch=90
05/29/2022 17:59:43 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.42 on epoch=90
05/29/2022 17:59:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.45 on epoch=91
05/29/2022 17:59:48 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.41 on epoch=91
05/29/2022 18:00:00 - INFO - __main__ - Global step 2200 Train loss 0.44 Classification-F1 0.3545535276937421 on epoch=91
05/29/2022 18:00:02 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.45 on epoch=92
05/29/2022 18:00:05 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.43 on epoch=92
05/29/2022 18:00:08 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.44 on epoch=92
05/29/2022 18:00:10 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.46 on epoch=93
05/29/2022 18:00:13 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.48 on epoch=93
05/29/2022 18:00:25 - INFO - __main__ - Global step 2250 Train loss 0.45 Classification-F1 0.24755374080347384 on epoch=93
05/29/2022 18:00:27 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.40 on epoch=94
05/29/2022 18:00:30 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.47 on epoch=94
05/29/2022 18:00:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.44 on epoch=94
05/29/2022 18:00:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.52 on epoch=95
05/29/2022 18:00:38 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.38 on epoch=95
05/29/2022 18:00:49 - INFO - __main__ - Global step 2300 Train loss 0.44 Classification-F1 0.26079343365253077 on epoch=95
05/29/2022 18:00:52 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.44 on epoch=96
05/29/2022 18:00:55 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.46 on epoch=96
05/29/2022 18:00:57 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.43 on epoch=97
05/29/2022 18:01:00 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.47 on epoch=97
05/29/2022 18:01:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.38 on epoch=97
05/29/2022 18:01:14 - INFO - __main__ - Global step 2350 Train loss 0.43 Classification-F1 0.2870771761737765 on epoch=97
05/29/2022 18:01:17 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.46 on epoch=98
05/29/2022 18:01:19 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.44 on epoch=98
05/29/2022 18:01:22 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.44 on epoch=99
05/29/2022 18:01:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.45 on epoch=99
05/29/2022 18:01:27 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.43 on epoch=99
05/29/2022 18:01:37 - INFO - __main__ - Global step 2400 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=99
05/29/2022 18:01:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.42 on epoch=100
05/29/2022 18:01:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.38 on epoch=100
05/29/2022 18:01:45 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.43 on epoch=101
05/29/2022 18:01:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.47 on epoch=101
05/29/2022 18:01:50 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.43 on epoch=102
05/29/2022 18:02:01 - INFO - __main__ - Global step 2450 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=102
05/29/2022 18:02:04 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.50 on epoch=102
05/29/2022 18:02:06 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.37 on epoch=102
05/29/2022 18:02:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.46 on epoch=103
05/29/2022 18:02:11 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.42 on epoch=103
05/29/2022 18:02:14 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.45 on epoch=104
05/29/2022 18:02:26 - INFO - __main__ - Global step 2500 Train loss 0.44 Classification-F1 0.21589854624188062 on epoch=104
05/29/2022 18:02:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.45 on epoch=104
05/29/2022 18:02:31 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.43 on epoch=104
05/29/2022 18:02:34 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.46 on epoch=105
05/29/2022 18:02:36 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.42 on epoch=105
05/29/2022 18:02:39 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.47 on epoch=106
05/29/2022 18:02:50 - INFO - __main__ - Global step 2550 Train loss 0.45 Classification-F1 0.2624770160231153 on epoch=106
05/29/2022 18:02:53 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.43 on epoch=106
05/29/2022 18:02:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.44 on epoch=107
05/29/2022 18:02:58 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.48 on epoch=107
05/29/2022 18:03:01 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.47 on epoch=107
05/29/2022 18:03:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.45 on epoch=108
05/29/2022 18:03:14 - INFO - __main__ - Global step 2600 Train loss 0.45 Classification-F1 0.2743678099032646 on epoch=108
05/29/2022 18:03:17 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.48 on epoch=108
05/29/2022 18:03:20 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.42 on epoch=109
05/29/2022 18:03:22 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.47 on epoch=109
05/29/2022 18:03:25 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.42 on epoch=109
05/29/2022 18:03:28 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.49 on epoch=110
05/29/2022 18:03:39 - INFO - __main__ - Global step 2650 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=110
05/29/2022 18:03:41 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.40 on epoch=110
05/29/2022 18:03:44 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.47 on epoch=111
05/29/2022 18:03:47 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.48 on epoch=111
05/29/2022 18:03:49 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.43 on epoch=112
05/29/2022 18:03:52 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.47 on epoch=112
05/29/2022 18:04:01 - INFO - __main__ - Global step 2700 Train loss 0.45 Classification-F1 0.192348123701259 on epoch=112
05/29/2022 18:04:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.42 on epoch=112
05/29/2022 18:04:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.49 on epoch=113
05/29/2022 18:04:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.40 on epoch=113
05/29/2022 18:04:12 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.43 on epoch=114
05/29/2022 18:04:15 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.48 on epoch=114
05/29/2022 18:04:24 - INFO - __main__ - Global step 2750 Train loss 0.44 Classification-F1 0.17765617875057005 on epoch=114
05/29/2022 18:04:27 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.43 on epoch=114
05/29/2022 18:04:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.44 on epoch=115
05/29/2022 18:04:32 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.45 on epoch=115
05/29/2022 18:04:35 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.48 on epoch=116
05/29/2022 18:04:37 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.50 on epoch=116
05/29/2022 18:04:49 - INFO - __main__ - Global step 2800 Train loss 0.46 Classification-F1 0.3112037580739871 on epoch=116
05/29/2022 18:04:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.43 on epoch=117
05/29/2022 18:04:54 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.45 on epoch=117
05/29/2022 18:04:57 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.41 on epoch=117
05/29/2022 18:04:59 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.44 on epoch=118
05/29/2022 18:05:02 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.43 on epoch=118
05/29/2022 18:05:14 - INFO - __main__ - Global step 2850 Train loss 0.43 Classification-F1 0.2916174311598095 on epoch=118
05/29/2022 18:05:16 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.45 on epoch=119
05/29/2022 18:05:19 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.46 on epoch=119
05/29/2022 18:05:22 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.43 on epoch=119
05/29/2022 18:05:24 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.43 on epoch=120
05/29/2022 18:05:27 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.40 on epoch=120
05/29/2022 18:05:38 - INFO - __main__ - Global step 2900 Train loss 0.43 Classification-F1 0.26259533995316403 on epoch=120
05/29/2022 18:05:41 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.47 on epoch=121
05/29/2022 18:05:44 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.43 on epoch=121
05/29/2022 18:05:46 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.44 on epoch=122
05/29/2022 18:05:49 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.44 on epoch=122
05/29/2022 18:05:52 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.42 on epoch=122
05/29/2022 18:06:03 - INFO - __main__ - Global step 2950 Train loss 0.44 Classification-F1 0.25362318840579706 on epoch=122
05/29/2022 18:06:06 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.48 on epoch=123
05/29/2022 18:06:09 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.42 on epoch=123
05/29/2022 18:06:11 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.42 on epoch=124
05/29/2022 18:06:14 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.50 on epoch=124
05/29/2022 18:06:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.44 on epoch=124
05/29/2022 18:06:18 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 18:06:18 - INFO - __main__ - Printing 3 examples
05/29/2022 18:06:18 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/29/2022 18:06:18 - INFO - __main__ - ['neutral']
05/29/2022 18:06:18 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/29/2022 18:06:18 - INFO - __main__ - ['neutral']
05/29/2022 18:06:18 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/29/2022 18:06:18 - INFO - __main__ - ['neutral']
05/29/2022 18:06:18 - INFO - __main__ - Tokenizing Input ...
05/29/2022 18:06:18 - INFO - __main__ - Tokenizing Output ...
05/29/2022 18:06:19 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 18:06:19 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 18:06:19 - INFO - __main__ - Printing 3 examples
05/29/2022 18:06:19 - INFO - __main__ -  [anli] premise: Arlette Roxburgh is a Trinidadian American singer and songwriter. She was born in Trinidad. She is best known for singing The Star-Spangled Banner before every New Jersey Devils home game started. When the Nets were in New Jersey, she also sang the national anthem before their home games at the time as well. [SEP] hypothesis: Arlette Roxburgh has only ever sung for the New Jersey Devils.
05/29/2022 18:06:19 - INFO - __main__ - ['neutral']
05/29/2022 18:06:19 - INFO - __main__ -  [anli] premise: Storm Keating (born 27 October 1981) is an Australian-born fashion designer, brand ambassador, producer-director and blogger, now based in London. Keating has worked on a number of Australian and British television programmes such as "The Apprentice Australia", "Masterchef Australia", "The X Factor", "The Voice Australia", and "The Voice UK". She is the wife of Ronan Keating. [SEP] hypothesis: Storm Keaton learned about fashion while studying abroad in the United States.
05/29/2022 18:06:19 - INFO - __main__ - ['neutral']
05/29/2022 18:06:19 - INFO - __main__ -  [anli] premise: Piton is a Pilsner beer brand from the island of Saint Lucia, brewed by Windward & Leeward Brewing Limited, which is owned by Heineken. The beer was named for the Gros Piton and Petit Piton mountains on the island. It was first brewed on October 7, 1992. [SEP] hypothesis: Heineken plans to stop production of Piton in the near future
05/29/2022 18:06:19 - INFO - __main__ - ['neutral']
05/29/2022 18:06:19 - INFO - __main__ - Tokenizing Input ...
05/29/2022 18:06:19 - INFO - __main__ - Tokenizing Output ...
05/29/2022 18:06:20 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 18:06:28 - INFO - __main__ - Global step 3000 Train loss 0.45 Classification-F1 0.19520299025405075 on epoch=124
05/29/2022 18:06:28 - INFO - __main__ - save last model!
05/29/2022 18:06:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 18:06:28 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 18:06:28 - INFO - __main__ - Printing 3 examples
05/29/2022 18:06:28 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 18:06:28 - INFO - __main__ - ['contradiction']
05/29/2022 18:06:28 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 18:06:28 - INFO - __main__ - ['entailment']
05/29/2022 18:06:28 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 18:06:28 - INFO - __main__ - ['contradiction']
05/29/2022 18:06:28 - INFO - __main__ - Tokenizing Input ...
05/29/2022 18:06:29 - INFO - __main__ - Tokenizing Output ...
05/29/2022 18:06:30 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 18:06:35 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 18:06:35 - INFO - __main__ - task name: anli
05/29/2022 18:06:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 18:06:36 - INFO - __main__ - Starting training!
05/29/2022 18:06:59 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_21_0.2_8_predictions.txt
05/29/2022 18:06:59 - INFO - __main__ - Classification-F1 on test data: 0.1901
05/29/2022 18:06:59 - INFO - __main__ - prefix=anli_128_21, lr=0.2, bsz=8, dev_performance=0.38209253620435923, test_performance=0.1900902458406337
05/29/2022 18:06:59 - INFO - __main__ - Running ... prefix=anli_128_42, lr=0.5, bsz=8 ...
05/29/2022 18:07:00 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 18:07:00 - INFO - __main__ - Printing 3 examples
05/29/2022 18:07:00 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/29/2022 18:07:00 - INFO - __main__ - ['neutral']
05/29/2022 18:07:00 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/29/2022 18:07:00 - INFO - __main__ - ['neutral']
05/29/2022 18:07:00 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/29/2022 18:07:00 - INFO - __main__ - ['neutral']
05/29/2022 18:07:00 - INFO - __main__ - Tokenizing Input ...
05/29/2022 18:07:00 - INFO - __main__ - Tokenizing Output ...
05/29/2022 18:07:01 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 18:07:01 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 18:07:01 - INFO - __main__ - Printing 3 examples
05/29/2022 18:07:01 - INFO - __main__ -  [anli] premise: Arlette Roxburgh is a Trinidadian American singer and songwriter. She was born in Trinidad. She is best known for singing The Star-Spangled Banner before every New Jersey Devils home game started. When the Nets were in New Jersey, she also sang the national anthem before their home games at the time as well. [SEP] hypothesis: Arlette Roxburgh has only ever sung for the New Jersey Devils.
05/29/2022 18:07:01 - INFO - __main__ - ['neutral']
05/29/2022 18:07:01 - INFO - __main__ -  [anli] premise: Storm Keating (born 27 October 1981) is an Australian-born fashion designer, brand ambassador, producer-director and blogger, now based in London. Keating has worked on a number of Australian and British television programmes such as "The Apprentice Australia", "Masterchef Australia", "The X Factor", "The Voice Australia", and "The Voice UK". She is the wife of Ronan Keating. [SEP] hypothesis: Storm Keaton learned about fashion while studying abroad in the United States.
05/29/2022 18:07:01 - INFO - __main__ - ['neutral']
05/29/2022 18:07:01 - INFO - __main__ -  [anli] premise: Piton is a Pilsner beer brand from the island of Saint Lucia, brewed by Windward & Leeward Brewing Limited, which is owned by Heineken. The beer was named for the Gros Piton and Petit Piton mountains on the island. It was first brewed on October 7, 1992. [SEP] hypothesis: Heineken plans to stop production of Piton in the near future
05/29/2022 18:07:01 - INFO - __main__ - ['neutral']
05/29/2022 18:07:01 - INFO - __main__ - Tokenizing Input ...
05/29/2022 18:07:01 - INFO - __main__ - Tokenizing Output ...
05/29/2022 18:07:01 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 18:07:17 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 18:07:17 - INFO - __main__ - task name: anli
05/29/2022 18:07:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 18:07:18 - INFO - __main__ - Starting training!
05/29/2022 18:07:21 - INFO - __main__ - Step 10 Global step 10 Train loss 6.31 on epoch=0
05/29/2022 18:07:24 - INFO - __main__ - Step 20 Global step 20 Train loss 2.68 on epoch=0
05/29/2022 18:07:26 - INFO - __main__ - Step 30 Global step 30 Train loss 1.31 on epoch=1
05/29/2022 18:07:29 - INFO - __main__ - Step 40 Global step 40 Train loss 0.90 on epoch=1
05/29/2022 18:07:31 - INFO - __main__ - Step 50 Global step 50 Train loss 0.82 on epoch=2
05/29/2022 18:07:39 - INFO - __main__ - Global step 50 Train loss 2.41 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 18:07:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 18:07:42 - INFO - __main__ - Step 60 Global step 60 Train loss 0.77 on epoch=2
05/29/2022 18:07:44 - INFO - __main__ - Step 70 Global step 70 Train loss 0.66 on epoch=2
05/29/2022 18:07:47 - INFO - __main__ - Step 80 Global step 80 Train loss 0.65 on epoch=3
05/29/2022 18:07:49 - INFO - __main__ - Step 90 Global step 90 Train loss 0.69 on epoch=3
05/29/2022 18:07:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.95 on epoch=4
05/29/2022 18:08:01 - INFO - __main__ - Global step 100 Train loss 0.74 Classification-F1 0.16666666666666666 on epoch=4
05/29/2022 18:08:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.69 on epoch=4
05/29/2022 18:08:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.57 on epoch=4
05/29/2022 18:08:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.58 on epoch=5
05/29/2022 18:08:12 - INFO - __main__ - Step 140 Global step 140 Train loss 0.64 on epoch=5
05/29/2022 18:08:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=6
05/29/2022 18:08:23 - INFO - __main__ - Global step 150 Train loss 0.61 Classification-F1 0.1775766716943188 on epoch=6
05/29/2022 18:08:23 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1775766716943188 on epoch=6, global_step=150
05/29/2022 18:08:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.57 on epoch=6
05/29/2022 18:08:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.58 on epoch=7
05/29/2022 18:08:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.56 on epoch=7
05/29/2022 18:08:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.55 on epoch=7
05/29/2022 18:08:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=8
05/29/2022 18:08:44 - INFO - __main__ - Global step 200 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=8
05/29/2022 18:08:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.59 on epoch=8
05/29/2022 18:08:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=9
05/29/2022 18:08:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=9
05/29/2022 18:08:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.56 on epoch=9
05/29/2022 18:08:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.59 on epoch=10
05/29/2022 18:09:06 - INFO - __main__ - Global step 250 Train loss 0.56 Classification-F1 0.24897828863346105 on epoch=10
05/29/2022 18:09:06 - INFO - __main__ - Saving model with best Classification-F1: 0.1775766716943188 -> 0.24897828863346105 on epoch=10, global_step=250
05/29/2022 18:09:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.50 on epoch=10
05/29/2022 18:09:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.54 on epoch=11
05/29/2022 18:09:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.58 on epoch=11
05/29/2022 18:09:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=12
05/29/2022 18:09:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=12
05/29/2022 18:09:30 - INFO - __main__ - Global step 300 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=12
05/29/2022 18:09:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.62 on epoch=12
05/29/2022 18:09:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.59 on epoch=13
05/29/2022 18:09:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.54 on epoch=13
05/29/2022 18:09:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.55 on epoch=14
05/29/2022 18:09:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.57 on epoch=14
05/29/2022 18:09:53 - INFO - __main__ - Global step 350 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=14
05/29/2022 18:09:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=14
05/29/2022 18:09:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.54 on epoch=15
05/29/2022 18:10:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.54 on epoch=15
05/29/2022 18:10:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.53 on epoch=16
05/29/2022 18:10:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.54 on epoch=16
05/29/2022 18:10:15 - INFO - __main__ - Global step 400 Train loss 0.53 Classification-F1 0.1752204585537919 on epoch=16
05/29/2022 18:10:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.50 on epoch=17
05/29/2022 18:10:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=17
05/29/2022 18:10:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.55 on epoch=17
05/29/2022 18:10:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.54 on epoch=18
05/29/2022 18:10:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.52 on epoch=18
05/29/2022 18:10:39 - INFO - __main__ - Global step 450 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=18
05/29/2022 18:10:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=19
05/29/2022 18:10:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=19
05/29/2022 18:10:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.50 on epoch=19
05/29/2022 18:10:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.51 on epoch=20
05/29/2022 18:10:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.54 on epoch=20
05/29/2022 18:11:03 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.26619621474664346 on epoch=20
05/29/2022 18:11:03 - INFO - __main__ - Saving model with best Classification-F1: 0.24897828863346105 -> 0.26619621474664346 on epoch=20, global_step=500
05/29/2022 18:11:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.55 on epoch=21
05/29/2022 18:11:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.54 on epoch=21
05/29/2022 18:11:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=22
05/29/2022 18:11:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.53 on epoch=22
05/29/2022 18:11:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.50 on epoch=22
05/29/2022 18:11:27 - INFO - __main__ - Global step 550 Train loss 0.52 Classification-F1 0.33058602772888496 on epoch=22
05/29/2022 18:11:27 - INFO - __main__ - Saving model with best Classification-F1: 0.26619621474664346 -> 0.33058602772888496 on epoch=22, global_step=550
05/29/2022 18:11:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=23
05/29/2022 18:11:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=23
05/29/2022 18:11:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=24
05/29/2022 18:11:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=24
05/29/2022 18:11:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=24
05/29/2022 18:11:51 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=24
05/29/2022 18:11:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=25
05/29/2022 18:11:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.53 on epoch=25
05/29/2022 18:11:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=26
05/29/2022 18:12:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=26
05/29/2022 18:12:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.50 on epoch=27
05/29/2022 18:12:15 - INFO - __main__ - Global step 650 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=27
05/29/2022 18:12:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.51 on epoch=27
05/29/2022 18:12:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.49 on epoch=27
05/29/2022 18:12:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=28
05/29/2022 18:12:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=28
05/29/2022 18:12:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=29
05/29/2022 18:12:37 - INFO - __main__ - Global step 700 Train loss 0.48 Classification-F1 0.2837473835143803 on epoch=29
05/29/2022 18:12:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=29
05/29/2022 18:12:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.49 on epoch=29
05/29/2022 18:12:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=30
05/29/2022 18:12:47 - INFO - __main__ - Step 740 Global step 740 Train loss 0.55 on epoch=30
05/29/2022 18:12:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=31
05/29/2022 18:12:59 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.1721607831834019 on epoch=31
05/29/2022 18:13:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=31
05/29/2022 18:13:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=32
05/29/2022 18:13:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=32
05/29/2022 18:13:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=32
05/29/2022 18:13:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.46 on epoch=33
05/29/2022 18:13:22 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.21438460441647922 on epoch=33
05/29/2022 18:13:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=33
05/29/2022 18:13:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=34
05/29/2022 18:13:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.50 on epoch=34
05/29/2022 18:13:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.51 on epoch=34
05/29/2022 18:13:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=35
05/29/2022 18:13:44 - INFO - __main__ - Global step 850 Train loss 0.49 Classification-F1 0.18931476564731267 on epoch=35
05/29/2022 18:13:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.51 on epoch=35
05/29/2022 18:13:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=36
05/29/2022 18:13:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.49 on epoch=36
05/29/2022 18:13:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=37
05/29/2022 18:13:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.53 on epoch=37
05/29/2022 18:14:07 - INFO - __main__ - Global step 900 Train loss 0.49 Classification-F1 0.2745267126584309 on epoch=37
05/29/2022 18:14:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.50 on epoch=37
05/29/2022 18:14:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=38
05/29/2022 18:14:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=38
05/29/2022 18:14:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.52 on epoch=39
05/29/2022 18:14:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=39
05/29/2022 18:14:31 - INFO - __main__ - Global step 950 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=39
05/29/2022 18:14:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=39
05/29/2022 18:14:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.46 on epoch=40
05/29/2022 18:14:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.52 on epoch=40
05/29/2022 18:14:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=41
05/29/2022 18:14:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=41
05/29/2022 18:14:55 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=41
05/29/2022 18:14:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.47 on epoch=42
05/29/2022 18:15:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=42
05/29/2022 18:15:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.47 on epoch=42
05/29/2022 18:15:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.47 on epoch=43
05/29/2022 18:15:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=43
05/29/2022 18:15:20 - INFO - __main__ - Global step 1050 Train loss 0.47 Classification-F1 0.2595789034813425 on epoch=43
05/29/2022 18:15:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=44
05/29/2022 18:15:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=44
05/29/2022 18:15:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.49 on epoch=44
05/29/2022 18:15:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=45
05/29/2022 18:15:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.47 on epoch=45
05/29/2022 18:15:44 - INFO - __main__ - Global step 1100 Train loss 0.47 Classification-F1 0.1807224878779611 on epoch=45
05/29/2022 18:15:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.49 on epoch=46
05/29/2022 18:15:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.38 on epoch=46
05/29/2022 18:15:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.58 on epoch=47
05/29/2022 18:15:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.46 on epoch=47
05/29/2022 18:15:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.84 on epoch=47
05/29/2022 18:16:07 - INFO - __main__ - Global step 1150 Train loss 1.75 Classification-F1 0.28751900928923385 on epoch=47
05/29/2022 18:16:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 3.27 on epoch=48
05/29/2022 18:16:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.96 on epoch=48
05/29/2022 18:16:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.90 on epoch=49
05/29/2022 18:16:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.75 on epoch=49
05/29/2022 18:16:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.65 on epoch=49
05/29/2022 18:16:28 - INFO - __main__ - Global step 1200 Train loss 1.51 Classification-F1 0.16666666666666666 on epoch=49
05/29/2022 18:16:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.57 on epoch=50
05/29/2022 18:16:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.55 on epoch=50
05/29/2022 18:16:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.54 on epoch=51
05/29/2022 18:16:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.49 on epoch=51
05/29/2022 18:16:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.54 on epoch=52
05/29/2022 18:16:50 - INFO - __main__ - Global step 1250 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=52
05/29/2022 18:16:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.49 on epoch=52
05/29/2022 18:16:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.48 on epoch=52
05/29/2022 18:16:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.50 on epoch=53
05/29/2022 18:17:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.48 on epoch=53
05/29/2022 18:17:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.52 on epoch=54
05/29/2022 18:17:13 - INFO - __main__ - Global step 1300 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=54
05/29/2022 18:17:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.56 on epoch=54
05/29/2022 18:17:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.50 on epoch=54
05/29/2022 18:17:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.53 on epoch=55
05/29/2022 18:17:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.54 on epoch=55
05/29/2022 18:17:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.52 on epoch=56
05/29/2022 18:17:35 - INFO - __main__ - Global step 1350 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=56
05/29/2022 18:17:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.45 on epoch=56
05/29/2022 18:17:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=57
05/29/2022 18:17:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=57
05/29/2022 18:17:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.53 on epoch=57
05/29/2022 18:17:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.49 on epoch=58
05/29/2022 18:17:58 - INFO - __main__ - Global step 1400 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=58
05/29/2022 18:18:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.50 on epoch=58
05/29/2022 18:18:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.51 on epoch=59
05/29/2022 18:18:06 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=59
05/29/2022 18:18:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.49 on epoch=59
05/29/2022 18:18:11 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.52 on epoch=60
05/29/2022 18:18:20 - INFO - __main__ - Global step 1450 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=60
05/29/2022 18:18:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.49 on epoch=60
05/29/2022 18:18:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.48 on epoch=61
05/29/2022 18:18:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.49 on epoch=61
05/29/2022 18:18:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.47 on epoch=62
05/29/2022 18:18:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.46 on epoch=62
05/29/2022 18:18:43 - INFO - __main__ - Global step 1500 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=62
05/29/2022 18:18:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=62
05/29/2022 18:18:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.47 on epoch=63
05/29/2022 18:18:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.45 on epoch=63
05/29/2022 18:18:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=64
05/29/2022 18:18:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=64
05/29/2022 18:19:06 - INFO - __main__ - Global step 1550 Train loss 0.45 Classification-F1 0.16699282452707112 on epoch=64
05/29/2022 18:19:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.46 on epoch=64
05/29/2022 18:19:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.46 on epoch=65
05/29/2022 18:19:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.51 on epoch=65
05/29/2022 18:19:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.49 on epoch=66
05/29/2022 18:19:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=66
05/29/2022 18:19:28 - INFO - __main__ - Global step 1600 Train loss 0.47 Classification-F1 0.17244846656611368 on epoch=66
05/29/2022 18:19:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.43 on epoch=67
05/29/2022 18:19:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.46 on epoch=67
05/29/2022 18:19:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.50 on epoch=67
05/29/2022 18:19:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.46 on epoch=68
05/29/2022 18:19:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.46 on epoch=68
05/29/2022 18:19:51 - INFO - __main__ - Global step 1650 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=68
05/29/2022 18:19:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.46 on epoch=69
05/29/2022 18:19:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.44 on epoch=69
05/29/2022 18:19:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.45 on epoch=69
05/29/2022 18:20:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.44 on epoch=70
05/29/2022 18:20:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.45 on epoch=70
05/29/2022 18:20:14 - INFO - __main__ - Global step 1700 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=70
05/29/2022 18:20:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.49 on epoch=71
05/29/2022 18:20:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.42 on epoch=71
05/29/2022 18:20:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.46 on epoch=72
05/29/2022 18:20:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.46 on epoch=72
05/29/2022 18:20:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.50 on epoch=72
05/29/2022 18:20:38 - INFO - __main__ - Global step 1750 Train loss 0.47 Classification-F1 0.16699282452707112 on epoch=72
05/29/2022 18:20:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.49 on epoch=73
05/29/2022 18:20:43 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=73
05/29/2022 18:20:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=74
05/29/2022 18:20:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=74
05/29/2022 18:20:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.47 on epoch=74
05/29/2022 18:21:01 - INFO - __main__ - Global step 1800 Train loss 0.45 Classification-F1 0.1961906943013092 on epoch=74
05/29/2022 18:21:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.46 on epoch=75
05/29/2022 18:21:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.48 on epoch=75
05/29/2022 18:21:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.50 on epoch=76
05/29/2022 18:21:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.41 on epoch=76
05/29/2022 18:21:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.50 on epoch=77
05/29/2022 18:21:26 - INFO - __main__ - Global step 1850 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=77
05/29/2022 18:21:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.43 on epoch=77
05/29/2022 18:21:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.45 on epoch=77
05/29/2022 18:21:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.49 on epoch=78
05/29/2022 18:21:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.47 on epoch=78
05/29/2022 18:21:39 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.47 on epoch=79
05/29/2022 18:21:50 - INFO - __main__ - Global step 1900 Train loss 0.46 Classification-F1 0.25534014385091186 on epoch=79
05/29/2022 18:21:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.43 on epoch=79
05/29/2022 18:21:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=79
05/29/2022 18:21:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.45 on epoch=80
05/29/2022 18:22:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.50 on epoch=80
05/29/2022 18:22:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.47 on epoch=81
05/29/2022 18:22:13 - INFO - __main__ - Global step 1950 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=81
05/29/2022 18:22:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.46 on epoch=81
05/29/2022 18:22:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.50 on epoch=82
05/29/2022 18:22:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=82
05/29/2022 18:22:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.49 on epoch=82
05/29/2022 18:22:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=83
05/29/2022 18:22:37 - INFO - __main__ - Global step 2000 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=83
05/29/2022 18:22:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.46 on epoch=83
05/29/2022 18:22:43 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.50 on epoch=84
05/29/2022 18:22:45 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.44 on epoch=84
05/29/2022 18:22:48 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.45 on epoch=84
05/29/2022 18:22:50 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.48 on epoch=85
05/29/2022 18:23:01 - INFO - __main__ - Global step 2050 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=85
05/29/2022 18:23:03 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.44 on epoch=85
05/29/2022 18:23:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.46 on epoch=86
05/29/2022 18:23:09 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.45 on epoch=86
05/29/2022 18:23:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.47 on epoch=87
05/29/2022 18:23:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.45 on epoch=87
05/29/2022 18:23:25 - INFO - __main__ - Global step 2100 Train loss 0.45 Classification-F1 0.1676489849377865 on epoch=87
05/29/2022 18:23:28 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.49 on epoch=87
05/29/2022 18:23:30 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.47 on epoch=88
05/29/2022 18:23:33 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.52 on epoch=88
05/29/2022 18:23:35 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.47 on epoch=89
05/29/2022 18:23:38 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.45 on epoch=89
05/29/2022 18:23:49 - INFO - __main__ - Global step 2150 Train loss 0.48 Classification-F1 0.24948631846547886 on epoch=89
05/29/2022 18:23:52 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.44 on epoch=89
05/29/2022 18:23:54 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.48 on epoch=90
05/29/2022 18:23:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.45 on epoch=90
05/29/2022 18:24:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.48 on epoch=91
05/29/2022 18:24:02 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.43 on epoch=91
05/29/2022 18:24:13 - INFO - __main__ - Global step 2200 Train loss 0.45 Classification-F1 0.182590043793011 on epoch=91
05/29/2022 18:24:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.45 on epoch=92
05/29/2022 18:24:18 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.38 on epoch=92
05/29/2022 18:24:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.47 on epoch=92
05/29/2022 18:24:24 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.43 on epoch=93
05/29/2022 18:24:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.44 on epoch=93
05/29/2022 18:24:37 - INFO - __main__ - Global step 2250 Train loss 0.44 Classification-F1 0.16699282452707112 on epoch=93
05/29/2022 18:24:40 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.45 on epoch=94
05/29/2022 18:24:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.45 on epoch=94
05/29/2022 18:24:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.45 on epoch=94
05/29/2022 18:24:48 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.46 on epoch=95
05/29/2022 18:24:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.45 on epoch=95
05/29/2022 18:25:02 - INFO - __main__ - Global step 2300 Train loss 0.45 Classification-F1 0.320343728998628 on epoch=95
05/29/2022 18:25:05 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.47 on epoch=96
05/29/2022 18:25:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.44 on epoch=96
05/29/2022 18:25:10 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.46 on epoch=97
05/29/2022 18:25:13 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.43 on epoch=97
05/29/2022 18:25:15 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.45 on epoch=97
05/29/2022 18:25:26 - INFO - __main__ - Global step 2350 Train loss 0.45 Classification-F1 0.2903559648825766 on epoch=97
05/29/2022 18:25:29 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.47 on epoch=98
05/29/2022 18:25:31 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.46 on epoch=98
05/29/2022 18:25:34 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.44 on epoch=99
05/29/2022 18:25:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.45 on epoch=99
05/29/2022 18:25:39 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.42 on epoch=99
05/29/2022 18:25:50 - INFO - __main__ - Global step 2400 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=99
05/29/2022 18:25:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.42 on epoch=100
05/29/2022 18:25:55 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.48 on epoch=100
05/29/2022 18:25:58 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.46 on epoch=101
05/29/2022 18:26:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.40 on epoch=101
05/29/2022 18:26:03 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.46 on epoch=102
05/29/2022 18:26:13 - INFO - __main__ - Global step 2450 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=102
05/29/2022 18:26:16 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.42 on epoch=102
05/29/2022 18:26:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.48 on epoch=102
05/29/2022 18:26:21 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.52 on epoch=103
05/29/2022 18:26:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.42 on epoch=103
05/29/2022 18:26:26 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.41 on epoch=104
05/29/2022 18:26:37 - INFO - __main__ - Global step 2500 Train loss 0.45 Classification-F1 0.27232608790493357 on epoch=104
05/29/2022 18:26:40 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.44 on epoch=104
05/29/2022 18:26:42 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.51 on epoch=104
05/29/2022 18:26:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.43 on epoch=105
05/29/2022 18:26:48 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.46 on epoch=105
05/29/2022 18:26:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.43 on epoch=106
05/29/2022 18:27:01 - INFO - __main__ - Global step 2550 Train loss 0.45 Classification-F1 0.17244846656611368 on epoch=106
05/29/2022 18:27:04 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.45 on epoch=106
05/29/2022 18:27:07 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.49 on epoch=107
05/29/2022 18:27:09 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.40 on epoch=107
05/29/2022 18:27:12 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.49 on epoch=107
05/29/2022 18:27:15 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.43 on epoch=108
05/29/2022 18:27:26 - INFO - __main__ - Global step 2600 Train loss 0.45 Classification-F1 0.2806683103255923 on epoch=108
05/29/2022 18:27:29 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.43 on epoch=108
05/29/2022 18:27:31 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.49 on epoch=109
05/29/2022 18:27:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.44 on epoch=109
05/29/2022 18:27:37 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.45 on epoch=109
05/29/2022 18:27:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.45 on epoch=110
05/29/2022 18:27:51 - INFO - __main__ - Global step 2650 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=110
05/29/2022 18:27:53 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.47 on epoch=110
05/29/2022 18:27:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.48 on epoch=111
05/29/2022 18:27:59 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.46 on epoch=111
05/29/2022 18:28:01 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.42 on epoch=112
05/29/2022 18:28:04 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.47 on epoch=112
05/29/2022 18:28:15 - INFO - __main__ - Global step 2700 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=112
05/29/2022 18:28:18 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.46 on epoch=112
05/29/2022 18:28:20 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.51 on epoch=113
05/29/2022 18:28:23 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.44 on epoch=113
05/29/2022 18:28:26 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.45 on epoch=114
05/29/2022 18:28:28 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.43 on epoch=114
05/29/2022 18:28:40 - INFO - __main__ - Global step 2750 Train loss 0.46 Classification-F1 0.2351843079117768 on epoch=114
05/29/2022 18:28:43 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.43 on epoch=114
05/29/2022 18:28:45 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.45 on epoch=115
05/29/2022 18:28:48 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.48 on epoch=115
05/29/2022 18:28:51 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.49 on epoch=116
05/29/2022 18:28:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.42 on epoch=116
05/29/2022 18:29:05 - INFO - __main__ - Global step 2800 Train loss 0.45 Classification-F1 0.17765617875057005 on epoch=116
05/29/2022 18:29:07 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.47 on epoch=117
05/29/2022 18:29:10 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.46 on epoch=117
05/29/2022 18:29:12 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.43 on epoch=117
05/29/2022 18:29:15 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.42 on epoch=118
05/29/2022 18:29:18 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.44 on epoch=118
05/29/2022 18:29:29 - INFO - __main__ - Global step 2850 Train loss 0.44 Classification-F1 0.1806445111063366 on epoch=118
05/29/2022 18:29:32 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.47 on epoch=119
05/29/2022 18:29:34 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.45 on epoch=119
05/29/2022 18:29:37 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.46 on epoch=119
05/29/2022 18:29:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.46 on epoch=120
05/29/2022 18:29:42 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.45 on epoch=120
05/29/2022 18:29:54 - INFO - __main__ - Global step 2900 Train loss 0.46 Classification-F1 0.21350762527233114 on epoch=120
05/29/2022 18:29:57 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.52 on epoch=121
05/29/2022 18:29:59 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.45 on epoch=121
05/29/2022 18:30:02 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.48 on epoch=122
05/29/2022 18:30:05 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.45 on epoch=122
05/29/2022 18:30:07 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.48 on epoch=122
05/29/2022 18:30:19 - INFO - __main__ - Global step 2950 Train loss 0.48 Classification-F1 0.3357226238505806 on epoch=122
05/29/2022 18:30:19 - INFO - __main__ - Saving model with best Classification-F1: 0.33058602772888496 -> 0.3357226238505806 on epoch=122, global_step=2950
05/29/2022 18:30:22 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.41 on epoch=123
05/29/2022 18:30:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.41 on epoch=123
05/29/2022 18:30:27 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.44 on epoch=124
05/29/2022 18:30:30 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.40 on epoch=124
05/29/2022 18:30:32 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.51 on epoch=124
05/29/2022 18:30:33 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 18:30:33 - INFO - __main__ - Printing 3 examples
05/29/2022 18:30:33 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/29/2022 18:30:33 - INFO - __main__ - ['neutral']
05/29/2022 18:30:33 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/29/2022 18:30:33 - INFO - __main__ - ['neutral']
05/29/2022 18:30:33 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/29/2022 18:30:33 - INFO - __main__ - ['neutral']
05/29/2022 18:30:33 - INFO - __main__ - Tokenizing Input ...
05/29/2022 18:30:34 - INFO - __main__ - Tokenizing Output ...
05/29/2022 18:30:34 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 18:30:34 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 18:30:34 - INFO - __main__ - Printing 3 examples
05/29/2022 18:30:34 - INFO - __main__ -  [anli] premise: Arlette Roxburgh is a Trinidadian American singer and songwriter. She was born in Trinidad. She is best known for singing The Star-Spangled Banner before every New Jersey Devils home game started. When the Nets were in New Jersey, she also sang the national anthem before their home games at the time as well. [SEP] hypothesis: Arlette Roxburgh has only ever sung for the New Jersey Devils.
05/29/2022 18:30:34 - INFO - __main__ - ['neutral']
05/29/2022 18:30:34 - INFO - __main__ -  [anli] premise: Storm Keating (born 27 October 1981) is an Australian-born fashion designer, brand ambassador, producer-director and blogger, now based in London. Keating has worked on a number of Australian and British television programmes such as "The Apprentice Australia", "Masterchef Australia", "The X Factor", "The Voice Australia", and "The Voice UK". She is the wife of Ronan Keating. [SEP] hypothesis: Storm Keaton learned about fashion while studying abroad in the United States.
05/29/2022 18:30:34 - INFO - __main__ - ['neutral']
05/29/2022 18:30:34 - INFO - __main__ -  [anli] premise: Piton is a Pilsner beer brand from the island of Saint Lucia, brewed by Windward & Leeward Brewing Limited, which is owned by Heineken. The beer was named for the Gros Piton and Petit Piton mountains on the island. It was first brewed on October 7, 1992. [SEP] hypothesis: Heineken plans to stop production of Piton in the near future
05/29/2022 18:30:34 - INFO - __main__ - ['neutral']
05/29/2022 18:30:34 - INFO - __main__ - Tokenizing Input ...
05/29/2022 18:30:34 - INFO - __main__ - Tokenizing Output ...
05/29/2022 18:30:35 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 18:30:44 - INFO - __main__ - Global step 3000 Train loss 0.43 Classification-F1 0.24071156678058403 on epoch=124
05/29/2022 18:30:44 - INFO - __main__ - save last model!
05/29/2022 18:30:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 18:30:44 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 18:30:44 - INFO - __main__ - Printing 3 examples
05/29/2022 18:30:44 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 18:30:44 - INFO - __main__ - ['contradiction']
05/29/2022 18:30:44 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 18:30:44 - INFO - __main__ - ['entailment']
05/29/2022 18:30:44 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 18:30:44 - INFO - __main__ - ['contradiction']
05/29/2022 18:30:44 - INFO - __main__ - Tokenizing Input ...
05/29/2022 18:30:45 - INFO - __main__ - Tokenizing Output ...
05/29/2022 18:30:46 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 18:30:53 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 18:30:53 - INFO - __main__ - task name: anli
05/29/2022 18:30:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 18:30:54 - INFO - __main__ - Starting training!
05/29/2022 18:31:17 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_42_0.5_8_predictions.txt
05/29/2022 18:31:17 - INFO - __main__ - Classification-F1 on test data: 0.2517
05/29/2022 18:31:17 - INFO - __main__ - prefix=anli_128_42, lr=0.5, bsz=8, dev_performance=0.3357226238505806, test_performance=0.25169720721060457
05/29/2022 18:31:17 - INFO - __main__ - Running ... prefix=anli_128_42, lr=0.4, bsz=8 ...
05/29/2022 18:31:18 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 18:31:18 - INFO - __main__ - Printing 3 examples
05/29/2022 18:31:18 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/29/2022 18:31:18 - INFO - __main__ - ['neutral']
05/29/2022 18:31:18 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/29/2022 18:31:18 - INFO - __main__ - ['neutral']
05/29/2022 18:31:18 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/29/2022 18:31:18 - INFO - __main__ - ['neutral']
05/29/2022 18:31:18 - INFO - __main__ - Tokenizing Input ...
05/29/2022 18:31:18 - INFO - __main__ - Tokenizing Output ...
05/29/2022 18:31:19 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 18:31:19 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 18:31:19 - INFO - __main__ - Printing 3 examples
05/29/2022 18:31:19 - INFO - __main__ -  [anli] premise: Arlette Roxburgh is a Trinidadian American singer and songwriter. She was born in Trinidad. She is best known for singing The Star-Spangled Banner before every New Jersey Devils home game started. When the Nets were in New Jersey, she also sang the national anthem before their home games at the time as well. [SEP] hypothesis: Arlette Roxburgh has only ever sung for the New Jersey Devils.
05/29/2022 18:31:19 - INFO - __main__ - ['neutral']
05/29/2022 18:31:19 - INFO - __main__ -  [anli] premise: Storm Keating (born 27 October 1981) is an Australian-born fashion designer, brand ambassador, producer-director and blogger, now based in London. Keating has worked on a number of Australian and British television programmes such as "The Apprentice Australia", "Masterchef Australia", "The X Factor", "The Voice Australia", and "The Voice UK". She is the wife of Ronan Keating. [SEP] hypothesis: Storm Keaton learned about fashion while studying abroad in the United States.
05/29/2022 18:31:19 - INFO - __main__ - ['neutral']
05/29/2022 18:31:19 - INFO - __main__ -  [anli] premise: Piton is a Pilsner beer brand from the island of Saint Lucia, brewed by Windward & Leeward Brewing Limited, which is owned by Heineken. The beer was named for the Gros Piton and Petit Piton mountains on the island. It was first brewed on October 7, 1992. [SEP] hypothesis: Heineken plans to stop production of Piton in the near future
05/29/2022 18:31:19 - INFO - __main__ - ['neutral']
05/29/2022 18:31:19 - INFO - __main__ - Tokenizing Input ...
05/29/2022 18:31:19 - INFO - __main__ - Tokenizing Output ...
05/29/2022 18:31:19 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 18:31:38 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 18:31:38 - INFO - __main__ - task name: anli
05/29/2022 18:31:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 18:31:39 - INFO - __main__ - Starting training!
05/29/2022 18:31:42 - INFO - __main__ - Step 10 Global step 10 Train loss 6.20 on epoch=0
05/29/2022 18:31:44 - INFO - __main__ - Step 20 Global step 20 Train loss 2.94 on epoch=0
05/29/2022 18:31:47 - INFO - __main__ - Step 30 Global step 30 Train loss 1.38 on epoch=1
05/29/2022 18:31:50 - INFO - __main__ - Step 40 Global step 40 Train loss 0.91 on epoch=1
05/29/2022 18:31:52 - INFO - __main__ - Step 50 Global step 50 Train loss 0.87 on epoch=2
05/29/2022 18:32:01 - INFO - __main__ - Global step 50 Train loss 2.46 Classification-F1 0.23404255319148937 on epoch=2
05/29/2022 18:32:01 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.23404255319148937 on epoch=2, global_step=50
05/29/2022 18:32:04 - INFO - __main__ - Step 60 Global step 60 Train loss 0.67 on epoch=2
05/29/2022 18:32:06 - INFO - __main__ - Step 70 Global step 70 Train loss 0.73 on epoch=2
05/29/2022 18:32:09 - INFO - __main__ - Step 80 Global step 80 Train loss 0.61 on epoch=3
05/29/2022 18:32:11 - INFO - __main__ - Step 90 Global step 90 Train loss 0.62 on epoch=3
05/29/2022 18:32:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=4
05/29/2022 18:32:22 - INFO - __main__ - Global step 100 Train loss 0.64 Classification-F1 0.16666666666666666 on epoch=4
05/29/2022 18:32:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.57 on epoch=4
05/29/2022 18:32:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.63 on epoch=4
05/29/2022 18:32:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.59 on epoch=5
05/29/2022 18:32:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.61 on epoch=5
05/29/2022 18:32:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.56 on epoch=6
05/29/2022 18:32:44 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.1721607831834019 on epoch=6
05/29/2022 18:32:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.56 on epoch=6
05/29/2022 18:32:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=7
05/29/2022 18:32:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.54 on epoch=7
05/29/2022 18:32:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.60 on epoch=7
05/29/2022 18:32:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=8
05/29/2022 18:33:06 - INFO - __main__ - Global step 200 Train loss 0.55 Classification-F1 0.24002054269630912 on epoch=8
05/29/2022 18:33:06 - INFO - __main__ - Saving model with best Classification-F1: 0.23404255319148937 -> 0.24002054269630912 on epoch=8, global_step=200
05/29/2022 18:33:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=8
05/29/2022 18:33:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=9
05/29/2022 18:33:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.61 on epoch=9
05/29/2022 18:33:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.55 on epoch=9
05/29/2022 18:33:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=10
05/29/2022 18:33:31 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.23835105558826783 on epoch=10
05/29/2022 18:33:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.58 on epoch=10
05/29/2022 18:33:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.55 on epoch=11
05/29/2022 18:33:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.59 on epoch=11
05/29/2022 18:33:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.55 on epoch=12
05/29/2022 18:33:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=12
05/29/2022 18:33:54 - INFO - __main__ - Global step 300 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=12
05/29/2022 18:33:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.57 on epoch=12
05/29/2022 18:33:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.52 on epoch=13
05/29/2022 18:34:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.54 on epoch=13
05/29/2022 18:34:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=14
05/29/2022 18:34:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=14
05/29/2022 18:34:18 - INFO - __main__ - Global step 350 Train loss 0.53 Classification-F1 0.29878045076240817 on epoch=14
05/29/2022 18:34:18 - INFO - __main__ - Saving model with best Classification-F1: 0.24002054269630912 -> 0.29878045076240817 on epoch=14, global_step=350
05/29/2022 18:34:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.54 on epoch=14
05/29/2022 18:34:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=15
05/29/2022 18:34:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=15
05/29/2022 18:34:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=16
05/29/2022 18:34:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.51 on epoch=16
05/29/2022 18:34:43 - INFO - __main__ - Global step 400 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=16
05/29/2022 18:34:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=17
05/29/2022 18:34:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=17
05/29/2022 18:34:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=17
05/29/2022 18:34:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.53 on epoch=18
05/29/2022 18:34:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.49 on epoch=18
05/29/2022 18:35:07 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=18
05/29/2022 18:35:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=19
05/29/2022 18:35:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=19
05/29/2022 18:35:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=19
05/29/2022 18:35:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=20
05/29/2022 18:35:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.51 on epoch=20
05/29/2022 18:35:31 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.17230055257310287 on epoch=20
05/29/2022 18:35:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.51 on epoch=21
05/29/2022 18:35:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=21
05/29/2022 18:35:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.51 on epoch=22
05/29/2022 18:35:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.52 on epoch=22
05/29/2022 18:35:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.54 on epoch=22
05/29/2022 18:35:56 - INFO - __main__ - Global step 550 Train loss 0.51 Classification-F1 0.2660968660968661 on epoch=22
05/29/2022 18:35:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=23
05/29/2022 18:36:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=23
05/29/2022 18:36:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.55 on epoch=24
05/29/2022 18:36:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.51 on epoch=24
05/29/2022 18:36:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=24
05/29/2022 18:36:19 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=24
05/29/2022 18:36:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=25
05/29/2022 18:36:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.50 on epoch=25
05/29/2022 18:36:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=26
05/29/2022 18:36:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=26
05/29/2022 18:36:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=27
05/29/2022 18:36:44 - INFO - __main__ - Global step 650 Train loss 0.47 Classification-F1 0.17244846656611368 on epoch=27
05/29/2022 18:36:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.53 on epoch=27
05/29/2022 18:36:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.53 on epoch=27
05/29/2022 18:36:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=28
05/29/2022 18:36:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.52 on epoch=28
05/29/2022 18:36:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=29
05/29/2022 18:37:08 - INFO - __main__ - Global step 700 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=29
05/29/2022 18:37:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=29
05/29/2022 18:37:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.51 on epoch=29
05/29/2022 18:37:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.48 on epoch=30
05/29/2022 18:37:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=30
05/29/2022 18:37:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.50 on epoch=31
05/29/2022 18:37:33 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=31
05/29/2022 18:37:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=31
05/29/2022 18:37:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=32
05/29/2022 18:37:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.49 on epoch=32
05/29/2022 18:37:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=32
05/29/2022 18:37:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=33
05/29/2022 18:37:57 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=33
05/29/2022 18:37:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.47 on epoch=33
05/29/2022 18:38:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.45 on epoch=34
05/29/2022 18:38:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=34
05/29/2022 18:38:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=34
05/29/2022 18:38:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=35
05/29/2022 18:38:22 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.28028527404320885 on epoch=35
05/29/2022 18:38:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.49 on epoch=35
05/29/2022 18:38:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.52 on epoch=36
05/29/2022 18:38:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=36
05/29/2022 18:38:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.47 on epoch=37
05/29/2022 18:38:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=37
05/29/2022 18:38:46 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=37
05/29/2022 18:38:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.48 on epoch=37
05/29/2022 18:38:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.47 on epoch=38
05/29/2022 18:38:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.48 on epoch=38
05/29/2022 18:38:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=39
05/29/2022 18:38:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=39
05/29/2022 18:39:11 - INFO - __main__ - Global step 950 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=39
05/29/2022 18:39:13 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=39
05/29/2022 18:39:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=40
05/29/2022 18:39:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.54 on epoch=40
05/29/2022 18:39:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.48 on epoch=41
05/29/2022 18:39:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=41
05/29/2022 18:39:34 - INFO - __main__ - Global step 1000 Train loss 0.48 Classification-F1 0.1721607831834019 on epoch=41
05/29/2022 18:39:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.49 on epoch=42
05/29/2022 18:39:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.51 on epoch=42
05/29/2022 18:39:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.48 on epoch=42
05/29/2022 18:39:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=43
05/29/2022 18:39:47 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.48 on epoch=43
05/29/2022 18:39:58 - INFO - __main__ - Global step 1050 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=43
05/29/2022 18:40:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.43 on epoch=44
05/29/2022 18:40:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=44
05/29/2022 18:40:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.48 on epoch=44
05/29/2022 18:40:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.50 on epoch=45
05/29/2022 18:40:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.50 on epoch=45
05/29/2022 18:40:23 - INFO - __main__ - Global step 1100 Train loss 0.47 Classification-F1 0.2365468309592976 on epoch=45
05/29/2022 18:40:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=46
05/29/2022 18:40:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.48 on epoch=46
05/29/2022 18:40:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.48 on epoch=47
05/29/2022 18:40:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=47
05/29/2022 18:40:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=47
05/29/2022 18:40:48 - INFO - __main__ - Global step 1150 Train loss 0.47 Classification-F1 0.20410943091468303 on epoch=47
05/29/2022 18:40:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.48 on epoch=48
05/29/2022 18:40:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.49 on epoch=48
05/29/2022 18:40:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.49 on epoch=49
05/29/2022 18:40:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=49
05/29/2022 18:41:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.51 on epoch=49
05/29/2022 18:41:13 - INFO - __main__ - Global step 1200 Train loss 0.48 Classification-F1 0.22179587937065628 on epoch=49
05/29/2022 18:41:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.50 on epoch=50
05/29/2022 18:41:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.55 on epoch=50
05/29/2022 18:41:20 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.46 on epoch=51
05/29/2022 18:41:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=51
05/29/2022 18:41:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.50 on epoch=52
05/29/2022 18:41:37 - INFO - __main__ - Global step 1250 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=52
05/29/2022 18:41:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.43 on epoch=52
05/29/2022 18:41:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.50 on epoch=52
05/29/2022 18:41:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.44 on epoch=53
05/29/2022 18:41:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=53
05/29/2022 18:41:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.50 on epoch=54
05/29/2022 18:42:01 - INFO - __main__ - Global step 1300 Train loss 0.47 Classification-F1 0.16732026143790854 on epoch=54
05/29/2022 18:42:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=54
05/29/2022 18:42:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.48 on epoch=54
05/29/2022 18:42:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.48 on epoch=55
05/29/2022 18:42:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.46 on epoch=55
05/29/2022 18:42:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=56
05/29/2022 18:42:25 - INFO - __main__ - Global step 1350 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=56
05/29/2022 18:42:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=56
05/29/2022 18:42:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=57
05/29/2022 18:42:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.44 on epoch=57
05/29/2022 18:42:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.47 on epoch=57
05/29/2022 18:42:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.47 on epoch=58
05/29/2022 18:42:50 - INFO - __main__ - Global step 1400 Train loss 0.45 Classification-F1 0.25743853293455216 on epoch=58
05/29/2022 18:42:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=58
05/29/2022 18:42:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.46 on epoch=59
05/29/2022 18:42:58 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.47 on epoch=59
05/29/2022 18:43:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.46 on epoch=59
05/29/2022 18:43:03 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.46 on epoch=60
05/29/2022 18:43:15 - INFO - __main__ - Global step 1450 Train loss 0.46 Classification-F1 0.22349483183114785 on epoch=60
05/29/2022 18:43:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.45 on epoch=60
05/29/2022 18:43:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=61
05/29/2022 18:43:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.45 on epoch=61
05/29/2022 18:43:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.46 on epoch=62
05/29/2022 18:43:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.47 on epoch=62
05/29/2022 18:43:40 - INFO - __main__ - Global step 1500 Train loss 0.45 Classification-F1 0.26872383106479797 on epoch=62
05/29/2022 18:43:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=62
05/29/2022 18:43:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.47 on epoch=63
05/29/2022 18:43:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.47 on epoch=63
05/29/2022 18:43:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.45 on epoch=64
05/29/2022 18:43:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=64
05/29/2022 18:44:05 - INFO - __main__ - Global step 1550 Train loss 0.45 Classification-F1 0.25290408872498427 on epoch=64
05/29/2022 18:44:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.50 on epoch=64
05/29/2022 18:44:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.44 on epoch=65
05/29/2022 18:44:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.44 on epoch=65
05/29/2022 18:44:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.46 on epoch=66
05/29/2022 18:44:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=66
05/29/2022 18:44:30 - INFO - __main__ - Global step 1600 Train loss 0.46 Classification-F1 0.1721607831834019 on epoch=66
05/29/2022 18:44:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.49 on epoch=67
05/29/2022 18:44:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.44 on epoch=67
05/29/2022 18:44:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=67
05/29/2022 18:44:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.50 on epoch=68
05/29/2022 18:44:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.48 on epoch=68
05/29/2022 18:44:54 - INFO - __main__ - Global step 1650 Train loss 0.47 Classification-F1 0.19573696145124717 on epoch=68
05/29/2022 18:44:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.45 on epoch=69
05/29/2022 18:44:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.50 on epoch=69
05/29/2022 18:45:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=69
05/29/2022 18:45:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.44 on epoch=70
05/29/2022 18:45:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.47 on epoch=70
05/29/2022 18:45:19 - INFO - __main__ - Global step 1700 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=70
05/29/2022 18:45:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.45 on epoch=71
05/29/2022 18:45:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.48 on epoch=71
05/29/2022 18:45:27 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.59 on epoch=72
05/29/2022 18:45:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.50 on epoch=72
05/29/2022 18:45:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.49 on epoch=72
05/29/2022 18:45:44 - INFO - __main__ - Global step 1750 Train loss 0.50 Classification-F1 0.2091306626190347 on epoch=72
05/29/2022 18:45:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.48 on epoch=73
05/29/2022 18:45:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.48 on epoch=73
05/29/2022 18:45:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.51 on epoch=74
05/29/2022 18:45:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.45 on epoch=74
05/29/2022 18:45:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=74
05/29/2022 18:46:09 - INFO - __main__ - Global step 1800 Train loss 0.47 Classification-F1 0.2002825968210115 on epoch=74
05/29/2022 18:46:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.43 on epoch=75
05/29/2022 18:46:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.50 on epoch=75
05/29/2022 18:46:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.51 on epoch=76
05/29/2022 18:46:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.45 on epoch=76
05/29/2022 18:46:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.54 on epoch=77
05/29/2022 18:46:33 - INFO - __main__ - Global step 1850 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=77
05/29/2022 18:46:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=77
05/29/2022 18:46:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.47 on epoch=77
05/29/2022 18:46:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.48 on epoch=78
05/29/2022 18:46:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.45 on epoch=78
05/29/2022 18:46:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.42 on epoch=79
05/29/2022 18:46:58 - INFO - __main__ - Global step 1900 Train loss 0.45 Classification-F1 0.16568819308545338 on epoch=79
05/29/2022 18:47:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.47 on epoch=79
05/29/2022 18:47:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=79
05/29/2022 18:47:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.49 on epoch=80
05/29/2022 18:47:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.48 on epoch=80
05/29/2022 18:47:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.44 on epoch=81
05/29/2022 18:47:22 - INFO - __main__ - Global step 1950 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=81
05/29/2022 18:47:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=81
05/29/2022 18:47:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.50 on epoch=82
05/29/2022 18:47:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.50 on epoch=82
05/29/2022 18:47:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.48 on epoch=82
05/29/2022 18:47:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.44 on epoch=83
05/29/2022 18:47:47 - INFO - __main__ - Global step 2000 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=83
05/29/2022 18:47:49 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.43 on epoch=83
05/29/2022 18:47:52 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.45 on epoch=84
05/29/2022 18:47:55 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.41 on epoch=84
05/29/2022 18:47:57 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.47 on epoch=84
05/29/2022 18:48:00 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.43 on epoch=85
05/29/2022 18:48:12 - INFO - __main__ - Global step 2050 Train loss 0.44 Classification-F1 0.19343497924555755 on epoch=85
05/29/2022 18:48:14 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.48 on epoch=85
05/29/2022 18:48:17 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.48 on epoch=86
05/29/2022 18:48:20 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.43 on epoch=86
05/29/2022 18:48:22 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.44 on epoch=87
05/29/2022 18:48:25 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.45 on epoch=87
05/29/2022 18:48:37 - INFO - __main__ - Global step 2100 Train loss 0.46 Classification-F1 0.24698362665265738 on epoch=87
05/29/2022 18:48:39 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.48 on epoch=87
05/29/2022 18:48:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.45 on epoch=88
05/29/2022 18:48:45 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.45 on epoch=88
05/29/2022 18:48:48 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.45 on epoch=89
05/29/2022 18:48:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.49 on epoch=89
05/29/2022 18:49:02 - INFO - __main__ - Global step 2150 Train loss 0.46 Classification-F1 0.2447196513293576 on epoch=89
05/29/2022 18:49:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.45 on epoch=89
05/29/2022 18:49:07 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.45 on epoch=90
05/29/2022 18:49:10 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.45 on epoch=90
05/29/2022 18:49:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.47 on epoch=91
05/29/2022 18:49:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.44 on epoch=91
05/29/2022 18:49:27 - INFO - __main__ - Global step 2200 Train loss 0.45 Classification-F1 0.20245472719942478 on epoch=91
05/29/2022 18:49:29 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.49 on epoch=92
05/29/2022 18:49:32 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.45 on epoch=92
05/29/2022 18:49:35 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.46 on epoch=92
05/29/2022 18:49:37 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.46 on epoch=93
05/29/2022 18:49:40 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.45 on epoch=93
05/29/2022 18:49:52 - INFO - __main__ - Global step 2250 Train loss 0.46 Classification-F1 0.24222222222222223 on epoch=93
05/29/2022 18:49:54 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.46 on epoch=94
05/29/2022 18:49:57 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.44 on epoch=94
05/29/2022 18:50:00 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.46 on epoch=94
05/29/2022 18:50:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.46 on epoch=95
05/29/2022 18:50:05 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.46 on epoch=95
05/29/2022 18:50:17 - INFO - __main__ - Global step 2300 Train loss 0.45 Classification-F1 0.22295954941116233 on epoch=95
05/29/2022 18:50:19 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.53 on epoch=96
05/29/2022 18:50:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.45 on epoch=96
05/29/2022 18:50:24 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.46 on epoch=97
05/29/2022 18:50:27 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.45 on epoch=97
05/29/2022 18:50:30 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.43 on epoch=97
05/29/2022 18:50:42 - INFO - __main__ - Global step 2350 Train loss 0.46 Classification-F1 0.1775132275132275 on epoch=97
05/29/2022 18:50:44 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.45 on epoch=98
05/29/2022 18:50:47 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.43 on epoch=98
05/29/2022 18:50:49 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.48 on epoch=99
05/29/2022 18:50:52 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.43 on epoch=99
05/29/2022 18:50:55 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.41 on epoch=99
05/29/2022 18:51:07 - INFO - __main__ - Global step 2400 Train loss 0.44 Classification-F1 0.16201859229747675 on epoch=99
05/29/2022 18:51:09 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.43 on epoch=100
05/29/2022 18:51:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.42 on epoch=100
05/29/2022 18:51:14 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.50 on epoch=101
05/29/2022 18:51:17 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.41 on epoch=101
05/29/2022 18:51:20 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.42 on epoch=102
05/29/2022 18:51:31 - INFO - __main__ - Global step 2450 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=102
05/29/2022 18:51:33 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.45 on epoch=102
05/29/2022 18:51:36 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.45 on epoch=102
05/29/2022 18:51:39 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.46 on epoch=103
05/29/2022 18:51:41 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.44 on epoch=103
05/29/2022 18:51:44 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.49 on epoch=104
05/29/2022 18:51:56 - INFO - __main__ - Global step 2500 Train loss 0.46 Classification-F1 0.27110694183864914 on epoch=104
05/29/2022 18:51:58 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.48 on epoch=104
05/29/2022 18:52:01 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.47 on epoch=104
05/29/2022 18:52:03 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.41 on epoch=105
05/29/2022 18:52:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.47 on epoch=105
05/29/2022 18:52:09 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.49 on epoch=106
05/29/2022 18:52:20 - INFO - __main__ - Global step 2550 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=106
05/29/2022 18:52:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.44 on epoch=106
05/29/2022 18:52:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.52 on epoch=107
05/29/2022 18:52:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.41 on epoch=107
05/29/2022 18:52:30 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.45 on epoch=107
05/29/2022 18:52:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.49 on epoch=108
05/29/2022 18:52:45 - INFO - __main__ - Global step 2600 Train loss 0.46 Classification-F1 0.27465650742838726 on epoch=108
05/29/2022 18:52:48 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.45 on epoch=108
05/29/2022 18:52:50 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.49 on epoch=109
05/29/2022 18:52:53 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.44 on epoch=109
05/29/2022 18:52:55 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.44 on epoch=109
05/29/2022 18:52:58 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.49 on epoch=110
05/29/2022 18:53:10 - INFO - __main__ - Global step 2650 Train loss 0.46 Classification-F1 0.20998993887895523 on epoch=110
05/29/2022 18:53:12 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.46 on epoch=110
05/29/2022 18:53:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.46 on epoch=111
05/29/2022 18:53:18 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.42 on epoch=111
05/29/2022 18:53:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.46 on epoch=112
05/29/2022 18:53:23 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.48 on epoch=112
05/29/2022 18:53:34 - INFO - __main__ - Global step 2700 Train loss 0.46 Classification-F1 0.23885454172591972 on epoch=112
05/29/2022 18:53:37 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.46 on epoch=112
05/29/2022 18:53:40 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.45 on epoch=113
05/29/2022 18:53:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.45 on epoch=113
05/29/2022 18:53:45 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.47 on epoch=114
05/29/2022 18:53:48 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.42 on epoch=114
05/29/2022 18:53:59 - INFO - __main__ - Global step 2750 Train loss 0.45 Classification-F1 0.21398218362429475 on epoch=114
05/29/2022 18:54:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.46 on epoch=114
05/29/2022 18:54:05 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.46 on epoch=115
05/29/2022 18:54:07 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.46 on epoch=115
05/29/2022 18:54:10 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.47 on epoch=116
05/29/2022 18:54:12 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.42 on epoch=116
05/29/2022 18:54:24 - INFO - __main__ - Global step 2800 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=116
05/29/2022 18:54:26 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.46 on epoch=117
05/29/2022 18:54:29 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.45 on epoch=117
05/29/2022 18:54:32 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.47 on epoch=117
05/29/2022 18:54:34 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.45 on epoch=118
05/29/2022 18:54:37 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.44 on epoch=118
05/29/2022 18:54:48 - INFO - __main__ - Global step 2850 Train loss 0.45 Classification-F1 0.1721607831834019 on epoch=118
05/29/2022 18:54:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.44 on epoch=119
05/29/2022 18:54:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.48 on epoch=119
05/29/2022 18:54:56 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.46 on epoch=119
05/29/2022 18:54:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.46 on epoch=120
05/29/2022 18:55:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.50 on epoch=120
05/29/2022 18:55:13 - INFO - __main__ - Global step 2900 Train loss 0.47 Classification-F1 0.21230158730158732 on epoch=120
05/29/2022 18:55:15 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.49 on epoch=121
05/29/2022 18:55:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.43 on epoch=121
05/29/2022 18:55:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.46 on epoch=122
05/29/2022 18:55:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.44 on epoch=122
05/29/2022 18:55:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.49 on epoch=122
05/29/2022 18:55:38 - INFO - __main__ - Global step 2950 Train loss 0.46 Classification-F1 0.18912661720961907 on epoch=122
05/29/2022 18:55:40 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.46 on epoch=123
05/29/2022 18:55:43 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.47 on epoch=123
05/29/2022 18:55:45 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.45 on epoch=124
05/29/2022 18:55:48 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.43 on epoch=124
05/29/2022 18:55:51 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.45 on epoch=124
05/29/2022 18:55:52 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 18:55:52 - INFO - __main__ - Printing 3 examples
05/29/2022 18:55:52 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/29/2022 18:55:52 - INFO - __main__ - ['neutral']
05/29/2022 18:55:52 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/29/2022 18:55:52 - INFO - __main__ - ['neutral']
05/29/2022 18:55:52 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/29/2022 18:55:52 - INFO - __main__ - ['neutral']
05/29/2022 18:55:52 - INFO - __main__ - Tokenizing Input ...
05/29/2022 18:55:52 - INFO - __main__ - Tokenizing Output ...
05/29/2022 18:55:52 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 18:55:52 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 18:55:52 - INFO - __main__ - Printing 3 examples
05/29/2022 18:55:52 - INFO - __main__ -  [anli] premise: Arlette Roxburgh is a Trinidadian American singer and songwriter. She was born in Trinidad. She is best known for singing The Star-Spangled Banner before every New Jersey Devils home game started. When the Nets were in New Jersey, she also sang the national anthem before their home games at the time as well. [SEP] hypothesis: Arlette Roxburgh has only ever sung for the New Jersey Devils.
05/29/2022 18:55:52 - INFO - __main__ - ['neutral']
05/29/2022 18:55:52 - INFO - __main__ -  [anli] premise: Storm Keating (born 27 October 1981) is an Australian-born fashion designer, brand ambassador, producer-director and blogger, now based in London. Keating has worked on a number of Australian and British television programmes such as "The Apprentice Australia", "Masterchef Australia", "The X Factor", "The Voice Australia", and "The Voice UK". She is the wife of Ronan Keating. [SEP] hypothesis: Storm Keaton learned about fashion while studying abroad in the United States.
05/29/2022 18:55:52 - INFO - __main__ - ['neutral']
05/29/2022 18:55:52 - INFO - __main__ -  [anli] premise: Piton is a Pilsner beer brand from the island of Saint Lucia, brewed by Windward & Leeward Brewing Limited, which is owned by Heineken. The beer was named for the Gros Piton and Petit Piton mountains on the island. It was first brewed on October 7, 1992. [SEP] hypothesis: Heineken plans to stop production of Piton in the near future
05/29/2022 18:55:52 - INFO - __main__ - ['neutral']
05/29/2022 18:55:52 - INFO - __main__ - Tokenizing Input ...
05/29/2022 18:55:53 - INFO - __main__ - Tokenizing Output ...
05/29/2022 18:55:53 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 18:56:02 - INFO - __main__ - Global step 3000 Train loss 0.45 Classification-F1 0.18137254901960784 on epoch=124
05/29/2022 18:56:02 - INFO - __main__ - save last model!
05/29/2022 18:56:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 18:56:03 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 18:56:03 - INFO - __main__ - Printing 3 examples
05/29/2022 18:56:03 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 18:56:03 - INFO - __main__ - ['contradiction']
05/29/2022 18:56:03 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 18:56:03 - INFO - __main__ - ['entailment']
05/29/2022 18:56:03 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 18:56:03 - INFO - __main__ - ['contradiction']
05/29/2022 18:56:03 - INFO - __main__ - Tokenizing Input ...
05/29/2022 18:56:03 - INFO - __main__ - Tokenizing Output ...
05/29/2022 18:56:04 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 18:56:09 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 18:56:09 - INFO - __main__ - task name: anli
05/29/2022 18:56:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 18:56:10 - INFO - __main__ - Starting training!
05/29/2022 18:56:35 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_42_0.4_8_predictions.txt
05/29/2022 18:56:35 - INFO - __main__ - Classification-F1 on test data: 0.1770
05/29/2022 18:56:36 - INFO - __main__ - prefix=anli_128_42, lr=0.4, bsz=8, dev_performance=0.29878045076240817, test_performance=0.17695628593122895
05/29/2022 18:56:36 - INFO - __main__ - Running ... prefix=anli_128_42, lr=0.3, bsz=8 ...
05/29/2022 18:56:37 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 18:56:37 - INFO - __main__ - Printing 3 examples
05/29/2022 18:56:37 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/29/2022 18:56:37 - INFO - __main__ - ['neutral']
05/29/2022 18:56:37 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/29/2022 18:56:37 - INFO - __main__ - ['neutral']
05/29/2022 18:56:37 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/29/2022 18:56:37 - INFO - __main__ - ['neutral']
05/29/2022 18:56:37 - INFO - __main__ - Tokenizing Input ...
05/29/2022 18:56:37 - INFO - __main__ - Tokenizing Output ...
05/29/2022 18:56:37 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 18:56:37 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 18:56:37 - INFO - __main__ - Printing 3 examples
05/29/2022 18:56:37 - INFO - __main__ -  [anli] premise: Arlette Roxburgh is a Trinidadian American singer and songwriter. She was born in Trinidad. She is best known for singing The Star-Spangled Banner before every New Jersey Devils home game started. When the Nets were in New Jersey, she also sang the national anthem before their home games at the time as well. [SEP] hypothesis: Arlette Roxburgh has only ever sung for the New Jersey Devils.
05/29/2022 18:56:37 - INFO - __main__ - ['neutral']
05/29/2022 18:56:37 - INFO - __main__ -  [anli] premise: Storm Keating (born 27 October 1981) is an Australian-born fashion designer, brand ambassador, producer-director and blogger, now based in London. Keating has worked on a number of Australian and British television programmes such as "The Apprentice Australia", "Masterchef Australia", "The X Factor", "The Voice Australia", and "The Voice UK". She is the wife of Ronan Keating. [SEP] hypothesis: Storm Keaton learned about fashion while studying abroad in the United States.
05/29/2022 18:56:37 - INFO - __main__ - ['neutral']
05/29/2022 18:56:37 - INFO - __main__ -  [anli] premise: Piton is a Pilsner beer brand from the island of Saint Lucia, brewed by Windward & Leeward Brewing Limited, which is owned by Heineken. The beer was named for the Gros Piton and Petit Piton mountains on the island. It was first brewed on October 7, 1992. [SEP] hypothesis: Heineken plans to stop production of Piton in the near future
05/29/2022 18:56:37 - INFO - __main__ - ['neutral']
05/29/2022 18:56:37 - INFO - __main__ - Tokenizing Input ...
05/29/2022 18:56:37 - INFO - __main__ - Tokenizing Output ...
05/29/2022 18:56:38 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 18:56:56 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 18:56:56 - INFO - __main__ - task name: anli
05/29/2022 18:56:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 18:56:57 - INFO - __main__ - Starting training!
05/29/2022 18:57:00 - INFO - __main__ - Step 10 Global step 10 Train loss 6.82 on epoch=0
05/29/2022 18:57:03 - INFO - __main__ - Step 20 Global step 20 Train loss 3.31 on epoch=0
05/29/2022 18:57:06 - INFO - __main__ - Step 30 Global step 30 Train loss 1.51 on epoch=1
05/29/2022 18:57:08 - INFO - __main__ - Step 40 Global step 40 Train loss 1.03 on epoch=1
05/29/2022 18:57:11 - INFO - __main__ - Step 50 Global step 50 Train loss 0.85 on epoch=2
05/29/2022 18:57:19 - INFO - __main__ - Global step 50 Train loss 2.70 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 18:57:19 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 18:57:21 - INFO - __main__ - Step 60 Global step 60 Train loss 0.70 on epoch=2
05/29/2022 18:57:24 - INFO - __main__ - Step 70 Global step 70 Train loss 0.75 on epoch=2
05/29/2022 18:57:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.75 on epoch=3
05/29/2022 18:57:29 - INFO - __main__ - Step 90 Global step 90 Train loss 0.68 on epoch=3
05/29/2022 18:57:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.69 on epoch=4
05/29/2022 18:57:40 - INFO - __main__ - Global step 100 Train loss 0.72 Classification-F1 0.1733228534847968 on epoch=4
05/29/2022 18:57:40 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1733228534847968 on epoch=4, global_step=100
05/29/2022 18:57:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.60 on epoch=4
05/29/2022 18:57:45 - INFO - __main__ - Step 120 Global step 120 Train loss 0.63 on epoch=4
05/29/2022 18:57:48 - INFO - __main__ - Step 130 Global step 130 Train loss 0.61 on epoch=5
05/29/2022 18:57:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.64 on epoch=5
05/29/2022 18:57:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.58 on epoch=6
05/29/2022 18:58:03 - INFO - __main__ - Global step 150 Train loss 0.61 Classification-F1 0.16666666666666666 on epoch=6
05/29/2022 18:58:05 - INFO - __main__ - Step 160 Global step 160 Train loss 1.25 on epoch=6
05/29/2022 18:58:08 - INFO - __main__ - Step 170 Global step 170 Train loss 2.57 on epoch=7
05/29/2022 18:58:11 - INFO - __main__ - Step 180 Global step 180 Train loss 2.16 on epoch=7
05/29/2022 18:58:13 - INFO - __main__ - Step 190 Global step 190 Train loss 1.77 on epoch=7
05/29/2022 18:58:16 - INFO - __main__ - Step 200 Global step 200 Train loss 1.02 on epoch=8
05/29/2022 18:58:27 - INFO - __main__ - Global step 200 Train loss 1.75 Classification-F1 0.17487087206633797 on epoch=8
05/29/2022 18:58:27 - INFO - __main__ - Saving model with best Classification-F1: 0.1733228534847968 -> 0.17487087206633797 on epoch=8, global_step=200
05/29/2022 18:58:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.70 on epoch=8
05/29/2022 18:58:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.62 on epoch=9
05/29/2022 18:58:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.95 on epoch=9
05/29/2022 18:58:37 - INFO - __main__ - Step 240 Global step 240 Train loss 1.51 on epoch=9
05/29/2022 18:58:40 - INFO - __main__ - Step 250 Global step 250 Train loss 3.42 on epoch=10
05/29/2022 19:00:24 - INFO - __main__ - Global step 250 Train loss 1.44 Classification-F1 0.0018500681997654887 on epoch=10
05/29/2022 19:00:27 - INFO - __main__ - Step 260 Global step 260 Train loss 6.25 on epoch=10
05/29/2022 19:00:29 - INFO - __main__ - Step 270 Global step 270 Train loss 5.18 on epoch=11
05/29/2022 19:00:32 - INFO - __main__ - Step 280 Global step 280 Train loss 5.46 on epoch=11
05/29/2022 19:00:34 - INFO - __main__ - Step 290 Global step 290 Train loss 3.76 on epoch=12
05/29/2022 19:00:37 - INFO - __main__ - Step 300 Global step 300 Train loss 2.02 on epoch=12
05/29/2022 19:00:48 - INFO - __main__ - Global step 300 Train loss 4.53 Classification-F1 0.1721607831834019 on epoch=12
05/29/2022 19:00:50 - INFO - __main__ - Step 310 Global step 310 Train loss 1.05 on epoch=12
05/29/2022 19:00:53 - INFO - __main__ - Step 320 Global step 320 Train loss 1.34 on epoch=13
05/29/2022 19:00:55 - INFO - __main__ - Step 330 Global step 330 Train loss 1.60 on epoch=13
05/29/2022 19:00:58 - INFO - __main__ - Step 340 Global step 340 Train loss 1.58 on epoch=14
05/29/2022 19:01:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.78 on epoch=14
05/29/2022 19:01:09 - INFO - __main__ - Global step 350 Train loss 1.27 Classification-F1 0.21298037849761986 on epoch=14
05/29/2022 19:01:09 - INFO - __main__ - Saving model with best Classification-F1: 0.17487087206633797 -> 0.21298037849761986 on epoch=14, global_step=350
05/29/2022 19:01:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.72 on epoch=14
05/29/2022 19:01:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.76 on epoch=15
05/29/2022 19:01:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.74 on epoch=15
05/29/2022 19:01:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.72 on epoch=16
05/29/2022 19:01:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.56 on epoch=16
05/29/2022 19:01:30 - INFO - __main__ - Global step 400 Train loss 0.70 Classification-F1 0.23763983763983762 on epoch=16
05/29/2022 19:01:30 - INFO - __main__ - Saving model with best Classification-F1: 0.21298037849761986 -> 0.23763983763983762 on epoch=16, global_step=400
05/29/2022 19:01:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.78 on epoch=17
05/29/2022 19:01:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.66 on epoch=17
05/29/2022 19:01:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.64 on epoch=17
05/29/2022 19:01:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.78 on epoch=18
05/29/2022 19:01:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.56 on epoch=18
05/29/2022 19:01:51 - INFO - __main__ - Global step 450 Train loss 0.68 Classification-F1 0.24150992685475445 on epoch=18
05/29/2022 19:01:51 - INFO - __main__ - Saving model with best Classification-F1: 0.23763983763983762 -> 0.24150992685475445 on epoch=18, global_step=450
05/29/2022 19:01:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.62 on epoch=19
05/29/2022 19:01:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.60 on epoch=19
05/29/2022 19:01:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.74 on epoch=19
05/29/2022 19:02:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.57 on epoch=20
05/29/2022 19:02:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.68 on epoch=20
05/29/2022 19:02:12 - INFO - __main__ - Global step 500 Train loss 0.64 Classification-F1 0.25303466691542575 on epoch=20
05/29/2022 19:02:12 - INFO - __main__ - Saving model with best Classification-F1: 0.24150992685475445 -> 0.25303466691542575 on epoch=20, global_step=500
05/29/2022 19:02:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.70 on epoch=21
05/29/2022 19:02:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.65 on epoch=21
05/29/2022 19:02:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.59 on epoch=22
05/29/2022 19:02:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.62 on epoch=22
05/29/2022 19:02:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.61 on epoch=22
05/29/2022 19:02:34 - INFO - __main__ - Global step 550 Train loss 0.64 Classification-F1 0.2392831113761346 on epoch=22
05/29/2022 19:02:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.60 on epoch=23
05/29/2022 19:02:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.72 on epoch=23
05/29/2022 19:02:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.66 on epoch=24
05/29/2022 19:02:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.57 on epoch=24
05/29/2022 19:02:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.63 on epoch=24
05/29/2022 19:02:57 - INFO - __main__ - Global step 600 Train loss 0.64 Classification-F1 0.2182795698924731 on epoch=24
05/29/2022 19:03:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.58 on epoch=25
05/29/2022 19:03:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.62 on epoch=25
05/29/2022 19:03:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.63 on epoch=26
05/29/2022 19:03:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.61 on epoch=26
05/29/2022 19:03:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.61 on epoch=27
05/29/2022 19:03:19 - INFO - __main__ - Global step 650 Train loss 0.61 Classification-F1 0.19886117238908688 on epoch=27
05/29/2022 19:03:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.58 on epoch=27
05/29/2022 19:03:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.56 on epoch=27
05/29/2022 19:03:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.57 on epoch=28
05/29/2022 19:03:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.66 on epoch=28
05/29/2022 19:03:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.61 on epoch=29
05/29/2022 19:03:41 - INFO - __main__ - Global step 700 Train loss 0.59 Classification-F1 0.2162358276643991 on epoch=29
05/29/2022 19:03:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.59 on epoch=29
05/29/2022 19:03:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.55 on epoch=29
05/29/2022 19:03:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.59 on epoch=30
05/29/2022 19:03:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.57 on epoch=30
05/29/2022 19:03:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.58 on epoch=31
05/29/2022 19:04:04 - INFO - __main__ - Global step 750 Train loss 0.58 Classification-F1 0.1721607831834019 on epoch=31
05/29/2022 19:04:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.60 on epoch=31
05/29/2022 19:04:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.64 on epoch=32
05/29/2022 19:04:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.57 on epoch=32
05/29/2022 19:04:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.69 on epoch=32
05/29/2022 19:04:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.60 on epoch=33
05/29/2022 19:04:27 - INFO - __main__ - Global step 800 Train loss 0.62 Classification-F1 0.1717171717171717 on epoch=33
05/29/2022 19:04:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.57 on epoch=33
05/29/2022 19:04:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.57 on epoch=34
05/29/2022 19:04:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.54 on epoch=34
05/29/2022 19:04:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.56 on epoch=34
05/29/2022 19:04:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.59 on epoch=35
05/29/2022 19:04:49 - INFO - __main__ - Global step 850 Train loss 0.57 Classification-F1 0.18115144847818113 on epoch=35
05/29/2022 19:04:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.52 on epoch=35
05/29/2022 19:04:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.57 on epoch=36
05/29/2022 19:04:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.57 on epoch=36
05/29/2022 19:05:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.51 on epoch=37
05/29/2022 19:05:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.55 on epoch=37
05/29/2022 19:05:12 - INFO - __main__ - Global step 900 Train loss 0.54 Classification-F1 0.1717171717171717 on epoch=37
05/29/2022 19:05:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.63 on epoch=37
05/29/2022 19:05:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.56 on epoch=38
05/29/2022 19:05:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.58 on epoch=38
05/29/2022 19:05:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.53 on epoch=39
05/29/2022 19:05:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.54 on epoch=39
05/29/2022 19:05:34 - INFO - __main__ - Global step 950 Train loss 0.57 Classification-F1 0.20489469615551406 on epoch=39
05/29/2022 19:05:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.48 on epoch=39
05/29/2022 19:05:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.58 on epoch=40
05/29/2022 19:05:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.58 on epoch=40
05/29/2022 19:05:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.52 on epoch=41
05/29/2022 19:05:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.53 on epoch=41
05/29/2022 19:05:56 - INFO - __main__ - Global step 1000 Train loss 0.54 Classification-F1 0.17244846656611368 on epoch=41
05/29/2022 19:05:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.60 on epoch=42
05/29/2022 19:06:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.52 on epoch=42
05/29/2022 19:06:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.55 on epoch=42
05/29/2022 19:06:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.59 on epoch=43
05/29/2022 19:06:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.60 on epoch=43
05/29/2022 19:06:18 - INFO - __main__ - Global step 1050 Train loss 0.57 Classification-F1 0.18225080132932178 on epoch=43
05/29/2022 19:06:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.63 on epoch=44
05/29/2022 19:06:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.57 on epoch=44
05/29/2022 19:06:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.55 on epoch=44
05/29/2022 19:06:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.50 on epoch=45
05/29/2022 19:06:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.54 on epoch=45
05/29/2022 19:06:40 - INFO - __main__ - Global step 1100 Train loss 0.56 Classification-F1 0.18225080132932178 on epoch=45
05/29/2022 19:06:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.58 on epoch=46
05/29/2022 19:06:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.53 on epoch=46
05/29/2022 19:06:47 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.61 on epoch=47
05/29/2022 19:06:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.52 on epoch=47
05/29/2022 19:06:53 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.55 on epoch=47
05/29/2022 19:07:01 - INFO - __main__ - Global step 1150 Train loss 0.56 Classification-F1 0.20833333333333334 on epoch=47
05/29/2022 19:07:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.53 on epoch=48
05/29/2022 19:07:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.50 on epoch=48
05/29/2022 19:07:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.55 on epoch=49
05/29/2022 19:07:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.47 on epoch=49
05/29/2022 19:07:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.58 on epoch=49
05/29/2022 19:07:23 - INFO - __main__ - Global step 1200 Train loss 0.53 Classification-F1 0.17142828576713307 on epoch=49
05/29/2022 19:07:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.51 on epoch=50
05/29/2022 19:07:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.57 on epoch=50
05/29/2022 19:07:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.54 on epoch=51
05/29/2022 19:07:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.51 on epoch=51
05/29/2022 19:07:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.59 on epoch=52
05/29/2022 19:07:45 - INFO - __main__ - Global step 1250 Train loss 0.54 Classification-F1 0.203627358556936 on epoch=52
05/29/2022 19:07:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.54 on epoch=52
05/29/2022 19:07:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.49 on epoch=52
05/29/2022 19:07:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.52 on epoch=53
05/29/2022 19:07:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.52 on epoch=53
05/29/2022 19:07:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.52 on epoch=54
05/29/2022 19:08:08 - INFO - __main__ - Global step 1300 Train loss 0.52 Classification-F1 0.19886117238908688 on epoch=54
05/29/2022 19:08:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.54 on epoch=54
05/29/2022 19:08:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.54 on epoch=54
05/29/2022 19:08:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.53 on epoch=55
05/29/2022 19:08:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.65 on epoch=55
05/29/2022 19:08:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.51 on epoch=56
05/29/2022 19:08:30 - INFO - __main__ - Global step 1350 Train loss 0.55 Classification-F1 0.20579266189234316 on epoch=56
05/29/2022 19:08:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.56 on epoch=56
05/29/2022 19:08:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.54 on epoch=57
05/29/2022 19:08:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.49 on epoch=57
05/29/2022 19:08:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.58 on epoch=57
05/29/2022 19:08:43 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.57 on epoch=58
05/29/2022 19:08:52 - INFO - __main__ - Global step 1400 Train loss 0.55 Classification-F1 0.17244846656611368 on epoch=58
05/29/2022 19:08:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.47 on epoch=58
05/29/2022 19:08:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=59
05/29/2022 19:09:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.55 on epoch=59
05/29/2022 19:09:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.51 on epoch=59
05/29/2022 19:09:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.56 on epoch=60
05/29/2022 19:09:15 - INFO - __main__ - Global step 1450 Train loss 0.51 Classification-F1 0.1721607831834019 on epoch=60
05/29/2022 19:09:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.60 on epoch=60
05/29/2022 19:09:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.50 on epoch=61
05/29/2022 19:09:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.52 on epoch=61
05/29/2022 19:09:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.60 on epoch=62
05/29/2022 19:09:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.52 on epoch=62
05/29/2022 19:09:37 - INFO - __main__ - Global step 1500 Train loss 0.55 Classification-F1 0.1721607831834019 on epoch=62
05/29/2022 19:09:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.56 on epoch=62
05/29/2022 19:09:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.51 on epoch=63
05/29/2022 19:09:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.57 on epoch=63
05/29/2022 19:09:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.57 on epoch=64
05/29/2022 19:09:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.46 on epoch=64
05/29/2022 19:09:59 - INFO - __main__ - Global step 1550 Train loss 0.53 Classification-F1 0.20921295737094747 on epoch=64
05/29/2022 19:10:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.52 on epoch=64
05/29/2022 19:10:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.52 on epoch=65
05/29/2022 19:10:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.51 on epoch=65
05/29/2022 19:10:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.46 on epoch=66
05/29/2022 19:10:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.47 on epoch=66
05/29/2022 19:10:22 - INFO - __main__ - Global step 1600 Train loss 0.50 Classification-F1 0.1721607831834019 on epoch=66
05/29/2022 19:10:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.54 on epoch=67
05/29/2022 19:10:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.52 on epoch=67
05/29/2022 19:10:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.51 on epoch=67
05/29/2022 19:10:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.55 on epoch=68
05/29/2022 19:10:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.57 on epoch=68
05/29/2022 19:10:44 - INFO - __main__ - Global step 1650 Train loss 0.54 Classification-F1 0.17702046042127015 on epoch=68
05/29/2022 19:10:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.55 on epoch=69
05/29/2022 19:10:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.49 on epoch=69
05/29/2022 19:10:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.53 on epoch=69
05/29/2022 19:10:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.51 on epoch=70
05/29/2022 19:10:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.57 on epoch=70
05/29/2022 19:11:07 - INFO - __main__ - Global step 1700 Train loss 0.53 Classification-F1 0.2600979544799769 on epoch=70
05/29/2022 19:11:07 - INFO - __main__ - Saving model with best Classification-F1: 0.25303466691542575 -> 0.2600979544799769 on epoch=70, global_step=1700
05/29/2022 19:11:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.54 on epoch=71
05/29/2022 19:11:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.50 on epoch=71
05/29/2022 19:11:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.53 on epoch=72
05/29/2022 19:11:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.51 on epoch=72
05/29/2022 19:11:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.56 on epoch=72
05/29/2022 19:11:30 - INFO - __main__ - Global step 1750 Train loss 0.53 Classification-F1 0.24631224631224632 on epoch=72
05/29/2022 19:11:33 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.55 on epoch=73
05/29/2022 19:11:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.52 on epoch=73
05/29/2022 19:11:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.53 on epoch=74
05/29/2022 19:11:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.49 on epoch=74
05/29/2022 19:11:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.53 on epoch=74
05/29/2022 19:11:52 - INFO - __main__ - Global step 1800 Train loss 0.52 Classification-F1 0.1717171717171717 on epoch=74
05/29/2022 19:11:55 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.51 on epoch=75
05/29/2022 19:11:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.55 on epoch=75
05/29/2022 19:12:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.55 on epoch=76
05/29/2022 19:12:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.50 on epoch=76
05/29/2022 19:12:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.49 on epoch=77
05/29/2022 19:12:15 - INFO - __main__ - Global step 1850 Train loss 0.52 Classification-F1 0.1721607831834019 on epoch=77
05/29/2022 19:12:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.46 on epoch=77
05/29/2022 19:12:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.50 on epoch=77
05/29/2022 19:12:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.50 on epoch=78
05/29/2022 19:12:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.53 on epoch=78
05/29/2022 19:12:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.54 on epoch=79
05/29/2022 19:12:38 - INFO - __main__ - Global step 1900 Train loss 0.51 Classification-F1 0.27447694345041523 on epoch=79
05/29/2022 19:12:38 - INFO - __main__ - Saving model with best Classification-F1: 0.2600979544799769 -> 0.27447694345041523 on epoch=79, global_step=1900
05/29/2022 19:12:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.54 on epoch=79
05/29/2022 19:12:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.48 on epoch=79
05/29/2022 19:12:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.50 on epoch=80
05/29/2022 19:12:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.55 on epoch=80
05/29/2022 19:12:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.59 on epoch=81
05/29/2022 19:13:00 - INFO - __main__ - Global step 1950 Train loss 0.53 Classification-F1 0.1721607831834019 on epoch=81
05/29/2022 19:13:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.51 on epoch=81
05/29/2022 19:13:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.50 on epoch=82
05/29/2022 19:13:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.50 on epoch=82
05/29/2022 19:13:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.46 on epoch=82
05/29/2022 19:13:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.50 on epoch=83
05/29/2022 19:13:23 - INFO - __main__ - Global step 2000 Train loss 0.49 Classification-F1 0.21891975180876946 on epoch=83
05/29/2022 19:13:25 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.50 on epoch=83
05/29/2022 19:13:28 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.48 on epoch=84
05/29/2022 19:13:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.55 on epoch=84
05/29/2022 19:13:33 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.51 on epoch=84
05/29/2022 19:13:36 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.48 on epoch=85
05/29/2022 19:13:45 - INFO - __main__ - Global step 2050 Train loss 0.50 Classification-F1 0.24970187660829724 on epoch=85
05/29/2022 19:13:48 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.55 on epoch=85
05/29/2022 19:13:51 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.52 on epoch=86
05/29/2022 19:13:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.48 on epoch=86
05/29/2022 19:13:56 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.52 on epoch=87
05/29/2022 19:13:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.49 on epoch=87
05/29/2022 19:14:08 - INFO - __main__ - Global step 2100 Train loss 0.51 Classification-F1 0.18625407514296402 on epoch=87
05/29/2022 19:14:11 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.60 on epoch=87
05/29/2022 19:14:13 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.49 on epoch=88
05/29/2022 19:14:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.45 on epoch=88
05/29/2022 19:14:18 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.51 on epoch=89
05/29/2022 19:14:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.52 on epoch=89
05/29/2022 19:14:31 - INFO - __main__ - Global step 2150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=89
05/29/2022 19:14:33 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.50 on epoch=89
05/29/2022 19:14:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.47 on epoch=90
05/29/2022 19:14:38 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.54 on epoch=90
05/29/2022 19:14:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.47 on epoch=91
05/29/2022 19:14:44 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.51 on epoch=91
05/29/2022 19:14:53 - INFO - __main__ - Global step 2200 Train loss 0.50 Classification-F1 0.1721607831834019 on epoch=91
05/29/2022 19:14:56 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.50 on epoch=92
05/29/2022 19:14:59 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.48 on epoch=92
05/29/2022 19:15:01 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.50 on epoch=92
05/29/2022 19:15:04 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.54 on epoch=93
05/29/2022 19:15:07 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.46 on epoch=93
05/29/2022 19:15:16 - INFO - __main__ - Global step 2250 Train loss 0.50 Classification-F1 0.25926086642077095 on epoch=93
05/29/2022 19:15:19 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.51 on epoch=94
05/29/2022 19:15:22 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.54 on epoch=94
05/29/2022 19:15:24 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.53 on epoch=94
05/29/2022 19:15:27 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.52 on epoch=95
05/29/2022 19:15:30 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.53 on epoch=95
05/29/2022 19:15:39 - INFO - __main__ - Global step 2300 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=95
05/29/2022 19:15:42 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.53 on epoch=96
05/29/2022 19:15:45 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.52 on epoch=96
05/29/2022 19:15:47 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.55 on epoch=97
05/29/2022 19:15:50 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.46 on epoch=97
05/29/2022 19:15:53 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.55 on epoch=97
05/29/2022 19:16:02 - INFO - __main__ - Global step 2350 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=97
05/29/2022 19:16:05 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.49 on epoch=98
05/29/2022 19:16:07 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.53 on epoch=98
05/29/2022 19:16:10 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.53 on epoch=99
05/29/2022 19:16:13 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.46 on epoch=99
05/29/2022 19:16:15 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.50 on epoch=99
05/29/2022 19:16:25 - INFO - __main__ - Global step 2400 Train loss 0.50 Classification-F1 0.21388558665657986 on epoch=99
05/29/2022 19:16:28 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.52 on epoch=100
05/29/2022 19:16:30 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.50 on epoch=100
05/29/2022 19:16:33 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.50 on epoch=101
05/29/2022 19:16:36 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.49 on epoch=101
05/29/2022 19:16:38 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.50 on epoch=102
05/29/2022 19:16:48 - INFO - __main__ - Global step 2450 Train loss 0.50 Classification-F1 0.1721607831834019 on epoch=102
05/29/2022 19:16:51 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.43 on epoch=102
05/29/2022 19:16:54 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.48 on epoch=102
05/29/2022 19:16:56 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.47 on epoch=103
05/29/2022 19:16:59 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.52 on epoch=103
05/29/2022 19:17:02 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.51 on epoch=104
05/29/2022 19:17:12 - INFO - __main__ - Global step 2500 Train loss 0.49 Classification-F1 0.17692307692307693 on epoch=104
05/29/2022 19:17:14 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.46 on epoch=104
05/29/2022 19:17:17 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.54 on epoch=104
05/29/2022 19:17:20 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.48 on epoch=105
05/29/2022 19:17:22 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.49 on epoch=105
05/29/2022 19:17:25 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.51 on epoch=106
05/29/2022 19:17:36 - INFO - __main__ - Global step 2550 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=106
05/29/2022 19:17:38 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.49 on epoch=106
05/29/2022 19:17:41 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.49 on epoch=107
05/29/2022 19:17:44 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.53 on epoch=107
05/29/2022 19:17:46 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.53 on epoch=107
05/29/2022 19:17:49 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.49 on epoch=108
05/29/2022 19:17:58 - INFO - __main__ - Global step 2600 Train loss 0.51 Classification-F1 0.24361110166706335 on epoch=108
05/29/2022 19:18:01 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.48 on epoch=108
05/29/2022 19:18:04 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.47 on epoch=109
05/29/2022 19:18:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.46 on epoch=109
05/29/2022 19:18:09 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.48 on epoch=109
05/29/2022 19:18:12 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.46 on epoch=110
05/29/2022 19:18:21 - INFO - __main__ - Global step 2650 Train loss 0.47 Classification-F1 0.1721607831834019 on epoch=110
05/29/2022 19:18:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.49 on epoch=110
05/29/2022 19:18:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.54 on epoch=111
05/29/2022 19:18:29 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.46 on epoch=111
05/29/2022 19:18:32 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.48 on epoch=112
05/29/2022 19:18:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.41 on epoch=112
05/29/2022 19:18:44 - INFO - __main__ - Global step 2700 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=112
05/29/2022 19:18:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.52 on epoch=112
05/29/2022 19:18:49 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.55 on epoch=113
05/29/2022 19:18:52 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.50 on epoch=113
05/29/2022 19:18:55 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.50 on epoch=114
05/29/2022 19:18:57 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.45 on epoch=114
05/29/2022 19:19:07 - INFO - __main__ - Global step 2750 Train loss 0.50 Classification-F1 0.2490055417672458 on epoch=114
05/29/2022 19:19:09 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.49 on epoch=114
05/29/2022 19:19:12 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.45 on epoch=115
05/29/2022 19:19:15 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.46 on epoch=115
05/29/2022 19:19:17 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.50 on epoch=116
05/29/2022 19:19:20 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.52 on epoch=116
05/29/2022 19:19:30 - INFO - __main__ - Global step 2800 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=116
05/29/2022 19:19:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.48 on epoch=117
05/29/2022 19:19:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.50 on epoch=117
05/29/2022 19:19:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.53 on epoch=117
05/29/2022 19:19:41 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.49 on epoch=118
05/29/2022 19:19:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.48 on epoch=118
05/29/2022 19:19:53 - INFO - __main__ - Global step 2850 Train loss 0.50 Classification-F1 0.209849457024189 on epoch=118
05/29/2022 19:19:55 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.50 on epoch=119
05/29/2022 19:19:58 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.49 on epoch=119
05/29/2022 19:20:01 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.49 on epoch=119
05/29/2022 19:20:03 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.50 on epoch=120
05/29/2022 19:20:06 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.56 on epoch=120
05/29/2022 19:20:16 - INFO - __main__ - Global step 2900 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=120
05/29/2022 19:20:18 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.49 on epoch=121
05/29/2022 19:20:21 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.45 on epoch=121
05/29/2022 19:20:24 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.49 on epoch=122
05/29/2022 19:20:26 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.50 on epoch=122
05/29/2022 19:20:29 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.47 on epoch=122
05/29/2022 19:20:39 - INFO - __main__ - Global step 2950 Train loss 0.48 Classification-F1 0.23286423535784473 on epoch=122
05/29/2022 19:20:41 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.51 on epoch=123
05/29/2022 19:20:44 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.50 on epoch=123
05/29/2022 19:20:47 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.49 on epoch=124
05/29/2022 19:20:49 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.51 on epoch=124
05/29/2022 19:20:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.52 on epoch=124
05/29/2022 19:20:53 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 19:20:53 - INFO - __main__ - Printing 3 examples
05/29/2022 19:20:53 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/29/2022 19:20:53 - INFO - __main__ - ['neutral']
05/29/2022 19:20:53 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/29/2022 19:20:53 - INFO - __main__ - ['neutral']
05/29/2022 19:20:53 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/29/2022 19:20:53 - INFO - __main__ - ['neutral']
05/29/2022 19:20:53 - INFO - __main__ - Tokenizing Input ...
05/29/2022 19:20:54 - INFO - __main__ - Tokenizing Output ...
05/29/2022 19:20:54 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 19:20:54 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 19:20:54 - INFO - __main__ - Printing 3 examples
05/29/2022 19:20:54 - INFO - __main__ -  [anli] premise: Arlette Roxburgh is a Trinidadian American singer and songwriter. She was born in Trinidad. She is best known for singing The Star-Spangled Banner before every New Jersey Devils home game started. When the Nets were in New Jersey, she also sang the national anthem before their home games at the time as well. [SEP] hypothesis: Arlette Roxburgh has only ever sung for the New Jersey Devils.
05/29/2022 19:20:54 - INFO - __main__ - ['neutral']
05/29/2022 19:20:54 - INFO - __main__ -  [anli] premise: Storm Keating (born 27 October 1981) is an Australian-born fashion designer, brand ambassador, producer-director and blogger, now based in London. Keating has worked on a number of Australian and British television programmes such as "The Apprentice Australia", "Masterchef Australia", "The X Factor", "The Voice Australia", and "The Voice UK". She is the wife of Ronan Keating. [SEP] hypothesis: Storm Keaton learned about fashion while studying abroad in the United States.
05/29/2022 19:20:54 - INFO - __main__ - ['neutral']
05/29/2022 19:20:54 - INFO - __main__ -  [anli] premise: Piton is a Pilsner beer brand from the island of Saint Lucia, brewed by Windward & Leeward Brewing Limited, which is owned by Heineken. The beer was named for the Gros Piton and Petit Piton mountains on the island. It was first brewed on October 7, 1992. [SEP] hypothesis: Heineken plans to stop production of Piton in the near future
05/29/2022 19:20:54 - INFO - __main__ - ['neutral']
05/29/2022 19:20:54 - INFO - __main__ - Tokenizing Input ...
05/29/2022 19:20:54 - INFO - __main__ - Tokenizing Output ...
05/29/2022 19:20:55 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 19:21:02 - INFO - __main__ - Global step 3000 Train loss 0.51 Classification-F1 0.17692307692307688 on epoch=124
05/29/2022 19:21:02 - INFO - __main__ - save last model!
05/29/2022 19:21:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 19:21:02 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 19:21:02 - INFO - __main__ - Printing 3 examples
05/29/2022 19:21:02 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 19:21:02 - INFO - __main__ - ['contradiction']
05/29/2022 19:21:02 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 19:21:02 - INFO - __main__ - ['entailment']
05/29/2022 19:21:02 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 19:21:02 - INFO - __main__ - ['contradiction']
05/29/2022 19:21:02 - INFO - __main__ - Tokenizing Input ...
05/29/2022 19:21:02 - INFO - __main__ - Tokenizing Output ...
05/29/2022 19:21:03 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 19:21:13 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 19:21:13 - INFO - __main__ - task name: anli
05/29/2022 19:21:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 19:21:14 - INFO - __main__ - Starting training!
05/29/2022 19:21:29 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_42_0.3_8_predictions.txt
05/29/2022 19:21:29 - INFO - __main__ - Classification-F1 on test data: 0.1726
05/29/2022 19:21:29 - INFO - __main__ - prefix=anli_128_42, lr=0.3, bsz=8, dev_performance=0.27447694345041523, test_performance=0.17257804271012958
05/29/2022 19:21:29 - INFO - __main__ - Running ... prefix=anli_128_42, lr=0.2, bsz=8 ...
05/29/2022 19:21:30 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 19:21:30 - INFO - __main__ - Printing 3 examples
05/29/2022 19:21:30 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/29/2022 19:21:30 - INFO - __main__ - ['neutral']
05/29/2022 19:21:30 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/29/2022 19:21:30 - INFO - __main__ - ['neutral']
05/29/2022 19:21:30 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/29/2022 19:21:30 - INFO - __main__ - ['neutral']
05/29/2022 19:21:30 - INFO - __main__ - Tokenizing Input ...
05/29/2022 19:21:30 - INFO - __main__ - Tokenizing Output ...
05/29/2022 19:21:31 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 19:21:31 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 19:21:31 - INFO - __main__ - Printing 3 examples
05/29/2022 19:21:31 - INFO - __main__ -  [anli] premise: Arlette Roxburgh is a Trinidadian American singer and songwriter. She was born in Trinidad. She is best known for singing The Star-Spangled Banner before every New Jersey Devils home game started. When the Nets were in New Jersey, she also sang the national anthem before their home games at the time as well. [SEP] hypothesis: Arlette Roxburgh has only ever sung for the New Jersey Devils.
05/29/2022 19:21:31 - INFO - __main__ - ['neutral']
05/29/2022 19:21:31 - INFO - __main__ -  [anli] premise: Storm Keating (born 27 October 1981) is an Australian-born fashion designer, brand ambassador, producer-director and blogger, now based in London. Keating has worked on a number of Australian and British television programmes such as "The Apprentice Australia", "Masterchef Australia", "The X Factor", "The Voice Australia", and "The Voice UK". She is the wife of Ronan Keating. [SEP] hypothesis: Storm Keaton learned about fashion while studying abroad in the United States.
05/29/2022 19:21:31 - INFO - __main__ - ['neutral']
05/29/2022 19:21:31 - INFO - __main__ -  [anli] premise: Piton is a Pilsner beer brand from the island of Saint Lucia, brewed by Windward & Leeward Brewing Limited, which is owned by Heineken. The beer was named for the Gros Piton and Petit Piton mountains on the island. It was first brewed on October 7, 1992. [SEP] hypothesis: Heineken plans to stop production of Piton in the near future
05/29/2022 19:21:31 - INFO - __main__ - ['neutral']
05/29/2022 19:21:31 - INFO - __main__ - Tokenizing Input ...
05/29/2022 19:21:31 - INFO - __main__ - Tokenizing Output ...
05/29/2022 19:21:31 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 19:21:47 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 19:21:47 - INFO - __main__ - task name: anli
05/29/2022 19:21:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 19:21:48 - INFO - __main__ - Starting training!
05/29/2022 19:21:51 - INFO - __main__ - Step 10 Global step 10 Train loss 7.39 on epoch=0
05/29/2022 19:21:54 - INFO - __main__ - Step 20 Global step 20 Train loss 5.07 on epoch=0
05/29/2022 19:21:57 - INFO - __main__ - Step 30 Global step 30 Train loss 2.89 on epoch=1
05/29/2022 19:21:59 - INFO - __main__ - Step 40 Global step 40 Train loss 1.74 on epoch=1
05/29/2022 19:22:02 - INFO - __main__ - Step 50 Global step 50 Train loss 1.48 on epoch=2
05/29/2022 19:22:10 - INFO - __main__ - Global step 50 Train loss 3.71 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 19:22:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 19:22:13 - INFO - __main__ - Step 60 Global step 60 Train loss 1.14 on epoch=2
05/29/2022 19:22:15 - INFO - __main__ - Step 70 Global step 70 Train loss 1.18 on epoch=2
05/29/2022 19:22:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.92 on epoch=3
05/29/2022 19:22:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.85 on epoch=3
05/29/2022 19:22:23 - INFO - __main__ - Step 100 Global step 100 Train loss 0.76 on epoch=4
05/29/2022 19:22:31 - INFO - __main__ - Global step 100 Train loss 0.97 Classification-F1 0.18338383185667037 on epoch=4
05/29/2022 19:22:31 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.18338383185667037 on epoch=4, global_step=100
05/29/2022 19:22:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.84 on epoch=4
05/29/2022 19:22:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.71 on epoch=4
05/29/2022 19:22:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.74 on epoch=5
05/29/2022 19:22:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.70 on epoch=5
05/29/2022 19:22:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.74 on epoch=6
05/29/2022 19:22:52 - INFO - __main__ - Global step 150 Train loss 0.75 Classification-F1 0.16666666666666666 on epoch=6
05/29/2022 19:22:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.66 on epoch=6
05/29/2022 19:22:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.72 on epoch=7
05/29/2022 19:23:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.68 on epoch=7
05/29/2022 19:23:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.64 on epoch=7
05/29/2022 19:23:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.59 on epoch=8
05/29/2022 19:23:13 - INFO - __main__ - Global step 200 Train loss 0.66 Classification-F1 0.16666666666666666 on epoch=8
05/29/2022 19:23:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.55 on epoch=8
05/29/2022 19:23:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.58 on epoch=9
05/29/2022 19:23:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.55 on epoch=9
05/29/2022 19:23:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.57 on epoch=9
05/29/2022 19:23:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.58 on epoch=10
05/29/2022 19:23:36 - INFO - __main__ - Global step 250 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=10
05/29/2022 19:23:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.61 on epoch=10
05/29/2022 19:23:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.55 on epoch=11
05/29/2022 19:23:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.57 on epoch=11
05/29/2022 19:23:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=12
05/29/2022 19:23:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.55 on epoch=12
05/29/2022 19:23:58 - INFO - __main__ - Global step 300 Train loss 0.56 Classification-F1 0.1717171717171717 on epoch=12
05/29/2022 19:24:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.63 on epoch=12
05/29/2022 19:24:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=13
05/29/2022 19:24:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.56 on epoch=13
05/29/2022 19:24:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.56 on epoch=14
05/29/2022 19:24:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=14
05/29/2022 19:24:20 - INFO - __main__ - Global step 350 Train loss 0.54 Classification-F1 0.1721607831834019 on epoch=14
05/29/2022 19:24:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.56 on epoch=14
05/29/2022 19:24:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.64 on epoch=15
05/29/2022 19:24:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=15
05/29/2022 19:24:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.53 on epoch=16
05/29/2022 19:24:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.53 on epoch=16
05/29/2022 19:24:42 - INFO - __main__ - Global step 400 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=16
05/29/2022 19:24:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.55 on epoch=17
05/29/2022 19:24:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.54 on epoch=17
05/29/2022 19:24:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.56 on epoch=17
05/29/2022 19:24:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.56 on epoch=18
05/29/2022 19:24:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.54 on epoch=18
05/29/2022 19:25:06 - INFO - __main__ - Global step 450 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=18
05/29/2022 19:25:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=19
05/29/2022 19:25:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.52 on epoch=19
05/29/2022 19:25:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.55 on epoch=19
05/29/2022 19:25:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.56 on epoch=20
05/29/2022 19:25:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.52 on epoch=20
05/29/2022 19:25:30 - INFO - __main__ - Global step 500 Train loss 0.53 Classification-F1 0.17040483575916646 on epoch=20
05/29/2022 19:25:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=21
05/29/2022 19:25:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.56 on epoch=21
05/29/2022 19:25:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.54 on epoch=22
05/29/2022 19:25:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.51 on epoch=22
05/29/2022 19:25:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.54 on epoch=22
05/29/2022 19:25:55 - INFO - __main__ - Global step 550 Train loss 0.53 Classification-F1 0.2424684320418918 on epoch=22
05/29/2022 19:25:55 - INFO - __main__ - Saving model with best Classification-F1: 0.18338383185667037 -> 0.2424684320418918 on epoch=22, global_step=550
05/29/2022 19:25:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.50 on epoch=23
05/29/2022 19:26:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.56 on epoch=23
05/29/2022 19:26:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=24
05/29/2022 19:26:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=24
05/29/2022 19:26:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.52 on epoch=24
05/29/2022 19:26:19 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.24910865971996307 on epoch=24
05/29/2022 19:26:19 - INFO - __main__ - Saving model with best Classification-F1: 0.2424684320418918 -> 0.24910865971996307 on epoch=24, global_step=600
05/29/2022 19:26:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=25
05/29/2022 19:26:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.58 on epoch=25
05/29/2022 19:26:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.56 on epoch=26
05/29/2022 19:26:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=26
05/29/2022 19:26:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.54 on epoch=27
05/29/2022 19:26:43 - INFO - __main__ - Global step 650 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=27
05/29/2022 19:26:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=27
05/29/2022 19:26:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.49 on epoch=27
05/29/2022 19:26:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=28
05/29/2022 19:26:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.51 on epoch=28
05/29/2022 19:26:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=29
05/29/2022 19:27:07 - INFO - __main__ - Global step 700 Train loss 0.49 Classification-F1 0.16699282452707112 on epoch=29
05/29/2022 19:27:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.50 on epoch=29
05/29/2022 19:27:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=29
05/29/2022 19:27:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.53 on epoch=30
05/29/2022 19:27:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.55 on epoch=30
05/29/2022 19:27:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=31
05/29/2022 19:27:31 - INFO - __main__ - Global step 750 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=31
05/29/2022 19:27:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=31
05/29/2022 19:27:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=32
05/29/2022 19:27:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=32
05/29/2022 19:27:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=32
05/29/2022 19:27:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=33
05/29/2022 19:27:56 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.21226028156523669 on epoch=33
05/29/2022 19:27:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=33
05/29/2022 19:28:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=34
05/29/2022 19:28:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=34
05/29/2022 19:28:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.51 on epoch=34
05/29/2022 19:28:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.51 on epoch=35
05/29/2022 19:28:21 - INFO - __main__ - Global step 850 Train loss 0.49 Classification-F1 0.23667570009033423 on epoch=35
05/29/2022 19:28:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.53 on epoch=35
05/29/2022 19:28:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.52 on epoch=36
05/29/2022 19:28:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.48 on epoch=36
05/29/2022 19:28:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.48 on epoch=37
05/29/2022 19:28:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.53 on epoch=37
05/29/2022 19:28:45 - INFO - __main__ - Global step 900 Train loss 0.51 Classification-F1 0.19450425926678314 on epoch=37
05/29/2022 19:28:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.52 on epoch=37
05/29/2022 19:28:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=38
05/29/2022 19:28:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.51 on epoch=38
05/29/2022 19:28:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.53 on epoch=39
05/29/2022 19:28:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.48 on epoch=39
05/29/2022 19:29:09 - INFO - __main__ - Global step 950 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=39
05/29/2022 19:29:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.49 on epoch=39
05/29/2022 19:29:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.46 on epoch=40
05/29/2022 19:29:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.47 on epoch=40
05/29/2022 19:29:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.48 on epoch=41
05/29/2022 19:29:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=41
05/29/2022 19:29:34 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=41
05/29/2022 19:29:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.50 on epoch=42
05/29/2022 19:29:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.50 on epoch=42
05/29/2022 19:29:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.47 on epoch=42
05/29/2022 19:29:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.50 on epoch=43
05/29/2022 19:29:47 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.46 on epoch=43
05/29/2022 19:29:58 - INFO - __main__ - Global step 1050 Train loss 0.49 Classification-F1 0.17273804346195953 on epoch=43
05/29/2022 19:30:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=44
05/29/2022 19:30:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=44
05/29/2022 19:30:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.50 on epoch=44
05/29/2022 19:30:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.52 on epoch=45
05/29/2022 19:30:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.49 on epoch=45
05/29/2022 19:30:23 - INFO - __main__ - Global step 1100 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=45
05/29/2022 19:30:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.51 on epoch=46
05/29/2022 19:30:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.50 on epoch=46
05/29/2022 19:30:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.53 on epoch=47
05/29/2022 19:30:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.53 on epoch=47
05/29/2022 19:30:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.47 on epoch=47
05/29/2022 19:30:48 - INFO - __main__ - Global step 1150 Train loss 0.51 Classification-F1 0.2031593005260647 on epoch=47
05/29/2022 19:30:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.50 on epoch=48
05/29/2022 19:30:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.48 on epoch=48
05/29/2022 19:30:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.47 on epoch=49
05/29/2022 19:30:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=49
05/29/2022 19:31:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.49 on epoch=49
05/29/2022 19:31:12 - INFO - __main__ - Global step 1200 Train loss 0.48 Classification-F1 0.17114127702362994 on epoch=49
05/29/2022 19:31:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.52 on epoch=50
05/29/2022 19:31:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.53 on epoch=50
05/29/2022 19:31:20 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.50 on epoch=51
05/29/2022 19:31:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.49 on epoch=51
05/29/2022 19:31:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=52
05/29/2022 19:31:37 - INFO - __main__ - Global step 1250 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=52
05/29/2022 19:31:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.49 on epoch=52
05/29/2022 19:31:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.47 on epoch=52
05/29/2022 19:31:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.45 on epoch=53
05/29/2022 19:31:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.43 on epoch=53
05/29/2022 19:31:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.50 on epoch=54
05/29/2022 19:32:01 - INFO - __main__ - Global step 1300 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=54
05/29/2022 19:32:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.46 on epoch=54
05/29/2022 19:32:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.48 on epoch=54
05/29/2022 19:32:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.50 on epoch=55
05/29/2022 19:32:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.45 on epoch=55
05/29/2022 19:32:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.53 on epoch=56
05/29/2022 19:32:25 - INFO - __main__ - Global step 1350 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=56
05/29/2022 19:32:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.46 on epoch=56
05/29/2022 19:32:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.47 on epoch=57
05/29/2022 19:32:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.46 on epoch=57
05/29/2022 19:32:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.49 on epoch=57
05/29/2022 19:32:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.45 on epoch=58
05/29/2022 19:32:50 - INFO - __main__ - Global step 1400 Train loss 0.47 Classification-F1 0.19822148373796114 on epoch=58
05/29/2022 19:32:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.45 on epoch=58
05/29/2022 19:32:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=59
05/29/2022 19:32:58 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.47 on epoch=59
05/29/2022 19:33:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=59
05/29/2022 19:33:03 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.48 on epoch=60
05/29/2022 19:33:15 - INFO - __main__ - Global step 1450 Train loss 0.46 Classification-F1 0.19355661881977673 on epoch=60
05/29/2022 19:33:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.47 on epoch=60
05/29/2022 19:33:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.51 on epoch=61
05/29/2022 19:33:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.45 on epoch=61
05/29/2022 19:33:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.48 on epoch=62
05/29/2022 19:33:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=62
05/29/2022 19:33:40 - INFO - __main__ - Global step 1500 Train loss 0.47 Classification-F1 0.1751478369125428 on epoch=62
05/29/2022 19:33:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=62
05/29/2022 19:33:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.47 on epoch=63
05/29/2022 19:33:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.48 on epoch=63
05/29/2022 19:33:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.50 on epoch=64
05/29/2022 19:33:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.49 on epoch=64
05/29/2022 19:34:05 - INFO - __main__ - Global step 1550 Train loss 0.48 Classification-F1 0.26077019667794527 on epoch=64
05/29/2022 19:34:05 - INFO - __main__ - Saving model with best Classification-F1: 0.24910865971996307 -> 0.26077019667794527 on epoch=64, global_step=1550
05/29/2022 19:34:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.51 on epoch=64
05/29/2022 19:34:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.49 on epoch=65
05/29/2022 19:34:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=65
05/29/2022 19:34:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.49 on epoch=66
05/29/2022 19:34:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=66
05/29/2022 19:34:30 - INFO - __main__ - Global step 1600 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=66
05/29/2022 19:34:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.50 on epoch=67
05/29/2022 19:34:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.48 on epoch=67
05/29/2022 19:34:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.46 on epoch=67
05/29/2022 19:34:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.49 on epoch=68
05/29/2022 19:34:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.47 on epoch=68
05/29/2022 19:34:54 - INFO - __main__ - Global step 1650 Train loss 0.48 Classification-F1 0.22902353771716863 on epoch=68
05/29/2022 19:34:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.46 on epoch=69
05/29/2022 19:34:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=69
05/29/2022 19:35:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.46 on epoch=69
05/29/2022 19:35:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.46 on epoch=70
05/29/2022 19:35:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.46 on epoch=70
05/29/2022 19:35:18 - INFO - __main__ - Global step 1700 Train loss 0.46 Classification-F1 0.22497395524257888 on epoch=70
05/29/2022 19:35:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.49 on epoch=71
05/29/2022 19:35:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.47 on epoch=71
05/29/2022 19:35:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.48 on epoch=72
05/29/2022 19:35:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.48 on epoch=72
05/29/2022 19:35:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.45 on epoch=72
05/29/2022 19:35:43 - INFO - __main__ - Global step 1750 Train loss 0.48 Classification-F1 0.21096152762913312 on epoch=72
05/29/2022 19:35:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.47 on epoch=73
05/29/2022 19:35:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=73
05/29/2022 19:35:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.51 on epoch=74
05/29/2022 19:35:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.44 on epoch=74
05/29/2022 19:35:56 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.49 on epoch=74
05/29/2022 19:36:08 - INFO - __main__ - Global step 1800 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=74
05/29/2022 19:36:10 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.40 on epoch=75
05/29/2022 19:36:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.46 on epoch=75
05/29/2022 19:36:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.52 on epoch=76
05/29/2022 19:36:18 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.44 on epoch=76
05/29/2022 19:36:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.45 on epoch=77
05/29/2022 19:36:32 - INFO - __main__ - Global step 1850 Train loss 0.46 Classification-F1 0.17273804346195953 on epoch=77
05/29/2022 19:36:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.46 on epoch=77
05/29/2022 19:36:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.46 on epoch=77
05/29/2022 19:36:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.46 on epoch=78
05/29/2022 19:36:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.45 on epoch=78
05/29/2022 19:36:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.49 on epoch=79
05/29/2022 19:36:57 - INFO - __main__ - Global step 1900 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=79
05/29/2022 19:36:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.44 on epoch=79
05/29/2022 19:37:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.48 on epoch=79
05/29/2022 19:37:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.47 on epoch=80
05/29/2022 19:37:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.49 on epoch=80
05/29/2022 19:37:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.46 on epoch=81
05/29/2022 19:37:21 - INFO - __main__ - Global step 1950 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=81
05/29/2022 19:37:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.46 on epoch=81
05/29/2022 19:37:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.48 on epoch=82
05/29/2022 19:37:29 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.47 on epoch=82
05/29/2022 19:37:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.51 on epoch=82
05/29/2022 19:37:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.44 on epoch=83
05/29/2022 19:37:45 - INFO - __main__ - Global step 2000 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=83
05/29/2022 19:37:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.43 on epoch=83
05/29/2022 19:37:50 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.47 on epoch=84
05/29/2022 19:37:53 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.49 on epoch=84
05/29/2022 19:37:55 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.44 on epoch=84
05/29/2022 19:37:58 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.47 on epoch=85
05/29/2022 19:38:09 - INFO - __main__ - Global step 2050 Train loss 0.46 Classification-F1 0.35555300606554235 on epoch=85
05/29/2022 19:38:09 - INFO - __main__ - Saving model with best Classification-F1: 0.26077019667794527 -> 0.35555300606554235 on epoch=85, global_step=2050
05/29/2022 19:38:12 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.47 on epoch=85
05/29/2022 19:38:15 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.43 on epoch=86
05/29/2022 19:38:17 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.45 on epoch=86
05/29/2022 19:38:20 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.46 on epoch=87
05/29/2022 19:38:23 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.44 on epoch=87
05/29/2022 19:38:34 - INFO - __main__ - Global step 2100 Train loss 0.45 Classification-F1 0.20073162620552043 on epoch=87
05/29/2022 19:38:37 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.50 on epoch=87
05/29/2022 19:38:39 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.42 on epoch=88
05/29/2022 19:38:42 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.42 on epoch=88
05/29/2022 19:38:45 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.50 on epoch=89
05/29/2022 19:38:47 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.49 on epoch=89
05/29/2022 19:38:58 - INFO - __main__ - Global step 2150 Train loss 0.47 Classification-F1 0.17782710198613258 on epoch=89
05/29/2022 19:39:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.43 on epoch=89
05/29/2022 19:39:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.46 on epoch=90
05/29/2022 19:39:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.44 on epoch=90
05/29/2022 19:39:09 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.43 on epoch=91
05/29/2022 19:39:11 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.46 on epoch=91
05/29/2022 19:39:22 - INFO - __main__ - Global step 2200 Train loss 0.44 Classification-F1 0.1868021757801317 on epoch=91
05/29/2022 19:39:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.48 on epoch=92
05/29/2022 19:39:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.46 on epoch=92
05/29/2022 19:39:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.47 on epoch=92
05/29/2022 19:39:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.54 on epoch=93
05/29/2022 19:39:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.41 on epoch=93
05/29/2022 19:39:46 - INFO - __main__ - Global step 2250 Train loss 0.47 Classification-F1 0.16699282452707112 on epoch=93
05/29/2022 19:39:49 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.48 on epoch=94
05/29/2022 19:39:51 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.45 on epoch=94
05/29/2022 19:39:54 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.48 on epoch=94
05/29/2022 19:39:57 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.50 on epoch=95
05/29/2022 19:39:59 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.49 on epoch=95
05/29/2022 19:40:10 - INFO - __main__ - Global step 2300 Train loss 0.48 Classification-F1 0.16699282452707112 on epoch=95
05/29/2022 19:40:13 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.52 on epoch=96
05/29/2022 19:40:16 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.43 on epoch=96
05/29/2022 19:40:18 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.49 on epoch=97
05/29/2022 19:40:21 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.41 on epoch=97
05/29/2022 19:40:24 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.44 on epoch=97
05/29/2022 19:40:35 - INFO - __main__ - Global step 2350 Train loss 0.46 Classification-F1 0.27848632320859207 on epoch=97
05/29/2022 19:40:37 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.42 on epoch=98
05/29/2022 19:40:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.45 on epoch=98
05/29/2022 19:40:42 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.46 on epoch=99
05/29/2022 19:40:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.42 on epoch=99
05/29/2022 19:40:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.45 on epoch=99
05/29/2022 19:40:59 - INFO - __main__ - Global step 2400 Train loss 0.44 Classification-F1 0.19008924822925555 on epoch=99
05/29/2022 19:41:02 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.47 on epoch=100
05/29/2022 19:41:05 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.46 on epoch=100
05/29/2022 19:41:07 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.44 on epoch=101
05/29/2022 19:41:10 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.45 on epoch=101
05/29/2022 19:41:12 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.46 on epoch=102
05/29/2022 19:41:23 - INFO - __main__ - Global step 2450 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=102
05/29/2022 19:41:26 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.43 on epoch=102
05/29/2022 19:41:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.46 on epoch=102
05/29/2022 19:41:31 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.46 on epoch=103
05/29/2022 19:41:33 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.47 on epoch=103
05/29/2022 19:41:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.46 on epoch=104
05/29/2022 19:41:46 - INFO - __main__ - Global step 2500 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=104
05/29/2022 19:41:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.47 on epoch=104
05/29/2022 19:41:51 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.43 on epoch=104
05/29/2022 19:41:54 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.41 on epoch=105
05/29/2022 19:41:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.48 on epoch=105
05/29/2022 19:41:59 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.45 on epoch=106
05/29/2022 19:42:10 - INFO - __main__ - Global step 2550 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=106
05/29/2022 19:42:13 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.48 on epoch=106
05/29/2022 19:42:16 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.49 on epoch=107
05/29/2022 19:42:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.48 on epoch=107
05/29/2022 19:42:21 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.52 on epoch=107
05/29/2022 19:42:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.45 on epoch=108
05/29/2022 19:42:35 - INFO - __main__ - Global step 2600 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=108
05/29/2022 19:42:37 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.44 on epoch=108
05/29/2022 19:42:40 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.45 on epoch=109
05/29/2022 19:42:42 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.42 on epoch=109
05/29/2022 19:42:45 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.47 on epoch=109
05/29/2022 19:42:48 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.42 on epoch=110
05/29/2022 19:42:59 - INFO - __main__ - Global step 2650 Train loss 0.44 Classification-F1 0.2683199375311334 on epoch=110
05/29/2022 19:43:02 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.42 on epoch=110
05/29/2022 19:43:05 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.50 on epoch=111
05/29/2022 19:43:07 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.41 on epoch=111
05/29/2022 19:43:10 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.42 on epoch=112
05/29/2022 19:43:12 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.44 on epoch=112
05/29/2022 19:43:24 - INFO - __main__ - Global step 2700 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=112
05/29/2022 19:43:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.51 on epoch=112
05/29/2022 19:43:29 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.51 on epoch=113
05/29/2022 19:43:32 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.40 on epoch=113
05/29/2022 19:43:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.49 on epoch=114
05/29/2022 19:43:37 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.42 on epoch=114
05/29/2022 19:43:48 - INFO - __main__ - Global step 2750 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=114
05/29/2022 19:43:51 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.46 on epoch=114
05/29/2022 19:43:53 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.42 on epoch=115
05/29/2022 19:43:56 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.47 on epoch=115
05/29/2022 19:43:59 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.46 on epoch=116
05/29/2022 19:44:01 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.40 on epoch=116
05/29/2022 19:44:13 - INFO - __main__ - Global step 2800 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=116
05/29/2022 19:44:15 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.48 on epoch=117
05/29/2022 19:44:18 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.48 on epoch=117
05/29/2022 19:44:20 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.49 on epoch=117
05/29/2022 19:44:23 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.47 on epoch=118
05/29/2022 19:44:26 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.45 on epoch=118
05/29/2022 19:44:37 - INFO - __main__ - Global step 2850 Train loss 0.47 Classification-F1 0.22437845842101156 on epoch=118
05/29/2022 19:44:40 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.45 on epoch=119
05/29/2022 19:44:43 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.48 on epoch=119
05/29/2022 19:44:45 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.44 on epoch=119
05/29/2022 19:44:48 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.46 on epoch=120
05/29/2022 19:44:50 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.49 on epoch=120
05/29/2022 19:45:02 - INFO - __main__ - Global step 2900 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=120
05/29/2022 19:45:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.51 on epoch=121
05/29/2022 19:45:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.46 on epoch=121
05/29/2022 19:45:10 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.45 on epoch=122
05/29/2022 19:45:12 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.43 on epoch=122
05/29/2022 19:45:15 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.45 on epoch=122
05/29/2022 19:45:26 - INFO - __main__ - Global step 2950 Train loss 0.46 Classification-F1 0.3674780824380414 on epoch=122
05/29/2022 19:45:26 - INFO - __main__ - Saving model with best Classification-F1: 0.35555300606554235 -> 0.3674780824380414 on epoch=122, global_step=2950
05/29/2022 19:45:29 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.46 on epoch=123
05/29/2022 19:45:32 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.45 on epoch=123
05/29/2022 19:45:34 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.49 on epoch=124
05/29/2022 19:45:37 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.43 on epoch=124
05/29/2022 19:45:40 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.45 on epoch=124
05/29/2022 19:45:41 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 19:45:41 - INFO - __main__ - Printing 3 examples
05/29/2022 19:45:41 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/29/2022 19:45:41 - INFO - __main__ - ['contradiction']
05/29/2022 19:45:41 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/29/2022 19:45:41 - INFO - __main__ - ['contradiction']
05/29/2022 19:45:41 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/29/2022 19:45:41 - INFO - __main__ - ['contradiction']
05/29/2022 19:45:41 - INFO - __main__ - Tokenizing Input ...
05/29/2022 19:45:41 - INFO - __main__ - Tokenizing Output ...
05/29/2022 19:45:41 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 19:45:42 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 19:45:42 - INFO - __main__ - Printing 3 examples
05/29/2022 19:45:42 - INFO - __main__ -  [anli] premise: Never Shout Never is an EP by Never Shout Never which was released on December 8, 2009. The physical release is sold exclusively at Hot Topic. The EP features two songs from his then upcoming Sire Records full-length debut, "What is Love?", one song that is a B-side from "The Summer EP" and one live track. [SEP] hypothesis:  The EP features two live tracks. 
05/29/2022 19:45:42 - INFO - __main__ - ['contradiction']
05/29/2022 19:45:42 - INFO - __main__ -  [anli] premise: The Hound of the Baskervilles is a 1978 British comedy film spoofing "The Hound of the Baskervilles" by Sir Arthur Conan Doyle. It starred Peter Cook as Sherlock Holmes and Dudley Moore as Dr. Watson. A number of other well-known British comedy actors appeared in the film including Terry-Thomas (in his final screen appearance), Kenneth Williams and Denholm Elliott. [SEP] hypothesis: The Hound of the Baskervilles is a american food dish.
05/29/2022 19:45:42 - INFO - __main__ - ['contradiction']
05/29/2022 19:45:42 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts is most famous for his music career.
05/29/2022 19:45:42 - INFO - __main__ - ['contradiction']
05/29/2022 19:45:42 - INFO - __main__ - Tokenizing Input ...
05/29/2022 19:45:42 - INFO - __main__ - Tokenizing Output ...
05/29/2022 19:45:42 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 19:45:50 - INFO - __main__ - Global step 3000 Train loss 0.46 Classification-F1 0.21531211949378662 on epoch=124
05/29/2022 19:45:50 - INFO - __main__ - save last model!
05/29/2022 19:45:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 19:45:50 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 19:45:50 - INFO - __main__ - Printing 3 examples
05/29/2022 19:45:50 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 19:45:50 - INFO - __main__ - ['contradiction']
05/29/2022 19:45:50 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 19:45:50 - INFO - __main__ - ['entailment']
05/29/2022 19:45:50 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 19:45:50 - INFO - __main__ - ['contradiction']
05/29/2022 19:45:50 - INFO - __main__ - Tokenizing Input ...
05/29/2022 19:45:50 - INFO - __main__ - Tokenizing Output ...
05/29/2022 19:45:51 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 19:45:58 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 19:45:58 - INFO - __main__ - task name: anli
05/29/2022 19:45:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 19:45:58 - INFO - __main__ - Starting training!
05/29/2022 19:46:16 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_42_0.2_8_predictions.txt
05/29/2022 19:46:16 - INFO - __main__ - Classification-F1 on test data: 0.2057
05/29/2022 19:46:17 - INFO - __main__ - prefix=anli_128_42, lr=0.2, bsz=8, dev_performance=0.3674780824380414, test_performance=0.2056542617046819
05/29/2022 19:46:17 - INFO - __main__ - Running ... prefix=anli_128_87, lr=0.5, bsz=8 ...
05/29/2022 19:46:18 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 19:46:18 - INFO - __main__ - Printing 3 examples
05/29/2022 19:46:18 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/29/2022 19:46:18 - INFO - __main__ - ['contradiction']
05/29/2022 19:46:18 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/29/2022 19:46:18 - INFO - __main__ - ['contradiction']
05/29/2022 19:46:18 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/29/2022 19:46:18 - INFO - __main__ - ['contradiction']
05/29/2022 19:46:18 - INFO - __main__ - Tokenizing Input ...
05/29/2022 19:46:18 - INFO - __main__ - Tokenizing Output ...
05/29/2022 19:46:18 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 19:46:18 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 19:46:18 - INFO - __main__ - Printing 3 examples
05/29/2022 19:46:18 - INFO - __main__ -  [anli] premise: Never Shout Never is an EP by Never Shout Never which was released on December 8, 2009. The physical release is sold exclusively at Hot Topic. The EP features two songs from his then upcoming Sire Records full-length debut, "What is Love?", one song that is a B-side from "The Summer EP" and one live track. [SEP] hypothesis:  The EP features two live tracks. 
05/29/2022 19:46:18 - INFO - __main__ - ['contradiction']
05/29/2022 19:46:18 - INFO - __main__ -  [anli] premise: The Hound of the Baskervilles is a 1978 British comedy film spoofing "The Hound of the Baskervilles" by Sir Arthur Conan Doyle. It starred Peter Cook as Sherlock Holmes and Dudley Moore as Dr. Watson. A number of other well-known British comedy actors appeared in the film including Terry-Thomas (in his final screen appearance), Kenneth Williams and Denholm Elliott. [SEP] hypothesis: The Hound of the Baskervilles is a american food dish.
05/29/2022 19:46:18 - INFO - __main__ - ['contradiction']
05/29/2022 19:46:18 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts is most famous for his music career.
05/29/2022 19:46:18 - INFO - __main__ - ['contradiction']
05/29/2022 19:46:18 - INFO - __main__ - Tokenizing Input ...
05/29/2022 19:46:18 - INFO - __main__ - Tokenizing Output ...
05/29/2022 19:46:19 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 19:46:37 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 19:46:37 - INFO - __main__ - task name: anli
05/29/2022 19:46:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 19:46:38 - INFO - __main__ - Starting training!
05/29/2022 19:46:41 - INFO - __main__ - Step 10 Global step 10 Train loss 6.57 on epoch=0
05/29/2022 19:46:44 - INFO - __main__ - Step 20 Global step 20 Train loss 2.71 on epoch=0
05/29/2022 19:46:46 - INFO - __main__ - Step 30 Global step 30 Train loss 1.19 on epoch=1
05/29/2022 19:46:49 - INFO - __main__ - Step 40 Global step 40 Train loss 0.90 on epoch=1
05/29/2022 19:46:52 - INFO - __main__ - Step 50 Global step 50 Train loss 0.70 on epoch=2
05/29/2022 19:47:01 - INFO - __main__ - Global step 50 Train loss 2.42 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 19:47:01 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 19:47:03 - INFO - __main__ - Step 60 Global step 60 Train loss 0.86 on epoch=2
05/29/2022 19:47:06 - INFO - __main__ - Step 70 Global step 70 Train loss 0.63 on epoch=2
05/29/2022 19:47:08 - INFO - __main__ - Step 80 Global step 80 Train loss 0.75 on epoch=3
05/29/2022 19:47:11 - INFO - __main__ - Step 90 Global step 90 Train loss 0.69 on epoch=3
05/29/2022 19:47:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.67 on epoch=4
05/29/2022 19:47:25 - INFO - __main__ - Global step 100 Train loss 0.72 Classification-F1 0.16666666666666666 on epoch=4
05/29/2022 19:47:27 - INFO - __main__ - Step 110 Global step 110 Train loss 0.65 on epoch=4
05/29/2022 19:47:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.62 on epoch=4
05/29/2022 19:47:33 - INFO - __main__ - Step 130 Global step 130 Train loss 0.57 on epoch=5
05/29/2022 19:47:35 - INFO - __main__ - Step 140 Global step 140 Train loss 0.58 on epoch=5
05/29/2022 19:47:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.56 on epoch=6
05/29/2022 19:47:50 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.28762095428762097 on epoch=6
05/29/2022 19:47:50 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.28762095428762097 on epoch=6, global_step=150
05/29/2022 19:47:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=6
05/29/2022 19:47:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=7
05/29/2022 19:47:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.57 on epoch=7
05/29/2022 19:48:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.55 on epoch=7
05/29/2022 19:48:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.55 on epoch=8
05/29/2022 19:48:12 - INFO - __main__ - Global step 200 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=8
05/29/2022 19:48:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=8
05/29/2022 19:48:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=9
05/29/2022 19:48:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.56 on epoch=9
05/29/2022 19:48:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.53 on epoch=9
05/29/2022 19:48:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=10
05/29/2022 19:48:37 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=10
05/29/2022 19:48:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.50 on epoch=10
05/29/2022 19:48:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.54 on epoch=11
05/29/2022 19:48:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.56 on epoch=11
05/29/2022 19:48:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=12
05/29/2022 19:48:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=12
05/29/2022 19:49:01 - INFO - __main__ - Global step 300 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=12
05/29/2022 19:49:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=12
05/29/2022 19:49:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.55 on epoch=13
05/29/2022 19:49:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.55 on epoch=13
05/29/2022 19:49:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=14
05/29/2022 19:49:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=14
05/29/2022 19:49:25 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=14
05/29/2022 19:49:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=14
05/29/2022 19:49:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=15
05/29/2022 19:49:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=15
05/29/2022 19:49:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=16
05/29/2022 19:49:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=16
05/29/2022 19:49:52 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.3358047035414153 on epoch=16
05/29/2022 19:49:52 - INFO - __main__ - Saving model with best Classification-F1: 0.28762095428762097 -> 0.3358047035414153 on epoch=16, global_step=400
05/29/2022 19:49:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=17
05/29/2022 19:49:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.53 on epoch=17
05/29/2022 19:50:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=17
05/29/2022 19:50:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=18
05/29/2022 19:50:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=18
05/29/2022 19:50:17 - INFO - __main__ - Global step 450 Train loss 0.50 Classification-F1 0.16699282452707112 on epoch=18
05/29/2022 19:50:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=19
05/29/2022 19:50:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=19
05/29/2022 19:50:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.55 on epoch=19
05/29/2022 19:50:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=20
05/29/2022 19:50:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=20
05/29/2022 19:50:43 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.18093327827266828 on epoch=20
05/29/2022 19:50:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=21
05/29/2022 19:50:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.55 on epoch=21
05/29/2022 19:50:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=22
05/29/2022 19:50:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.52 on epoch=22
05/29/2022 19:50:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=22
05/29/2022 19:51:08 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=22
05/29/2022 19:51:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=23
05/29/2022 19:51:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=23
05/29/2022 19:51:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=24
05/29/2022 19:51:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=24
05/29/2022 19:51:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=24
05/29/2022 19:51:33 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.23198525085317537 on epoch=24
05/29/2022 19:51:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=25
05/29/2022 19:51:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.46 on epoch=25
05/29/2022 19:51:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=26
05/29/2022 19:51:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=26
05/29/2022 19:51:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.47 on epoch=27
05/29/2022 19:51:59 - INFO - __main__ - Global step 650 Train loss 0.48 Classification-F1 0.22077618288144604 on epoch=27
05/29/2022 19:52:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.51 on epoch=27
05/29/2022 19:52:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=27
05/29/2022 19:52:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=28
05/29/2022 19:52:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=28
05/29/2022 19:52:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=29
05/29/2022 19:52:24 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=29
05/29/2022 19:52:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=29
05/29/2022 19:52:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=29
05/29/2022 19:52:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=30
05/29/2022 19:52:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=30
05/29/2022 19:52:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=31
05/29/2022 19:52:49 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.26318663160768424 on epoch=31
05/29/2022 19:52:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.51 on epoch=31
05/29/2022 19:52:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=32
05/29/2022 19:52:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.53 on epoch=32
05/29/2022 19:53:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=32
05/29/2022 19:53:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=33
05/29/2022 19:53:15 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.18759726112667288 on epoch=33
05/29/2022 19:53:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=33
05/29/2022 19:53:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=34
05/29/2022 19:53:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.49 on epoch=34
05/29/2022 19:53:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=34
05/29/2022 19:53:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=35
05/29/2022 19:53:39 - INFO - __main__ - Global step 850 Train loss 0.48 Classification-F1 0.18930532459718494 on epoch=35
05/29/2022 19:53:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=35
05/29/2022 19:53:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=36
05/29/2022 19:53:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.48 on epoch=36
05/29/2022 19:53:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=37
05/29/2022 19:53:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.52 on epoch=37
05/29/2022 19:54:04 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.27274515749979344 on epoch=37
05/29/2022 19:54:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.41 on epoch=37
05/29/2022 19:54:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=38
05/29/2022 19:54:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=38
05/29/2022 19:54:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.51 on epoch=39
05/29/2022 19:54:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.49 on epoch=39
05/29/2022 19:54:30 - INFO - __main__ - Global step 950 Train loss 0.48 Classification-F1 0.22748314302826564 on epoch=39
05/29/2022 19:54:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.49 on epoch=39
05/29/2022 19:54:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.46 on epoch=40
05/29/2022 19:54:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=40
05/29/2022 19:54:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=41
05/29/2022 19:54:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.50 on epoch=41
05/29/2022 19:54:56 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.3117837706979306 on epoch=41
05/29/2022 19:54:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=42
05/29/2022 19:55:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=42
05/29/2022 19:55:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=42
05/29/2022 19:55:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=43
05/29/2022 19:55:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=43
05/29/2022 19:55:21 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.25193462000184685 on epoch=43
05/29/2022 19:55:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=44
05/29/2022 19:55:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.47 on epoch=44
05/29/2022 19:55:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=44
05/29/2022 19:55:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.47 on epoch=45
05/29/2022 19:55:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=45
05/29/2022 19:55:46 - INFO - __main__ - Global step 1100 Train loss 0.46 Classification-F1 0.21763757721063384 on epoch=45
05/29/2022 19:55:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.43 on epoch=46
05/29/2022 19:55:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=46
05/29/2022 19:55:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=47
05/29/2022 19:55:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.47 on epoch=47
05/29/2022 19:55:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.45 on epoch=47
05/29/2022 19:56:10 - INFO - __main__ - Global step 1150 Train loss 0.45 Classification-F1 0.21621621621621623 on epoch=47
05/29/2022 19:56:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=48
05/29/2022 19:56:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=48
05/29/2022 19:56:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.48 on epoch=49
05/29/2022 19:56:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=49
05/29/2022 19:56:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.51 on epoch=49
05/29/2022 19:56:35 - INFO - __main__ - Global step 1200 Train loss 0.45 Classification-F1 0.2661268556005398 on epoch=49
05/29/2022 19:56:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.46 on epoch=50
05/29/2022 19:56:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=50
05/29/2022 19:56:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.40 on epoch=51
05/29/2022 19:56:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.54 on epoch=51
05/29/2022 19:56:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.42 on epoch=52
05/29/2022 19:57:00 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.2493855662669814 on epoch=52
05/29/2022 19:57:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=52
05/29/2022 19:57:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.40 on epoch=52
05/29/2022 19:57:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.46 on epoch=53
05/29/2022 19:57:10 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=53
05/29/2022 19:57:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.47 on epoch=54
05/29/2022 19:57:24 - INFO - __main__ - Global step 1300 Train loss 0.45 Classification-F1 0.17624156851780656 on epoch=54
05/29/2022 19:57:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=54
05/29/2022 19:57:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=54
05/29/2022 19:57:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.48 on epoch=55
05/29/2022 19:57:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.46 on epoch=55
05/29/2022 19:57:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=56
05/29/2022 19:57:49 - INFO - __main__ - Global step 1350 Train loss 0.45 Classification-F1 0.26373100394878546 on epoch=56
05/29/2022 19:57:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.47 on epoch=56
05/29/2022 19:57:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=57
05/29/2022 19:57:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=57
05/29/2022 19:58:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.42 on epoch=57
05/29/2022 19:58:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.48 on epoch=58
05/29/2022 19:58:14 - INFO - __main__ - Global step 1400 Train loss 0.46 Classification-F1 0.27224310776942356 on epoch=58
05/29/2022 19:58:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=58
05/29/2022 19:58:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.50 on epoch=59
05/29/2022 19:58:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.44 on epoch=59
05/29/2022 19:58:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.50 on epoch=59
05/29/2022 19:58:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.48 on epoch=60
05/29/2022 19:58:38 - INFO - __main__ - Global step 1450 Train loss 0.47 Classification-F1 0.1775766716943188 on epoch=60
05/29/2022 19:58:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=60
05/29/2022 19:58:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.45 on epoch=61
05/29/2022 19:58:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.48 on epoch=61
05/29/2022 19:58:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=62
05/29/2022 19:58:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=62
05/29/2022 19:59:03 - INFO - __main__ - Global step 1500 Train loss 0.44 Classification-F1 0.18485390578413832 on epoch=62
05/29/2022 19:59:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=62
05/29/2022 19:59:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.44 on epoch=63
05/29/2022 19:59:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.44 on epoch=63
05/29/2022 19:59:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.43 on epoch=64
05/29/2022 19:59:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=64
05/29/2022 19:59:28 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.2660716537451588 on epoch=64
05/29/2022 19:59:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.46 on epoch=64
05/29/2022 19:59:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.44 on epoch=65
05/29/2022 19:59:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=65
05/29/2022 19:59:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=66
05/29/2022 19:59:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.47 on epoch=66
05/29/2022 19:59:52 - INFO - __main__ - Global step 1600 Train loss 0.45 Classification-F1 0.3170227132520466 on epoch=66
05/29/2022 19:59:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.43 on epoch=67
05/29/2022 19:59:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=67
05/29/2022 20:00:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=67
05/29/2022 20:00:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=68
05/29/2022 20:00:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.45 on epoch=68
05/29/2022 20:00:17 - INFO - __main__ - Global step 1650 Train loss 0.44 Classification-F1 0.2645502645502646 on epoch=68
05/29/2022 20:00:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.43 on epoch=69
05/29/2022 20:00:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.43 on epoch=69
05/29/2022 20:00:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.42 on epoch=69
05/29/2022 20:00:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.49 on epoch=70
05/29/2022 20:00:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=70
05/29/2022 20:00:42 - INFO - __main__ - Global step 1700 Train loss 0.44 Classification-F1 0.2186123745577937 on epoch=70
05/29/2022 20:00:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=71
05/29/2022 20:00:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.47 on epoch=71
05/29/2022 20:00:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.45 on epoch=72
05/29/2022 20:00:52 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=72
05/29/2022 20:00:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=72
05/29/2022 20:01:06 - INFO - __main__ - Global step 1750 Train loss 0.43 Classification-F1 0.17248822009423928 on epoch=72
05/29/2022 20:01:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.43 on epoch=73
05/29/2022 20:01:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.45 on epoch=73
05/29/2022 20:01:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.45 on epoch=74
05/29/2022 20:01:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.43 on epoch=74
05/29/2022 20:01:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=74
05/29/2022 20:01:31 - INFO - __main__ - Global step 1800 Train loss 0.44 Classification-F1 0.3331728146387392 on epoch=74
05/29/2022 20:01:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.44 on epoch=75
05/29/2022 20:01:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=75
05/29/2022 20:01:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.46 on epoch=76
05/29/2022 20:01:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.45 on epoch=76
05/29/2022 20:01:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.44 on epoch=77
05/29/2022 20:01:55 - INFO - __main__ - Global step 1850 Train loss 0.44 Classification-F1 0.20571653955147418 on epoch=77
05/29/2022 20:01:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.49 on epoch=77
05/29/2022 20:02:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.40 on epoch=77
05/29/2022 20:02:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=78
05/29/2022 20:02:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.44 on epoch=78
05/29/2022 20:02:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=79
05/29/2022 20:02:19 - INFO - __main__ - Global step 1900 Train loss 0.45 Classification-F1 0.16699282452707112 on epoch=79
05/29/2022 20:02:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.42 on epoch=79
05/29/2022 20:02:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.40 on epoch=79
05/29/2022 20:02:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.41 on epoch=80
05/29/2022 20:02:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.43 on epoch=80
05/29/2022 20:02:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=81
05/29/2022 20:02:44 - INFO - __main__ - Global step 1950 Train loss 0.42 Classification-F1 0.21807264399344153 on epoch=81
05/29/2022 20:02:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=81
05/29/2022 20:02:50 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=82
05/29/2022 20:02:52 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.43 on epoch=82
05/29/2022 20:02:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.41 on epoch=82
05/29/2022 20:02:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.48 on epoch=83
05/29/2022 20:03:09 - INFO - __main__ - Global step 2000 Train loss 0.43 Classification-F1 0.2613431398301375 on epoch=83
05/29/2022 20:03:12 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.43 on epoch=83
05/29/2022 20:03:14 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.44 on epoch=84
05/29/2022 20:03:17 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.44 on epoch=84
05/29/2022 20:03:19 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.41 on epoch=84
05/29/2022 20:03:22 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.42 on epoch=85
05/29/2022 20:03:34 - INFO - __main__ - Global step 2050 Train loss 0.43 Classification-F1 0.24891690571472416 on epoch=85
05/29/2022 20:03:36 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.42 on epoch=85
05/29/2022 20:03:39 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.41 on epoch=86
05/29/2022 20:03:41 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.43 on epoch=86
05/29/2022 20:03:44 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.41 on epoch=87
05/29/2022 20:03:47 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.46 on epoch=87
05/29/2022 20:03:58 - INFO - __main__ - Global step 2100 Train loss 0.43 Classification-F1 0.26098621911429154 on epoch=87
05/29/2022 20:04:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.42 on epoch=87
05/29/2022 20:04:04 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.42 on epoch=88
05/29/2022 20:04:06 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.38 on epoch=88
05/29/2022 20:04:09 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.46 on epoch=89
05/29/2022 20:04:11 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.41 on epoch=89
05/29/2022 20:04:23 - INFO - __main__ - Global step 2150 Train loss 0.42 Classification-F1 0.20101857576025198 on epoch=89
05/29/2022 20:04:26 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.41 on epoch=89
05/29/2022 20:04:28 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.46 on epoch=90
05/29/2022 20:04:31 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.42 on epoch=90
05/29/2022 20:04:34 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.40 on epoch=91
05/29/2022 20:04:36 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.43 on epoch=91
05/29/2022 20:04:48 - INFO - __main__ - Global step 2200 Train loss 0.43 Classification-F1 0.2701388023968669 on epoch=91
05/29/2022 20:04:51 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.41 on epoch=92
05/29/2022 20:04:53 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.47 on epoch=92
05/29/2022 20:04:56 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.40 on epoch=92
05/29/2022 20:04:59 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.44 on epoch=93
05/29/2022 20:05:01 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.39 on epoch=93
05/29/2022 20:05:13 - INFO - __main__ - Global step 2250 Train loss 0.42 Classification-F1 0.20666902366807913 on epoch=93
05/29/2022 20:05:16 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.47 on epoch=94
05/29/2022 20:05:19 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.44 on epoch=94
05/29/2022 20:05:21 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.40 on epoch=94
05/29/2022 20:05:24 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.45 on epoch=95
05/29/2022 20:05:26 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.42 on epoch=95
05/29/2022 20:05:38 - INFO - __main__ - Global step 2300 Train loss 0.44 Classification-F1 0.2927308157660918 on epoch=95
05/29/2022 20:05:41 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.45 on epoch=96
05/29/2022 20:05:43 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.44 on epoch=96
05/29/2022 20:05:46 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.42 on epoch=97
05/29/2022 20:05:49 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.44 on epoch=97
05/29/2022 20:05:51 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.41 on epoch=97
05/29/2022 20:06:03 - INFO - __main__ - Global step 2350 Train loss 0.43 Classification-F1 0.19668322470439412 on epoch=97
05/29/2022 20:06:05 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.47 on epoch=98
05/29/2022 20:06:08 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.46 on epoch=98
05/29/2022 20:06:11 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.46 on epoch=99
05/29/2022 20:06:13 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.44 on epoch=99
05/29/2022 20:06:16 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.48 on epoch=99
05/29/2022 20:06:28 - INFO - __main__ - Global step 2400 Train loss 0.46 Classification-F1 0.33798665736899686 on epoch=99
05/29/2022 20:06:28 - INFO - __main__ - Saving model with best Classification-F1: 0.3358047035414153 -> 0.33798665736899686 on epoch=99, global_step=2400
05/29/2022 20:06:30 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.48 on epoch=100
05/29/2022 20:06:33 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.43 on epoch=100
05/29/2022 20:06:36 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.36 on epoch=101
05/29/2022 20:06:38 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.42 on epoch=101
05/29/2022 20:06:41 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.44 on epoch=102
05/29/2022 20:06:54 - INFO - __main__ - Global step 2450 Train loss 0.43 Classification-F1 0.2343779185884449 on epoch=102
05/29/2022 20:06:57 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.44 on epoch=102
05/29/2022 20:07:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.39 on epoch=102
05/29/2022 20:07:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.44 on epoch=103
05/29/2022 20:07:05 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.43 on epoch=103
05/29/2022 20:07:08 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.45 on epoch=104
05/29/2022 20:07:19 - INFO - __main__ - Global step 2500 Train loss 0.43 Classification-F1 0.20712130508993035 on epoch=104
05/29/2022 20:07:22 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.44 on epoch=104
05/29/2022 20:07:24 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.43 on epoch=104
05/29/2022 20:07:27 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.43 on epoch=105
05/29/2022 20:07:30 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.43 on epoch=105
05/29/2022 20:07:32 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.42 on epoch=106
05/29/2022 20:07:44 - INFO - __main__ - Global step 2550 Train loss 0.43 Classification-F1 0.2879920121134652 on epoch=106
05/29/2022 20:07:47 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.48 on epoch=106
05/29/2022 20:07:49 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.38 on epoch=107
05/29/2022 20:07:52 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.44 on epoch=107
05/29/2022 20:07:54 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.39 on epoch=107
05/29/2022 20:07:57 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.48 on epoch=108
05/29/2022 20:08:09 - INFO - __main__ - Global step 2600 Train loss 0.44 Classification-F1 0.3201313711715931 on epoch=108
05/29/2022 20:08:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.41 on epoch=108
05/29/2022 20:08:14 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.49 on epoch=109
05/29/2022 20:08:17 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.43 on epoch=109
05/29/2022 20:08:19 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.43 on epoch=109
05/29/2022 20:08:22 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.42 on epoch=110
05/29/2022 20:08:34 - INFO - __main__ - Global step 2650 Train loss 0.44 Classification-F1 0.3251008326827999 on epoch=110
05/29/2022 20:08:36 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.39 on epoch=110
05/29/2022 20:08:39 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.43 on epoch=111
05/29/2022 20:08:41 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.44 on epoch=111
05/29/2022 20:08:44 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.41 on epoch=112
05/29/2022 20:08:47 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.45 on epoch=112
05/29/2022 20:08:59 - INFO - __main__ - Global step 2700 Train loss 0.42 Classification-F1 0.18840855408126056 on epoch=112
05/29/2022 20:09:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.37 on epoch=112
05/29/2022 20:09:04 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.42 on epoch=113
05/29/2022 20:09:07 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.38 on epoch=113
05/29/2022 20:09:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.44 on epoch=114
05/29/2022 20:09:12 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.41 on epoch=114
05/29/2022 20:09:23 - INFO - __main__ - Global step 2750 Train loss 0.40 Classification-F1 0.23408605600386423 on epoch=114
05/29/2022 20:09:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.44 on epoch=114
05/29/2022 20:09:28 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.43 on epoch=115
05/29/2022 20:09:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.40 on epoch=115
05/29/2022 20:09:33 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.42 on epoch=116
05/29/2022 20:09:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.39 on epoch=116
05/29/2022 20:09:53 - INFO - __main__ - Global step 2800 Train loss 0.42 Classification-F1 0.22843732696566998 on epoch=116
05/29/2022 20:09:56 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.40 on epoch=117
05/29/2022 20:09:58 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.44 on epoch=117
05/29/2022 20:10:01 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.40 on epoch=117
05/29/2022 20:10:04 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.47 on epoch=118
05/29/2022 20:10:06 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.39 on epoch=118
05/29/2022 20:10:22 - INFO - __main__ - Global step 2850 Train loss 0.42 Classification-F1 0.1837013736814604 on epoch=118
05/29/2022 20:10:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.47 on epoch=119
05/29/2022 20:10:27 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.38 on epoch=119
05/29/2022 20:10:30 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.45 on epoch=119
05/29/2022 20:10:33 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.40 on epoch=120
05/29/2022 20:10:35 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.38 on epoch=120
05/29/2022 20:10:45 - INFO - __main__ - Global step 2900 Train loss 0.42 Classification-F1 0.27885413382331087 on epoch=120
05/29/2022 20:10:47 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.42 on epoch=121
05/29/2022 20:10:50 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.43 on epoch=121
05/29/2022 20:10:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.42 on epoch=122
05/29/2022 20:10:55 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.45 on epoch=122
05/29/2022 20:10:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.38 on epoch=122
05/29/2022 20:11:14 - INFO - __main__ - Global step 2950 Train loss 0.42 Classification-F1 0.24130359612724758 on epoch=122
05/29/2022 20:11:17 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.43 on epoch=123
05/29/2022 20:11:20 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.42 on epoch=123
05/29/2022 20:11:22 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.43 on epoch=124
05/29/2022 20:11:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.40 on epoch=124
05/29/2022 20:11:28 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.45 on epoch=124
05/29/2022 20:11:29 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 20:11:29 - INFO - __main__ - Printing 3 examples
05/29/2022 20:11:29 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/29/2022 20:11:29 - INFO - __main__ - ['contradiction']
05/29/2022 20:11:29 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/29/2022 20:11:29 - INFO - __main__ - ['contradiction']
05/29/2022 20:11:29 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/29/2022 20:11:29 - INFO - __main__ - ['contradiction']
05/29/2022 20:11:29 - INFO - __main__ - Tokenizing Input ...
05/29/2022 20:11:29 - INFO - __main__ - Tokenizing Output ...
05/29/2022 20:11:29 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 20:11:29 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 20:11:29 - INFO - __main__ - Printing 3 examples
05/29/2022 20:11:29 - INFO - __main__ -  [anli] premise: Never Shout Never is an EP by Never Shout Never which was released on December 8, 2009. The physical release is sold exclusively at Hot Topic. The EP features two songs from his then upcoming Sire Records full-length debut, "What is Love?", one song that is a B-side from "The Summer EP" and one live track. [SEP] hypothesis:  The EP features two live tracks. 
05/29/2022 20:11:29 - INFO - __main__ - ['contradiction']
05/29/2022 20:11:29 - INFO - __main__ -  [anli] premise: The Hound of the Baskervilles is a 1978 British comedy film spoofing "The Hound of the Baskervilles" by Sir Arthur Conan Doyle. It starred Peter Cook as Sherlock Holmes and Dudley Moore as Dr. Watson. A number of other well-known British comedy actors appeared in the film including Terry-Thomas (in his final screen appearance), Kenneth Williams and Denholm Elliott. [SEP] hypothesis: The Hound of the Baskervilles is a american food dish.
05/29/2022 20:11:29 - INFO - __main__ - ['contradiction']
05/29/2022 20:11:29 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts is most famous for his music career.
05/29/2022 20:11:29 - INFO - __main__ - ['contradiction']
05/29/2022 20:11:29 - INFO - __main__ - Tokenizing Input ...
05/29/2022 20:11:30 - INFO - __main__ - Tokenizing Output ...
05/29/2022 20:11:30 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 20:11:40 - INFO - __main__ - Global step 3000 Train loss 0.43 Classification-F1 0.3113806271701009 on epoch=124
05/29/2022 20:11:40 - INFO - __main__ - save last model!
05/29/2022 20:11:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 20:11:40 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 20:11:40 - INFO - __main__ - Printing 3 examples
05/29/2022 20:11:40 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 20:11:40 - INFO - __main__ - ['contradiction']
05/29/2022 20:11:40 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 20:11:40 - INFO - __main__ - ['entailment']
05/29/2022 20:11:40 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 20:11:40 - INFO - __main__ - ['contradiction']
05/29/2022 20:11:40 - INFO - __main__ - Tokenizing Input ...
05/29/2022 20:11:41 - INFO - __main__ - Tokenizing Output ...
05/29/2022 20:11:42 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 20:11:46 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 20:11:46 - INFO - __main__ - task name: anli
05/29/2022 20:11:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 20:11:47 - INFO - __main__ - Starting training!
05/29/2022 20:12:12 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_87_0.5_8_predictions.txt
05/29/2022 20:12:12 - INFO - __main__ - Classification-F1 on test data: 0.3033
05/29/2022 20:12:13 - INFO - __main__ - prefix=anli_128_87, lr=0.5, bsz=8, dev_performance=0.33798665736899686, test_performance=0.3032517133982821
05/29/2022 20:12:13 - INFO - __main__ - Running ... prefix=anli_128_87, lr=0.4, bsz=8 ...
05/29/2022 20:12:14 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 20:12:14 - INFO - __main__ - Printing 3 examples
05/29/2022 20:12:14 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/29/2022 20:12:14 - INFO - __main__ - ['contradiction']
05/29/2022 20:12:14 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/29/2022 20:12:14 - INFO - __main__ - ['contradiction']
05/29/2022 20:12:14 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/29/2022 20:12:14 - INFO - __main__ - ['contradiction']
05/29/2022 20:12:14 - INFO - __main__ - Tokenizing Input ...
05/29/2022 20:12:14 - INFO - __main__ - Tokenizing Output ...
05/29/2022 20:12:14 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 20:12:14 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 20:12:14 - INFO - __main__ - Printing 3 examples
05/29/2022 20:12:14 - INFO - __main__ -  [anli] premise: Never Shout Never is an EP by Never Shout Never which was released on December 8, 2009. The physical release is sold exclusively at Hot Topic. The EP features two songs from his then upcoming Sire Records full-length debut, "What is Love?", one song that is a B-side from "The Summer EP" and one live track. [SEP] hypothesis:  The EP features two live tracks. 
05/29/2022 20:12:14 - INFO - __main__ - ['contradiction']
05/29/2022 20:12:14 - INFO - __main__ -  [anli] premise: The Hound of the Baskervilles is a 1978 British comedy film spoofing "The Hound of the Baskervilles" by Sir Arthur Conan Doyle. It starred Peter Cook as Sherlock Holmes and Dudley Moore as Dr. Watson. A number of other well-known British comedy actors appeared in the film including Terry-Thomas (in his final screen appearance), Kenneth Williams and Denholm Elliott. [SEP] hypothesis: The Hound of the Baskervilles is a american food dish.
05/29/2022 20:12:14 - INFO - __main__ - ['contradiction']
05/29/2022 20:12:14 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts is most famous for his music career.
05/29/2022 20:12:14 - INFO - __main__ - ['contradiction']
05/29/2022 20:12:14 - INFO - __main__ - Tokenizing Input ...
05/29/2022 20:12:15 - INFO - __main__ - Tokenizing Output ...
05/29/2022 20:12:15 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 20:12:31 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 20:12:31 - INFO - __main__ - task name: anli
05/29/2022 20:12:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 20:12:32 - INFO - __main__ - Starting training!
05/29/2022 20:12:35 - INFO - __main__ - Step 10 Global step 10 Train loss 6.32 on epoch=0
05/29/2022 20:12:38 - INFO - __main__ - Step 20 Global step 20 Train loss 2.45 on epoch=0
05/29/2022 20:12:40 - INFO - __main__ - Step 30 Global step 30 Train loss 1.24 on epoch=1
05/29/2022 20:12:43 - INFO - __main__ - Step 40 Global step 40 Train loss 1.01 on epoch=1
05/29/2022 20:12:45 - INFO - __main__ - Step 50 Global step 50 Train loss 0.77 on epoch=2
05/29/2022 20:12:54 - INFO - __main__ - Global step 50 Train loss 2.36 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 20:12:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 20:12:56 - INFO - __main__ - Step 60 Global step 60 Train loss 0.67 on epoch=2
05/29/2022 20:12:59 - INFO - __main__ - Step 70 Global step 70 Train loss 0.62 on epoch=2
05/29/2022 20:13:02 - INFO - __main__ - Step 80 Global step 80 Train loss 0.65 on epoch=3
05/29/2022 20:13:04 - INFO - __main__ - Step 90 Global step 90 Train loss 0.64 on epoch=3
05/29/2022 20:13:07 - INFO - __main__ - Step 100 Global step 100 Train loss 0.66 on epoch=4
05/29/2022 20:13:16 - INFO - __main__ - Global step 100 Train loss 0.65 Classification-F1 0.16666666666666666 on epoch=4
05/29/2022 20:13:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.62 on epoch=4
05/29/2022 20:13:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.59 on epoch=4
05/29/2022 20:13:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.56 on epoch=5
05/29/2022 20:13:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=5
05/29/2022 20:13:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=6
05/29/2022 20:13:41 - INFO - __main__ - Global step 150 Train loss 0.56 Classification-F1 0.26870661432360154 on epoch=6
05/29/2022 20:13:41 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.26870661432360154 on epoch=6, global_step=150
05/29/2022 20:13:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.61 on epoch=6
05/29/2022 20:13:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=7
05/29/2022 20:13:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.58 on epoch=7
05/29/2022 20:13:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=7
05/29/2022 20:13:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.59 on epoch=8
05/29/2022 20:14:02 - INFO - __main__ - Global step 200 Train loss 0.55 Classification-F1 0.16568819308545338 on epoch=8
05/29/2022 20:14:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=8
05/29/2022 20:14:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.57 on epoch=9
05/29/2022 20:14:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=9
05/29/2022 20:14:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.57 on epoch=9
05/29/2022 20:14:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.56 on epoch=10
05/29/2022 20:14:27 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.27411826778684195 on epoch=10
05/29/2022 20:14:27 - INFO - __main__ - Saving model with best Classification-F1: 0.26870661432360154 -> 0.27411826778684195 on epoch=10, global_step=250
05/29/2022 20:14:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=10
05/29/2022 20:14:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=11
05/29/2022 20:14:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.57 on epoch=11
05/29/2022 20:14:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=12
05/29/2022 20:14:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=12
05/29/2022 20:14:51 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=12
05/29/2022 20:14:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=12
05/29/2022 20:14:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=13
05/29/2022 20:14:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.53 on epoch=13
05/29/2022 20:15:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=14
05/29/2022 20:15:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=14
05/29/2022 20:15:15 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=14
05/29/2022 20:15:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=14
05/29/2022 20:15:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.53 on epoch=15
05/29/2022 20:15:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=15
05/29/2022 20:15:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.53 on epoch=16
05/29/2022 20:15:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=16
05/29/2022 20:15:40 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.17651734429130608 on epoch=16
05/29/2022 20:15:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=17
05/29/2022 20:15:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=17
05/29/2022 20:15:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=17
05/29/2022 20:15:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.55 on epoch=18
05/29/2022 20:15:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=18
05/29/2022 20:16:05 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.16699282452707112 on epoch=18
05/29/2022 20:16:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=19
05/29/2022 20:16:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=19
05/29/2022 20:16:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.49 on epoch=19
05/29/2022 20:16:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=20
05/29/2022 20:16:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=20
05/29/2022 20:16:27 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=20
05/29/2022 20:16:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=21
05/29/2022 20:16:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.53 on epoch=21
05/29/2022 20:16:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=22
05/29/2022 20:16:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.51 on epoch=22
05/29/2022 20:16:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=22
05/29/2022 20:16:51 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=22
05/29/2022 20:16:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=23
05/29/2022 20:16:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=23
05/29/2022 20:16:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.51 on epoch=24
05/29/2022 20:17:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=24
05/29/2022 20:17:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=24
05/29/2022 20:17:16 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.1800497976968565 on epoch=24
05/29/2022 20:17:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=25
05/29/2022 20:17:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=25
05/29/2022 20:17:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.49 on epoch=26
05/29/2022 20:17:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=26
05/29/2022 20:17:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.41 on epoch=27
05/29/2022 20:17:41 - INFO - __main__ - Global step 650 Train loss 0.47 Classification-F1 0.21910497637127646 on epoch=27
05/29/2022 20:17:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=27
05/29/2022 20:17:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.42 on epoch=27
05/29/2022 20:17:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=28
05/29/2022 20:17:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.50 on epoch=28
05/29/2022 20:17:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.52 on epoch=29
05/29/2022 20:18:05 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.16699282452707112 on epoch=29
05/29/2022 20:18:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=29
05/29/2022 20:18:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=29
05/29/2022 20:18:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=30
05/29/2022 20:18:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=30
05/29/2022 20:18:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=31
05/29/2022 20:18:30 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.19164548600640444 on epoch=31
05/29/2022 20:18:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.51 on epoch=31
05/29/2022 20:18:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=32
05/29/2022 20:18:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=32
05/29/2022 20:18:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=32
05/29/2022 20:18:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=33
05/29/2022 20:18:55 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.24645999101410207 on epoch=33
05/29/2022 20:18:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=33
05/29/2022 20:19:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.49 on epoch=34
05/29/2022 20:19:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=34
05/29/2022 20:19:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=34
05/29/2022 20:19:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=35
05/29/2022 20:19:19 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.2317133154686719 on epoch=35
05/29/2022 20:19:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=35
05/29/2022 20:19:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.48 on epoch=36
05/29/2022 20:19:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.48 on epoch=36
05/29/2022 20:19:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.47 on epoch=37
05/29/2022 20:19:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.52 on epoch=37
05/29/2022 20:19:44 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.16699282452707112 on epoch=37
05/29/2022 20:19:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=37
05/29/2022 20:19:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.45 on epoch=38
05/29/2022 20:19:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.46 on epoch=38
05/29/2022 20:19:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.49 on epoch=39
05/29/2022 20:19:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=39
05/29/2022 20:20:05 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=39
05/29/2022 20:20:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.48 on epoch=39
05/29/2022 20:20:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=40
05/29/2022 20:20:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.47 on epoch=40
05/29/2022 20:20:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=41
05/29/2022 20:20:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.48 on epoch=41
05/29/2022 20:20:30 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.219453053073821 on epoch=41
05/29/2022 20:20:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=42
05/29/2022 20:20:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=42
05/29/2022 20:20:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=42
05/29/2022 20:20:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.46 on epoch=43
05/29/2022 20:20:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=43
05/29/2022 20:20:55 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.2363877164535654 on epoch=43
05/29/2022 20:20:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=44
05/29/2022 20:21:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=44
05/29/2022 20:21:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.49 on epoch=44
05/29/2022 20:21:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=45
05/29/2022 20:21:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.47 on epoch=45
05/29/2022 20:21:17 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.2045799705252153 on epoch=45
05/29/2022 20:21:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.43 on epoch=46
05/29/2022 20:21:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.47 on epoch=46
05/29/2022 20:21:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=47
05/29/2022 20:21:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.51 on epoch=47
05/29/2022 20:21:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=47
05/29/2022 20:21:41 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.1989722270338934 on epoch=47
05/29/2022 20:21:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=48
05/29/2022 20:21:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=48
05/29/2022 20:21:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.51 on epoch=49
05/29/2022 20:21:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=49
05/29/2022 20:21:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.48 on epoch=49
05/29/2022 20:22:06 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.25044833250448334 on epoch=49
05/29/2022 20:22:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.46 on epoch=50
05/29/2022 20:22:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=50
05/29/2022 20:22:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=51
05/29/2022 20:22:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.49 on epoch=51
05/29/2022 20:22:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.42 on epoch=52
05/29/2022 20:22:30 - INFO - __main__ - Global step 1250 Train loss 0.44 Classification-F1 0.24399323556729832 on epoch=52
05/29/2022 20:22:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.48 on epoch=52
05/29/2022 20:22:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=52
05/29/2022 20:22:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.45 on epoch=53
05/29/2022 20:22:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=53
05/29/2022 20:22:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.48 on epoch=54
05/29/2022 20:22:55 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.17273804346195953 on epoch=54
05/29/2022 20:22:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=54
05/29/2022 20:23:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.45 on epoch=54
05/29/2022 20:23:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=55
05/29/2022 20:23:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.43 on epoch=55
05/29/2022 20:23:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=56
05/29/2022 20:23:20 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.22926893894635827 on epoch=56
05/29/2022 20:23:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.46 on epoch=56
05/29/2022 20:23:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.49 on epoch=57
05/29/2022 20:23:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.46 on epoch=57
05/29/2022 20:23:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.44 on epoch=57
05/29/2022 20:23:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.46 on epoch=58
05/29/2022 20:23:44 - INFO - __main__ - Global step 1400 Train loss 0.46 Classification-F1 0.2523809523809524 on epoch=58
05/29/2022 20:23:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=58
05/29/2022 20:23:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.50 on epoch=59
05/29/2022 20:23:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.43 on epoch=59
05/29/2022 20:23:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.48 on epoch=59
05/29/2022 20:23:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.44 on epoch=60
05/29/2022 20:24:08 - INFO - __main__ - Global step 1450 Train loss 0.46 Classification-F1 0.213850666586985 on epoch=60
05/29/2022 20:24:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.45 on epoch=60
05/29/2022 20:24:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=61
05/29/2022 20:24:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=61
05/29/2022 20:24:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=62
05/29/2022 20:24:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.50 on epoch=62
05/29/2022 20:24:33 - INFO - __main__ - Global step 1500 Train loss 0.45 Classification-F1 0.20309029297793343 on epoch=62
05/29/2022 20:24:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=62
05/29/2022 20:24:38 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=63
05/29/2022 20:24:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.45 on epoch=63
05/29/2022 20:24:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.44 on epoch=64
05/29/2022 20:24:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=64
05/29/2022 20:24:57 - INFO - __main__ - Global step 1550 Train loss 0.43 Classification-F1 0.18789733935225583 on epoch=64
05/29/2022 20:25:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=64
05/29/2022 20:25:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=65
05/29/2022 20:25:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.45 on epoch=65
05/29/2022 20:25:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=66
05/29/2022 20:25:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.48 on epoch=66
05/29/2022 20:25:22 - INFO - __main__ - Global step 1600 Train loss 0.44 Classification-F1 0.25541347783472684 on epoch=66
05/29/2022 20:25:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.42 on epoch=67
05/29/2022 20:25:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.44 on epoch=67
05/29/2022 20:25:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=67
05/29/2022 20:25:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.47 on epoch=68
05/29/2022 20:25:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.46 on epoch=68
05/29/2022 20:25:43 - INFO - __main__ - Global step 1650 Train loss 0.44 Classification-F1 0.16633922724296005 on epoch=68
05/29/2022 20:25:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.43 on epoch=69
05/29/2022 20:25:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=69
05/29/2022 20:25:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.46 on epoch=69
05/29/2022 20:25:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=70
05/29/2022 20:25:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.43 on epoch=70
05/29/2022 20:26:04 - INFO - __main__ - Global step 1700 Train loss 0.44 Classification-F1 0.1881810228266921 on epoch=70
05/29/2022 20:26:07 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=71
05/29/2022 20:26:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.46 on epoch=71
05/29/2022 20:26:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.42 on epoch=72
05/29/2022 20:26:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=72
05/29/2022 20:26:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=72
05/29/2022 20:26:27 - INFO - __main__ - Global step 1750 Train loss 0.43 Classification-F1 0.20034312221273207 on epoch=72
05/29/2022 20:26:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.42 on epoch=73
05/29/2022 20:26:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=73
05/29/2022 20:26:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.47 on epoch=74
05/29/2022 20:26:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.45 on epoch=74
05/29/2022 20:26:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.47 on epoch=74
05/29/2022 20:26:52 - INFO - __main__ - Global step 1800 Train loss 0.45 Classification-F1 0.28742633981708604 on epoch=74
05/29/2022 20:26:52 - INFO - __main__ - Saving model with best Classification-F1: 0.27411826778684195 -> 0.28742633981708604 on epoch=74, global_step=1800
05/29/2022 20:26:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.44 on epoch=75
05/29/2022 20:26:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.44 on epoch=75
05/29/2022 20:27:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.43 on epoch=76
05/29/2022 20:27:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.42 on epoch=76
05/29/2022 20:27:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=77
05/29/2022 20:27:16 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.2422521772115268 on epoch=77
05/29/2022 20:27:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.43 on epoch=77
05/29/2022 20:27:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.41 on epoch=77
05/29/2022 20:27:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.47 on epoch=78
05/29/2022 20:27:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.45 on epoch=78
05/29/2022 20:27:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.42 on epoch=79
05/29/2022 20:27:38 - INFO - __main__ - Global step 1900 Train loss 0.44 Classification-F1 0.2010728111226742 on epoch=79
05/29/2022 20:27:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.45 on epoch=79
05/29/2022 20:27:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.44 on epoch=79
05/29/2022 20:27:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.46 on epoch=80
05/29/2022 20:27:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=80
05/29/2022 20:27:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.42 on epoch=81
05/29/2022 20:27:59 - INFO - __main__ - Global step 1950 Train loss 0.44 Classification-F1 0.24662768031189086 on epoch=81
05/29/2022 20:28:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=81
05/29/2022 20:28:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.39 on epoch=82
05/29/2022 20:28:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.44 on epoch=82
05/29/2022 20:28:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.44 on epoch=82
05/29/2022 20:28:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.48 on epoch=83
05/29/2022 20:28:21 - INFO - __main__ - Global step 2000 Train loss 0.44 Classification-F1 0.17273804346195953 on epoch=83
05/29/2022 20:28:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.47 on epoch=83
05/29/2022 20:28:26 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.48 on epoch=84
05/29/2022 20:28:29 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.44 on epoch=84
05/29/2022 20:28:31 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.44 on epoch=84
05/29/2022 20:28:34 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.45 on epoch=85
05/29/2022 20:28:42 - INFO - __main__ - Global step 2050 Train loss 0.46 Classification-F1 0.17651734429130608 on epoch=85
05/29/2022 20:28:45 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.38 on epoch=85
05/29/2022 20:28:48 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.45 on epoch=86
05/29/2022 20:28:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.39 on epoch=86
05/29/2022 20:28:53 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.44 on epoch=87
05/29/2022 20:28:55 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.41 on epoch=87
05/29/2022 20:29:04 - INFO - __main__ - Global step 2100 Train loss 0.41 Classification-F1 0.1822643665219694 on epoch=87
05/29/2022 20:29:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.41 on epoch=87
05/29/2022 20:29:09 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.44 on epoch=88
05/29/2022 20:29:12 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.45 on epoch=88
05/29/2022 20:29:14 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.44 on epoch=89
05/29/2022 20:29:17 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.42 on epoch=89
05/29/2022 20:29:28 - INFO - __main__ - Global step 2150 Train loss 0.43 Classification-F1 0.19404535385853752 on epoch=89
05/29/2022 20:29:30 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.43 on epoch=89
05/29/2022 20:29:33 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.42 on epoch=90
05/29/2022 20:29:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.43 on epoch=90
05/29/2022 20:29:38 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.45 on epoch=91
05/29/2022 20:29:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.47 on epoch=91
05/29/2022 20:29:52 - INFO - __main__ - Global step 2200 Train loss 0.44 Classification-F1 0.23200596586408503 on epoch=91
05/29/2022 20:29:54 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.41 on epoch=92
05/29/2022 20:29:57 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.44 on epoch=92
05/29/2022 20:29:59 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.45 on epoch=92
05/29/2022 20:30:02 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.44 on epoch=93
05/29/2022 20:30:05 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.41 on epoch=93
05/29/2022 20:30:13 - INFO - __main__ - Global step 2250 Train loss 0.43 Classification-F1 0.23208297811878378 on epoch=93
05/29/2022 20:30:16 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.42 on epoch=94
05/29/2022 20:30:18 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.42 on epoch=94
05/29/2022 20:30:21 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.40 on epoch=94
05/29/2022 20:30:24 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.45 on epoch=95
05/29/2022 20:30:26 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.42 on epoch=95
05/29/2022 20:30:35 - INFO - __main__ - Global step 2300 Train loss 0.42 Classification-F1 0.24784158124839265 on epoch=95
05/29/2022 20:30:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.45 on epoch=96
05/29/2022 20:30:40 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.48 on epoch=96
05/29/2022 20:30:43 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.43 on epoch=97
05/29/2022 20:30:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.47 on epoch=97
05/29/2022 20:30:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.42 on epoch=97
05/29/2022 20:30:55 - INFO - __main__ - Global step 2350 Train loss 0.45 Classification-F1 0.2727443219393748 on epoch=97
05/29/2022 20:30:58 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.46 on epoch=98
05/29/2022 20:31:01 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.40 on epoch=98
05/29/2022 20:31:03 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.45 on epoch=99
05/29/2022 20:31:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.43 on epoch=99
05/29/2022 20:31:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.43 on epoch=99
05/29/2022 20:31:17 - INFO - __main__ - Global step 2400 Train loss 0.44 Classification-F1 0.2391050391050391 on epoch=99
05/29/2022 20:31:19 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.45 on epoch=100
05/29/2022 20:31:22 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.44 on epoch=100
05/29/2022 20:31:25 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.42 on epoch=101
05/29/2022 20:31:27 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.46 on epoch=101
05/29/2022 20:31:30 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.41 on epoch=102
05/29/2022 20:31:38 - INFO - __main__ - Global step 2450 Train loss 0.44 Classification-F1 0.16699282452707112 on epoch=102
05/29/2022 20:31:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.45 on epoch=102
05/29/2022 20:31:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.39 on epoch=102
05/29/2022 20:31:46 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.44 on epoch=103
05/29/2022 20:31:49 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.41 on epoch=103
05/29/2022 20:31:51 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.43 on epoch=104
05/29/2022 20:32:02 - INFO - __main__ - Global step 2500 Train loss 0.42 Classification-F1 0.17782710198613258 on epoch=104
05/29/2022 20:32:05 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.43 on epoch=104
05/29/2022 20:32:07 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.45 on epoch=104
05/29/2022 20:32:10 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.46 on epoch=105
05/29/2022 20:32:13 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.42 on epoch=105
05/29/2022 20:32:15 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.43 on epoch=106
05/29/2022 20:32:29 - INFO - __main__ - Global step 2550 Train loss 0.44 Classification-F1 0.3217633148516846 on epoch=106
05/29/2022 20:32:29 - INFO - __main__ - Saving model with best Classification-F1: 0.28742633981708604 -> 0.3217633148516846 on epoch=106, global_step=2550
05/29/2022 20:32:32 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.41 on epoch=106
05/29/2022 20:32:34 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.45 on epoch=107
05/29/2022 20:32:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.42 on epoch=107
05/29/2022 20:32:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.40 on epoch=107
05/29/2022 20:32:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.47 on epoch=108
05/29/2022 20:32:54 - INFO - __main__ - Global step 2600 Train loss 0.43 Classification-F1 0.1911463748248923 on epoch=108
05/29/2022 20:32:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.45 on epoch=108
05/29/2022 20:32:59 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.41 on epoch=109
05/29/2022 20:33:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.42 on epoch=109
05/29/2022 20:33:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.40 on epoch=109
05/29/2022 20:33:07 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.45 on epoch=110
05/29/2022 20:33:17 - INFO - __main__ - Global step 2650 Train loss 0.42 Classification-F1 0.29344487870625224 on epoch=110
05/29/2022 20:33:20 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.40 on epoch=110
05/29/2022 20:33:22 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.43 on epoch=111
05/29/2022 20:33:25 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.41 on epoch=111
05/29/2022 20:33:28 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.41 on epoch=112
05/29/2022 20:33:30 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.46 on epoch=112
05/29/2022 20:33:39 - INFO - __main__ - Global step 2700 Train loss 0.42 Classification-F1 0.21591283562891617 on epoch=112
05/29/2022 20:33:41 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.42 on epoch=112
05/29/2022 20:33:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.41 on epoch=113
05/29/2022 20:33:47 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.42 on epoch=113
05/29/2022 20:33:49 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.46 on epoch=114
05/29/2022 20:33:52 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.39 on epoch=114
05/29/2022 20:34:01 - INFO - __main__ - Global step 2750 Train loss 0.42 Classification-F1 0.2713987341208954 on epoch=114
05/29/2022 20:34:03 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.46 on epoch=114
05/29/2022 20:34:06 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.40 on epoch=115
05/29/2022 20:34:09 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.40 on epoch=115
05/29/2022 20:34:11 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.39 on epoch=116
05/29/2022 20:34:14 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.43 on epoch=116
05/29/2022 20:34:26 - INFO - __main__ - Global step 2800 Train loss 0.41 Classification-F1 0.22680421052683727 on epoch=116
05/29/2022 20:34:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.40 on epoch=117
05/29/2022 20:34:31 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.46 on epoch=117
05/29/2022 20:34:34 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.40 on epoch=117
05/29/2022 20:34:36 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.45 on epoch=118
05/29/2022 20:34:39 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.45 on epoch=118
05/29/2022 20:34:57 - INFO - __main__ - Global step 2850 Train loss 0.43 Classification-F1 0.284032007834545 on epoch=118
05/29/2022 20:35:00 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.44 on epoch=119
05/29/2022 20:35:02 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.41 on epoch=119
05/29/2022 20:35:05 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.42 on epoch=119
05/29/2022 20:35:08 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.41 on epoch=120
05/29/2022 20:35:10 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.40 on epoch=120
05/29/2022 20:35:21 - INFO - __main__ - Global step 2900 Train loss 0.42 Classification-F1 0.23591777459180774 on epoch=120
05/29/2022 20:35:23 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.42 on epoch=121
05/29/2022 20:35:26 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.37 on epoch=121
05/29/2022 20:35:28 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.45 on epoch=122
05/29/2022 20:35:31 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.43 on epoch=122
05/29/2022 20:35:34 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.39 on epoch=122
05/29/2022 20:35:43 - INFO - __main__ - Global step 2950 Train loss 0.41 Classification-F1 0.22573998260272768 on epoch=122
05/29/2022 20:35:46 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.43 on epoch=123
05/29/2022 20:35:48 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.40 on epoch=123
05/29/2022 20:35:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.45 on epoch=124
05/29/2022 20:35:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.38 on epoch=124
05/29/2022 20:35:56 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.42 on epoch=124
05/29/2022 20:35:58 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 20:35:58 - INFO - __main__ - Printing 3 examples
05/29/2022 20:35:58 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/29/2022 20:35:58 - INFO - __main__ - ['contradiction']
05/29/2022 20:35:58 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/29/2022 20:35:58 - INFO - __main__ - ['contradiction']
05/29/2022 20:35:58 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/29/2022 20:35:58 - INFO - __main__ - ['contradiction']
05/29/2022 20:35:58 - INFO - __main__ - Tokenizing Input ...
05/29/2022 20:35:58 - INFO - __main__ - Tokenizing Output ...
05/29/2022 20:35:58 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 20:35:58 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 20:35:58 - INFO - __main__ - Printing 3 examples
05/29/2022 20:35:58 - INFO - __main__ -  [anli] premise: Never Shout Never is an EP by Never Shout Never which was released on December 8, 2009. The physical release is sold exclusively at Hot Topic. The EP features two songs from his then upcoming Sire Records full-length debut, "What is Love?", one song that is a B-side from "The Summer EP" and one live track. [SEP] hypothesis:  The EP features two live tracks. 
05/29/2022 20:35:58 - INFO - __main__ - ['contradiction']
05/29/2022 20:35:58 - INFO - __main__ -  [anli] premise: The Hound of the Baskervilles is a 1978 British comedy film spoofing "The Hound of the Baskervilles" by Sir Arthur Conan Doyle. It starred Peter Cook as Sherlock Holmes and Dudley Moore as Dr. Watson. A number of other well-known British comedy actors appeared in the film including Terry-Thomas (in his final screen appearance), Kenneth Williams and Denholm Elliott. [SEP] hypothesis: The Hound of the Baskervilles is a american food dish.
05/29/2022 20:35:58 - INFO - __main__ - ['contradiction']
05/29/2022 20:35:58 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts is most famous for his music career.
05/29/2022 20:35:58 - INFO - __main__ - ['contradiction']
05/29/2022 20:35:58 - INFO - __main__ - Tokenizing Input ...
05/29/2022 20:35:59 - INFO - __main__ - Tokenizing Output ...
05/29/2022 20:35:59 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 20:36:06 - INFO - __main__ - Global step 3000 Train loss 0.42 Classification-F1 0.20166546380446726 on epoch=124
05/29/2022 20:36:06 - INFO - __main__ - save last model!
05/29/2022 20:36:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 20:36:06 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 20:36:06 - INFO - __main__ - Printing 3 examples
05/29/2022 20:36:06 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 20:36:06 - INFO - __main__ - ['contradiction']
05/29/2022 20:36:06 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 20:36:06 - INFO - __main__ - ['entailment']
05/29/2022 20:36:06 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 20:36:06 - INFO - __main__ - ['contradiction']
05/29/2022 20:36:06 - INFO - __main__ - Tokenizing Input ...
05/29/2022 20:36:07 - INFO - __main__ - Tokenizing Output ...
05/29/2022 20:36:08 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 20:36:17 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 20:36:17 - INFO - __main__ - task name: anli
05/29/2022 20:36:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 20:36:19 - INFO - __main__ - Starting training!
05/29/2022 20:36:33 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_87_0.4_8_predictions.txt
05/29/2022 20:36:33 - INFO - __main__ - Classification-F1 on test data: 0.2120
05/29/2022 20:36:33 - INFO - __main__ - prefix=anli_128_87, lr=0.4, bsz=8, dev_performance=0.3217633148516846, test_performance=0.21204707187511354
05/29/2022 20:36:33 - INFO - __main__ - Running ... prefix=anli_128_87, lr=0.3, bsz=8 ...
05/29/2022 20:36:34 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 20:36:34 - INFO - __main__ - Printing 3 examples
05/29/2022 20:36:34 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/29/2022 20:36:34 - INFO - __main__ - ['contradiction']
05/29/2022 20:36:34 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/29/2022 20:36:34 - INFO - __main__ - ['contradiction']
05/29/2022 20:36:34 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/29/2022 20:36:34 - INFO - __main__ - ['contradiction']
05/29/2022 20:36:34 - INFO - __main__ - Tokenizing Input ...
05/29/2022 20:36:34 - INFO - __main__ - Tokenizing Output ...
05/29/2022 20:36:35 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 20:36:35 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 20:36:35 - INFO - __main__ - Printing 3 examples
05/29/2022 20:36:35 - INFO - __main__ -  [anli] premise: Never Shout Never is an EP by Never Shout Never which was released on December 8, 2009. The physical release is sold exclusively at Hot Topic. The EP features two songs from his then upcoming Sire Records full-length debut, "What is Love?", one song that is a B-side from "The Summer EP" and one live track. [SEP] hypothesis:  The EP features two live tracks. 
05/29/2022 20:36:35 - INFO - __main__ - ['contradiction']
05/29/2022 20:36:35 - INFO - __main__ -  [anli] premise: The Hound of the Baskervilles is a 1978 British comedy film spoofing "The Hound of the Baskervilles" by Sir Arthur Conan Doyle. It starred Peter Cook as Sherlock Holmes and Dudley Moore as Dr. Watson. A number of other well-known British comedy actors appeared in the film including Terry-Thomas (in his final screen appearance), Kenneth Williams and Denholm Elliott. [SEP] hypothesis: The Hound of the Baskervilles is a american food dish.
05/29/2022 20:36:35 - INFO - __main__ - ['contradiction']
05/29/2022 20:36:35 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts is most famous for his music career.
05/29/2022 20:36:35 - INFO - __main__ - ['contradiction']
05/29/2022 20:36:35 - INFO - __main__ - Tokenizing Input ...
05/29/2022 20:36:35 - INFO - __main__ - Tokenizing Output ...
05/29/2022 20:36:35 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 20:36:54 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 20:36:54 - INFO - __main__ - task name: anli
05/29/2022 20:36:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 20:36:55 - INFO - __main__ - Starting training!
05/29/2022 20:36:58 - INFO - __main__ - Step 10 Global step 10 Train loss 6.77 on epoch=0
05/29/2022 20:37:00 - INFO - __main__ - Step 20 Global step 20 Train loss 3.47 on epoch=0
05/29/2022 20:37:03 - INFO - __main__ - Step 30 Global step 30 Train loss 1.86 on epoch=1
05/29/2022 20:37:06 - INFO - __main__ - Step 40 Global step 40 Train loss 1.19 on epoch=1
05/29/2022 20:37:08 - INFO - __main__ - Step 50 Global step 50 Train loss 0.84 on epoch=2
05/29/2022 20:37:20 - INFO - __main__ - Global step 50 Train loss 2.83 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 20:37:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 20:37:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.89 on epoch=2
05/29/2022 20:37:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.71 on epoch=2
05/29/2022 20:37:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.81 on epoch=3
05/29/2022 20:37:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.74 on epoch=3
05/29/2022 20:37:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.68 on epoch=4
05/29/2022 20:37:43 - INFO - __main__ - Global step 100 Train loss 0.76 Classification-F1 0.1949881632888735 on epoch=4
05/29/2022 20:37:43 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1949881632888735 on epoch=4, global_step=100
05/29/2022 20:37:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.68 on epoch=4
05/29/2022 20:37:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.65 on epoch=4
05/29/2022 20:37:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.63 on epoch=5
05/29/2022 20:37:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=5
05/29/2022 20:37:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.54 on epoch=6
05/29/2022 20:38:07 - INFO - __main__ - Global step 150 Train loss 0.61 Classification-F1 0.2619092899611688 on epoch=6
05/29/2022 20:38:07 - INFO - __main__ - Saving model with best Classification-F1: 0.1949881632888735 -> 0.2619092899611688 on epoch=6, global_step=150
05/29/2022 20:38:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.59 on epoch=6
05/29/2022 20:38:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=7
05/29/2022 20:38:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.63 on epoch=7
05/29/2022 20:38:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=7
05/29/2022 20:38:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.54 on epoch=8
05/29/2022 20:38:29 - INFO - __main__ - Global step 200 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=8
05/29/2022 20:38:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=8
05/29/2022 20:38:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.56 on epoch=9
05/29/2022 20:38:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.55 on epoch=9
05/29/2022 20:38:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=9
05/29/2022 20:38:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.57 on epoch=10
05/29/2022 20:38:51 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=10
05/29/2022 20:38:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=10
05/29/2022 20:38:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=11
05/29/2022 20:38:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.54 on epoch=11
05/29/2022 20:39:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=12
05/29/2022 20:39:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.57 on epoch=12
05/29/2022 20:39:12 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=12
05/29/2022 20:39:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=12
05/29/2022 20:39:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.53 on epoch=13
05/29/2022 20:39:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.54 on epoch=13
05/29/2022 20:39:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.53 on epoch=14
05/29/2022 20:39:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=14
05/29/2022 20:39:36 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.19403345698489174 on epoch=14
05/29/2022 20:39:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.52 on epoch=14
05/29/2022 20:39:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=15
05/29/2022 20:39:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.53 on epoch=15
05/29/2022 20:39:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.55 on epoch=16
05/29/2022 20:39:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=16
05/29/2022 20:40:00 - INFO - __main__ - Global step 400 Train loss 0.52 Classification-F1 0.1818181818181818 on epoch=16
05/29/2022 20:40:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=17
05/29/2022 20:40:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.51 on epoch=17
05/29/2022 20:40:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=17
05/29/2022 20:40:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=18
05/29/2022 20:40:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=18
05/29/2022 20:40:22 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.1721607831834019 on epoch=18
05/29/2022 20:40:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=19
05/29/2022 20:40:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.51 on epoch=19
05/29/2022 20:40:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=19
05/29/2022 20:40:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.57 on epoch=20
05/29/2022 20:40:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.53 on epoch=20
05/29/2022 20:40:44 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.1818181818181818 on epoch=20
05/29/2022 20:40:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=21
05/29/2022 20:40:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.46 on epoch=21
05/29/2022 20:40:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=22
05/29/2022 20:40:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.52 on epoch=22
05/29/2022 20:40:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=22
05/29/2022 20:41:08 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=22
05/29/2022 20:41:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=23
05/29/2022 20:41:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=23
05/29/2022 20:41:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=24
05/29/2022 20:41:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=24
05/29/2022 20:41:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=24
05/29/2022 20:41:30 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=24
05/29/2022 20:41:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.51 on epoch=25
05/29/2022 20:41:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=25
05/29/2022 20:41:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=26
05/29/2022 20:41:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=26
05/29/2022 20:41:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=27
05/29/2022 20:41:53 - INFO - __main__ - Global step 650 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=27
05/29/2022 20:41:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.52 on epoch=27
05/29/2022 20:41:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=27
05/29/2022 20:42:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=28
05/29/2022 20:42:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.51 on epoch=28
05/29/2022 20:42:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.52 on epoch=29
05/29/2022 20:42:18 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.1775766716943188 on epoch=29
05/29/2022 20:42:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.49 on epoch=29
05/29/2022 20:42:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=29
05/29/2022 20:42:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=30
05/29/2022 20:42:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=30
05/29/2022 20:42:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=31
05/29/2022 20:42:43 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.292917471116423 on epoch=31
05/29/2022 20:42:43 - INFO - __main__ - Saving model with best Classification-F1: 0.2619092899611688 -> 0.292917471116423 on epoch=31, global_step=750
05/29/2022 20:42:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.49 on epoch=31
05/29/2022 20:42:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=32
05/29/2022 20:42:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.49 on epoch=32
05/29/2022 20:42:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=32
05/29/2022 20:42:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=33
05/29/2022 20:43:08 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.19838440442216662 on epoch=33
05/29/2022 20:43:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=33
05/29/2022 20:43:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.46 on epoch=34
05/29/2022 20:43:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=34
05/29/2022 20:43:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=34
05/29/2022 20:43:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=35
05/29/2022 20:43:33 - INFO - __main__ - Global step 850 Train loss 0.46 Classification-F1 0.21808937076907906 on epoch=35
05/29/2022 20:43:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.46 on epoch=35
05/29/2022 20:43:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=36
05/29/2022 20:43:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=36
05/29/2022 20:43:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=37
05/29/2022 20:43:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.49 on epoch=37
05/29/2022 20:43:56 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.18291616051030557 on epoch=37
05/29/2022 20:43:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=37
05/29/2022 20:44:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=38
05/29/2022 20:44:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=38
05/29/2022 20:44:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.52 on epoch=39
05/29/2022 20:44:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.51 on epoch=39
05/29/2022 20:44:21 - INFO - __main__ - Global step 950 Train loss 0.48 Classification-F1 0.1721607831834019 on epoch=39
05/29/2022 20:44:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=39
05/29/2022 20:44:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=40
05/29/2022 20:44:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.51 on epoch=40
05/29/2022 20:44:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.53 on epoch=41
05/29/2022 20:44:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.49 on epoch=41
05/29/2022 20:44:45 - INFO - __main__ - Global step 1000 Train loss 0.49 Classification-F1 0.1929000858860578 on epoch=41
05/29/2022 20:44:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.44 on epoch=42
05/29/2022 20:44:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.49 on epoch=42
05/29/2022 20:44:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=42
05/29/2022 20:44:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=43
05/29/2022 20:44:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.48 on epoch=43
05/29/2022 20:45:09 - INFO - __main__ - Global step 1050 Train loss 0.46 Classification-F1 0.22987218463792314 on epoch=43
05/29/2022 20:45:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=44
05/29/2022 20:45:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.52 on epoch=44
05/29/2022 20:45:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.48 on epoch=44
05/29/2022 20:45:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=45
05/29/2022 20:45:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.46 on epoch=45
05/29/2022 20:45:33 - INFO - __main__ - Global step 1100 Train loss 0.47 Classification-F1 0.2267502612330199 on epoch=45
05/29/2022 20:45:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=46
05/29/2022 20:45:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.49 on epoch=46
05/29/2022 20:45:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=47
05/29/2022 20:45:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.49 on epoch=47
05/29/2022 20:45:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=47
05/29/2022 20:45:57 - INFO - __main__ - Global step 1150 Train loss 0.47 Classification-F1 0.1721607831834019 on epoch=47
05/29/2022 20:46:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=48
05/29/2022 20:46:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.76 on epoch=48
05/29/2022 20:46:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.99 on epoch=49
05/29/2022 20:46:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.49 on epoch=49
05/29/2022 20:46:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=49
05/29/2022 20:46:21 - INFO - __main__ - Global step 1200 Train loss 0.82 Classification-F1 0.2954904656273107 on epoch=49
05/29/2022 20:46:21 - INFO - __main__ - Saving model with best Classification-F1: 0.292917471116423 -> 0.2954904656273107 on epoch=49, global_step=1200
05/29/2022 20:46:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=50
05/29/2022 20:46:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.47 on epoch=50
05/29/2022 20:46:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.43 on epoch=51
05/29/2022 20:46:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=51
05/29/2022 20:46:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=52
05/29/2022 20:46:46 - INFO - __main__ - Global step 1250 Train loss 0.44 Classification-F1 0.29934879642914075 on epoch=52
05/29/2022 20:46:46 - INFO - __main__ - Saving model with best Classification-F1: 0.2954904656273107 -> 0.29934879642914075 on epoch=52, global_step=1250
05/29/2022 20:46:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.51 on epoch=52
05/29/2022 20:46:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.46 on epoch=52
05/29/2022 20:46:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.47 on epoch=53
05/29/2022 20:46:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.44 on epoch=53
05/29/2022 20:46:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.48 on epoch=54
05/29/2022 20:47:10 - INFO - __main__ - Global step 1300 Train loss 0.47 Classification-F1 0.17790539519419676 on epoch=54
05/29/2022 20:47:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.46 on epoch=54
05/29/2022 20:47:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.40 on epoch=54
05/29/2022 20:47:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=55
05/29/2022 20:47:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=55
05/29/2022 20:47:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=56
05/29/2022 20:47:35 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.2715021141987434 on epoch=56
05/29/2022 20:47:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.48 on epoch=56
05/29/2022 20:47:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.43 on epoch=57
05/29/2022 20:47:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.45 on epoch=57
05/29/2022 20:47:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=57
05/29/2022 20:47:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.48 on epoch=58
05/29/2022 20:47:59 - INFO - __main__ - Global step 1400 Train loss 0.44 Classification-F1 0.3168886937002879 on epoch=58
05/29/2022 20:47:59 - INFO - __main__ - Saving model with best Classification-F1: 0.29934879642914075 -> 0.3168886937002879 on epoch=58, global_step=1400
05/29/2022 20:48:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=58
05/29/2022 20:48:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.48 on epoch=59
05/29/2022 20:48:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.46 on epoch=59
05/29/2022 20:48:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=59
05/29/2022 20:48:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.43 on epoch=60
05/29/2022 20:48:24 - INFO - __main__ - Global step 1450 Train loss 0.45 Classification-F1 0.28598243895238556 on epoch=60
05/29/2022 20:48:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=60
05/29/2022 20:48:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=61
05/29/2022 20:48:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=61
05/29/2022 20:48:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.44 on epoch=62
05/29/2022 20:48:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.46 on epoch=62
05/29/2022 20:48:49 - INFO - __main__ - Global step 1500 Train loss 0.44 Classification-F1 0.29222972833179817 on epoch=62
05/29/2022 20:48:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.38 on epoch=62
05/29/2022 20:48:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.51 on epoch=63
05/29/2022 20:48:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=63
05/29/2022 20:49:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=64
05/29/2022 20:49:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=64
05/29/2022 20:49:14 - INFO - __main__ - Global step 1550 Train loss 0.44 Classification-F1 0.27383352733595845 on epoch=64
05/29/2022 20:49:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.44 on epoch=64
05/29/2022 20:49:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.48 on epoch=65
05/29/2022 20:49:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=65
05/29/2022 20:49:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.50 on epoch=66
05/29/2022 20:49:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.49 on epoch=66
05/29/2022 20:49:38 - INFO - __main__ - Global step 1600 Train loss 0.47 Classification-F1 0.3212122805864177 on epoch=66
05/29/2022 20:49:38 - INFO - __main__ - Saving model with best Classification-F1: 0.3168886937002879 -> 0.3212122805864177 on epoch=66, global_step=1600
05/29/2022 20:49:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.43 on epoch=67
05/29/2022 20:49:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=67
05/29/2022 20:49:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=67
05/29/2022 20:49:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=68
05/29/2022 20:49:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=68
05/29/2022 20:50:03 - INFO - __main__ - Global step 1650 Train loss 0.43 Classification-F1 0.2701974506350944 on epoch=68
05/29/2022 20:50:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.49 on epoch=69
05/29/2022 20:50:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.44 on epoch=69
05/29/2022 20:50:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.46 on epoch=69
05/29/2022 20:50:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.49 on epoch=70
05/29/2022 20:50:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.44 on epoch=70
05/29/2022 20:50:28 - INFO - __main__ - Global step 1700 Train loss 0.46 Classification-F1 0.1816820276497696 on epoch=70
05/29/2022 20:50:31 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.45 on epoch=71
05/29/2022 20:50:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.48 on epoch=71
05/29/2022 20:50:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.45 on epoch=72
05/29/2022 20:50:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.46 on epoch=72
05/29/2022 20:50:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.45 on epoch=72
05/29/2022 20:50:52 - INFO - __main__ - Global step 1750 Train loss 0.46 Classification-F1 0.3020829373356993 on epoch=72
05/29/2022 20:50:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.45 on epoch=73
05/29/2022 20:50:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.42 on epoch=73
05/29/2022 20:51:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.45 on epoch=74
05/29/2022 20:51:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.41 on epoch=74
05/29/2022 20:51:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=74
05/29/2022 20:51:17 - INFO - __main__ - Global step 1800 Train loss 0.44 Classification-F1 0.3220585204455272 on epoch=74
05/29/2022 20:51:17 - INFO - __main__ - Saving model with best Classification-F1: 0.3212122805864177 -> 0.3220585204455272 on epoch=74, global_step=1800
05/29/2022 20:51:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=75
05/29/2022 20:51:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.43 on epoch=75
05/29/2022 20:51:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.41 on epoch=76
05/29/2022 20:51:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.46 on epoch=76
05/29/2022 20:51:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.46 on epoch=77
05/29/2022 20:51:42 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.26408743773775983 on epoch=77
05/29/2022 20:51:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.48 on epoch=77
05/29/2022 20:51:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=77
05/29/2022 20:51:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.46 on epoch=78
05/29/2022 20:51:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.44 on epoch=78
05/29/2022 20:51:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=79
05/29/2022 20:52:06 - INFO - __main__ - Global step 1900 Train loss 0.45 Classification-F1 0.18495175848117026 on epoch=79
05/29/2022 20:52:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.44 on epoch=79
05/29/2022 20:52:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.46 on epoch=79
05/29/2022 20:52:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.42 on epoch=80
05/29/2022 20:52:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.45 on epoch=80
05/29/2022 20:52:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.42 on epoch=81
05/29/2022 20:52:31 - INFO - __main__ - Global step 1950 Train loss 0.44 Classification-F1 0.3034738104801799 on epoch=81
05/29/2022 20:52:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=81
05/29/2022 20:52:36 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.41 on epoch=82
05/29/2022 20:52:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.48 on epoch=82
05/29/2022 20:52:42 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=82
05/29/2022 20:52:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.47 on epoch=83
05/29/2022 20:52:55 - INFO - __main__ - Global step 2000 Train loss 0.45 Classification-F1 0.22842225940385452 on epoch=83
05/29/2022 20:52:58 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.44 on epoch=83
05/29/2022 20:53:01 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.43 on epoch=84
05/29/2022 20:53:03 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.40 on epoch=84
05/29/2022 20:53:06 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.43 on epoch=84
05/29/2022 20:53:09 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.47 on epoch=85
05/29/2022 20:53:20 - INFO - __main__ - Global step 2050 Train loss 0.43 Classification-F1 0.16568819308545338 on epoch=85
05/29/2022 20:53:22 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.41 on epoch=85
05/29/2022 20:53:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.43 on epoch=86
05/29/2022 20:53:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.42 on epoch=86
05/29/2022 20:53:30 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.44 on epoch=87
05/29/2022 20:53:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.48 on epoch=87
05/29/2022 20:53:44 - INFO - __main__ - Global step 2100 Train loss 0.44 Classification-F1 0.20309592408251342 on epoch=87
05/29/2022 20:53:47 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.40 on epoch=87
05/29/2022 20:53:50 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.47 on epoch=88
05/29/2022 20:53:52 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.42 on epoch=88
05/29/2022 20:53:55 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.45 on epoch=89
05/29/2022 20:53:58 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.43 on epoch=89
05/29/2022 20:54:09 - INFO - __main__ - Global step 2150 Train loss 0.43 Classification-F1 0.18225080132932178 on epoch=89
05/29/2022 20:54:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.42 on epoch=89
05/29/2022 20:54:14 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.44 on epoch=90
05/29/2022 20:54:17 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.43 on epoch=90
05/29/2022 20:54:19 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.43 on epoch=91
05/29/2022 20:54:22 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.45 on epoch=91
05/29/2022 20:54:34 - INFO - __main__ - Global step 2200 Train loss 0.44 Classification-F1 0.2719619709796765 on epoch=91
05/29/2022 20:54:36 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.41 on epoch=92
05/29/2022 20:54:39 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.44 on epoch=92
05/29/2022 20:54:42 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.44 on epoch=92
05/29/2022 20:54:45 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.46 on epoch=93
05/29/2022 20:54:47 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.43 on epoch=93
05/29/2022 20:54:58 - INFO - __main__ - Global step 2250 Train loss 0.44 Classification-F1 0.22670125922926568 on epoch=93
05/29/2022 20:55:01 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.48 on epoch=94
05/29/2022 20:55:04 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.43 on epoch=94
05/29/2022 20:55:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.44 on epoch=94
05/29/2022 20:55:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.49 on epoch=95
05/29/2022 20:55:12 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.41 on epoch=95
05/29/2022 20:55:23 - INFO - __main__ - Global step 2300 Train loss 0.45 Classification-F1 0.26415845755817563 on epoch=95
05/29/2022 20:55:26 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.38 on epoch=96
05/29/2022 20:55:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.40 on epoch=96
05/29/2022 20:55:31 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.41 on epoch=97
05/29/2022 20:55:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.47 on epoch=97
05/29/2022 20:55:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.42 on epoch=97
05/29/2022 20:55:47 - INFO - __main__ - Global step 2350 Train loss 0.42 Classification-F1 0.23588706362126519 on epoch=97
05/29/2022 20:55:50 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.50 on epoch=98
05/29/2022 20:55:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.40 on epoch=98
05/29/2022 20:55:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.43 on epoch=99
05/29/2022 20:55:58 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.46 on epoch=99
05/29/2022 20:56:01 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.42 on epoch=99
05/29/2022 20:56:12 - INFO - __main__ - Global step 2400 Train loss 0.44 Classification-F1 0.25248379995566966 on epoch=99
05/29/2022 20:56:15 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.49 on epoch=100
05/29/2022 20:56:17 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.42 on epoch=100
05/29/2022 20:56:20 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.45 on epoch=101
05/29/2022 20:56:23 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.46 on epoch=101
05/29/2022 20:56:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.44 on epoch=102
05/29/2022 20:56:37 - INFO - __main__ - Global step 2450 Train loss 0.45 Classification-F1 0.2369260683460845 on epoch=102
05/29/2022 20:56:40 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.49 on epoch=102
05/29/2022 20:56:42 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.44 on epoch=102
05/29/2022 20:56:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.48 on epoch=103
05/29/2022 20:56:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.40 on epoch=103
05/29/2022 20:56:50 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.46 on epoch=104
05/29/2022 20:57:01 - INFO - __main__ - Global step 2500 Train loss 0.46 Classification-F1 0.17790539519419676 on epoch=104
05/29/2022 20:57:04 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.40 on epoch=104
05/29/2022 20:57:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.44 on epoch=104
05/29/2022 20:57:09 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.48 on epoch=105
05/29/2022 20:57:12 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.41 on epoch=105
05/29/2022 20:57:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.43 on epoch=106
05/29/2022 20:57:26 - INFO - __main__ - Global step 2550 Train loss 0.43 Classification-F1 0.22568814646738397 on epoch=106
05/29/2022 20:57:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.40 on epoch=106
05/29/2022 20:57:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.45 on epoch=107
05/29/2022 20:57:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.50 on epoch=107
05/29/2022 20:57:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.42 on epoch=107
05/29/2022 20:57:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.45 on epoch=108
05/29/2022 20:57:51 - INFO - __main__ - Global step 2600 Train loss 0.44 Classification-F1 0.2251867296821425 on epoch=108
05/29/2022 20:57:54 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.44 on epoch=108
05/29/2022 20:57:57 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.47 on epoch=109
05/29/2022 20:57:59 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.44 on epoch=109
05/29/2022 20:58:02 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.42 on epoch=109
05/29/2022 20:58:05 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.47 on epoch=110
05/29/2022 20:58:16 - INFO - __main__ - Global step 2650 Train loss 0.45 Classification-F1 0.28119588004739765 on epoch=110
05/29/2022 20:58:19 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.44 on epoch=110
05/29/2022 20:58:21 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.41 on epoch=111
05/29/2022 20:58:24 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.44 on epoch=111
05/29/2022 20:58:27 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.45 on epoch=112
05/29/2022 20:58:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.46 on epoch=112
05/29/2022 20:58:41 - INFO - __main__ - Global step 2700 Train loss 0.44 Classification-F1 0.290250322058652 on epoch=112
05/29/2022 20:58:43 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.43 on epoch=112
05/29/2022 20:58:46 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.43 on epoch=113
05/29/2022 20:58:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.47 on epoch=113
05/29/2022 20:58:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.46 on epoch=114
05/29/2022 20:58:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.42 on epoch=114
05/29/2022 20:59:05 - INFO - __main__ - Global step 2750 Train loss 0.44 Classification-F1 0.23641098840557936 on epoch=114
05/29/2022 20:59:08 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.40 on epoch=114
05/29/2022 20:59:11 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.43 on epoch=115
05/29/2022 20:59:13 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.42 on epoch=115
05/29/2022 20:59:16 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.45 on epoch=116
05/29/2022 20:59:19 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.47 on epoch=116
05/29/2022 20:59:32 - INFO - __main__ - Global step 2800 Train loss 0.44 Classification-F1 0.31738631039915494 on epoch=116
05/29/2022 20:59:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.44 on epoch=117
05/29/2022 20:59:37 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.44 on epoch=117
05/29/2022 20:59:40 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.41 on epoch=117
05/29/2022 20:59:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.46 on epoch=118
05/29/2022 20:59:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.42 on epoch=118
05/29/2022 20:59:58 - INFO - __main__ - Global step 2850 Train loss 0.43 Classification-F1 0.28077273555677057 on epoch=118
05/29/2022 21:00:01 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.47 on epoch=119
05/29/2022 21:00:04 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.43 on epoch=119
05/29/2022 21:00:06 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.41 on epoch=119
05/29/2022 21:00:09 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.43 on epoch=120
05/29/2022 21:00:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.43 on epoch=120
05/29/2022 21:00:23 - INFO - __main__ - Global step 2900 Train loss 0.44 Classification-F1 0.341129367968404 on epoch=120
05/29/2022 21:00:23 - INFO - __main__ - Saving model with best Classification-F1: 0.3220585204455272 -> 0.341129367968404 on epoch=120, global_step=2900
05/29/2022 21:00:26 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.45 on epoch=121
05/29/2022 21:00:28 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.41 on epoch=121
05/29/2022 21:00:31 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.43 on epoch=122
05/29/2022 21:00:34 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.40 on epoch=122
05/29/2022 21:00:36 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.42 on epoch=122
05/29/2022 21:00:48 - INFO - __main__ - Global step 2950 Train loss 0.42 Classification-F1 0.2981365854284084 on epoch=122
05/29/2022 21:00:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.44 on epoch=123
05/29/2022 21:00:53 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.40 on epoch=123
05/29/2022 21:00:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.46 on epoch=124
05/29/2022 21:00:58 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.45 on epoch=124
05/29/2022 21:01:01 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.42 on epoch=124
05/29/2022 21:01:02 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 21:01:02 - INFO - __main__ - Printing 3 examples
05/29/2022 21:01:02 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/29/2022 21:01:02 - INFO - __main__ - ['contradiction']
05/29/2022 21:01:02 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/29/2022 21:01:02 - INFO - __main__ - ['contradiction']
05/29/2022 21:01:02 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/29/2022 21:01:02 - INFO - __main__ - ['contradiction']
05/29/2022 21:01:02 - INFO - __main__ - Tokenizing Input ...
05/29/2022 21:01:02 - INFO - __main__ - Tokenizing Output ...
05/29/2022 21:01:03 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 21:01:03 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 21:01:03 - INFO - __main__ - Printing 3 examples
05/29/2022 21:01:03 - INFO - __main__ -  [anli] premise: Never Shout Never is an EP by Never Shout Never which was released on December 8, 2009. The physical release is sold exclusively at Hot Topic. The EP features two songs from his then upcoming Sire Records full-length debut, "What is Love?", one song that is a B-side from "The Summer EP" and one live track. [SEP] hypothesis:  The EP features two live tracks. 
05/29/2022 21:01:03 - INFO - __main__ - ['contradiction']
05/29/2022 21:01:03 - INFO - __main__ -  [anli] premise: The Hound of the Baskervilles is a 1978 British comedy film spoofing "The Hound of the Baskervilles" by Sir Arthur Conan Doyle. It starred Peter Cook as Sherlock Holmes and Dudley Moore as Dr. Watson. A number of other well-known British comedy actors appeared in the film including Terry-Thomas (in his final screen appearance), Kenneth Williams and Denholm Elliott. [SEP] hypothesis: The Hound of the Baskervilles is a american food dish.
05/29/2022 21:01:03 - INFO - __main__ - ['contradiction']
05/29/2022 21:01:03 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts is most famous for his music career.
05/29/2022 21:01:03 - INFO - __main__ - ['contradiction']
05/29/2022 21:01:03 - INFO - __main__ - Tokenizing Input ...
05/29/2022 21:01:03 - INFO - __main__ - Tokenizing Output ...
05/29/2022 21:01:03 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 21:01:12 - INFO - __main__ - Global step 3000 Train loss 0.43 Classification-F1 0.21086422557010795 on epoch=124
05/29/2022 21:01:12 - INFO - __main__ - save last model!
05/29/2022 21:01:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 21:01:12 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 21:01:12 - INFO - __main__ - Printing 3 examples
05/29/2022 21:01:12 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 21:01:12 - INFO - __main__ - ['contradiction']
05/29/2022 21:01:12 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 21:01:12 - INFO - __main__ - ['entailment']
05/29/2022 21:01:12 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 21:01:12 - INFO - __main__ - ['contradiction']
05/29/2022 21:01:12 - INFO - __main__ - Tokenizing Input ...
05/29/2022 21:01:13 - INFO - __main__ - Tokenizing Output ...
05/29/2022 21:01:14 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 21:01:22 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 21:01:22 - INFO - __main__ - task name: anli
05/29/2022 21:01:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 21:01:23 - INFO - __main__ - Starting training!
05/29/2022 21:01:43 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_87_0.3_8_predictions.txt
05/29/2022 21:01:43 - INFO - __main__ - Classification-F1 on test data: 0.2039
05/29/2022 21:01:44 - INFO - __main__ - prefix=anli_128_87, lr=0.3, bsz=8, dev_performance=0.341129367968404, test_performance=0.20393111014294582
05/29/2022 21:01:44 - INFO - __main__ - Running ... prefix=anli_128_87, lr=0.2, bsz=8 ...
05/29/2022 21:01:45 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 21:01:45 - INFO - __main__ - Printing 3 examples
05/29/2022 21:01:45 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/29/2022 21:01:45 - INFO - __main__ - ['contradiction']
05/29/2022 21:01:45 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/29/2022 21:01:45 - INFO - __main__ - ['contradiction']
05/29/2022 21:01:45 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/29/2022 21:01:45 - INFO - __main__ - ['contradiction']
05/29/2022 21:01:45 - INFO - __main__ - Tokenizing Input ...
05/29/2022 21:01:45 - INFO - __main__ - Tokenizing Output ...
05/29/2022 21:01:45 - INFO - __main__ - Loaded 384 examples from train data
05/29/2022 21:01:45 - INFO - __main__ - Start tokenizing ... 384 instances
05/29/2022 21:01:45 - INFO - __main__ - Printing 3 examples
05/29/2022 21:01:45 - INFO - __main__ -  [anli] premise: Never Shout Never is an EP by Never Shout Never which was released on December 8, 2009. The physical release is sold exclusively at Hot Topic. The EP features two songs from his then upcoming Sire Records full-length debut, "What is Love?", one song that is a B-side from "The Summer EP" and one live track. [SEP] hypothesis:  The EP features two live tracks. 
05/29/2022 21:01:45 - INFO - __main__ - ['contradiction']
05/29/2022 21:01:45 - INFO - __main__ -  [anli] premise: The Hound of the Baskervilles is a 1978 British comedy film spoofing "The Hound of the Baskervilles" by Sir Arthur Conan Doyle. It starred Peter Cook as Sherlock Holmes and Dudley Moore as Dr. Watson. A number of other well-known British comedy actors appeared in the film including Terry-Thomas (in his final screen appearance), Kenneth Williams and Denholm Elliott. [SEP] hypothesis: The Hound of the Baskervilles is a american food dish.
05/29/2022 21:01:45 - INFO - __main__ - ['contradiction']
05/29/2022 21:01:45 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts is most famous for his music career.
05/29/2022 21:01:45 - INFO - __main__ - ['contradiction']
05/29/2022 21:01:45 - INFO - __main__ - Tokenizing Input ...
05/29/2022 21:01:45 - INFO - __main__ - Tokenizing Output ...
05/29/2022 21:01:46 - INFO - __main__ - Loaded 384 examples from dev data
05/29/2022 21:02:02 - INFO - __main__ - try to initialize prompt embeddings
05/29/2022 21:02:02 - INFO - __main__ - task name: anli
05/29/2022 21:02:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 21:02:03 - INFO - __main__ - Starting training!
05/29/2022 21:02:06 - INFO - __main__ - Step 10 Global step 10 Train loss 7.24 on epoch=0
05/29/2022 21:02:08 - INFO - __main__ - Step 20 Global step 20 Train loss 4.54 on epoch=0
05/29/2022 21:02:11 - INFO - __main__ - Step 30 Global step 30 Train loss 2.73 on epoch=1
05/29/2022 21:02:14 - INFO - __main__ - Step 40 Global step 40 Train loss 1.74 on epoch=1
05/29/2022 21:02:16 - INFO - __main__ - Step 50 Global step 50 Train loss 1.22 on epoch=2
05/29/2022 21:02:25 - INFO - __main__ - Global step 50 Train loss 3.49 Classification-F1 0.16666666666666666 on epoch=2
05/29/2022 21:02:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/29/2022 21:02:27 - INFO - __main__ - Step 60 Global step 60 Train loss 1.06 on epoch=2
05/29/2022 21:02:30 - INFO - __main__ - Step 70 Global step 70 Train loss 0.93 on epoch=2
05/29/2022 21:02:33 - INFO - __main__ - Step 80 Global step 80 Train loss 0.90 on epoch=3
05/29/2022 21:02:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.68 on epoch=3
05/29/2022 21:02:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.68 on epoch=4
05/29/2022 21:02:49 - INFO - __main__ - Global step 100 Train loss 0.85 Classification-F1 0.20064471420568156 on epoch=4
05/29/2022 21:02:49 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.20064471420568156 on epoch=4, global_step=100
05/29/2022 21:02:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.72 on epoch=4
05/29/2022 21:02:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.71 on epoch=4
05/29/2022 21:02:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.63 on epoch=5
05/29/2022 21:02:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.64 on epoch=5
05/29/2022 21:03:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.67 on epoch=6
05/29/2022 21:03:13 - INFO - __main__ - Global step 150 Train loss 0.67 Classification-F1 0.3079898066511495 on epoch=6
05/29/2022 21:03:13 - INFO - __main__ - Saving model with best Classification-F1: 0.20064471420568156 -> 0.3079898066511495 on epoch=6, global_step=150
05/29/2022 21:03:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.67 on epoch=6
05/29/2022 21:03:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.55 on epoch=7
05/29/2022 21:03:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.73 on epoch=7
05/29/2022 21:03:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=7
05/29/2022 21:03:27 - INFO - __main__ - Step 200 Global step 200 Train loss 0.61 on epoch=8
05/29/2022 21:03:35 - INFO - __main__ - Global step 200 Train loss 0.62 Classification-F1 0.16666666666666666 on epoch=8
05/29/2022 21:03:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=8
05/29/2022 21:03:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.61 on epoch=9
05/29/2022 21:03:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.60 on epoch=9
05/29/2022 21:03:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.56 on epoch=9
05/29/2022 21:03:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.65 on epoch=10
05/29/2022 21:04:00 - INFO - __main__ - Global step 250 Train loss 0.59 Classification-F1 0.2518453094441991 on epoch=10
05/29/2022 21:04:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.54 on epoch=10
05/29/2022 21:04:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.55 on epoch=11
05/29/2022 21:04:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.58 on epoch=11
05/29/2022 21:04:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=12
05/29/2022 21:04:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.62 on epoch=12
05/29/2022 21:04:24 - INFO - __main__ - Global step 300 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=12
05/29/2022 21:04:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=12
05/29/2022 21:04:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.63 on epoch=13
05/29/2022 21:04:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.53 on epoch=13
05/29/2022 21:04:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.56 on epoch=14
05/29/2022 21:04:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.54 on epoch=14
05/29/2022 21:04:48 - INFO - __main__ - Global step 350 Train loss 0.55 Classification-F1 0.18115144847818113 on epoch=14
05/29/2022 21:04:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=14
05/29/2022 21:04:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.53 on epoch=15
05/29/2022 21:04:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=15
05/29/2022 21:04:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.51 on epoch=16
05/29/2022 21:05:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.53 on epoch=16
05/29/2022 21:05:13 - INFO - __main__ - Global step 400 Train loss 0.52 Classification-F1 0.26591091701882313 on epoch=16
05/29/2022 21:05:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.52 on epoch=17
05/29/2022 21:05:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.63 on epoch=17
05/29/2022 21:05:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.49 on epoch=17
05/29/2022 21:05:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.53 on epoch=18
05/29/2022 21:05:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=18
05/29/2022 21:05:38 - INFO - __main__ - Global step 450 Train loss 0.54 Classification-F1 0.1988215475325562 on epoch=18
05/29/2022 21:05:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.53 on epoch=19
05/29/2022 21:05:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=19
05/29/2022 21:05:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=19
05/29/2022 21:05:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.58 on epoch=20
05/29/2022 21:05:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=20
05/29/2022 21:06:04 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.18627450980392157 on epoch=20
05/29/2022 21:06:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.51 on epoch=21
05/29/2022 21:06:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=21
05/29/2022 21:06:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=22
05/29/2022 21:06:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.54 on epoch=22
05/29/2022 21:06:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=22
05/29/2022 21:06:29 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=22
05/29/2022 21:06:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=23
05/29/2022 21:06:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.51 on epoch=23
05/29/2022 21:06:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=24
05/29/2022 21:06:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.51 on epoch=24
05/29/2022 21:06:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=24
05/29/2022 21:06:54 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.2603854476740051 on epoch=24
05/29/2022 21:06:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.52 on epoch=25
05/29/2022 21:07:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=25
05/29/2022 21:07:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.54 on epoch=26
05/29/2022 21:07:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=26
05/29/2022 21:07:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.50 on epoch=27
05/29/2022 21:07:19 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.1745956528190621 on epoch=27
05/29/2022 21:07:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=27
05/29/2022 21:07:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.49 on epoch=27
05/29/2022 21:07:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.54 on epoch=28
05/29/2022 21:07:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.50 on epoch=28
05/29/2022 21:07:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.51 on epoch=29
05/29/2022 21:07:44 - INFO - __main__ - Global step 700 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=29
05/29/2022 21:07:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.49 on epoch=29
05/29/2022 21:07:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=29
05/29/2022 21:07:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=30
05/29/2022 21:07:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=30
05/29/2022 21:07:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=31
05/29/2022 21:08:10 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.24076004960498176 on epoch=31
05/29/2022 21:08:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.49 on epoch=31
05/29/2022 21:08:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=32
05/29/2022 21:08:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.52 on epoch=32
05/29/2022 21:08:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=32
05/29/2022 21:08:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=33
05/29/2022 21:08:35 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=33
05/29/2022 21:08:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=33
05/29/2022 21:08:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.51 on epoch=34
05/29/2022 21:08:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.47 on epoch=34
05/29/2022 21:08:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.48 on epoch=34
05/29/2022 21:08:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=35
05/29/2022 21:08:58 - INFO - __main__ - Global step 850 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=35
05/29/2022 21:09:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=35
05/29/2022 21:09:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.47 on epoch=36
05/29/2022 21:09:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=36
05/29/2022 21:09:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.48 on epoch=37
05/29/2022 21:09:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.53 on epoch=37
05/29/2022 21:09:24 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=37
05/29/2022 21:09:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=37
05/29/2022 21:09:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.48 on epoch=38
05/29/2022 21:09:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.47 on epoch=38
05/29/2022 21:09:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.52 on epoch=39
05/29/2022 21:09:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.47 on epoch=39
05/29/2022 21:09:50 - INFO - __main__ - Global step 950 Train loss 0.47 Classification-F1 0.28184329630282723 on epoch=39
05/29/2022 21:09:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.47 on epoch=39
05/29/2022 21:09:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.51 on epoch=40
05/29/2022 21:09:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.48 on epoch=40
05/29/2022 21:10:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=41
05/29/2022 21:10:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.50 on epoch=41
05/29/2022 21:10:14 - INFO - __main__ - Global step 1000 Train loss 0.48 Classification-F1 0.20667397438335847 on epoch=41
05/29/2022 21:10:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.47 on epoch=42
05/29/2022 21:10:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=42
05/29/2022 21:10:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=42
05/29/2022 21:10:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=43
05/29/2022 21:10:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=43
05/29/2022 21:10:39 - INFO - __main__ - Global step 1050 Train loss 0.46 Classification-F1 0.17806114389925184 on epoch=43
05/29/2022 21:10:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.52 on epoch=44
05/29/2022 21:10:44 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=44
05/29/2022 21:10:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.49 on epoch=44
05/29/2022 21:10:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.47 on epoch=45
05/29/2022 21:10:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.47 on epoch=45
05/29/2022 21:11:03 - INFO - __main__ - Global step 1100 Train loss 0.48 Classification-F1 0.24018952062430324 on epoch=45
05/29/2022 21:11:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=46
05/29/2022 21:11:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.47 on epoch=46
05/29/2022 21:11:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=47
05/29/2022 21:11:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.50 on epoch=47
05/29/2022 21:11:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.45 on epoch=47
05/29/2022 21:11:28 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.1721607831834019 on epoch=47
05/29/2022 21:11:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.52 on epoch=48
05/29/2022 21:11:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.44 on epoch=48
05/29/2022 21:11:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.47 on epoch=49
05/29/2022 21:11:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.47 on epoch=49
05/29/2022 21:11:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=49
05/29/2022 21:11:52 - INFO - __main__ - Global step 1200 Train loss 0.47 Classification-F1 0.22018031719524256 on epoch=49
05/29/2022 21:11:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=50
05/29/2022 21:11:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.46 on epoch=50
05/29/2022 21:12:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.43 on epoch=51
05/29/2022 21:12:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.52 on epoch=51
05/29/2022 21:12:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=52
05/29/2022 21:12:16 - INFO - __main__ - Global step 1250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=52
05/29/2022 21:12:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=52
05/29/2022 21:12:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=52
05/29/2022 21:12:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.56 on epoch=53
05/29/2022 21:12:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=53
05/29/2022 21:12:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.50 on epoch=54
05/29/2022 21:12:41 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.17894578998031277 on epoch=54
05/29/2022 21:12:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.45 on epoch=54
05/29/2022 21:12:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.52 on epoch=54
05/29/2022 21:12:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=55
05/29/2022 21:12:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=55
05/29/2022 21:12:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.48 on epoch=56
05/29/2022 21:13:04 - INFO - __main__ - Global step 1350 Train loss 0.46 Classification-F1 0.282486956229973 on epoch=56
05/29/2022 21:13:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.47 on epoch=56
05/29/2022 21:13:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=57
05/29/2022 21:13:12 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=57
05/29/2022 21:13:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.44 on epoch=57
05/29/2022 21:13:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.48 on epoch=58
05/29/2022 21:13:28 - INFO - __main__ - Global step 1400 Train loss 0.45 Classification-F1 0.16699282452707112 on epoch=58
05/29/2022 21:13:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.46 on epoch=58
05/29/2022 21:13:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.50 on epoch=59
05/29/2022 21:13:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.44 on epoch=59
05/29/2022 21:13:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=59
05/29/2022 21:13:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.49 on epoch=60
05/29/2022 21:13:52 - INFO - __main__ - Global step 1450 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=60
05/29/2022 21:13:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=60
05/29/2022 21:13:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.48 on epoch=61
05/29/2022 21:14:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.48 on epoch=61
05/29/2022 21:14:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=62
05/29/2022 21:14:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.46 on epoch=62
05/29/2022 21:14:16 - INFO - __main__ - Global step 1500 Train loss 0.45 Classification-F1 0.19561047399044093 on epoch=62
05/29/2022 21:14:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.45 on epoch=62
05/29/2022 21:14:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=63
05/29/2022 21:14:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.51 on epoch=63
05/29/2022 21:14:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.46 on epoch=64
05/29/2022 21:14:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.52 on epoch=64
05/29/2022 21:14:41 - INFO - __main__ - Global step 1550 Train loss 0.48 Classification-F1 0.21134326334875364 on epoch=64
05/29/2022 21:14:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.48 on epoch=64
05/29/2022 21:14:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.44 on epoch=65
05/29/2022 21:14:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.54 on epoch=65
05/29/2022 21:14:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.45 on epoch=66
05/29/2022 21:14:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.46 on epoch=66
05/29/2022 21:15:05 - INFO - __main__ - Global step 1600 Train loss 0.47 Classification-F1 0.31050398827356823 on epoch=66
05/29/2022 21:15:05 - INFO - __main__ - Saving model with best Classification-F1: 0.3079898066511495 -> 0.31050398827356823 on epoch=66, global_step=1600
05/29/2022 21:15:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=67
05/29/2022 21:15:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.49 on epoch=67
05/29/2022 21:15:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.42 on epoch=67
05/29/2022 21:15:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=68
05/29/2022 21:15:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.41 on epoch=68
05/29/2022 21:15:30 - INFO - __main__ - Global step 1650 Train loss 0.43 Classification-F1 0.3110467628213967 on epoch=68
05/29/2022 21:15:30 - INFO - __main__ - Saving model with best Classification-F1: 0.31050398827356823 -> 0.3110467628213967 on epoch=68, global_step=1650
05/29/2022 21:15:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.48 on epoch=69
05/29/2022 21:15:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.46 on epoch=69
05/29/2022 21:15:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.49 on epoch=69
05/29/2022 21:15:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.47 on epoch=70
05/29/2022 21:15:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.43 on epoch=70
05/29/2022 21:15:54 - INFO - __main__ - Global step 1700 Train loss 0.47 Classification-F1 0.19327006039689387 on epoch=70
05/29/2022 21:15:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.46 on epoch=71
05/29/2022 21:15:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.48 on epoch=71
05/29/2022 21:16:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.42 on epoch=72
05/29/2022 21:16:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.48 on epoch=72
05/29/2022 21:16:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=72
05/29/2022 21:16:18 - INFO - __main__ - Global step 1750 Train loss 0.45 Classification-F1 0.1775766716943188 on epoch=72
05/29/2022 21:16:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.43 on epoch=73
05/29/2022 21:16:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=73
05/29/2022 21:16:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.50 on epoch=74
05/29/2022 21:16:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.44 on epoch=74
05/29/2022 21:16:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.42 on epoch=74
05/29/2022 21:16:41 - INFO - __main__ - Global step 1800 Train loss 0.45 Classification-F1 0.24668064290705796 on epoch=74
05/29/2022 21:16:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.47 on epoch=75
05/29/2022 21:16:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.38 on epoch=75
05/29/2022 21:16:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.43 on epoch=76
05/29/2022 21:16:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.48 on epoch=76
05/29/2022 21:16:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=77
05/29/2022 21:17:05 - INFO - __main__ - Global step 1850 Train loss 0.44 Classification-F1 0.16699282452707112 on epoch=77
05/29/2022 21:17:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.48 on epoch=77
05/29/2022 21:17:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=77
05/29/2022 21:17:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.46 on epoch=78
05/29/2022 21:17:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.49 on epoch=78
05/29/2022 21:17:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.46 on epoch=79
05/29/2022 21:17:29 - INFO - __main__ - Global step 1900 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=79
05/29/2022 21:17:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.43 on epoch=79
05/29/2022 21:17:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.44 on epoch=79
05/29/2022 21:17:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.44 on epoch=80
05/29/2022 21:17:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.43 on epoch=80
05/29/2022 21:17:42 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=81
05/29/2022 21:17:53 - INFO - __main__ - Global step 1950 Train loss 0.43 Classification-F1 0.21079620120098028 on epoch=81
05/29/2022 21:17:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=81
05/29/2022 21:17:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=82
05/29/2022 21:18:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.53 on epoch=82
05/29/2022 21:18:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.44 on epoch=82
05/29/2022 21:18:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.52 on epoch=83
05/29/2022 21:18:17 - INFO - __main__ - Global step 2000 Train loss 0.47 Classification-F1 0.26834963396671896 on epoch=83
05/29/2022 21:18:19 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.46 on epoch=83
05/29/2022 21:18:22 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.41 on epoch=84
05/29/2022 21:18:25 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.47 on epoch=84
05/29/2022 21:18:27 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.46 on epoch=84
05/29/2022 21:18:30 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.48 on epoch=85
05/29/2022 21:18:41 - INFO - __main__ - Global step 2050 Train loss 0.46 Classification-F1 0.2293303404414516 on epoch=85
05/29/2022 21:18:44 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.40 on epoch=85
05/29/2022 21:18:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.43 on epoch=86
05/29/2022 21:18:49 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.43 on epoch=86
05/29/2022 21:18:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.43 on epoch=87
05/29/2022 21:18:54 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.45 on epoch=87
05/29/2022 21:19:05 - INFO - __main__ - Global step 2100 Train loss 0.43 Classification-F1 0.16699282452707112 on epoch=87
05/29/2022 21:19:08 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.43 on epoch=87
05/29/2022 21:19:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.50 on epoch=88
05/29/2022 21:19:13 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.47 on epoch=88
05/29/2022 21:19:15 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.46 on epoch=89
05/29/2022 21:19:18 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.42 on epoch=89
05/29/2022 21:19:29 - INFO - __main__ - Global step 2150 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=89
05/29/2022 21:19:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.41 on epoch=89
05/29/2022 21:19:34 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.47 on epoch=90
05/29/2022 21:19:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.50 on epoch=90
05/29/2022 21:19:40 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.46 on epoch=91
05/29/2022 21:19:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.41 on epoch=91
05/29/2022 21:19:53 - INFO - __main__ - Global step 2200 Train loss 0.45 Classification-F1 0.25173154824748595 on epoch=91
05/29/2022 21:19:56 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.42 on epoch=92
05/29/2022 21:19:59 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.47 on epoch=92
05/29/2022 21:20:01 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.40 on epoch=92
05/29/2022 21:20:04 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.43 on epoch=93
05/29/2022 21:20:06 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.43 on epoch=93
05/29/2022 21:20:18 - INFO - __main__ - Global step 2250 Train loss 0.43 Classification-F1 0.2832319775365701 on epoch=93
05/29/2022 21:20:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.48 on epoch=94
05/29/2022 21:20:23 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.44 on epoch=94
05/29/2022 21:20:26 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.42 on epoch=94
05/29/2022 21:20:28 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.44 on epoch=95
05/29/2022 21:20:31 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.44 on epoch=95
05/29/2022 21:20:42 - INFO - __main__ - Global step 2300 Train loss 0.44 Classification-F1 0.2776066393961843 on epoch=95
05/29/2022 21:20:44 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.46 on epoch=96
05/29/2022 21:20:47 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.46 on epoch=96
05/29/2022 21:20:50 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.47 on epoch=97
05/29/2022 21:20:52 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.46 on epoch=97
05/29/2022 21:20:55 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.46 on epoch=97
05/29/2022 21:21:06 - INFO - __main__ - Global step 2350 Train loss 0.46 Classification-F1 0.27414903152089876 on epoch=97
05/29/2022 21:21:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.43 on epoch=98
05/29/2022 21:21:11 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.41 on epoch=98
05/29/2022 21:21:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.44 on epoch=99
05/29/2022 21:21:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.46 on epoch=99
05/29/2022 21:21:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.43 on epoch=99
05/29/2022 21:21:30 - INFO - __main__ - Global step 2400 Train loss 0.43 Classification-F1 0.2264476315587517 on epoch=99
05/29/2022 21:21:33 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.44 on epoch=100
05/29/2022 21:21:36 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.43 on epoch=100
05/29/2022 21:21:38 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.49 on epoch=101
05/29/2022 21:21:41 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.43 on epoch=101
05/29/2022 21:21:43 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.43 on epoch=102
05/29/2022 21:21:55 - INFO - __main__ - Global step 2450 Train loss 0.44 Classification-F1 0.19567987433426937 on epoch=102
05/29/2022 21:21:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.47 on epoch=102
05/29/2022 21:22:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.43 on epoch=102
05/29/2022 21:22:03 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.45 on epoch=103
05/29/2022 21:22:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.43 on epoch=103
05/29/2022 21:22:08 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.46 on epoch=104
05/29/2022 21:22:20 - INFO - __main__ - Global step 2500 Train loss 0.45 Classification-F1 0.2893543157105643 on epoch=104
05/29/2022 21:22:22 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.44 on epoch=104
05/29/2022 21:22:25 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.45 on epoch=104
05/29/2022 21:22:27 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.48 on epoch=105
05/29/2022 21:22:30 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.44 on epoch=105
05/29/2022 21:22:33 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.44 on epoch=106
05/29/2022 21:22:44 - INFO - __main__ - Global step 2550 Train loss 0.45 Classification-F1 0.2261291859160076 on epoch=106
05/29/2022 21:22:47 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.46 on epoch=106
05/29/2022 21:22:50 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.42 on epoch=107
05/29/2022 21:22:52 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.47 on epoch=107
05/29/2022 21:22:55 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.39 on epoch=107
05/29/2022 21:22:57 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.43 on epoch=108
05/29/2022 21:23:09 - INFO - __main__ - Global step 2600 Train loss 0.43 Classification-F1 0.22012841789718054 on epoch=108
05/29/2022 21:23:12 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.48 on epoch=108
05/29/2022 21:23:14 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.50 on epoch=109
05/29/2022 21:23:17 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.46 on epoch=109
05/29/2022 21:23:20 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.45 on epoch=109
05/29/2022 21:23:22 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.45 on epoch=110
05/29/2022 21:23:34 - INFO - __main__ - Global step 2650 Train loss 0.47 Classification-F1 0.2633421257817814 on epoch=110
05/29/2022 21:23:36 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.40 on epoch=110
05/29/2022 21:23:39 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.44 on epoch=111
05/29/2022 21:23:42 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.46 on epoch=111
05/29/2022 21:23:44 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.44 on epoch=112
05/29/2022 21:23:47 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.46 on epoch=112
05/29/2022 21:23:58 - INFO - __main__ - Global step 2700 Train loss 0.44 Classification-F1 0.27380603124310215 on epoch=112
05/29/2022 21:24:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.38 on epoch=112
05/29/2022 21:24:04 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.45 on epoch=113
05/29/2022 21:24:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.46 on epoch=113
05/29/2022 21:24:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.42 on epoch=114
05/29/2022 21:24:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.45 on epoch=114
05/29/2022 21:24:23 - INFO - __main__ - Global step 2750 Train loss 0.43 Classification-F1 0.17248822009423928 on epoch=114
05/29/2022 21:24:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.44 on epoch=114
05/29/2022 21:24:28 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.44 on epoch=115
05/29/2022 21:24:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.42 on epoch=115
05/29/2022 21:24:33 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.44 on epoch=116
05/29/2022 21:24:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.48 on epoch=116
05/29/2022 21:24:47 - INFO - __main__ - Global step 2800 Train loss 0.44 Classification-F1 0.2589514066496164 on epoch=116
05/29/2022 21:24:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.43 on epoch=117
05/29/2022 21:24:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.46 on epoch=117
05/29/2022 21:24:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.39 on epoch=117
05/29/2022 21:24:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.45 on epoch=118
05/29/2022 21:25:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.40 on epoch=118
05/29/2022 21:25:12 - INFO - __main__ - Global step 2850 Train loss 0.42 Classification-F1 0.30525463318159346 on epoch=118
05/29/2022 21:25:15 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.47 on epoch=119
05/29/2022 21:25:17 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.41 on epoch=119
05/29/2022 21:25:20 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.44 on epoch=119
05/29/2022 21:25:23 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.46 on epoch=120
05/29/2022 21:25:25 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.40 on epoch=120
05/29/2022 21:25:36 - INFO - __main__ - Global step 2900 Train loss 0.44 Classification-F1 0.20774929237936593 on epoch=120
05/29/2022 21:25:39 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.40 on epoch=121
05/29/2022 21:25:42 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.48 on epoch=121
05/29/2022 21:25:44 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.42 on epoch=122
05/29/2022 21:25:47 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.48 on epoch=122
05/29/2022 21:25:49 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.41 on epoch=122
05/29/2022 21:26:01 - INFO - __main__ - Global step 2950 Train loss 0.44 Classification-F1 0.23714517815428504 on epoch=122
05/29/2022 21:26:03 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.43 on epoch=123
05/29/2022 21:26:06 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.39 on epoch=123
05/29/2022 21:26:08 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.43 on epoch=124
05/29/2022 21:26:11 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.39 on epoch=124
05/29/2022 21:26:14 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.40 on epoch=124
05/29/2022 21:26:25 - INFO - __main__ - Global step 3000 Train loss 0.41 Classification-F1 0.23061038941025994 on epoch=124
05/29/2022 21:26:25 - INFO - __main__ - save last model!
05/29/2022 21:26:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 21:26:25 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 21:26:25 - INFO - __main__ - Printing 3 examples
05/29/2022 21:26:25 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 21:26:25 - INFO - __main__ - ['contradiction']
05/29/2022 21:26:25 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 21:26:25 - INFO - __main__ - ['entailment']
05/29/2022 21:26:25 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 21:26:25 - INFO - __main__ - ['contradiction']
05/29/2022 21:26:25 - INFO - __main__ - Tokenizing Input ...
05/29/2022 21:26:26 - INFO - __main__ - Tokenizing Output ...
05/29/2022 21:26:27 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 21:26:57 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-anli/anli_128_87_0.2_8_predictions.txt
05/29/2022 21:26:57 - INFO - __main__ - Classification-F1 on test data: 0.2304
05/29/2022 21:26:57 - INFO - __main__ - prefix=anli_128_87, lr=0.2, bsz=8, dev_performance=0.3110467628213967, test_performance=0.23041966381702006
